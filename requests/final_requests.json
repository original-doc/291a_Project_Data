[
        {
        "query": "Does pytorch-lightning support saving and loading models from remote cloud storage?",
        "query_type": "doc_reference",
        "relevant_docs": [
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/common/checkpointing_advanced.html",
                "index": 9,
                "text": "### Cloud checkpoints\nLightning is integrated with the major remote file systems including local filesystems and several cloud storage providers such as\nS3 on AWS, GCS on Google Cloud,\nor ADL on Azure.",
                "score": 10
            },
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/common/checkpointing_advanced.html",
                "index": 9,
                "text": "To save to a remote filesystem, prepend a protocol like \"s3:/\" to the root_dir used for writing and reading model data.",
                "score": 9
            },
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/common/checkpointing_advanced.html",
                "index": 9,
                "text": "To resume training from a cloud checkpoint use a cloud url.",
                "score": 9
            }
        ]
    },
    {
        "query": "How to figure out the slow part in the code?",
        "query_type": "doc_reference",
        "relevant_docs": [
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/tuning/profiler_basic.html",
                "index": 57,
                "text": "Profiling helps you find bottlenecks in your code by capturing analytics such as how long a function takes or how much memory is used.",
                "score": 10
            }
        ]
    },
    {
        "query": "Does lightning require using torch dataloader as the data loader?",
        "query_type": "doc_reference",
        "relevant_docs": [
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/starter/introduction.html",
                "index": 53,
                "text": "Lightning supports ANY iterable (~torch.utils.data.DataLoader, numpy, etc...) for the train/val/test/predict splits.",
                "score": 10
            }
        ]
    },
    {
        "query": "How much improvement can be expected when using half precision compared to full precision?",
        "query_type": "doc_reference",
        "relevant_docs": [
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/common/precision_basic.html",
                "index": 23,
                "text": "If your GPUs are [Tensor Core] GPUs, you can expect a ~3x speed improvement.",
                "score": 10
            },
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/common/precision_basic.html",
                "index": 23,
                "text": "Use 16-bit mixed precision to speed up training and inference.",
                "score": 7
            }
        ]
    },
    {
        "query": "Can I use two config files' configuration in one runtime?",
        "query_type": "doc_reference",
        "relevant_docs": [
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
                "index": 1,
                "text": "### Compose config files\nMultiple config files can be provided, and they will be parsed sequentially.",
                "score": 10
            },
            {
                "file": "docs.json",
                "entry_filename": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
                "index": 1,
                "text": "Let's say we have two configs with common\nsettings:\n\n```yaml\n# config_1.yaml\ntrainer:\n  num_epochs: 10\n  ...\n\n# config_2.yaml\ntrainer:\n  num_epochs: 20\n  ...\n```\nThe value from the last config will be used, num_epochs = 20 in this case:\n\n```bash\n$ python main.py fit --config config_1.yaml --config config_2.yaml\n```",
                "score": 8
            }
        ]
    },
    {
    "query": "Training is taking a long time. How do I speed up the training for multiple datasets?",
    "query_type": "debugging",
    "relevant_docs": [
      {
        "file": "../data/final_data/discussions.json",
        "index": 1,
        "text": "Multiple Sequential trainings slows down speed\nHi.\nI have a task where I need to run a training script multiple time with a for-loop like this:\nfor dataset in datasets:\n      ... Training script with datamodule, model, Trainer Init and trainer.fit()\n\nHowever, after each loop the the training it self slows down incrementally (epoch / sec). I am thinking it is because I need to reset something but I have not been able to find that information. I am using Torch-cpu 2.0.0\nAny idea on what I am doing wrong? :(\nAnswer: I tried to remove the neptune logger and model saving functionality and it seemed to have solved the issue",
        "score": 7
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 57,
        "text": "Find bottlenecks in your code (basic)\n## Find bottlenecks in your code (basic)\n**Audience**: Users who want to learn the basics of removing bottlenecks from their code\n\n----\n\n### Why do I need profiling?\nProfiling helps you find bottlenecks in your code by capturing analytics such as how long a function takes or how much memory is used.\n\n------------\n\n### Find training loop bottlenecks\nThe most basic profile measures all the key methods across **Callbacks**, **DataModules** and the **LightningModule** in the training loop.\n\n```python\ntrainer = Trainer(profiler=\"simple\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nFIT Profiler Report\n\n-------------------------------------------------------------------------------------------\n|  Action                                          |  Mean duration (s) |  Total time (s) |\n-------------------------------------------------------------------------------------------\n|  [LightningModule]BoringModel.prepare_data       |  10.0001           |  20.00          |\n|  run_training_epoch                              |  6.1558            |  6.1558         |\n|  run_training_batch                              |  0.0022506         |  0.015754       |\n|  [LightningModule]BoringModel.optimizer_step     |  0.0017477         |  0.012234       |\n|  [LightningModule]BoringModel.val_dataloader     |  0.00024388        |  0.00024388     |\n|  on_train_batch_start                            |  0.00014637        |  0.0010246      |\n|  [LightningModule]BoringModel.teardown           |  2.15e-06          |  2.15e-06       |\n|  [LightningModule]BoringModel.on_train_start     |  1.644e-06         |  1.644e-06      |\n|  [LightningModule]BoringModel.on_train_end       |  1.516e-06         |  1.516e-06      |\n|  [LightningModule]BoringModel.on_fit_end         |  1.426e-06         |  1.426e-06      |\n|  [LightningModule]BoringModel.setup              |  1.403e-06         |  1.403e-06      |\n|  [LightningModule]BoringModel.on_fit_start       |  1.226e-06         |  1.226e-06      |\n-------------------------------------------------------------------------------------------\n```\nIn this report we can see that the slowest function is **prepare_data**. Now you can figure out why data preparation is slowing down your training.\n\nThe simple profiler measures all the standard methods used in the training loop automatically, including:\n\n- on_train_epoch_start\n- on_train_epoch_end\n- on_train_batch_start\n- model_backward\n- on_after_backward\n- optimizer_step\n- on_train_batch_end\n- on_training_end\n- etc...\n\n----\n\n### Profile the time within every function\nTo profile the time within every function, use the ~lightning.pytorch.profilers.advanced.AdvancedProfiler built on top of Python's cProfiler.\n\n```python\ntrainer = Trainer(profiler=\"advanced\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nProfiler Report\n\nProfile stats for: get_train_batch\n        4869394 function calls (4863767 primitive calls) in 18.893 seconds\nOrdered by: cumulative time\nList reduced from 76 to 10 due to restriction <10>\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n    1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n    1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n    1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n    1875     0.084    0.000   18.290    0.010 fetch.py:44(<listcomp>)\n    60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n    60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n    60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n    60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n    60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n```\nIf the profiler report becomes too long, you can stream the report to a file:\n\n```python\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nprofiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\ntrainer = Trainer(profiler=profiler)\n```\n----\n\n### Measure accelerator usage\nAnother helpful technique to detect bottlenecks is to ensure that you're using the full capacity of your accelerator (GPU/TPU/HPU).\nThis can be measured with the ~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor:\n\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\n\ntrainer = Trainer(callbacks=[DeviceStatsMonitor()])\nCPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set DeviceStatsMonitor(cpu_stats=True). To disable logging\nCPU metrics, you can specify DeviceStatsMonitor(cpu_stats=False).\n\n**Do not wrap** Trainer.fit(), Trainer.validate(), or other Trainer methods inside a manual\ntorch.profiler.profile context manager. This will cause unexpected crashes and cryptic errors due to\nincompatibility between PyTorch Profiler's context management and Lightning's internal training loop.\nInstead, always use the profiler argument in the Trainer constructor or the\n~lightning.pytorch.profilers.pytorch.PyTorchProfiler profiler class if you want to customize the profiling.\n\nExample:\n\n.. code-block:: python\n\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.profilers import PytorchProfiler\n\ntrainer = Trainer(profiler=\"pytorch\")\n# or\ntrainer = Trainer(profiler=PytorchProfiler(dirpath=\".\", filename=\"perf_logs\"))",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 58,
        "text": "Find bottlenecks in your code (expert)\n## Find bottlenecks in your code (expert)\n**Audience**: Users who want to build their own profilers.\n\n----\n\n### Build your own profiler\nTo build your own profiler, subclass ~lightning.pytorch.profilers.profiler.Profiler\nand override some of its methods. Here is a simple example that profiles the first occurrence and total calls of each action:\n\n```python\nfrom lightning.pytorch.profilers import Profiler\nfrom collections import defaultdict\nimport time\n\nclass ActionCountProfiler(Profiler):\n    def __init__(self, dirpath=None, filename=None):\n        super().__init__(dirpath=dirpath, filename=filename)\n        self._action_count = defaultdict(int)\n        self._action_first_occurrence = {}\n\n    def start(self, action_name):\n        if action_name not in self._action_first_occurrence:\n            self._action_first_occurrence[action_name] = time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n\n    def stop(self, action_name):\n        self._action_count[action_name] += 1\n\n    def summary(self):\n        res = f\"\\nProfile Summary: \\n\"\n        max_len = max(len(x) for x in self._action_count)\n\n        for action_name in self._action_count:\n            # generate summary for actions called more than once\n            if self._action_count[action_name] > 1:\n                res += (\n                    f\"{action_name:<{max_len}s} \\t \"\n                    + \"self._action_first_occurrence[action_name]} \\t \"\n                    + \"{self._action_count[action_name]} \\n\"\n                )\n\n        return res\n\n    def teardown(self, stage):\n        self._action_count = {}\n        self._action_first_occurrence = {}\n        super().teardown(stage=stage)\n```\n```python\ntrainer = Trainer(profiler=ActionCountProfiler())\ntrainer.fit(...)\n```\n----\n\n### Profile custom actions of interest\nTo profile a specific action of interest, reference a profiler in the LightningModule.\n\n```python\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n```\nTo profile in any part of your code, use the **self.profiler.profile()** function\n\n```python\nclass MyModel(LightningModule):\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n```\nHere's the full code:\n\n```python\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n\nprofiler = SimpleProfiler()\nmodel = MyModel(profiler)\ntrainer = Trainer(profiler=profiler, max_epochs=1)\n```",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 59,
        "text": "Find bottlenecks in your code (intermediate)\n## Find bottlenecks in your code (intermediate)\n**Audience**: Users who want to see more granular profiling information\n\n----\n\n### Profile pytorch operations\nTo understand the cost of each PyTorch operation, use the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler built on top of the PyTorch profiler_.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler()\ntrainer = Trainer(profiler=profiler)\n```\nThe profiler will generate an output like this:\n\n```\nProfiler Report\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\nWhen using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\nThis is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\nIt is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\nthe SimpleProfiler.\n----\n\n### Profile a distributed model\nTo profile a distributed model, use the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler with the *filename* argument which will save a report per rank.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(filename=\"perf-logs\")\ntrainer = Trainer(profiler=profiler)\n```\nWith two ranks, it will generate a report like so:\n\n```\nProfiler Report: rank 0\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\n```\nProfiler Report: rank 1\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      42.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\nThis profiler will record training_step, validation_step, test_step, and predict_step.\nThe output above shows the profiling for the action training_step.\n\nWhen using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\nThis is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\nIt is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\nthe SimpleProfiler.\n----\n\n### Visualize profiled operations\nTo visualize the profiled operations, enable **emit_nvtx** in the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(emit_nvtx=True)\ntrainer = Trainer(profiler=profiler)\n```\nThen run as following:\n\n```\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n```\nTo visualize the profiled operation, you can either use **nvvp**:\n\n```\nnvvp trace_name.prof\n```\nor python:\n\n```\npython -c 'import torch; print(torch.autograd.profiler.load_nvprof(\"trace_name.prof\"))'\n```",
        "score": 9
      }
    ]
  },
  {
    "query": "What do I need to pass into LightningCLI() to get it working? Also, please give an example.",
    "query_type": "api_usage",
    "relevant_docs": [
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 6,
        "text": "Configure hyperparameters from the CLI (Intermediate)\n## Configure hyperparameters from the CLI (Intermediate)\n**Audience:** Users who want advanced modularity via a command line interface (CLI).\n\n**Pre-reqs:** You must already understand how to use the command line and LightningDataModule <../data/datamodule>.\n\n----\n\n### LightningCLI requirements\n\nThe ~lightning.pytorch.cli.LightningCLI class is designed to significantly ease the implementation of CLIs. To\nuse this class, an additional Python requirement is necessary than the minimal installation of Lightning provides. To\nenable, either install all extras:\n\n```bash\npip install \"lightning[pytorch-extra]\"\n```\nor if only interested in LightningCLI, just install jsonargparse:\n\n```bash\npip install \"jsonargparse[signatures]\"\n```\n----\n\n### Implementing a CLI\nImplementing a CLI is as simple as instantiating a ~lightning.pytorch.cli.LightningCLI object giving as\narguments classes for a LightningModule and optionally a LightningDataModule:\n\n```python\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\n\n# simple demo classes for your convenience\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\ndef cli_main():\n    cli = LightningCLI(DemoModel, BoringDataModule)\n    # note: don't call fit!!\n\nif __name__ == \"__main__\":\n    cli_main()\n    # note: it is good practice to implement the CLI in a function and call it in the main if block\n```\nNow your model can be managed via the CLI. To see the available commands type:\n\n```bash\n$ python main.py --help\n```\nwhich prints out:\n\n```bash\nusage: main.py [-h] [-c CONFIG] [--print_config [={comments,skip_null,skip_default}+]]\n        {fit,validate,test,predict} ...\n\nLightning Trainer command line tool\n\noptional arguments:\n-h, --help            Show this help message and exit.\n-c CONFIG, --config CONFIG\n                        Path to a configuration file in json or yaml format.\n--print_config [={comments,skip_null,skip_default}+]\n                        Print configuration and exit.\n\nsubcommands:\nFor more details of each subcommand add it as argument followed by --help.\n\n{fit,validate,test,predict}\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n```\nThe message tells us that we have a few available subcommands:\n\n```bash\npython main.py [subcommand]\n```\nwhich you can use depending on your use case:\n\n```bash\n$ python main.py fit\n$ python main.py validate\n$ python main.py test\n$ python main.py predict\n```\n----\n\n### Train a model with the CLI\nTo train a model, use the fit subcommand:\n\n```bash\npython main.py fit\n```\nView all available options with the --help argument given after the subcommand:\n\n```bash\n$ python main.py fit --help\n\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n                            ...\n                            [--ckpt_path CKPT_PATH]\n    --trainer.logger LOGGER\n\noptional arguments:\n<class '__main__.DemoModel'>:\n    --model.out_dim OUT_DIM\n                            (type: int, default: 10)\n    --model.learning_rate LEARNING_RATE\n                            (type: float, default: 0.02)\n<class 'lightning.pytorch.demos.boring_classes.BoringDataModule'>:\n--data CONFIG         Path to a configuration file.\n--data.data_dir DATA_DIR\n                        (type: str, default: ./)\n```\nWith the Lightning CLI enabled, you can now change the parameters without touching your code:\n\n```bash\n# change the learning_rate\npython main.py fit --model.learning_rate 0.1\n\n# change the output dimensions also\npython main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n# change trainer and data arguments too\npython main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n```\nThe options that become available in the CLI are the __init__ parameters of the LightningModule and\nLightningDataModule classes. Thus, to make hyperparameters configurable, just add them to your class's\n__init__. It is highly recommended that these parameters are described in the docstring so that the CLI shows\nthem in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give\nunderstandable error messages when incorrect values are given.",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 7,
        "text": "Configure hyperparameters from the CLI (Intermediate)\n## Configure hyperparameters from the CLI (Intermediate)\n**Audience:** Users who have multiple models and datasets per project.\n\n**Pre-reqs:** You must have read (Control it all from the CLI) <lightning_cli_intermediate>.\n\n----\n\n### Why mix models and datasets\nLightning projects usually begin with one model and one dataset. As the project grows in complexity and you introduce\nmore models and more datasets, it becomes desirable to mix any model with any dataset directly from the command line\nwithout changing your code.\n\n```bash\n# Mix and match anything\n$ python main.py fit --model=GAN --data=MNIST\n$ python main.py fit --model=Transformer --data=MNIST\n```\nLightningCLI makes this very simple. Otherwise, this kind of configuration requires a significant amount of\nboilerplate that often looks like this:\n\n```python\n# choose model\nif args.model == \"gan\":\n    model = GAN(args.feat_dim)\nelif args.model == \"transformer\":\n    model = Transformer(args.feat_dim)\n...\n\n# choose datamodule\nif args.data == \"MNIST\":\n    datamodule = MNIST()\nelif args.data == \"imagenet\":\n    datamodule = Imagenet()\n...\n\n# mix them!\ntrainer.fit(model, datamodule)\n```\nIt is highly recommended that you avoid writing this kind of boilerplate and use LightningCLI instead.\n\n----\n\n### Multiple LightningModules\nTo support multiple models, when instantiating LightningCLI omit the model_class parameter:\n\n```python\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass Model1(DemoModel):\n    def configure_optimizers(self):\n        print(\"\u26a1\", \"using Model1\", \"\u26a1\")\n        return super().configure_optimizers()\n\nclass Model2(DemoModel):\n    def configure_optimizers(self):\n        print(\"\u26a1\", \"using Model2\", \"\u26a1\")\n        return super().configure_optimizers()\n\ncli = LightningCLI(datamodule_class=BoringDataModule)\n```\nNow you can choose between any model from the CLI:\n\n```bash\n# use Model1\npython main.py fit --model Model1\n\n# use Model2\npython main.py fit --model Model2\n```\nInstead of omitting the model_class parameter, you can give a base class and subclass_mode_model=True. This\nwill make the CLI only accept models which are a subclass of the given base class.\n----\n\n### Multiple LightningDataModules\nTo support multiple data modules, when instantiating LightningCLI omit the datamodule_class parameter:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass FakeDataset1(BoringDataModule):\n    def train_dataloader(self):\n        print(\"\u26a1\", \"using FakeDataset1\", \"\u26a1\")\n        return torch.utils.data.DataLoader(self.random_train)\n\nclass FakeDataset2(BoringDataModule):\n    def train_dataloader(self):\n        print(\"\u26a1\", \"using FakeDataset2\", \"\u26a1\")\n        return torch.utils.data.DataLoader(self.random_train)\n\ncli = LightningCLI(DemoModel)\n```\nNow you can choose between any dataset at runtime:\n\n```bash\n# use Model1\npython main.py fit --data FakeDataset1\n\n# use Model2\npython main.py fit --data FakeDataset2\n```\nInstead of omitting the datamodule_class parameter, you can give a base class and subclass_mode_data=True.\nThis will make the CLI only accept data modules that are a subclass of the given base class.\n----\n\n### Multiple optimizers\nStandard optimizers from torch.optim work out of the box:\n\n```bash\npython main.py fit --optimizer AdamW\n```\nIf the optimizer you want needs other arguments, add them via the CLI (no need to change your code)!\n\n```bash\npython main.py fit --optimizer SGD --optimizer.lr=0.01\n```\nFurthermore, any custom subclass of torch.optim.Optimizer can be used as an optimizer:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass LitAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"\u26a1\", \"using LitAdam\", \"\u26a1\")\n        super().step(closure)\n\nclass FancyAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"\u26a1\", \"using FancyAdam\", \"\u26a1\")\n        super().step(closure)\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\nNow you can choose between any optimizer at runtime:\n\n```bash\n# use LitAdam\npython main.py fit --optimizer LitAdam\n\n# use FancyAdam\npython main.py fit --optimizer FancyAdam\n```\n----\n\n### Multiple schedulers\nStandard learning rate schedulers from torch.optim.lr_scheduler work out of the box:\n\n```bash\npython main.py fit --optimizer=Adam --lr_scheduler CosineAnnealingLR\n```\nPlease note that --optimizer must be added for --lr_scheduler to have an effect.\n\nIf the scheduler you want needs other arguments, add them via the CLI (no need to change your code)!\n\n```bash\npython main.py fit --optimizer=Adam --lr_scheduler=ReduceLROnPlateau --lr_scheduler.monitor=epoch\n```\nFurthermore, any custom subclass of torch.optim.lr_scheduler.LRScheduler can be used as learning rate scheduler:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass LitLRScheduler(torch.optim.lr_scheduler.CosineAnnealingLR):\n    def step(self):\n        print(\"\u26a1\", \"using LitLRScheduler\", \"\u26a1\")\n        super().step()\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\nNow you can choose between any learning rate scheduler at runtime:\n\n```bash\n# LitLRScheduler\npython main.py fit --optimizer=Adam --lr_scheduler LitLRScheduler\n```\n----\n\n### Classes from any package\nIn the previous sections, custom classes to select were defined in the same python file where the LightningCLI class\nis run. To select classes from any package by using only the class name, import the respective package:\n\n```python\nfrom lightning.pytorch.cli import LightningCLI\nimport my_code.models  # noqa: F401\nimport my_code.data_modules  # noqa: F401\nimport my_code.optimizers  # noqa: F401\n\ncli = LightningCLI()\n```\nNow use any of the classes:\n\n```bash\npython main.py fit --model Model1 --data FakeDataset1 --optimizer LitAdam --lr_scheduler LitLRScheduler\n```\nThe # noqa: F401 comment avoids a linter warning that the import is unused.\n\nIt is also possible to select subclasses that have not been imported by giving the full import path:\n\n```bash\npython main.py fit --model my_code.models.Model1\n```\n----\n\n### Help for specific classes\nWhen multiple models or datasets are accepted, the main help of the CLI does not include their specific parameters. To\nshow this specific help, additional help arguments expect the class name or its import path. For example:\n\n```bash\npython main.py fit --model.help Model1\npython main.py fit --data.help FakeDataset2\npython main.py fit --optimizer.help Adagrad\npython main.py fit --lr_scheduler.help StepLR\n```",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 4,
        "text": "Configure hyperparameters from the CLI (Expert)\n## Configure hyperparameters from the CLI (Expert)\n**Audience:** Users who already understand the LightningCLI and want to customize it.\n\n----\n\n### Customize the LightningCLI\n\nThe init parameters of the ~lightning.pytorch.cli.LightningCLI class can be used to customize some things,\ne.g., the description of the tool, enabling parsing of environment variables, and additional arguments to instantiate\nthe trainer and configuration parser.\n\nNevertheless, the init arguments are not enough for many use cases. For this reason, the class is designed so that it\ncan be extended to customize different parts of the command line tool. The argument parser class used by\n~lightning.pytorch.cli.LightningCLI is ~lightning.pytorch.cli.LightningArgumentParser, which is an\nextension of python's argparse, thus adding arguments can be done using the add_argument method. In contrast to\nargparse, it has additional methods to add arguments. For example add_class_arguments add all arguments from the\ninit of a class. For more details, see the `respective documentation\n<https://jsonargparse.readthedocs.io/en/stable/#classes-methods-and-functions>`_.\n\nThe ~lightning.pytorch.cli.LightningCLI class has the\n~lightning.pytorch.cli.LightningCLI.add_arguments_to_parser method can be implemented to include more arguments.\nAfter parsing, the configuration is stored in the config attribute of the class instance. The\n~lightning.pytorch.cli.LightningCLI class also has two methods that can be used to run code before and after\nthe trainer runs: before_<subcommand> and after_<subcommand>. A realistic example of this would be to send an\nemail before and after the execution. The code for the fit subcommand would be something like this:\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_argument(\"--notification_email\", default=\"will@email.com\")\n\ndef before_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit starting\")\n\ndef after_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit finished\")\n\ncli = MyLightningCLI(MyModel)\nNote that the config object self.config is a namespace whose keys are global options or groups of options. It has\nthe same structure as the YAML format described previously. This means that the parameters used for instantiating the\ntrainer class can be found in self.config['fit']['trainer'].\n\nHave a look at the ~lightning.pytorch.cli.LightningCLI class API reference to learn about other methods\nthat can be extended to customize a CLI.\n----\n\n### Configure forced callbacks\nAs explained previously, any Lightning callback can be added by passing it through the command line or including it in\nthe config via class_path and init_args entries.\n\nHowever, certain callbacks **must** be coupled with a model so they are always present and configurable. This can be\nimplemented as follows:\n\nfrom lightning.pytorch.callbacks import EarlyStopping\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_lightning_class_args(EarlyStopping, \"my_early_stopping\")\nparser.set_defaults({\"my_early_stopping.monitor\": \"val_loss\", \"my_early_stopping.patience\": 5})\n\ncli = MyLightningCLI(MyModel)\nTo change the parameters for EarlyStopping in the config it would be:\n\n```yaml\nmodel:\n  ...\ntrainer:\n  ...\nmy_early_stopping:\n  patience: 5\n```\nThe example above overrides a default in add_arguments_to_parser. This is included to show that defaults can be\nchanged if needed. However, note that overriding defaults in the source code is not intended to be used to store the\nbest hyperparameters for a task after experimentation. To guarantee reproducibility, the source code should be\nstable. It is better to practice storing the best hyperparameters for a task in a configuration file independent\nfrom the source code.\n----\n\n### Class type defaults\n\nThe support for classes as type hints allows to try many possibilities with the same CLI. This is a useful feature, but\nit is tempting to use an instance of a class as a default. For example:\n\nclass MyMainModel(LightningModule):\ndef __init__(\nself,\nbackbone: torch.nn.Module = MyModel(encoder_layers=24), # BAD PRACTICE!\n):\nsuper().__init__()\nself.backbone = backbone\nNormally classes are mutable, as in this case. The instance of MyModel would be created the moment that the module\nthat defines MyMainModel is first imported. This means that the default of backbone will be initialized before\nthe CLI class runs seed_everything, making it non-reproducible. Furthermore, if MyMainModel is used more than\nonce in the same Python process and the backbone parameter is not overridden, the same instance would be used in\nmultiple places. Most likely, this is not what the developer intended. Having an instance as default also makes it\nimpossible to generate the complete config file since it is not known which arguments were used to instantiate it for\narbitrary classes.\n\nAn excellent solution to these problems is not to have a default or set the default to a unique value (e.g., a string).\nThen check this value and instantiate it in the __init__ body. If a class parameter has no default and the CLI is\nsubclassed, then a default can be set as follows:\n\ndefault_backbone = {\n\"class_path\": \"import.path.of.MyModel\",\n\"init_args\": {\n\"encoder_layers\": 24,\n},\n}\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": default_backbone})\nA more compact version that avoids writing a dictionary would be:\n\nfrom jsonargparse import lazy_instance\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": lazy_instance(MyModel, encoder_layers=24)})\n----\n\n### Argument linking\nAnother case in which it might be desired to extend ~lightning.pytorch.cli.LightningCLI is that the model and\ndata module depends on a common parameter. For example, in some cases, both classes require to know the batch_size.\nIt is a burden and error-prone to give the same value twice in a config file. To avoid this, the parser can be\nconfigured so that a value is only given once and then propagated accordingly. With a tool implemented like the one\nshown below, the batch_size only has to be provided in the data section of the config.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.batch_size\", \"model.batch_size\")\n\ncli = MyLightningCLI(MyModel, MyDataModule)\nThe linking of arguments is observed in the help of the tool, which for this example would look like:\n\n```bash\n$ python trainer.py fit --help\n  ...\n    --data.batch_size BATCH_SIZE\n                          Number of samples in a batch (type: int, default: 8)\n\n  Linked arguments:\n    data.batch_size --> model.batch_size\n                          Number of samples in a batch (type: int)\n```\nSometimes a parameter value is only available after class instantiation. An example could be that your model requires\nthe number of classes to instantiate its fully connected layer (for a classification task). But the value is not\navailable until the data module has been instantiated. The code below illustrates how to address this.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.num_classes\", \"model.num_classes\", apply_on=\"instantiate\")\n\ncli = MyLightningCLI(MyClassModel, MyDataModule)\nInstantiation links are used to automatically determine the order of instantiation, in this case data first.\n\nThe linking of arguments is intended for things that are meant to be non-configurable. This improves the CLI user\nexperience since it avoids the need to provide more parameters. A related concept is a variable interpolation that\nkeeps things configurable.\nThe linking of arguments can be used for more complex cases. For example to derive a value via a function that takes\nmultiple settings as input. For more details have a look at the API of `link_arguments\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentLinking.link_arguments>`_.",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 1,
        "text": "Configure hyperparameters from the CLI (Advanced)\n## Configure hyperparameters from the CLI (Advanced)\n**Audience:** Users looking to modularize their code for a professional project.\n\n**Pre-reqs:** You must have read (Mix models and datasets) <lightning_cli_intermediate_2>.\n\nAs a project becomes more complex, the number of configurable options becomes very large, making it inconvenient to\ncontrol through individual command line arguments. To address this, CLIs implemented using\n~lightning.pytorch.cli.LightningCLI always support receiving input from configuration files. The default format\nused for config files is YAML.\n\nIf you are unfamiliar with YAML, it is recommended that you first read what-is-a-yaml-config-file.\n----\n\n### Run using a config file\nTo run the CLI using a yaml config, do:\n\n```bash\npython main.py fit --config config.yaml\n```\nIndividual arguments can be given to override options in the config file:\n\n```bash\npython main.py fit --config config.yaml --trainer.max_epochs 100\n```\n----\n\n### Automatic save of config\n\nTo ease experiment reporting and reproducibility, by default LightningCLI automatically saves the full YAML\nconfiguration in the log directory. After multiple fit runs with different hyperparameters, each one will have in its\nrespective log directory a config.yaml file. These files can be used to trivially reproduce an experiment, e.g.:\n\n```bash\npython main.py fit --config lightning_logs/version_7/config.yaml\n```\nThe automatic saving of the config is done by the special callback ~lightning.pytorch.cli.SaveConfigCallback.\nThis callback is automatically added to the Trainer. To disable the save of the config, instantiate LightningCLI\nwith save_config_callback=None.\n\nTo change the file name of the saved configs to e.g. name.yaml, do:\n\n.. code:: python\n\ncli = LightningCLI(..., save_config_kwargs={\"config_filename\": \"name.yaml\"})\nIt is also possible to extend the ~lightning.pytorch.cli.SaveConfigCallback class, for instance to additionally\nsave the config in a logger. An example of this is:\n\n```python\n    class LoggerSaveConfigCallback(SaveConfigCallback):\n        def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\n            if isinstance(trainer.logger, Logger):\n                config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\n                trainer.logger.log_hyperparams({\"config\": config})\n\n    cli = LightningCLI(..., save_config_callback=LoggerSaveConfigCallback)\n```\nIf you want to disable the standard behavior of saving the config to the log_dir, then you can either implement\n__init__ and call super().__init__(*args, save_to_log_dir=False, **kwargs) or instantiate the\nLightningCLI as:\n\n.. code:: python\n\ncli = LightningCLI(..., save_config_kwargs={\"save_to_log_dir\": False})\nThe save_config method is only called on rank zero. This allows to implement a custom save config without having\nto worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the process\nhang waiting for a broadcast. If you need to make collective calls, implement the setup method instead.\n----\n\n### Prepare a config file for the CLI\nThe --help option of the CLIs can be used to learn which configuration options are available and how to use them.\nHowever, writing a config from scratch can be time-consuming and error-prone. To alleviate this, the CLIs have the\n--print_config argument, which prints to stdout the configuration without running the command.\n\nFor a CLI implemented as LightningCLI(DemoModel, BoringDataModule), executing:\n\n```bash\npython main.py fit --print_config\n```\ngenerates a config with all default values like the following:\n\n```bash\nseed_everything: null\ntrainer:\n  logger: true\n  ...\nmodel:\n  out_dim: 10\n  learning_rate: 0.02\ndata:\n  data_dir: ./\nckpt_path: null\n```\nOther command line arguments can be given and considered in the printed configuration. A use case for this is CLIs that\naccept multiple models. By default, no model is selected, meaning the printed config will not include model settings. To\nget a config with the default values of a particular model would be:\n\n```bash\npython main.py fit --model DemoModel --print_config\n```\nwhich generates a config like:\n\n```bash\nseed_everything: null\ntrainer:\n  ...\nmodel:\n  class_path: lightning.pytorch.demos.boring_classes.DemoModel\n  init_args:\n    out_dim: 10\n    learning_rate: 0.02\nckpt_path: null\n```\nA standard procedure to run experiments can be:\n\n.. code:: bash\n\n# Print a configuration to have as reference\npython main.py fit --print_config > config.yaml\n# Modify the config to your liking - you can remove all default arguments\nnano config.yaml\n# Fit your model using the edited configuration\npython main.py fit --config config.yaml\nConfiguration items can be either simple Python objects such as int and str,\nor complex objects comprised of a class_path and init_args arguments. The class_path refers\nto the complete import path of the item class, while init_args are the arguments to be passed\nto the class constructor. For example, your model is defined as:\n\n```python\n# model.py\nclass MyModel(L.LightningModule):\n    def __init__(self, criterion: torch.nn.Module):\n        self.criterion = criterion\n```\nThen the config would be:\n\n```yaml\nmodel:\n  class_path: model.MyModel\n  init_args:\n    criterion:\n      class_path: torch.nn.CrossEntropyLoss\n      init_args:\n        reduction: mean\n    ...\n```\nLightningCLI uses jsonargparse under the hood for parsing\nconfiguration files and automatic creation of objects, so you don't need to do it yourself.\n\nLightning automatically registers all subclasses of ~lightning.pytorch.core.LightningModule,\nso the complete import path is not required for them and can be replaced by the class name.\nParsers make a best effort to determine the correct names and types that the parser should accept.\nHowever, there can be cases not yet supported or cases for which it would be impossible to support.\nTo somewhat overcome these limitations, there is a special key dict_kwargs that can be used\nto provide arguments that will not be validated during parsing, but will be used for class instantiation.\n\nFor example, then using the lightning.pytorch.profilers.PyTorchProfiler profiler,\nthe profile_memory argument has a type that is determined dynamically. As a result, it's not possible\nto know the expected type during parsing. To account for this, your config file should be set up like this:\n\n.. code:: yaml\n\ntrainer:\nprofiler:\nclass_path: lightning.pytorch.profilers.PyTorchProfiler\ndict_kwargs:\nprofile_memory: true\n----\n\n### Compose config files\nMultiple config files can be provided, and they will be parsed sequentially. Let's say we have two configs with common\nsettings:\n\n```yaml\n# config_1.yaml\ntrainer:\n  num_epochs: 10\n  ...\n\n# config_2.yaml\ntrainer:\n  num_epochs: 20\n  ...\n```\nThe value from the last config will be used, num_epochs = 20 in this case:\n\n```bash\n$ python main.py fit --config config_1.yaml --config config_2.yaml\n```\n----\n\n### Use groups of options\nGroups of options can also be given as independent config files. For configs like:\n\n```yaml\n# trainer.yaml\nnum_epochs: 10\n\n# model.yaml\nout_dim: 7\n\n# data.yaml\ndata_dir: ./data\n```\na fit command can be run as:\n\n```bash\n$ python main.py fit --trainer trainer.yaml --model model.yaml --data data.yaml [...]\n```",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 2,
        "text": "Configure hyperparameters from the CLI (Advanced)\n## Configure hyperparameters from the CLI (Advanced)\n\n### Customize arguments by subcommand\nTo customize arguments by subcommand, pass the config *before* the subcommand:\n\n```bash\n$ python main.py [before] [subcommand] [after]\n$ python main.py  ...         fit       ...\n```\nFor example, here we set the Trainer argument [max_steps = 100] for the full training routine and [max_steps = 10] for\ntesting:\n\n```bash\n# config.yaml\nfit:\n    trainer:\n        max_steps: 100\ntest:\n    trainer:\n        max_epochs: 10\n```\nnow you can toggle this behavior by subcommand:\n\n```bash\n# full routine with max_steps = 100\n$ python main.py --config config.yaml fit\n\n# test only with max_epochs = 10\n$ python main.py --config config.yaml test\n```\n----\n\n### Run from cloud yaml configs\nFor certain enterprise workloads, Lightning CLI supports running from hosted configs:\n\n```bash\n$ python main.py [subcommand] --config s3://bucket/config.yaml\n```\nFor more options, refer to Remote filesystems <../common/remote_fs>.\n\n----\n\n### Use a config via environment variables\nFor certain CI/CD systems, it's useful to pass in raw yaml config as environment variables:\n\n```bash\n$ python main.py fit --trainer \"$TRAINER_CONFIG\" --model \"$MODEL_CONFIG\" [...]\n```\n----\n\n### Run from environment variables directly\nThe Lightning CLI can convert every possible CLI flag into an environment variable. To enable this, add to\nparser_kwargs the default_env argument:\n\n```python\ncli = LightningCLI(..., parser_kwargs={\"default_env\": True})\n```\nnow use the --help CLI flag with any subcommand:\n\n```bash\n$ python main.py fit --help\n```\nwhich will show you ALL possible environment variables that can be set:\n\n```bash\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            ...\n\noptional arguments:\n...\nARG:   --model.out_dim OUT_DIM\nENV:   PL_FIT__MODEL__OUT_DIM\n                        (type: int, default: 10)\nARG:   --model.learning_rate LEARNING_RATE\nENV:   PL_FIT__MODEL__LEARNING_RATE\n                        (type: float, default: 0.02)\n```\nnow you can customize the behavior via environment variables:\n\n```bash\n# set the options via env vars\n$ export PL_FIT__MODEL__LEARNING_RATE=0.01\n$ export PL_FIT__MODEL__OUT_DIM=5\n\n$ python main.py fit\n```\n----\n\n### Set default config files\nTo set a path to a config file of defaults, use the default_config_files argument:\n\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"default_config_files\": [\"my_cli_defaults.yaml\"]})\nor if you want defaults per subcommand:\n\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"fit\": {\"default_config_files\": [\"my_fit_defaults.yaml\"]}})\n----\n\n### Enable variable interpolation\nIn certain cases where multiple settings need to share a value, consider using variable interpolation. For instance:\n\n```yaml\nmodel:\n  encoder_layers: 12\n  decoder_layers:\n  - ${model.encoder_layers}\n  - 4\n```\nTo enable variable interpolation, first install omegaconf:\n\n```bash\npip install omegaconf\n```\nThen set omegaconf when instantiating the LightningCLI class:\n\n```python\ncli = LightningCLI(MyModel, parser_kwargs={\"parser_mode\": \"omegaconf\"})\n```\nAfter this, the CLI will automatically perform interpolation in yaml files:\n\n```bash\npython main.py --model.encoder_layers=12\n```\nFor more details about the interpolation support and its limitations, have a look at the `jsonargparse\n<https://jsonargparse.readthedocs.io/en/stable/#variable-interpolation>`__ and the `omegaconf\n<https://omegaconf.readthedocs.io/en/2.1_branch/usage.html#variable-interpolation>`__ documentations.\n\nThere are many use cases in which variable interpolation is not the correct approach. When a parameter **must\nalways** be derived from other settings, it shouldn't be up to the CLI user to do this in a config file. For\nexample, if the data and model both require batch_size and must be the same value, then\ncli_link_arguments should be used instead of interpolation.",
        "score": 9
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 3,
        "text": "Configure hyperparameters from the CLI (Advanced)\n## Configure hyperparameters from the CLI (Advanced)\n\n### Instantiation only mode\n\nThe CLI is designed to start fitting with minimal code changes. On class instantiation, the CLI will automatically call\nthe trainer function associated with the subcommand provided, so you don't have to do it. To avoid this, you can set the\nfollowing argument:\n\ncli = LightningCLI(MyModel, run=False) # True by default\n# you'll have to call fit yourself:\ncli.trainer.fit(cli.model)\nIn this mode, subcommands are **not** added to the parser. This can be useful to implement custom logic without having\nto subclass the CLI, but still, use the CLI's instantiation and argument parsing capabilities.\n\n### Trainer Callbacks and arguments with class type\n\nA very important argument of the ~lightning.pytorch.trainer.trainer.Trainer class is the callbacks. In\ncontrast to simpler arguments that take numbers or strings, callbacks expects a list of instances of subclasses of\n~lightning.pytorch.callbacks.Callback. To specify this kind of argument in a config file, each callback must be\ngiven as a dictionary, including a class_path entry with an import path of the class and optionally an init_args\nentry with arguments to use to instantiate. Therefore, a simple configuration file that defines two callbacks is the\nfollowing:\n\n```yaml\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n      init_args:\n        save_weights_only: true\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: 'epoch'\n```\nSimilar to the callbacks, any parameter in ~lightning.pytorch.trainer.trainer.Trainer and user extended\n~lightning.pytorch.core.LightningModule and\n~lightning.pytorch.core.datamodule.LightningDataModule classes that have as type hint a class, can be\nconfigured the same way using class_path and init_args. If the package that defines a subclass is imported\nbefore the ~lightning.pytorch.cli.LightningCLI class is run, the name can be used instead of the full import\npath.\n\nFrom command line the syntax is the following:\n\n```bash\n$ python ... \\\n    --trainer.callbacks+={CALLBACK_1_NAME} \\\n    --trainer.callbacks.{CALLBACK_1_ARGS_1}=... \\\n    --trainer.callbacks.{CALLBACK_1_ARGS_2}=... \\\n    ...\n    --trainer.callbacks+={CALLBACK_N_NAME} \\\n    --trainer.callbacks.{CALLBACK_N_ARGS_1}=... \\\n    ...\n```\nNote the use of + to append a new callback to the list and that the init_args are applied to the previous\ncallback appended. Here is an example:\n\n```bash\n$ python ... \\\n    --trainer.callbacks+=EarlyStopping \\\n    --trainer.callbacks.patience=5 \\\n    --trainer.callbacks+=LearningRateMonitor \\\n    --trainer.callbacks.logging_interval=epoch\n```\nSerialized config files (e.g. --print_config or ~lightning.pytorch.cli.SaveConfigCallback) always have\nthe full class_path, even when class name shorthand notation is used in the command line or in input config\nfiles.\n### Multiple models and/or datasets\n\nA CLI can be written such that a model and/or a datamodule is specified by an import path and init arguments. For\nexample, with a tool implemented as:\n\n```python\ncli = LightningCLI(MyModelBaseClass, MyDataModuleBaseClass, subclass_mode_model=True, subclass_mode_data=True)\n```\nA possible config file could be as follows:\n\n```yaml\nmodel:\n  class_path: mycode.mymodels.MyModel\n  init_args:\n    decoder_layers:\n    - 2\n    - 4\n    encoder_layers: 12\ndata:\n  class_path: mycode.mydatamodules.MyDataModule\n  init_args:\n    ...\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        patience: 5\n    ...\n```\nOnly model classes that are a subclass of MyModelBaseClass would be allowed, and similarly, only subclasses of\nMyDataModuleBaseClass. If as base classes ~lightning.pytorch.core.LightningModule and\n~lightning.pytorch.core.datamodule.LightningDataModule is given, then the CLI would allow any lightning module\nand data module.\n\nNote that with the subclass modes, the --help option does not show information for a specific subclass. To get\nhelp for a subclass, the options --model.help and --data.help can be used, followed by the desired class\npath. Similarly, --print_config does not include the settings for a particular subclass. To include them, the\nclass path should be given before the --print_config option. Examples for both help and print config are:\n\n.. code-block:: bash\n\n$ python trainer.py fit --model.help mycode.mymodels.MyModel\n$ python trainer.py fit --model mycode.mymodels.MyModel --print_config\n### Models with multiple submodules\n\nMany use cases require to have several modules, each with its own configurable options. One possible way to handle this\nwith LightningCLI is to implement a single module having as init parameters each of the submodules. This is known as\ndependency injection_ which is a good approach to improve\ndecoupling in your code base.\n\nSince the init parameters of the model have as a type hint a class, in the configuration, these would be specified with\nclass_path and init_args entries. For instance, a model could be implemented as:\n\nclass MyMainModel(LightningModule):\ndef __init__(self, encoder: nn.Module, decoder: nn.Module):\n\"\"\"Example encoder-decoder submodules model\n\nArgs:\nencoder: Instance of a module for encoding\ndecoder: Instance of a module for decoding\n\"\"\"\nsuper().__init__()\nself.save_hyperparameters()\nself.encoder = encoder\nself.decoder = decoder\nIf the CLI is implemented as LightningCLI(MyMainModel) the configuration would be as follows:\n\n```yaml\nmodel:\n  encoder:\n    class_path: mycode.myencoders.MyEncoder\n    init_args:\n      ...\n  decoder:\n    class_path: mycode.mydecoders.MyDecoder\n    init_args:\n      ...\n```\nIt is also possible to combine subclass_mode_model=True and submodules, thereby having two levels of class_path.\n\nBy having self.save_hyperparameters() it becomes possible to load the model from a checkpoint. Simply do\nModelClass.load_from_checkpoint(\"path/to/checkpoint.ckpt\"). In the case of using subclass_mode_model=True,\nthen load it like LightningModule.load_from_checkpoint(\"path/to/checkpoint.ckpt\"). save_hyperparameters is\noptional and can be safely removed if there is no need to load from a checkpoint.\n### Fixed optimizer and scheduler\n\nIn some cases, fixing the optimizer and/or learning scheduler might be desired instead of allowing multiple. For this,\nyou can manually add the arguments for specific classes by subclassing the CLI. The following code snippet shows how to\nimplement it:\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_optimizer_args(torch.optim.Adam)\nparser.add_lr_scheduler_args(torch.optim.lr_scheduler.ExponentialLR)\nWith this, in the config, the optimizer and lr_scheduler groups would accept all of the options for the given\nclasses, in this example, Adam and ExponentialLR. Therefore, the config file would be structured like:\n\n```yaml\noptimizer:\n  lr: 0.01\nlr_scheduler:\n  gamma: 0.2\nmodel:\n  ...\ntrainer:\n  ...\n```\nwhere the arguments can be passed directly through the command line without specifying the class. For example:\n\n```bash\n$ python trainer.py fit --optimizer.lr=0.01 --lr_scheduler.gamma=0.2\n```\n### Multiple optimizers and schedulers\n\nBy default, the CLIs support multiple optimizers and/or learning schedulers, automatically implementing\nconfigure_optimizers. This behavior can be disabled by providing auto_configure_optimizers=False on\ninstantiation of ~lightning.pytorch.cli.LightningCLI. This would be required for example to support multiple\noptimizers, for each selecting a particular optimizer class. Similar to multiple submodules, this can be done via\ndependency injection_. Unlike the submodules, it is not possible\nto expect an instance of a class, because optimizers require the module's parameters to optimize, which are only\navailable after instantiation of the module. Learning schedulers are a similar situation, requiring an optimizer\ninstance. For these cases, dependency injection involves providing a function that instantiates the respective class\nwhen called.\n\nAn example of a model that uses two optimizers is the following:\n\n```python\nfrom typing import Iterable\nfrom torch.optim import Optimizer\n\nOptimizerCallable = Callable[[Iterable], Optimizer]\n\nclass MyModel(LightningModule):\n    def __init__(self, optimizer1: OptimizerCallable, optimizer2: OptimizerCallable):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer1 = optimizer1\n        self.optimizer2 = optimizer2\n\n    def configure_optimizers(self):\n        optimizer1 = self.optimizer1(self.parameters())\n        optimizer2 = self.optimizer2(self.parameters())\n        return [optimizer1, optimizer2]\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\nNote the type Callable[[Iterable], Optimizer], which denotes a function that receives a single argument, some\nlearnable parameters, and returns an optimizer instance. With this, from the command line it is possible to select the\nclass and init arguments for each of the optimizers, as follows:\n\n```bash\n$ python trainer.py fit \\\n    --model.optimizer1=Adam \\\n    --model.optimizer1.lr=0.01 \\\n    --model.optimizer2=AdamW \\\n    --model.optimizer2.lr=0.0001\n```\nIn the example above, the OptimizerCallable type alias was created to illustrate what the type hint means. For\nconvenience, this type alias and one for learning schedulers is available in the cli module. An example of a model\nthat uses dependency injection for an optimizer and a learning scheduler is:\n\n```python\nfrom lightning.pytorch.cli import OptimizerCallable, LRSchedulerCallable, LightningCLI\n\nclass MyModel(LightningModule):\n    def __init__(\n        self,\n        optimizer: OptimizerCallable = torch.optim.Adam,\n        scheduler: LRSchedulerCallable = torch.optim.lr_scheduler.ConstantLR,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer(self.parameters())\n        scheduler = self.scheduler(optimizer)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\nNote that for this example, classes are used as defaults. This is compatible with the type hints, since they are also\ncallables that receive the same first argument and return an instance of the class. Classes that have more than one\nrequired argument will not work as default. For these cases a lambda function can be used, e.g. ``optimizer:\nOptimizerCallable = lambda p: torch.optim.SGD(p, lr=0.01)``.\n\n### Run from Python\n\nEven though the ~lightning.pytorch.cli.LightningCLI class is designed to help in the implementation of command\nline tools, for some use cases it is desired to run directly from Python. To allow this there is the args parameter.\nAn example could be to first implement a normal CLI script, but adding an args parameter with default None to\nthe main function as follows:\n\n```python\nfrom lightning.pytorch.cli import ArgsType, LightningCLI\n\ndef cli_main(args: ArgsType = None):\n    cli = LightningCLI(MyModel, ..., args=args)\n    ...\n\nif __name__ == \"__main__\":\n    cli_main()\n```\nThen it is possible to import the cli_main function to run it. Executing in a shell ``my_cli.py\n--trainer.max_epochs=100 --model.encoder_layers=24`` would be equivalent to:\n\n```python\nfrom my_module.my_cli import cli_main\n\ncli_main([\"--trainer.max_epochs=100\", \"--model.encoder_layers=24\"])\n```\nAll the features that are supported from the command line can be used when giving args as a list of strings. It is\nalso possible to provide a dict or `jsonargparse.Namespace\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.Namespace>`__. For example in a jupyter notebook someone\nmight do:\n\n```python\nargs = {\n    \"trainer\": {\n        \"max_epochs\": 100,\n    },\n    \"model\": {},\n}\n\nargs[\"model\"][\"encoder_layers\"] = 8\ncli_main(args)\nargs[\"model\"][\"encoder_layers\"] = 12\ncli_main(args)\nargs[\"trainer\"][\"max_epochs\"] = 200\ncli_main(args)\n```\nThe args parameter must be None when running from command line so that sys.argv is used as arguments.\nAlso, note that the purpose of trainer_defaults is different to args. It is okay to use trainer_defaults\nin the cli_main function to modify the defaults of some trainer parameters.",
        "score": 9
      },
      {
        "file": "../data/final_data/discussions.json",
        "index": 0,
        "text": "ValueError(f\"{func} does not have a return type annotation\") when implementing LightningCLI\nDear All,\nI am trying to implement the LightningCLI class, but it tells me my model does not have any Type annotation, although I added class function type return type annotation.\nWhat am I doing wrong?\nThank you for your input!\nLG\nMax\nAnswer: Hi Mauvilsa,\nthank you for your reply. While trying to come up with a minimal reproduction example I actually found the solution to my problem:\nLightningCLI() does not take the instance of the model as a parameter but the class itself (as I guess instantiation is then done by the pipeline itself). To be more clear:\nWrong / Not Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nmodel = MyFancyModelClass()\ndatamodule = MyFancyDataModuleClass()\n\nLightningCLI = LightningCLI(model, datamodule)\n\nCorrect / Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nLightningCLI = LightningCLI(MyFancyModelClass, MyFancyDataModuleClass)\n\nHope that also helps others!\nCheers,\nMax",
        "score": 6
      },
      {
        "file": "../data/final_data/discussions.json",
        "index": 18,
        "text": "Specify Trainer strategy in CLI config?\nHi folks,\nI can't figure out how to give a Strategy class (rather than a string) as an argument to the trainer in a CLI config file. Is doing so supported?\nDoing this works fine:\ntrainer:\n    ...\n    strategy: 'deepspeed_stage_2_offload'\n    ...\n\nBut this gives a jsonargparse error:\ntrainer:\n    ...\n    strategy:\n        class_path: pytorch_lightning.strategies.DeepSpeedStrategy\n        init_args:\n            stage: 2\n            offload_optimizer: True\n            logging_batch_size_per_gpu: 16\n    ...\n\nThe error:\nTraceback (most recent call last):\n  File \"/media/sharon/wbrannon/github/clip-graph/bin/trainer.py\", line 7, in <module>\n    cli = cl.LightningCLI(\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 552, in __init__\n    self.parse_arguments(self.parser)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 692, in parse_arguments\n    self.config = parser.parse_args()\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 268, in parse_args\n    return super().parse_args(*args, **kwargs)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/deprecated.py\", line 112, in patched_parse\n    cfg = parse_method(*args, _skip_check=_skip_check, **kwargs)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 366, in parse_args\n    self.error(str(ex), ex)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 983, in error\n    raise ParserError(message) from ex\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, s\ntage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000\n.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\" does not validate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\n  - Problem with given class_path \"pytorch_lightning.strategies.DeepSpeedStrategy\":\n    - Configuration check failed :: Parser key \"params_buffer_size\": Expected a <class 'int'> but got \"100000000.0\"\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\nAnswer: It's a bug! Will be fixed with #12989 which should be included in next week's bugfix release\nThanks for the report!",
        "score": 6
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 5,
        "text": "Frequently asked questions for LightningCLI\n## Frequently asked questions for LightningCLI\n\n### What does CLI stand for?\nCLI is short for command line interface. This means it is a tool intended to be run from a terminal, similar to commands\nlike git.\n\n----\n\n### What is a yaml config file?\nA YAML is a standard for configuration files used to describe parameters for sections of a program. It is a common tool\nin engineering and has recently started to gain popularity in machine learning. An example of a YAML file is the\nfollowing:\n\n```yaml\n# file.yaml\ncar:\n    max_speed:100\n    max_passengers:2\nplane:\n    fuel_capacity: 50\nclass_3:\n    option_1: 'x'\n    option_2: 'y'\n```\nIf you are unfamiliar with YAML, the short introduction at `realpython.com#yaml-syntax\n<https://realpython.com/python-yaml/#yaml-syntax>`__ might be a good starting point.\n\n----\n\n### What is a subcommand?\nA subcommand is what is the action the LightningCLI applies to the script:\n\n```bash\npython main.py [subcommand]\n```\nSee the Potential subcommands with:\n\n```bash\npython main.py --help\n```\nwhich prints:\n\n```bash\n    ...\n\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n```\nuse a subcommand as follows:\n\n```bash\npython main.py fit\npython main.py test\n```\n----\n\n### What is the relation between LightningCLI and argparse?\n\n~lightning.pytorch.cli.LightningCLI makes use of jsonargparse_\nwhich is an extension of argparse_. Due to this,\n~lightning.pytorch.cli.LightningCLI follows the same arguments style as many POSIX command line tools. Long\noptions are prefixed with two dashes and its corresponding values are separated by space or an equal sign, as ``--option\nvalue or --option=value``. Command line options are parsed from left to right, therefore if a setting appears\nmultiple times, the value most to the right will override the previous ones.\n\n----\n\n### What is the override order of LightningCLI?\n\nThe final configuration of CLIs implemented with ~lightning.pytorch.cli.LightningCLI can depend on default\nconfig files (if defined), environment variables (if enabled) and command line arguments. The override order between\nthese is the following:\n\n1. Defaults defined in the source code.\n2. Existing default config files in the order defined in default_config_files, e.g. ~/.myapp.yaml.\n3. Entire config environment variable, e.g. PL_FIT__CONFIG.\n4. Individual argument environment variables, e.g. PL_FIT__SEED_EVERYTHING.\n5. Command line arguments in order left to right (might include config files).\n\n----\n\n### How do I troubleshoot a CLI?\nThe standard behavior for CLIs, when they fail, is to terminate the process with a non-zero exit code and a short\nmessage to hint the user about the cause. This is problematic while developing the CLI since there is no information to\ntrack down the root of the problem. To troubleshoot set the environment variable JSONARGPARSE_DEBUG to any value\nbefore running the CLI:\n\n```bash\nexport JSONARGPARSE_DEBUG=true\npython main.py fit\n```\nWhen asking about problems and reporting issues, please set the JSONARGPARSE_DEBUG and include the stack trace\nin your description. With this, users are more likely to help identify the cause without needing to create a\nreproducible script.",
        "score": 7
      }
    ]
  },
  {
    "query": "For defining a training step function, when should I use manual backward and what issues can arrise from using it?",
    "query_type": "api_usage",
    "relevant_docs": [
      {
        "file": "../data/final_data/discussions.json",
        "index": 20,
        "text": "When I use the official sample to carry out back propagation manually, I make mistakes. First, there is no optimizer, and second, there is no attribute in the image\nimport torch\nfrom torch import Tensor\nfrom pytorch_lightning import LightningModule\nclass Generator:\n    def __init__(self):\n        pass\n    def forward(self):\n        pass\n\nclass Discriminator:\n    def __init__(self):\n        pass\n    def forward(self):\n        pass\nclass SimpleGAN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.G = Generator()\n        self.D = Discriminator()\n\n        # Important: This property activates manual optimization.\n        self.automatic_optimization = False\n\n    def sample_z(self, n) -> Tensor:\n        sample = self._Z.sample((n,))\n        return sample\n\n    def sample_G(self, n) -> Tensor:\n        z = self.sample_z(n)\n        return self.G(z)\n\n    def training_step(self, batch, batch_idx):\n        # Implementation follows the PyTorch tutorial:\n        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n        g_opt, d_opt = self.optimizers()\n\n        X, _ = batch\n        batch_size = X.shape[0]\n\n        real_label = torch.ones((batch_size, 1), device=self.device)\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\n\n        g_X = self.sample_G(batch_size)\n\n        ##########################\n        # Optimize Discriminator #\n        ##########################\n        d_x = self.D(X)\n        errD_real = self.criterion(d_x, real_label)\n\n        d_z = self.D(g_X.detach())\n        errD_fake = self.criterion(d_z, fake_label)\n\n        errD = errD_real + errD_fake\n\n        d_opt.zero_grad()\n        self.manual_backward(errD)\n        d_opt.step()\n\n        ######################\n        # Optimize Generator #\n        ######################\n        d_z = self.D(g_X)\n        errG = self.criterion(d_z, real_label)\n\n        g_opt.zero_grad()\n        self.manual_backward(errG)\n        g_opt.step()\n\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\n    def configure_optimizers(self):\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n        return g_opt, d_opt\nbatch=torch.randn(3,2)\nbatch_idx=torch.ones(3)\nSimpleGAN().training_step(batch,batch_idx)\nAnswer: When I upgraded the version to the latest version, I solved this problem\u3002\nHowever, my running speed has been greatly affected. I used to have an epoch every 8 minutes, but now it has been delayed for a long time, and the data can't be loaded. I don't know why\nAnd this is the code\n[https://colab.research.google.com/drive/1dCP7-1xK48-PohGc8-RKx3Ne2HWd4Jkq#scrollTo=frTD9xWvBEUT]",
        "score": 7
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 20,
        "text": "LightningModule\n## LightningModule\n\nA ~lightning.pytorch.core.LightningModule organizes your PyTorch code into 6 sections:\n\n- Initialization (__init__ and ~lightning.pytorch.core.hooks.ModelHooks.setup).\n- Train Loop (~lightning.pytorch.core.LightningModule.training_step)\n- Validation Loop (~lightning.pytorch.core.LightningModule.validation_step)\n- Test Loop (~lightning.pytorch.core.LightningModule.test_step)\n- Prediction Loop (~lightning.pytorch.core.LightningModule.predict_step)\n- Optimizers and LR Schedulers (~lightning.pytorch.core.LightningModule.configure_optimizers)\n\nWhen you convert to use Lightning, the code IS NOT abstracted - just organized.\nAll the other code that's not in the ~lightning.pytorch.core.LightningModule\nhas been automated for you by the ~lightning.pytorch.trainer.trainer.Trainer.\n\n|\n\n```python\n    net = MyLightningModuleNet()\n    trainer = Trainer()\n    trainer.fit(net)\n```\nThere are no .cuda() or .to(device) calls required. Lightning does these for you.\n\n|\n\n```python\n    # don't do in Lightning\n    x = torch.Tensor(2, 3)\n    x = x.cuda()\n    x = x.to(device)\n\n    # do this instead\n    x = x  # leave it alone!\n\n    # or to init a new tensor\n    new_x = torch.Tensor(2, 3)\n    new_x = new_x.to(x)\n```\nWhen running under a distributed strategy, Lightning handles the distributed sampler for you by default.\n\n|\n\n```python\n    # Don't do in Lightning...\n    data = MNIST(...)\n    sampler = DistributedSampler(data)\n    DataLoader(data, sampler=sampler)\n\n    # do this instead\n    data = MNIST(...)\n    DataLoader(data)\n```\nA ~lightning.pytorch.core.LightningModule is a torch.nn.Module but with added functionality. Use it as such!\n\n|\n\n```python\n    net = Net.load_from_checkpoint(PATH)\n    net.freeze()\n    out = net(x)\n```\nThus, to use Lightning, you just need to organize your code which takes about 30 minutes,\n(and let's be real, you probably should do anyway).\n\n------------\n\n### Starter Example\n\nHere are the only required methods.\n\n```python\nimport lightning as L\nimport torch\n\nfrom lightning.pytorch.demos import Transformer\n\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, inputs, target):\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n```\nWhich you can train by doing:\n\n```python\nfrom lightning.pytorch.demos import WikiText2\nfrom torch.utils.data import DataLoader\n\ndataset = WikiText2()\ndataloader = DataLoader(dataset)\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\n\ntrainer = L.Trainer(fast_dev_run=100)\ntrainer.fit(model=model, train_dataloaders=dataloader)\n```\nThe LightningModule has many convenient methods, but the core ones you need to know about are:\n\n* - Name\n- Description\n* - __init__ and ~lightning.pytorch.core.hooks.ModelHooks.setup\n- Define initialization here\n* - ~lightning.pytorch.core.LightningModule.forward\n- To run data through your model only (separate from training_step)\n* - ~lightning.pytorch.core.LightningModule.training_step\n- the complete training step\n* - ~lightning.pytorch.core.LightningModule.validation_step\n- the complete validation step\n* - ~lightning.pytorch.core.LightningModule.test_step\n- the complete test step\n* - ~lightning.pytorch.core.LightningModule.predict_step\n- the complete prediction step\n* - ~lightning.pytorch.core.LightningModule.configure_optimizers\n- define optimizers and LR schedulers\n----------\n\n### Training\n\n# Training Loop\n\nTo activate the training loop, override the ~lightning.pytorch.core.LightningModule.training_step method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# enable gradient calculation\ntorch.set_grad_enabled(True)\n\nfor batch_idx, batch in enumerate(train_dataloader):\n    loss = training_step(batch, batch_idx)\n\n    # clear gradients\n    optimizer.zero_grad()\n\n    # backward\n    loss.backward()\n\n    # update parameters\n    optimizer.step()\n```\n# Train Epoch-level Metrics\n\nIf you want to calculate epoch-level metrics and log them, use ~lightning.pytorch.core.LightningModule.log.\n\n```python\ndef training_step(self, batch, batch_idx):\n    inputs, target = batch\n    output = self.model(inputs, target)\n    loss = torch.nn.functional.nll_loss(output, target.view(-1))\n\n    # logs metrics for each training_step,\n    # and the average across the epoch, to the progress bar and logger\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n```\nThe ~lightning.pytorch.core.LightningModule.log method automatically reduces the\nrequested metrics across a complete epoch and devices. Here's the pseudocode of what it does under the hood:\n\n```python\nouts = []\nfor batch_idx, batch in enumerate(train_dataloader):\n    # forward\n    loss = training_step(batch, batch_idx)\n    outs.append(loss.detach())\n\n    # clear gradients\n    optimizer.zero_grad()\n    # backward\n    loss.backward()\n    # update parameters\n    optimizer.step()\n\n# note: in reality, we do this incrementally, instead of keeping all outputs in memory\nepoch_metric = torch.mean(torch.stack(outs))\n```\n# Train Epoch-level Operations\n\nIn the case that you need to make use of all the outputs from each ~lightning.pytorch.LightningModule.training_step,\noverride the ~lightning.pytorch.LightningModule.on_train_epoch_end method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.training_step_outputs = []\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        preds = ...\n        self.training_step_outputs.append(preds)\n        return loss\n\n    def on_train_epoch_end(self):\n        all_preds = torch.stack(self.training_step_outputs)\n        # do something with all preds\n        ...\n        self.training_step_outputs.clear()  # free memory\n```\n------------------\n\n### Validation\n\n# Validation Loop\n\nTo activate the validation loop while training, override the ~lightning.pytorch.core.LightningModule.validation_step method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def validation_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"val_loss\", loss)\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# ...\nfor batch_idx, batch in enumerate(train_dataloader):\n    loss = model.training_step(batch, batch_idx)\n    loss.backward()\n    # ...\n\n    if validate_at_some_point:\n        # disable grads + batchnorm + dropout\n        torch.set_grad_enabled(False)\n        model.eval()\n\n        # ----------------- VAL LOOP ---------------\n        for val_batch_idx, val_batch in enumerate(val_dataloader):\n            val_out = model.validation_step(val_batch, val_batch_idx)\n        # ----------------- VAL LOOP ---------------\n\n        # enable grads + batchnorm + dropout\n        torch.set_grad_enabled(True)\n        model.train()\n```\nYou can also run just the validation loop on your validation dataloaders by overriding ~lightning.pytorch.core.LightningModule.validation_step\nand calling ~lightning.pytorch.trainer.trainer.Trainer.validate.\n\n```python\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ntrainer = L.Trainer()\ntrainer.validate(model)\n```\nIt is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\nThis is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\nmulti-device setting, samples could occur duplicated when ~torch.utils.data.distributed.DistributedSampler\nis used, for eg. with strategy=\"ddp\". It replicates some samples on some devices to make sure all devices have\nsame batch size in case of uneven inputs.\n# Validation Epoch-level Metrics\n\nIn the case that you need to make use of all the outputs from each ~lightning.pytorch.LightningModule.validation_step,\noverride the ~lightning.pytorch.LightningModule.on_validation_epoch_end method.\nNote that this method is called before ~lightning.pytorch.LightningModule.on_train_epoch_end.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.validation_step_outputs = []\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        pred = ...\n        self.validation_step_outputs.append(pred)\n        return pred\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.stack(self.validation_step_outputs)\n        # do something with all preds\n        ...\n        self.validation_step_outputs.clear()  # free memory\n```\n----------------\n\n### Testing\n\n# Test Loop\n\nThe process for enabling a test loop is the same as the process for enabling a validation loop. Please refer to\nthe section above for details. For this you need to override the ~lightning.pytorch.core.LightningModule.test_step method.\n\nThe only difference is that the test loop is only called when ~lightning.pytorch.trainer.trainer.Trainer.test is used.\n\n```python\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ndataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically loads the best weights for you\ntrainer.test(model)\n```\nThere are two ways to call test():\n\n```python\n# call after training\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically auto-loads the best weights from the previous run\ntrainer.test(dataloaders=test_dataloaders)\n\n# or call with pretrained model\nmodel = LightningTransformer.load_from_checkpoint(PATH)\ndataset = WikiText2()\ntest_dataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.test(model, dataloaders=test_dataloader)\n```\n`WikiText2` is used in a manner that does not create a train, test, val split. This is done for illustrative purposes only.\nA proper split can be created in lightning.pytorch.core.LightningModule.setup or lightning.pytorch.core.LightningDataModule.setup.\nIt is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\nThis is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\nmulti-device setting, samples could occur duplicated when ~torch.utils.data.distributed.DistributedSampler\nis used, for eg. with strategy=\"ddp\". It replicates some samples on some devices to make sure all devices have\nsame batch size in case of uneven inputs.\n----------\n\n### Inference\n\n# Prediction Loop\n\nBy default, the ~lightning.pytorch.core.LightningModule.predict_step method runs the\n~lightning.pytorch.core.LightningModule.forward method. In order to customize this behaviour,\nsimply override the ~lightning.pytorch.core.LightningModule.predict_step method.\n\nFor the example let's override predict_step:\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def predict_step(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# disable grads + batchnorm + dropout\ntorch.set_grad_enabled(False)\nmodel.eval()\nall_preds = []\n\nfor batch_idx, batch in enumerate(predict_dataloader):\n    pred = model.predict_step(batch, batch_idx)\n    all_preds.append(pred)\n```\nThere are two ways to call predict():\n\n```python\n# call after training\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically auto-loads the best weights from the previous run\npredictions = trainer.predict(dataloaders=predict_dataloader)\n\n# or call with pretrained model\nmodel = LightningTransformer.load_from_checkpoint(PATH)\ndataset = WikiText2()\ntest_dataloader = DataLoader(dataset)\ntrainer = L.Trainer()\npredictions = trainer.predict(model, dataloaders=test_dataloader)\n```\n# Inference in Research\n\nIf you want to perform inference with the system, you can add a forward method to the LightningModule.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\n\nmodel.eval()\nwith torch.no_grad():\n    batch = dataloader.dataset[0]\n    pred = model(batch)\n```\nThe advantage of adding a forward is that in complex systems, you can do a much more involved inference procedure,\nsuch as text generation:\n\n```python\nclass Seq2Seq(L.LightningModule):\n    def forward(self, x):\n        embeddings = self(x)\n        hidden_states = self.encoder(embeddings)\n        for h in hidden_states:\n            # decode\n            ...\n        return decoded\n```\nIn the case where you want to scale your inference, you should be using\n~lightning.pytorch.core.LightningModule.predict_step.\n\n```python\nclass Autoencoder(L.LightningModule):\n    def forward(self, x):\n        return self.decoder(x)\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        # this calls forward\n        return self(batch)\n\ndata_module = ...\nmodel = Autoencoder()\ntrainer = Trainer(accelerator=\"gpu\", devices=2)\ntrainer.predict(model, data_module)\n```\n# Inference in Production\n\nFor cases like production, you might want to iterate different models inside a LightningModule.\n\n```python\nfrom torchmetrics.functional import accuracy\n\nclass ClassificationTask(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def test_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        return loss, acc\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch\n        y_hat = self.model(x)\n        return y_hat\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=0.02)\n```\nThen pass in any arbitrary model to be fit with this task\n\n```python\nfor model in [resnet50(), vgg16(), BidirectionalRNN()]:\n    task = ClassificationTask(model)\n\n    trainer = Trainer(accelerator=\"gpu\", devices=2)\n    trainer.fit(task, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n```\nTasks can be arbitrarily complex such as implementing GAN training, self-supervised or even RL.\n\n```python\nclass GANTask(L.LightningModule):\n    def __init__(self, generator, discriminator):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n\n    ...\n```\nWhen used like this, the model can be separated from the Task and thus used in production without needing to keep it in\na LightningModule.\n\nThe following example shows how you can run inference in the Python runtime:\n\n```python\ntask = ClassificationTask(model)\ntrainer = Trainer(accelerator=\"gpu\", devices=2)\ntrainer.fit(task, train_dataloader, val_dataloader)\ntrainer.save_checkpoint(\"best_model.ckpt\")\n\n# use model after training or load weights and drop into the production system\nmodel = ClassificationTask.load_from_checkpoint(\"best_model.ckpt\")\nx = ...\nmodel.eval()\nwith torch.no_grad():\n    y_hat = model(x)\n```\nCheck out Inference in Production <production_inference> guide to learn about the possible ways to perform inference in production.\n\n-----------\n\n### Save Hyperparameters\n\nOften times we train many versions of a model. You might share that model or come back to it a few months later at which\npoint it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc...).\n\nLightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to\nimprove readability and reproducibility.\n\n# save_hyperparameters\n\nUse ~lightning.pytorch.core.LightningModule.save_hyperparameters within your\n~lightning.pytorch.core.LightningModule's __init__ method. It will enable Lightning to store all the\nprovided arguments under the self.hparams attribute. These hyperparameters will also be stored within the model\ncheckpoint, which simplifies model re-instantiation after training.\n\n```python\nclass LitMNIST(L.LightningModule):\n    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n        super().__init__()\n        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n        self.save_hyperparameters()\n\n        # equivalent\n        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n\n        # Now possible to access layer_1_dim from hparams\n        self.hparams.layer_1_dim\n```\nIn addition, loggers that support it will automatically log the contents of self.hparams.\n\n# Excluding hyperparameters\n\nBy default, every parameter of the __init__ method will be considered a hyperparameter to the LightningModule.\nHowever, sometimes some parameters need to be excluded from saving, for example when they are not serializable. Those\nparameters should be provided back when reloading the LightningModule. In this case, exclude them explicitly:\n\n```python\nclass LitMNIST(L.LightningModule):\n    def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n        super().__init__()\n        self.layer_1_dim = layer_1_dim\n        self.loss_fx = loss_fx\n\n        # call this to save only (layer_1_dim=128) to the checkpoint\n        self.save_hyperparameters(\"layer_1_dim\")\n\n        # equivalent\n        self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])\n```\n# load_from_checkpoint\n\nLightningModules that have hyperparameters automatically saved with\n~lightning.pytorch.core.LightningModule.save_hyperparameters can conveniently be loaded and instantiated\ndirectly from a checkpoint with ~lightning.pytorch.core.LightningModule.load_from_checkpoint:\n\n```python\n# to load specify the other args\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\nIf parameters were excluded, they need to be provided at the time of loading:\n\n```python\n# the excluded parameters were `loss_fx` and `generator_network`\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\n-----------\n\n### Child Modules\n\n-----------\n\n### LightningModule API\n\n# Methods\n\n### all_gather\n\n### configure_callbacks\n\n### configure_optimizers\n\n### forward\n\n### freeze\n\n### log\n\n### log_dict\n\n### lr_schedulers\n\n### manual_backward\n\n### optimizers\n\n### print\n\n### predict_step\n\n### save_hyperparameters\n\n### toggle_optimizer\n\n### test_step\n\n### to_onnx\n\n### to_torchscript\n\n### training_step\n\n### unfreeze\n\n### untoggle_optimizer\n\n### validation_step\n\n-----------\n\n# Properties\n\nThese are properties available in a LightningModule.\n\n### current_epoch\n\nThe number of epochs run.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.current_epoch == 0:\n        ...\n```\n### device\n\nThe device the module is on. Use it to keep your code device agnostic.\n\n```python\ndef training_step(self, batch, batch_idx):\n    z = torch.rand(2, 3, device=self.device)\n```\n### global_rank\n\nThe global_rank is the index of the current process across all nodes and devices.\nLightning will perform some operations such as logging, weight checkpointing only when global_rank=0. You\nusually do not need to use this property, but it is useful to know how to access it if needed.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.global_rank == 0:\n        # do something only once across all the nodes\n        ...\n```\n### global_step\n\nThe number of optimizer steps taken (does not reset each epoch).\nThis includes multiple optimizers (if enabled).\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.logger.experiment.log_image(..., step=self.global_step)\n```\n### hparams\n\nThe arguments passed through LightningModule.__init__() and saved by calling\n~lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin.save_hyperparameters could be accessed by the hparams attribute.\n\n```python\ndef __init__(self, learning_rate):\n    self.save_hyperparameters()\n\ndef configure_optimizers(self):\n    return Adam(self.parameters(), lr=self.hparams.learning_rate)\n```\n### logger\n\nThe current logger being used (tensorboard or other supported logger)\n\n```python\ndef training_step(self, batch, batch_idx):\n    # the generic logger (same no matter if tensorboard or other supported logger)\n    self.logger\n\n    # the particular logger\n    tensorboard_logger = self.logger.experiment\n```\n### loggers\n\nThe list of loggers currently being used by the Trainer.\n\n```python\ndef training_step(self, batch, batch_idx):\n    # List of Logger objects\n    loggers = self.loggers\n    for logger in loggers:\n        logger.log_metrics({\"foo\": 1.0})\n```\n### local_rank\n\nThe local_rank is the index of the current process across all the devices for the current node.\nYou usually do not need to use this property, but it is useful to know how to access it if needed.\nFor example, if using 10 machines (or nodes), the GPU at index 0 on each machine has local_rank = 0.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.local_rank == 0:\n        # do something only once across each node\n        ...\n```\n### precision\n\nThe type of precision used:\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.precision == \"16-true\":\n        ...\n```\n### trainer\n\nPointer to the trainer\n\n```python\ndef training_step(self, batch, batch_idx):\n    max_steps = self.trainer.max_steps\n    any_flag = self.trainer.any_flag\n```\n### prepare_data_per_node\n\nIf set to True will call prepare_data() on LOCAL_RANK=0 for every node.\nIf set to False will only call from NODE_RANK=0, LOCAL_RANK=0.\n\nclass LitModel(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.prepare_data_per_node = True\n### automatic_optimization\n\nWhen set to False, Lightning does not automate the optimization process. This means you are responsible for handling\nyour optimizers. However, we do take care of precision and any accelerators used.\n\nSee manual optimization <common/optimization:Manual optimization> for details.\n\n```python\ndef __init__(self):\n    self.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\n    opt = self.optimizers(use_pl_optimizer=True)\n\n    loss = ...\n    opt.zero_grad()\n    self.manual_backward(loss)\n    opt.step()\n```\nManual optimization is most useful for research topics like reinforcement learning, sparse coding, and GAN research.\nIt is required when you are using 2+ optimizers because with automatic optimization, you can only use one optimizer.\n\n```python\ndef __init__(self):\n    self.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\n    # access your optimizers with use_pl_optimizer=False. Default is True\n    opt_a, opt_b = self.optimizers(use_pl_optimizer=True)\n\n    gen_loss = ...\n    opt_a.zero_grad()\n    self.manual_backward(gen_loss)\n    opt_a.step()\n\n    disc_loss = ...\n    opt_b.zero_grad()\n    self.manual_backward(disc_loss)\n    opt_b.step()\n```\n### example_input_array\n\nSet and access example_input_array, which basically represents a single batch.\n\n```python\ndef __init__(self):\n    self.example_input_array = ...\n    self.generator = ...\n\ndef on_train_epoch_end(self):\n    # generate some images using the example_input_array\n    gen_images = self.generator(self.example_input_array)\n```\n--------------\n\n# Hooks\n\nThis is the pseudocode to describe the structure of ~lightning.pytorch.trainer.trainer.Trainer.fit.\nThe inputs and outputs of each function are not represented for simplicity. Please check each function's API reference\nfor more information.\n\n```python\n# runs on every device: devices can be GPUs, TPUs, ...\ndef fit(self):\n    configure_callbacks()\n\n    if local_rank == 0:\n        prepare_data()\n\n    setup(\"fit\")\n    configure_model()\n    configure_optimizers()\n\n    on_fit_start()\n\n    # the sanity check runs here\n\n    on_train_start()\n    for epoch in epochs:\n        fit_loop()\n    on_train_end()\n\n    on_fit_end()\n    teardown(\"fit\")\n\ndef fit_loop():\n    torch.set_grad_enabled(True)\n\n    on_train_epoch_start()\n\n    for batch_idx, batch in enumerate(train_dataloader()):\n        on_train_batch_start()\n\n        on_before_batch_transfer()\n        transfer_batch_to_device()\n        on_after_batch_transfer()\n\n        out = training_step()\n\n        on_before_zero_grad()\n        optimizer_zero_grad()\n\n        on_before_backward()\n        backward()\n        on_after_backward()\n\n        on_before_optimizer_step()\n        configure_gradient_clipping()\n        optimizer_step()\n\n        on_train_batch_end(out, batch, batch_idx)\n\n        if should_check_val:\n            val_loop()\n\n    on_train_epoch_end()\n\ndef val_loop():\n    on_validation_model_eval()  # calls `model.eval()`\n    torch.set_grad_enabled(False)\n\n    on_validation_start()\n    on_validation_epoch_start()\n\n    for batch_idx, batch in enumerate(val_dataloader()):\n        on_validation_batch_start(batch, batch_idx)\n\n        batch = on_before_batch_transfer(batch)\n        batch = transfer_batch_to_device(batch)\n        batch = on_after_batch_transfer(batch)\n\n        out = validation_step(batch, batch_idx)\n\n        on_validation_batch_end(out, batch, batch_idx)\n\n    on_validation_epoch_end()\n    on_validation_end()\n\n    # set up for train\n    on_validation_model_train()  # calls `model.train()`\n    torch.set_grad_enabled(True)\n```\n### backward\n\n### on_before_backward\n\n### on_after_backward\n\n### on_before_zero_grad\n### on_fit_start\n\n### on_fit_end\n\n### on_load_checkpoint\n\n### on_save_checkpoint\n\n### load_from_checkpoint\n\n### on_train_start\n\n### on_train_end\n\n### on_validation_start\n\n### on_validation_end\n\n### on_test_batch_start\n\n### on_test_batch_end\n\n### on_test_epoch_start\n\n### on_test_epoch_end\n\n### on_test_start\n\n### on_test_end\n\n### on_predict_batch_start\n\n### on_predict_batch_end\n\n### on_predict_epoch_start\n\n### on_predict_epoch_end\n\n### on_predict_start\n\n### on_predict_end\n\n### on_train_batch_start\n\n### on_train_batch_end\n\n### on_train_epoch_start\n\n### on_train_epoch_end\n\n### on_validation_batch_start\n\n### on_validation_batch_end\n\n### on_validation_epoch_start\n\n### on_validation_epoch_end\n\n### configure_model\n\n### on_validation_model_eval\n\n### on_validation_model_train\n\n### on_test_model_eval\n\n### on_test_model_train\n\n### on_before_optimizer_step\n\n### configure_gradient_clipping\n\n### optimizer_step\n\n### optimizer_zero_grad\n\n### prepare_data\n\n### setup\n\n### teardown\n\n### train_dataloader\n\n### val_dataloader\n\n### test_dataloader\n\n### predict_dataloader\n\n### transfer_batch_to_device\n\n### on_before_batch_transfer\n\n### on_after_batch_transfer",
        "score": 8
      },
      {
        "file": "../data/final_data/lightning_docs_cleaned.json",
        "index": 28,
        "text": "Truncated Backpropagation Through Time (TBPTT)\n## Truncated Backpropagation Through Time (TBPTT)\n\nTruncated Backpropagation Through Time (TBPTT) performs backpropagation every k steps of\na much longer sequence. This is made possible by passing training batches\nsplit along the time-dimensions into splits of size k to the\ntraining_step. In order to keep the same forward propagation behavior, all\nhidden states should be kept in-between each time-dimension split.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightning as L\n\nclass AverageDataset(Dataset):\n    def __init__(self, dataset_len=300, sequence_len=100):\n        self.dataset_len = dataset_len\n        self.sequence_len = sequence_len\n        self.input_seq = torch.randn(dataset_len, sequence_len, 10)\n        top, bottom = self.input_seq.chunk(2, -1)\n        self.output_seq = top + bottom.roll(shifts=1, dims=-1)\n\n    def __len__(self):\n        return self.dataset_len\n\n    def __getitem__(self, item):\n        return self.input_seq[item], self.output_seq[item]\n\nclass LitModel(L.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n\n        self.batch_size = 10\n        self.in_features = 10\n        self.out_features = 5\n        self.hidden_dim = 20\n\n        # 1. Switch to manual optimization\n        self.automatic_optimization = False\n        self.truncated_bptt_steps = 10\n\n        self.rnn = nn.LSTM(self.in_features, self.hidden_dim, batch_first=True)\n        self.linear_out = nn.Linear(in_features=self.hidden_dim, out_features=self.out_features)\n\n    def forward(self, x, hs):\n        seq, hs = self.rnn(x, hs)\n        return self.linear_out(seq), hs\n\n    # 2. Remove the `hiddens` argument\n    def training_step(self, batch, batch_idx):\n        # 3. Split the batch in chunks along the time dimension\n        x, y = batch\n        split_x, split_y = [\n            x.tensor_split(self.truncated_bptt_steps, dim=1),\n            y.tensor_split(self.truncated_bptt_steps, dim=1)\n        ]\n\n        hiddens = None\n        optimizer = self.optimizers()\n        losses = []\n\n        # 4. Perform the optimization in a loop\n        for x, y in zip(split_x, split_y):\n            y_pred, hiddens = self(x, hiddens)\n            loss = F.mse_loss(y_pred, y)\n\n            optimizer.zero_grad()\n            self.manual_backward(loss)\n            optimizer.step()\n\n            # 5. \"Truncate\"\n            hiddens = [h.detach() for h in hiddens]\n            losses.append(loss.detach())\n\n        avg_loss = sum(losses) / len(losses)\n        self.log(\"train_loss\", avg_loss, prog_bar=True)\n\n        # 6. Remove the return of `hiddens`\n        # Returning loss in manual optimization is not needed\n        return None\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=0.001)\n\n    def train_dataloader(self):\n        return DataLoader(AverageDataset(), batch_size=self.batch_size)\n\nif __name__ == \"__main__\":\n    model = LitModel()\n    trainer = L.Trainer(max_epochs=5)\n    trainer.fit(model)\n```",
        "score": 6
      }
    ]
  },
  {
    "query": "init_meta_context() isn't available. What are some alternitives that have similar functionality?",
    "query_type": "debugging",
    "relevant_docs": [
      {
        "file": "../data/final_data/discussions.json",
        "index": 3,
        "text": "Current best practices to initialize massive (50B parameter+) models\nHi, I am working with GPT-style models and need to intitialize a model at the GPT-3 scale.  Unfortunately, this means the model will run out of memory during initialization on CPU (or take an eternity to initialize layer-by-layer on cpu before shipping to GPU).  In vanilla Pytorch I solved this using FSDP by initializing my models on the \"meta\" device, with full initialization on GPU afterward.  What is the current best, most performant method to accomplish this with lightning?\nNote: I found this, which references an init_meta_context(), but my pytorch-lightning (v1.9.0) has no such functionality:\nhttps://devblog.pytorchlightning.ai/experiment-with-billion-parameter-models-faster-using-deepspeed-and-meta-tensors-2e9c255edd71\nAnswer: Hi! The init_meta_context functionality was replaced with a torchdistx integration in #13868. You can do the following:\nfrom torchdistx.deferred_init import deferred_init\n\nmodel = deferred_init(YourLightningModule)\nAnd we'll materialize it for you in the Trainer. This is very experimental, and you might encounter installation issues.\nIn the long term, we'll adopt the fake tensor mode from PyTorch: #16448.\nOtherwise, for a stable(r) solution, you can use the DeepSpeed integration: https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed-zero-stage-3",
        "score": 7
      },
      {
        "file": "../data/final_data/src_filtered_data.json",
        "index": 1346,
        "text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\strategies\\strategy.py\nFunction Name: model_sharded_context\nLanguage: python\nPartition: valid\n\n--- Docstring ---\nProvide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\n\n--- Code ---\ndef model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield\n\n--- Original String ---\ndef model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield",
        "score": 7
      }
    ]
  }

]