[
  {
    "file": "docs/pytorch/stable/cli/lightning_cli.html",
    "label": "docs",
    "title": "Why use a CLI",
    "text": "When running deep learning experiments, there are a couple of good practices that are recommended to follow:\n\n- Separate configuration from source code\n- Guarantee reproducibility of experiments\n\nImplementing a command line interface (CLI) makes it possible to execute an experiment from a shell terminal. By having\na CLI, there is a clear separation between the Python source code and what hyperparameters are used for a particular\nexperiment. If the CLI corresponds to a stable version of the code, reproducing an experiment can be achieved by\ninstalling the same version of the code plus dependencies and running with the same configuration (CLI arguments).\n\n----",
    "parent_index": 0,
    "index": 0
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
    "label": "docs",
    "title": "Run using a config file",
    "text": "To run the CLI using a yaml config, do:\n\n```bash\npython main.py fit --config config.yaml\n```\nIndividual arguments can be given to override options in the config file:\n\n```bash\npython main.py fit --config config.yaml --trainer.max_epochs 100\n```\n----",
    "parent_index": 1,
    "index": 1
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
    "label": "docs",
    "title": "Automatic save of config",
    "text": "To ease experiment reporting and reproducibility, by default LightningCLI automatically saves the full YAML\nconfiguration in the log directory. After multiple fit runs with different hyperparameters, each one will have in its\nrespective log directory a config.yaml file. These files can be used to trivially reproduce an experiment, e.g.:\n\n```bash\npython main.py fit --config lightning_logs/version_7/config.yaml\n```\nThe automatic saving of the config is done by the special callback ~lightning.pytorch.cli.SaveConfigCallback.\nThis callback is automatically added to the Trainer. To disable the save of the config, instantiate LightningCLI\nwith save_config_callback=None.\n\nTo change the file name of the saved configs to e.g. name.yaml, do:\n\n.. code:: python\n\ncli = LightningCLI(..., save_config_kwargs={\"config_filename\": \"name.yaml\"})\nIt is also possible to extend the ~lightning.pytorch.cli.SaveConfigCallback class, for instance to additionally\nsave the config in a logger. An example of this is:\n\n```python\n    class LoggerSaveConfigCallback(SaveConfigCallback):\n        def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\n            if isinstance(trainer.logger, Logger):\n                config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\n                trainer.logger.log_hyperparams({\"config\": config})\n\n    cli = LightningCLI(..., save_config_callback=LoggerSaveConfigCallback)\n```\nIf you want to disable the standard behavior of saving the config to the log_dir, then you can either implement\n__init__ and call super().__init__(*args, save_to_log_dir=False, **kwargs) or instantiate the\nLightningCLI as:\n\n.. code:: python\n\ncli = LightningCLI(..., save_config_kwargs={\"save_to_log_dir\": False})\nThe save_config method is only called on rank zero. This allows to implement a custom save config without having\nto worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the process\nhang waiting for a broadcast. If you need to make collective calls, implement the setup method instead.\n----",
    "parent_index": 1,
    "index": 2
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
    "label": "docs",
    "title": "Prepare a config file for the CLI",
    "text": "The --help option of the CLIs can be used to learn which configuration options are available and how to use them.\nHowever, writing a config from scratch can be time-consuming and error-prone. To alleviate this, the CLIs have the\n--print_config argument, which prints to stdout the configuration without running the command.\n\nFor a CLI implemented as LightningCLI(DemoModel, BoringDataModule), executing:\n\n```bash\npython main.py fit --print_config\n```\ngenerates a config with all default values like the following:\n\n```bash\nseed_everything: null\ntrainer:\n  logger: true\n  ...\nmodel:\n  out_dim: 10\n  learning_rate: 0.02\ndata:\n  data_dir: ./\nckpt_path: null\n```\nOther command line arguments can be given and considered in the printed configuration. A use case for this is CLIs that\naccept multiple models. By default, no model is selected, meaning the printed config will not include model settings. To\nget a config with the default values of a particular model would be:\n\n```bash\npython main.py fit --model DemoModel --print_config\n```\nwhich generates a config like:\n\n```bash\nseed_everything: null\ntrainer:\n  ...\nmodel:\n  class_path: lightning.pytorch.demos.boring_classes.DemoModel\n  init_args:\n    out_dim: 10\n    learning_rate: 0.02\nckpt_path: null\n```\nA standard procedure to run experiments can be:\n\n.. code:: bash\n\n# Print a configuration to have as reference\npython main.py fit --print_config > config.yaml\n# Modify the config to your liking - you can remove all default arguments\nnano config.yaml\n# Fit your model using the edited configuration\npython main.py fit --config config.yaml\nConfiguration items can be either simple Python objects such as int and str,\nor complex objects comprised of a class_path and init_args arguments. The class_path refers\nto the complete import path of the item class, while init_args are the arguments to be passed\nto the class constructor. For example, your model is defined as:\n\n```python\n# model.py\nclass MyModel(L.LightningModule):\n    def __init__(self, criterion: torch.nn.Module):\n        self.criterion = criterion\n```\nThen the config would be:\n\n```yaml\nmodel:\n  class_path: model.MyModel\n  init_args:\n    criterion:\n      class_path: torch.nn.CrossEntropyLoss\n      init_args:\n        reduction: mean\n    ...\n```\nLightningCLI uses jsonargparse under the hood for parsing\nconfiguration files and automatic creation of objects, so you don't need to do it yourself.\n\nLightning automatically registers all subclasses of ~lightning.pytorch.core.LightningModule,\nso the complete import path is not required for them and can be replaced by the class name.\nParsers make a best effort to determine the correct names and types that the parser should accept.\nHowever, there can be cases not yet supported or cases for which it would be impossible to support.\nTo somewhat overcome these limitations, there is a special key dict_kwargs that can be used\nto provide arguments that will not be validated during parsing, but will be used for class instantiation.\n\nFor example, then using the lightning.pytorch.profilers.PyTorchProfiler profiler,\nthe profile_memory argument has a type that is determined dynamically. As a result, it's not possible\nto know the expected type during parsing. To account for this, your config file should be set up like this:\n\n.. code:: yaml\n\ntrainer:\nprofiler:\nclass_path: lightning.pytorch.profilers.PyTorchProfiler\ndict_kwargs:\nprofile_memory: true\n----",
    "parent_index": 1,
    "index": 3
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
    "label": "docs",
    "title": "Compose config files",
    "text": "Multiple config files can be provided, and they will be parsed sequentially. Let's say we have two configs with common\nsettings:\n\n```yaml\n# config_1.yaml\ntrainer:\n  num_epochs: 10\n  ...\n\n# config_2.yaml\ntrainer:\n  num_epochs: 20\n  ...\n```\nThe value from the last config will be used, num_epochs = 20 in this case:\n\n```bash\n$ python main.py fit --config config_1.yaml --config config_2.yaml\n```\n----",
    "parent_index": 1,
    "index": 4
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced.html",
    "label": "docs",
    "title": "Use groups of options",
    "text": "Groups of options can also be given as independent config files. For configs like:\n\n```yaml\n# trainer.yaml\nnum_epochs: 10\n\n# model.yaml\nout_dim: 7\n\n# data.yaml\ndata_dir: ./data\n```\na fit command can be run as:\n\n```bash\n$ python main.py fit --trainer trainer.yaml --model model.yaml --data data.yaml [...]\n```",
    "parent_index": 1,
    "index": 5
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Customize arguments by subcommand",
    "text": "To customize arguments by subcommand, pass the config *before* the subcommand:\n\n```bash\n$ python main.py [before] [subcommand] [after]\n$ python main.py  ...         fit       ...\n```\nFor example, here we set the Trainer argument [max_steps = 100] for the full training routine and [max_steps = 10] for\ntesting:\n\n```bash\n# config.yaml\nfit:\n    trainer:\n        max_steps: 100\ntest:\n    trainer:\n        max_epochs: 10\n```\nnow you can toggle this behavior by subcommand:\n\n```bash\n# full routine with max_steps = 100\n$ python main.py --config config.yaml fit\n\n# test only with max_epochs = 10\n$ python main.py --config config.yaml test\n```\n----",
    "parent_index": 2,
    "index": 6
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Run from cloud yaml configs",
    "text": "For certain enterprise workloads, Lightning CLI supports running from hosted configs:\n\n```bash\n$ python main.py [subcommand] --config s3://bucket/config.yaml\n```\nFor more options, refer to Remote filesystems <../common/remote_fs>.\n\n----",
    "parent_index": 2,
    "index": 7
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Use a config via environment variables",
    "text": "For certain CI/CD systems, it's useful to pass in raw yaml config as environment variables:\n\n```bash\n$ python main.py fit --trainer \"$TRAINER_CONFIG\" --model \"$MODEL_CONFIG\" [...]\n```\n----",
    "parent_index": 2,
    "index": 8
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Run from environment variables directly",
    "text": "The Lightning CLI can convert every possible CLI flag into an environment variable. To enable this, add to\nparser_kwargs the default_env argument:\n\n```python\ncli = LightningCLI(..., parser_kwargs={\"default_env\": True})\n```\nnow use the --help CLI flag with any subcommand:\n\n```bash\n$ python main.py fit --help\n```\nwhich will show you ALL possible environment variables that can be set:\n\n```bash\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            ...\n\noptional arguments:\n...\nARG:   --model.out_dim OUT_DIM\nENV:   PL_FIT__MODEL__OUT_DIM\n                        (type: int, default: 10)\nARG:   --model.learning_rate LEARNING_RATE\nENV:   PL_FIT__MODEL__LEARNING_RATE\n                        (type: float, default: 0.02)\n```\nnow you can customize the behavior via environment variables:\n\n```bash\n# set the options via env vars\n$ export PL_FIT__MODEL__LEARNING_RATE=0.01\n$ export PL_FIT__MODEL__OUT_DIM=5\n\n$ python main.py fit\n```\n----",
    "parent_index": 2,
    "index": 9
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Set default config files",
    "text": "To set a path to a config file of defaults, use the default_config_files argument:\n\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"default_config_files\": [\"my_cli_defaults.yaml\"]})\nor if you want defaults per subcommand:\n\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"fit\": {\"default_config_files\": [\"my_fit_defaults.yaml\"]}})\n----",
    "parent_index": 2,
    "index": 10
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_advanced_2.html",
    "label": "docs",
    "title": "Enable variable interpolation",
    "text": "In certain cases where multiple settings need to share a value, consider using variable interpolation. For instance:\n\n```yaml\nmodel:\n  encoder_layers: 12\n  decoder_layers:\n  - ${model.encoder_layers}\n  - 4\n```\nTo enable variable interpolation, first install omegaconf:\n\n```bash\npip install omegaconf\n```\nThen set omegaconf when instantiating the LightningCLI class:\n\n```python\ncli = LightningCLI(MyModel, parser_kwargs={\"parser_mode\": \"omegaconf\"})\n```\nAfter this, the CLI will automatically perform interpolation in yaml files:\n\n```bash\npython main.py --model.encoder_layers=12\n```\nFor more details about the interpolation support and its limitations, have a look at the `jsonargparse\n<https://jsonargparse.readthedocs.io/en/stable/#variable-interpolation>`__ and the `omegaconf\n<https://omegaconf.readthedocs.io/en/2.1_branch/usage.html#variable-interpolation>`__ documentations.\n\nThere are many use cases in which variable interpolation is not the correct approach. When a parameter **must\nalways** be derived from other settings, it shouldn't be up to the CLI user to do this in a config file. For\nexample, if the data and model both require batch_size and must be the same value, then\ncli_link_arguments should be used instead of interpolation.",
    "parent_index": 2,
    "index": 11
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_expert.html",
    "label": "docs",
    "title": "Customize the LightningCLI",
    "text": "The init parameters of the ~lightning.pytorch.cli.LightningCLI class can be used to customize some things,\ne.g., the description of the tool, enabling parsing of environment variables, and additional arguments to instantiate\nthe trainer and configuration parser.\n\nNevertheless, the init arguments are not enough for many use cases. For this reason, the class is designed so that it\ncan be extended to customize different parts of the command line tool. The argument parser class used by\n~lightning.pytorch.cli.LightningCLI is ~lightning.pytorch.cli.LightningArgumentParser, which is an\nextension of python's argparse, thus adding arguments can be done using the add_argument method. In contrast to\nargparse, it has additional methods to add arguments. For example add_class_arguments add all arguments from the\ninit of a class. For more details, see the `respective documentation\n<https://jsonargparse.readthedocs.io/en/stable/#classes-methods-and-functions>`_.\n\nThe ~lightning.pytorch.cli.LightningCLI class has the\n~lightning.pytorch.cli.LightningCLI.add_arguments_to_parser method can be implemented to include more arguments.\nAfter parsing, the configuration is stored in the config attribute of the class instance. The\n~lightning.pytorch.cli.LightningCLI class also has two methods that can be used to run code before and after\nthe trainer runs: before_<subcommand> and after_<subcommand>. A realistic example of this would be to send an\nemail before and after the execution. The code for the fit subcommand would be something like this:\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_argument(\"--notification_email\", default=\"will@email.com\")\n\ndef before_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit starting\")\n\ndef after_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit finished\")\n\ncli = MyLightningCLI(MyModel)\nNote that the config object self.config is a namespace whose keys are global options or groups of options. It has\nthe same structure as the YAML format described previously. This means that the parameters used for instantiating the\ntrainer class can be found in self.config['fit']['trainer'].\n\nHave a look at the ~lightning.pytorch.cli.LightningCLI class API reference to learn about other methods\nthat can be extended to customize a CLI.\n----",
    "parent_index": 4,
    "index": 12
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_expert.html",
    "label": "docs",
    "title": "Configure forced callbacks",
    "text": "As explained previously, any Lightning callback can be added by passing it through the command line or including it in\nthe config via class_path and init_args entries.\n\nHowever, certain callbacks **must** be coupled with a model so they are always present and configurable. This can be\nimplemented as follows:\n\nfrom lightning.pytorch.callbacks import EarlyStopping\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_lightning_class_args(EarlyStopping, \"my_early_stopping\")\nparser.set_defaults({\"my_early_stopping.monitor\": \"val_loss\", \"my_early_stopping.patience\": 5})\n\ncli = MyLightningCLI(MyModel)\nTo change the parameters for EarlyStopping in the config it would be:\n\n```yaml\nmodel:\n  ...\ntrainer:\n  ...\nmy_early_stopping:\n  patience: 5\n```\nThe example above overrides a default in add_arguments_to_parser. This is included to show that defaults can be\nchanged if needed. However, note that overriding defaults in the source code is not intended to be used to store the\nbest hyperparameters for a task after experimentation. To guarantee reproducibility, the source code should be\nstable. It is better to practice storing the best hyperparameters for a task in a configuration file independent\nfrom the source code.\n----",
    "parent_index": 4,
    "index": 13
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_expert.html",
    "label": "docs",
    "title": "Class type defaults",
    "text": "The support for classes as type hints allows to try many possibilities with the same CLI. This is a useful feature, but\nit is tempting to use an instance of a class as a default. For example:\n\nclass MyMainModel(LightningModule):\ndef __init__(\nself,\nbackbone: torch.nn.Module = MyModel(encoder_layers=24), # BAD PRACTICE!\n):\nsuper().__init__()\nself.backbone = backbone\nNormally classes are mutable, as in this case. The instance of MyModel would be created the moment that the module\nthat defines MyMainModel is first imported. This means that the default of backbone will be initialized before\nthe CLI class runs seed_everything, making it non-reproducible. Furthermore, if MyMainModel is used more than\nonce in the same Python process and the backbone parameter is not overridden, the same instance would be used in\nmultiple places. Most likely, this is not what the developer intended. Having an instance as default also makes it\nimpossible to generate the complete config file since it is not known which arguments were used to instantiate it for\narbitrary classes.\n\nAn excellent solution to these problems is not to have a default or set the default to a unique value (e.g., a string).\nThen check this value and instantiate it in the __init__ body. If a class parameter has no default and the CLI is\nsubclassed, then a default can be set as follows:\n\ndefault_backbone = {\n\"class_path\": \"import.path.of.MyModel\",\n\"init_args\": {\n\"encoder_layers\": 24,\n},\n}\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": default_backbone})\nA more compact version that avoids writing a dictionary would be:\n\nfrom jsonargparse import lazy_instance\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": lazy_instance(MyModel, encoder_layers=24)})\n----",
    "parent_index": 4,
    "index": 14
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_expert.html",
    "label": "docs",
    "title": "Argument linking",
    "text": "Another case in which it might be desired to extend ~lightning.pytorch.cli.LightningCLI is that the model and\ndata module depends on a common parameter. For example, in some cases, both classes require to know the batch_size.\nIt is a burden and error-prone to give the same value twice in a config file. To avoid this, the parser can be\nconfigured so that a value is only given once and then propagated accordingly. With a tool implemented like the one\nshown below, the batch_size only has to be provided in the data section of the config.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.batch_size\", \"model.batch_size\")\n\ncli = MyLightningCLI(MyModel, MyDataModule)\nThe linking of arguments is observed in the help of the tool, which for this example would look like:\n\n```bash\n$ python trainer.py fit --help\n  ...\n    --data.batch_size BATCH_SIZE\n                          Number of samples in a batch (type: int, default: 8)\n\n  Linked arguments:\n    data.batch_size --> model.batch_size\n                          Number of samples in a batch (type: int)\n```\nSometimes a parameter value is only available after class instantiation. An example could be that your model requires\nthe number of classes to instantiate its fully connected layer (for a classification task). But the value is not\navailable until the data module has been instantiated. The code below illustrates how to address this.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.num_classes\", \"model.num_classes\", apply_on=\"instantiate\")\n\ncli = MyLightningCLI(MyClassModel, MyDataModule)\nInstantiation links are used to automatically determine the order of instantiation, in this case data first.\n\nThe linking of arguments is intended for things that are meant to be non-configurable. This improves the CLI user\nexperience since it avoids the need to provide more parameters. A related concept is a variable interpolation that\nkeeps things configurable.\nThe linking of arguments can be used for more complex cases. For example to derive a value via a function that takes\nmultiple settings as input. For more details have a look at the API of `link_arguments\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentLinking.link_arguments>`_.",
    "parent_index": 4,
    "index": 15
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "What does CLI stand for?",
    "text": "CLI is short for command line interface. This means it is a tool intended to be run from a terminal, similar to commands\nlike git.\n\n----",
    "parent_index": 5,
    "index": 16
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "What is a yaml config file?",
    "text": "A YAML is a standard for configuration files used to describe parameters for sections of a program. It is a common tool\nin engineering and has recently started to gain popularity in machine learning. An example of a YAML file is the\nfollowing:\n\n```yaml\n# file.yaml\ncar:\n    max_speed:100\n    max_passengers:2\nplane:\n    fuel_capacity: 50\nclass_3:\n    option_1: 'x'\n    option_2: 'y'\n```\nIf you are unfamiliar with YAML, the short introduction at `realpython.com#yaml-syntax\n<https://realpython.com/python-yaml/#yaml-syntax>`__ might be a good starting point.\n\n----",
    "parent_index": 5,
    "index": 17
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "What is a subcommand?",
    "text": "A subcommand is what is the action the LightningCLI applies to the script:\n\n```bash\npython main.py [subcommand]\n```\nSee the Potential subcommands with:\n\n```bash\npython main.py --help\n```\nwhich prints:\n\n```bash\n    ...\n\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n```\nuse a subcommand as follows:\n\n```bash\npython main.py fit\npython main.py test\n```\n----",
    "parent_index": 5,
    "index": 18
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "What is the relation between LightningCLI and argparse?",
    "text": "~lightning.pytorch.cli.LightningCLI makes use of jsonargparse_\nwhich is an extension of argparse_. Due to this,\n~lightning.pytorch.cli.LightningCLI follows the same arguments style as many POSIX command line tools. Long\noptions are prefixed with two dashes and its corresponding values are separated by space or an equal sign, as ``--option\nvalue or --option=value``. Command line options are parsed from left to right, therefore if a setting appears\nmultiple times, the value most to the right will override the previous ones.\n\n----",
    "parent_index": 5,
    "index": 19
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "What is the override order of LightningCLI?",
    "text": "The final configuration of CLIs implemented with ~lightning.pytorch.cli.LightningCLI can depend on default\nconfig files (if defined), environment variables (if enabled) and command line arguments. The override order between\nthese is the following:\n\n1. Defaults defined in the source code.\n2. Existing default config files in the order defined in default_config_files, e.g. ~/.myapp.yaml.\n3. Entire config environment variable, e.g. PL_FIT__CONFIG.\n4. Individual argument environment variables, e.g. PL_FIT__SEED_EVERYTHING.\n5. Command line arguments in order left to right (might include config files).\n\n----",
    "parent_index": 5,
    "index": 20
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_faq.html",
    "label": "docs",
    "title": "How do I troubleshoot a CLI?",
    "text": "The standard behavior for CLIs, when they fail, is to terminate the process with a non-zero exit code and a short\nmessage to hint the user about the cause. This is problematic while developing the CLI since there is no information to\ntrack down the root of the problem. To troubleshoot set the environment variable JSONARGPARSE_DEBUG to any value\nbefore running the CLI:\n\n```bash\nexport JSONARGPARSE_DEBUG=true\npython main.py fit\n```\nWhen asking about problems and reporting issues, please set the JSONARGPARSE_DEBUG and include the stack trace\nin your description. With this, users are more likely to help identify the cause without needing to create a\nreproducible script.",
    "parent_index": 5,
    "index": 21
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate.html",
    "label": "docs",
    "title": "LightningCLI requirements",
    "text": "The ~lightning.pytorch.cli.LightningCLI class is designed to significantly ease the implementation of CLIs. To\nuse this class, an additional Python requirement is necessary than the minimal installation of Lightning provides. To\nenable, either install all extras:\n\n```bash\npip install \"lightning[pytorch-extra]\"\n```\nor if only interested in LightningCLI, just install jsonargparse:\n\n```bash\npip install \"jsonargparse[signatures]\"\n```\n----",
    "parent_index": 6,
    "index": 22
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate.html",
    "label": "docs",
    "title": "Implementing a CLI",
    "text": "Implementing a CLI is as simple as instantiating a ~lightning.pytorch.cli.LightningCLI object giving as\narguments classes for a LightningModule and optionally a LightningDataModule:\n\n```python\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\n\n# simple demo classes for your convenience\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\ndef cli_main():\n    cli = LightningCLI(DemoModel, BoringDataModule)\n    # note: don't call fit!!\n\nif __name__ == \"__main__\":\n    cli_main()\n    # note: it is good practice to implement the CLI in a function and call it in the main if block\n```\nNow your model can be managed via the CLI. To see the available commands type:\n\n```bash\n$ python main.py --help\n```\nwhich prints out:\n\n```bash\nusage: main.py [-h] [-c CONFIG] [--print_config [={comments,skip_null,skip_default}+]]\n        {fit,validate,test,predict} ...\n\nLightning Trainer command line tool\n\noptional arguments:\n-h, --help            Show this help message and exit.\n-c CONFIG, --config CONFIG\n                        Path to a configuration file in json or yaml format.\n--print_config [={comments,skip_null,skip_default}+]\n                        Print configuration and exit.\n\nsubcommands:\nFor more details of each subcommand add it as argument followed by --help.\n\n{fit,validate,test,predict}\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n```\nThe message tells us that we have a few available subcommands:\n\n```bash\npython main.py [subcommand]\n```\nwhich you can use depending on your use case:\n\n```bash\n$ python main.py fit\n$ python main.py validate\n$ python main.py test\n$ python main.py predict\n```\n----",
    "parent_index": 6,
    "index": 23
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate.html",
    "label": "docs",
    "title": "Train a model with the CLI",
    "text": "To train a model, use the fit subcommand:\n\n```bash\npython main.py fit\n```\nView all available options with the --help argument given after the subcommand:\n\n```bash\n$ python main.py fit --help\n\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n                            ...\n                            [--ckpt_path CKPT_PATH]\n    --trainer.logger LOGGER\n\noptional arguments:\n<class '__main__.DemoModel'>:\n    --model.out_dim OUT_DIM\n                            (type: int, default: 10)\n    --model.learning_rate LEARNING_RATE\n                            (type: float, default: 0.02)\n<class 'lightning.pytorch.demos.boring_classes.BoringDataModule'>:\n--data CONFIG         Path to a configuration file.\n--data.data_dir DATA_DIR\n                        (type: str, default: ./)\n```\nWith the Lightning CLI enabled, you can now change the parameters without touching your code:\n\n```bash\n# change the learning_rate\npython main.py fit --model.learning_rate 0.1\n\n# change the output dimensions also\npython main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n# change trainer and data arguments too\npython main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n```\nThe options that become available in the CLI are the __init__ parameters of the LightningModule and\nLightningDataModule classes. Thus, to make hyperparameters configurable, just add them to your class's\n__init__. It is highly recommended that these parameters are described in the docstring so that the CLI shows\nthem in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give\nunderstandable error messages when incorrect values are given.",
    "parent_index": 6,
    "index": 24
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Why mix models and datasets",
    "text": "Lightning projects usually begin with one model and one dataset. As the project grows in complexity and you introduce\nmore models and more datasets, it becomes desirable to mix any model with any dataset directly from the command line\nwithout changing your code.\n\n```bash\n# Mix and match anything\n$ python main.py fit --model=GAN --data=MNIST\n$ python main.py fit --model=Transformer --data=MNIST\n```\nLightningCLI makes this very simple. Otherwise, this kind of configuration requires a significant amount of\nboilerplate that often looks like this:\n\n```python\n# choose model\nif args.model == \"gan\":\n    model = GAN(args.feat_dim)\nelif args.model == \"transformer\":\n    model = Transformer(args.feat_dim)\n...\n\n# choose datamodule\nif args.data == \"MNIST\":\n    datamodule = MNIST()\nelif args.data == \"imagenet\":\n    datamodule = Imagenet()\n...\n\n# mix them!\ntrainer.fit(model, datamodule)\n```\nIt is highly recommended that you avoid writing this kind of boilerplate and use LightningCLI instead.\n\n----",
    "parent_index": 7,
    "index": 25
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Multiple LightningModules",
    "text": "To support multiple models, when instantiating LightningCLI omit the model_class parameter:\n\n```python\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass Model1(DemoModel):\n    def configure_optimizers(self):\n        print(\"⚡\", \"using Model1\", \"⚡\")\n        return super().configure_optimizers()\n\nclass Model2(DemoModel):\n    def configure_optimizers(self):\n        print(\"⚡\", \"using Model2\", \"⚡\")\n        return super().configure_optimizers()\n\ncli = LightningCLI(datamodule_class=BoringDataModule)\n```\nNow you can choose between any model from the CLI:\n\n```bash\n# use Model1\npython main.py fit --model Model1\n\n# use Model2\npython main.py fit --model Model2\n```\nInstead of omitting the model_class parameter, you can give a base class and subclass_mode_model=True. This\nwill make the CLI only accept models which are a subclass of the given base class.\n----",
    "parent_index": 7,
    "index": 26
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Multiple LightningDataModules",
    "text": "To support multiple data modules, when instantiating LightningCLI omit the datamodule_class parameter:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass FakeDataset1(BoringDataModule):\n    def train_dataloader(self):\n        print(\"⚡\", \"using FakeDataset1\", \"⚡\")\n        return torch.utils.data.DataLoader(self.random_train)\n\nclass FakeDataset2(BoringDataModule):\n    def train_dataloader(self):\n        print(\"⚡\", \"using FakeDataset2\", \"⚡\")\n        return torch.utils.data.DataLoader(self.random_train)\n\ncli = LightningCLI(DemoModel)\n```\nNow you can choose between any dataset at runtime:\n\n```bash\n# use Model1\npython main.py fit --data FakeDataset1\n\n# use Model2\npython main.py fit --data FakeDataset2\n```\nInstead of omitting the datamodule_class parameter, you can give a base class and subclass_mode_data=True.\nThis will make the CLI only accept data modules that are a subclass of the given base class.\n----",
    "parent_index": 7,
    "index": 27
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Multiple optimizers",
    "text": "Standard optimizers from torch.optim work out of the box:\n\n```bash\npython main.py fit --optimizer AdamW\n```\nIf the optimizer you want needs other arguments, add them via the CLI (no need to change your code)!\n\n```bash\npython main.py fit --optimizer SGD --optimizer.lr=0.01\n```\nFurthermore, any custom subclass of torch.optim.Optimizer can be used as an optimizer:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass LitAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"⚡\", \"using LitAdam\", \"⚡\")\n        super().step(closure)\n\nclass FancyAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"⚡\", \"using FancyAdam\", \"⚡\")\n        super().step(closure)\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\nNow you can choose between any optimizer at runtime:\n\n```bash\n# use LitAdam\npython main.py fit --optimizer LitAdam\n\n# use FancyAdam\npython main.py fit --optimizer FancyAdam\n```\n----",
    "parent_index": 7,
    "index": 28
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Multiple schedulers",
    "text": "Standard learning rate schedulers from torch.optim.lr_scheduler work out of the box:\n\n```bash\npython main.py fit --optimizer=Adam --lr_scheduler CosineAnnealingLR\n```\nPlease note that --optimizer must be added for --lr_scheduler to have an effect.\n\nIf the scheduler you want needs other arguments, add them via the CLI (no need to change your code)!\n\n```bash\npython main.py fit --optimizer=Adam --lr_scheduler=ReduceLROnPlateau --lr_scheduler.monitor=epoch\n```\nFurthermore, any custom subclass of torch.optim.lr_scheduler.LRScheduler can be used as learning rate scheduler:\n\n```python\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\nclass LitLRScheduler(torch.optim.lr_scheduler.CosineAnnealingLR):\n    def step(self):\n        print(\"⚡\", \"using LitLRScheduler\", \"⚡\")\n        super().step()\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\nNow you can choose between any learning rate scheduler at runtime:\n\n```bash\n# LitLRScheduler\npython main.py fit --optimizer=Adam --lr_scheduler LitLRScheduler\n```\n----",
    "parent_index": 7,
    "index": 29
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Classes from any package",
    "text": "In the previous sections, custom classes to select were defined in the same python file where the LightningCLI class\nis run. To select classes from any package by using only the class name, import the respective package:\n\n```python\nfrom lightning.pytorch.cli import LightningCLI\nimport my_code.models  # noqa: F401\nimport my_code.data_modules  # noqa: F401\nimport my_code.optimizers  # noqa: F401\n\ncli = LightningCLI()\n```\nNow use any of the classes:\n\n```bash\npython main.py fit --model Model1 --data FakeDataset1 --optimizer LitAdam --lr_scheduler LitLRScheduler\n```\nThe # noqa: F401 comment avoids a linter warning that the import is unused.\n\nIt is also possible to select subclasses that have not been imported by giving the full import path:\n\n```bash\npython main.py fit --model my_code.models.Model1\n```\n----",
    "parent_index": 7,
    "index": 30
  },
  {
    "file": "docs/pytorch/stable/cli/lightning_cli_intermediate_2.html",
    "label": "docs",
    "title": "Help for specific classes",
    "text": "When multiple models or datasets are accepted, the main help of the CLI does not include their specific parameters. To\nshow this specific help, additional help arguments expect the class name or its import path. For example:\n\n```bash\npython main.py fit --model.help Model1\npython main.py fit --data.help FakeDataset2\npython main.py fit --optimizer.help Adagrad\npython main.py fit --lr_scheduler.help StepLR\n```",
    "parent_index": 7,
    "index": 31
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_advanced.html",
    "label": "docs",
    "title": "Cloud checkpoints",
    "text": "Lightning is integrated with the major remote file systems including local filesystems and several cloud storage providers such as\nS3 on AWS, GCS on Google Cloud,\nor ADL on Azure.\n\nPyTorch Lightning uses fsspec internally to handle all filesystem operations.\n\n----",
    "parent_index": 8,
    "index": 32
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_advanced.html",
    "label": "docs",
    "title": "Save a cloud checkpoint",
    "text": "To save to a remote filesystem, prepend a protocol like \"s3:/\" to the root_dir used for writing and reading model data.\n\n```python\n# `default_root_dir` is the default path used for logs and checkpoints\ntrainer = Trainer(default_root_dir=\"s3://my_bucket/data/\")\ntrainer.fit(model)\n```\n----",
    "parent_index": 8,
    "index": 33
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_advanced.html",
    "label": "docs",
    "title": "Resume training from a cloud checkpoint",
    "text": "To resume training from a cloud checkpoint use a cloud url.\n\n```python\ntrainer = Trainer(default_root_dir=tmpdir, max_steps=3)\ntrainer.fit(model, ckpt_path=\"s3://my_bucket/ckpts/classifier.ckpt\")\n```\nPyTorch Lightning uses fsspec internally to handle all filesystem operations.",
    "parent_index": 8,
    "index": 34
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "What is a checkpoint?",
    "text": "When a model is training, the performance changes as it continues to see more data. It is a best practice to save the state of a model throughout the training process. This gives you a version of the model, *a checkpoint*, at each key point during the development of the model. Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n\nCheckpoints also enable your training to resume from where it was in case the training process is interrupted.\n\nPyTorch Lightning checkpoints are fully usable in plain PyTorch.\n\n----\n\n**Important Update: Deprecated Method**\n\nStarting from PyTorch Lightning v1.0.0, the `resume_from_checkpoint` argument has been deprecated. To resume training from a checkpoint, use the `ckpt_path` argument in the `fit()` method.\nPlease update your code accordingly to avoid potential compatibility issues.",
    "parent_index": 9,
    "index": 35
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Contents of a checkpoint",
    "text": "A Lightning checkpoint contains a dump of the model's entire internal state. Unlike plain PyTorch, Lightning saves *everything* you need to restore a model even in the most complex distributed training environments.\n\nInside a Lightning checkpoint you'll find:\n\n- 16-bit scaling factor (if using 16-bit precision training)\n- Current epoch\n- Global step\n- LightningModule's state_dict\n- State of all optimizers\n- State of all learning rate schedulers\n- State of all callbacks (for stateful callbacks)\n- State of datamodule (for stateful datamodules)\n- The hyperparameters (init arguments) with which the model was created\n- The hyperparameters (init arguments) with which the datamodule was created\n- State of Loops\n\n----",
    "parent_index": 9,
    "index": 36
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Save a checkpoint",
    "text": "Lightning automatically saves a checkpoint for you in your current working directory, with the state of your last training epoch. This makes sure you can resume training in case it was interrupted.\n\n```python\n# simply by using the Trainer you get automatic checkpointing\ntrainer = Trainer()\n```",
    "parent_index": 9,
    "index": 37
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Checkpoint save location",
    "text": "The location where checkpoints are saved depends on whether you have configured a logger:\n\n**Without a logger**, checkpoints are saved to the default_root_dir:\n\n```python\n# saves checkpoints to 'some/path/checkpoints/'\ntrainer = Trainer(default_root_dir=\"some/path/\", logger=False)\n```\n**With a logger**, checkpoints are saved to the logger's directory, **not** to default_root_dir:\n\n```python\nfrom lightning.pytorch.loggers import CSVLogger\n\n# checkpoints will be saved to 'logs/my_experiment/version_0/checkpoints/'\n# NOT to 'some/path/checkpoints/'\ntrainer = Trainer(\n    default_root_dir=\"some/path/\",  # This will be ignored for checkpoints!\n    logger=CSVLogger(\"logs\", \"my_experiment\")\n)\n```\nTo explicitly control the checkpoint location when using a logger, use the\n~lightning.pytorch.callbacks.ModelCheckpoint callback:\n\n```python\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# explicitly set checkpoint directory\ncheckpoint_callback = ModelCheckpoint(dirpath=\"my/custom/checkpoint/path/\")\ntrainer = Trainer(\n    logger=CSVLogger(\"logs\", \"my_experiment\"),\n    callbacks=[checkpoint_callback]\n)\n```\n----",
    "parent_index": 9,
    "index": 38
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "LightningModule from checkpoint",
    "text": "To load a LightningModule along with its weights and hyperparameters use the following method:\n\n```python\nmodel = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n\n# disable randomness, dropout, etc...\nmodel.eval()\n\n# predict with the model\ny_hat = model(x)\n```\n----",
    "parent_index": 9,
    "index": 39
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Save hyperparameters",
    "text": "The LightningModule allows you to automatically save all the hyperparameters passed to *init* simply by calling *self.save_hyperparameters()*.\n\n```python\nclass MyLightningModule(LightningModule):\n    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n```\nThe hyperparameters are saved to the \"hyper_parameters\" key in the checkpoint\n\n```python\ncheckpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\nprint(checkpoint[\"hyper_parameters\"])\n# {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n```\nThe LightningModule also has access to the Hyperparameters\n\n```python\nmodel = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\nprint(model.hparams.learning_rate)\n```\n----",
    "parent_index": 9,
    "index": 40
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Initialize with other parameters",
    "text": "If you used the *self.save_hyperparameters()* method in the *__init__* method of the LightningModule, you can override these and initialize the model with different hyperparameters.\n\n```python\n# if you train and save the model like this it will use these values when loading\n# the weights. But you can overwrite this\nLitModel(in_dim=32, out_dim=10)\n\n# uses in_dim=32, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH)\n\n# uses in_dim=128, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n```\nIn some cases, we may also pass entire PyTorch modules to the __init__ method, which you don't want to save as hyperparameters due to their large size. If you didn't call self.save_hyperparameters() or ignore parameters via save_hyperparameters(ignore=...), then you must pass the missing positional arguments or keyword arguments when calling load_from_checkpoint method:\n\n```python\nclass LitAutoencoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        ...\n\n    ...\n\nmodel = LitAutoEncoder.load_from_checkpoint(PATH, encoder=encoder, decoder=decoder)\n```\n----",
    "parent_index": 9,
    "index": 41
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "nn.Module from checkpoint",
    "text": "Lightning checkpoints are fully compatible with plain torch nn.Modules.\n\n```python\ncheckpoint = torch.load(CKPT_PATH)\nprint(checkpoint.keys())\n```\nFor example, let's pretend we created a LightningModule like so:\n\n```python\nclass Encoder(nn.Module):\n    ...\n\nclass Decoder(nn.Module):\n    ...\n\nclass Autoencoder(L.LightningModule):\n    def __init__(self, encoder, decoder, *args, **kwargs):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\nautoencoder = Autoencoder(Encoder(), Decoder())\n```\nOnce the autoencoder has trained, pull out the relevant weights for your torch nn.Module:\n\n```python\ncheckpoint = torch.load(CKPT_PATH)\nencoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"encoder.\")}\ndecoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"decoder.\")}\n```\n----",
    "parent_index": 9,
    "index": 42
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Disable checkpointing",
    "text": "You can disable checkpointing by passing:\n\ntrainer = Trainer(enable_checkpointing=False)\n----",
    "parent_index": 9,
    "index": 43
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_basic.html",
    "label": "docs",
    "title": "Resume training state",
    "text": "If you don't just want to load weights, but instead restore the full training, do the following:\n\nCorrect usage:\n\n```python\nmodel = LitModel()\ntrainer = Trainer()\n\n# automatically restores model, epoch, step, LR schedulers, etc...\ntrainer.fit(model, ckpt_path=\"path/to/your/checkpoint.ckpt\")\n```\nThe argument `resume_from_checkpoint` has been deprecated in versions of PyTorch Lightning >= 1.0.0.\nTo resume training from a checkpoint, use the `ckpt_path` argument in the `fit()` method instead.\nIncorrect (deprecated) usage:\n\n```python\ntrainer = Trainer(resume_from_checkpoint=\"path/to/your/checkpoint.ckpt\")\ntrainer.fit(model)\n```",
    "parent_index": 9,
    "index": 44
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_expert.html",
    "label": "docs",
    "title": "Save a distributed checkpoint",
    "text": "The distributed checkpoint format can be enabled when you train with the FSDP strategy <../advanced/model_parallel/fsdp>.\n\n```python\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n# 2. Pass the strategy to the Trainer\ntrainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n# 3. Run the trainer\ntrainer.fit(model)\n```\nWith state_dict_type=\"sharded\", each process/GPU will save its own file into a folder at the given path.\nThis reduces memory peaks and speeds up the saving to disk.\n\n.. code-block:: python\n\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import LightningTransformer\n\nmodel = LightningTransformer()\n\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\ntrainer = L.Trainer(\naccelerator=\"cuda\",\ndevices=4,\nstrategy=strategy,\nmax_steps=3,\n)\ntrainer.fit(model)\n\nCheck the contents of the checkpoint folder:\n\n.. code-block:: bash\n\nls -a lightning_logs/version_0/checkpoints/epoch=0-step=3.ckpt/\n\n.. code-block::\n\nepoch=0-step=3.ckpt/\n├── __0_0.distcp\n├── __1_0.distcp\n├── __2_0.distcp\n├── __3_0.distcp\n├── .metadata\n└── meta.pt\n\nThe .distcp files contain the tensor shards from each process/GPU. You can see that the size of these files\nis roughly 1/4 of the total size of the checkpoint since the script distributes the model across 4 GPUs.\n----",
    "parent_index": 10,
    "index": 45
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_expert.html",
    "label": "docs",
    "title": "Load a distributed checkpoint",
    "text": "You can easily load a distributed checkpoint in Trainer if your script uses FSDP <../advanced/model_parallel/fsdp>.\n\n```python\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n# 2. Pass the strategy to the Trainer\ntrainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n# 3. Set the checkpoint path to load\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint\")\n```\nNote that you can load the distributed checkpoint even if the world size has changed, i.e., you are running on a different number of GPUs than when you saved the checkpoint.\n\n.. code-block:: python\n\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import LightningTransformer\n\nmodel = LightningTransformer()\n\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\ntrainer = L.Trainer(\naccelerator=\"cuda\",\ndevices=2,\nstrategy=strategy,\nmax_steps=5,\n)\ntrainer.fit(model, ckpt_path=\"lightning_logs/version_0/checkpoints/epoch=0-step=3.ckpt\")\nIf you want to load a distributed checkpoint into a script that doesn't use FSDP (or Trainer at all), then you will have to convert it to a single-file checkpoint first <Convert dist-checkpoint>.\n----\n\n.. _Convert dist-checkpoint:",
    "parent_index": 10,
    "index": 46
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_expert.html",
    "label": "docs",
    "title": "Convert a distributed checkpoint",
    "text": "It is possible to convert a distributed checkpoint to a regular, single-file checkpoint with this utility:\n\n```bash\npython -m lightning.pytorch.utilities.consolidate_checkpoint path/to/my/checkpoint\n```\nYou will need to do this for example if you want to load the checkpoint into a script that doesn't use FSDP, or need to export the checkpoint to a different format for deployment, evaluation, etc.\n\nAll tensors in the checkpoint will be converted to CPU tensors, and no GPUs are required to run the conversion command.\nThis function assumes you have enough free CPU memory to hold the entire checkpoint in memory.\nAssuming you have saved a checkpoint epoch=0-step=3.ckpt using the examples above, run the following command to convert it:\n\n.. code-block:: bash\n\ncd lightning_logs/version_0/checkpoints\npython -m lightning.pytorch.utilities.consolidate_checkpoint epoch=0-step=3.ckpt\n\nThis saves a new file epoch=0-step=3.ckpt.consolidated next to the sharded checkpoint which you can load normally in PyTorch:\n\n.. code-block:: python\n\nimport torch\n\ncheckpoint = torch.load(\"epoch=0-step=3.ckpt.consolidated\")\nprint(list(checkpoint.keys()))\nprint(checkpoint[\"state_dict\"][\"model.transformer.decoder.layers.31.norm1.weight\"])\n|",
    "parent_index": 10,
    "index": 47
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_migration.html",
    "label": "docs",
    "title": "Resume training from an old checkpoint",
    "text": "Next to the model weights and trainer state, a Lightning checkpoint contains the version number of Lightning with which the checkpoint was saved.\nWhen you load a checkpoint file, either by resuming training\n\n```python\ntrainer = Trainer(...)\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint.ckpt\")\n```\nor by loading the state directly into your model,\n\n```python\nmodel = LitModel.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n```\nLightning will automatically recognize that it is from an older version and migrates the internal structure so it can be loaded properly.\nThis is done without any action required by the user.\n\n----",
    "parent_index": 11,
    "index": 48
  },
  {
    "file": "docs/pytorch/stable/common/checkpointing_migration.html",
    "label": "docs",
    "title": "Upgrade checkpoint files permanently",
    "text": "When Lightning loads a checkpoint, it applies the version migration on-the-fly as explained above, but it does not modify your checkpoint files.\nYou can upgrade checkpoint files permanently with the following command\n\n```\npython -m lightning.pytorch.utilities.upgrade_checkpoint path/to/model.ckpt\n```\nor a folder with multiple files:\n\n```\npython -m lightning.pytorch.utilities.upgrade_checkpoint /path/to/checkpoints/folder\n```",
    "parent_index": 11,
    "index": 49
  },
  {
    "file": "docs/pytorch/stable/common/console_logs.html",
    "label": "docs",
    "title": "Enable console logs",
    "text": "Lightning logs useful information about the training process and user warnings to the console.\nYou can retrieve the Lightning console logger and change it to your liking. For example, adjust the logging level\nor redirect output for certain modules to log files:\n\nimport logging\n\n# configure logging at the root level of Lightning\nlogging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n# configure logging on module level, redirect to file\nlogger = logging.getLogger(\"lightning.pytorch.core\")\nlogger.addHandler(logging.FileHandler(\"core.log\"))\nRead more about custom Python logging here.",
    "parent_index": 12,
    "index": 50
  },
  {
    "file": "docs/pytorch/stable/common/early_stopping.html",
    "label": "docs",
    "title": "Stopping an Epoch Early",
    "text": "You can stop and skip the rest of the current epoch early by overriding ~lightning.pytorch.core.hooks.ModelHooks.on_train_batch_start to return -1 when some condition is met.\n\nIf you do this repeatedly, for every epoch you had originally requested, then this will stop your entire training.",
    "parent_index": 13,
    "index": 51
  },
  {
    "file": "docs/pytorch/stable/common/early_stopping.html",
    "label": "docs",
    "title": "EarlyStopping Callback",
    "text": "The ~lightning.pytorch.callbacks.early_stopping.EarlyStopping callback can be used to monitor a metric and stop the training when no improvement is observed.\n\nTo enable it:\n\n- Import ~lightning.pytorch.callbacks.early_stopping.EarlyStopping callback.\n- Log the metric you want to monitor using ~lightning.pytorch.core.LightningModule.log method.\n- Init the callback, and set monitor to the logged metric of your choice.\n- Set the mode based on the metric needs to be monitored.\n- Pass the ~lightning.pytorch.callbacks.early_stopping.EarlyStopping callback to the ~lightning.pytorch.trainer.trainer.Trainer callbacks flag.\n\n```python\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n```\nYou can customize the callbacks behaviour by changing its parameters.\n\nearly_stop_callback = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")\ntrainer = Trainer(callbacks=[early_stop_callback])\nAdditional parameters that stop training at extreme points:\n\n- stopping_threshold: Stops training immediately once the monitored quantity reaches this threshold.\nIt is useful when we know that going beyond a certain optimal value does not further benefit us.\n- divergence_threshold: Stops training as soon as the monitored quantity becomes worse than this threshold.\nWhen reaching a value this bad, we believes the model cannot recover anymore and it is better to stop early and run with different initial conditions.\n- check_finite: When turned on, it stops training if the monitored metric becomes NaN or infinite.\n- check_on_train_epoch_end: When turned on, it checks the metric at the end of a training epoch. Use this only when you are monitoring any metric logged within\ntraining-specific hooks on epoch-level.\n\nAfter training completes, you can programmatically check why early stopping occurred using the stopping_reason\nattribute, which returns an EarlyStoppingReason enum value.\n\n```python\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStoppingReason\n\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\ntrainer = Trainer(callbacks=[early_stopping])\ntrainer.fit(model)\n\n# Check why training stopped\nif early_stopping.stopping_reason == EarlyStoppingReason.PATIENCE_EXHAUSTED:\n    print(\"Training stopped due to patience exhaustion\")\nelif early_stopping.stopping_reason == EarlyStoppingReason.STOPPING_THRESHOLD:\n    print(\"Training stopped due to reaching stopping threshold\")\nelif early_stopping.stopping_reason == EarlyStoppingReason.NOT_STOPPED:\n    print(\"Training completed normally without early stopping\")\n\n# Access human-readable message\nif early_stopping.stopping_reason_message:\n    print(f\"Details: {early_stopping.stopping_reason_message}\")\n```\nThe available stopping reasons are:\n\n- NOT_STOPPED: Training completed normally without early stopping\n- STOPPING_THRESHOLD: Training stopped because the monitored metric reached the stopping threshold\n- DIVERGENCE_THRESHOLD: Training stopped because the monitored metric exceeded the divergence threshold\n- PATIENCE_EXHAUSTED: Training stopped because the metric didn't improve for the specified patience\n- NON_FINITE_METRIC: Training stopped because the monitored metric became NaN or infinite\n\nIn case you need early stopping in a different part of training, subclass ~lightning.pytorch.callbacks.early_stopping.EarlyStopping\nand change where it is called:\n\nclass MyEarlyStopping(EarlyStopping):\ndef on_validation_end(self, trainer, pl_module):\n# override this to disable early stopping at the end of val loop\npass\n\ndef on_train_end(self, trainer, pl_module):\n# instead, do it at the end of training loop\nself._run_early_stopping_check(trainer)\nThe ~lightning.pytorch.callbacks.early_stopping.EarlyStopping callback runs\nat the end of every validation epoch by default. However, the frequency of validation\ncan be modified by setting various parameters in the ~lightning.pytorch.trainer.trainer.Trainer,\nfor example ~lightning.pytorch.trainer.trainer.Trainer.check_val_every_n_epoch\nand ~lightning.pytorch.trainer.trainer.Trainer.val_check_interval.\nIt must be noted that the patience parameter counts the number of\nvalidation checks with no improvement, and not the number of training epochs.\nTherefore, with parameters check_val_every_n_epoch=10 and patience=3, the trainer\nwill perform at least 40 training epochs before being stopped.",
    "parent_index": 13,
    "index": 52
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Add a test loop",
    "text": "To make sure a model can generalize to an unseen dataset (ie: to publish a paper or in a production environment) a dataset is normally split into two parts, the *train* split and the *test* split.\n\nThe test set is **NOT** used during training, it is **ONLY** used once the model has been trained to see how the model will do in the real-world.\n\n----",
    "parent_index": 14,
    "index": 53
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Find the train and test splits",
    "text": "Datasets come with two splits. Refer to the dataset documentation to find the *train* and *test* splits.\n\n```python\nimport torch.utils.data as data\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Load data sets\ntransform = transforms.ToTensor()\ntrain_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\ntest_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n```\n----",
    "parent_index": 14,
    "index": 54
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Define the test loop",
    "text": "To add a test loop, implement the **test_step** method of the LightningModule\n\n```python\nclass LitAutoEncoder(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        ...\n\n    def test_step(self, batch, batch_idx):\n        # this is the test loop\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        test_loss = F.mse_loss(x_hat, x)\n        self.log(\"test_loss\", test_loss)\n```\n----",
    "parent_index": 14,
    "index": 55
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Train with the test loop",
    "text": "Once the model has finished training, call **.test**\n\n```python\nfrom torch.utils.data import DataLoader\n\n# initialize the Trainer\ntrainer = Trainer()\n\n# test the model\ntrainer.test(model, dataloaders=DataLoader(test_set))\n```\n----",
    "parent_index": 14,
    "index": 56
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Add a validation loop",
    "text": "During training, it's common practice to use a small portion of the train split to determine when the model has finished training.\n\n----",
    "parent_index": 14,
    "index": 57
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Split the training data",
    "text": "As a rule of thumb, we use 20% of the training set as the **validation set**. This number varies from dataset to dataset.\n\n```python\n# use 20% of training data for validation\ntrain_set_size = int(len(train_set) * 0.8)\nvalid_set_size = len(train_set) - train_set_size\n\n# split the train set into two\nseed = torch.Generator().manual_seed(42)\ntrain_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n```\n----",
    "parent_index": 14,
    "index": 58
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Define the validation loop",
    "text": "To add a validation loop, implement the **validation_step** method of the LightningModule\n\n```python\nclass LitAutoEncoder(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        ...\n\n    def validation_step(self, batch, batch_idx):\n        # this is the validation loop\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        val_loss = F.mse_loss(x_hat, x)\n        self.log(\"val_loss\", val_loss)\n```\n----",
    "parent_index": 14,
    "index": 59
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_basic.html",
    "label": "docs",
    "title": "Train with the validation loop",
    "text": "To run the validation loop, pass in the validation set to **.fit**\n\n```python\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set)\nvalid_loader = DataLoader(valid_set)\nmodel = LitAutoEncoder(...)\n\n# train with both splits\ntrainer = L.Trainer()\ntrainer.fit(model, train_loader, valid_loader)\n```",
    "parent_index": 14,
    "index": 60
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Testing",
    "text": "Lightning allows the user to test their models with any compatible test dataloaders. This can be done before/after training\nand is completely agnostic to ~lightning.pytorch.trainer.trainer.Trainer.fit call. The logic used here is defined under\n~lightning.pytorch.core.LightningModule.test_step.\n\nTesting is performed using the Trainer object's .test() method.",
    "parent_index": 15,
    "index": 61
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Test after Fit",
    "text": "To run the test set after training completes, use this method.\n\n```python\n# run full training\ntrainer.fit(model)\n\n# (1) load the best checkpoint automatically (lightning tracks this for you during .fit())\ntrainer.test(ckpt_path=\"best\")\n\n# (2) load the last available checkpoint (only works if `ModelCheckpoint(save_last=True)`)\ntrainer.test(ckpt_path=\"last\")\n\n# (3) test using a specific checkpoint\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\n\n# (4) test with an explicit model (will use this model and not load a checkpoint)\ntrainer.test(model)\n```\nIt is recommended to test with Trainer(devices=1) since distributed strategies such as DDP\nuse ~torch.utils.data.distributed.DistributedSampler internally, which replicates some samples to\nmake sure all devices have same batch size in case of uneven inputs. This is helpful to make sure\nbenchmarking for research papers is done the right way.",
    "parent_index": 15,
    "index": 62
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Test Multiple Models",
    "text": "You can run the test set on multiple models using the same trainer instance.\n\n```python\nmodel1 = LitModel()\nmodel2 = GANModel()\n\ntrainer = Trainer()\ntrainer.test(model1)\ntrainer.test(model2)\n```",
    "parent_index": 15,
    "index": 63
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Test Pre-Trained Model",
    "text": "To run the test set on a pre-trained model, use this method.\n\n```python\nmodel = MyLightningModule.load_from_checkpoint(\n    checkpoint_path=\"/path/to/pytorch_checkpoint.ckpt\",\n    hparams_file=\"/path/to/experiment/version/hparams.yaml\",\n    map_location=None,\n)\n\n# init trainer with whatever options\ntrainer = Trainer(...)\n\n# test (pass in the model)\ntrainer.test(model)\n```\nIn this case, the options you pass to trainer will be used when\nrunning the test set (ie: 16-bit, dp, ddp, etc...)",
    "parent_index": 15,
    "index": 64
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Test with Additional DataLoaders",
    "text": "You can still run inference on a test dataset even if the ~lightning.pytorch.core.hooks.DataHooks.test_dataloader method hasn't been\ndefined within your lightning module <../common/lightning_module> instance. This would be the case when your test data\nis not available at the time your model was declared.\n\n```python\n# setup your data loader\ntest_dataloader = DataLoader(...)\n\n# test (pass in the loader)\ntrainer.test(dataloaders=test_dataloader)\n```\nYou can either pass in a single dataloader or a list of them. This optional named\nparameter can be used in conjunction with any of the above use cases. Additionally,\nyou can also pass in an datamodules <../data/datamodule> that have overridden the\ndatamodule_test_dataloader_label method.\n\n```python\nclass MyDataModule(L.LightningDataModule):\n    ...\n\n    def test_dataloader(self):\n        return DataLoader(...)\n\n# setup your datamodule\ndm = MyDataModule(...)\n\n# test (pass in datamodule)\ntrainer.test(datamodule=dm)\n```",
    "parent_index": 15,
    "index": 65
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Test with Multiple DataLoaders",
    "text": "When you need to evaluate your model on multiple test datasets simultaneously (e.g., different domains, conditions, or\nevaluation scenarios), PyTorch Lightning supports multiple test dataloaders out of the box.\n\nTo use multiple test dataloaders, simply return a list of dataloaders from your test_dataloader() method:\n\n```python\nclass LitModel(L.LightningModule):\n    def test_dataloader(self):\n        return [\n            DataLoader(clean_test_dataset, batch_size=32),\n            DataLoader(noisy_test_dataset, batch_size=32),\n            DataLoader(adversarial_test_dataset, batch_size=32),\n        ]\n```\nWhen using multiple test dataloaders, your test_step method **must** include a dataloader_idx parameter:\n\n```python\ndef test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n\n    # Use dataloader_idx to handle different test scenarios\n    return {'test_loss': loss}\n```",
    "parent_index": 15,
    "index": 66
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Logging Metrics Per Dataloader",
    "text": "Lightning provides automatic support for logging metrics per dataloader:\n\n```python\ndef test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n    x, y = batch\n    y_hat = self(x)\n    loss = F.cross_entropy(y_hat, y)\n    acc = (y_hat.argmax(dim=1) == y).float().mean()\n\n    # Lightning automatically adds \"/dataloader_idx_X\" suffix\n    self.log('test_loss', loss, add_dataloader_idx=True)\n    self.log('test_acc', acc, add_dataloader_idx=True)\n\n    return loss\n```\nThis will create metrics like test_loss/dataloader_idx_0, test_loss/dataloader_idx_1, etc.\n\nFor more meaningful metric names, you can use custom naming where you need to make sure that individual names are\nunique across dataloaders.\n\n```python\ndef test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n    # Define meaningful names for each dataloader\n    dataloader_names = {0: \"clean\", 1: \"noisy\", 2: \"adversarial\"}\n    dataset_name = dataloader_names.get(dataloader_idx, f\"dataset_{dataloader_idx}\")\n\n    # Log with custom names\n    self.log(f'test_loss_{dataset_name}', loss, add_dataloader_idx=False)\n    self.log(f'test_acc_{dataset_name}', acc, add_dataloader_idx=False)\n```",
    "parent_index": 15,
    "index": 67
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Processing Entire Datasets Per Dataloader",
    "text": "To perform calculations on the entire test dataset for each dataloader (e.g., computing overall metrics, creating\nvisualizations), accumulate results during test_step and process them in on_test_epoch_end:\n\n```python\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Store outputs per dataloader\n        self.test_outputs = {}\n\n    def test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n\n        # Initialize and store results\n        if dataloader_idx not in self.test_outputs:\n            self.test_outputs[dataloader_idx] = {'predictions': [], 'targets': []}\n        self.test_outputs[dataloader_idx]['predictions'].append(y_hat)\n        self.test_outputs[dataloader_idx]['targets'].append(y)\n        return loss\n\n    def on_test_epoch_end(self):\n        for dataloader_idx, outputs in self.test_outputs.items():\n            # Concatenate all predictions and targets for this dataloader\n            all_predictions = torch.cat(outputs['predictions'], dim=0)\n            all_targets = torch.cat(outputs['targets'], dim=0)\n\n            # Calculate metrics on the entire dataset, log and create visualizations\n            overall_accuracy = (all_predictions.argmax(dim=1) == all_targets).float().mean()\n            self.log(f'test_overall_acc_dataloader_{dataloader_idx}', overall_accuracy)\n            self._save_results(all_predictions, all_targets, dataloader_idx)\n\n        self.test_outputs.clear()\n```\nWhen using multiple test dataloaders, trainer.test() returns a list of results, one for each dataloader:\n\n.. code-block:: python\n\nresults = trainer.test(model)\nprint(f\"Results from {len(results)} test dataloaders:\")\nfor i, result in enumerate(results):\nprint(f\"Dataloader {i}: {result}\")\n----------",
    "parent_index": 15,
    "index": 68
  },
  {
    "file": "docs/pytorch/stable/common/evaluation_intermediate.html",
    "label": "docs",
    "title": "Validation",
    "text": "Lightning allows the user to validate their models with any compatible val dataloaders. This can be done before/after training.\nThe logic associated to the validation is defined within the ~lightning.pytorch.core.LightningModule.validation_step.\n\nApart from this .validate has same API as .test, but would rely respectively on ~lightning.pytorch.core.LightningModule.validation_step and ~lightning.pytorch.core.LightningModule.test_step.\n\n.validate method uses the same validation logic being used under validation happening within\n~lightning.pytorch.trainer.trainer.Trainer.fit call.\nWhen using trainer.validate(), it is recommended to use Trainer(devices=1) since distributed strategies such as DDP\nuses ~torch.utils.data.distributed.DistributedSampler internally, which replicates some samples to\nmake sure all devices have same batch size in case of uneven inputs. This is helpful to make sure\nbenchmarking for research papers is done the right way.",
    "parent_index": 15,
    "index": 69
  },
  {
    "file": "docs/pytorch/stable/common/hooks.html",
    "label": "docs",
    "title": "Training Loop Hook Order",
    "text": "The following diagram shows the execution order of hooks during a typical training loop e.g. calling `trainer.fit()`,\nwith the source of each hook indicated:\n\n```text\nTraining Process Flow:\n\ntrainer.fit()\n│\n├── setup(stage=\"fit\")\n│   ├── [LightningDataModule]\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   ├── [LightningModule.configure_shared_model()]\n│   ├── [LightningModule.configure_model()]\n│   ├── Strategy.restore_checkpoint_before_setup\n│   │   ├── [LightningModule.on_load_checkpoint()]\n│   │   ├── [LightningModule.load_state_dict()]\n│   │   ├── [LightningDataModule.load_state_dict()]\n│   │   ├── [Callbacks.on_load_checkpoint()]\n│   │   └── [Callbacks.load_state_dict()]\n│   └── [Strategy]\n│\n├── on_fit_start()\n│   ├── [Callbacks]\n│   └── [LightningModule]\n│\n├── Strategy.restore_checkpoint_after_setup\n│   ├── [LightningModule.on_load_checkpoint()]\n│   ├── [LightningModule.load_state_dict()]\n│   ├── [LightningDataModule.load_state_dict()]\n│   ├── [Callbacks.on_load_checkpoint()]\n│   └── [Callbacks.load_state_dict()]\n│\n├── on_sanity_check_start()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│   ├── on_validation_start()\n│   │   ├── [Callbacks]\n│   │   ├── [LightningModule]\n│   │   └── [Strategy]\n│   ├── on_validation_epoch_start()\n│   │   ├── [Callbacks]\n│   │   ├── [LightningModule]\n│   │   └── [Strategy]\n│   │   ├── [for each validation batch]\n│   │   │   ├── on_validation_batch_start()\n│   │   │   │   ├── [Callbacks]\n│   │   │   │   ├── [LightningModule]\n│   │   │   │   └── [Strategy]\n│   │   │   └── on_validation_batch_end()\n│   │   │       ├── [Callbacks]\n│   │   │       ├── [LightningModule]\n│   │   │       └── [Strategy]\n│   │   └── [end validation batches]\n│   ├── on_validation_epoch_end()\n│   │   ├── [Callbacks]\n│   │   ├── [LightningModule]\n│   │   └── [Strategy]\n│   └── on_validation_end()\n│       ├── [Callbacks]\n│       ├── [LightningModule]\n│       └── [Strategy]\n├── on_sanity_check_end()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│\n├── on_train_start()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│\n├── [Training Epochs Loop]\n│   │\n│   ├── on_train_epoch_start()\n│   │   ├── [Callbacks]\n│   │   └── [LightningModule]\n│   │\n│   ├── [Training Batches Loop]\n│   │   │\n│   │   ├── on_train_batch_start()\n│   │   │   ├── [Callbacks]\n│   │   │   ├── [LightningModule]\n│   │   │   └── [Strategy]\n│   │   │\n│   │   ├── [Forward Pass - training_step()]\n│   │   │   └── [Strategy only]\n│   │   │\n│   │   ├── on_before_zero_grad()\n│   │   │   ├── [Callbacks]\n│   │   │   └── [LightningModule]\n│   │   │\n│   │   ├── optimizer_zero_grad()\n│   │   │   └── [LightningModule only - optimizer_zero_grad()]\n│   │   │\n│   │   ├── [Backward Pass - Strategy.backward()]\n│   │   │   ├── on_before_backward()\n│   │   │   │   ├── [Callbacks]\n│   │   │   │   └── [LightningModule]\n│   │   │   ├── LightningModule.backward()\n│   │   │   └── on_after_backward()\n│   │   │       ├── [Callbacks]\n│   │   │       └── [LightningModule]\n│   │   │\n│   │   ├── on_before_optimizer_step()\n│   │   │   ├── [Callbacks]\n│   │   │   └── [LightningModule]\n│   │   │\n│   │   ├── [Optimizer Step]\n│   │   │   └── [LightningModule only - optimizer_step()]\n│   │   │\n│   │   └── on_train_batch_end()\n│   │       ├── [Callbacks]\n│   │       └── [LightningModule]\n│   │\n│   │   [Optional: Validation during training]\n│   │   ├── on_validation_start()\n│   │   │   ├── [Callbacks]\n│   │   │   ├── [LightningModule]\n│   │   │   └── [Strategy]\n│   │   ├── on_validation_epoch_start()\n│   │   │   ├── [Callbacks]\n│   │   │   ├── [LightningModule]\n│   │   │   └── [Strategy]\n│   │   │   ├── [for each validation batch]\n│   │   │   │   ├── on_validation_batch_start()\n│   │   │   │   │   ├── [Callbacks]\n│   │   │   │   │   ├── [LightningModule]\n│   │   │   │   │   └── [Strategy]\n│   │   │   │   └── on_validation_batch_end()\n│   │   │   │       ├── [Callbacks]\n│   │   │   │       ├── [LightningModule]\n│   │   │   │       └── [Strategy]\n│   │   │   └── [end validation batches]\n│   │   ├── on_validation_epoch_end()\n│   │   │   ├── [Callbacks]\n│   │   │   ├── [LightningModule]\n│   │   │   └── [Strategy]\n│   │   └── on_validation_end()\n│   │       ├── [Callbacks]\n│   │       ├── [LightningModule]\n│   │       └── [Strategy]\n│   │\n│   └── on_train_epoch_end() **SPECIAL CASE**\n│       ├── [Callbacks - Non-monitoring only]\n│       ├── [LightningModule]\n│       └── [Callbacks - Monitoring only]\n│\n├── [End Training Epochs]\n│\n├── on_train_end()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│\n└── teardown(stage=\"fit\")\n    ├── [Strategy]\n    ├── on_fit_end()\n    │   ├── [Callbacks]\n    │   └── [LightningModule]\n    ├── [LightningDataModule]\n    ├── [Callbacks]\n    └── [LightningModule]\n```",
    "parent_index": 16,
    "index": 70
  },
  {
    "file": "docs/pytorch/stable/common/hooks.html",
    "label": "docs",
    "title": "Testing Loop Hook Order",
    "text": "When running tests with trainer.test():\n\n```text\ntrainer.test()\n│\n├── setup(stage=\"test\")\n│   └── [Callbacks only]\n├── on_test_start()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│\n├── [Test Epochs Loop]\n│   │\n│   ├── on_test_epoch_start()\n│   │   ├── [Callbacks]\n│   │   ├── [LightningModule]\n│   │   └── [Strategy]\n│   │\n│   ├── [Test Batches Loop]\n│   │   │\n│   │   ├── on_test_batch_start()\n│   │   │   ├── [Callbacks]\n│   │   │   ├── [LightningModule]\n│   │   │   └── [Strategy]\n│   │   │\n│   │   └── on_test_batch_end()\n│   │       ├── [Callbacks]\n│   │       ├── [LightningModule]\n│   │       └── [Strategy]\n│   │\n│   └── on_test_epoch_end()\n│       ├── [Callbacks]\n│       ├── [LightningModule]\n│       └── [Strategy]\n│\n├── on_test_end()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n└── teardown(stage=\"test\")\n    └── [Callbacks only]\n```",
    "parent_index": 16,
    "index": 71
  },
  {
    "file": "docs/pytorch/stable/common/hooks.html",
    "label": "docs",
    "title": "Prediction Loop Hook Order",
    "text": "When running predictions with trainer.predict():\n\n```text\ntrainer.predict()\n│\n├── setup(stage=\"predict\")\n│   └── [Callbacks only]\n├── on_predict_start()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n│\n├── [Prediction Epochs Loop]\n│   │\n│   ├── on_predict_epoch_start()\n│   │   ├── [Callbacks]\n│   │   └── [LightningModule]\n│   │\n│   ├── [Prediction Batches Loop]\n│   │   │\n│   │   ├── on_predict_batch_start()\n│   │   │   ├── [Callbacks]\n│   │   │   └── [LightningModule]\n│   │   │\n│   │   └── on_predict_batch_end()\n│   │       ├── [Callbacks]\n│   │       └── [LightningModule]\n│   │\n│   └── on_predict_epoch_end()\n│       ├── [Callbacks]\n│       └── [LightningModule]\n│\n├── on_predict_end()\n│   ├── [Callbacks]\n│   ├── [LightningModule]\n│   └── [Strategy]\n└── teardown(stage=\"predict\")\n    └── [Callbacks only]\n```",
    "parent_index": 16,
    "index": 72
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Starter Example",
    "text": "Here are the only required methods.\n\n```python\nimport lightning as L\nimport torch\n\nfrom lightning.pytorch.demos import Transformer\n\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, inputs, target):\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n```\nWhich you can train by doing:\n\n```python\nfrom lightning.pytorch.demos import WikiText2\nfrom torch.utils.data import DataLoader\n\ndataset = WikiText2()\ndataloader = DataLoader(dataset)\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\n\ntrainer = L.Trainer(fast_dev_run=100)\ntrainer.fit(model=model, train_dataloaders=dataloader)\n```\nThe LightningModule has many convenient methods, but the core ones you need to know about are:\n\n* - Name\n- Description\n* - __init__ and ~lightning.pytorch.core.hooks.ModelHooks.setup\n- Define initialization here\n* - ~lightning.pytorch.core.LightningModule.forward\n- To run data through your model only (separate from training_step)\n* - ~lightning.pytorch.core.LightningModule.training_step\n- the complete training step\n* - ~lightning.pytorch.core.LightningModule.validation_step\n- the complete validation step\n* - ~lightning.pytorch.core.LightningModule.test_step\n- the complete test step\n* - ~lightning.pytorch.core.LightningModule.predict_step\n- the complete prediction step\n* - ~lightning.pytorch.core.LightningModule.configure_optimizers\n- define optimizers and LR schedulers\n----------",
    "parent_index": 18,
    "index": 73
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Training Loop",
    "text": "To activate the training loop, override the ~lightning.pytorch.core.LightningModule.training_step method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# enable gradient calculation\ntorch.set_grad_enabled(True)\n\nfor batch_idx, batch in enumerate(train_dataloader):\n    loss = training_step(batch, batch_idx)\n\n    # clear gradients\n    optimizer.zero_grad()\n\n    # backward\n    loss.backward()\n\n    # update parameters\n    optimizer.step()\n```",
    "parent_index": 18,
    "index": 74
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Train Epoch-level Metrics",
    "text": "If you want to calculate epoch-level metrics and log them, use ~lightning.pytorch.core.LightningModule.log.\n\n```python\ndef training_step(self, batch, batch_idx):\n    inputs, target = batch\n    output = self.model(inputs, target)\n    loss = torch.nn.functional.nll_loss(output, target.view(-1))\n\n    # logs metrics for each training_step,\n    # and the average across the epoch, to the progress bar and logger\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n```\nThe ~lightning.pytorch.core.LightningModule.log method automatically reduces the\nrequested metrics across a complete epoch and devices. Here's the pseudocode of what it does under the hood:\n\n```python\nouts = []\nfor batch_idx, batch in enumerate(train_dataloader):\n    # forward\n    loss = training_step(batch, batch_idx)\n    outs.append(loss.detach())\n\n    # clear gradients\n    optimizer.zero_grad()\n    # backward\n    loss.backward()\n    # update parameters\n    optimizer.step()\n\n# note: in reality, we do this incrementally, instead of keeping all outputs in memory\nepoch_metric = torch.mean(torch.stack(outs))\n```",
    "parent_index": 18,
    "index": 75
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Train Epoch-level Operations",
    "text": "In the case that you need to make use of all the outputs from each ~lightning.pytorch.LightningModule.training_step,\noverride the ~lightning.pytorch.LightningModule.on_train_epoch_end method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.training_step_outputs = []\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        preds = ...\n        self.training_step_outputs.append(preds)\n        return loss\n\n    def on_train_epoch_end(self):\n        all_preds = torch.stack(self.training_step_outputs)\n        # do something with all preds\n        ...\n        self.training_step_outputs.clear()  # free memory\n```\n------------------",
    "parent_index": 18,
    "index": 76
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Validation Loop",
    "text": "To activate the validation loop while training, override the ~lightning.pytorch.core.LightningModule.validation_step method.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def validation_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"val_loss\", loss)\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# ...\nfor batch_idx, batch in enumerate(train_dataloader):\n    loss = model.training_step(batch, batch_idx)\n    loss.backward()\n    # ...\n\n    if validate_at_some_point:\n        # disable grads + batchnorm + dropout\n        torch.set_grad_enabled(False)\n        model.eval()\n\n        # ----------------- VAL LOOP ---------------\n        for val_batch_idx, val_batch in enumerate(val_dataloader):\n            val_out = model.validation_step(val_batch, val_batch_idx)\n        # ----------------- VAL LOOP ---------------\n\n        # enable grads + batchnorm + dropout\n        torch.set_grad_enabled(True)\n        model.train()\n```\nYou can also run just the validation loop on your validation dataloaders by overriding ~lightning.pytorch.core.LightningModule.validation_step\nand calling ~lightning.pytorch.trainer.trainer.Trainer.validate.\n\n```python\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ntrainer = L.Trainer()\ntrainer.validate(model)\n```\nIt is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\nThis is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\nmulti-device setting, samples could occur duplicated when ~torch.utils.data.distributed.DistributedSampler\nis used, for eg. with strategy=\"ddp\". It replicates some samples on some devices to make sure all devices have\nsame batch size in case of uneven inputs.",
    "parent_index": 18,
    "index": 77
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Validation Epoch-level Metrics",
    "text": "In the case that you need to make use of all the outputs from each ~lightning.pytorch.LightningModule.validation_step,\noverride the ~lightning.pytorch.LightningModule.on_validation_epoch_end method.\nNote that this method is called before ~lightning.pytorch.LightningModule.on_train_epoch_end.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.validation_step_outputs = []\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        pred = ...\n        self.validation_step_outputs.append(pred)\n        return pred\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.stack(self.validation_step_outputs)\n        # do something with all preds\n        ...\n        self.validation_step_outputs.clear()  # free memory\n```\n----------------",
    "parent_index": 18,
    "index": 78
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Test Loop",
    "text": "The process for enabling a test loop is the same as the process for enabling a validation loop. Please refer to\nthe section above for details. For this you need to override the ~lightning.pytorch.core.LightningModule.test_step method.\n\nThe only difference is that the test loop is only called when ~lightning.pytorch.trainer.trainer.Trainer.test is used.\n\n```python\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ndataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically loads the best weights for you\ntrainer.test(model)\n```\nThere are two ways to call test():\n\n```python\n# call after training\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically auto-loads the best weights from the previous run\ntrainer.test(dataloaders=test_dataloaders)\n\n# or call with pretrained model\nmodel = LightningTransformer.load_from_checkpoint(PATH)\ndataset = WikiText2()\ntest_dataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.test(model, dataloaders=test_dataloader)\n```\n`WikiText2` is used in a manner that does not create a train, test, val split. This is done for illustrative purposes only.\nA proper split can be created in lightning.pytorch.core.LightningModule.setup or lightning.pytorch.core.LightningDataModule.setup.\nIt is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\nThis is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\nmulti-device setting, samples could occur duplicated when ~torch.utils.data.distributed.DistributedSampler\nis used, for eg. with strategy=\"ddp\". It replicates some samples on some devices to make sure all devices have\nsame batch size in case of uneven inputs.\n----------",
    "parent_index": 18,
    "index": 79
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Prediction Loop",
    "text": "By default, the ~lightning.pytorch.core.LightningModule.predict_step method runs the\n~lightning.pytorch.core.LightningModule.forward method. In order to customize this behaviour,\nsimply override the ~lightning.pytorch.core.LightningModule.predict_step method.\n\nFor the example let's override predict_step:\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def predict_step(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n```\nUnder the hood, Lightning does the following (pseudocode):\n\n```python\n# disable grads + batchnorm + dropout\ntorch.set_grad_enabled(False)\nmodel.eval()\nall_preds = []\n\nfor batch_idx, batch in enumerate(predict_dataloader):\n    pred = model.predict_step(batch, batch_idx)\n    all_preds.append(pred)\n```\nThere are two ways to call predict():\n\n```python\n# call after training\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically auto-loads the best weights from the previous run\npredictions = trainer.predict(dataloaders=predict_dataloader)\n\n# or call with pretrained model\nmodel = LightningTransformer.load_from_checkpoint(PATH)\ndataset = WikiText2()\ntest_dataloader = DataLoader(dataset)\ntrainer = L.Trainer()\npredictions = trainer.predict(model, dataloaders=test_dataloader)\n```",
    "parent_index": 18,
    "index": 80
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Inference in Research",
    "text": "If you want to perform inference with the system, you can add a forward method to the LightningModule.\n\n```python\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\n\nmodel.eval()\nwith torch.no_grad():\n    batch = dataloader.dataset[0]\n    pred = model(batch)\n```\nThe advantage of adding a forward is that in complex systems, you can do a much more involved inference procedure,\nsuch as text generation:\n\n```python\nclass Seq2Seq(L.LightningModule):\n    def forward(self, x):\n        embeddings = self(x)\n        hidden_states = self.encoder(embeddings)\n        for h in hidden_states:\n            # decode\n            ...\n        return decoded\n```\nIn the case where you want to scale your inference, you should be using\n~lightning.pytorch.core.LightningModule.predict_step.\n\n```python\nclass Autoencoder(L.LightningModule):\n    def forward(self, x):\n        return self.decoder(x)\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        # this calls forward\n        return self(batch)\n\ndata_module = ...\nmodel = Autoencoder()\ntrainer = Trainer(accelerator=\"gpu\", devices=2)\ntrainer.predict(model, data_module)\n```",
    "parent_index": 18,
    "index": 81
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Inference in Production",
    "text": "For cases like production, you might want to iterate different models inside a LightningModule.\n\n```python\nfrom torchmetrics.functional import accuracy\n\nclass ClassificationTask(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def test_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        return loss, acc\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch\n        y_hat = self.model(x)\n        return y_hat\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=0.02)\n```\nThen pass in any arbitrary model to be fit with this task\n\n```python\nfor model in [resnet50(), vgg16(), BidirectionalRNN()]:\n    task = ClassificationTask(model)\n\n    trainer = Trainer(accelerator=\"gpu\", devices=2)\n    trainer.fit(task, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n```\nTasks can be arbitrarily complex such as implementing GAN training, self-supervised or even RL.\n\n```python\nclass GANTask(L.LightningModule):\n    def __init__(self, generator, discriminator):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n\n    ...\n```\nWhen used like this, the model can be separated from the Task and thus used in production without needing to keep it in\na LightningModule.\n\nThe following example shows how you can run inference in the Python runtime:\n\n```python\ntask = ClassificationTask(model)\ntrainer = Trainer(accelerator=\"gpu\", devices=2)\ntrainer.fit(task, train_dataloader, val_dataloader)\ntrainer.save_checkpoint(\"best_model.ckpt\")\n\n# use model after training or load weights and drop into the production system\nmodel = ClassificationTask.load_from_checkpoint(\"best_model.ckpt\")\nx = ...\nmodel.eval()\nwith torch.no_grad():\n    y_hat = model(x)\n```\nCheck out Inference in Production <production_inference> guide to learn about the possible ways to perform inference in production.\n\n-----------",
    "parent_index": 18,
    "index": 82
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Save Hyperparameters",
    "text": "Often times we train many versions of a model. You might share that model or come back to it a few months later at which\npoint it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc...).\n\nLightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to\nimprove readability and reproducibility.",
    "parent_index": 18,
    "index": 83
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "save_hyperparameters",
    "text": "Use ~lightning.pytorch.core.LightningModule.save_hyperparameters within your\n~lightning.pytorch.core.LightningModule's __init__ method. It will enable Lightning to store all the\nprovided arguments under the self.hparams attribute. These hyperparameters will also be stored within the model\ncheckpoint, which simplifies model re-instantiation after training.\n\n```python\nclass LitMNIST(L.LightningModule):\n    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n        super().__init__()\n        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n        self.save_hyperparameters()\n\n        # equivalent\n        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n\n        # Now possible to access layer_1_dim from hparams\n        self.hparams.layer_1_dim\n```\nIn addition, loggers that support it will automatically log the contents of self.hparams.",
    "parent_index": 18,
    "index": 84
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Excluding hyperparameters",
    "text": "By default, every parameter of the __init__ method will be considered a hyperparameter to the LightningModule.\nHowever, sometimes some parameters need to be excluded from saving, for example when they are not serializable. Those\nparameters should be provided back when reloading the LightningModule. In this case, exclude them explicitly:\n\n```python\nclass LitMNIST(L.LightningModule):\n    def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n        super().__init__()\n        self.layer_1_dim = layer_1_dim\n        self.loss_fx = loss_fx\n\n        # call this to save only (layer_1_dim=128) to the checkpoint\n        self.save_hyperparameters(\"layer_1_dim\")\n\n        # equivalent\n        self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])\n```",
    "parent_index": 18,
    "index": 85
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "load_from_checkpoint",
    "text": "LightningModules that have hyperparameters automatically saved with\n~lightning.pytorch.core.LightningModule.save_hyperparameters can conveniently be loaded and instantiated\ndirectly from a checkpoint with ~lightning.pytorch.core.LightningModule.load_from_checkpoint:\n\n```python\n# to load specify the other args\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\nIf parameters were excluded, they need to be provided at the time of loading:\n\n```python\n# the excluded parameters were `loss_fx` and `generator_network`\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\n-----------",
    "parent_index": 18,
    "index": 86
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "current_epoch",
    "text": "The number of epochs run.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.current_epoch == 0:\n        ...\n```",
    "parent_index": 18,
    "index": 87
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "device",
    "text": "The device the module is on. Use it to keep your code device agnostic.\n\n```python\ndef training_step(self, batch, batch_idx):\n    z = torch.rand(2, 3, device=self.device)\n```",
    "parent_index": 18,
    "index": 88
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "global_rank",
    "text": "The global_rank is the index of the current process across all nodes and devices.\nLightning will perform some operations such as logging, weight checkpointing only when global_rank=0. You\nusually do not need to use this property, but it is useful to know how to access it if needed.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.global_rank == 0:\n        # do something only once across all the nodes\n        ...\n```",
    "parent_index": 18,
    "index": 89
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "global_step",
    "text": "The number of optimizer steps taken (does not reset each epoch).\nThis includes multiple optimizers (if enabled).\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.logger.experiment.log_image(..., step=self.global_step)\n```",
    "parent_index": 18,
    "index": 90
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "hparams",
    "text": "The arguments passed through LightningModule.__init__() and saved by calling\n~lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin.save_hyperparameters could be accessed by the hparams attribute.\n\n```python\ndef __init__(self, learning_rate):\n    self.save_hyperparameters()\n\ndef configure_optimizers(self):\n    return Adam(self.parameters(), lr=self.hparams.learning_rate)\n```",
    "parent_index": 18,
    "index": 91
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "logger",
    "text": "The current logger being used (tensorboard or other supported logger)\n\n```python\ndef training_step(self, batch, batch_idx):\n    # the generic logger (same no matter if tensorboard or other supported logger)\n    self.logger\n\n    # the particular logger\n    tensorboard_logger = self.logger.experiment\n```",
    "parent_index": 18,
    "index": 92
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "loggers",
    "text": "The list of loggers currently being used by the Trainer.\n\n```python\ndef training_step(self, batch, batch_idx):\n    # List of Logger objects\n    loggers = self.loggers\n    for logger in loggers:\n        logger.log_metrics({\"foo\": 1.0})\n```",
    "parent_index": 18,
    "index": 93
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "local_rank",
    "text": "The local_rank is the index of the current process across all the devices for the current node.\nYou usually do not need to use this property, but it is useful to know how to access it if needed.\nFor example, if using 10 machines (or nodes), the GPU at index 0 on each machine has local_rank = 0.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.local_rank == 0:\n        # do something only once across each node\n        ...\n```",
    "parent_index": 18,
    "index": 94
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "precision",
    "text": "The type of precision used:\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.precision == \"16-true\":\n        ...\n```",
    "parent_index": 18,
    "index": 95
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "trainer",
    "text": "Pointer to the trainer\n\n```python\ndef training_step(self, batch, batch_idx):\n    max_steps = self.trainer.max_steps\n    any_flag = self.trainer.any_flag\n```",
    "parent_index": 18,
    "index": 96
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "prepare_data_per_node",
    "text": "If set to True will call prepare_data() on LOCAL_RANK=0 for every node.\nIf set to False will only call from NODE_RANK=0, LOCAL_RANK=0.\n\nclass LitModel(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.prepare_data_per_node = True",
    "parent_index": 18,
    "index": 97
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "automatic_optimization",
    "text": "When set to False, Lightning does not automate the optimization process. This means you are responsible for handling\nyour optimizers. However, we do take care of precision and any accelerators used.\n\nSee manual optimization <common/optimization:Manual optimization> for details.\n\n```python\ndef __init__(self):\n    self.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\n    opt = self.optimizers(use_pl_optimizer=True)\n\n    loss = ...\n    opt.zero_grad()\n    self.manual_backward(loss)\n    opt.step()\n```\nManual optimization is most useful for research topics like reinforcement learning, sparse coding, and GAN research.\nIt is required when you are using 2+ optimizers because with automatic optimization, you can only use one optimizer.\n\n```python\ndef __init__(self):\n    self.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\n    # access your optimizers with use_pl_optimizer=False. Default is True\n    opt_a, opt_b = self.optimizers(use_pl_optimizer=True)\n\n    gen_loss = ...\n    opt_a.zero_grad()\n    self.manual_backward(gen_loss)\n    opt_a.step()\n\n    disc_loss = ...\n    opt_b.zero_grad()\n    self.manual_backward(disc_loss)\n    opt_b.step()\n```",
    "parent_index": 18,
    "index": 98
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "example_input_array",
    "text": "Set and access example_input_array, which basically represents a single batch.\n\n```python\ndef __init__(self):\n    self.example_input_array = ...\n    self.generator = ...\n\ndef on_train_epoch_end(self):\n    # generate some images using the example_input_array\n    gen_images = self.generator(self.example_input_array)\n```\n--------------",
    "parent_index": 18,
    "index": 99
  },
  {
    "file": "docs/pytorch/stable/common/lightning_module.html",
    "label": "docs",
    "title": "Hooks",
    "text": "This is the pseudocode to describe the structure of ~lightning.pytorch.trainer.trainer.Trainer.fit.\nThe inputs and outputs of each function are not represented for simplicity. Please check each function's API reference\nfor more information.\n\n```python\n# runs on every device: devices can be GPUs, TPUs, ...\ndef fit(self):\n    configure_callbacks()\n\n    if local_rank == 0:\n        prepare_data()\n\n    setup(\"fit\")\n    configure_model()\n    configure_optimizers()\n\n    on_fit_start()\n\n    # the sanity check runs here\n\n    on_train_start()\n    for epoch in epochs:\n        fit_loop()\n    on_train_end()\n\n    on_fit_end()\n    teardown(\"fit\")\n\ndef fit_loop():\n    torch.set_grad_enabled(True)\n\n    on_train_epoch_start()\n\n    for batch_idx, batch in enumerate(train_dataloader()):\n        on_train_batch_start()\n\n        on_before_batch_transfer()\n        transfer_batch_to_device()\n        on_after_batch_transfer()\n\n        out = training_step()\n\n        on_before_zero_grad()\n        optimizer_zero_grad()\n\n        on_before_backward()\n        backward()\n        on_after_backward()\n\n        on_before_optimizer_step()\n        configure_gradient_clipping()\n        optimizer_step()\n\n        on_train_batch_end(out, batch, batch_idx)\n\n        if should_check_val:\n            val_loop()\n\n    on_train_epoch_end()\n\ndef val_loop():\n    on_validation_model_eval()  # calls `model.eval()`\n    torch.set_grad_enabled(False)\n\n    on_validation_start()\n    on_validation_epoch_start()\n\n    for batch_idx, batch in enumerate(val_dataloader()):\n        on_validation_batch_start(batch, batch_idx)\n\n        batch = on_before_batch_transfer(batch)\n        batch = transfer_batch_to_device(batch)\n        batch = on_after_batch_transfer(batch)\n\n        out = validation_step(batch, batch_idx)\n\n        on_validation_batch_end(out, batch, batch_idx)\n\n    on_validation_epoch_end()\n    on_validation_end()\n\n    # set up for train\n    on_validation_model_train()  # calls `model.train()`\n    torch.set_grad_enabled(True)\n```",
    "parent_index": 18,
    "index": 100
  },
  {
    "file": "docs/pytorch/stable/common/notebooks.html",
    "label": "docs",
    "title": "Lightning in notebooks",
    "text": "You can use the Lightning Trainer in interactive notebooks just like in a regular Python script, including multi-GPU training!\n\n```python\nimport lightning as L\n\n# Works in Jupyter, Colab and Kaggle!\ntrainer = L.Trainer(accelerator=\"auto\", devices=\"auto\")\n```\nYou can find many notebook examples on our tutorials page <../tutorials> too!\n\n----",
    "parent_index": 19,
    "index": 101
  },
  {
    "file": "docs/pytorch/stable/common/notebooks.html",
    "label": "docs",
    "title": "Full example",
    "text": "Paste the following code block into a notebook cell:\n\n```python\nimport lightning as L\nfrom torch import nn, optim, utils\nimport torchvision\n\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=1e-3)\n\n    def prepare_data(self):\n        torchvision.datasets.MNIST(\".\", download=True)\n\n    def train_dataloader(self):\n        dataset = torchvision.datasets.MNIST(\".\", transform=torchvision.transforms.ToTensor())\n        return utils.data.DataLoader(dataset, batch_size=64)\n\nautoencoder = LitAutoEncoder(encoder, decoder)\ntrainer = L.Trainer(max_epochs=2, devices=\"auto\")\ntrainer.fit(model=autoencoder)\n```\n----",
    "parent_index": 19,
    "index": 102
  },
  {
    "file": "docs/pytorch/stable/common/notebooks.html",
    "label": "docs",
    "title": "Multi-GPU Limitations",
    "text": "The multi-GPU capabilities in Jupyter are enabled by launching processes using the 'fork' start method.\nIt is the only supported way of multi-processing in notebooks, but also brings some limitations that you should be aware of.",
    "parent_index": 19,
    "index": 103
  },
  {
    "file": "docs/pytorch/stable/common/notebooks.html",
    "label": "docs",
    "title": "Avoid initializing CUDA before .fit()",
    "text": "Don't run torch CUDA functions before calling trainer.fit() in any of the notebook cells beforehand, otherwise your code may hang or crash.\n\n```python\n# BAD: Don't run CUDA-related code before `.fit()`\nx = torch.tensor(1).cuda()\ntorch.cuda.empty_cache()\ntorch.cuda.is_available()\n\ntrainer = L.Trainer(accelerator=\"cuda\", devices=2)\ntrainer.fit(model)\n```",
    "parent_index": 19,
    "index": 104
  },
  {
    "file": "docs/pytorch/stable/common/notebooks.html",
    "label": "docs",
    "title": "Move data loading code inside the hooks",
    "text": "If you define/load your data in the main process before calling trainer.fit(), you may see a slowdown or crashes (segmentation fault, SIGSEV, etc.).\n\n```python\n# BAD: Don't load data in the main process\ndataset = MyDataset(\"data/\")\ntrain_dataloader = torch.utils.data.DataLoader(dataset)\n\ntrainer = L.Trainer(accelerator=\"cuda\", devices=2)\ntrainer.fit(model, train_dataloader)\n```\nThe best practice is to move your data loading code inside the *_dataloader() hooks in the ~lightning.pytorch.core.LightningModule or ~lightning.pytorch.core.datamodule.LightningDataModule as shown in the example above <jupyter_notebook_example>.",
    "parent_index": 19,
    "index": 105
  },
  {
    "file": "docs/pytorch/stable/common/precision_basic.html",
    "label": "docs",
    "title": "16-bit Precision",
    "text": "Use 16-bit mixed precision to speed up training and inference.\nIf your GPUs are [Tensor Core] GPUs, you can expect a ~3x speed improvement.\n\n```python\nTrainer(precision=\"16-mixed\")\n```\nIn most cases, mixed precision uses FP16. Supported PyTorch operations_ automatically run in FP16, saving memory and improving throughput on the supported accelerators.\nSince computation happens in FP16, which has a very limited \"dynamic range\", there is a chance of numerical instability during training. This is handled internally by a dynamic grad scaler which skips invalid steps and adjusts the scaler to ensure subsequent steps fall within a finite range. For more information see the autocast docs_.\n\nWith true 16-bit precision you can additionally lower your memory consumption by up to half so that you can train and deploy larger models.\nHowever, this setting can sometimes lead to unstable training.\n\n```python\nTrainer(precision=\"16-true\")\n```\nFloat16 cannot represent values smaller than ~6e-5. Values like Adam's default eps=1e-8 become zero, which can cause\nNaN during training. Increase eps to 1e-4 or higher, and avoid extremely small values in your model weights and data.\nBFloat16 (\"bf16-mixed\" or \"bf16-true\") has better numerical stability with a wider dynamic range.\n----",
    "parent_index": 20,
    "index": 106
  },
  {
    "file": "docs/pytorch/stable/common/precision_basic.html",
    "label": "docs",
    "title": "32-bit Precision",
    "text": "32-bit precision is the default used across all models and research. This precision is known to be stable in contrast to lower precision settings.\n\nTrainer(precision=\"32-true\")\n\n# or (legacy)\nTrainer(precision=\"32\")\n\n# or (legacy)\nTrainer(precision=32)\n----",
    "parent_index": 20,
    "index": 107
  },
  {
    "file": "docs/pytorch/stable/common/precision_basic.html",
    "label": "docs",
    "title": "64-bit Precision",
    "text": "For certain scientific computations, 64-bit precision enables more accurate models. However, doubling the precision from 32 to 64 bit also doubles the memory requirements.\n\nTrainer(precision=\"64-true\")\n\n# or (legacy)\nTrainer(precision=\"64\")\n\n# or (legacy)\nTrainer(precision=64)\nSince in deep learning, memory is always a bottleneck, especially when dealing with a large volume of data and with limited resources.\nIt is recommended using single precision for better speed. Although you can still use it if you want for your particular use-case.\n\nWhen working with complex numbers, instantiation of complex tensors should be done in the\n~lightning.pytorch.core.hooks.ModelHooks.configure_model hook or under the\n~lightning.pytorch.trainer.trainer.Trainer.init_module context manager so that the `complex128` dtype\nis properly selected.\n\n```python\ntrainer = Trainer(precision=\"64-true\")\n\n# init the model directly on the device and with parameters in full-precision\nwith trainer.init_module():\n    model = MyModel()\n\ntrainer.fit(model)\n```\n----",
    "parent_index": 20,
    "index": 108
  },
  {
    "file": "docs/pytorch/stable/common/precision_basic.html",
    "label": "docs",
    "title": "Precision support by accelerator",
    "text": "* - Precision\n- CPU\n- GPU\n- TPU\n* - 16 Mixed\n- No\n- Yes\n- No\n* - BFloat16 Mixed\n- Yes\n- Yes\n- Yes\n* - 32 True\n- Yes\n- Yes\n- Yes\n* - 64 True\n- Yes\n- Yes\n- No",
    "parent_index": 20,
    "index": 109
  },
  {
    "file": "docs/pytorch/stable/common/precision_expert.html",
    "label": "docs",
    "title": "Precision Plugins",
    "text": "You can also customize and pass your own Precision Plugin by subclassing the ~lightning.pytorch.plugins.precision.precision.Precision class.\n\n- Perform pre and post backward/optimizer step operations such as scaling gradients.\n- Provide context managers for forward, training_step, etc.\n\n```python\nclass CustomPrecision(Precision):\n    precision = \"16-mixed\"\n\n    ...\n\ntrainer = Trainer(plugins=[CustomPrecision()])\n```",
    "parent_index": 21,
    "index": 110
  },
  {
    "file": "docs/pytorch/stable/common/precision_intermediate.html",
    "label": "docs",
    "title": "What is Mixed Precision?",
    "text": "PyTorch, like most deep learning frameworks, trains on 32-bit floating-point (FP32) arithmetic by default. However, many deep learning models do not require this to reach complete accuracy. By conducting\noperations in half-precision format while keeping minimum information in single-precision to maintain as much information as possible in crucial areas of the network, mixed precision training delivers\nsignificant computational speedup. Switching to mixed precision has resulted in considerable training speedups since the introduction of Tensor Cores in the Volta and Turing architectures. It combines\nFP32 and lower-bit floating-points (such as FP16) to reduce memory footprint and increase performance during model training and evaluation. It accomplishes this by recognizing the steps that require\ncomplete accuracy and employing a 32-bit floating-point for those steps only, while using a 16-bit floating-point for the rest. When compared to complete precision training, mixed precision training\ndelivers all of these benefits while ensuring that no task-specific accuracy is lost. [2].\n\nIn some cases, it is essential to remain in FP32 for numerical stability, so keep this in mind when using mixed precision.\nFor example, when running scatter operations during the forward (such as torchpoint3d), computation must remain in FP32.\nDo not cast anything to other dtypes manually using torch.autocast or tensor.half() when using native precision because\nthis can bring instability.\n\n.. code-block:: python\n\nclass LitModel(LightningModule):\ndef training_step(self, batch, batch_idx):\nouts = self(batch)\n\na_float32 = torch.rand((8, 8), device=self.device, dtype=self.dtype)\nb_float32 = torch.rand((8, 4), device=self.device, dtype=self.dtype)\n\n# casting to float16 manually\nwith torch.autocast(device_type=self.device.type):\nc_float16 = torch.mm(a_float32, b_float32)\ntarget = self.layer(c_float16.flatten()[None])\n\n# here outs is of type float32 and target is of type float16\nloss = torch.mm(target @ outs).float()\nreturn loss\n\ntrainer = Trainer(accelerator=\"gpu\", devices=1, precision=32)\n----",
    "parent_index": 22,
    "index": 111
  },
  {
    "file": "docs/pytorch/stable/common/precision_intermediate.html",
    "label": "docs",
    "title": "BFloat16 Mixed Precision",
    "text": "BFloat16 may not provide significant speedups or memory improvements or offer better numerical stability.\nFor GPUs, the most significant benefits require Ampere_ based GPUs or newer, such as A100s or 3090s.\nBFloat16 Mixed precision is similar to FP16 mixed precision, however, it maintains more of the \"dynamic range\" that FP32 offers. This means it is able to improve numerical stability than FP16 mixed precision. For more information, see this TPU performance blogpost_.\n\nUnder the hood, we use torch.autocast_ with the dtype set to bfloat16, with no gradient scaling.\n\nTrainer(accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\")\nIt is also possible to use BFloat16 mixed precision on the CPU, relying on MKLDNN under the hood.\n\nTrainer(precision=\"bf16-mixed\")\n----",
    "parent_index": 22,
    "index": 112
  },
  {
    "file": "docs/pytorch/stable/common/precision_intermediate.html",
    "label": "docs",
    "title": "True Half Precision",
    "text": "As mentioned before, for numerical stability mixed precision keeps the model weights in full float32 precision while casting only supported operations to lower bit precision.\nHowever, in some cases it is indeed possible to train completely in half precision. Similarly, for inference the model weights can often be cast to half precision without a loss in accuracy (even when trained with mixed precision).\n\n```python\n# Select FP16 precision\ntrainer = Trainer(precision=\"16-true\")\ntrainer.fit(model)  # model gets cast to torch.float16\n\n# Select BF16 precision\ntrainer = Trainer(precision=\"bf16-true\")\ntrainer.fit(model)  # model gets cast to torch.bfloat16\n```\nTip: For faster initialization, you can create model parameters with the desired dtype directly on the device:\n\n```python\ntrainer = Trainer(precision=\"bf16-true\")\n\n# init the model directly on the device and with parameters in half-precision\nwith trainer.init_module():\n    model = MyModel()\n\ntrainer.fit(model)\n```\nSee also: ../advanced/model_init\n\n----",
    "parent_index": 22,
    "index": 113
  },
  {
    "file": "docs/pytorch/stable/common/precision_intermediate.html",
    "label": "docs",
    "title": "Float8 Mixed Precision via Nvidia's TransformerEngine",
    "text": "Transformer Engine_ (TE) is a library for accelerating models on the\nlatest NVIDIA GPUs using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower\nmemory utilization in both training and inference. It offers improved performance over half precision with no degradation in accuracy.\n\nUsing TE requires replacing some of the layers in your model. Fabric automatically replaces the torch.nn.Linear\nand torch.nn.LayerNorm layers in your model with their TE alternatives, however, TE also offers\nfused layers_\nto squeeze out all the possible performance. If Fabric detects that any layer has been replaced already, automatic\nreplacement is not done.\n\nThis plugin is a combination of \"mixed\" and \"true\" precision. The computation is downcasted to FP8 precision on the fly, but\nthe model and inputs can be kept in true full or half precision.\n\n```python\n# Select 8bit mixed precision via TransformerEngine, with model weights in bfloat16\ntrainer = Trainer(precision=\"transformer-engine\")\n\n# Select 8bit mixed precision via TransformerEngine, with model weights in float16\ntrainer = Trainer(precision=\"transformer-engine-float16\")\n\n# Customize the fp8 recipe or set a different base precision:\nfrom lightning.trainer.plugins import TransformerEnginePrecision\n\nrecipe = {\"fp8_format\": \"HYBRID\", \"amax_history_len\": 16, \"amax_compute_algo\": \"max\"}\nprecision = TransformerEnginePrecision(weights_dtype=torch.bfloat16, recipe=recipe)\ntrainer = Trainer(plugins=precision)\n```\nUnder the hood, we use transformer_engine.pytorch.fp8_autocast_ with the default fp8 recipe.\n\nThis requires Hopper based GPUs or newer, such the H100.\n----",
    "parent_index": 22,
    "index": 114
  },
  {
    "file": "docs/pytorch/stable/common/precision_intermediate.html",
    "label": "docs",
    "title": "Quantization via Bitsandbytes",
    "text": "bitsandbytes_ (BNB) is a library that supports quantizing torch.nn.Linear weights.\n\nBoth 4-bit (paper reference_) and 8-bit (paper reference_) quantization is supported.\nSpecifically, we support the following modes:\n\n* **nf4**: Uses the normalized float 4-bit data type. This is recommended over \"fp4\" based on the paper's experimental results and theoretical analysis.\n* **nf4-dq**: \"dq\" stands for \"Double Quantization\" which reduces the average memory footprint by quantizing the quantization constants. In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).\n* **fp4**: Uses regular float 4-bit data type.\n* **fp4-dq**: \"dq\" stands for \"Double Quantization\" which reduces the average memory footprint by quantizing the quantization constants. In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).\n* **int8**: Uses unsigned int8 data type.\n* **int8-training**: Meant for int8 activations with fp16 precision weights.\n\nWhile these techniques store weights in 4 or 8 bit, the computation still happens in 16 or 32-bit (float16, bfloat16, float32).\nThis is configurable via the dtype argument in the plugin.\nIf your model weights can fit on a single device with 16 bit precision, it's recommended that this plugin is not used as it will slow down training.\n\nQuantizing the model will dramatically reduce the weight's memory requirements but may have a negative impact on the model's performance or runtime.\n\nThe ~lightning.pytorch.plugins.precision.bitsandbytes.BitsandbytesPrecision automatically replaces the torch.nn.Linear layers in your model with their BNB alternatives.\n\n```python\nfrom lightning.pytorch.plugins import BitsandbytesPrecision\n\n# this will pick out the compute dtype automatically, by default `bfloat16`\nprecision = BitsandbytesPrecision(mode=\"nf4-dq\")\ntrainer = Trainer(plugins=precision)\n\n# Customize the dtype, or skip some modules\nprecision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\ntrainer = Trainer(plugins=precision)\n\nclass MyModel(LightningModule):\n    def configure_model(self):\n        # instantiate your model in this hook\n        self.model = MyModel()\n```\nOnly supports CUDA devices and the Linux operating system. Windows users should use\nWSL2_.\nThis plugin does not take care of replacing your optimizer with an 8-bit optimizer e.g. bitsandbytes.optim.Adam8bit.\nYou might want to do this for extra memory savings.\n\n```python\nimport bitsandbytes as bnb\n\nclass MyModel(LightningModule):\n    def configure_optimizers(self):\n        optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n\n        # (optional) force embedding layers to use 32 bit for numerical stability\n        # https://github.com/huggingface/transformers/issues/14819#issuecomment-1003445038\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n        return optimizer\n```",
    "parent_index": 22,
    "index": 115
  },
  {
    "file": "docs/pytorch/stable/common/progress_bar.html",
    "label": "docs",
    "title": "Customize the progress bar",
    "text": "Lightning supports two different types of progress bars (tqdm and rich). ~lightning.pytorch.callbacks.TQDMProgressBar is used by default,\nbut you can override it by passing a custom ~lightning.pytorch.callbacks.TQDMProgressBar or ~lightning.pytorch.callbacks.RichProgressBar to the callbacks argument of the ~lightning.pytorch.trainer.trainer.Trainer.\n\nYou could also use the ~lightning.pytorch.callbacks.ProgressBar class to implement your own progress bar.\n\n-------------\n\n###### TQDMProgressBar\n\nThe ~lightning.pytorch.callbacks.TQDMProgressBar uses the tqdm library internally and is the default progress bar used by Lightning.\nIt prints to stdout and shows up to four different bars:\n\n- **sanity check progress:** the progress during the sanity check run\n- **train progress:** shows the training progress. It will pause if validation starts and will resume when it ends, and also accounts for multiple validation runs during training when ~lightning.pytorch.trainer.trainer.Trainer.val_check_interval is used.\n- **validation progress:** only visible during validation; shows total progress over all validation datasets.\n- **test progress:** only active when testing; shows total progress over all test datasets.\n\nFor infinite datasets, the progress bar never ends.\n\nYou can update refresh_rate (rate (number of batches) at which the progress bar get updated) for ~lightning.pytorch.callbacks.TQDMProgressBar by:\n\n```python\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\ntrainer = Trainer(callbacks=[TQDMProgressBar(refresh_rate=10)])\n```\nThe smoothing option has no effect when using the default implementation of ~lightning.pytorch.callbacks.TQDMProgressBar, as the progress bar is updated using the bar.refresh() method instead of bar.update(). This can cause the progress bar to become desynchronized with the actual progress. To avoid this issue, you can use the bar.update() method instead, but this may require customizing the ~lightning.pytorch.callbacks.TQDMProgressBar class.\nBy default the training progress bar is reset (overwritten) at each new epoch.\nIf you wish for a new progress bar to be displayed at the end of every epoch, set\nTQDMProgressBar.leave <lightning.pytorch.callbacks.TQDMProgressBar.leave> to True.\n\n```python\ntrainer = Trainer(callbacks=[TQDMProgressBar(leave=True)])\n```\nIf you want to customize the default ~lightning.pytorch.callbacks.TQDMProgressBar used by Lightning, you can override\nspecific methods of the callback class and pass your custom implementation to the ~lightning.pytorch.trainer.trainer.Trainer.\n\n```python\nclass LitProgressBar(TQDMProgressBar):\n    def init_validation_tqdm(self):\n        bar = super().init_validation_tqdm()\n        bar.set_description(\"running validation...\")\n        return bar\n\ntrainer = Trainer(callbacks=[LitProgressBar()])\n```\n- ~lightning.pytorch.callbacks.TQDMProgressBar docs.\n- tqdm library_\n----------------\n\n###### RichProgressBar\n\nRich is a Python library for rich text and beautiful formatting in the terminal.\nTo use the ~lightning.pytorch.callbacks.RichProgressBar as your progress bar, first install the package:\n\n```bash\npip install rich\n```\nThen configure the callback and pass it to the ~lightning.pytorch.trainer.trainer.Trainer:\n\n```python\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar()])\n```\nCustomize the theme for your ~lightning.pytorch.callbacks.RichProgressBar like this:\n\n```python\nfrom lightning.pytorch.callbacks import RichProgressBar\nfrom lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n# create your own theme!\nprogress_bar = RichProgressBar(\n    theme=RichProgressBarTheme(\n        description=\"green_yellow\",\n        progress_bar=\"green1\",\n        progress_bar_finished=\"green1\",\n        progress_bar_pulse=\"#6206E0\",\n        batch_progress=\"green_yellow\",\n        time=\"grey82\",\n        processing_speed=\"grey82\",\n        metrics=\"grey82\",\n        metrics_text_delimiter=\"\\n\",\n        metrics_format=\".3e\",\n    )\n)\n\ntrainer = Trainer(callbacks=progress_bar)\n```\nYou can customize the components used within ~lightning.pytorch.callbacks.RichProgressBar with ease by overriding the\n~lightning.pytorch.callbacks.RichProgressBar.configure_columns method.\n\n```python\nfrom rich.progress import TextColumn\n\ncustom_column = TextColumn(\"[progress.description]Custom Rich Progress Bar!\")\n\nclass CustomRichProgressBar(RichProgressBar):\n    def configure_columns(self, trainer):\n        return [custom_column]\n\nprogress_bar = CustomRichProgressBar()\n```\nIf you wish for a new progress bar to be displayed at the end of every epoch, you should enable\nRichProgressBar.leave <lightning.pytorch.callbacks.RichProgressBar.leave> by passing True\n\n```python\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar(leave=True)])\n```\n- ~lightning.pytorch.callbacks.RichProgressBar docs.\n- ~lightning.pytorch.callbacks.RichModelSummary docs to customize the model summary table.\n- Rich library_.\nProgress bar is automatically enabled with the Trainer, and to disable it, one should do this:\n\n.. code-block:: python\n\ntrainer = Trainer(enable_progress_bar=False)",
    "parent_index": 23,
    "index": 116
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "Trainer",
    "text": "Once you've organized your PyTorch code into a ~lightning.pytorch.core.LightningModule, the Trainer automates everything else.\n\nThe Trainer achieves the following:\n\n1. You maintain control over all aspects via PyTorch code in your ~lightning.pytorch.core.LightningModule.\n\n2. The trainer uses best practices embedded by contributors and users\nfrom top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc...\n\n3. The trainer allows disabling any key part that you don't want automated.\n\n|\n\n-----------\n\n###### Basic use\n\nThis is the basic use of the trainer:\n\n```python\nmodel = MyLightningModule()\n\ntrainer = Trainer()\ntrainer.fit(model, train_dataloader, val_dataloader)\n```\n--------\n\n###### Under the hood\n\nThe Lightning Trainer does much more than just \"training\". Under the hood, it handles all loop details for you, some examples include:\n\n- Automatically enabling/disabling grads\n- Running the training, validation and test dataloaders\n- Calling the Callbacks at the appropriate times\n- Putting batches and computations on the correct devices\n\nHere's the pseudocode for what the trainer does under the hood (showing the train loop only)\n\n```python\n# enable grads\ntorch.set_grad_enabled(True)\n\nlosses = []\nfor batch in train_dataloader:\n    # calls hooks like this one\n    on_train_batch_start()\n\n    # train step\n    loss = training_step(batch)\n\n    # clear gradients\n    optimizer.zero_grad()\n\n    # backward\n    loss.backward()\n\n    # update parameters\n    optimizer.step()\n\n    losses.append(loss)\n```\n--------\n\n###### Trainer in Python scripts\nIn Python scripts, it's recommended you use a main function to call the Trainer.\n\n```python\nfrom argparse import ArgumentParser\n\ndef main(hparams):\n    model = LightningModule()\n    trainer = Trainer(accelerator=hparams.accelerator, devices=hparams.devices)\n    trainer.fit(model)\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--accelerator\", default=None)\n    parser.add_argument(\"--devices\", default=None)\n    args = parser.parse_args()\n\n    main(args)\n```\nSo you can run it like so:\n\n```bash\npython main.py --accelerator 'gpu' --devices 2\n```\nPro-tip: You don't need to define all flags manually.\nYou can let the LightningCLI <../cli/lightning_cli> create the Trainer and model with arguments supplied from the CLI.\nIf you want to stop a training run early, you can press \"Ctrl + C\" on your keyboard.\nThe trainer will catch the KeyboardInterrupt and attempt a graceful shutdown. The trainer object will also set\nan attribute interrupted to True in such cases. If you have a callback which shuts down compute\nresources, for example, you can conditionally run the shutdown logic for only uninterrupted runs by overriding lightning.pytorch.Callback.on_exception.\n\n------------\n\n###### Validation\nYou can perform an evaluation epoch over the validation set, outside of the training loop,\nusing ~lightning.pytorch.trainer.trainer.Trainer.validate. This might be\nuseful if you want to collect new metrics from a model right at its initialization\nor after it has already been trained.\n\n```python\ntrainer.validate(model=model, dataloaders=val_dataloaders)\n```\n------------\n\n###### Testing\nOnce you're done training, feel free to run the test set!\n(Only right before publishing your paper or pushing to production)\n\n```python\ntrainer.test(dataloaders=test_dataloaders)\n```\n------------\n\n###### Reproducibility\n\nTo ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\nand set deterministic flag in Trainer.\n\nExample:\n```\n\nfrom lightning.pytorch import Trainer, seed_everything\n\nseed_everything(42, workers=True)\n# sets seeds for numpy, torch and python.random.\nmodel = Model()\ntrainer = Trainer(deterministic=True)\n```\nBy setting workers=True in ~lightning.pytorch.seed_everything, Lightning derives\nunique seeds across all dataloader workers and processes for torch, numpy and stdlib\nrandom number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n\n-------\n\n###### Trainer flags\n\n#### accelerator\n\nSupports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"auto\")\nas well as custom accelerator instances.\n\n```python\n# CPU accelerator\ntrainer = Trainer(accelerator=\"cpu\")\n\n# Training with GPU Accelerator using 2 GPUs\ntrainer = Trainer(devices=2, accelerator=\"gpu\")\n\n# Training with TPU Accelerator using 8 tpu cores\ntrainer = Trainer(devices=8, accelerator=\"tpu\")\n\n# Training with GPU Accelerator using the DistributedDataParallel strategy\ntrainer = Trainer(devices=4, accelerator=\"gpu\", strategy=\"ddp\")\n```\n```python\n# If your machine has GPUs, it will use the GPU Accelerator for training\ntrainer = Trainer(devices=2, accelerator=\"auto\")\n```\nYou can also modify hardware behavior by subclassing an existing accelerator to adjust for your needs.\n\nExample:\n```\n\nclass MyOwnAcc(CPUAccelerator):\n    ...\n\nTrainer(accelerator=MyOwnAcc())\n```\nIf the devices flag is not defined, it will assume devices to be \"auto\" and fetch the auto_device_count\nfrom the accelerator.\n\n.. code-block:: python\n\n# This is part of the built-in `CUDAAccelerator`\nclass CUDAAccelerator(Accelerator):\n\"\"\"Accelerator for GPU devices.\"\"\"\n\n@staticmethod\ndef auto_device_count() -> int:\n\"\"\"Get the devices when set to auto.\"\"\"\nreturn torch.cuda.device_count()\n\n# Training with GPU Accelerator using total number of gpus available on the system\nTrainer(accelerator=\"gpu\")\n#### accumulate_grad_batches\n\nAccumulates gradients over k batches before stepping the optimizer.\n\n# default used by the Trainer (no accumulation)\ntrainer = Trainer(accumulate_grad_batches=1)\nExample:\n```\n\n# accumulate every 4 batches (effective batch size is batch*4)\ntrainer = Trainer(accumulate_grad_batches=4)\n```\nSee also: gradient_accumulation to enable more fine-grained accumulation schedules.\n\n#### barebones\n\nWhether to run in \"barebones mode\", where all features that may impact raw speed are disabled. This is meant for\nanalyzing the Trainer overhead and is discouraged during regular training runs.\n\nWhen enabled, the following features are automatically deactivated:\n- Checkpointing: enable_checkpointing=False\n- Logging: logger=False, log_every_n_steps=0\n- Progress bar: enable_progress_bar=False\n- Model summary: enable_model_summary=False\n- Sanity checking: num_sanity_val_steps=0\n\n# default used by the Trainer\ntrainer = Trainer(barebones=False)\n\n# enable barebones mode for speed analysis\ntrainer = Trainer(barebones=True)\n#### benchmark\n\nThe value (True or False) to set torch.backends.cudnn.benchmark to. The value for\ntorch.backends.cudnn.benchmark set in the current session will be used (False if not manually set).\nIf ~lightning.pytorch.trainer.trainer.Trainer.deterministic is set to True, this will default to False.\nYou can read more about the interaction of torch.backends.cudnn.benchmark and torch.backends.cudnn.deterministic\nhere_\n\nSetting this flag to True can increase the speed of your system if your input sizes don't\nchange. However, if they do, then it might make your system slower. The CUDNN auto-tuner will try to find the best\nalgorithm for the hardware when a new input size is encountered. This might also increase the memory usage.\nRead more about it here_.\n\nExample:\n```\n\n# Will use whatever the current value for torch.backends.cudnn.benchmark, normally False\ntrainer = Trainer(benchmark=None)  # default\n\n# you can overwrite the value\ntrainer = Trainer(benchmark=True)\n```\n#### deterministic\n\nThis flag sets the torch.backends.cudnn.deterministic flag.\nMight make your system slower, but ensures reproducibility.\n\nFor more info check PyTorch docs.\n\nExample:\n```\n\n# default used by the Trainer\ntrainer = Trainer(deterministic=False)\n```\n#### callbacks\n\nThis argument can be used to add a ~lightning.pytorch.callbacks.callback.Callback or a list of them.\nCallbacks run sequentially in the order defined here\nwith the exception of ~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint callbacks which run\nafter all others to ensure all states are saved to the checkpoints.\n\n```python\n# single callback\ntrainer = Trainer(callbacks=PrintCallback())\n\n# a list of callbacks\ntrainer = Trainer(callbacks=[PrintCallback()])\n```\nExample:\n```\n\nfrom lightning.pytorch.callbacks import Callback\n\nclass PrintCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training is started!\")\n    def on_train_end(self, trainer, pl_module):\n        print(\"Training is done.\")\n```\nModel-specific callbacks can also be added inside the LightningModule through\n~lightning.pytorch.core.LightningModule.configure_callbacks.\nCallbacks returned in this hook will extend the list initially given to the Trainer argument, and replace\nthe trainer callbacks should there be two or more of the same type.\n~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint callbacks always run last.\n\n#### check_val_every_n_epoch\n\nCheck val every n train epochs.\n\nExample:\n```\n\n# default used by the Trainer\ntrainer = Trainer(check_val_every_n_epoch=1)\n\n# run val loop every 10 training epochs\ntrainer = Trainer(check_val_every_n_epoch=10)\n```\n#### default_root_dir\n\nDefault path for logs and weights when no logger or\nlightning.pytorch.callbacks.ModelCheckpoint callback passed. On\ncertain clusters you might want to separate where logs and checkpoints are\nstored. If you don't then use this argument for convenience. Paths can be local\npaths or remote paths such as s3://bucket/path or hdfs://path/. Credentials\nwill need to be set up to use remote filepaths.\n\n# default used by the Trainer\ntrainer = Trainer(default_root_dir=os.getcwd())\n#### detect_anomaly\n\nEnable anomaly detection for the autograd engine. This will significantly slow down compute speed and is recommended\nonly for model debugging.\n\n# default used by the Trainer\ntrainer = Trainer(detect_anomaly=False)\n\n# enable anomaly detection for debugging\ntrainer = Trainer(detect_anomaly=True)\n#### devices\n\nNumber of devices to train on (int), which devices to train on (list or str), or \"auto\".\n\n```python\n# Training with CPU Accelerator using 2 processes\ntrainer = Trainer(devices=2, accelerator=\"cpu\")\n\n# Training with GPU Accelerator using GPUs 1 and 3\ntrainer = Trainer(devices=[1, 3], accelerator=\"gpu\")\n\n# Training with TPU Accelerator using 8 tpu cores\ntrainer = Trainer(devices=8, accelerator=\"tpu\")\n```\n```python\n# Use whatever hardware your machine has available\ntrainer = Trainer(devices=\"auto\", accelerator=\"auto\")\n\n# Training with CPU Accelerator using 1 process\ntrainer = Trainer(devices=\"auto\", accelerator=\"cpu\")\n\n# Training with TPU Accelerator using 8 tpu cores\ntrainer = Trainer(devices=\"auto\", accelerator=\"tpu\")\n```\nIf the devices flag is not defined, it will assume devices to be \"auto\" and fetch the auto_device_count\nfrom the accelerator.\n\n.. code-block:: python\n\n# This is part of the built-in `CUDAAccelerator`\nclass CUDAAccelerator(Accelerator):\n\"\"\"Accelerator for GPU devices.\"\"\"\n\n@staticmethod\ndef auto_device_count() -> int:\n\"\"\"Get the devices when set to auto.\"\"\"\nreturn torch.cuda.device_count()\n\n# Training with GPU Accelerator using total number of gpus available on the system\nTrainer(accelerator=\"gpu\")\n#### enable_autolog_hparams\n\nWhether to log hyperparameters at the start of a run. Defaults to True.\n\n# default used by the Trainer\ntrainer = Trainer(enable_autolog_hparams=True)\n\n# disable logging hyperparams\ntrainer = Trainer(enable_autolog_hparams=False)\nWith the parameter set to false, you can add custom code to log hyperparameters.\n\n```python\nmodel = LitModel()\ntrainer = Trainer(enable_autolog_hparams=False)\nfor logger in trainer.loggers:\n    if isinstance(logger, lightning.pytorch.loggers.CSVLogger):\n        logger.log_hyperparams(hparams_dict_1)\n    else:\n        logger.log_hyperparams(hparams_dict_2)\n```\nYou can also use `self.logger.log_hyperparams(...)` inside `LightningModule` to log.\n\n#### enable_checkpointing\n\nBy default Lightning saves a checkpoint for you in your current working directory, with the state of your last training epoch,\nCheckpoints capture the exact value of all parameters used by a model.\nTo disable automatic checkpointing, set this to `False`.\n\n```python\n# default used by Trainer, saves the most recent model to a single checkpoint after each epoch\ntrainer = Trainer(enable_checkpointing=True)\n\n# turn off automatic checkpointing\ntrainer = Trainer(enable_checkpointing=False)\n```\nYou can override the default behavior by initializing the ~lightning.pytorch.callbacks.ModelCheckpoint\ncallback, and adding it to the ~lightning.pytorch.trainer.trainer.Trainer.callbacks list.\nSee Saving and Loading Checkpoints <../common/checkpointing> for how to customize checkpointing.\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# Init ModelCheckpoint callback, monitoring 'val_loss'\ncheckpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n\n# Add your callback to the callbacks list\ntrainer = Trainer(callbacks=[checkpoint_callback])\n#### enable_model_summary\n\nWhether to enable or disable the model summarization. Defaults to True.\n\n# default used by the Trainer\ntrainer = Trainer(enable_model_summary=True)\n\n# disable summarization\ntrainer = Trainer(enable_model_summary=False)\n\n# enable custom summarization\nfrom lightning.pytorch.callbacks import ModelSummary\n\ntrainer = Trainer(enable_model_summary=True, callbacks=[ModelSummary(max_depth=-1)])\n#### enable_progress_bar\n\nWhether to enable or disable the progress bar. Defaults to True.\n\n# default used by the Trainer\ntrainer = Trainer(enable_progress_bar=True)\n\n# disable progress bar\ntrainer = Trainer(enable_progress_bar=False)\n#### fast_dev_run\n\nRuns n if set to n (int) else 1 if set to True batch(es) to ensure your code will execute without errors. This\napplies to fitting, validating, testing, and predicting. This flag is **only** recommended for debugging purposes and\nshould not be used to limit the number of batches to run.\n\n```python\n# default used by the Trainer\ntrainer = Trainer(fast_dev_run=False)\n\n# runs only 1 training and 1 validation batch and the program ends\ntrainer = Trainer(fast_dev_run=True)\ntrainer.fit(...)\n\n# runs 7 predict batches and program ends\ntrainer = Trainer(fast_dev_run=7)\ntrainer.predict(...)\n```\nThis argument is different from limit_{train,val,test,predict}_batches because side effects are avoided to reduce the\nimpact to subsequent runs. These are the changes enabled:\n\n- Sets Trainer(max_epochs=1).\n- Sets Trainer(max_steps=...) to 1 or the number passed.\n- Sets Trainer(num_sanity_val_steps=0).\n- Sets Trainer(val_check_interval=1.0).\n- Sets Trainer(check_every_n_epoch=1).\n- Disables all loggers.\n- Disables passing logged metrics to loggers.\n- The ~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint callbacks will not trigger.\n- The ~lightning.pytorch.callbacks.early_stopping.EarlyStopping callbacks will not trigger.\n- Sets limit_{train,val,test,predict}_batches to 1 or the number passed.\n- Disables the tuning callbacks (~lightning.pytorch.callbacks.batch_size_finder.BatchSizeFinder, ~lightning.pytorch.callbacks.lr_finder.LearningRateFinder).\n- If using the CLI, the configuration file is not saved.\n\n#### gradient_clip_algorithm\n\nThe gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and\ngradient_clip_algorithm=\"norm\" to clip by norm. By default it will be set to \"norm\".\n\n# default used by the Trainer (defaults to \"norm\" when gradient_clip_val is set)\ntrainer = Trainer(gradient_clip_algorithm=None)\n\n# clip by value\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n\n# clip by norm\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n#### gradient_clip_val\n\nGradient clipping value\n\n# default used by the Trainer\ntrainer = Trainer(gradient_clip_val=None)\n#### inference_mode\n\nWhether to use torch.inference_mode or torch.no_grad mode during evaluation\n(validate/test/predict)\n\n# default used by the Trainer\ntrainer = Trainer(inference_mode=True)\n\n# Use `torch.no_grad` instead\ntrainer = Trainer(inference_mode=False)\nWith torch.inference_mode disabled, you can enable the grad of your model layers if required.\n\n```python\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        preds = self.layer1(batch)\n        with torch.enable_grad():\n            grad_preds = preds.requires_grad_()\n            preds2 = self.layer2(grad_preds)\n\nmodel = LitModel()\ntrainer = Trainer(inference_mode=False)\ntrainer.validate(model)\n```\n#### limit_train_batches\n\nHow much of training dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\nValue is per device.\n\n# default used by the Trainer\ntrainer = Trainer(limit_train_batches=1.0)\nExample:\n```\n\n# default used by the Trainer\ntrainer = Trainer(limit_train_batches=1.0)\n\n# run through only 25% of the training set each epoch\ntrainer = Trainer(limit_train_batches=0.25)\n\n# run through only 10 batches of the training set each epoch\ntrainer = Trainer(limit_train_batches=10)\n```\n#### limit_predict_batches\n\nHow much of prediction dataset to check. Value is per device.\n\n# default used by the Trainer\ntrainer = Trainer(limit_predict_batches=1.0)\n\n# run through only 25% of the prediction set\ntrainer = Trainer(limit_predict_batches=0.25)\n\n# run for only 10 batches\ntrainer = Trainer(limit_predict_batches=10)\nIn the case of multiple prediction dataloaders, the limit applies to each dataloader individually.\n\n#### limit_test_batches\n\nHow much of test dataset to check. Value is per device.\n\n# default used by the Trainer\ntrainer = Trainer(limit_test_batches=1.0)\n\n# run through only 25% of the test set each epoch\ntrainer = Trainer(limit_test_batches=0.25)\n\n# run for only 10 batches\ntrainer = Trainer(limit_test_batches=10)\nIn the case of multiple test dataloaders, the limit applies to each dataloader individually.\n\n#### limit_val_batches\n\nHow much of validation dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\nValue is per device.\n\n# default used by the Trainer\ntrainer = Trainer(limit_val_batches=1.0)\n\n# run through only 25% of the validation set each epoch\ntrainer = Trainer(limit_val_batches=0.25)\n\n# run for only 10 batches\ntrainer = Trainer(limit_val_batches=10)\n\n# disable validation\ntrainer = Trainer(limit_val_batches=0)\nIn the case of multiple validation dataloaders, the limit applies to each dataloader individually.\n\n#### log_every_n_steps\n\nHow often to add logging rows (does not write to disk)\n\n# default used by the Trainer\ntrainer = Trainer(log_every_n_steps=50)\nSee Also:\n- logging <../extensions/logging>\n\n#### logger\n\nLogger <../visualize/loggers> (or iterable collection of loggers) for experiment tracking. A True value uses the default TensorBoardLogger shown below. False will disable logging.\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n# default logger used by trainer (if tensorboard is installed)\nlogger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\nTrainer(logger=logger)\n#### max_epochs\n\nStop training once this number of epochs is reached\n\n# default used by the Trainer\ntrainer = Trainer(max_epochs=1000)\nIf both max_epochs and max_steps aren't specified, max_epochs will default to 1000.\nTo enable infinite training, set max_epochs = -1.\n\n#### min_epochs\n\nForce training for at least these many epochs\n\n# default used by the Trainer\ntrainer = Trainer(min_epochs=1)\n#### max_steps\n\nStop training after this number of global steps <common/trainer:global_step>.\nTraining will stop if max_steps or max_epochs have reached (earliest).\n\n# Default (disabled)\ntrainer = Trainer(max_steps=-1)\n\n# Stop after 100 steps\ntrainer = Trainer(max_steps=100)\nIf max_steps is not specified, max_epochs will be used instead (and max_epochs defaults to\n1000 if max_epochs is not specified). To disable this default, set max_steps = -1.\n\n#### min_steps\n\nForce training for at least this number of global steps <common/trainer:global_step>.\nTrainer will train model for at least min_steps or min_epochs (latest).\n\n# Default (disabled)\ntrainer = Trainer(min_steps=None)\n\n# Run at least for 100 steps (disable min_epochs)\ntrainer = Trainer(min_steps=100, min_epochs=0)\n#### max_time\n\nSet the maximum amount of time for training. Training will get interrupted mid-epoch.\nFor customizable options use the ~lightning.pytorch.callbacks.timer.Timer callback.\n\n# Default (disabled)\ntrainer = Trainer(max_time=None)\n\n# Stop after 12 hours of training or when reaching 10 epochs (string)\ntrainer = Trainer(max_time=\"00:12:00:00\", max_epochs=10)\n\n# Stop after 1 day and 5 hours (dict)\ntrainer = Trainer(max_time={\"days\": 1, \"hours\": 5})\nIn case max_time is used together with min_steps or min_epochs, the min_* requirement\nalways has precedence.\n\n#### model_registry\n\nIf specified will upload the model to lightning model registry under the provided name.\n\n# default used by the Trainer\ntrainer = Trainer(model_registry=None)\n\n# specify model name for model hub upload\ntrainer = Trainer(model_registry=\"my-model-name\")\nSee Lightning model registry docs for more info.\n\n#### num_nodes\n\nNumber of GPU nodes for distributed training.\n\n# default used by the Trainer\ntrainer = Trainer(num_nodes=1)\n\n# to train on 8 nodes\ntrainer = Trainer(num_nodes=8)\n#### num_sanity_val_steps\n\nSanity check runs n batches of val before starting the training routine.\nThis catches any bugs in your validation without having to wait for the first validation check.\nThe Trainer uses 2 steps by default. Turn it off or modify it here.\n\n# default used by the Trainer\ntrainer = Trainer(num_sanity_val_steps=2)\n\n# turn it off\ntrainer = Trainer(num_sanity_val_steps=0)\n\n# check all validation data\ntrainer = Trainer(num_sanity_val_steps=-1)\nThis option will reset the validation dataloader unless num_sanity_val_steps=0.\n\n#### overfit_batches\n\nUses this much data of the training & validation set.\nIf the training & validation dataloaders have shuffle=True, Lightning will automatically disable it.\n\n* When set to a value > 0, sequential sampling (no shuffling) is used\n* Consistent batches are used for both training and validation across epochs, but training and validation use different sets of data\n\nUseful for quickly debugging or trying to overfit on purpose.\n\n# default used by the Trainer\ntrainer = Trainer(overfit_batches=0.0)\n\n# use only 1% of the train & val set\ntrainer = Trainer(overfit_batches=0.01)\n\n# overfit on 10 consistent train batches & 10 consistent val batches\ntrainer = Trainer(overfit_batches=10)\n\n# debug using a single consistent train batch and a single consistent val batch\n#### plugins\n\nPlugins allow you to connect arbitrary backends, precision libraries, clusters etc. and modification of core lightning logic.\nExamples of plugin types:\n- Checkpoint IO <checkpointing_expert>\n- TorchElastic\n- Precision Plugins <precision_expert>\n- ~lightning.pytorch.plugins.environments.ClusterEnvironment\n\n# default used by the Trainer\ntrainer = Trainer(plugins=None)\n\n# example using built in slurm plugin\nfrom lightning.fabric.plugins.environments import SLURMEnvironment\ntrainer = Trainer(plugins=[SLURMEnvironment()])\nTo define your own behavior, subclass the relevant class and pass it in. Here's an example linking up your own\n~lightning.pytorch.plugins.environments.ClusterEnvironment.\n\n```python\nfrom lightning.pytorch.plugins.environments import ClusterEnvironment\n\nclass MyCluster(ClusterEnvironment):\n    def main_address(self):\n        return your_main_address\n\n    def main_port(self):\n        return your_main_port\n\n    def world_size(self):\n        return the_world_size\n\ntrainer = Trainer(plugins=[MyCluster()], ...)\n```\n#### precision\n\nThere are two different techniques to set the mixed precision. \"True\" precision and \"Mixed\" precision.\n\nLightning supports doing floating point operations in 64-bit precision (\"double\"), 32-bit precision (\"full\"), or 16-bit (\"half\") with both regular and bfloat16).\nThis selected precision will have a direct impact in the performance and memory usage based on your hardware.\nAutomatic mixed precision settings are denoted by a \"-mixed\" suffix, while \"true\" precision settings have a \"-true\" suffix:\n\n```python\n# Default used by the Trainer\nfabric = Fabric(precision=\"32-true\", devices=1)\n\n# the same as:\ntrainer = Trainer(precision=\"32\", devices=1)\n\n# 16-bit mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"16-mixed\", devices=1)\n\n# 16-bit bfloat mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"bf16-mixed\", devices=1)\n\n# 8-bit mixed precision via TransformerEngine (model weights get cast to torch.bfloat16)\ntrainer = Trainer(precision=\"transformer-engine\", devices=1)\n\n# 16-bit precision (model weights get cast to torch.float16)\ntrainer = Trainer(precision=\"16-true\", devices=1)\n\n# 16-bit bfloat precision (model weights get cast to torch.bfloat16)\ntrainer = Trainer(precision=\"bf16-true\", devices=1)\n\n# 64-bit (double) precision (model weights get cast to torch.float64)\ntrainer = Trainer(precision=\"64-true\", devices=1)\n```\nSee the N-bit precision guide <../common/precision> for more details.\n\n#### profiler\n\nTo profile individual steps during training and assist in identifying bottlenecks.\n\nSee the profiler documentation <../tuning/profiler> for more details.\n\nfrom lightning.pytorch.profilers import SimpleProfiler, AdvancedProfiler\n\n# default used by the Trainer\ntrainer = Trainer(profiler=None)\n\n# to profile standard training events, equivalent to `profiler=SimpleProfiler()`\ntrainer = Trainer(profiler=\"simple\")\n\n# advanced profiler for function-level stats, equivalent to `profiler=AdvancedProfiler()`\ntrainer = Trainer(profiler=\"advanced\")\n#### reload_dataloaders_every_n_epochs\n\nSet to a positive integer to reload dataloaders every n epochs from your currently used data source.\nDataSource can be a LightningModule or a LightningDataModule.\n\n```python\n# if 0 (default)\ntrain_loader = model.train_dataloader()\n# or if using data module: datamodule.train_dataloaders()\nfor epoch in epochs:\n    for batch in train_loader:\n        ...\n\n# if a positive integer\nfor epoch in epochs:\n    if not epoch % reload_dataloaders_every_n_epochs:\n        train_loader = model.train_dataloader()\n        # or if using data module: datamodule.train_dataloader()\n    for batch in train_loader:\n        ...\n```\nThe pseudocode applies also to the val_dataloader.\n\n#### strategy\n\nSupports passing different training strategies with aliases (ddp, fsdp, etc) as well as configured strategies.\n\n```python\n# Data-parallel training with the DDP strategy on 4 GPUs\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Model-parallel training with the FSDP strategy on 4 GPUs\ntrainer = Trainer(strategy=\"fsdp\", accelerator=\"gpu\", devices=4)\n```\nAdditionally, you can pass a strategy object.\n\n```python\nfrom lightning.pytorch.strategies import DDPStrategy\n\ntrainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=2)\n```\nSee Also:\n- Multi GPU Training <multi_gpu>.\n- Model Parallel GPU training guide <../advanced/model_parallel>.\n- TPU training guide <../accelerators/tpu>.\n\n#### sync_batchnorm\n\nEnable synchronization between batchnorm layers across all GPUs.\n\ntrainer = Trainer(sync_batchnorm=True)\n#### use_distributed_sampler\n\nSee lightning.pytorch.trainer.Trainer.params.use_distributed_sampler.\n\n# default used by the Trainer\ntrainer = Trainer(use_distributed_sampler=True)\nBy setting to False, you have to add your own distributed sampler:\n\n```python\n# in your LightningModule or LightningDataModule\ndef train_dataloader(self):\n    dataset = ...\n    # default used by the Trainer\n    sampler = torch.utils.data.DistributedSampler(dataset, shuffle=True)\n    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n    return dataloader\n```\n#### val_check_interval\n\nHow often within one training epoch to check the validation set.\nCan specify as float, int, or a time-based duration.\n\n- pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch.\n- pass an int to check after a fixed number of training batches. An int value can only be higher than the number of training\nbatches when check_val_every_n_epoch=None, which validates after every N training batches across epochs or iteration-based training.\n- pass a string duration in the format \"DD:HH:MM:SS\", a datetime.timedelta object, or a dictionary of keyword arguments that can be passed\nto datetime.timedelta for time-based validation. When using a time-based duration, validation will trigger once the elapsed wall-clock time\nsince the last validation exceeds the interval. The validation check occurs after the current batch completes, the validation loop runs, and\nthe timer resets.\n\n**Time-based validation behavior with check_val_every_n_epoch:** When used together with val_check_interval (time-based) and\ncheck_val_every_n_epoch > 1, validation is aligned to epoch multiples:\n\n- If the time-based interval elapses **before** the next multiple-N epoch, validation runs at the start of that epoch (after the first batch),\nand the timer resets.\n- If the interval elapses **during** a multiple-N epoch, validation runs after the current batch.\n- For cases where check_val_every_n_epoch=None or 1, the time-based behavior of val_check_interval applies without additional alignment.\n\n# default used by the Trainer\ntrainer = Trainer(val_check_interval=1.0)\n\n# check validation set 4 times during a training epoch\ntrainer = Trainer(val_check_interval=0.25)\n\n# check validation set every 1000 training batches in the current epoch\ntrainer = Trainer(val_check_interval=1000)\n\n# check validation set every 1000 training batches across complete epochs or during iteration-based training\n# use this when using iterableDataset and your dataset has no length\n# (ie: production cases with streaming data)\ntrainer = Trainer(val_check_interval=1000, check_val_every_n_epoch=None)\n\n# check validation every 15 minutes of wall-clock time using a string-based approach\ntrainer = Trainer(val_check_interval=\"00:00:15:00\")\n\n# check validation every 15 minutes of wall-clock time using a dictionary-based approach\ntrainer = Trainer(val_check_interval={\"minutes\": 15})\n\n# check validation every 1 hour of wall-clock time using a dictionary-based approach\ntrainer = Trainer(val_check_interval={\"hours\": 1})\n\n# check validation every 1 hour of wall-clock time using a datetime.timedelta object\nfrom datetime import timedelta\ntrainer = Trainer(val_check_interval=timedelta(hours=1))\n```python\n# Here is the computation to estimate the total number of batches seen within an epoch.\n# This logic applies when `val_check_interval` is specified as an integer or a float.\n\n# Find the total number of train batches\ntotal_train_batches = total_train_samples // (train_batch_size * world_size)\n\n# Compute how many times we will call validation during the training loop\nval_check_batch = max(1, int(total_train_batches * val_check_interval))\nval_checks_per_epoch = total_train_batches / val_check_batch\n\n# Find the total number of validation batches\ntotal_val_batches = total_val_samples // (val_batch_size * world_size)\n\n# Total number of batches run\ntotal_fit_batches = total_train_batches + total_val_batches\n```\n-----\n\n###### Trainer class API\n\n#### Methods",
    "parent_index": 26,
    "index": 117
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "callback_metrics",
    "text": "The metrics available to callbacks.\n\nThis includes metrics logged via ~lightning.pytorch.core.LightningModule.log.\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.log(\"a_val\", 2.0)\n\ncallback_metrics = trainer.callback_metrics\nassert callback_metrics[\"a_val\"] == 2.0\n```",
    "parent_index": 26,
    "index": 118
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "logged_metrics",
    "text": "The metrics sent to the loggers.\n\nThis includes metrics logged via ~lightning.pytorch.core.LightningModule.log with the\n~lightning.pytorch.core.LightningModule.log.logger argument set.",
    "parent_index": 26,
    "index": 119
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "progress_bar_metrics",
    "text": "The metrics sent to the progress bar.\n\nThis includes metrics logged via ~lightning.pytorch.core.LightningModule.log with the\n~lightning.pytorch.core.LightningModule.log.prog_bar argument set.",
    "parent_index": 26,
    "index": 120
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "current_epoch",
    "text": "The current epoch, updated after the epoch end hooks are run.",
    "parent_index": 26,
    "index": 121
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "datamodule",
    "text": "The current datamodule, which is used by the trainer.\n\n```python\nused_datamodule = trainer.datamodule\n```",
    "parent_index": 26,
    "index": 122
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "global_step",
    "text": "The number of optimizer steps taken (does not reset each epoch).\n\nThis includes multiple optimizers (if enabled).",
    "parent_index": 26,
    "index": 123
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "loggers",
    "text": "The list of ~lightning.pytorch.loggers.logger.Logger used.\n\n```python\nfor logger in trainer.loggers:\n    logger.log_metrics({\"foo\": 1.0})\n```",
    "parent_index": 26,
    "index": 124
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "log_dir",
    "text": "The directory for the current experiment. Use this to save images to, etc...\n\n```python\ndef training_step(self, batch, batch_idx):\n    img = ...\n    save_img(img, self.trainer.log_dir)\n```",
    "parent_index": 26,
    "index": 125
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "is_global_zero",
    "text": "Whether this process is the global zero in multi-node training.\n\n```python\ndef training_step(self, batch, batch_idx):\n    if self.trainer.is_global_zero:\n        print(\"in node 0, accelerator 0\")\n```",
    "parent_index": 26,
    "index": 126
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "estimated_stepping_batches",
    "text": "The estimated number of batches that will optimizer.step() during training.\n\nThis accounts for gradient accumulation and the current trainer configuration. This might sets up your training\ndataloader if hadn't been set up already.\n\n```python\ndef configure_optimizers(self):\n    optimizer = ...\n    stepping_batches = self.trainer.estimated_stepping_batches\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n    }\n```",
    "parent_index": 26,
    "index": 127
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "state",
    "text": "The current state of the Trainer, including the current function that is running, the stage of\nexecution within that function, and the status of the Trainer.\n\n```python\n# fn in (\"fit\", \"validate\", \"test\", \"predict\")\ntrainer.state.fn\n# status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\ntrainer.state.status\n# stage in (\"train\", \"sanity_check\", \"validate\", \"test\", \"predict\")\ntrainer.state.stage\n```",
    "parent_index": 26,
    "index": 128
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "should_stop",
    "text": "If you want to terminate the training during .fit, you can set trainer.should_stop=True to terminate the training\nas soon as possible. Note that, it will respect the arguments min_steps and min_epochs to check whether to stop. If these\narguments are set and the current_epoch or global_step don't meet these minimum conditions, training will continue until\nboth conditions are met. If any of these arguments is not set, it won't be considered for the final decision.\n\n```python\n# setting `trainer.should_stop` at any point of training will terminate it\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        self.trainer.should_stop = True\n\ntrainer = Trainer()\nmodel = LitModel()\ntrainer.fit(model)\n```\n```python\n# setting `trainer.should_stop` will stop training only after at least 5 epochs have run\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.current_epoch == 2:\n            self.trainer.should_stop = True\n\ntrainer = Trainer(min_epochs=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```\n```python\n# setting `trainer.should_stop` will stop training only after at least 5 steps have run\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.global_step == 2:\n            self.trainer.should_stop = True\n\ntrainer = Trainer(min_steps=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```\n```python\n# setting `trainer.should_stop` at any until both min_steps and min_epochs are satisfied\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.global_step == 7:\n            self.trainer.should_stop = True\n\ntrainer = Trainer(min_steps=5, min_epochs=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```",
    "parent_index": 26,
    "index": 129
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "sanity_checking",
    "text": "Indicates if the trainer is currently running sanity checking. This property can be useful to disable some hooks,\nlogging or callbacks during the sanity checking.\n\n```python\ndef validation_step(self, batch, batch_idx):\n    ...\n    if not self.trainer.sanity_checking:\n        self.log(\"value\", value)\n```",
    "parent_index": 26,
    "index": 130
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "num_training_batches",
    "text": "The number of training batches that will be used during trainer.fit().",
    "parent_index": 26,
    "index": 131
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "num_sanity_val_batches",
    "text": "The number of validation batches that will be used during the sanity-checking part of trainer.fit().",
    "parent_index": 26,
    "index": 132
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "num_val_batches",
    "text": "The number of validation batches that will be used during trainer.fit() or trainer.validate().",
    "parent_index": 26,
    "index": 133
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "num_test_batches",
    "text": "The number of test batches that will be used during trainer.test().",
    "parent_index": 26,
    "index": 134
  },
  {
    "file": "docs/pytorch/stable/common/trainer.html",
    "label": "docs",
    "title": "num_predict_batches",
    "text": "The number of prediction batches that will be used during trainer.predict().",
    "parent_index": 26,
    "index": 135
  },
  {
    "file": "docs/pytorch/stable/data/access.html",
    "label": "docs",
    "title": "Accessing DataLoaders",
    "text": "In the case that you require access to the torch.utils.data.DataLoader or torch.utils.data.Dataset objects, DataLoaders for each step can be accessed\nvia the trainer properties ~lightning.pytorch.trainer.trainer.Trainer.train_dataloader,\n~lightning.pytorch.trainer.trainer.Trainer.val_dataloaders,\n~lightning.pytorch.trainer.trainer.Trainer.test_dataloaders, and\n~lightning.pytorch.trainer.trainer.Trainer.predict_dataloaders.\n\n```python\ndataloaders = trainer.train_dataloader\ndataloaders = trainer.val_dataloaders\ndataloaders = trainer.test_dataloaders\ndataloaders = trainer.predict_dataloaders\n```\nThese properties will match exactly what was returned in your *_dataloader hooks or passed to the Trainer,\nmeaning that if you returned a dictionary of dataloaders, these will return a dictionary of dataloaders.\n\n###### Replacing DataLoaders\n\nIf you are using a ~lightning.pytorch.utilities.CombinedLoader. A flattened list of DataLoaders can be accessed by doing:\n\n```python\nfrom lightning.pytorch.utilities import CombinedLoader\n\niterables = {\"dl1\": dl1, \"dl2\": dl2}\ncombined_loader = CombinedLoader(iterables)\n# access the original iterables\nassert combined_loader.iterables is iterables\n# the `.flattened` property can be convenient\nassert combined_loader.flattened == [dl1, dl2]\n# for example, to do a simple loop\nupdated = []\nfor dl in combined_loader.flattened:\n    new_dl = apply_some_transformation_to(dl)\n    updated.append(new_dl)\n# it also allows you to easily replace the dataloaders\ncombined_loader.flattened = updated\n```",
    "parent_index": 27,
    "index": 136
  },
  {
    "file": "docs/pytorch/stable/data/alternatives.html",
    "label": "docs",
    "title": "Using 3rd Party Data Iterables",
    "text": "When training a model on a specific task, data loading and preprocessing might become a bottleneck.\nLightning does not enforce a specific data loading approach nor does it try to control it.\nThe only assumption Lightning makes is that a valid iterable is provided.\n\nFor PyTorch-based programs, these iterables are typically instances of ~torch.utils.data.DataLoader.\nHowever, Lightning also supports other data types such as a list of batches, generators, or other custom iterables or\ncollections of the former.\n\n```python\n# random list of batches\ndata = [(torch.rand(32, 3, 32, 32), torch.randint(0, 10, (32,))) for _ in range(100)]\nmodel = LitClassifier()\ntrainer = Trainer()\ntrainer.fit(model, data)\n```\nBelow we showcase Lightning examples with packages that compete with the generic PyTorch DataLoader and might be\nfaster depending on your use case. They might require custom data serialization, loading, and preprocessing that\nis often hardware accelerated.\n\n#### StreamingDataset\n\nAs datasets grow in size and the number of nodes scales, loading training data can become a significant challenge.\nThe StreamingDataset_ can make training on large datasets from cloud storage\nas fast, cheap, and scalable as possible.\n\nThis library uses a custom built ~torch.utils.data.IterableDataset. The library recommends iterating through it\nvia a regular ~torch.utils.data.DataLoader. This means that support in the Trainer is seamless:\n\n```python\nimport lightning as L\nfrom streaming import MDSWriter, StreamingDataset\n\nclass YourDataset(StreamingDataset):\n    ...\n\n# you could do this in the `prepare_data` hook too\nwith MDSWriter(out=\"...\", columns=...) as out:\n    out.write(...)\n\ntrain_dataset = YourDataset()\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size)\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n#### FFCV\n\nTaking the example from the FFCV_ readme, we can use it with Lightning\nby just removing the hardcoded ToDevice(0) as Lightning takes care of GPU placement. In case you want to use some\ndata transformations on GPUs, change the ToDevice(0) to ToDevice(self.trainer.local_rank) to correctly map to\nthe desired GPU in your pipeline. When moving data to a specific device, you can always refer to\nself.trainer.local_rank to get the accelerator used by the current process.\n\n```python\nimport lightning as L\nfrom ffcv.loader import Loader, OrderOption\nfrom ffcv.transforms import ToTensor, ToDevice, ToTorchImage, Cutout\nfrom ffcv.fields.decoders import IntDecoder, RandomResizedCropRGBImageDecoder\n\n# Random resized crop\ndecoder = RandomResizedCropRGBImageDecoder((224, 224))\n# Data decoding and augmentation\nimage_pipeline = [decoder, Cutout(), ToTensor(), ToTorchImage()]\nlabel_pipeline = [IntDecoder(), ToTensor()]\n# Pipeline for each data field\npipelines = {\"image\": image_pipeline, \"label\": label_pipeline}\n# Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\ntrain_dataloader = Loader(\n    write_path, batch_size=bs, num_workers=num_workers, order=OrderOption.RANDOM, pipelines=pipelines\n)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n#### WebDataset\n\nThe WebDataset_ makes it easy to write I/O pipelines for large datasets.\nDatasets can be stored locally or in the cloud. WebDataset is just an instance of a standard IterableDataset.\nThe webdataset library contains a small wrapper (WebLoader) that adds a fluid interface to the DataLoader (and is otherwise identical).\n\n```python\nimport lightning as L\nimport webdataset as wds\n\ndataset = wds.WebDataset(\n    urls,\n    # needed for multi-gpu or multi-node training\n    workersplitter=wds.shardlists.split_by_worker,\n    nodesplitter=wds.shardlists.split_by_node,\n)\ntrain_dataloader = wds.WebLoader(dataset)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\nYou can find a complete example here_.\n\n#### NVIDIA DALI\n\nBy just changing device_id=0 to device_id=self.trainer.local_rank we can also leverage DALI's GPU decoding:\n\n```python\nimport lightning as L\nfrom nvidia.dali.pipeline import pipeline_def\nimport nvidia.dali.types as types\nimport nvidia.dali.fn as fn\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\nimport os\n\n# To run with different data, see documentation of nvidia.dali.fn.readers.file\n# points to https://github.com/NVIDIA/DALI_extra\ndata_root_dir = os.environ[\"DALI_EXTRA_PATH\"]\nimages_dir = os.path.join(data_root_dir, \"db\", \"single\", \"jpeg\")\n\n@pipeline_def(num_threads=4, device_id=self.trainer.local_rank)\ndef get_dali_pipeline():\n    images, labels = fn.readers.file(file_root=images_dir, random_shuffle=True, name=\"Reader\")\n    # decode data on the GPU\n    images = fn.decoders.image_random_crop(images, device=\"mixed\", output_type=types.RGB)\n    # the rest of processing happens on the GPU as well\n    images = fn.resize(images, resize_x=256, resize_y=256)\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_h=224,\n        crop_w=224,\n        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n        mirror=fn.random.coin_flip(),\n    )\n    return images, labels\n\ntrain_dataloader = DALIGenericIterator(\n    [get_dali_pipeline(batch_size=16)],\n    [\"data\", \"label\"],\n    reader_name=\"Reader\",\n)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\nYou can find a complete tutorial here_.\n\n###### Limitations\nLightning works with all kinds of custom data iterables as shown above. There are, however, a few features that cannot\nbe supported this way. These restrictions come from the fact that for their support,\nLightning needs to know a lot on the internals of these iterables.\n\n- In a distributed multi-GPU setting (ddp), Lightning wraps the DataLoader's sampler with a wrapper for distributed\nsupport. This makes sure that each GPU sees a different part of the dataset. As sampling can be implemented in\narbitrary ways with custom iterables, Lightning might not be able to do this for you. If this is the case, you can use\nthe ~lightning.pytorch.trainer.trainer.Trainer.use_distributed_sampler argument to disable this logic and\nset the distributed sampler yourself.",
    "parent_index": 28,
    "index": 137
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "Why do I need a DataModule?",
    "text": "In normal PyTorch code, the data cleaning/preparation is usually scattered across many files. This makes\nsharing and reusing the exact splits and transforms across projects impossible.\n\nDatamodules are for you if you ever asked the questions:\n\n- what splits did you use?\n- what transforms did you use?\n- what normalization did you use?\n- how did you prepare/tokenize the data?\n\n--------------",
    "parent_index": 29,
    "index": 138
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "What is a DataModule?",
    "text": "The ~lightning.pytorch.core.datamodule.LightningDataModule is a convenient way to manage data in PyTorch Lightning.\nIt encapsulates training, validation, testing, and prediction dataloaders, as well as any necessary steps for data processing,\ndownloads, and transformations. By using a ~lightning.pytorch.core.datamodule.LightningDataModule, you can\neasily develop dataset-agnostic models, hot-swap different datasets, and share data splits and transformations across projects.\n\nHere's a simple PyTorch example:\n\n```python\n# regular PyTorch\ntest_data = MNIST(my_path, train=False, download=True)\npredict_data = MNIST(my_path, train=False, download=True)\ntrain_data = MNIST(my_path, train=True, download=True)\ntrain_data, val_data = random_split(train_data, [55000, 5000])\n\ntrain_loader = DataLoader(train_data, batch_size=32)\nval_loader = DataLoader(val_data, batch_size=32)\ntest_loader = DataLoader(test_data, batch_size=32)\npredict_loader = DataLoader(predict_data, batch_size=32)\n```\nThe equivalent DataModule just organizes the same exact code, but makes it reusable across projects.\n\n```python\nclass MNISTDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n    def setup(self, stage: str):\n        self.mnist_test = MNIST(self.data_dir, train=False)\n        self.mnist_predict = MNIST(self.data_dir, train=False)\n        mnist_full = MNIST(self.data_dir, train=True)\n        self.mnist_train, self.mnist_val = random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n\n    def teardown(self, stage: str):\n        # Used to clean-up when the run is finished\n        ...\n```\nBut now, as the complexity of your processing grows (transforms, multiple-GPU training), you can\nlet Lightning handle those details for you while making this dataset reusable so you can share with\ncolleagues or use in different projects.\n\n```python\nmnist = MNISTDataModule(my_path)\nmodel = LitClassifier()\n\ntrainer = Trainer()\ntrainer.fit(model, mnist)\n```\nHere's a more realistic, complex DataModule that shows how much more reusable the datamodule is.\n\n```python\nimport lightning as L\nfrom torch.utils.data import random_split, DataLoader\n\n# Note - you must have torchvision installed for this example\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\nclass MNISTDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str = \"./\"):\n        super().__init__()\n        self.data_dir = data_dir\n        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\":\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n            )\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\":\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n        if stage == \"predict\":\n            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=32)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=32)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=32)\n\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=32)\n```\n---------------",
    "parent_index": 29,
    "index": 139
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "LightningDataModule API",
    "text": "To define a DataModule the following methods are used to create train/val/test/predict dataloaders:\n\n- prepare_data<data/datamodule:prepare_data> (how to download, tokenize, etc...)\n- setup<data/datamodule:setup> (how to split, define dataset, etc...)\n- train_dataloader<data/datamodule:train_dataloader>\n- val_dataloader<data/datamodule:val_dataloader>\n- test_dataloader<data/datamodule:test_dataloader>\n- predict_dataloader<data/datamodule:predict_dataloader>",
    "parent_index": 29,
    "index": 140
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "prepare_data",
    "text": "Downloading and saving data with multiple processes (distributed settings) will result in corrupted data. Lightning\nensures the ~lightning.pytorch.core.hooks.DataHooks.prepare_data is called only within a single process on CPU,\nso you can safely add your downloading logic within. In case of multi-node training, the execution of this hook\ndepends upon prepare_data_per_node<data/datamodule:prepare_data_per_node>. ~lightning.pytorch.core.hooks.DataHooks.setup is called after\nprepare_data and there is a barrier in between which ensures that all the processes proceed to setup once the data is prepared and available for use.\n\n- download, i.e. download data only once on the disk from a single process\n- tokenize. Since it's a one time process, it is not recommended to do it on all processes\n- etc...\n\n```python\nclass MNISTDataModule(L.LightningDataModule):\n    def prepare_data(self):\n        # download\n        MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n        MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n```\nprepare_data is called from the main process. It is not recommended to assign state here (e.g. self.x = y) since it is called on a single process and if you assign\nstates here then they won't be available for other processes.",
    "parent_index": 29,
    "index": 141
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "setup",
    "text": "There are also data operations you might want to perform on every GPU. Use ~lightning.pytorch.core.hooks.DataHooks.setup to do things like:\n\n- count number of classes\n- build vocabulary\n- perform train/val/test splits\n- create datasets\n- apply transforms (defined explicitly in your datamodule)\n- etc...\n\n```python\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def setup(self, stage: str):\n        # Assign Train/val split(s) for use in Dataloaders\n        if stage == \"fit\":\n            mnist_full = MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n            )\n\n        # Assign Test split(s) for use in Dataloaders\n        if stage == \"test\":\n            self.mnist_test = MNIST(self.data_dir, train=False, download=True, transform=self.transform)\n```\nFor eg., if you are working with NLP task where you need to tokenize the text and use it, then you can do something like as follows:\n\n```python\nclass LitDataModule(L.LightningDataModule):\n    def prepare_data(self):\n        dataset = load_Dataset(...)\n        train_dataset = ...\n        val_dataset = ...\n        # tokenize\n        # save it to disk\n\n    def setup(self, stage):\n        # load it back here\n        dataset = load_dataset_from_disk(...)\n```\nThis method expects a stage argument.\nIt is used to separate setup logic for trainer.{fit,validate,test,predict}.",
    "parent_index": 29,
    "index": 142
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "train_dataloader",
    "text": "Use the ~lightning.pytorch.core.hooks.DataHooks.train_dataloader method to generate the training dataloader(s).\nUsually you just wrap the dataset you defined in setup<data/datamodule:setup>. This is the dataloader that the Trainer\n~lightning.pytorch.trainer.trainer.Trainer.fit method uses.\n\n```python\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=64)\n```",
    "parent_index": 29,
    "index": 143
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "val_dataloader",
    "text": "Use the ~lightning.pytorch.core.hooks.DataHooks.val_dataloader method to generate the validation dataloader(s).\nUsually you just wrap the dataset you defined in setup<data/datamodule:setup>. This is the dataloader that the Trainer\n~lightning.pytorch.trainer.trainer.Trainer.fit and ~lightning.pytorch.trainer.trainer.Trainer.validate methods uses.\n\n```python\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=64)\n```",
    "parent_index": 29,
    "index": 144
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "test_dataloader",
    "text": "Use the ~lightning.pytorch.core.hooks.DataHooks.test_dataloader method to generate the test dataloader(s).\nUsually you just wrap the dataset you defined in setup<data/datamodule:setup>. This is the dataloader that the Trainer\n~lightning.pytorch.trainer.trainer.Trainer.test method uses.\n\n```python\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=64)\n```",
    "parent_index": 29,
    "index": 145
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "predict_dataloader",
    "text": "Use the ~lightning.pytorch.core.hooks.DataHooks.predict_dataloader method to generate the prediction dataloader(s).\nUsually you just wrap the dataset you defined in setup<data/datamodule:setup>. This is the dataloader that the Trainer\n~lightning.pytorch.trainer.trainer.Trainer.predict method uses.\n\n```python\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=64)\n```",
    "parent_index": 29,
    "index": 146
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "prepare_data_per_node",
    "text": "If set to True will call prepare_data() on LOCAL_RANK=0 for every node.\nIf set to False will only call from NODE_RANK=0, LOCAL_RANK=0.\n\nclass LitDataModule(LightningDataModule):\ndef __init__(self):\nsuper().__init__()\nself.prepare_data_per_node = True\n------------------",
    "parent_index": 29,
    "index": 147
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "Using a DataModule",
    "text": "The recommended way to use a DataModule is simply:\n\n```python\ndm = MNISTDataModule()\nmodel = Model()\ntrainer.fit(model, datamodule=dm)\ntrainer.test(datamodule=dm)\ntrainer.validate(datamodule=dm)\ntrainer.predict(datamodule=dm)\n```\nIf you need information from the dataset to build your model, then run\nprepare_data<data/datamodule:prepare_data> and\nsetup<data/datamodule:setup> manually (Lightning ensures\nthe method runs on the correct devices).\n\n```python\ndm = MNISTDataModule()\ndm.prepare_data()\ndm.setup(stage=\"fit\")\n\nmodel = Model(num_classes=dm.num_classes, width=dm.width, vocab=dm.vocab)\ntrainer.fit(model, dm)\n\ndm.setup(stage=\"test\")\ntrainer.test(datamodule=dm)\n```\nYou can access the current used datamodule of a trainer via trainer.datamodule and the current used\ndataloaders via the trainer properties ~lightning.pytorch.trainer.trainer.Trainer.train_dataloader,\n~lightning.pytorch.trainer.trainer.Trainer.val_dataloaders,\n~lightning.pytorch.trainer.trainer.Trainer.test_dataloaders, and\n~lightning.pytorch.trainer.trainer.Trainer.predict_dataloaders.\n\n----------------",
    "parent_index": 29,
    "index": 148
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "DataModules without Lightning",
    "text": "You can of course use DataModules in plain PyTorch code as well.\n\n```python\n# download, etc...\ndm = MNISTDataModule()\ndm.prepare_data()\n\n# splits/transforms\ndm.setup(stage=\"fit\")\n\n# use data\nfor batch in dm.train_dataloader():\n    ...\n\nfor batch in dm.val_dataloader():\n    ...\n\ndm.teardown(stage=\"fit\")\n\n# lazy load test data\ndm.setup(stage=\"test\")\nfor batch in dm.test_dataloader():\n    ...\n\ndm.teardown(stage=\"test\")\n```\nBut overall, DataModules encourage reproducibility by allowing all details of a dataset to be specified in a unified\nstructure.\n\n----------------",
    "parent_index": 29,
    "index": 149
  },
  {
    "file": "docs/pytorch/stable/data/datamodule.html",
    "label": "docs",
    "title": "Hyperparameters in DataModules",
    "text": "Like LightningModules, DataModules support hyperparameters with the same API.\n\n```python\nimport lightning as L\n\nclass CustomDataModule(L.LightningDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n    def configure_optimizers(self):\n        # access the saved hyperparameters\n        opt = optim.Adam(self.parameters(), lr=self.hparams.lr)\n```\nRefer to save_hyperparameters in lightning module <../common/lightning_module> for more details.\n\n----",
    "parent_index": 29,
    "index": 150
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_advanced.html",
    "label": "docs",
    "title": "Debug distributed models",
    "text": "To debug a distributed model, we recommend you debug it locally by running the distributed version on CPUs:\n\n```python\ntrainer = Trainer(accelerator=\"cpu\", strategy=\"ddp\", devices=2)\n```\nOn the CPU, you can use pdb or breakpoint()\nor use regular print statements.\n\nclass LitModel(LightningModule):\ndef training_step(self, batch, batch_idx):\ndebugging_message = ...\nprint(f\"RANK - {self.trainer.global_rank}: {debugging_message}\")\n\nif self.trainer.global_rank == 0:\nimport pdb\n\npdb.set_trace()\n\n# to prevent other processes from moving forward until all processes are in sync\nself.trainer.strategy.barrier()\nWhen everything works, switch back to GPU by changing only the accelerator.\n\n```python\ntrainer = Trainer(accelerator=\"gpu\", strategy=\"ddp\", devices=2)\n```",
    "parent_index": 30,
    "index": 151
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "How does Lightning help me debug ?",
    "text": "The Lightning Trainer has *a lot* of arguments devoted to maximizing your debugging productivity.\n\n----",
    "parent_index": 31,
    "index": 152
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Set a breakpoint",
    "text": "A breakpoint stops your code execution so you can inspect variables, etc... and allow your code to execute one line at a time.\n\n```python\ndef function_to_debug():\n    x = 2\n\n    # set breakpoint\n    breakpoint()\n    y = x**2\n```\nIn this example, the code will stop before executing the y = x**2 line.\n\n----",
    "parent_index": 31,
    "index": 153
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Run all your model code once quickly",
    "text": "If you've ever trained a model for days only to crash during validation or testing then this trainer argument is about to become your best friend.\n\nThe ~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run argument in the trainer runs 5 batch of training, validation, test and prediction data through your trainer to see if there are any bugs:\n\n```python\ntrainer = Trainer(fast_dev_run=True)\n```\nTo change how many batches to use, change the argument to an integer. Here we run 7 batches of each:\n\n```python\ntrainer = Trainer(fast_dev_run=7)\n```\nThis argument will disable tuner, checkpoint callbacks, early stopping callbacks,\nloggers and logger callbacks like ~lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor and\n~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor.\n----",
    "parent_index": 31,
    "index": 154
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Shorten the epoch length",
    "text": "Sometimes it's helpful to only use a fraction of your training, val, test, or predict data (or a set number of batches).\nFor example, you can use 20% of the training set and 1% of the validation set.\n\nOn larger datasets like Imagenet, this can help you debug or test a few things faster than waiting for a full epoch.\n\n# use only 10% of training data and 1% of val data\ntrainer = Trainer(limit_train_batches=0.1, limit_val_batches=0.01)\n\n# use 10 batches of train and 5 batches of val\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=5)\n----",
    "parent_index": 31,
    "index": 155
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Run a Sanity Check",
    "text": "Lightning runs **2** steps of validation in the beginning of training.\nThis avoids crashing in the validation loop sometime deep into a lengthy training loop.\n\n(See: ~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps\nargument of ~lightning.pytorch.trainer.trainer.Trainer)\n\ntrainer = Trainer(num_sanity_val_steps=2)\n----",
    "parent_index": 31,
    "index": 156
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Print LightningModule weights summary",
    "text": "Whenever the .fit() function gets called, the Trainer will print the weights summary for the LightningModule.\n\n```python\ntrainer.fit(...)\n```\nthis generate a table like:\n\n```text\n  | Name  | Type        | Params | Mode\n-------------------------------------------\n0 | net   | Sequential  | 132 K  | train\n1 | net.0 | Linear      | 131 K  | train\n2 | net.1 | BatchNorm1d | 1.0 K  | train\n```\nTo add the child modules to the summary add a ~lightning.pytorch.callbacks.model_summary.ModelSummary:\n\nfrom lightning.pytorch.callbacks import ModelSummary\n\ntrainer = Trainer(callbacks=[ModelSummary(max_depth=-1)])\nTo print the model summary if .fit() is not called:\n\n```python\nfrom lightning.pytorch.utilities.model_summary import ModelSummary\n\nmodel = LitModel()\nsummary = ModelSummary(model, max_depth=-1)\nprint(summary)\n```\nTo turn off the autosummary use:\n\n```python\ntrainer = Trainer(enable_model_summary=False)\n```\n----",
    "parent_index": 31,
    "index": 157
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_basic.html",
    "label": "docs",
    "title": "Print input output layer dimensions",
    "text": "Another debugging tool is to display the intermediate input- and output sizes of all your layers by setting the\nexample_input_array attribute in your LightningModule.\n\n```python\nclass LitModel(LightningModule):\n    def __init__(self, *args, **kwargs):\n        self.example_input_array = torch.Tensor(32, 1, 28, 28)\n```\nWith the input array, the summary table will include the input and output layer dimensions:\n\n```text\n  | Name  | Type        | Params | Mode  | In sizes  | Out sizes\n----------------------------------------------------------------------\n0 | net   | Sequential  | 132 K  | train | [10, 256] | [10, 512]\n1 | net.0 | Linear      | 131 K  | train | [10, 256] | [10, 512]\n2 | net.1 | BatchNorm1d | 1.0 K  | train | [10, 512] | [10, 512]\n```\nwhen you call .fit() on the Trainer. This can help you find bugs in the composition of your layers.",
    "parent_index": 31,
    "index": 158
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_intermediate.html",
    "label": "docs",
    "title": "Why should I debug ML code?",
    "text": "Machine learning code requires debugging mathematical correctness, which is not something non-ML code has to deal with. Lightning implements a few best-practice techniques to give all users, expert level ML debugging abilities.\n\n----",
    "parent_index": 32,
    "index": 159
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_intermediate.html",
    "label": "docs",
    "title": "Overfit your model on a Subset of Data",
    "text": "A good debugging technique is to take a tiny portion of your data (say 2 samples per class),\nand try to get your model to overfit. If it can't, it's a sign it won't work with large datasets.\n\n(See: ~lightning.pytorch.trainer.trainer.Trainer.overfit_batches\nargument of ~lightning.pytorch.trainer.trainer.Trainer)\n\n# use only 1% of training data\ntrainer = Trainer(overfit_batches=0.01)\n\n# similar, but with a fixed 10 batches\ntrainer = Trainer(overfit_batches=10)\n\n# equivalent to\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=10)\nSetting overfit_batches is the same as setting limit_train_batches and limit_val_batches to the same value, but in addition will also turn off shuffling in the training dataloader.\n\n----",
    "parent_index": 32,
    "index": 160
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_intermediate.html",
    "label": "docs",
    "title": "Look-out for exploding gradients",
    "text": "One major problem that plagues models is exploding gradients.\nGradient clipping is one technique that can help keep gradients from exploding.\n\nYou can keep an eye on the gradient norm by logging it in your LightningModule:\n\n```python\nfrom lightning.pytorch.utilities import grad_norm\n\ndef on_before_optimizer_step(self, optimizer):\n    # Compute the 2-norm for each layer\n    # If using mixed precision, the gradients are already unscaled here\n    norms = grad_norm(self.layer, norm_type=2)\n    self.log_dict(norms)\n```\nThis will plot the 2-norm of each layer to your experiment manager.\nIf you notice the norm is going up, there's a good chance your gradients will explode.\n\nOne technique to stop exploding gradients is to clip the gradient when the norm is above a certain threshold:\n\n# DEFAULT (ie: don't clip)\ntrainer = Trainer(gradient_clip_val=0)\n\n# clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\ntrainer = Trainer(gradient_clip_val=0.5)\n\n# clip gradients' maximum magnitude to <=0.5\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n----",
    "parent_index": 32,
    "index": 161
  },
  {
    "file": "docs/pytorch/stable/debug/debugging_intermediate.html",
    "label": "docs",
    "title": "Detect autograd anomalies",
    "text": "Lightning helps you detect anomalies in the PyTorh autograd engine via PyTorch's built-in\nAnomaly Detection Context-manager.\n\nEnable it via the **detect_anomaly** trainer argument:\n\ntrainer = Trainer(detect_anomaly=True)",
    "parent_index": 32,
    "index": 162
  },
  {
    "file": "docs/pytorch/stable/extensions/callbacks.html",
    "label": "docs",
    "title": "Built-in Callbacks",
    "text": "Lightning has a few built-in callbacks.\n\nFor a richer collection of callbacks, check out our\nbolts library.\nBackboneFinetuning\nBaseFinetuning\nBasePredictionWriter\nBatchSizeFinder\nCallback\nDeviceStatsMonitor\nEarlyStopping\nGradientAccumulationScheduler\nLambdaCallback\nLearningRateFinder\nLearningRateMonitor\nModelCheckpoint\nModelPruning\nModelSummary\nProgressBar\nRichModelSummary\nRichProgressBar\nStochasticWeightAveraging\nTimer\nTQDMProgressBar\nWeightAveraging\n----------\n\n----------",
    "parent_index": 34,
    "index": 163
  },
  {
    "file": "docs/pytorch/stable/extensions/callbacks.html",
    "label": "docs",
    "title": "Best Practices",
    "text": "The following are best practices when using/designing callbacks.\n\n1. Callbacks should be isolated in their functionality.\n2. Your callback should not rely on the behavior of other callbacks in order to work properly.\n3. Do not manually call methods from the callback.\n4. Directly calling methods (eg. `on_validation_end`) is strongly discouraged.\n5. Whenever possible, your callbacks should not depend on the order in which they are executed.\n\n-----------\n\n-----------",
    "parent_index": 34,
    "index": 164
  },
  {
    "file": "docs/pytorch/stable/extensions/callbacks.html",
    "label": "docs",
    "title": "Callback API",
    "text": "Here is the full API of methods available in the Callback base class.\n\nThe ~lightning.pytorch.callbacks.Callback class is the base for all the callbacks in Lightning just like the ~lightning.pytorch.core.LightningModule is the base for all models.\nIt defines a public interface that each callback implementation must follow, the key ones are:",
    "parent_index": 34,
    "index": 165
  },
  {
    "file": "docs/pytorch/stable/extensions/callbacks.html",
    "label": "docs",
    "title": "Hooks",
    "text": "#### setup\n\n#### teardown\n\n#### on_fit_start\n\n#### on_fit_end\n\n#### on_sanity_check_start\n\n#### on_sanity_check_end\n\n#### on_train_batch_start\n\n#### on_train_batch_end\n\n#### on_train_epoch_start\n\n#### on_train_epoch_end\n\n#### on_validation_epoch_start\n\n#### on_validation_epoch_end\n\n#### on_test_epoch_start\n\n#### on_test_epoch_end\n\n#### on_predict_epoch_start\n\n#### on_predict_epoch_end\n\n#### on_validation_batch_start\n\n#### on_validation_batch_end\n\n#### on_test_batch_start\n\n#### on_test_batch_end\n\n#### on_predict_batch_start\n\n#### on_predict_batch_end\n\n#### on_train_start\n\n#### on_train_end\n\n#### on_validation_start\n\n#### on_validation_end\n\n#### on_test_start\n\n#### on_test_end\n\n#### on_predict_start\n\n#### on_predict_end\n\n#### on_exception\n\n#### state_dict\n\n#### on_save_checkpoint\n\n#### load_state_dict\n\n#### on_load_checkpoint\n\n#### on_before_backward\n\n#### on_after_backward\n\n#### on_before_optimizer_step\n\n#### on_before_zero_grad",
    "parent_index": 34,
    "index": 166
  },
  {
    "file": "docs/pytorch/stable/extensions/callbacks_state.html",
    "label": "docs",
    "title": "Save Callback state",
    "text": "Some callbacks require internal state in order to function properly. You can optionally\nchoose to persist your callback's state as part of model checkpoint files using\n~lightning.pytorch.callbacks.Callback.state_dict and ~lightning.pytorch.callbacks.Callback.load_state_dict.\nNote that the returned state must be able to be pickled.\n\nWhen your callback is meant to be used only as a singleton callback then implementing the above two hooks is enough\nto persist state effectively. However, if passing multiple instances of the callback to the Trainer is supported, then\nthe callback must define a ~lightning.pytorch.callbacks.Callback.state_key property in order for Lightning\nto be able to distinguish the different states when loading the callback state. This concept is best illustrated by\nthe following example.\n\nclass Counter(Callback):\ndef __init__(self, what=\"epochs\", verbose=True):\nself.what = what\nself.verbose = verbose\nself.state = {\"epochs\": 0, \"batches\": 0}\n\n@property\ndef state_key(self) -> str:\n# note: we do not include `verbose` here on purpose\nreturn f\"Counter[what={self.what}]\"\n\ndef on_train_epoch_end(self, *args, **kwargs):\nif self.what == \"epochs\":\nself.state[\"epochs\"] += 1\n\ndef on_train_batch_end(self, *args, **kwargs):\nif self.what == \"batches\":\nself.state[\"batches\"] += 1\n\ndef load_state_dict(self, state_dict):\nself.state.update(state_dict)\n\ndef state_dict(self):\nreturn self.state.copy()\n\n# two callbacks of the same type are being used\ntrainer = Trainer(callbacks=[Counter(what=\"epochs\"), Counter(what=\"batches\")])\nA Lightning checkpoint from this Trainer with the two stateful callbacks will include the following information:\n\n```\n{\n    \"state_dict\": ...,\n    \"callbacks\": {\n        \"Counter{'what': 'batches'}\": {\"batches\": 32, \"epochs\": 0},\n        \"Counter{'what': 'epochs'}\": {\"batches\": 0, \"epochs\": 2},\n        ...\n    }\n}\n```\nThe implementation of a ~lightning.pytorch.callbacks.Callback.state_key is essential here. If it were missing,\nLightning would not be able to disambiguate the state for these two callbacks, and ~lightning.pytorch.callbacks.Callback.state_key\nby default only defines the class name as the key, e.g., here Counter.",
    "parent_index": 35,
    "index": 167
  },
  {
    "file": "docs/pytorch/stable/extensions/datamodules_state.html",
    "label": "docs",
    "title": "Save DataModule state",
    "text": "When a checkpoint is created, it asks every DataModule for their state. If your DataModule defines the *state_dict* and *load_state_dict* methods, the checkpoint will automatically track and restore your DataModules.\n\n```python\nimport lightning as L\n\nclass LitDataModule(L.LightningDataModule):\n    def state_dict(self):\n        # track whatever you want here\n        state = {\"current_train_batch_index\": self.current_train_batch_index}\n        return state\n\n    def load_state_dict(self, state_dict):\n        # restore the state based on what you tracked in (def state_dict)\n        self.current_train_batch_index = state_dict[\"current_train_batch_index\"]\n```",
    "parent_index": 36,
    "index": 168
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Supported Loggers",
    "text": "The following are loggers we support:\n\nCometLogger\nCSVLogger\nMLFlowLogger\nNeptuneLogger\nTensorBoardLogger\nWandbLogger\nThe above loggers will normally plot an additional chart (**global_step VS epoch**). Depending on the loggers you use, there might be some additional charts too.\n\nBy default, Lightning uses TensorBoard logger under the hood, and stores the logs to a directory (by default in lightning_logs/).\n\nfrom lightning.pytorch import Trainer\n\n# Automatically logs to a directory (by default lightning_logs/)\ntrainer = Trainer()\nTo see your logs:\n\n```bash\ntensorboard --logdir=lightning_logs/\n```\nTo visualize tensorboard in a jupyter notebook environment, run the following command in a jupyter cell:\n\n```bash\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/\n```\nYou can also pass a custom Logger to the ~lightning.pytorch.trainer.trainer.Trainer.\n\nfrom lightning.pytorch import loggers as pl_loggers\n\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\ntrainer = Trainer(logger=tb_logger)\nChoose from any of the others such as MLflow, Comet, Neptune, WandB, etc.\n\n```python\ncomet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\ntrainer = Trainer(logger=comet_logger)\n```\nTo use multiple loggers, simply pass in a list or tuple of loggers.\n\n```python\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\ncomet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\ntrainer = Trainer(logger=[tb_logger, comet_logger])\n```\nBy default, Lightning logs every 50 steps. Use Trainer flags to logging_frequency.\nBy default, all loggers log to os.getcwd(). You can change the logging path using\nTrainer(default_root_dir=\"/your/path/to/save/checkpoints\") without instantiating a logger.\n----------",
    "parent_index": 37,
    "index": 169
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Logging from a LightningModule",
    "text": "Lightning offers automatic log functionalities for logging scalars, or manual logging for anything else.",
    "parent_index": 37,
    "index": 170
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Automatic Logging",
    "text": "Use the ~lightning.pytorch.core.LightningModule.log or ~lightning.pytorch.core.LightningModule.log_dict\nmethods to log from anywhere in a LightningModule <../common/lightning_module> and callbacks <../extensions/callbacks>.\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.log(\"my_metric\", x)\n\n# or a dict to log all metrics at once with individual plots\ndef training_step(self, batch, batch_idx):\n    self.log_dict({\"acc\": acc, \"recall\": recall})\n```\nEverything explained below applies to both ~lightning.pytorch.core.LightningModule.log or ~lightning.pytorch.core.LightningModule.log_dict methods.\nWhen using TorchMetrics with Lightning, we recommend referring to the TorchMetrics Lightning integration documentation for logging best practices, common pitfalls, and proper usage patterns.\nDepending on where the ~lightning.pytorch.core.LightningModule.log method is called, Lightning auto-determines\nthe correct logging mode for you. Of course you can override the default behavior by manually setting the\n~lightning.pytorch.core.LightningModule.log parameters.\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n```\nThe ~lightning.pytorch.core.LightningModule.log method has a few options:\n\n* on_step: Logs the metric at the current step.\n* on_epoch: Automatically accumulates and logs at the end of the epoch.\n* prog_bar: Logs to the progress bar (Default: False).\n* logger: Logs to the logger like Tensorboard, or any other custom logger passed to the ~lightning.pytorch.trainer.trainer.Trainer (Default: True).\n* reduce_fx: Reduction function over step values for end of epoch. Uses torch.mean by default and is not applied when a torchmetrics.Metric is logged.\n* enable_graph: If True, will not auto detach the graph.\n* sync_dist: If True, averages the metric across devices. Use with care as this may lead to a significant communication overhead.\n* sync_dist_group: The DDP group to sync across.\n* add_dataloader_idx: If True, appends the index of the current dataloader to the name (when using multiple dataloaders). If False, user needs to give unique names for each dataloader to not mix the values.\n* batch_size: Current batch size used for accumulating logs logged with on_epoch=True. This will be directly inferred from the loaded batch, but for some data structures you might need to explicitly provide it.\n* rank_zero_only: Set this to True only if you call self.log explicitly only from rank 0. If True you won't be able to access or specify this metric in callbacks (e.g. early stopping).\n\n* - Hook\n- on_step\n- on_epoch\n* - on_train_start, on_train_epoch_start, on_train_epoch_end\n- False\n- True\n* - on_before_backward, on_after_backward, on_before_optimizer_step, on_before_zero_grad\n- True\n- False\n* - on_train_batch_start, on_train_batch_end, training_step\n- True\n- False\n* - on_validation_start, on_validation_epoch_start, on_validation_epoch_end\n- False\n- True\n* - on_validation_batch_start, on_validation_batch_end, validation_step\n- False\n- True\nWhile logging tensor metrics with on_epoch=True inside step-level hooks and using mean-reduction (default) to accumulate the metrics across the current epoch, Lightning tries to extract the\nbatch size from the current batch. If multiple possible batch sizes are found, a warning is logged and if it fails to extract the batch size from the current batch, which is possible if\nthe batch is a custom structure/collection, then an error is raised. To avoid this, you can specify the batch_size inside the self.log(... batch_size=batch_size) call.\n\n.. code-block:: python\n\ndef training_step(self, batch, batch_idx):\n# extracts the batch size from `batch`\nself.log(\"train_loss\", loss, on_epoch=True)\n\ndef validation_step(self, batch, batch_idx):\n# uses `batch_size=10`\nself.log(\"val_loss\", loss, batch_size=10)\n- The above config for validation applies for test hooks as well.\n\n- Setting on_epoch=True will cache all your logged values during the full training epoch and perform a\nreduction in on_train_epoch_end. We recommend using TorchMetrics, when working with custom reduction.\n\n- Setting both on_step=True and on_epoch=True will create two keys per metric you log with\nsuffix _step and _epoch respectively. You can refer to these keys e.g. in the `monitor`\nargument of ~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint or in the graphs plotted to the logger of your choice.\nIf your work requires to log in an unsupported method, please open an issue with a clear description of why it is blocking you.",
    "parent_index": 37,
    "index": 171
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Manual Logging Non-Scalar Artifacts",
    "text": "If you want to log anything that is not a scalar, like histograms, text, images, etc., you may need to use the logger object directly.\n\n```python\ndef training_step(self):\n    ...\n    # the logger you used (in this case tensorboard)\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n```\n----------",
    "parent_index": 37,
    "index": 172
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Make a Custom Logger",
    "text": "You can implement your own logger by writing a class that inherits from ~lightning.pytorch.loggers.logger.Logger.\nUse the ~lightning.pytorch.loggers.logger.rank_zero_experiment and ~lightning.pytorch.utilities.rank_zero.rank_zero_only decorators to make sure that only the first process in DDP training creates the experiment and logs the data respectively.\n\nfrom lightning.pytorch.loggers.logger import Logger, rank_zero_experiment\nfrom lightning.pytorch.utilities import rank_zero_only\n\nclass MyLogger(Logger):\n@property\ndef name(self):\nreturn \"MyLogger\"\n\n@property\ndef version(self):\n# Return the experiment version, int or str.\nreturn \"0.1\"\n\n@rank_zero_only\ndef log_hyperparams(self, params):\n# params is an argparse.Namespace\n# your code to record hyperparameters goes here\npass\n\n@rank_zero_only\ndef log_metrics(self, metrics, step):\n# metrics is a dictionary of metric names and values\n# your code to record metrics goes here\npass\n\n@rank_zero_only\ndef save(self):\n# Optional. Any code necessary to save logger data goes here\npass\n\n@rank_zero_only\ndef finalize(self, status):\n# Optional. Any code that needs to be run after training\n# finishes goes here\npass\nIf you write a logger that may be useful to others, please send\na pull request to add it to Lightning!\n\n----------",
    "parent_index": 37,
    "index": 173
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Logging frequency",
    "text": "It may slow down training to log on every single batch. By default, Lightning logs every 50 rows, or 50 training steps.\nTo change this behaviour, set the log_every_n_steps ~lightning.pytorch.trainer.trainer.Trainer flag.\n\nk = 10\ntrainer = Trainer(log_every_n_steps=k)",
    "parent_index": 37,
    "index": 174
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Log Writing Frequency",
    "text": "Individual logger implementations determine their flushing frequency. For example, on the\n~lightning.pytorch.loggers.csv_logs.CSVLogger you can set the flag flush_logs_every_n_steps.\n\n----------",
    "parent_index": 37,
    "index": 175
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Progress Bar",
    "text": "You can add any metric to the progress bar using ~lightning.pytorch.core.LightningModule.log\nmethod, setting prog_bar=True.\n\n```python\ndef training_step(self, batch, batch_idx):\n    self.log(\"my_loss\", loss, prog_bar=True)\n```\nYou could learn more about progress bars supported by Lightning here <../common/progress_bar>.",
    "parent_index": 37,
    "index": 176
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Modifying the Progress Bar",
    "text": "The progress bar by default already includes the training loss and version number of the experiment\nif you are using a logger. These defaults can be customized by overriding the\n~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar.get_metrics hook in your logger.\n\n```python\nfrom lightning.pytorch.callbacks.progress import TQDMProgressBar\n\nclass CustomProgressBar(TQDMProgressBar):\n    def get_metrics(self, *args, **kwargs):\n        # don't show the version number\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        return items\n```\n----------",
    "parent_index": 37,
    "index": 177
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Configure Console Logging",
    "text": "Lightning logs useful information about the training process and user warnings to the console.\nYou can retrieve the Lightning console logger and change it to your liking. For example, adjust the logging level\nor redirect output for certain modules to log files:\n\nimport logging\n\n# configure logging at the root level of Lightning\nlogging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n# configure logging on module level, redirect to file\nlogger = logging.getLogger(\"lightning.pytorch.core\")\nlogger.addHandler(logging.FileHandler(\"core.log\"))\nRead more about custom Python logging here.\n\n----------",
    "parent_index": 37,
    "index": 178
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Logging Hyperparameters",
    "text": "When training a model, it is useful to know what hyperparams went into that model.\nWhen Lightning creates a checkpoint, it stores a key \"hyper_parameters\" with the hyperparams.\n\n```python\nlightning_checkpoint = torch.load(filepath, map_location=lambda storage, loc: storage)\nhyperparams = lightning_checkpoint[\"hyper_parameters\"]\n```\nSome loggers also allow logging the hyperparams used in the experiment. For instance,\nwhen using the TensorBoardLogger, all hyperparams will show\nin the hparams tab at torch.utils.tensorboard.writer.SummaryWriter.add_hparams.\n\nIf you want to track a metric in the tensorboard hparams tab, log scalars to the key hp_metric. If tracking multiple metrics, initialize TensorBoardLogger with default_hp_metric=False and call log_hyperparams only once with your metric keys and initial values. Subsequent updates can simply be logged to the metric keys. Refer to the examples below for setting up proper hyperparams metrics tracking within the LightningModule <../common/lightning_module>.\n\n.. code-block:: python\n\n# Using default_hp_metric\ndef validation_step(self, batch, batch_idx):\nself.log(\"hp_metric\", some_scalar)\n\n# Using custom or multiple metrics (default_hp_metric=False)\ndef on_train_start(self):\nself.logger.log_hyperparams(self.hparams, {\"hp/metric_1\": 0, \"hp/metric_2\": 0})\n\ndef validation_step(self, batch, batch_idx):\nself.log(\"hp/metric_1\", some_scalar_1)\nself.log(\"hp/metric_2\", some_scalar_2)\n\nIn the example, using \"hp/\" as a prefix allows for the metrics to be grouped under \"hp\" in the tensorboard scalar tab where you can collapse them.\n-----------",
    "parent_index": 37,
    "index": 179
  },
  {
    "file": "docs/pytorch/stable/extensions/logging.html",
    "label": "docs",
    "title": "Managing Remote Filesystems",
    "text": "Lightning supports saving logs to a variety of filesystems, including local filesystems and several cloud storage providers.\n\nCheck out the Remote Filesystems <../common/remote_fs> doc for more info.",
    "parent_index": 37,
    "index": 180
  },
  {
    "file": "docs/pytorch/stable/extensions/plugins.html",
    "label": "docs",
    "title": "Precision Plugins",
    "text": "We provide precision plugins for you to benefit from numerical representations with lower precision than\n32-bit floating-point or higher precision, such as 64-bit floating-point.\n\n```python\n# Training with 16-bit precision\ntrainer = Trainer(precision=16)\n```\nThe full list of built-in precision plugins is listed below.\n\nDeepSpeedPrecision\nDoublePrecision\nHalfPrecision\nFSDPPrecision\nMixedPrecision\nPrecision\nXLAPrecision\nTransformerEnginePrecision\nBitsandbytesPrecision\nMore information regarding precision with Lightning can be found here <precision>\n\n-----------",
    "parent_index": 38,
    "index": 181
  },
  {
    "file": "docs/pytorch/stable/extensions/plugins.html",
    "label": "docs",
    "title": "CheckpointIO Plugins",
    "text": "As part of our commitment to extensibility, we have abstracted Lightning's checkpointing logic into the ~lightning.pytorch.plugins.io.CheckpointIO plugin.\nWith this, you have the ability to customize the checkpointing logic to match the needs of your infrastructure.\n\nBelow is a list of built-in plugins for checkpointing.\n\nAsyncCheckpointIO\nCheckpointIO\nTorchCheckpointIO\nXLACheckpointIO\nLearn more about custom checkpointing with Lightning here <checkpointing_expert>.\n\n-----------",
    "parent_index": 38,
    "index": 182
  },
  {
    "file": "docs/pytorch/stable/extensions/plugins.html",
    "label": "docs",
    "title": "Cluster Environments",
    "text": "You can define the interface of your own cluster environment based on the requirements of your infrastructure.\n\nClusterEnvironment\nKubeflowEnvironment\nLightningEnvironment\nLSFEnvironment\nSLURMEnvironment\nTorchElasticEnvironment\nXLAEnvironment",
    "parent_index": 38,
    "index": 183
  },
  {
    "file": "docs/pytorch/stable/extensions/strategy.html",
    "label": "docs",
    "title": "Selecting a Built-in Strategy",
    "text": "Built-in strategies can be selected in two ways.\n\n1. Pass the shorthand name to the strategy Trainer argument\n2. Import a Strategy from lightning.pytorch.strategies, instantiate it and pass it to the strategy Trainer argument\n\nThe latter allows you to configure further options on the specific strategy.\nHere are some examples:\n\n```python\n# Training with the DistributedDataParallel strategy on 4 GPUs\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Training with the DistributedDataParallel strategy on 4 GPUs, with options configured\ntrainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=4)\n\n# Training with the DDP Spawn strategy using auto accelerator selection\ntrainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"auto\", devices=4)\n\n# Training with the DeepSpeed strategy on available GPUs\ntrainer = Trainer(strategy=\"deepspeed\", accelerator=\"gpu\", devices=\"auto\")\n\n# Training with the DDP strategy using 3 CPU processes\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"cpu\", devices=3)\n\n# Training with the DDP Spawn strategy on 8 TPU cores\ntrainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"tpu\", devices=8)\n```\nThe below table lists all relevant strategies available in Lightning with their corresponding short-hand name:\n\n* - Name\n- Class\n- Description\n* - fsdp\n- ~lightning.pytorch.strategies.FSDPStrategy\n- Strategy for Fully Sharded Data Parallel training. Learn more. <../advanced/model_parallel/fsdp>\n* - ddp\n- ~lightning.pytorch.strategies.DDPStrategy\n- Strategy for multi-process single-device training on one or multiple nodes. Learn more. <accelerators/gpu_intermediate:Distributed Data Parallel>\n* - ddp_spawn\n- ~lightning.pytorch.strategies.DDPStrategy\n- Same as \"ddp\" but launches processes using torch.multiprocessing.spawn method and joins processes after training finishes. Learn more. <accelerators/gpu_intermediate:Distributed Data Parallel Spawn>\n* - deepspeed\n- ~lightning.pytorch.strategies.DeepSpeedStrategy\n- Provides capabilities to run training using the DeepSpeed library, with training optimizations for large billion parameter models. Learn more. <../advanced/model_parallel/deepspeed>\n* - hpu_parallel\n- HPUParallelStrategy\n- Strategy for distributed training on multiple HPU devices. Learn more. <../integrations/hpu/index>\n* - hpu_single\n- SingleHPUStrategy\n- Strategy for training on a single HPU device. Learn more. <../integrations/hpu/index>\n* - xla\n- ~lightning.pytorch.strategies.XLAStrategy\n- Strategy for training on multiple TPU devices using the torch_xla.distributed.xla_multiprocessing.spawn method. Learn more. <../accelerators/tpu>\n* - single_xla\n- ~lightning.pytorch.strategies.SingleXLAStrategy\n- Strategy for training on a single XLA device, like TPUs. Learn more. <../accelerators/tpu>\n----",
    "parent_index": 39,
    "index": 184
  },
  {
    "file": "docs/pytorch/stable/extensions/strategy.html",
    "label": "docs",
    "title": "Third-party Strategies",
    "text": "There are powerful third-party strategies that integrate well with Lightning but aren't maintained as part of the lightning package.\nCheckout the gallery over here <../integrations/strategies/index>.\n\n----",
    "parent_index": 39,
    "index": 185
  },
  {
    "file": "docs/pytorch/stable/extensions/strategy.html",
    "label": "docs",
    "title": "Create a Custom Strategy",
    "text": "Every strategy in Lightning is a subclass of one of the main base classes: ~lightning.pytorch.strategies.Strategy, ~lightning.pytorch.strategies.SingleDeviceStrategy or ~lightning.pytorch.strategies.ParallelStrategy.\n\nAs an expert user, you may choose to extend either an existing built-in Strategy or create a completely new one by\nsubclassing the base classes.\n\n```python\nfrom lightning.pytorch.strategies import DDPStrategy\n\nclass CustomDDPStrategy(DDPStrategy):\n    def configure_ddp(self):\n        self.model = MyCustomDistributedDataParallel(\n            self.model,\n            device_ids=...,\n        )\n\n    def setup(self, trainer):\n        # you can access the accelerator and plugins directly\n        self.accelerator.setup()\n        self.precision_plugin.connect(...)\n```\nThe custom strategy can then be passed into the Trainer directly via the strategy parameter.\n\n```python\n# custom strategy\ntrainer = Trainer(strategy=CustomDDPStrategy())\n```\nSince the strategy also hosts the Accelerator and various plugins, you can customize all of them to work together as you like:\n\n```python\n# custom strategy, with new accelerator and plugins\naccelerator = MyAccelerator()\nprecision_plugin = MyPrecisionPlugin()\nstrategy = CustomDDPStrategy(accelerator=accelerator, precision_plugin=precision_plugin)\ntrainer = Trainer(strategy=strategy)\n```",
    "parent_index": 39,
    "index": 186
  },
  {
    "file": "docs/pytorch/stable/model/build_model_intermediate.html",
    "label": "docs",
    "title": "Enable training features",
    "text": "Enable advanced training features using Trainer arguments. These are SOTA techniques that are automatically integrated into your training loop without changes to your code.\n\n```\n# train 1T+ parameter models with DeepSpeed/FSDP\ntrainer = Trainer(\n   devices=4,\n   accelerator=\"gpu\",\n   strategy=\"deepspeed_stage_2\",\n   precision=\"16-mixed\",\n)\n\n# 20+ helpful arguments for rapid idea iteration\ntrainer = Trainer(\n   max_epochs=10,\n   min_epochs=5,\n   overfit_batches=1\n)\n\n# access the latest state of the art techniques\ntrainer = Trainer(callbacks=[WeightAveraging(...)])\n```\n----",
    "parent_index": 40,
    "index": 187
  },
  {
    "file": "docs/pytorch/stable/model/build_model_intermediate.html",
    "label": "docs",
    "title": "Extend the Trainer",
    "text": "If you have multiple lines of code with similar functionalities, you can use *callbacks* to easily group them together and toggle all of those lines on or off at the same time.\n\n```\ntrainer = Trainer(callbacks=[AWSCheckpoints()])\n```",
    "parent_index": 40,
    "index": 188
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Manual Optimization",
    "text": "For advanced research topics like reinforcement learning, sparse coding, or GAN research, it may be desirable to\nmanually manage the optimization process, especially when dealing with multiple optimizers at the same time.\n\nIn this mode, Lightning will handle only accelerator, precision and strategy logic.\nThe users are left with optimizer.zero_grad(), gradient accumulation, optimizer toggling, etc..\n\nTo manually optimize, do the following:\n\n* Set self.automatic_optimization=False in your LightningModule's __init__.\n* Use the following functions and call them manually:\n\n* self.optimizers() to access your optimizers (one or multiple)\n* optimizer.zero_grad() to clear the gradients from the previous training step\n* self.manual_backward(loss) instead of loss.backward()\n* optimizer.step() to update your model parameters\n* self.toggle_optimizer() and self.untoggle_optimizer(), or self.toggled_optimizer() if needed\n\nHere is a minimal example of manual optimization.\n\nfrom lightning.pytorch import LightningModule\n\nclass MyModel(LightningModule):\ndef __init__(self):\nsuper().__init__()\n# Important: This property activates manual optimization.\nself.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\nopt = self.optimizers()\nopt.zero_grad()\nloss = self.compute_loss(batch)\nself.manual_backward(loss)\nopt.step()\nBe careful where you call optimizer.zero_grad(), or your model won't converge.\nIt is good practice to call optimizer.zero_grad() before self.manual_backward(loss).",
    "parent_index": 41,
    "index": 189
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Access your Own Optimizer",
    "text": "The provided optimizer is a ~lightning.pytorch.core.optimizer.LightningOptimizer object wrapping your own optimizer\nconfigured in your ~lightning.pytorch.core.LightningModule.configure_optimizers. You can access your own optimizer\nwith optimizer.optimizer. However, if you use your own optimizer to perform a step, Lightning won't be able to\nsupport accelerators, precision and profiling for you.\n\nclass Model(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n...\n\ndef training_step(self, batch, batch_idx):\noptimizer = self.optimizers()\n\n# `optimizer` is a `LightningOptimizer` wrapping the optimizer.\n# To access it, do the following.\n# However, it won't work on TPU, AMP, etc...\noptimizer = optimizer.optimizer\n...",
    "parent_index": 41,
    "index": 190
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Gradient Accumulation",
    "text": "You can accumulate gradients over batches similarly to accumulate_grad_batches argument in\nTrainer <trainer> for automatic optimization. To perform gradient accumulation with one optimizer\nafter every N steps, you can do as such.\n\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\nopt = self.optimizers()\n\n# scale losses by 1/N (for N batches of gradient accumulation)\nloss = self.compute_loss(batch) / N\nself.manual_backward(loss)\n\n# accumulate gradients of N batches\nif (batch_idx + 1) % N == 0:\nopt.step()\nopt.zero_grad()",
    "parent_index": 41,
    "index": 191
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Gradient Clipping",
    "text": "You can clip optimizer gradients during manual optimization similar to passing the gradient_clip_val and\ngradient_clip_algorithm argument in Trainer <trainer> during automatic optimization.\nTo perform gradient clipping with one optimizer with manual optimization, you can do as such.\n\nfrom lightning.pytorch import LightningModule\n\nclass SimpleModel(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef training_step(self, batch, batch_idx):\nopt = self.optimizers()\n\n# compute loss\nloss = self.compute_loss(batch)\n\nopt.zero_grad()\nself.manual_backward(loss)\n\n# clip gradients\nself.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n\nopt.step()\n* Note that configure_gradient_clipping() won't be called in Manual Optimization. Instead consider using self. clip_gradients() manually like in the example above.",
    "parent_index": 41,
    "index": 192
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Use Multiple Optimizers (like GANs)",
    "text": "Here is an example training a simple GAN with multiple optimizers using manual optimization.\n\nimport torch\nfrom torch import Tensor\nfrom lightning.pytorch import LightningModule\n\nclass SimpleGAN(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.G = Generator()\nself.D = Discriminator()\n\n# Important: This property activates manual optimization.\nself.automatic_optimization = False\n\ndef sample_z(self, n) -> Tensor:\nsample = self._Z.sample((n,))\nreturn sample\n\ndef sample_G(self, n) -> Tensor:\nz = self.sample_z(n)\nreturn self.G(z)\n\ndef training_step(self, batch, batch_idx):\n# Implementation follows the PyTorch tutorial:\n# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\ng_opt, d_opt = self.optimizers()\n\nX, _ = batch\nbatch_size = X.shape[0]\n\nreal_label = torch.ones((batch_size, 1), device=self.device)\nfake_label = torch.zeros((batch_size, 1), device=self.device)\n\ng_X = self.sample_G(batch_size)\n\n##########################\n# Optimize Discriminator #\n##########################\nd_x = self.D(X)\nerrD_real = self.criterion(d_x, real_label)\n\nd_z = self.D(g_X.detach())\nerrD_fake = self.criterion(d_z, fake_label)\n\nerrD = errD_real + errD_fake\n\nd_opt.zero_grad()\nself.manual_backward(errD)\nd_opt.step()\n\n######################\n# Optimize Generator #\n######################\nd_z = self.D(g_X)\nerrG = self.criterion(d_z, real_label)\n\ng_opt.zero_grad()\nself.manual_backward(errG)\ng_opt.step()\n\nself.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\ndef configure_optimizers(self):\ng_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\nd_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\nreturn g_opt, d_opt",
    "parent_index": 41,
    "index": 193
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Learning Rate Scheduling",
    "text": "Every optimizer you use can be paired with any\nLearning Rate Scheduler. Please see the\ndocumentation of ~lightning.pytorch.core.LightningModule.configure_optimizers for all the available options\n\nYou can call lr_scheduler.step() at arbitrary intervals.\nUse self.lr_schedulers() in your ~lightning.pytorch.core.LightningModule to access any learning rate schedulers\ndefined in your ~lightning.pytorch.core.LightningModule.configure_optimizers.\n\n* lr_scheduler.step() can be called at arbitrary intervals by the user in case of manual optimization, or by Lightning if \"interval\" is defined in ~lightning.pytorch.core.LightningModule.configure_optimizers in case of automatic optimization.\n* Note that the lr_scheduler_config keys, such as \"frequency\" and \"interval\", will be ignored even if they are provided in\nyour ~lightning.pytorch.core.LightningModule.configure_optimizers during manual optimization.\nHere is an example calling lr_scheduler.step() every step.\n\n# step every batch\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef configure_optimizers(self):\noptimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\nreturn [optimizer], [scheduler]\n\ndef training_step(self, batch, batch_idx):\n# do forward, backward, and optimization\n...\n\n# single scheduler\nsch = self.lr_schedulers()\nsch.step()\n\n# multiple schedulers\nsch1, sch2 = self.lr_schedulers()\nsch1.step()\nsch2.step()\nIf you want to call lr_scheduler.step() every N steps/epochs, do the following.\n\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef configure_optimizers(self):\noptimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\nreturn [optimizer], [scheduler]\n\ndef training_step(self, batch, batch_idx):\n# do forward, backward, and optimization\n...\n\nsch = self.lr_schedulers()\n\n# step every N batches\nif (batch_idx + 1) % N == 0:\nsch.step()\n\n# step every N epochs\nif self.trainer.is_last_batch and (self.trainer.current_epoch + 1) % N == 0:\nsch.step()\nIf you want to call schedulers that require a metric value after each epoch, consider doing the following:\n\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef configure_optimizers(self):\noptimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\nreturn [optimizer], [scheduler]\n\ndef on_train_epoch_end(self):\nsch = self.lr_schedulers()\n\nsch.step(self.trainer.callback_metrics[\"loss\"])\noptimizers and learning rate schedulers. Regardless of the way you define them, `self.optimizers()` will always return\neither a single optimizer if you defined a single optimizer, or a list of optimizers if you defined multiple\noptimizers. The same applies to the `self.lr_schedulers()` method, which will return a single scheduler\nif you defined a single scheduler, or a list of schedulers if you defined multiple schedulers",
    "parent_index": 41,
    "index": 194
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Optimizer Steps at Different Frequencies",
    "text": "In manual optimization, you are free to step() one optimizer more often than another one.\nFor example, here we step the optimizer for the *discriminator* weights twice as often as the optimizer for the *generator*.\n\n# Alternating schedule for optimizer steps (e.g. GANs)\ndef training_step(self, batch, batch_idx):\ng_opt, d_opt = self.optimizers()\n...\n\n# update discriminator every other step\nd_opt.zero_grad()\nself.manual_backward(errD)\nif (batch_idx + 1) % 2 == 0:\nd_opt.step()\n\n...\n\n# update generator every step\ng_opt.zero_grad()\nself.manual_backward(errG)\ng_opt.step()",
    "parent_index": 41,
    "index": 195
  },
  {
    "file": "docs/pytorch/stable/model/manual_optimization.html",
    "label": "docs",
    "title": "Use Closure for LBFGS-like Optimizers",
    "text": "It is a good practice to provide the optimizer with a closure function that performs a forward, zero_grad and\nbackward of your model. It is optional for most optimizers, but makes your code compatible if you switch to an\noptimizer which requires a closure, such as ~torch.optim.LBFGS.\n\nSee the PyTorch docs for more about the closure.\n\nHere is an example using a closure function.\n\ndef __init__(self):\nsuper().__init__()\nself.automatic_optimization = False\n\ndef configure_optimizers(self):\nreturn torch.optim.LBFGS(...)\n\ndef training_step(self, batch, batch_idx):\nopt = self.optimizers()\n\ndef closure():\nloss = self.compute_loss(batch)\nopt.zero_grad()\nself.manual_backward(loss)\nreturn loss\n\nopt.step(closure=closure)\nThe ~torch.optim.LBFGS optimizer is not supported for AMP or DeepSpeed.",
    "parent_index": 41,
    "index": 196
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Add imports",
    "text": "Add the relevant imports at the top of the file\n\n```python\nimport os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader\nimport lightning as L\n```\n----",
    "parent_index": 42,
    "index": 197
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Define the PyTorch nn.Modules",
    "text": "```python\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n\n    def forward(self, x):\n        return self.l1(x)\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n    def forward(self, x):\n        return self.l1(x)\n```\n----",
    "parent_index": 42,
    "index": 198
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Define a LightningModule",
    "text": "The LightningModule is the full **recipe** that defines how your nn.Modules interact.\n\n- The **training_step** defines how the *nn.Modules* interact together.\n- In the **configure_optimizers** define the optimizer(s) for your models.\n\n```python\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n```\n----",
    "parent_index": 42,
    "index": 199
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Define the training dataset",
    "text": "Define a PyTorch ~torch.utils.data.DataLoader which contains your training dataset.\n\n```python\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\ntrain_loader = DataLoader(dataset)\n```\n----",
    "parent_index": 42,
    "index": 200
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Train the model",
    "text": "To train the model use the Lightning Trainer <../common/trainer> which handles all the engineering and abstracts away all the complexity needed for scale.\n\n```python\n# model\nautoencoder = LitAutoEncoder(Encoder(), Decoder())\n\n# train model\ntrainer = L.Trainer()\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n```\n----",
    "parent_index": 42,
    "index": 201
  },
  {
    "file": "docs/pytorch/stable/model/train_model_basic.html",
    "label": "docs",
    "title": "Eliminate the training loop",
    "text": "Under the hood, the Lightning Trainer runs the following training loop on your behalf\n\n```python\nautoencoder = LitAutoEncoder(Encoder(), Decoder())\noptimizer = autoencoder.configure_optimizers()\n\nfor batch_idx, batch in enumerate(train_loader):\n    loss = autoencoder.training_step(batch, batch_idx)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\nThe power of Lightning comes when the training loop gets complicated as you add validation/test splits, schedulers, distributed training and all the latest SOTA techniques.\n\nWith Lightning, you can add mix all these techniques together without needing to rewrite a new loop every time.",
    "parent_index": 42,
    "index": 202
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "1. Keep Your Computational Code",
    "text": "Keep your regular nn.Module architecture\n\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LitModel(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.layer_1 = nn.Linear(28 * 28, 128)\nself.layer_2 = nn.Linear(128, 10)\n\ndef forward(self, x):\nx = x.view(x.size(0), -1)\nx = self.layer_1(x)\nx = F.relu(x)\nx = self.layer_2(x)\nreturn x\n--------",
    "parent_index": 43,
    "index": 203
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "2. Configure Training Logic",
    "text": "In the training_step of the LightningModule configure how your training routine behaves with a batch of training data:\n\nclass LitModel(L.LightningModule):\ndef __init__(self, encoder):\nsuper().__init__()\nself.encoder = encoder\n\ndef training_step(self, batch, batch_idx):\nx, y = batch\ny_hat = self.encoder(x)\nloss = F.cross_entropy(y_hat, y)\nreturn loss\n----",
    "parent_index": 43,
    "index": 204
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "3. Move Optimizer(s) and LR Scheduler(s)",
    "text": "Move your optimizers to the ~lightning.pytorch.core.LightningModule.configure_optimizers hook.\n\nclass LitModel(L.LightningModule):\ndef configure_optimizers(self):\noptimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-3)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\nreturn [optimizer], [lr_scheduler]\n--------",
    "parent_index": 43,
    "index": 205
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "4. Organize Validation Logic (optional)",
    "text": "If you need a validation loop, configure how your validation routine behaves with a batch of validation data:\n\nclass LitModel(L.LightningModule):\ndef validation_step(self, batch, batch_idx):\nx, y = batch\ny_hat = self.encoder(x)\nval_loss = F.cross_entropy(y_hat, y)\nself.log(\"val_loss\", val_loss)\n--------",
    "parent_index": 43,
    "index": 206
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "5. Organize Testing Logic (optional)",
    "text": "If you need a test loop, configure how your testing routine behaves with a batch of test data:\n\nclass LitModel(L.LightningModule):\ndef test_step(self, batch, batch_idx):\nx, y = batch\ny_hat = self.encoder(x)\ntest_loss = F.cross_entropy(y_hat, y)\nself.log(\"test_loss\", test_loss)\n--------",
    "parent_index": 43,
    "index": 207
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "6. Configure Prediction Logic (optional)",
    "text": "If you need a prediction loop, configure how your prediction routine behaves with a batch of test data:\n\nclass LitModel(L.LightningModule):\ndef predict_step(self, batch, batch_idx):\nx, y = batch\npred = self.encoder(x)\nreturn pred\n--------",
    "parent_index": 43,
    "index": 208
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "7. Remove any .cuda() or .to(device) Calls",
    "text": "Your LightningModule <../common/lightning_module> can automatically run on any hardware!\n\nIf you have any explicit calls to .cuda() or .to(device), you can remove them since Lightning makes sure that the data coming from ~torch.utils.data.DataLoader\nand all the ~torch.nn.Module instances initialized inside LightningModule.__init__ are moved to the respective devices automatically.\nIf you still need to access the current device, you can use self.device anywhere in your LightningModule except in the __init__ and setup methods.\n\nclass LitModel(L.LightningModule):\ndef training_step(self, batch, batch_idx):\nz = torch.randn(4, 5, device=self.device)\n...\nHint: If you are initializing a ~torch.Tensor within the LightningModule.__init__ method and want it to be moved to the device automatically you should call\n~torch.nn.Module.register_buffer to register it as a parameter.\n\nclass LitModel(L.LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.register_buffer(\"running_mean\", torch.zeros(num_features))\n--------",
    "parent_index": 43,
    "index": 209
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "8. Use your own data",
    "text": "Regular PyTorch DataLoaders work with Lightning. For more modular and scalable datasets, check out LightningDataModule <../data/datamodule>.\n\n----",
    "parent_index": 43,
    "index": 210
  },
  {
    "file": "docs/pytorch/stable/starter/converting.html",
    "label": "docs",
    "title": "Good to know",
    "text": "Additionally, you can run only the validation loop using ~lightning.pytorch.trainer.trainer.Trainer.validate method.\n\n```python\nmodel = LitModel()\ntrainer.validate(model)\n```\nThe test loop isn't used within ~lightning.pytorch.trainer.trainer.Trainer.fit, therefore, you would need to explicitly call ~lightning.pytorch.trainer.trainer.Trainer.test.\n\n```python\nmodel = LitModel()\ntrainer.test(model)\n```\nThe predict loop will not be used until you call ~lightning.pytorch.trainer.trainer.Trainer.predict.\n\n```python\nmodel = LitModel()\ntrainer.predict(model)\n```",
    "parent_index": 43,
    "index": 211
  },
  {
    "file": "docs/pytorch/stable/starter/installation.html",
    "label": "docs",
    "title": "Install with pip",
    "text": "Install lightning inside a virtual env or conda environment with pip\n\n```bash\npython -m pip install lightning\n```\n----",
    "parent_index": 44,
    "index": 212
  },
  {
    "file": "docs/pytorch/stable/starter/installation.html",
    "label": "docs",
    "title": "Install with Conda",
    "text": "If you don't have conda installed, follow the Conda Installation Guide.\nLightning can be installed with conda using the following command:\n\n```bash\nconda install lightning -c conda-forge\n```\nYou can also use Conda Environments:\n\n```bash\nconda activate my_env\nconda install lightning -c conda-forge\n```\n----\n\nIn case you face difficulty with pulling the GRPC package, please follow this thread\n\n----",
    "parent_index": 44,
    "index": 213
  },
  {
    "file": "docs/pytorch/stable/starter/installation.html",
    "label": "docs",
    "title": "Build from Source",
    "text": "Install nightly from the source. Note that it contains all the bug fixes and newly released features that\nare not published yet. This is the bleeding edge, so use it at your own discretion.\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\nInstall future patch releases from the source. Note that the patch release contains only the bug fixes for the recent major release.\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n#### Custom PyTorch Version\n\nTo use any PyTorch version visit the PyTorch Installation Page.\nYou can find the list of supported PyTorch versions in our compatibility matrix <versioning:Compatibility matrix>.\n\n----",
    "parent_index": 44,
    "index": 214
  },
  {
    "file": "docs/pytorch/stable/starter/installation.html",
    "label": "docs",
    "title": "Optimized for ML workflows (Lightning Apps)",
    "text": "If you are deploying workflows built with Lightning in production and require fewer dependencies, try using the optimized lightning[apps] package:\n\n```bash\npip install lightning-app\n```",
    "parent_index": 44,
    "index": 215
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "1: Install PyTorch Lightning",
    "text": "For pip users\n\n```bash\npip install lightning\n```\nFor conda users\n\n```bash\nconda install lightning -c conda-forge\n```\nOr read the advanced install guide\n\n----",
    "parent_index": 45,
    "index": 216
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "2: Define a LightningModule",
    "text": "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\n\nimport os\nfrom torch import optim, nn, utils, Tensor\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport lightning as L\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n# define the LightningModule\nclass LitAutoEncoder(L.LightningModule):\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder\nself.decoder = decoder\n\ndef training_step(self, batch, batch_idx):\n# training_step defines the train loop.\n# it is independent of forward\nx, _ = batch\nx = x.view(x.size(0), -1)\nz = self.encoder(x)\nx_hat = self.decoder(z)\nloss = nn.functional.mse_loss(x_hat, x)\n# Logging to TensorBoard (if installed) by default\nself.log(\"train_loss\", loss)\nreturn loss\n\ndef configure_optimizers(self):\noptimizer = optim.Adam(self.parameters(), lr=1e-3)\nreturn optimizer\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n----",
    "parent_index": 45,
    "index": 217
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "3: Define a dataset",
    "text": "Lightning supports ANY iterable (~torch.utils.data.DataLoader, numpy, etc...) for the train/val/test/predict splits.\n\n```python\n# setup data\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\ntrain_loader = utils.data.DataLoader(dataset)\n```\n----",
    "parent_index": 45,
    "index": 218
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "4: Train the model",
    "text": "The Lightning Trainer <../common/trainer> \"mixes\" any LightningModule <../common/lightning_module> with any dataset and abstracts away all the engineering complexity needed for scale.\n\n```python\n# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\ntrainer = L.Trainer(limit_train_batches=100, max_epochs=1)\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n```\nThe Lightning Trainer <../common/trainer> automates 40+ tricks including:\n\n* Epoch and batch iteration\n* optimizer.step(), loss.backward(), optimizer.zero_grad() calls\n* Calling of model.eval(), enabling/disabling grads during evaluation\n* Checkpoint Saving and Loading <../common/checkpointing>\n* Tensorboard (see loggers <../visualize/loggers> options)\n* Multi-GPU <../accelerators/gpu> support\n* TPU <../accelerators/tpu>\n* 16-bit precision AMP <speed-amp> support\n\n----",
    "parent_index": 45,
    "index": 219
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "5: Use the model",
    "text": "Once you've trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions.\n\n```python\n# load checkpoint\ncheckpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\nautoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n\n# choose your trained nn.Module\nencoder = autoencoder.encoder\nencoder.eval()\n\n# embed 4 fake images!\nfake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\nembeddings = encoder(fake_image_batch)\nprint(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)\n```\n----",
    "parent_index": 45,
    "index": 220
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "6: Visualize training",
    "text": "If you have tensorboard installed, you can use it for visualizing experiments.\n\nRun this on your commandline and open your browser to **http://localhost:6006/**\n\n```bash\ntensorboard --logdir .\n```\n----",
    "parent_index": 45,
    "index": 221
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "7: Supercharge training",
    "text": "Enable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code.\n\n```\n# train on 4 GPUs\ntrainer = L.Trainer(\n   devices=4,\n   accelerator=\"gpu\",\n)\n\n# train 1TB+ parameter models with Deepspeed/fsdp\ntrainer = L.Trainer(\n   devices=4,\n   accelerator=\"gpu\",\n   strategy=\"deepspeed_stage_2\",\n   precision=16\n)\n\n# 20+ helpful flags for rapid idea iteration\ntrainer = L.Trainer(\n   max_epochs=10,\n   min_epochs=5,\n   overfit_batches=1\n)\n\n# access the latest state of the art techniques\ntrainer = L.Trainer(callbacks=[WeightAveraging(...)])\n```\n----",
    "parent_index": 45,
    "index": 222
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "Maximize flexibility",
    "text": "Lightning's core guiding principle is to always provide maximal flexibility **without ever hiding any of the PyTorch**.\n\nLightning offers 5 *added* degrees of flexibility depending on your project's complexity.\n\n----",
    "parent_index": 45,
    "index": 223
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "Customize training loop",
    "text": "Inject custom code anywhere in the Training loop using any of the 20+ methods (lightning_hooks) available in the LightningModule.\n\nclass LitAutoEncoder(L.LightningModule):\ndef backward(self, loss):\nloss.backward()\n----",
    "parent_index": 45,
    "index": 224
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "Extend the Trainer",
    "text": "If you have multiple lines of code with similar functionalities, you can use callbacks to easily group them together and toggle all of those lines on or off at the same time.\n\n```\ntrainer = Trainer(callbacks=[AWSCheckpoints()])\n```\n----",
    "parent_index": 45,
    "index": 225
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "Use a raw PyTorch loop",
    "text": "For certain types of work at the bleeding-edge of research, Lightning offers experts full control of optimization or the training loop in various ways.\n\n.. Add callout items below this line\n\n.. End of callout item section\n\n----",
    "parent_index": 45,
    "index": 226
  },
  {
    "file": "docs/pytorch/stable/starter/introduction.html",
    "label": "docs",
    "title": "Next steps",
    "text": "Depending on your use case, you might want to check one of these out next.\n\n.. Add callout items below this line",
    "parent_index": 45,
    "index": 227
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "Systems vs Models",
    "text": "The main principle behind a LightningModule is that a full system should be self-contained.\nIn Lightning, we differentiate between a system and a model.\n\nA model is something like a resnet18, RNN, and so on.\n\nA system defines how a collection of models interact with each other with user-defined training/evaluation logic. Examples of this are:\n\n* GANs\n* Seq2Seq\n* BERT\n* etc.\n\nA LightningModule can define both a system and a model:\n\nHere's a LightningModule that defines a system. This structure is what we recommend as a best practice. Keeping the model separate from the system improves\nmodularity, which eventually helps in better testing, reduces dependencies on the system and makes it easier to refactor.\n\nclass Encoder(nn.Module):\n...\n\nclass Decoder(nn.Module):\n...\n\nclass AutoEncoder(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.encoder = Encoder()\nself.decoder = Decoder()\n\ndef forward(self, x):\nreturn self.encoder(x)\n\nclass AutoEncoderSystem(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.auto_encoder = AutoEncoder()\nFor fast prototyping, it's often useful to define all the computations in a LightningModule. For reusability\nand scalability, it might be better to pass in the relevant backbones.\n\nHere's a LightningModule that defines a model. Although, we do not recommend to define a model like in the example.\n\nclass LitModel(LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.layer_1 = nn.Linear()\nself.layer_2 = nn.Linear()\nself.layer_3 = nn.Linear()",
    "parent_index": 46,
    "index": 228
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "Self-contained",
    "text": "A Lightning module should be self-contained. To see how self-contained your model is, a good test is to ask\nyourself this question:\n\n\"Can someone drop this file into a Trainer without knowing anything about the internals?\"\n\nFor example, we couple the optimizer with a model because the majority of models require a specific optimizer with\na specific learning rate scheduler to work well.",
    "parent_index": 46,
    "index": 229
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "Init",
    "text": "The first place where LightningModules tend to stop being self-contained is in the init. Try to define all the relevant\nsensible defaults in the init so that the user doesn't have to guess.\n\nHere's an example where a user will have to go hunt through files to figure out how to init this LightningModule.\n\nclass LitModel(LightningModule):\ndef __init__(self, params):\nself.lr = params.lr\nself.coef_x = params.coef_x\nModels defined as such leave you with many questions, such as what is coef_x? Is it a string? A float? What is the range?\nInstead, be explicit in your init\n\nclass LitModel(LightningModule):\ndef __init__(self, encoder: nn.Module, coef_x: float = 0.2, lr: float = 1e-3):\n...\nNow the user doesn't have to guess. Instead, they know the value type, and the model has a sensible default where the\nuser can see the value immediately.",
    "parent_index": 46,
    "index": 230
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "Method Order",
    "text": "The only required methods in the LightningModule are:\n\n* init\n* training_step\n* configure_optimizers\n\nHowever, if you decide to implement the rest of the optional methods, the recommended order is:\n\n* model/system definition (init)\n* if doing inference, define forward\n* training hooks\n* validation hooks\n* test hooks\n* predict hooks\n* configure_optimizers\n* any other hooks\n\nIn practice, the code looks like this:\n\n```\nclass LitModel(L.LightningModule):\n\n    def __init__(...):\n\n    def forward(...):\n\n    def training_step(...):\n\n    def on_train_epoch_end(...):\n\n    def validation_step(...):\n\n    def on_validation_epoch_end(...):\n\n    def test_step(...):\n\n    def on_test_epoch_end(...):\n\n    def configure_optimizers(...):\n\n    def any_extra_hook(...):\n```",
    "parent_index": 46,
    "index": 231
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "Forward vs training_step",
    "text": "We recommend using ~lightning.pytorch.core.LightningModule.forward for inference/predictions and keeping\n~lightning.pytorch.core.LightningModule.training_step independent.\n\n```python\ndef forward(self, x):\n    embeddings = self.encoder(x)\n    return embeddings\n\ndef training_step(self, batch, batch_idx):\n    x, _ = batch\n    z = self.encoder(x)\n    pred = self.decoder(z)\n    ...\n```\n--------------",
    "parent_index": 46,
    "index": 232
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "DataLoaders",
    "text": "Lightning uses ~torch.utils.data.DataLoader to handle all the data flow through the system. Whenever you structure dataloaders,\nmake sure to tune the number of workers for maximum efficiency.",
    "parent_index": 46,
    "index": 233
  },
  {
    "file": "docs/pytorch/stable/starter/style_guide.html",
    "label": "docs",
    "title": "DataModules",
    "text": "The ~lightning.pytorch.core.datamodule.LightningDataModule is designed as a way of decoupling data-related\nhooks from the ~lightning.pytorch.core.LightningModule so you can develop dataset agnostic models. It makes it easy to hot swap different\ndatasets with your model, so you can test it and benchmark it across domains. It also makes sharing and reusing the exact data splits and transforms across projects possible.\n\nCheck out data document to understand data management within Lightning and its best practices.\n\n* What dataset splits were used?\n* How many samples does this dataset have overall and within each split?\n* Which transforms were used?\n\nIt's for this reason that we recommend you use datamodules. This is especially important when collaborating because\nit will save your team a lot of time as well.\n\nAll they need to do is drop a datamodule into the Trainer and not worry about what was done to the data.\n\nThis is true for both academic and corporate settings where data cleaning and ad-hoc instructions slow down the progress\nof iterating through ideas.\n\n- Check out the live examples to get your hands dirty:\n- Introduction to PyTorch Lightning\n- Introduction to DataModules",
    "parent_index": 46,
    "index": 234
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "Profile cloud TPU models",
    "text": "To profile TPU models use the ~lightning.pytorch.profilers.xla.XLAProfiler\n\n```python\nfrom lightning.pytorch.profilers import XLAProfiler\n\nprofiler = XLAProfiler(port=9001)\ntrainer = Trainer(profiler=profiler)\n```\n----",
    "parent_index": 47,
    "index": 235
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "Capture profiling logs in Tensorboard",
    "text": "To capture profile logs in Tensorboard, follow these instructions:\n\n----",
    "parent_index": 47,
    "index": 236
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "0: Setup the required installs",
    "text": "Use this guide to help you with the Cloud TPU required installations.\n\n----",
    "parent_index": 47,
    "index": 237
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "1: Start Tensorboard",
    "text": "Start the TensorBoard server:\n\n```bash\ntensorboard --logdir ./tensorboard --port 9001\n```\nNow open the following url on your browser\n\n```bash\nhttp://localhost:9001/#profile\n```\n----",
    "parent_index": 47,
    "index": 238
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "2: Capture the profile",
    "text": "Once the code you want to profile is running:\n\n1. click on the CAPTURE PROFILE button.\n2. Enter localhost:9001 (default port for XLA Profiler) as the Profile Service URL.\n3. Enter the number of milliseconds for the profiling duration\n4. Click CAPTURE\n\n----",
    "parent_index": 47,
    "index": 239
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "3: Don't stop your code",
    "text": "Make sure the code is running while you are trying to capture the traces. It will lead to better performance insights if the profiling duration is longer than the step time.\n\n----",
    "parent_index": 47,
    "index": 240
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_advanced.html",
    "label": "docs",
    "title": "4: View the profiling logs",
    "text": "Once the capture is finished, the page will refresh and you can browse through the insights using the **Tools** dropdown at the top left",
    "parent_index": 47,
    "index": 241
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_basic.html",
    "label": "docs",
    "title": "Why do I need profiling?",
    "text": "Profiling helps you find bottlenecks in your code by capturing analytics such as how long a function takes or how much memory is used.\n\n------------",
    "parent_index": 48,
    "index": 242
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_basic.html",
    "label": "docs",
    "title": "Find training loop bottlenecks",
    "text": "The most basic profile measures all the key methods across **Callbacks**, **DataModules** and the **LightningModule** in the training loop.\n\n```python\ntrainer = Trainer(profiler=\"simple\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nFIT Profiler Report\n\n-------------------------------------------------------------------------------------------\n|  Action                                          |  Mean duration (s) |  Total time (s) |\n-------------------------------------------------------------------------------------------\n|  [LightningModule]BoringModel.prepare_data       |  10.0001           |  20.00          |\n|  run_training_epoch                              |  6.1558            |  6.1558         |\n|  run_training_batch                              |  0.0022506         |  0.015754       |\n|  [LightningModule]BoringModel.optimizer_step     |  0.0017477         |  0.012234       |\n|  [LightningModule]BoringModel.val_dataloader     |  0.00024388        |  0.00024388     |\n|  on_train_batch_start                            |  0.00014637        |  0.0010246      |\n|  [LightningModule]BoringModel.teardown           |  2.15e-06          |  2.15e-06       |\n|  [LightningModule]BoringModel.on_train_start     |  1.644e-06         |  1.644e-06      |\n|  [LightningModule]BoringModel.on_train_end       |  1.516e-06         |  1.516e-06      |\n|  [LightningModule]BoringModel.on_fit_end         |  1.426e-06         |  1.426e-06      |\n|  [LightningModule]BoringModel.setup              |  1.403e-06         |  1.403e-06      |\n|  [LightningModule]BoringModel.on_fit_start       |  1.226e-06         |  1.226e-06      |\n-------------------------------------------------------------------------------------------\n```\nIn this report we can see that the slowest function is **prepare_data**. Now you can figure out why data preparation is slowing down your training.\n\nThe simple profiler measures all the standard methods used in the training loop automatically, including:\n\n- on_train_epoch_start\n- on_train_epoch_end\n- on_train_batch_start\n- model_backward\n- on_after_backward\n- optimizer_step\n- on_train_batch_end\n- on_training_end\n- etc...\n\n----",
    "parent_index": 48,
    "index": 243
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_basic.html",
    "label": "docs",
    "title": "Profile the time within every function",
    "text": "To profile the time within every function, use the ~lightning.pytorch.profilers.advanced.AdvancedProfiler built on top of Python's cProfiler.\n\n```python\ntrainer = Trainer(profiler=\"advanced\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nProfiler Report\n\nProfile stats for: get_train_batch\n        4869394 function calls (4863767 primitive calls) in 18.893 seconds\nOrdered by: cumulative time\nList reduced from 76 to 10 due to restriction <10>\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n    1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n    1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n    1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n    1875     0.084    0.000   18.290    0.010 fetch.py:44(<listcomp>)\n    60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n    60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n    60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n    60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n    60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n```\nIf the profiler report becomes too long, you can stream the report to a file:\n\n```python\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nprofiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\ntrainer = Trainer(profiler=profiler)\n```\n----",
    "parent_index": 48,
    "index": 244
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_basic.html",
    "label": "docs",
    "title": "Measure accelerator usage",
    "text": "Another helpful technique to detect bottlenecks is to ensure that you're using the full capacity of your accelerator (GPU/TPU/HPU).\nThis can be measured with the ~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor:\n\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\n\ntrainer = Trainer(callbacks=[DeviceStatsMonitor()])\nCPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set DeviceStatsMonitor(cpu_stats=True). To disable logging\nCPU metrics, you can specify DeviceStatsMonitor(cpu_stats=False).\n\n**Do not wrap** Trainer.fit(), Trainer.validate(), or other Trainer methods inside a manual\ntorch.profiler.profile context manager. This will cause unexpected crashes and cryptic errors due to\nincompatibility between PyTorch Profiler's context management and Lightning's internal training loop.\nInstead, always use the profiler argument in the Trainer constructor or the\n~lightning.pytorch.profilers.pytorch.PyTorchProfiler profiler class if you want to customize the profiling.\n\nExample:\n\n.. code-block:: python\n\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.profilers import PytorchProfiler\n\ntrainer = Trainer(profiler=\"pytorch\")\n# or\ntrainer = Trainer(profiler=PytorchProfiler(dirpath=\".\", filename=\"perf_logs\"))",
    "parent_index": 48,
    "index": 245
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_expert.html",
    "label": "docs",
    "title": "Build your own profiler",
    "text": "To build your own profiler, subclass ~lightning.pytorch.profilers.profiler.Profiler\nand override some of its methods. Here is a simple example that profiles the first occurrence and total calls of each action:\n\n```python\nfrom lightning.pytorch.profilers import Profiler\nfrom collections import defaultdict\nimport time\n\nclass ActionCountProfiler(Profiler):\n    def __init__(self, dirpath=None, filename=None):\n        super().__init__(dirpath=dirpath, filename=filename)\n        self._action_count = defaultdict(int)\n        self._action_first_occurrence = {}\n\n    def start(self, action_name):\n        if action_name not in self._action_first_occurrence:\n            self._action_first_occurrence[action_name] = time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n\n    def stop(self, action_name):\n        self._action_count[action_name] += 1\n\n    def summary(self):\n        res = f\"\\nProfile Summary: \\n\"\n        max_len = max(len(x) for x in self._action_count)\n\n        for action_name in self._action_count:\n            # generate summary for actions called more than once\n            if self._action_count[action_name] > 1:\n                res += (\n                    f\"{action_name:<{max_len}s} \\t \"\n                    + \"self._action_first_occurrence[action_name]} \\t \"\n                    + \"{self._action_count[action_name]} \\n\"\n                )\n\n        return res\n\n    def teardown(self, stage):\n        self._action_count = {}\n        self._action_first_occurrence = {}\n        super().teardown(stage=stage)\n```\n```python\ntrainer = Trainer(profiler=ActionCountProfiler())\ntrainer.fit(...)\n```\n----",
    "parent_index": 49,
    "index": 246
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_expert.html",
    "label": "docs",
    "title": "Profile custom actions of interest",
    "text": "To profile a specific action of interest, reference a profiler in the LightningModule.\n\n```python\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n```\nTo profile in any part of your code, use the **self.profiler.profile()** function\n\n```python\nclass MyModel(LightningModule):\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n```\nHere's the full code:\n\n```python\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n\nprofiler = SimpleProfiler()\nmodel = MyModel(profiler)\ntrainer = Trainer(profiler=profiler, max_epochs=1)\n```",
    "parent_index": 49,
    "index": 247
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_intermediate.html",
    "label": "docs",
    "title": "Profile pytorch operations",
    "text": "To understand the cost of each PyTorch operation, use the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler built on top of the PyTorch profiler_.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler()\ntrainer = Trainer(profiler=profiler)\n```\nThe profiler will generate an output like this:\n\n```\nProfiler Report\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\nWhen using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\nThis is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\nIt is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\nthe SimpleProfiler.\n----",
    "parent_index": 50,
    "index": 248
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_intermediate.html",
    "label": "docs",
    "title": "Profile a distributed model",
    "text": "To profile a distributed model, use the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler with the *filename* argument which will save a report per rank.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(filename=\"perf-logs\")\ntrainer = Trainer(profiler=profiler)\n```\nWith two ranks, it will generate a report like so:\n\n```\nProfiler Report: rank 0\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\n```\nProfiler Report: rank 1\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      42.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n```\nThis profiler will record training_step, validation_step, test_step, and predict_step.\nThe output above shows the profiling for the action training_step.\n\nWhen using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\nThis is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\nIt is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\nthe SimpleProfiler.\n----",
    "parent_index": 50,
    "index": 249
  },
  {
    "file": "docs/pytorch/stable/tuning/profiler_intermediate.html",
    "label": "docs",
    "title": "Visualize profiled operations",
    "text": "To visualize the profiled operations, enable **emit_nvtx** in the ~lightning.pytorch.profilers.pytorch.PyTorchProfiler.\n\n```python\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(emit_nvtx=True)\ntrainer = Trainer(profiler=profiler)\n```\nThen run as following:\n\n```\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n```\nTo visualize the profiled operation, you can either use **nvvp**:\n\n```\nnvvp trace_name.prof\n```\nor python:\n\n```\npython -c 'import torch; print(torch.autograd.profiler.load_nvprof(\"trace_name.prof\"))'\n```",
    "parent_index": 50,
    "index": 250
  },
  {
    "file": "docs/pytorch/stable/visualize/experiment_managers.html",
    "label": "docs",
    "title": "Manage Experiments",
    "text": "To track other artifacts, such as histograms or model topology graphs first select one of the many experiment managers (*loggers*) supported by Lightning\n\n```python\nfrom lightning.pytorch import loggers as pl_loggers\n\ntensorboard = pl_loggers.TensorBoardLogger()\ntrainer = Trainer(logger=tensorboard)\n```\nthen access the logger's API directly\n\n```python\ndef training_step(self):\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n```\n----",
    "parent_index": 51,
    "index": 251
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Change progress bar defaults",
    "text": "To change the default values (ie: version number) shown in the progress bar, override the ~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar.get_metrics method in your logger.\n\n```python\nfrom lightning.pytorch.callbacks.progress import Tqdm\n\nclass CustomProgressBar(Tqdm):\n    def get_metrics(self, *args, **kwargs):\n        # don't show the version number\n        items = super().get_metrics()\n        items.pop(\"v_num\", None)\n        return items\n```\n----",
    "parent_index": 52,
    "index": 252
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Modify logging frequency",
    "text": "Logging a metric on every single batch can slow down training. By default, Lightning logs every 50 rows, or 50 training steps.\nTo change this behaviour, set the *log_every_n_steps* ~lightning.pytorch.trainer.trainer.Trainer flag.\n\nk = 10\ntrainer = Trainer(log_every_n_steps=k)\n----",
    "parent_index": 52,
    "index": 253
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Modify flushing frequency",
    "text": "Some loggers keep logged metrics in memory for N steps and only periodically flush them to disk to improve training efficiency.\nEvery logger handles this a bit differently. For example, here is how to fine-tune flushing for the TensorBoard logger:\n\n```python\n# Default used by TensorBoard: Write to disk after 10 logging events or every two minutes\nlogger = TensorBoardLogger(..., max_queue=10, flush_secs=120)\n\n# Faster training, more memory used\nlogger = TensorBoardLogger(..., max_queue=100)\n\n# Slower training, less memory used\nlogger = TensorBoardLogger(..., max_queue=1)\n```\n----",
    "parent_index": 52,
    "index": 254
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Customize self.log",
    "text": "The LightningModule *self.log* method offers many configurations to customize its behavior.\n\n----",
    "parent_index": 52,
    "index": 255
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "add_dataloader_idx",
    "text": "**Default:** True\n\nIf True, appends the index of the current dataloader to the name (when using multiple dataloaders). If False, user needs to give unique names for each dataloader to not mix the values.\n\n```python\nself.log(add_dataloader_idx=True)\n```\n----",
    "parent_index": 52,
    "index": 256
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "batch_size",
    "text": "**Default:** None\n\nCurrent batch size used for accumulating logs logged with on_epoch=True. This will be directly inferred from the loaded batch, but for some data structures you might need to explicitly provide it.\n\n```python\nself.log(batch_size=32)\n```\n----",
    "parent_index": 52,
    "index": 257
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "enable_graph",
    "text": "**Default:** True\n\nIf True, will not auto detach the graph.\n\n```python\nself.log(enable_graph=True)\n```\n----",
    "parent_index": 52,
    "index": 258
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "logger",
    "text": "**Default:** True\n\nSend logs to the logger like Tensorboard, or any other custom logger passed to the ~lightning.pytorch.trainer.trainer.Trainer (Default: True).\n\n```python\nself.log(logger=True)\n```\n----",
    "parent_index": 52,
    "index": 259
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "on_epoch",
    "text": "**Default:** It varies\n\nIf this is True, that specific *self.log* call accumulates and reduces all metrics to the end of the epoch.\n\n```python\nself.log(on_epoch=True)\n```\nThe default value depends in which function this is called\n\n```python\ndef training_step(self, batch, batch_idx):\n  # Default: False\n  self.log(on_epoch=False)\n\ndef validation_step(self, batch, batch_idx):\n  # Default: True\n  self.log(on_epoch=True)\n\ndef test_step(self, batch, batch_idx):\n  # Default: True\n  self.log(on_epoch=True)\n```\n----",
    "parent_index": 52,
    "index": 260
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "on_step",
    "text": "**Default:** It varies\n\nIf this is True, that specific *self.log* call will NOT accumulate metrics. Instead it will generate a timeseries across steps.\n\n```python\nself.log(on_step=True)\n```\nThe default value depends in which function this is called\n\n```python\ndef training_step(self, batch, batch_idx):\n  # Default: True\n  self.log(on_step=True)\n\ndef validation_step(self, batch, batch_idx):\n  # Default: False\n  self.log(on_step=False)\n\ndef test_step(self, batch, batch_idx):\n  # Default: False\n  self.log(on_step=False)\n```\n----",
    "parent_index": 52,
    "index": 261
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "prog_bar",
    "text": "**Default:** False\n\nIf set to True, logs will be sent to the progress bar.\n\n```python\nself.log(prog_bar=True)\n```\n----",
    "parent_index": 52,
    "index": 262
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "rank_zero_only",
    "text": "**Default:** False\n\nTells Lightning if you are calling self.log from every process (default) or only from rank 0.\nThis is for advanced users who want to reduce their metric manually across processes, but still want to benefit from automatic logging via self.log.\n\n- Set False (default) if you are calling self.log from every process.\n- Set True if you are calling self.log from rank 0 only. Caveat: you won't be able to use this metric as a monitor in callbacks (e.g., early stopping).\n\n```python\n# Default\nself.log(..., rank_zero_only=False)\n\n# If you call `self.log` on rank 0 only, you need to set `rank_zero_only=True`\nif self.trainer.global_rank == 0:\n    self.log(..., rank_zero_only=True)\n\n# DON'T do this, it will cause deadlocks!\nself.log(..., rank_zero_only=True)\n```\n----",
    "parent_index": 52,
    "index": 263
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "reduce_fx",
    "text": "**Default:** torch.mean\n\nReduction function over step values for end of epoch. Uses torch.mean by default and is not applied when a torchmetrics.Metric is logged.\n\n```python\nself.log(..., reduce_fx=torch.mean)\n```\n----",
    "parent_index": 52,
    "index": 264
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "sync_dist",
    "text": "**Default:** False\n\nIf True, reduces the metric across devices. Use with care as this may lead to a significant communication overhead.\n\n```python\nself.log(sync_dist=False)\n```\n----",
    "parent_index": 52,
    "index": 265
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "sync_dist_group",
    "text": "**Default:** None\n\nThe DDP group to sync across.\n\n```python\nimport torch.distributed as dist\n\ngroup = dist.init_process_group(\"nccl\", rank=self.global_rank, world_size=self.world_size)\nself.log(sync_dist_group=group)\n```\n----",
    "parent_index": 52,
    "index": 266
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Enable metrics for distributed training",
    "text": "For certain types of metrics that need complex aggregation, we recommended to build your metric using torchmetric which ensures all the complexities of metric aggregation in distributed environments is handled.\n\nFirst, implement your metric:\n\n```python\nimport torch\nimport torchmetrics\n\nclass MyAccuracy(Metric):\n  def __init__(self, dist_sync_on_step=False):\n      # call `self.add_state`for every internal state that is needed for the metrics computations\n      # dist_reduce_fx indicates the function that should be used to reduce\n      # state from multiple processes\n      super().__init__(dist_sync_on_step=dist_sync_on_step)\n\n      self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n      self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n  def update(self, preds: torch.Tensor, target: torch.Tensor):\n      # update metric states\n      preds, target = self._input_format(preds, target)\n      assert preds.shape == target.shape\n\n      self.correct += torch.sum(preds == target)\n      self.total += target.numel()\n\n  def compute(self):\n      # compute final result\n      return self.correct.float() / self.total\n```\nTo use the metric inside Lightning, 1) initialize it in the init, 2) compute the metric, 3) pass it into *self.log*\n\n```python\nclass LitModel(LightningModule):\n  def __init__(self):\n      # 1. initialize the metric\n      self.accuracy = MyAccuracy()\n\n  def training_step(self, batch, batch_idx):\n      x, y = batch\n      preds = self(x)\n\n      # 2. compute the metric\n      self.accuracy(preds, y)\n\n      # 3. log it\n      self.log(\"train_acc_step\", self.accuracy)\n```\n----",
    "parent_index": 52,
    "index": 267
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Log to a custom cloud filesystem",
    "text": "Lightning is integrated with the major remote file systems including local filesystems and several cloud storage providers such as\nS3 on AWS, GCS on Google Cloud,\nor ADL on Azure.\n\nPyTorch Lightning uses fsspec internally to handle all filesystem operations.\n\nTo save logs to a remote filesystem, prepend a protocol like \"s3:/\" to the root_dir used for writing and reading model data.\n\n```python\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nlogger = TensorBoardLogger(save_dir=\"s3://my_bucket/logs/\")\n\ntrainer = Trainer(logger=logger)\ntrainer.fit(model)\n```\n----",
    "parent_index": 52,
    "index": 268
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Track both step and epoch metrics",
    "text": "To track the timeseries over steps (*on_step*) as well as the accumulated epoch metric (*on_epoch*), set both to True\n\n```python\nself.log(on_step=True, on_epoch=True)\n```\nSetting both to True will generate two graphs with *_step* for the timeseries over steps and *_epoch* for the epoch metric.\n\n----",
    "parent_index": 52,
    "index": 269
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "Understand self.log automatic behavior",
    "text": "This table shows the default values of *on_step* and *on_epoch* depending on the *LightningModule* or *Callback* method.\n\n----",
    "parent_index": 52,
    "index": 270
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "In LightningModule",
    "text": "* - Method\n- on_step\n- on_epoch\n* - on_after_backward, on_before_backward, on_before_optimizer_step, optimizer_step, configure_gradient_clipping, on_before_zero_grad, training_step\n- True\n- False\n* - test_step, validation_step\n- False\n- True\n----",
    "parent_index": 52,
    "index": 271
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_advanced.html",
    "label": "docs",
    "title": "In Callback",
    "text": "* - Method\n- on_step\n- on_epoch\n* - on_after_backward, on_before_backward, on_before_optimizer_step, on_before_zero_grad, on_train_batch_start, on_train_batch_end\n- True\n- False\n* - on_train_epoch_start, on_train_epoch_end, on_train_start, on_validation_batch_start, on_validation_batch_end, on_validation_start, on_validation_epoch_start, on_validation_epoch_end\n- False\n- True",
    "parent_index": 52,
    "index": 272
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "Why do I need to track metrics?",
    "text": "In model development, we track values of interest such as the *validation_loss* to visualize the learning process for our models. Model development is like driving a car without windows, charts and logs provide the *windows* to know where to drive the car.\n\nWith Lightning, you can visualize virtually anything you can think of: numbers, text, images, audio. Your creativity and imagination are the only limiting factor.\n\n----",
    "parent_index": 53,
    "index": 273
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "Track metrics",
    "text": "Metric visualization is the most basic but powerful way of understanding how your model is doing throughout the model development process.\n\nTo track a metric, simply use the *self.log* method available inside the *LightningModule*\n\n```python\nclass LitModel(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        value = ...\n        self.log(\"some_value\", value)\n```\nTo log multiple metrics at once, use *self.log_dict*\n\n```python\nvalues = {\"loss\": loss, \"acc\": acc, \"metric_n\": metric_n}  # add more items if needed\nself.log_dict(values)\n```\n----",
    "parent_index": 53,
    "index": 274
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "View in the commandline",
    "text": "To view metrics in the commandline progress bar, set the *prog_bar* argument to True.\n\n```python\nself.log(..., prog_bar=True)\n```\n```bash\nEpoch 3:  33%|███▉        | 307/938 [00:01<00:02, 289.04it/s, loss=0.198, v_num=51, acc=0.211, metric_n=0.937]\n```\n----",
    "parent_index": 53,
    "index": 275
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "View in the browser",
    "text": "To view metrics in the browser you need to use an *experiment manager* with these capabilities.\n\nBy Default, Lightning uses Tensorboard (if available) and a simple CSV logger otherwise.\n\n```python\n# every trainer already has tensorboard enabled by default (if the dependency is available)\ntrainer = Trainer()\n```\nTo launch the tensorboard dashboard run the following command on the commandline.\n\n```bash\ntensorboard --logdir=lightning_logs/\n```\nIf you're using a notebook environment such as *colab* or *kaggle* or *jupyter*, launch Tensorboard with this command\n\n```bash\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/\n```\n----",
    "parent_index": 53,
    "index": 276
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "Accumulate a metric",
    "text": "When *self.log* is called inside the *training_step*, it generates a timeseries showing how the metric behaves over time.\n\nHowever, For the validation and test sets we are not generally interested in plotting the metric values per batch of data. Instead, we want to compute a summary statistic (such as average, min or max) across the full split of data.\n\nWhen you call self.log inside the *validation_step* and *test_step*, Lightning automatically accumulates the metric and averages it once it's gone through the whole split (*epoch*).\n\n```python\ndef validation_step(self, batch, batch_idx):\n    value = batch_idx + 1\n    self.log(\"average_value\", value)\n```\nIf you don't want to average you can also choose from {min,max,sum} by passing the *reduce_fx* argument.\n\n```python\n# default function\nself.log(..., reduce_fx=\"mean\")\n```\nFor other reductions, we recommend logging a torchmetrics.Metric instance instead.\n\n----",
    "parent_index": 53,
    "index": 277
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_basic.html",
    "label": "docs",
    "title": "Configure the saving directory",
    "text": "By default, anything that is logged is saved to the current working directory. To use a different directory, set the *default_root_dir* argument in the Trainer.\n\n```python\nTrainer(default_root_dir=\"/your/custom/path\")\n```",
    "parent_index": 53,
    "index": 278
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Change the progress bar",
    "text": "If you'd like to change the way the progress bar displays information you can use some of our built-in progress bard or build your own.\n\n----",
    "parent_index": 54,
    "index": 279
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Use the TQDMProgressBar",
    "text": "To use the TQDMProgressBar pass it into the *callbacks* ~lightning.pytorch.trainer.trainer.Trainer argument.\n\n```python\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\ntrainer = Trainer(callbacks=[TQDMProgressBar()])\n```\n----",
    "parent_index": 54,
    "index": 280
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Use the RichProgressBar",
    "text": "The RichProgressBar can add custom colors and beautiful formatting for your progress bars. First, install the *rich* library\n\n```bash\npip install rich\n```\nThen pass the callback into the callbacks ~lightning.pytorch.trainer.trainer.Trainer argument:\n\n```python\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar()])\n```\nThe rich progress bar can also have custom themes\n\n```python\nfrom lightning.pytorch.callbacks import RichProgressBar\nfrom lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n# create your own theme!\ntheme = RichProgressBarTheme(description=\"green_yellow\", progress_bar=\"green1\")\n\n# init as normal\nprogress_bar = RichProgressBar(theme=theme)\ntrainer = Trainer(callbacks=progress_bar)\n```\n----",
    "parent_index": 54,
    "index": 281
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Customize a progress bar",
    "text": "To customize either the ~lightning.pytorch.callbacks.TQDMProgressBar or the ~lightning.pytorch.callbacks.RichProgressBar, subclass it and override any of its methods.\n\n```python\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\nclass LitProgressBar(TQDMProgressBar):\n    def init_validation_tqdm(self):\n        bar = super().init_validation_tqdm()\n        bar.set_description(\"running validation...\")\n        return bar\n```\n----",
    "parent_index": 54,
    "index": 282
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Build your own progress bar",
    "text": "To build your own progress bar, subclass ~lightning.pytorch.callbacks.ProgressBar\n\n```python\nfrom lightning.pytorch.callbacks import ProgressBar\n\nclass LitProgressBar(ProgressBar):\n    def __init__(self):\n        super().__init__()  # don't forget this :)\n        self.enable = True\n\n    def disable(self):\n        self.enable = False\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):\n        super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)\n        percent = (self.train_batch_idx / self.total_train_batches) * 100\n        sys.stdout.flush()\n        sys.stdout.write(f\"{percent:.01f} percent complete \\r\")\n\nbar = LitProgressBar()\ntrainer = Trainer(callbacks=[bar])\n```\n----",
    "parent_index": 54,
    "index": 283
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_expert.html",
    "label": "docs",
    "title": "Integrate an experiment manager",
    "text": "To create an integration between a custom logger and Lightning, subclass ~lightning.pytorch.loggers.Logger\n\n```python\nfrom lightning.pytorch.loggers import Logger\n\nclass LitLogger(Logger):\n    @property\n    def name(self) -> str:\n        return \"my-experiment\"\n\n    @property\n    def version(self):\n        return \"version_0\"\n\n    def log_metrics(self, metrics, step=None):\n        print(\"my logged metrics\", metrics)\n\n    def log_hyperparams(self, params, *args, **kwargs):\n        print(\"my logged hyperparameters\", params)\n```",
    "parent_index": 54,
    "index": 284
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_intermediate.html",
    "label": "docs",
    "title": "Track audio and other artifacts",
    "text": "To track other artifacts, such as histograms or model topology graphs first select one of the many loggers supported by Lightning\n\n```python\nfrom lightning.pytorch import loggers as pl_loggers\n\ntensorboard = pl_loggers.TensorBoardLogger(save_dir=\"\")\ntrainer = Trainer(logger=tensorboard)\n```\nthen access the logger's API directly\n\n```python\ndef training_step(self):\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n```\n----\n\n----",
    "parent_index": 55,
    "index": 285
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_intermediate.html",
    "label": "docs",
    "title": "Track hyperparameters",
    "text": "To track hyperparameters, first call *save_hyperparameters* from the LightningModule init:\n\n```python\nclass MyLightningModule(LightningModule):\n    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n```\nIf your logger supports tracked hyperparameters, the hyperparameters will automatically show up on the logger dashboard.\n\n----",
    "parent_index": 55,
    "index": 286
  },
  {
    "file": "docs/pytorch/stable/visualize/logging_intermediate.html",
    "label": "docs",
    "title": "Track model topology",
    "text": "Multiple loggers support visualizing the model topology. Here's an example that tracks the model topology using Tensorboard.\n\n```python\ndef any_lightning_module_function_or_hook(self):\n    tensorboard_logger = self.logger\n\n    prototype_array = torch.Tensor(32, 1, 28, 27)\n    tensorboard_logger.log_graph(model=self, input_array=prototype_array)\n```",
    "parent_index": 55,
    "index": 287
  }
]