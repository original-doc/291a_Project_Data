[
  {
    "title": "ValueError(f\"{func} does not have a return type annotation\") when implementing LightningCLI",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19637",
    "createdAt": "2024-03-15T14:13:40Z",
    "author": {
      "login": "djtschke"
    },
    "bodyText": "Dear All,\nI am trying to implement the LightningCLI class, but it tells me my model does not have any Type annotation, although I added class function type return type annotation.\nWhat am I doing wrong?\nThank you for your input!\nLG\nMax",
    "answer": {
      "author": {
        "login": "djtschke"
      },
      "bodyText": "Hi Mauvilsa,\nthank you for your reply. While trying to come up with a minimal reproduction example I actually found the solution to my problem:\nLightningCLI() does not take the instance of the model as a parameter but the class itself (as I guess instantiation is then done by the pipeline itself). To be more clear:\nWrong / Not Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nmodel = MyFancyModelClass()\ndatamodule = MyFancyDataModuleClass()\n\nLightningCLI = LightningCLI(model, datamodule)\n\nCorrect / Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nLightningCLI = LightningCLI(MyFancyModelClass, MyFancyDataModuleClass)\n\nHope that also helps others!\nCheers,\nMax"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 0
  },
  {
    "title": "Multiple Sequential trainings slows down speed",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17490",
    "createdAt": "2023-04-27T07:43:47Z",
    "author": {
      "login": "HadiSDev"
    },
    "bodyText": "Hi.\nI have a task where I need to run a training script multiple time with a for-loop like this:\nfor dataset in datasets:\n      ... Training script with datamodule, model, Trainer Init and trainer.fit()\n\nHowever, after each loop the the training it self slows down incrementally (epoch / sec). I am thinking it is because I need to reset something but I have not been able to find that information. I am using Torch-cpu 2.0.0\nAny idea on what I am doing wrong? :(",
    "answer": {
      "author": {
        "login": "HadiSDev"
      },
      "bodyText": "I tried to remove the neptune logger and model saving functionality and it seemed to have solved the issue"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 1
  },
  {
    "title": "TypeError: on_validation_end() missing 1 required positional argument: 'outputs'",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17182",
    "createdAt": "2023-03-23T22:20:17Z",
    "author": {
      "login": "dimgag"
    },
    "bodyText": "Hello,\nWhen I train my model, I get this error:\nTypeError: on_validation_end() missing 1 required positional argument: 'outputs'\nThis is the function inside my model class:\ndef on_validation_end(self, outputs):\n     loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n     metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n     self.log('val_loss', loss)\n     self.log('val_metric', metric)",
    "answer": {
      "author": {
        "login": "AzulGarza"
      },
      "bodyText": "hey @dimgag! I had the same problem recently. As of lightning>=2.0.0 you have to define the validation_step_outputs in the init clause of your module to store the validation outputs of each epoch. Here's an example (from this PR).\n class MyLightningModule(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.validation_step_outputs = []\n\n     def validation_step(self, ...):\n         loss = ...\n         self.validation_step_outputs.append(loss)\n         return loss\n\n    def on_validation_epoch_end(self):\n        epoch_average = torch.stack(self.validation_step_outputs).mean()\n        self.log(\"validation_epoch_average\", epoch_average)\n        self.validation_step_outputs.clear()  # free memory"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 2
  },
  {
    "title": "Current best practices to initialize massive (50B parameter+) models",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16944",
    "createdAt": "2023-03-03T15:44:50Z",
    "author": {
      "login": "azton"
    },
    "bodyText": "Hi, I am working with GPT-style models and need to intitialize a model at the GPT-3 scale.  Unfortunately, this means the model will run out of memory during initialization on CPU (or take an eternity to initialize layer-by-layer on cpu before shipping to GPU).  In vanilla Pytorch I solved this using FSDP by initializing my models on the \"meta\" device, with full initialization on GPU afterward.  What is the current best, most performant method to accomplish this with lightning?\nNote: I found this, which references an init_meta_context(), but my pytorch-lightning (v1.9.0) has no such functionality:\nhttps://devblog.pytorchlightning.ai/experiment-with-billion-parameter-models-faster-using-deepspeed-and-meta-tensors-2e9c255edd71",
    "answer": {
      "author": {
        "login": "carmocca"
      },
      "bodyText": "Hi! The init_meta_context functionality was replaced with a torchdistx integration in #13868. You can do the following:\nfrom torchdistx.deferred_init import deferred_init\n\nmodel = deferred_init(YourLightningModule)\nAnd we'll materialize it for you in the Trainer. This is very experimental, and you might encounter installation issues.\nIn the long term, we'll adopt the fake tensor mode from PyTorch: #16448.\nOtherwise, for a stable(r) solution, you can use the DeepSpeed integration: https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed-zero-stage-3"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 3
  },
  {
    "title": "MLFlow UI and save_dir specification",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16112",
    "createdAt": "2022-12-19T09:39:19Z",
    "author": null,
    "bodyText": "Hello,\nWhen I use the MLFlowLogger without setting save_dir argument, then run mlflow ui, I have no issue to display artefacts.\nIn mlruns/438809872721685038/d0b0373f48c549f3b36e97f5017dda71/meta.yml, I see\nartifact_uri: file:./mlruns/438809872721685038/d0b0373f48c549f3b36e97f5017dda71/artifacts\nWhen I set save_dir (here \"experiments/mlruns\"), then run cd experiments;mlflow ui, I don't see the artefacts.\nIn experiments/mlruns/933121669842833043/64cd5a18fe71423d8c9423a320ee7b91/meta.yml, I see\nartifact_uri: file:experiments/mlruns/933121669842833043/64cd5a18fe71423d8c9423a320ee7b91/artifacts\nI fix it by setting the tracking_uri to set an absolute path, like this :\nsave_dir = \"experiments/mlruns\"\npl.loggers.mlflow.MLFlowLogger(\n    save_dir=save_dir,\n    experiment_name=experiment_name,\n    run_name=run_name,\n    tracking_uri=f\"file://{save_dir}\"\n)\n\nIs there a better way to fix it ?\nIs it a good idea to open an issue about it ?",
    "answer": {
      "author": {
        "login": "akihironitta"
      },
      "bodyText": "@NicolasNeovision It would be great if you could submit a bug report by creating a GitHub issue with a minimal script and env detail. Thank you \u26a1"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 4
  },
  {
    "title": "Accuracy/loss is not improving when train huggingface transformer bert",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15044",
    "createdAt": "2022-10-08T13:48:41Z",
    "author": {
      "login": "Klassikcat"
    },
    "bodyText": "Summary\nLoss does not decrease and accuracy/F1-score is not improving during training HuggingFace Transformer BertForSequenceClassification with Pytorch-Lightning\nIssue\nHello PTL team, previously, I trained huggingface bert model with my own trainer code. To improve code quality and implement MLOps system, I\u2019m trying to train huggingface\u2019s transformers Bert with pytorch lightning.\nWhen I train BertForSequenceClassification in the transformers with PTL, however, Loss, accuracy, and even f1 score seems to not improve during a training phase. I think there are some bugs in the optimizer or back-propagation in my code, but I can\u2019t find any problems. my question is, what is the problem with my code?\nmy assumtions:\n\nconfigure_optimizer is not correctly configured\nhuggingface sequence classification module\nsegment_ids should not be passes during training step\netc.\n\npackage versions\npython==3.7\npytorch==1.11.0\npytorch-lightning == 1.7.7\ntransformers == 4.2.2\ntorchmetrics == up-to-date\ncode snippet\nimport os\nimport csv\nimport tqdm\nfrom typing import *\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nimport pytorch_lightning as pl\nimport torchmetrics.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers import BertForSequenceClassification, BertTokenizer, InputExample, InputFeatures, AutoConfig, get_linear_schedule_with_warmup\n\n\ndef get_input_example(guid: int, text_a: str, label: str) -> Tuple[InputExample, str]:\n    input_example = InputExample(\n        guid=guid,\n        text_a=text_a,\n        text_b=None,\n        label=label\n    )\n    return input_example, label\n\n\ndef add_examples(\n        texts_or_text_and_labels: Union[List[str], str],\n        text_index: int,\n        label_index: int,\n        label_dict: Dict[str, int] = None,\n        remove_top: bool = False\n) -> Tuple[List[InputExample], Dict[str, int]]:\n    examples = list()\n    labels = list()\n\n    tmp = []\n    if isinstance(texts_or_text_and_labels, str):\n        with open(texts_or_text_and_labels, 'r') as f:\n            if texts_or_text_and_labels.endswith('csv'):\n                delimiter = ','\n            elif texts_or_text_and_labels.endswith('tsv'):\n                delimiter='\\t'\n\n            reader = csv.reader(f, delimiter=delimiter, quotechar='\"')\n            for idx, line in enumerate(tqdm.tqdm(reader)):\n                if remove_top is True and idx == 0:\n                    pass\n                else:\n                    tmp.append(line)\n        texts_or_text_and_labels = tmp\n\n    for line in tqdm.tqdm(texts_or_text_and_labels):\n        text_a = line[text_index]\n        label = line[label_index]\n\n        input_example, label = get_input_example(guid=line[0], text_a=text_a, label=label)\n        examples.append(input_example)\n        if label_dict is None:\n            labels.append(label)\n    if label_dict is None:\n        label_dict = {i: idx for idx, i in enumerate(list(set(labels)))}\n    return examples, label_dict\n\n\nclass BertDataset(Dataset):\n    def __init__(self, examples, tokenizer, label_dict, max_length):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.label_dict = label_dict\n        self.max_length = max_length\n\n    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length) -> None:\n        \"\"\"\n        Truncates a sequence pair in place to the maximum length.\n        This is a simple heuristic which will always truncate the longer sequence\n        one token at a time. This makes more sense than truncating an equal percent\n        of tokens from each, since if one sequence is very short then each token\n        that's truncated likely contains more information than a longer sequence.\n        \"\"\"\n        while True:\n            total_length = len(tokens_a) + len(tokens_b)\n            if total_length <= max_length - 3:\n                break\n            if len(tokens_a) > len(tokens_b):\n                tokens_a.pop()\n            else:\n                tokens_b.pop()\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(\n            self,\n            idx: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\n        def _tokens_and_segment_id(token_a: List[str], token_b: List[str] = None) -> Tuple[Any, List[int]]:\n            tokens = ['[CLS]'] + token_a + ['[SEP]']  # See in 1-1. Section in /docs/Appendix.md\n            token_type_ids = [0] * len(tokens)  # for more information of 138-145 lines\n            if token_b:\n                tokens += token_b + ['[SEP]']\n                token_type_ids += [1] * (len(token_b) + 1)\n            return tokens, token_type_ids\n\n        text_a = self.examples[idx].text_a\n        text_b = self.examples[idx].text_b\n        label = self.examples[idx].label\n\n            #   Convert texts into tokens\n        tokens_a = self.tokenizer.tokenize(text_a)\n        tokens_b = None\n        if text_b:\n            tokens_b = self.tokenizer.tokenize(text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with '- 3'\n            self._truncate_seq_pair(tokens_a, tokens_b, self.max_length)\n        else:\n            if len(tokens_a) > self.max_length - 2:\n                tokens_a = tokens_a[:(self.max_length - 2)]\n\n        tokens, token_type_ids = _tokens_and_segment_id(tokens_a, tokens_b)\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        label_ids = self.label_dict[label]\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        attention_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (self.max_length - len(input_ids))\n        input_ids += padding\n        attention_mask += padding\n        token_type_ids += padding\n\n        assert len(input_ids) == self.max_length\n        assert len(attention_mask) == self.max_length\n        assert len(token_type_ids) == self.max_length\n\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\n        labels = torch.tensor(label_ids, dtype=torch.long)\n        return input_ids, attention_mask, token_type_ids, labels\n\n\nclass BertAccTestModel(pl.LightningModule):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.save_hyperparameters()\n        self.config = AutoConfig.from_pretrained('klue/bert-base', num_labels=num_classes)\n        self.model = BertForSequenceClassification.from_pretrained('klue/bert-base', config=self.config)\n        self.tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n        self.num_classes = num_classes\n\n    def forward(self, init_ids, input_mask, segment_ids) -> SequenceClassifierOutput:\n        outputs = self.model(init_ids, input_mask, segment_ids)\n        return outputs\n\n    def info(self, dictionary: dict) -> None:\n        r\"\"\"\n        Logging information from dictionary.\n        Args:\n            dictionary (dict): dictionary contains information.\n        \"\"\"\n        for key, value in dictionary.items():\n            self.log(key, value, prog_bar=True, sync_dist=True)\n\n    def training_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\n        init_ids, input_mask, segment_ids, label_ids = batch\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\n        top1_acc = F.accuracy(outputs.logits, label_ids)\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\n        loss = outputs.loss\n        self.info({\n            'train_loss': loss,\n            'train_acc': top1_acc,\n            'train_f1': top1_f1,\n        })\n        return loss\n\n    def validation_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\n        init_ids, input_mask, segment_ids, label_ids = batch\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\n        top1_acc = F.accuracy(outputs.logits, label_ids)\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\n        loss = outputs.loss\n        self.info({\n            'val_loss': loss,\n            'val_acc': top1_acc,\n            'val_f1': top1_f1,\n        })\n        return loss\n\n    def test_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\n        init_ids, input_mask, segment_ids, label_ids = batch\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\n        top1_acc = F.accuracy(outputs.logits, label_ids)\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\n        loss = outputs.loss\n        self.info({\n            'test_loss': loss,\n            'test_acc': top1_acc,\n            'test_f1': top1_f1,\n        })\n        return loss\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n        init_ids, input_mask, segment_ids, label_ids = batch\n        outputs = self(init_ids, input_mask, segment_ids, label_ids)\n        return torch.argmax(outputs.logits)\n\n    def configure_optimizers(self):\n        model = self.model\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optim = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n        scheduler = get_linear_schedule_with_warmup(\n            optim,\n            num_warmup_steps=500,\n            num_training_steps=self.trainer.estimated_stepping_batches,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optim], [scheduler]\n\n\ndef main():\n    root = '/'.join(os.getcwd().split('/'))\n    print(root)\n    tokenizer = BertTokenizer.from_pretrained('klue/bert-base', do_lower_case=False)\n\n    train_examples, label_dict = add_examples(os.path.join(root, 'data/training_merged_1d.tsv'), text_index=2, label_index=7,  remove_top=True)\n    eval_examples, _ = add_examples(os.path.join(root, 'data/validation.tsv'), text_index=2, label_index=7, label_dict=label_dict)\n    test_examples, _ = add_examples(os.path.join(root, 'data/test.tsv'), text_index=2, label_index=7, label_dict=label_dict, remove_top=True)\n    train_dataset = BertDataset(train_examples, tokenizer, max_length=256, label_dict=label_dict)\n    eval_dataset = BertDataset(eval_examples, tokenizer, max_length=256, label_dict=label_dict)\n    test_dataset = BertDataset(test_examples, tokenizer, max_length=256, label_dict=label_dict)\n\n    print(len(train_dataset), len(eval_dataset), len(test_dataset))\n    trn_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=4, sampler=RandomSampler(train_dataset))\n    eval_dataloader = DataLoader(eval_dataset, batch_size=64, num_workers=4, sampler=SequentialSampler(eval_dataset))\n    test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=4, sampler=SequentialSampler(test_dataset))\n    inference_label_dict = {v: k for k, v in label_dict.items()}\n\n    model = BertAccTestModel(num_classes= len(label_dict))\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        save_last=True,\n        save_weights_only=True,\n        monitor='val_f1',\n        mode='max',\n        dirpath=os.path.join(root, 'weights'),\n        filename='pytorch_model'\n    )\n    trainer = pl.Trainer(gpus=4, max_epochs=10, accelerator='cuda', strategy='ddp', precision=32, callbacks=[checkpoint_callback])\n    trainer.fit(model, train_dataloaders=trn_dataloader, val_dataloaders=eval_dataloader)\n    trainer.test(model, test_dataloader)\n\n    with open(os.path.join(root, 'weights', 'labels.dict'), 'w') as f:\n        import json\n        json.dump(inference_label_dict, f)\n\nif __name__ == '__main__':\n    main()",
    "answer": {
      "author": {
        "login": "Klassikcat"
      },
      "bodyText": "Updates\nI found that DDP was the problem. Accuracy and loss improve when I train my model on single-GPU, It seems to add_examples code was a problem because they make a label list which is not a fixed index."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 5
  },
  {
    "title": "Using data from training_epoch_end in validation_epoch_end.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14491",
    "createdAt": "2022-09-01T23:51:37Z",
    "author": {
      "login": "amdson"
    },
    "bodyText": "I'm running a metric learning model, and I'd like to use embeddings from the training step for KNN lookups in the validation step. I tried to do so with the following code.\ndef training_epoch_end(self, outputs):\n        embeddings = []\n        for output in outputs:\n            embeddings.append(output[\"embeddings\"])\n        self.training_embeddings = torch.cat(embeddings, dim=0)\n        print(\"Finished training epoch\")\n        return super().training_epoch_end(outputs)\n\ndef validation_epoch_end(self, outputs):\n      train_embeddings = self.train_embeddings\n      #Use train embeddings...\n      self.train_embeddings = None #Delete embeddings for future use.\n\nHowever, self.train_embeddings is either None, or uninitialized, when validation_epoch_end executes. Is there a way to communicate between these two methods, or send information from both to a final callback?",
    "answer": {
      "author": {
        "login": "amdson"
      },
      "bodyText": "Just saw other answers explaining the order of train_epoch_end and val_epoch_end hooks. This issue was solved by moving any code needing both validation and training data to \"on_train_epoch_end\"."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 6
  },
  {
    "title": "Proper way to do contrastive learning with DDP & PT-Lightning",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14390",
    "createdAt": "2022-08-25T15:50:33Z",
    "author": {
      "login": "kkarrancsu"
    },
    "bodyText": "I want to use DDP and experiment with contrastive losses. Since DDP processes each subset of the data independently, negative examples that could be used to increase the contrastive power cannot be taken into account using automatic optimization. Suppose I am training with 2 GPU's and each GPU sees a mini-batch of size 4. This leads to missing signal between (x1, x5), (x1, x6), (x1, x7), etc... since x1-x4 are on GPU1 and x5-x8 are on GPU2.\nWhat is the recommended method to account for this in PT-Lightning?\nOne approach seems to be to use the on_train_batch_end() callback, and 1) gather the outputs from all GPUs, 2) compute the loss on rank=0, and 3) distribute that loss back to each GPU.\nAfter computing loss, I'm unclear as to the mechanics for how to distribute that loss computed on rank=0 back to all of the GPU's so that the gradients are synced. Is this something that happens automatically under the hood, or do I need to do something w.r.t. manual optimization?",
    "answer": {
      "author": {
        "login": "awaelchli"
      },
      "bodyText": "@kkarrancsu You are definitely on the right track here. In the LightningModule, you have this method for gathering a tensor from all processes:\ntensors_from_all = self.all_gather(my_tensor)\nWhat you want is to back-propagate through this all_gather function, and this is possible if you set\ntensors_from_all = self.all_gather(my_tensor, sync_grad=True)\nIn your case, your training_step method could look something like this:\n    def training_step(self, batch, batch_idx):\n        outputs = self(batch)\n        ...\n\n        all_outputs = self.all_gather(outputs, sync_grads=True)\n\n        loss = contrastive_loss_fn(all_outputs, ...)\n        return loss"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 7
  },
  {
    "title": "`RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14377",
    "createdAt": "2022-08-24T01:27:09Z",
    "author": {
      "login": "morestart"
    },
    "bodyText": "RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.\nmy model is transformer.",
    "answer": {
      "author": {
        "login": "morestart"
      },
      "bodyText": "fine i solved this question\nthis is the solution:\nchange PositionalEncoding to this:\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 8
  },
  {
    "title": "How to register a (repeatedly) sampled random tensor?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14131",
    "createdAt": "2022-08-10T02:49:42Z",
    "author": {
      "login": "RylanSchaeffer"
    },
    "bodyText": "In my code, with each batch, I sample a random tensor. However, when I try porting to GPU, I get an error that I'm trying to multiply a tensor on CPU with a tensor on GPU. The documentation states\n\nThe LightningModule knows what device it is on. You can access the reference via self.device. Sometimes it is necessary to store tensors as module attributes. However, if they are not parameters they will remain on the CPU even if the module gets moved to a new device. To prevent that and remain device agnostic, register the tensor as a buffer in your modules\u2019 init method with register_buffer().\n\nThe example given is:\nclass LitModel(LightningModule):\n    def __init__(self):\n        ...\n        self.register_buffer(\"sigma\", torch.eye(3))\n        # you can now access self.sigma anywhere in your module\n\nUsing this example, suppose I want to randomly sample a 3x3 matrix sigma with each batch. How do I properly register this tensor?",
    "answer": {
      "author": {
        "login": "RylanSchaeffer"
      },
      "bodyText": "I just found this answer from 2020: https://stackoverflow.com/questions/63660624/normal-distribution-sampling-in-pytorch-lightning\nIs the best solution currently to specify the device?"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 9
  },
  {
    "title": "Warning during save_hyperparameter() gives misleading advice?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13615",
    "createdAt": "2022-07-12T09:48:33Z",
    "author": {
      "login": "hogru"
    },
    "bodyText": "I try to understand / rectify a warning about saving my hyper parameters and would need some assistance please.\nI build a model this way:\nfrom pytorch_lightning.core.mixins import HyperparametersMixin\n\nclass MyModel(nn.Module, HyperparametersMixin):\n    def __init__(...):\n        super().__init__()\n        self.save_hyperparameters()  # Logs to self.hparams only, not to the logger (since there isn't any yet)\n\nclass MyModule(pl.LightningModule):\n    def __init__(model: nn.Module):\n        super().__init__()\n        self.save_hyperparameters(\"model\", logger=False)\n        self.save_hyperparameters()\n        self.save_hyperparameters(model.hparams)\n\nmodel = MyModel(...)\nmodule = MyModule(model)\nThis works, and I can load a checkpoint with model = MyModule.load_from_checkpoint(model_path_to_load_from)\nBut I also get this warning during the initialization of MyModule: Attribute 'model' is an instance of 'nn.Module' and is already saved during checkpointing. It is recommended to ignore them using 'self.save_hyperparameters(ignore=['model'])'.\nSo I change the corresponding code in MyModule to:\nself.save_hyperparameters(ignore=[\"model\"])\nself.save_hyperparameters(model.hparams)\nThe created checkpoint is marginally reduced by ~3KB, the checkpoint size is ~1MB.\nBut when I want to load the  checkpoint I get this error:\nFile \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 161, in load_from_checkpoint\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\n  File \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 203, in _load_model_state\n    model = cls(**_cls_kwargs)\nTypeError: __init__() missing 1 required keyword-only argument: 'model'\nWhich seems to indicate that I need to save model as a hyper parameter.\nWhat am I missing? What is the correct way to save the hyper parameters / model in the pl.LightningModule?",
    "answer": {
      "author": {
        "login": "rohitgr7"
      },
      "bodyText": "the attributes that are not saved as hparams need to be passed explicitly. Considering you are using load_from_checkpoint API, you can use model = MyModule.load_from_checkpoint(ckpt_path, model=model).\nIf you include it in the hparams, your checkpoints will be unnecessarily big and can create issues if you have large models.\nBy\nis already saved during checkpointing.\n\nit means the model weights are already saved in the checkpoint and are loaded using PyTorch API, not as hparams."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 10
  },
  {
    "title": "when I configure callbacks in the model instead of in the Trainer function, will the model look for the best model or the current model when testing?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13611",
    "createdAt": "2022-07-12T06:39:32Z",
    "author": {
      "login": "Struggle-Forever"
    },
    "bodyText": "A question: when I configure callbacks in the model instead of in the Train function, will the model look for the best model or the current model when testing?\nFor example:  configure the   configure_callbacks in my model\n`\n    def configure_callbacks(self):\n        early_stop_callback = EarlyStopping(monitor=\"val_f1\", min_delta=0.00, patience=self.args.patience,\n                                            verbose=False, mode=\"max\")\n        checkpoint_callback = ModelCheckpoint(monitor='val_f1',\n                                              dirpath=\"{}\".format(self.save_log_path),\n                                              filename='best_{}'.format(self.index_times),\n                                              save_top_k=1,\n                                              mode='max',\n                                              save_last=False)\n        return [checkpoint_callback, early_stop_callback]\n\n    def on_train_start(self):\n        self.print(\"Training is started!\")\n\n    def on_train_end(self):\n        self.print(\"Training is done!\")\n\n`\nNot  configure the   configure_callbacks in the Trainer\ntrainer = Trainer(devices=\"auto\", accelerator=\"auto\", logger=False, multiple_trainloader_mode='max_size_cycle', terminate_on_nan=True, logger=tb_logger, log_every_n_steps=1, flush_logs_every_n_steps=5, max_epochs=args.epochs)\nWhen i perform the trainer  like this:\n`\n    trainer.fit(model)\n\n    result = trainer.test(model)\n\n`\nMy question is: Does the model use the parameters at the last epoch when testing, or the best parameters loaded?",
    "answer": {
      "author": {
        "login": "akihironitta"
      },
      "bodyText": "Does the model use the parameters at the last epoch when testing, or the best parameters loaded?\n\nHi @Struggle-Forever, yes, the model is in the last state, but not in the best state. I hope the following example in the documentation will explain it enough:\n# run full training\ntrainer.fit(model)\n\n# (1) load the best checkpoint automatically (lightning tracks this for you)\ntrainer.test(ckpt_path=\"best\")\n\n# (2) test using a specific checkpoint\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\n\n# (3) test with an explicit model (will use this model and not load a checkpoint)\ntrainer.test(model)\nhttps://pytorch-lightning.readthedocs.io/en/1.6.4/common/evaluation.html#test-after-fit"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 11
  },
  {
    "title": "DDP Hangs with TORCH_DISTRIBUTED_DEBUG = DETAIL",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13503",
    "createdAt": "2022-07-02T00:31:58Z",
    "author": {
      "login": "kelvins64"
    },
    "bodyText": "I'm not certain whether this is user error or a PyTorch/Lightning issue, so am posting a discussion instead.\nAdding the line os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL' while using multiple GPUs and DDP causes the program to hang indefinitely.\nTo reproduce:\nimport argparse\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom pytorch_lightning import LightningModule, Trainer\n\nclass RandomDataset(Dataset):\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\n\nclass BoringModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(32, 2)\n\n    def forward(self, x):\n        return self.layer(x)\n\n    def training_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"train_loss\", loss)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"valid_loss\", loss)\n\n    def test_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"test_loss\", loss)\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n\n\ndef run(cl_args):\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n\n    model = BoringModel()\n\n    # Start changed code\n    import os\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n\n    parser = argparse.ArgumentParser()\n    parser = Trainer.add_argparse_args(parser)\n    args = parser.parse_args(cl_args.split() if cl_args else None)\n    trainer = Trainer.from_argparse_args(args)\n    # End changed code\n\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\n    trainer.test(model, dataloaders=test_data)\n\n\nif __name__ == \"__main__\":\n    run('--gpus 2 --strategy ddp')",
    "answer": {
      "author": {
        "login": "kelvins64"
      },
      "bodyText": "Reading @akihironitta 's response and looking at the documentation again, I noticed that they set the environment variable prior to calling mp.spawn. Moving the os.environ['TORCH_DISTRIBUTED_DEBUG] = 'DETAIL' line outside of the main function prevented hanging.\nimport argparse\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_lightning import LightningModule, Trainer\n\nclass RandomDataset(Dataset):\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nclass BoringModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(32, 2)\n\n    def forward(self, x):\n        return self.layer(x)\n\n    def training_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"train_loss\", loss)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"valid_loss\", loss)\n\n    def test_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"test_loss\", loss)\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n\n# Start changed code\nimport os\nos.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\n# End changed code\n\ndef run(cl_args):\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n\n    model = BoringModel()\n\n    # Start changed code\n    parser = argparse.ArgumentParser()\n    parser = Trainer.add_argparse_args(parser)\n    args = parser.parse_args(cl_args.split() if cl_args else None)\n    trainer = Trainer.from_argparse_args(args)\n    # End changed code\n\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\n    trainer.test(model, dataloaders=test_data)\n\n\nif __name__ == \"__main__\":\n    run('--gpus 2 --strategy ddp')\nI presume this has to do with where the Trainer is forking the process. In summary, it seems one can\n\nSet the environment variable via os.environ outside of the main function\nSet the environment variable in the shell"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 12
  },
  {
    "title": "Training seems to pause every N steps",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13375",
    "createdAt": "2022-06-22T22:18:02Z",
    "author": {
      "login": "mfoglio"
    },
    "bodyText": "I am doing feature extraction using an efficientnet_b0 model. The training process works fine but it seems to pause every once in a while. I verified this using nvidia-smi dmon. There are spikes of a few seconds where the GPU utilization is anywhere between 50% and 100%, followed by a few seconds where the GPU utilization is 0%.\nRight now I am training with 4 Tesla T4, but I verified the same issue with a single GPU (T4 and V100).\nI am using a batch size of 200 (per GPU).  I have 48 CPUs and their usage is pretty low (I'd say 20-40%).\nI noticed the training pausing at epoch 48, 96, 144,... So it pauses every 48 steps.\nI thought that the pause were caused by logging so in my Trainer I set  log_every_n_steps=500 and I also initialize my logger with TensorBoardLogger(\"tb_logs\", name=\"vehicles\", max_queue=1000, flush_secs=120). I can that the processes pauses more frequently than 120 seconds.\nOriginally, I thought it was a PyTorch \"issue\". So I opened a post here https://discuss.pytorch.org/t/gpu-usage-is-not-constant-during-training/154718 . However I am wondering whether this could be caused by torch lightning.\nThank you",
    "answer": {
      "author": {
        "login": "mfoglio"
      },
      "bodyText": "I analyzed the problem a little bit more. I noticed that I have 48 CPUs and 48 workers. That makes the training process pausing every 48 steps. If use 12 workers, the pause happens every 12 steps.\nI'd like to increase the number of workers but the RAM usage is crazy high. With 48 workers I am almost using all the 180Gb of RAM available. Is this normal for simply loading images of a few Kbytes?\nAny suggestion on how to speed this up?\nEDIT: I think I am facing this issue pytorch/pytorch#13246 (comment) even though I am not entirely sure. My memory consumption is of about 100-150 gb right after the training starts. I tried to used a numpy array to store the huge list of integers containing the IDs of the record in the dataset. However, this didn't reduce the RAM usage.\nSuppose my dataset has a property myobject of type MyObject, and that myobject internally references a list of integers. Should I convert this list of integers to a numpy array too?"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 13
  },
  {
    "title": "DDP: NCCL \" The server socket has failed to bind to...\"",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13264",
    "createdAt": "2022-06-10T03:23:03Z",
    "author": {
      "login": "drscotthawley"
    },
    "bodyText": "Hi, I'm trying to use my Pytorch Lightning code in conjunction with Jukebox which has its own set of routines for distributed training via the torch.distributed.run method.   I have read the PyTorchLightning docs on torch.distributed and followed them as far as I know:\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 --rdzv_id=31459 --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1 ./my_script.py --myarg1=thing  ...etc\nIf I only ever run on 1 GPU there's no problem, but when I try to run on more than 1 GPU via DDP, then I get many errors from NCCL such as\n[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).\n[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.\nCaught error during NCCL init (attempt 0 of 5): The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n\n\"Already in use\":  Presumably Jukebox, which it runs its own MPI initialization in the form of\nfrom jukebox.utils.dist_utils import setup_dist_from_mpi\n...\nrank, local_rank, device = setup_dist_from_mpi()\n(^^ This call is inside my TRAINER module, BTW, so it should be AFTER Lightning sets up the init_process_group(), right?)\n...Jukebox is trying to setup re-reserve the slots ALREADY setup/reserved by the Lightning Trainer when the DDP Spawn routine in PytorchLightning itself already calls  torch.distributed.init_process_group() ; and rather than just polling the os. environment for keys like RANK and MASTER_ADDR, Jukebox is ignoring those for now.\nMy question:\nIf PyTorch Lightning is setting the torch.distributed.dist object already, then is there a way I can get access to it?  ( for interfacing with  the Jukebox code?)\n(or can I call init_process_group() myself or obtain the result from when PyTorch Lightning called it?)\nBecause right now, if I try NOT calling their MPI initialization routine setup_dist_from_mpi() and instead just communicate key values based on environment variables a la:\n        rank, local_rank, device = os.getenv('RANK'), os.getenv('RANK'), self.device\n...then whereever the Jukebox code calls something like dist.barrier(), then I get an error about\nRuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1102, internal error, NCCL version 21.0.3\nncclInternalError: Internal check failed. This is either a bug in NCCL or due to memory corruption\n\n...\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group....\n\nBut I thought Lightning was supposedly calling init_process_group() already?....?\nSo I'm confused.   Any tips?\nUPDATE: Before the call to the Jukebox stuff, I did check and the pytorch distributed dist.is_available() is True, so it looks like Lightning may have done something already by that point.  But in that case I'm confused about why we're still seeing that RuntimeError about process group not being initialized:\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.",
    "answer": {
      "author": {
        "login": "drscotthawley"
      },
      "bodyText": "MY SOLUTION:\nI think I've distilled it to two simple parts. Default values all ended up being ok, and no special environment-variable-setting proved necessary (e.g. I unset all the NCCL flags I'd tried earlier).  Two things:\n\nrunning as instructed with the torch.distributed.run as before.  Although the default values were fine. e.g.\n\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\n\n\nNear the top of my Trainer init code (and before the Jukebox stuff), initialize :\n\ndist.init_process_group(backend=\"nccl\")\nno other parts were essential.   And I could either have the trainer strategy set to \"ddp\" or \"fsdp\" or nothing at all; made no difference.\n\nALTHOUGH, one extra other thing that makes it go even faster: For some reason OMP_NUM_THREADS is not being set and so you see a warning message that it's getting set to 1 by default.  No need to leave that!   So my final, well-performing invocation looks like:\n\nOMP_NUM_THREADS=12 python -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\n\none could also permanenty set export OMP_NUM_THREADS=12 but I haven't bothered to do that yet."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 14
  },
  {
    "title": "RuntimeError: Trying to backward through the graph a second time",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13219",
    "createdAt": "2022-06-03T07:59:20Z",
    "author": {
      "login": "josyulakrishna"
    },
    "bodyText": "Hi Everyone,\nI'm trying to use the torchdyn library to solve a problem of mine, code is given here,  when I try to run this code I'm getting an error which says\nError:\n\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already \n\nbeen freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify \n\nretain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after \n\ncalling backward.\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport copy\n\nfrom torch.autograd import grad\n\nimport torch\n\n\ndef set_initial_conditions(n_agents):\n    if n_agents == 12:\n        #px,py,qx,qy\n        x0 = torch.tensor([[0], [0.5], [-3], [5],\n                           [0], [0.5], [-3], [3],\n                           [0], [0.5], [-3], [1],\n                           [0], [-0.5], [-3], [-1],\n                           [0], [-0.5], [-3], [-3],\n                           [0], [-0.5], [-3], [-5],\n                           # second column\n                           [-0], [0.5], [3], [5],\n                           [-0], [0.5], [3], [3],\n                           [-0], [0.5], [3], [1],\n                           [0], [-0.5], [3], [-1],\n                           [0], [-0.5], [3], [-3],\n                           [0], [-0.5], [3], [-5],\n                           ])\n        xbar = torch.tensor([[0], [0], [3], [-5],\n                             [0], [0], [3], [-3],\n                             [0], [0], [3], [-1],\n                             [0], [0], [3], [1],\n                             [0], [0], [3], [3],\n                             [0], [0], [3], [5],\n                             # second column\n                             [0], [0], [-3], [-5],\n                             [0], [0], [-3], [-3],\n                             [0], [0], [-3], [-1],\n                             [0], [0], [-3], [1],\n                             [0], [0], [-3], [3],\n                             [0], [0], [-3], [5.0],\n                             ])\n    else:\n        x0 = (torch.rand(4*n_agents, 1)-0.5)*10\n        xbar = (torch.rand(4*n_agents, 1)-0.5)*10\n    return x0, xbar\n\n# X  = (J-R)*dV/dx + Fy + Gu\n# Y = G*dV/dx\n# forward = return (J-R)*dV/dx\n\nclass SystemEnv(nn.Module):\n    def __init__(self, V, K, n_agents=1, xbar=None, ctls=None, batch_size=1, **kwargs):\n        \"\"\" Initialize the environment. Here we represent the system.\n        \"\"\"\n        super().__init__()\n        self.K = K\n        self.V = V\n        self.k = torch.tensor(1.0)\n        self.b = torch.tensor(0.2)\n        self.m = torch.tensor(1.0)\n        J = torch.tensor([[0, 0, -1, 0],\n                          [0, 0, 0, -1],\n                          [1., 0, 0, 0],\n                          [0, 1., 0, 0]])\n        R = torch.tensor([[self.b, 0, 0, 0],\n                          [0, self.b, 0, 0],\n                          [0, 0, 0, 0],\n                          [0, 0, 0, 0]])\n        #number of agents\n        self.n_agents = n_agents\n        #dimension of state space q,p = (2,2)\n        self.ni = 4\n        self.n = self.ni * n_agents\n        if ctls is None:\n            ctls = torch.ones(1, n_agents)\n            # n_of_inputs x n_masses\n        self.interconnection = ctls\n        self.J = torch.zeros((self.n, self.n))\n        self.R = torch.zeros((self.n, self.n))\n        for i in range(0, n_agents):\n            self.J[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = J\n            self.R[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = R\n        if xbar is None:\n            xbar = torch.zeros(self.n, 1)\n        self.xbar = xbar\n        self.B = torch.tensor([[1.0, 0], [0, 1.0]])\n        self.batch_size = batch_size\n\n    def g(self, t, x):\n        # g = torch.zeros((self.n, 2*int(self.interconnection.sum())))\n        # idx = 0\n        # for i, j in self.interconnection.nonzero():\n        #     g[(4*j), idx] = 1\n        #     g[(4*j)+1, idx+1] = 1\n        #     idx += 2\n        # return g\n        g_agent = torch.tensor([[1.0, 0], [0, 1.0], [0, 0], [0, 0]])\n        self.g_agent = copy.deepcopy(g_agent)\n        g = torch.zeros(0, 0)\n        for i in range(self.n_agents):\n            g = torch.block_diag(g, g_agent)\n        return g\n\n    def H(self, t, x):\n        delta_x = x - self.xbar\n        Q_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\n        Q = torch.zeros((self.n, self.n))\n        for i in range(self.n_agents):\n            Q[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = Q_agent\n        R_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\n        return 0.5 * F.linear(F.linear(delta_x.T, Q), delta_x.T)\n\n    def gradH(self, t, x):\n        x = x.requires_grad_(True)\n        return torch.autograd.grad(self.H(t, x), x, allow_unused=False, create_graph=True)[0]\n\n    def f(self, t, x):\n        dHdx = self.gradH(t, x)\n        return F.linear(dHdx.T, self.J - self.R)\n\n    def _dynamics(self, t, x, u):\n        # p = torch.cat(x[0::4], x[1::4], 0)\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).to(0)\n        p = torch.stack((x[:, 0::4], x[:, 1::4]), dim=2).view(x.shape[0], self.n_agents * 2).to(0)\n        # [p;q] = [J-R]*delV+[B,0]*u\n        delVq = self._energy_shaping(q)\n        # delVp = p from formulation\n        r1 = torch.stack((delVq, p), dim=2).to(0)\n        r1 = r1.view(x.shape[0], self.n_agents*4)\n        JR = (self.J - self.R).to(0)\n        # input, matrix\n        result  = torch.zeros(x.shape[0], self.n_agents*4).to(0)\n        u = u.view(x.shape[0], self.n_agents * 2).to(0)\n        for i in range(r1.shape[0]):\n            par1 = torch.matmul(JR, r1[i, :])\n            g = self.g(t, x).to(0)\n            # u\u03b8 = \u2212BInv*\u2207qV(q) \u2212 K\u2217(t, q, p)B*qdot\n            par2 = F.linear(g, u[i,:])\n            result[i,:] = torch.add(par1, par2).to(0)\n        return result\n\n    def _energy_shaping(self,q):\n        # dVdx = grad(self.V(q).sum(), q, create_graph=True)[0]\n        dVdx = grad(self.V(q).sum(), q, create_graph=True, retain_graph=True)[0]\n        return -1*dVdx\n\n    # def _energy(self,t,x):\n    #     Q_agent = torch.diag(torch.tensor([1 / 2*self.m, self.k]))\n    #     temp_x = torch.zeros(self.n_agents*2)\n    #     temp_p = torch.zeros(self.n_agents*2)\n    #     x_temp = x\n    #     x_temp = x.view(self.n_agents, 4)\n    #     for i in range(self.n_agents):\n    #         for j in range(len(x[i])):\n    #             temp_x[i]=(0.5*self.m)*()\n    #             temp_p[i] = (0.5 * self.m)\n    #\n    #     F.linear(Q_agent, torch.cat(torch.cdist(x[:,2:...],torch.zeros_like(x[:,2:...])),torch.cdist(x[...,:2]-xbar)))\n\n    def _damping_injection(self,x):\n        # x = [pdot, qdot]\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True)\n        Kmat = torch.diag(self.K(x.to(0)).ravel())\n        return -1*F.linear(Kmat, q.view(1, x.shape[0]*self.n_agents*2).to(0))\n\n    def forward(self, t, x):\n        # x = [p,q]\n        print(\"in forward\")\n        x = x.requires_grad_(True)\n        #batch_size, n_agents*4, n_agents = x\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True).to(0)\n        u1 = self._energy_shaping(q)\n        u1 = u1.view(x.shape[0]*self.n_agents*2, 1 )\n        u2 = self._damping_injection(x)\n        u = u1+u2\n        return  (self._dynamics(t,x,u),u)\n        # return self.f(t, x).T\n\nclass AugmentedDynamics(nn.Module):\n    # \"augmented\" vector field to take into account integral loss functions\n    def __init__(self, f, int_loss):\n        super().__init__()\n        self.f = f\n        self.int_loss = int_loss\n        self.nfe = 0.\n\n    def forward(self, t, x):\n        self.nfe += 1\n        x = x[:,:f.n_agents*4]\n        (dxdt,u) = self.f.forward(t, x)\n        dldt =   self.int_loss.int_loss(x, u, self.f)\n        return torch.cat([dxdt, dldt], 1).cpu()\n\n\n\nclass ControlEffort(nn.Module):\n    # control effort integral cost\n    def __init__(self, f, x0, xbar, dt):\n        super().__init__()\n        self.f = f\n\n        self.x = torch.cat((x0[0::4], x0[1::4]), dim=1)\n        self.x = self.x.repeat(f.batch_size, 1)\n        self.x = self.x.reshape(f.batch_size,f.n_agents*2)\n\n        self.xbar = torch.cat((xbar[0::4], xbar[1::4]), dim=1)\n        self.xbar = self.xbar.repeat(f.batch_size, 1)\n        self.xbar = self.xbar.reshape(batch_size, f.n_agents*2)\n\n        self.dt = dt\n\n    def forward(self, t, x):\n        with torch.set_grad_enabled(True):\n            q = torch.cat((x[2::4],x[3::4]),0).requires_grad_(True)\n            # q = torch.transpose(q, 0, 1)\n            u1 = torch.transpose(self.f._energy_shaping(q), 0, 1)\n            u2 = self.f._damping_injection(x).to(0)\n            u = u1+u2\n        return u\n\n    def int_loss(self, x, u, clsys):\n        x = x.reshape(x.shape[0],self.f.n_agents,2,2)\n        vel = torch.index_select(x, 2, torch.tensor([1]))\n        vel = vel.reshape(x.shape[0],self.f.n_agents*2)\n        self.x = self.x.cpu()+torch.mul(vel,self.dt)\n        self.x = self.x.to(0)\n        self.xbar = self.xbar.to(0)\n        self.u = u.reshape(self.f.batch_size,self.f.n_agents*2)\n        self.clsys = clsys\n        lx = self.f_loss_states().reshape(self.f.batch_size,1).to(0)\n        lu = self.f_loss_u().reshape(self.f.batch_size,1).to(0)\n        lca = self.f_loss_ca()\n        loss = lx+lu+lca\n        return loss.to(0)\n\n    def f_loss_states(self, test=False):\n        # clsys = SystemEnv\n        loss_function = nn.MSELoss(reduction='none')\n        xbar = self.clsys.xbar\n        # steps = t.shape[0]\n        if test:\n            gamma = 1\n        else:\n            gamma = 0.95\n        loss = loss_function(self.x, self.xbar)\n        loss = loss.view(self.f.batch_size, 2*self.f.n_agents)\n        loss = loss.sum(dim=1)\n        return loss\n\n    def f_loss_u(self):\n        loss_u = ((self.u*self.clsys.b) ** 2).sum(dim=1)\n        return loss_u\n\n\n    def f_loss_ca(self, min_dist=0.5):\n        steps = self.x.shape[0]\n        min_sec_dist = 1.4 * min_dist\n        # for i in range(steps):\n        #     for j in range(i+1, steps):\n        #         dist = torch.norm(self.x[i, :] - self.x[j, :])\n        #         if dist < min_sec_dist:\n        #             return torch.tensor(1e10)\n        # return torch.tensor(0)\n        loss_ca_ = torch.zeros(self.f.batch_size,1).to(0)\n        for i in range(self.x.shape[0]):\n            x = self.x[i,:].view(self.f.n_agents*2,1).to(0)\n            clsys = self.clsys\n            # collision avoidance:\n            # deltax = x[:, 2::4].repeat(1, 1, clsys.n // 4) - x[:, 2::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\n            deltax = x[0::2].repeat(1, clsys.n // 4) - x[0::2].transpose(0, 1).repeat( clsys.n // 4, 1)\n            # deltay = x[:, 3::4].repeat(1, 1, clsys.n // 4) - x[:, 3::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\n            deltay = x[1::2].repeat(1, clsys.n // 4) - x[1::2].transpose(0, 1).repeat(clsys.n // 4, 1)\n            distance_sq = deltax ** 2 + deltay ** 2\n            mask = torch.logical_not(torch.eye(clsys.n // 4)).unsqueeze(0).repeat(steps, 1, 1)\n            mask = mask.to(0)\n            loss_ca_[i,:] = (1 / (distance_sq + 1e-3) * (distance_sq.detach() < (min_sec_dist ** 2)) * mask).sum() / 2\n        return loss_ca_\n\n\nimport pytorch_lightning as pl\nimport torch.utils.data as data\n\ndef weighted_log_likelihood_loss(x, target, weight):\n    # weighted negative log likelihood loss\n    log_prob = target.log_prob(x)\n    weighted_log_p = weight * log_prob\n    return -torch.mean(weighted_log_p.sum(1))\n\nclass EnergyShapingLearner(pl.LightningModule):\n    def __init__(self, model: nn.Module, prior_dist, target_dist, t_span, sensitivity='autograd', n_agents=1):\n        super().__init__()\n        self.model = model\n        self.prior, self.target = prior_dist, target_dist\n        self.t_span = t_span\n        self.batch_size = batch_size\n        self.lr = 5e-3\n        self.n_agents = n_agents\n        self.weight = torch.ones(n_agents * 4).reshape(1, n_agents * 4)\n\n    def forward(self, x):\n        return self.model.odeint(x, self.t_span)\n\n    def training_step(self, batch, batch_idx):\n        # sample a batch of initial conditions\n        # x0 = self.prior.sample((self.batch_size,))\n        n_agents = self.n_agents\n        x0 = torch.rand((self.batch_size,n_agents*4))\n        # x0, _ = set_initial_conditions(n_agents)\n        # Integrate the model\n        x0 = torch.cat([x0, torch.zeros(self.batch_size, 1)], -1).to(x0)\n        xs, xTl = self(x0)\n        xT, l = xTl[-1, :, :2], xTl[-1, :, -1:]\n\n        # Compute loss\n        # terminal_loss = weighted_log_likelihood_loss(xT, self.target, self.weight.to(xT))\n        integral_loss = torch.mean(l)\n        loss = 0 + 0.01 * integral_loss\n        return {'loss': loss.cpu()}\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n        return [optimizer], [scheduler]\n\n    def train_dataloader(self):\n        dummy_trainloader = data.DataLoader(\n            data.TensorDataset(torch.Tensor(1, 1), torch.Tensor(1, 1)),\n            batch_size=1)\n        return dummy_trainloader\n\n# # # # # # # # Parameters # # # # # # # #\nn_agents = 2  # agents are not interconnected (even when having a controller). Each of them acts independently\nt_end = 5\nsteps = 101\nmin_dist = 0.5  # min distance for collision avoidance\n# (px, py, qx, qy) - for each agent\nhdim = 64\nV = nn.Sequential(\n    nn.Linear(n_agents*2, hdim),\n    nn.Softplus(),\n    nn.Linear(hdim, hdim),\n    nn.Tanh(),\n    nn.Linear(hdim, 1))\nK = nn.Sequential(\n    nn.Linear(n_agents*4, hdim),\n    nn.Softplus(),\n    nn.Linear(hdim, (n_agents*2)),\n    nn.Softplus())\n\n\nfrom torch.distributions import Uniform, Normal\n\n\ndef prior_dist(q_min, q_max, p_min, p_max, device='cpu'):\n    # uniform \"prior\" distribution of initial conditions x(0)=[q(0),p(0)]\n    lb = torch.Tensor([q_min, p_min]).to(device)\n    ub = torch.Tensor([q_max, p_max]).to(device)\n    return Uniform(lb, ub)\n\n\ndef target_dist(mu, sigma, device='cpu'):\n    # normal target distribution of terminal states x(T)\n    mu, sigma = torch.Tensor(mu).reshape(1, 2).to(device), torch.Tensor(sigma).reshape(1, 2).to(device)\n    return Normal(mu, torch.sqrt(sigma))\n\ndef weighted_log_likelihood_loss(x, target, weight):\n    # weighted negative log likelihood loss\n    log_prob = target.log_prob(x)\n    weighted_log_p = weight * log_prob\n    return -torch.mean(weighted_log_p.sum(1))\n\nfrom torchdyn.models import ODEProblem\n\n# choose solver and sensitivity method\nsolver = 'rk4'\nsensitivity = 'autograd'\n\n# init to zero par.s of the final layer\n# for p in V[-1].parameters(): torch.nn.init.zeros_(p)\n# for p in K[-2].parameters(): torch.nn.init.zeros_(p)\n\n# define controlled system dynamics\nx0, xbar = set_initial_conditions(n_agents)\nbatch_size = 4\n\nf = SystemEnv(V.to(0), K.to(0), n_agents=2, xbar=xbar, batch_size = batch_size)\n\nt_span = torch.linspace(0, 3, 30)\ndt = t_span[1]-t_span[0]\naug_f = AugmentedDynamics(f, ControlEffort(f,x0,xbar,dt))\n# define time horizon\n\n\nprob = ODEProblem(aug_f, sensitivity=sensitivity, solver=solver)\n\n# train (it can be very slow on CPU)\n# (don't be scared if the loss starts very high)\nprior = prior_dist(-1, 1, -1, 1) # Uniform \"prior\" distribution of initial conditions x(0)\ntarget = target_dist([0, 0], [.001, .001]) # Normal target distribution for x(T)\nlearn = EnergyShapingLearner(prob, prior, target, t_span, batch_size, n_agents=n_agents)\n# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650).fit(learn)\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650)\ntrainer.fit(learn)",
    "answer": {
      "author": {
        "login": "josyulakrishna"
      },
      "bodyText": "This can be resolved by using\nreturn Variable(dyn).requires_grad_(True) , x = Variable(x.data, requires_grad=True)"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 15
  },
  {
    "title": "Effective batch size in DDP",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13165",
    "createdAt": "2022-05-27T06:49:44Z",
    "author": {
      "login": "pengzhangzhi"
    },
    "bodyText": "I have max batch size of 4 in single gpu. If 2 gpus are used, should I increase the batch size to 8 such that each gpu gets 4 batches. Or I just keep it as 4 and PL will load 2 four-batches data to 2 gpus?\nBased on the doc,\nIn DDP, DDP_SPAWN, Deepspeed, DDP_SHARDED, or Horovod your effective batch size will be 7 * devices * num_nodes.\n\nI think it is the second case?\nI also have another problem related to ddp training, which is posted on this link below.\nhttps://forums.pytorchlightning.ai/t/how-to-initialize-tensors-that-are-in-the-right-device-when-ddp-are-used/1708\nI post it here for convenience.\nI am incorporating  a pytorch based model into the pl framework for ddp training.\nI have a lightning model\nclass ZfoldLightning(pl.LightningModule):\n    def __init__(self, hparams):\n        ...\n        self.model = XFold(MODEL_PARAM)\nwhich initializes the XFold model in __init__.\nHowever, the XFold model contains many 'to device' code like b = torch.randn(1).to(a.device), which is not recommended by PL.\nI tried to increase the batch size and train this model on two device. this does not work. OOM error appears. Turns out even DDP is used, I can only use the same batch size as that of single gpu. I think the reason is that all the tensors are stored in one gpu no matter how many gpus are ultized.\nOne solution is to refactor those to device code and use the recommended usage a.type_as(b). But there are to many of code to refactor.\nI am wondering if there are better solutions?\nAny helps?",
    "answer": {
      "author": {
        "login": "pengzhangzhi"
      },
      "bodyText": "I have solved my problem and find out that the answer is: each gpu get #batch_size batches. If you have batch_size of 2 and 2 gpus are utilized,  each gpu gets 2 batches and 4 batches in total are feed into a forward pass."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 16
  },
  {
    "title": "ddp help",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13032",
    "createdAt": "2022-05-10T22:34:55Z",
    "author": {
      "login": "sneiman"
    },
    "bodyText": "Training a simple autoencoder as part of a larger project, and using it to get back up to speed on Lightning. Pytorch Lightning v 1.6.2, AMD Ryzen, 2 A6000s, Ubuntu 21.10, using DDP. I have 2 questions:\n\n\nDuring script startup, I warn the user if they are about to overwrite existing log files. When using DDP and both devices, pl asks this question twice, of course, once on each process. Where can I put this interaction to avoid doing it twice? I suppose I could put it in prepare_data() - but am hoping for something that feels more appropriate.\n\n\nSometimes - but not always - calling trainer.test() immediately following training fails to find the checkpoint file. This happens with unchanged code and model - sometimes it works and sometimes it doesn't. Interestingly, when it fails, a message is displayed saying it is going to to use a file which does exists and is the best checkpoint, but it does not load it. Instead it fails twice, each time trying to load a different file name which does not exist. Guessing that each process is trying to run its own version of best checkpoint ... Is there something I need to do/call before running test() to make sure this has all been resolved back to one process? Or have I found a bug ...\n\n\nAny help appreciated.\nseth",
    "answer": {
      "author": {
        "login": "sneiman"
      },
      "bodyText": "I resolved the first question. For those with a similar problem:\nThere is a set of rank_zero decorators which can be imported from utilities. The one I used is imported like so:\nfrom   pytorch_lightning.utilities.rank_zero import rank_zero_only\nPut user interaction into a function like so:\n@rank_zero_only\ndef prelim(args):\n    # do as you please here - it will only run once from rank 0\n\nCall it before the trainer.fit(), and it will only run once.\nSee utilities docs for details ..."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 17
  },
  {
    "title": "Specify Trainer strategy in CLI config?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12978",
    "createdAt": "2022-05-04T18:24:02Z",
    "author": {
      "login": "wwbrannon"
    },
    "bodyText": "Hi folks,\nI can't figure out how to give a Strategy class (rather than a string) as an argument to the trainer in a CLI config file. Is doing so supported?\nDoing this works fine:\ntrainer:\n    ...\n    strategy: 'deepspeed_stage_2_offload'\n    ...\n\nBut this gives a jsonargparse error:\ntrainer:\n    ...\n    strategy:\n        class_path: pytorch_lightning.strategies.DeepSpeedStrategy\n        init_args:\n            stage: 2\n            offload_optimizer: True\n            logging_batch_size_per_gpu: 16\n    ...\n\nThe error:\nTraceback (most recent call last):\n  File \"/media/sharon/wbrannon/github/clip-graph/bin/trainer.py\", line 7, in <module>\n    cli = cl.LightningCLI(\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 552, in __init__\n    self.parse_arguments(self.parser)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 692, in parse_arguments\n    self.config = parser.parse_args()\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 268, in parse_args\n    return super().parse_args(*args, **kwargs)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/deprecated.py\", line 112, in patched_parse\n    cfg = parse_method(*args, _skip_check=_skip_check, **kwargs)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 366, in parse_args\n    self.error(str(ex), ex)\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 983, in error\n    raise ParserError(message) from ex\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, s\ntage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000\n.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\" does not validate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\n  - Problem with given class_path \"pytorch_lightning.strategies.DeepSpeedStrategy\":\n    - Configuration check failed :: Parser key \"params_buffer_size\": Expected a <class 'int'> but got \"100000000.0\"\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"",
    "answer": {
      "author": {
        "login": "carmocca"
      },
      "bodyText": "It's a bug! Will be fixed with #12989 which should be included in next week's bugfix release\nThanks for the report!"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 18
  },
  {
    "title": "How to get different random minibatch orders?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12867",
    "createdAt": "2022-04-23T15:11:29Z",
    "author": {
      "login": "jaak-s"
    },
    "bodyText": "Hi,\nI'm training DDP with 4 GPUs but noticed that if I rerun the experiment the first epoch has the exact same but random order of the minibatches as the previous experiment.\nHow can I make it so that each time I run the experiment I get a different random order?\nI'm using PL version is 1.4.5 and pytorch 1.10.0.\nThank you",
    "answer": {
      "author": {
        "login": "jaak-s"
      },
      "bodyText": "Solved. Use seed_everything(random_seed)."
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 19
  },
  {
    "title": "When I use the official sample to carry out back propagation manually, I make mistakes. First, there is no optimizer, and second, there is no attribute in the image",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12683",
    "createdAt": "2022-04-10T03:40:54Z",
    "author": {
      "login": "Hou-jing"
    },
    "bodyText": "import torch\nfrom torch import Tensor\nfrom pytorch_lightning import LightningModule\nclass Generator:\n    def __init__(self):\n        pass\n    def forward(self):\n        pass\n\nclass Discriminator:\n    def __init__(self):\n        pass\n    def forward(self):\n        pass\nclass SimpleGAN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.G = Generator()\n        self.D = Discriminator()\n\n        # Important: This property activates manual optimization.\n        self.automatic_optimization = False\n\n    def sample_z(self, n) -> Tensor:\n        sample = self._Z.sample((n,))\n        return sample\n\n    def sample_G(self, n) -> Tensor:\n        z = self.sample_z(n)\n        return self.G(z)\n\n    def training_step(self, batch, batch_idx):\n        # Implementation follows the PyTorch tutorial:\n        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n        g_opt, d_opt = self.optimizers()\n\n        X, _ = batch\n        batch_size = X.shape[0]\n\n        real_label = torch.ones((batch_size, 1), device=self.device)\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\n\n        g_X = self.sample_G(batch_size)\n\n        ##########################\n        # Optimize Discriminator #\n        ##########################\n        d_x = self.D(X)\n        errD_real = self.criterion(d_x, real_label)\n\n        d_z = self.D(g_X.detach())\n        errD_fake = self.criterion(d_z, fake_label)\n\n        errD = errD_real + errD_fake\n\n        d_opt.zero_grad()\n        self.manual_backward(errD)\n        d_opt.step()\n\n        ######################\n        # Optimize Generator #\n        ######################\n        d_z = self.D(g_X)\n        errG = self.criterion(d_z, real_label)\n\n        g_opt.zero_grad()\n        self.manual_backward(errG)\n        g_opt.step()\n\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\n    def configure_optimizers(self):\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n        return g_opt, d_opt\nbatch=torch.randn(3,2)\nbatch_idx=torch.ones(3)\nSimpleGAN().training_step(batch,batch_idx)",
    "answer": {
      "author": {
        "login": "Hou-jing"
      },
      "bodyText": "When I upgraded the version to the latest version, I solved this problem\u3002\nHowever, my running speed has been greatly affected. I used to have an epoch every 8 minutes, but now it has been delayed for a long time, and the data can't be loaded. I don't know why\nAnd this is the code\n[https://colab.research.google.com/drive/1dCP7-1xK48-PohGc8-RKx3Ne2HWd4Jkq#scrollTo=frTD9xWvBEUT]"
    },
    "label": "discussion",
    "file": "../final_data/discussions.json",
    "index": 20
  }
]