{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "reduce", "original_string": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "language": "python", "code": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "Any", "]", ":", "\"", "\"", "\"", "Reduces", "the", "given", "tensor", "(", "e", ".", "g", ".", "across", "GPUs", "/", "processes", ")", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "group", ":", "the", "process", "group", "to", "reduce", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "or", "ReduceOp", ".", "\"", "\"", "\""], "docstring": "Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.", "docstring_tokens": ["reduces", "the", "given", "tensor", "e", "g", "across", "gpus", "processes", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "group", "the", "process", "group", "to", "reduce", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "can", "also", "be", "a", "string", "sum", "or", "reduceop"], "docstring_summary": "Reduces the given tensor (e.g. across GPUs/processes).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 295, "end_line": 309, "hash": "6ff047ac884e88a898553e53ebf88d98", "complexity": 1, "parameters": ["tensor", "Any]", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "barrier", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function", ".", "Args", ":", "name", ":", "an", "optional", "name", "to", "pass", "into", "barrier", ".", "\"", "\"", "\""], "docstring": "Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.", "docstring_tokens": ["synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function", "args", "name", "an", "optional", "name", "to", "pass", "into", "barrier"], "docstring_summary": "Synchronizes all processes which blocks processes until the whole group enters this function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 312, "end_line": 318, "hash": "ca72c9ec1f7f7614e6b9d2b1c306093c", "complexity": 1, "parameters": ["name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "broadcast", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "\"", "\"", "\"", "Broadcasts", "an", "object", "to", "all", "processes", ".", "Args", ":", "obj", ":", "the", "object", "to", "broadcast", "src", ":", "source", "rank", "\"", "\"", "\""], "docstring": "Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank", "docstring_tokens": ["broadcasts", "an", "object", "to", "all", "processes", "args", "obj", "the", "object", "to", "broadcast", "src", "source", "rank"], "docstring_summary": "Broadcasts an object to all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 321, "end_line": 328, "hash": "f157c4881aa188f48798f316687d7fe0", "complexity": 1, "parameters": ["obj", "src"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "all_gather", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Perform", "an", "all_gather", "on", "all", "processes", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "all_gather", "group", ":", "the", "process", "group", "to", "gather", "results", "from", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op", "\"", "\"", "\""], "docstring": "Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op", "docstring_tokens": ["perform", "an", "all_gather", "on", "all", "processes", "args", "tensor", "the", "tensor", "to", "all_gather", "group", "the", "process", "group", "to", "gather", "results", "from", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op"], "docstring_summary": "Perform an all_gather on all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 331, "end_line": 339, "hash": "94b681ef555b6830d18d6434861941c9", "complexity": 1, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "training_step", "original_string": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual training step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.training_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.train_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\r\n            return self.lightning_module.training_step(*args, **kwargs)", "language": "python", "code": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual training step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.training_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.train_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\r\n            return self.lightning_module.training_step(*args, **kwargs)", "code_tokens": ["def", "training_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "\"", "\"", "\"", "The", "actual", "training", "step", ".", "See", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "training_step", "`", "for", "more", "details", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "train_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "\"", "training_step", "\"", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "training_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual training step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.training_step` for more details", "docstring_tokens": ["the", "actual", "training", "step", "see", "meth", "lightning", "pytorch", "core", "lightningmodule", "training_step", "for", "more", "details"], "docstring_summary": "The actual training step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 379, "end_line": 390, "hash": "9c722704b0aa9c74b9a5b000a544aaf8", "complexity": 3, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "post_training_step", "original_string": "def post_training_step(self) -> None:\r\n        \"\"\"This hook is deprecated.\r\n\r\n        Override :meth:`training_step` instead.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def post_training_step(self) -> None:\r\n        \"\"\"This hook is deprecated.\r\n\r\n        Override :meth:`training_step` instead.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "post_training_step", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "hook", "is", "deprecated", ".", "Override", ":", "meth", ":", "`", "training_step", "`", "instead", ".", "\"", "\"", "\"", "pass"], "docstring": "This hook is deprecated.\r\n\r\n        Override :meth:`training_step` instead.", "docstring_tokens": ["this", "hook", "is", "deprecated", "override", "meth", "training_step", "instead"], "docstring_summary": "This hook is deprecated.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 392, "end_line": 398, "hash": "8d2bd4e779b1d08a6c3ad3e069be4d39", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "validation_step", "original_string": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual validation step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.validation_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.val_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\r\n            return self.lightning_module.validation_step(*args, **kwargs)", "language": "python", "code": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual validation step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.validation_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.val_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\r\n            return self.lightning_module.validation_step(*args, **kwargs)", "code_tokens": ["def", "validation_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "\"", "\"", "\"", "The", "actual", "validation", "step", ".", "See", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "validation_step", "`", "for", "more", "details", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "val_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "\"", "validation_step", "\"", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "validation_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual validation step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.validation_step` for more details", "docstring_tokens": ["the", "actual", "validation", "step", "see", "meth", "lightning", "pytorch", "core", "lightningmodule", "validation_step", "for", "more", "details"], "docstring_summary": "The actual validation step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 400, "end_line": 411, "hash": "5d815ab7cdeeb5fcd2d244370f0046fb", "complexity": 3, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "test_step", "original_string": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual test step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.test_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.test_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"test_step\", *args, **kwargs)\r\n            return self.lightning_module.test_step(*args, **kwargs)", "language": "python", "code": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual test step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.test_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.test_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"test_step\", *args, **kwargs)\r\n            return self.lightning_module.test_step(*args, **kwargs)", "code_tokens": ["def", "test_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "\"", "\"", "\"", "The", "actual", "test", "step", ".", "See", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "test_step", "`", "for", "more", "details", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "test_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "\"", "test_step", "\"", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "test_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual test step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.test_step` for more details", "docstring_tokens": ["the", "actual", "test", "step", "see", "meth", "lightning", "pytorch", "core", "lightningmodule", "test_step", "for", "more", "details"], "docstring_summary": "The actual test step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 413, "end_line": 424, "hash": "a27ff1368847c4c282bbd8214ccd53f1", "complexity": 3, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "predict_step", "original_string": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"The actual predict step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.predict_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.predict_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"predict_step\", *args, **kwargs)\r\n            return self.lightning_module.predict_step(*args, **kwargs)", "language": "python", "code": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"The actual predict step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.predict_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.predict_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"predict_step\", *args, **kwargs)\r\n            return self.lightning_module.predict_step(*args, **kwargs)", "code_tokens": ["def", "predict_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "The", "actual", "predict", "step", ".", "See", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "predict_step", "`", "for", "more", "details", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "predict_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "\"", "predict_step", "\"", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "predict_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual predict step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.predict_step` for more details", "docstring_tokens": ["the", "actual", "predict", "step", "see", "meth", "lightning", "pytorch", "core", "lightningmodule", "predict_step", "for", "more", "details"], "docstring_summary": "The actual predict step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 426, "end_line": 437, "hash": "9f50f39d2b99e6b6030bba299dbe838c", "complexity": 3, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "process_dataloader", "original_string": "def process_dataloader(self, dataloader: object) -> object:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "language": "python", "code": "def process_dataloader(self, dataloader: object) -> object:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "code_tokens": ["def", "process_dataloader", "(", "self", ",", "dataloader", ":", "object", ")", "-", ">", "object", ":", "\"", "\"", "\"", "Wraps", "the", "dataloader", "if", "necessary", ".", "Args", ":", "dataloader", ":", "iterable", ".", "Ideally", "of", "type", ":", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "\"", "\"", "\"", "return", "dataloader"], "docstring": "Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`", "docstring_tokens": ["wraps", "the", "dataloader", "if", "necessary", "args", "dataloader", "iterable", "ideally", "of", "type", "class", "torch", "utils", "data", "dataloader"], "docstring_summary": "Wraps the dataloader if necessary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 439, "end_line": 446, "hash": "ea201e65147fe218149e6e937079144a", "complexity": 1, "parameters": ["dataloader"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "restore_checkpoint_after_setup", "original_string": "def restore_checkpoint_after_setup(self) -> bool:\r\n        \"\"\"Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when\r\n        the strategy requires all the setup hooks to run before loading checkpoint.\r\n\r\n        Returns:\r\n            If ``True``, restore checkpoint after strategy setup.\r\n\r\n        \"\"\"\r\n        return False", "language": "python", "code": "def restore_checkpoint_after_setup(self) -> bool:\r\n        \"\"\"Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when\r\n        the strategy requires all the setup hooks to run before loading checkpoint.\r\n\r\n        Returns:\r\n            If ``True``, restore checkpoint after strategy setup.\r\n\r\n        \"\"\"\r\n        return False", "code_tokens": ["def", "restore_checkpoint_after_setup", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Override", "to", "delay", "restoring", "from", "checkpoint", "till", "after", "the", "setup", "phase", "has", "completed", ".", "This", "is", "useful", "when", "the", "strategy", "requires", "all", "the", "setup", "hooks", "to", "run", "before", "loading", "checkpoint", ".", "Returns", ":", "If", "`", "`", "True", "`", "`", ",", "restore", "checkpoint", "after", "strategy", "setup", ".", "\"", "\"", "\"", "return", "False"], "docstring": "Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when\r\n        the strategy requires all the setup hooks to run before loading checkpoint.\r\n\r\n        Returns:\r\n            If ``True``, restore checkpoint after strategy setup.", "docstring_tokens": ["override", "to", "delay", "restoring", "from", "checkpoint", "till", "after", "the", "setup", "phase", "has", "completed", "this", "is", "useful", "when", "the", "strategy", "requires", "all", "the", "setup", "hooks", "to", "run", "before", "loading", "checkpoint", "returns", "if", "true", "restore", "checkpoint", "after", "strategy", "setup"], "docstring_summary": "Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 449, "end_line": 457, "hash": "99dbd0b32b425449abb1145f2b68abf2", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "lightning_restore_optimizer", "original_string": "def lightning_restore_optimizer(self) -> bool:\r\n        \"\"\"Override to disable Lightning restoring optimizers/schedulers.\r\n\r\n        This is useful for strategies which manage restoring optimizers/schedulers.\r\n\r\n        \"\"\"\r\n        return True", "language": "python", "code": "def lightning_restore_optimizer(self) -> bool:\r\n        \"\"\"Override to disable Lightning restoring optimizers/schedulers.\r\n\r\n        This is useful for strategies which manage restoring optimizers/schedulers.\r\n\r\n        \"\"\"\r\n        return True", "code_tokens": ["def", "lightning_restore_optimizer", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Override", "to", "disable", "Lightning", "restoring", "optimizers", "/", "schedulers", ".", "This", "is", "useful", "for", "strategies", "which", "manage", "restoring", "optimizers", "/", "schedulers", ".", "\"", "\"", "\"", "return", "True"], "docstring": "Override to disable Lightning restoring optimizers/schedulers.\r\n\r\n        This is useful for strategies which manage restoring optimizers/schedulers.", "docstring_tokens": ["override", "to", "disable", "lightning", "restoring", "optimizers", "schedulers", "this", "is", "useful", "for", "strategies", "which", "manage", "restoring", "optimizers", "schedulers"], "docstring_summary": "Override to disable Lightning restoring optimizers/schedulers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 460, "end_line": 466, "hash": "83e316c483dc988ea2a9df7a210f0a1b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "lightning_module_state_dict", "original_string": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        assert self.lightning_module is not None\r\n        return self.lightning_module.state_dict()", "language": "python", "code": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        assert self.lightning_module is not None\r\n        return self.lightning_module.state_dict()", "code_tokens": ["def", "lightning_module_state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Returns", "model", "state", ".", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "return", "self", ".", "lightning_module", ".", "state_dict", "(", ")"], "docstring": "Returns model state.", "docstring_tokens": ["returns", "model", "state"], "docstring_summary": "Returns model state.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 473, "end_line": 476, "hash": "b001d04822fb4af2346a8d07f0d7b4b7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self, checkpoint: dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            filepath: write-target file's path\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)", "language": "python", "code": "def save_checkpoint(\r\n        self, checkpoint: dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            filepath: write-target file's path\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "filepath", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", "/", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "-", "dump", "and", "file", "-", "write", ".", "Args", ":", "checkpoint", ":", "dict", "containing", "model", "and", "trainer", "state", "filepath", ":", "write", "-", "target", "file", "'", "s", "path", "storage_options", ":", "parameter", "for", "how", "to", "save", "to", "storage", ",", "passed", "to", "`", "`", "CheckpointIO", "`", "`", "plugin", "\"", "\"", "\"", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "checkpoint", ",", "filepath", ",", "storage_options", "=", "storage_options", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            filepath: write-target file's path\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write", "args", "checkpoint", "dict", "containing", "model", "and", "trainer", "state", "filepath", "write", "target", "file", "s", "path", "storage_options", "parameter", "for", "how", "to", "save", "to", "storage", "passed", "to", "checkpointio", "plugin"], "docstring_summary": "Save model/training states as a checkpoint file through state-dump and file-write.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 478, "end_line": 490, "hash": "41626dc2ac5f80e0da12c9a8449c2f21", "complexity": 2, "parameters": ["checkpoint", "Any]", "filepath", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "remove_checkpoint", "original_string": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "language": "python", "code": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Remove", "checkpoint", "filepath", "from", "the", "filesystem", ".", "Args", ":", "filepath", ":", "Path", "to", "checkpoint", "\"", "\"", "\"", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "filepath", ")"], "docstring": "Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint", "docstring_tokens": ["remove", "checkpoint", "filepath", "from", "the", "filesystem", "args", "filepath", "path", "to", "checkpoint"], "docstring_summary": "Remove checkpoint filepath from the filesystem.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 492, "end_line": 500, "hash": "204318f3e7f99195ee8ed3551cb728e4", "complexity": 2, "parameters": ["filepath"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "tensor_init_context", "original_string": "def tensor_init_context(self, empty_init: Optional[bool] = None) -> Generator[None, None, None]:\r\n        \"\"\"Controls how tensors get created (device, dtype).\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        empty_init_context = _EmptyInit(enabled=bool(empty_init))\r\n        with empty_init_context, self.root_device, self.precision_plugin.tensor_init_context():\r\n            yield", "language": "python", "code": "def tensor_init_context(self, empty_init: Optional[bool] = None) -> Generator[None, None, None]:\r\n        \"\"\"Controls how tensors get created (device, dtype).\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        empty_init_context = _EmptyInit(enabled=bool(empty_init))\r\n        with empty_init_context, self.root_device, self.precision_plugin.tensor_init_context():\r\n            yield", "code_tokens": ["def", "tensor_init_context", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "Controls", "how", "tensors", "get", "created", "(", "device", ",", "dtype", ")", ".", "Args", ":", "empty_init", ":", "Whether", "to", "initialize", "the", "model", "with", "empty", "weights", "(", "uninitialized", "memory", ")", ".", "If", "`", "`", "None", "`", "`", ",", "the", "strategy", "will", "decide", ".", "Some", "strategies", "may", "not", "support", "all", "options", ".", "\"", "\"", "\"", "empty_init_context", "=", "_EmptyInit", "(", "enabled", "=", "bool", "(", "empty_init", ")", ")", "with", "empty_init_context", ",", "self", ".", "root_device", ",", "self", ".", "precision_plugin", ".", "tensor_init_context", "(", ")", ":", "yield"], "docstring": "Controls how tensors get created (device, dtype).\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.", "docstring_tokens": ["controls", "how", "tensors", "get", "created", "device", "dtype", "args", "empty_init", "whether", "to", "initialize", "the", "model", "with", "empty", "weights", "uninitialized", "memory", "if", "none", "the", "strategy", "will", "decide", "some", "strategies", "may", "not", "support", "all", "options"], "docstring_summary": "Controls how tensors get created (device, dtype).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 503, "end_line": 513, "hash": "03e895834da5365c8c981e3440ca104b", "complexity": 2, "parameters": ["empty_init"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "model_sharded_context", "original_string": "def model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield", "language": "python", "code": "def model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield", "code_tokens": ["def", "model_sharded_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "Provide", "hook", "to", "create", "modules", "in", "a", "distributed", "aware", "context", ".", "This", "is", "useful", "for", "when", "we", "'", "d", "like", "to", "shard", "the", "model", "instantly", ",", "which", "is", "useful", "for", "extremely", "large", "models", "which", "can", "save", "memory", "and", "initialization", "time", ".", "Returns", ":", "Model", "parallel", "context", ".", "\"", "\"", "\"", "yield"], "docstring": "Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.", "docstring_tokens": ["provide", "hook", "to", "create", "modules", "in", "a", "distributed", "aware", "context", "this", "is", "useful", "for", "when", "we", "d", "like", "to", "shard", "the", "model", "instantly", "which", "is", "useful", "for", "extremely", "large", "models", "which", "can", "save", "memory", "and", "initialization", "time", "returns", "model", "parallel", "context"], "docstring_summary": "Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 516, "end_line": 523, "hash": "f1dbd6885c2bb4b92a2fa012477749a4", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "teardown", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        _optimizers_to_device(self.optimizers, torch.device(\"cpu\"))\r\n\r\n        if self.lightning_module is not None:\r\n            log.debug(f\"{self.__class__.__name__}: moving model to CPU\")\r\n            self.lightning_module.cpu()\r\n        self.precision_plugin.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        _optimizers_to_device(self.optimizers, torch.device(\"cpu\"))\r\n\r\n        if self.lightning_module is not None:\r\n            log.debug(f\"{self.__class__.__name__}: moving model to CPU\")\r\n            self.lightning_module.cpu()\r\n        self.precision_plugin.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "method", "is", "called", "to", "teardown", "the", "training", "process", ".", "It", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources", ".", "\"", "\"", "\"", "_optimizers_to_device", "(", "self", ".", "optimizers", ",", "torch", ".", "device", "(", "\"", "cpu", "\"", ")", ")", "if", "self", ".", "lightning_module", "is", "not", "None", ":", "log", ".", "debug", "(", "f", "\"", "{", "self", ".", "__class__", ".", "__name__", "}", ":", "moving", "model", "to", "CPU", "\"", ")", "self", ".", "lightning_module", ".", "cpu", "(", ")", "self", ".", "precision_plugin", ".", "teardown", "(", ")", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "teardown", "(", ")", "self", ".", "checkpoint_io", ".", "teardown", "(", ")"], "docstring": "This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process", "it", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources"], "docstring_summary": "This method is called to teardown the training process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "Strategy", "start_line": 525, "end_line": 539, "hash": "15f50540dad0119f623e41d16afa26f8", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "__call__", "original_string": "def __call__(\r\n        self, wrapper_module: Module, original_module: \"pl.LightningModule\", method_name: str, *args: Any, **kwargs: Any\r\n    ) -> STEP_OUTPUT:\r\n        \"\"\"Reroutes a method call through the `wrapper_module`'s `forward` method.\r\n\r\n        Args:\r\n            wrapper_module: The module that has `original_module` wrapped.\r\n            original_module: The module that was wrapped inside `wrapper_module`.\r\n            method_name: The name of the method that should be called on the `original_module` after inputs get\r\n                redirected through the `wrapper_module`'s `forward` method.\r\n            *args: The positional arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n\r\n        \"\"\"\r\n        assert method_name != \"forward\"\r\n        original_forward = original_module.forward\r\n\r\n        def wrapped_forward(*_args: Any, **_kwargs: Any) -> Any:\r\n            # Unpatch ourselves immediately before calling the method `method_name`\r\n            # because itself may want to call the real `forward`\r\n            original_module.forward = original_forward  # type: ignore[method-assign]\r\n            # Call the actual method e.g. `.training_step(...)`\r\n            method = getattr(original_module, method_name)\r\n            out = method(*_args, **_kwargs)\r\n            self.on_after_inner_forward(wrapper_module, original_module)\r\n            return out\r\n\r\n        # Patch the original_module's forward so we can redirect the arguments back to the real method\r\n        original_module.forward = wrapped_forward  # type: ignore[method-assign]\r\n\r\n        wrapper_output = wrapper_module(*args, **kwargs)\r\n        self.on_after_outer_forward(wrapper_module, original_module)\r\n        return wrapper_output", "language": "python", "code": "def __call__(\r\n        self, wrapper_module: Module, original_module: \"pl.LightningModule\", method_name: str, *args: Any, **kwargs: Any\r\n    ) -> STEP_OUTPUT:\r\n        \"\"\"Reroutes a method call through the `wrapper_module`'s `forward` method.\r\n\r\n        Args:\r\n            wrapper_module: The module that has `original_module` wrapped.\r\n            original_module: The module that was wrapped inside `wrapper_module`.\r\n            method_name: The name of the method that should be called on the `original_module` after inputs get\r\n                redirected through the `wrapper_module`'s `forward` method.\r\n            *args: The positional arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n\r\n        \"\"\"\r\n        assert method_name != \"forward\"\r\n        original_forward = original_module.forward\r\n\r\n        def wrapped_forward(*_args: Any, **_kwargs: Any) -> Any:\r\n            # Unpatch ourselves immediately before calling the method `method_name`\r\n            # because itself may want to call the real `forward`\r\n            original_module.forward = original_forward  # type: ignore[method-assign]\r\n            # Call the actual method e.g. `.training_step(...)`\r\n            method = getattr(original_module, method_name)\r\n            out = method(*_args, **_kwargs)\r\n            self.on_after_inner_forward(wrapper_module, original_module)\r\n            return out\r\n\r\n        # Patch the original_module's forward so we can redirect the arguments back to the real method\r\n        original_module.forward = wrapped_forward  # type: ignore[method-assign]\r\n\r\n        wrapper_output = wrapper_module(*args, **kwargs)\r\n        self.on_after_outer_forward(wrapper_module, original_module)\r\n        return wrapper_output", "code_tokens": ["def", "__call__", "(", "self", ",", "wrapper_module", ":", "Module", ",", "original_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "method_name", ":", "str", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "\"", "\"", "\"", "Reroutes", "a", "method", "call", "through", "the", "`", "wrapper_module", "`", "'", "s", "`", "forward", "`", "method", ".", "Args", ":", "wrapper_module", ":", "The", "module", "that", "has", "`", "original_module", "`", "wrapped", ".", "original_module", ":", "The", "module", "that", "was", "wrapped", "inside", "`", "wrapper_module", "`", ".", "method_name", ":", "The", "name", "of", "the", "method", "that", "should", "be", "called", "on", "the", "`", "original_module", "`", "after", "inputs", "get", "redirected", "through", "the", "`", "wrapper_module", "`", "'", "s", "`", "forward", "`", "method", ".", "*", "args", ":", "The", "positional", "arguments", "to", "the", "method", "`", "method_name", "`", ".", "They", "will", "get", "passed", "to", "a", "patched", "`", "forward", "`", "method", "instead", ".", "*", "*", "kwargs", ":", "The", "keyword", "arguments", "to", "the", "method", "`", "method_name", "`", ".", "They", "will", "get", "passed", "to", "a", "patched", "`", "forward", "`", "method", "instead", ".", "\"", "\"", "\"", "assert", "method_name", "!", "=", "\"", "forward", "\"", "original_forward", "=", "original_module", ".", "forward", "def", "wrapped_forward", "(", "*", "_args", ":", "Any", ",", "*", "*", "_kwargs", ":", "Any", ")", "-", ">", "Any", ":", "original_module", ".", "forward", "=", "original_forward", "method", "=", "getattr", "(", "original_module", ",", "method_name", ")", "out", "=", "method", "(", "*", "_args", ",", "*", "*", "_kwargs", ")", "self", ".", "on_after_inner_forward", "(", "wrapper_module", ",", "original_module", ")", "return", "out", "original_module", ".", "forward", "=", "wrapped_forward", "wrapper_output", "=", "wrapper_module", "(", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "on_after_outer_forward", "(", "wrapper_module", ",", "original_module", ")", "return", "wrapper_output"], "docstring": "Reroutes a method call through the `wrapper_module`'s `forward` method.\r\n\r\n        Args:\r\n            wrapper_module: The module that has `original_module` wrapped.\r\n            original_module: The module that was wrapped inside `wrapper_module`.\r\n            method_name: The name of the method that should be called on the `original_module` after inputs get\r\n                redirected through the `wrapper_module`'s `forward` method.\r\n            *args: The positional arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.", "docstring_tokens": ["reroutes", "a", "method", "call", "through", "the", "wrapper_module", "s", "forward", "method", "args", "wrapper_module", "the", "module", "that", "has", "original_module", "wrapped", "original_module", "the", "module", "that", "was", "wrapped", "inside", "wrapper_module", "method_name", "the", "name", "of", "the", "method", "that", "should", "be", "called", "on", "the", "original_module", "after", "inputs", "get", "redirected", "through", "the", "wrapper_module", "s", "forward", "method", "args", "the", "positional", "arguments", "to", "the", "method", "method_name", "they", "will", "get", "passed", "to", "a", "patched", "forward", "method", "instead", "kwargs", "the", "keyword", "arguments", "to", "the", "method", "method_name", "they", "will", "get", "passed", "to", "a", "patched", "forward", "method", "instead"], "docstring_summary": "Reroutes a method call through the `wrapper_module`'s `forward` method.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "valid", "function_type": "class_method", "class_name": "_ForwardRedirection", "start_line": 608, "end_line": 642, "hash": "6656ae24652129795a6effb3b336f3b8", "complexity": 1, "parameters": ["wrapper_module", "original_module", "method_name", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\xla.py", "func_name": "remove_checkpoint", "original_string": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "language": "python", "code": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Remove", "checkpoint", "filepath", "from", "the", "filesystem", ".", "Args", ":", "filepath", ":", "Path", "to", "checkpoint", "\"", "\"", "\"", "if", "self", ".", "local_rank", "=", "=", "0", ":", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "filepath", ")"], "docstring": "Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint", "docstring_tokens": ["remove", "checkpoint", "filepath", "from", "the", "filesystem", "args", "filepath", "path", "to", "checkpoint"], "docstring_summary": "Remove checkpoint filepath from the filesystem.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\xla.py", "partition": "valid", "function_type": "class_method", "class_name": "XLAStrategy", "start_line": 310, "end_line": 318, "hash": "dac83bd931f0e6d1ce8b0836e4d98284", "complexity": 2, "parameters": ["filepath"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\xla.py", "func_name": "all_gather", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", ".", "Args", ":", "tensor", ":", "tensor", "to", "all", "-", "gather", ".", "group", ":", "unused", ".", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "-", "gather", "operation", ".", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", ".", ".", ".", ")", "\"", "\"", "\"", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "all_gather", "`", "is", "only", "implemented", "for", "tensors", ".", "Given", "{", "tensor", "}", "\"", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", "args", "tensor", "tensor", "to", "all", "gather", "group", "unused", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "gather", "operation", "return", "a", "tensor", "of", "shape", "world_size"], "docstring_summary": "Function to gather a tensor from several distributed processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\xla.py", "partition": "valid", "function_type": "class_method", "class_name": "XLAStrategy", "start_line": 321, "end_line": 348, "hash": "f8b747ddf7e2b93d37aa05415f08aacb", "complexity": 5, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            # resolving https://github.com/Lightning-AI/pytorch-lightning/issues/18775 will lift this restriction\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this limitation by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        # The default cluster environment in Lightning chooses a random free port number\r\n        # This needs to be done in the main process here before starting processes to ensure each rank will connect\r\n        # through the same port\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [trainer, function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [trainer, function, args, kwargs, return_queue]\r\n\r\n        process_context = mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n        )\r\n        self.procs = process_context.processes\r\n        while not process_context.join():\r\n            pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            # resolving https://github.com/Lightning-AI/pytorch-lightning/issues/18775 will lift this restriction\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this limitation by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        # The default cluster environment in Lightning chooses a random free port number\r\n        # This needs to be done in the main process here before starting processes to ensure each rank will connect\r\n        # through the same port\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [trainer, function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [trainer, function, args, kwargs, return_queue]\r\n\r\n        process_context = mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n        )\r\n        self.procs = process_context.processes\r\n        while not process_context.join():\r\n            pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "\"", "pl", ".", "Trainer", "\"", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", ".", "The", "function", "is", "allowed", "to", "have", "a", "return", "value", ".", "However", ",", "when", "all", "processes", "join", ",", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "`", "launch", "`", "method", "in", "the", "main", "process", ".", "Arguments", ":", "function", ":", "The", "entry", "point", "for", "all", "launched", "processes", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "trainer", ":", "Optional", "reference", "to", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "for", "which", "a", "selected", "set", "of", "attributes", "get", "restored", "in", "the", "main", "process", "after", "processes", "join", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "if", "self", ".", "_start_method", "in", "(", "\"", "fork", "\"", ",", "\"", "forkserver", "\"", ")", ":", "_check_bad_cuda_fork", "(", ")", "if", "self", ".", "_start_method", "=", "=", "\"", "spawn", "\"", ":", "_check_missing_main_guard", "(", ")", "if", "self", ".", "_already_fit", "and", "trainer", "is", "not", "None", "and", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "raise", "NotImplementedError", "(", "\"", "Calling", "`", "trainer", ".", "fit", "(", ")", "`", "twice", "on", "the", "same", "Trainer", "instance", "using", "a", "spawn", "-", "based", "strategy", "is", "not", "\"", "\"", "supported", ".", "You", "can", "work", "around", "this", "limitation", "by", "creating", "a", "new", "Trainer", "instance", "and", "passing", "the", "\"", "\"", "`", "fit", "(", "ckpt_path", "=", ".", ".", ".", ")", "`", "argument", ".", "\"", ")", "assert", "self", ".", "_strategy", ".", "cluster_environment", "is", "not", "None", "os", ".", "environ", "[", "\"", "MASTER_PORT", "\"", "]", "=", "str", "(", "self", ".", "_strategy", ".", "cluster_environment", ".", "main_port", ")", "context", "=", "mp", ".", "get_context", "(", "self", ".", "_start_method", ")", "return_queue", "=", "context", ".", "SimpleQueue", "(", ")", "if", "self", ".", "_start_method", "=", "=", "\"", "spawn", "\"", ":", "global_states", "=", "_GlobalStateSnapshot", ".", "capture", "(", ")", "process_args", "=", "[", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", ",", "global_states", "]", "else", ":", "process_args", "=", "[", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", "]", "process_context", "=", "mp", ".", "start_processes", "(", "self", ".", "_wrapping_function", ",", "args", "=", "process_args", ",", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", ",", "start_method", "=", "self", ".", "_start_method", ",", "join", "=", "False", ",", ")", "self", ".", "procs", "=", "process_context", ".", "processes", "while", "not", "process_context", ".", "join", "(", ")", ":", "pass", "worker_output", "=", "return_queue", ".", "get", "(", ")", "if", "trainer", "is", "None", ":", "return", "worker_output", "self", ".", "_already_fit", "|", "=", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "self", ".", "_recover_results_in_main_process", "(", "worker_output", ",", "trainer", ")", "return", "worker_output", ".", "trainer_results"], "docstring": "Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", "the", "function", "is", "allowed", "to", "have", "a", "return", "value", "however", "when", "all", "processes", "join", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "launch", "method", "in", "the", "main", "process", "arguments", "function", "the", "entry", "point", "for", "all", "launched", "processes", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "trainer", "optional", "reference", "to", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "for", "which", "a", "selected", "set", "of", "attributes", "get", "restored", "in", "the", "main", "process", "after", "processes", "join", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Launches processes that run the given function in parallel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "partition": "valid", "function_type": "class_method", "class_name": "_MultiProcessingLauncher", "start_line": 94, "end_line": 152, "hash": "a733d3bbb9edfebd855c39b7bf66cdac", "complexity": 9, "parameters": ["function", "*args", "trainer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "get_extra_results", "original_string": "def get_extra_results(self, trainer: \"pl.Trainer\") -> dict[str, Any]:\r\n        \"\"\"Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To\r\n        avoid issues with memory sharing, we convert tensors to bytes.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n\r\n        Returns:\r\n            A dictionary with items to send back to the main process where :meth:`update_main_process_results` will\r\n            process this output.\r\n\r\n        \"\"\"\r\n        callback_metrics = apply_to_collection(trainer.callback_metrics, Tensor, lambda t: t.cpu())\r\n        buffer = io.BytesIO()\r\n        torch.save(callback_metrics, buffer)\r\n        # send tensors as bytes to avoid issues with memory sharing\r\n        return {\"callback_metrics_bytes\": buffer.getvalue()}", "language": "python", "code": "def get_extra_results(self, trainer: \"pl.Trainer\") -> dict[str, Any]:\r\n        \"\"\"Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To\r\n        avoid issues with memory sharing, we convert tensors to bytes.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n\r\n        Returns:\r\n            A dictionary with items to send back to the main process where :meth:`update_main_process_results` will\r\n            process this output.\r\n\r\n        \"\"\"\r\n        callback_metrics = apply_to_collection(trainer.callback_metrics, Tensor, lambda t: t.cpu())\r\n        buffer = io.BytesIO()\r\n        torch.save(callback_metrics, buffer)\r\n        # send tensors as bytes to avoid issues with memory sharing\r\n        return {\"callback_metrics_bytes\": buffer.getvalue()}", "code_tokens": ["def", "get_extra_results", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Gather", "extra", "state", "from", "the", "Trainer", "and", "return", "it", "as", "a", "dictionary", "for", "sending", "back", "to", "the", "main", "process", ".", "To", "avoid", "issues", "with", "memory", "sharing", ",", "we", "convert", "tensors", "to", "bytes", ".", "Args", ":", "trainer", ":", "reference", "to", "the", "Trainer", ".", "Returns", ":", "A", "dictionary", "with", "items", "to", "send", "back", "to", "the", "main", "process", "where", ":", "meth", ":", "`", "update_main_process_results", "`", "will", "process", "this", "output", ".", "\"", "\"", "\"", "callback_metrics", "=", "apply_to_collection", "(", "trainer", ".", "callback_metrics", ",", "Tensor", ",", "lambda", "t", ":", "t", ".", "cpu", "(", ")", ")", "buffer", "=", "io", ".", "BytesIO", "(", ")", "torch", ".", "save", "(", "callback_metrics", ",", "buffer", ")", "return", "{", "\"", "callback_metrics_bytes", "\"", ":", "buffer", ".", "getvalue", "(", ")", "}"], "docstring": "Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To\r\n        avoid issues with memory sharing, we convert tensors to bytes.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n\r\n        Returns:\r\n            A dictionary with items to send back to the main process where :meth:`update_main_process_results` will\r\n            process this output.", "docstring_tokens": ["gather", "extra", "state", "from", "the", "trainer", "and", "return", "it", "as", "a", "dictionary", "for", "sending", "back", "to", "the", "main", "process", "to", "avoid", "issues", "with", "memory", "sharing", "we", "convert", "tensors", "to", "bytes", "args", "trainer", "reference", "to", "the", "trainer", "returns", "a", "dictionary", "with", "items", "to", "send", "back", "to", "the", "main", "process", "where", "meth", "update_main_process_results", "will", "process", "this", "output"], "docstring_summary": "Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "partition": "valid", "function_type": "class_method", "class_name": "_MultiProcessingLauncher", "start_line": 226, "end_line": 242, "hash": "49f7afa4ab8c4950c98fc665136354b7", "complexity": 1, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "update_main_process_results", "original_string": "def update_main_process_results(self, trainer: \"pl.Trainer\", extra: dict[str, Any]) -> None:\r\n        \"\"\"Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we\r\n        convert bytes back to ``torch.Tensor``.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n            extra: A dictionary with trainer state that was sent from the worker process and needs to be restored\r\n                on the current trainer.\r\n\r\n        \"\"\"\r\n        # NOTE: `get_extra_results` needs to be called before\r\n        callback_metrics_bytes = extra[\"callback_metrics_bytes\"]\r\n        callback_metrics = torch.load(io.BytesIO(callback_metrics_bytes), weights_only=True)\r\n        trainer.callback_metrics.update(callback_metrics)", "language": "python", "code": "def update_main_process_results(self, trainer: \"pl.Trainer\", extra: dict[str, Any]) -> None:\r\n        \"\"\"Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we\r\n        convert bytes back to ``torch.Tensor``.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n            extra: A dictionary with trainer state that was sent from the worker process and needs to be restored\r\n                on the current trainer.\r\n\r\n        \"\"\"\r\n        # NOTE: `get_extra_results` needs to be called before\r\n        callback_metrics_bytes = extra[\"callback_metrics_bytes\"]\r\n        callback_metrics = torch.load(io.BytesIO(callback_metrics_bytes), weights_only=True)\r\n        trainer.callback_metrics.update(callback_metrics)", "code_tokens": ["def", "update_main_process_results", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "extra", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Retrieve", "the", ":", "attr", ":", "`", "trainer", ".", "callback_metrics", "`", "dictionary", "from", "the", "given", "queue", ".", "To", "preserve", "consistency", ",", "we", "convert", "bytes", "back", "to", "`", "`", "torch", ".", "Tensor", "`", "`", ".", "Args", ":", "trainer", ":", "reference", "to", "the", "Trainer", ".", "extra", ":", "A", "dictionary", "with", "trainer", "state", "that", "was", "sent", "from", "the", "worker", "process", "and", "needs", "to", "be", "restored", "on", "the", "current", "trainer", ".", "\"", "\"", "\"", "callback_metrics_bytes", "=", "extra", "[", "\"", "callback_metrics_bytes", "\"", "]", "callback_metrics", "=", "torch", ".", "load", "(", "io", ".", "BytesIO", "(", "callback_metrics_bytes", ")", ",", "weights_only", "=", "True", ")", "trainer", ".", "callback_metrics", ".", "update", "(", "callback_metrics", ")"], "docstring": "Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we\r\n        convert bytes back to ``torch.Tensor``.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n            extra: A dictionary with trainer state that was sent from the worker process and needs to be restored\r\n                on the current trainer.", "docstring_tokens": ["retrieve", "the", "attr", "trainer", "callback_metrics", "dictionary", "from", "the", "given", "queue", "to", "preserve", "consistency", "we", "convert", "bytes", "back", "to", "torch", "tensor", "args", "trainer", "reference", "to", "the", "trainer", "extra", "a", "dictionary", "with", "trainer", "state", "that", "was", "sent", "from", "the", "worker", "process", "and", "needs", "to", "be", "restored", "on", "the", "current", "trainer"], "docstring_summary": "Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "partition": "valid", "function_type": "class_method", "class_name": "_MultiProcessingLauncher", "start_line": 244, "end_line": 257, "hash": "807a6fb805b797578130a87c7f489573", "complexity": 1, "parameters": ["trainer", "extra", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "capture", "original_string": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "language": "python", "code": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "code_tokens": ["def", "capture", "(", "cls", ")", "-", ">", "\"", "_GlobalStateSnapshot", "\"", ":", "\"", "\"", "\"", "Capture", "a", "few", "global", "states", "from", "torch", ",", "numpy", ",", "etc", ".", ",", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process", ".", "\"", "\"", "\"", "return", "cls", "(", "use_deterministic_algorithms", "=", "torch", ".", "are_deterministic_algorithms_enabled", "(", ")", ",", "use_deterministic_algorithms_warn_only", "=", "torch", ".", "is_deterministic_algorithms_warn_only_enabled", "(", ")", ",", "cudnn_benchmark", "=", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", ",", "rng_states", "=", "_collect_rng_states", "(", ")", ",", ")"], "docstring": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "docstring_tokens": ["capture", "a", "few", "global", "states", "from", "torch", "numpy", "etc", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process"], "docstring_summary": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "partition": "valid", "function_type": "class_method", "class_name": "_GlobalStateSnapshot", "start_line": 306, "end_line": 313, "hash": "110125338e797add4550ce58c61fcfdf", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "restore", "original_string": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "language": "python", "code": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "code_tokens": ["def", "restore", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "all", "globals", "to", "the", "values", "captured", "in", "the", ":", "meth", ":", "`", "capture", "`", "method", ".", "\"", "\"", "\"", "torch", ".", "use_deterministic_algorithms", "(", "self", ".", "use_deterministic_algorithms", ",", "warn_only", "=", "self", ".", "use_deterministic_algorithms_warn_only", ")", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "self", ".", "cudnn_benchmark", "_set_rng_states", "(", "self", ".", "rng_states", ")"], "docstring": "Restores all globals to the values captured in the :meth:`capture` method.", "docstring_tokens": ["restores", "all", "globals", "to", "the", "values", "captured", "in", "the", "meth", "capture", "method"], "docstring_summary": "Restores all globals to the values captured in the :meth:`capture` method.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "partition": "valid", "function_type": "class_method", "class_name": "_GlobalStateSnapshot", "start_line": 315, "end_line": 321, "hash": "e0ee77b196b67a7498babe45229e9b73", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\subprocess_script.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "\"", "pl", ".", "Trainer", "\"", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Creates", "new", "processes", ",", "then", "calls", "the", "given", "function", ".", "Arguments", ":", "function", ":", "A", "callback", "function", "to", "execute", "after", "all", "processes", "have", "been", "created", ".", "It", "is", "up", "to", "the", "implementation", "of", "this", "function", "to", "synchronize", "the", "processes", ",", "e", ".", "g", ".", ",", "with", "barriers", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "trainer", ":", "Optional", "reference", "to", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "self", ".", "cluster_environment", ".", "validate_settings", "(", "num_devices", "=", "self", ".", "num_processes", ",", "num_nodes", "=", "self", ".", "num_nodes", ")", "if", "not", "self", ".", "cluster_environment", ".", "creates_processes_externally", ":", "self", ".", "_call_children_scripts", "(", ")", "_launch_process_observer", "(", "self", ".", "procs", ")", "_set_num_threads_if_needed", "(", "num_processes", "=", "self", ".", "num_processes", ")", "return", "function", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["creates", "new", "processes", "then", "calls", "the", "given", "function", "arguments", "function", "a", "callback", "function", "to", "execute", "after", "all", "processes", "have", "been", "created", "it", "is", "up", "to", "the", "implementation", "of", "this", "function", "to", "synchronize", "the", "processes", "e", "g", "with", "barriers", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "trainer", "optional", "reference", "to", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Creates new processes, then calls the given function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\subprocess_script.py", "partition": "valid", "function_type": "class_method", "class_name": "_SubprocessScriptLauncher", "start_line": 87, "end_line": 104, "hash": "e404e8947eb028d8d62388accd82f10e", "complexity": 2, "parameters": ["function", "*args", "trainer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\xla.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            # resolving https://github.com/Lightning-AI/pytorch-lightning/issues/18775 will lift this restriction\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        # pjrt requires that the queue is serializable\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            # avoid warning: \"Unsupported nprocs\". If it's 1, it will call the launched function directly.\r\n            # otherwise it will use all devices\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        process_context = xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(trainer, function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n            **spawn_kwargs,\r\n        )\r\n        # xla will not actually create processes if only 1 device\r\n        if process_context is not None:\r\n            self.procs = process_context.processes\r\n            while not process_context.join():\r\n                pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            # resolving https://github.com/Lightning-AI/pytorch-lightning/issues/18775 will lift this restriction\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        # pjrt requires that the queue is serializable\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            # avoid warning: \"Unsupported nprocs\". If it's 1, it will call the launched function directly.\r\n            # otherwise it will use all devices\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        process_context = xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(trainer, function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n            **spawn_kwargs,\r\n        )\r\n        # xla will not actually create processes if only 1 device\r\n        if process_context is not None:\r\n            self.procs = process_context.processes\r\n            while not process_context.join():\r\n                pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "\"", "pl", ".", "Trainer", "\"", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", ".", "The", "function", "is", "allowed", "to", "have", "a", "return", "value", ".", "However", ",", "when", "all", "processes", "join", ",", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "`", "launch", "`", "method", "in", "the", "main", "process", ".", "Arguments", ":", "function", ":", "The", "entry", "point", "for", "all", "launched", "processes", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "trainer", ":", "Optional", "reference", "to", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "for", "which", "a", "selected", "set", "of", "attributes", "get", "restored", "in", "the", "main", "process", "after", "processes", "join", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "if", "self", ".", "_already_fit", "and", "trainer", "is", "not", "None", "and", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "raise", "NotImplementedError", "(", "\"", "Calling", "`", "trainer", ".", "fit", "(", ")", "`", "twice", "on", "the", "same", "Trainer", "instance", "using", "a", "spawn", "-", "based", "strategy", "is", "not", "\"", "\"", "supported", ".", "You", "can", "work", "around", "this", "by", "creating", "a", "new", "Trainer", "instance", "and", "passing", "the", "\"", "\"", "`", "fit", "(", "ckpt_path", "=", ".", ".", ".", ")", "`", "argument", ".", "\"", ")", "return_queue", "=", "mp", ".", "Manager", "(", ")", ".", "Queue", "(", ")", "import", "torch_xla", ".", "distributed", ".", "xla_multiprocessing", "as", "xmp", "spawn_kwargs", "=", "{", "}", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", "if", "nprocs", "=", "=", "1", ":", "spawn_kwargs", "[", "\"", "nprocs", "\"", "]", "=", "nprocs", "process_context", "=", "xmp", ".", "spawn", "(", "self", ".", "_wrapping_function", ",", "args", "=", "(", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", ")", ",", "start_method", "=", "self", ".", "_start_method", ",", "join", "=", "False", ",", "*", "*", "spawn_kwargs", ",", ")", "if", "process_context", "is", "not", "None", ":", "self", ".", "procs", "=", "process_context", ".", "processes", "while", "not", "process_context", ".", "join", "(", ")", ":", "pass", "worker_output", "=", "return_queue", ".", "get", "(", ")", "if", "trainer", "is", "None", ":", "return", "worker_output", "self", ".", "_already_fit", "|", "=", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "self", ".", "_recover_results_in_main_process", "(", "worker_output", ",", "trainer", ")", "return", "worker_output", ".", "trainer_results"], "docstring": "Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", "the", "function", "is", "allowed", "to", "have", "a", "return", "value", "however", "when", "all", "processes", "join", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "launch", "method", "in", "the", "main", "process", "arguments", "function", "the", "entry", "point", "for", "all", "launched", "processes", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "trainer", "optional", "reference", "to", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "for", "which", "a", "selected", "set", "of", "attributes", "get", "restored", "in", "the", "main", "process", "after", "processes", "join", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Launches processes that run the given function in parallel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\launchers\\xla.py", "partition": "valid", "function_type": "class_method", "class_name": "_XLALauncher", "start_line": 63, "end_line": 116, "hash": "458fb8a276c5fb443627a986ca1d79ed", "complexity": 8, "parameters": ["function", "*args", "trainer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "_call_and_handle_interrupt", "original_string": "def _call_and_handle_interrupt(trainer: \"pl.Trainer\", trainer_fn: Callable, *args: Any, **kwargs: Any) -> Any:\r\n    r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\r\n    as all errors should funnel through them.\r\n\r\n    Args:\r\n        trainer_fn: one of (fit, validate, test, predict)\r\n        *args: positional arguments to be passed to the `trainer_fn`\r\n        **kwargs: keyword arguments to be passed to `trainer_fn`\r\n\r\n    \"\"\"\r\n    try:\r\n        if trainer.strategy.launcher is not None:\r\n            return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n        return trainer_fn(*args, **kwargs)\r\n\r\n    except _TunerExitException:\r\n        _call_teardown_hook(trainer)\r\n        trainer._teardown()\r\n        trainer.state.status = TrainerStatus.FINISHED\r\n        trainer.state.stage = None\r\n\r\n    except KeyboardInterrupt as exception:\r\n        rank_zero_info(\"\\nDetected KeyboardInterrupt, attempting graceful shutdown ...\")\r\n        # user could press Ctrl+C many times, disable KeyboardInterrupt for shutdown\r\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        launcher = trainer.strategy.launcher\r\n        if isinstance(launcher, _SubprocessScriptLauncher):\r\n            launcher.kill(_get_sigkill_signal())\r\n        sys.exit(1)\r\n\r\n    except BaseException as exception:\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        # teardown might access the stage so we reset it after\r\n        trainer.state.stage = None\r\n        raise", "language": "python", "code": "def _call_and_handle_interrupt(trainer: \"pl.Trainer\", trainer_fn: Callable, *args: Any, **kwargs: Any) -> Any:\r\n    r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\r\n    as all errors should funnel through them.\r\n\r\n    Args:\r\n        trainer_fn: one of (fit, validate, test, predict)\r\n        *args: positional arguments to be passed to the `trainer_fn`\r\n        **kwargs: keyword arguments to be passed to `trainer_fn`\r\n\r\n    \"\"\"\r\n    try:\r\n        if trainer.strategy.launcher is not None:\r\n            return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n        return trainer_fn(*args, **kwargs)\r\n\r\n    except _TunerExitException:\r\n        _call_teardown_hook(trainer)\r\n        trainer._teardown()\r\n        trainer.state.status = TrainerStatus.FINISHED\r\n        trainer.state.stage = None\r\n\r\n    except KeyboardInterrupt as exception:\r\n        rank_zero_info(\"\\nDetected KeyboardInterrupt, attempting graceful shutdown ...\")\r\n        # user could press Ctrl+C many times, disable KeyboardInterrupt for shutdown\r\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        launcher = trainer.strategy.launcher\r\n        if isinstance(launcher, _SubprocessScriptLauncher):\r\n            launcher.kill(_get_sigkill_signal())\r\n        sys.exit(1)\r\n\r\n    except BaseException as exception:\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        # teardown might access the stage so we reset it after\r\n        trainer.state.stage = None\r\n        raise", "code_tokens": ["def", "_call_and_handle_interrupt", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "trainer_fn", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "r", "\"", "\"", "\"", "Error", "handling", ",", "intended", "to", "be", "used", "only", "for", "main", "trainer", "function", "entry", "points", "(", "fit", ",", "validate", ",", "test", ",", "predict", ")", "as", "all", "errors", "should", "funnel", "through", "them", ".", "Args", ":", "trainer_fn", ":", "one", "of", "(", "fit", ",", "validate", ",", "test", ",", "predict", ")", "*", "args", ":", "positional", "arguments", "to", "be", "passed", "to", "the", "`", "trainer_fn", "`", "*", "*", "kwargs", ":", "keyword", "arguments", "to", "be", "passed", "to", "`", "trainer_fn", "`", "\"", "\"", "\"", "try", ":", "if", "trainer", ".", "strategy", ".", "launcher", "is", "not", "None", ":", "return", "trainer", ".", "strategy", ".", "launcher", ".", "launch", "(", "trainer_fn", ",", "*", "args", ",", "trainer", "=", "trainer", ",", "*", "*", "kwargs", ")", "return", "trainer_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "except", "_TunerExitException", ":", "_call_teardown_hook", "(", "trainer", ")", "trainer", ".", "_teardown", "(", ")", "trainer", ".", "state", ".", "status", "=", "TrainerStatus", ".", "FINISHED", "trainer", ".", "state", ".", "stage", "=", "None", "except", "KeyboardInterrupt", "as", "exception", ":", "rank_zero_info", "(", "\"", "\\", "nDetected", "KeyboardInterrupt", ",", "attempting", "graceful", "shutdown", ".", ".", ".", "\"", ")", "signal", ".", "signal", "(", "signal", ".", "SIGINT", ",", "signal", ".", "SIG_IGN", ")", "_interrupt", "(", "trainer", ",", "exception", ")", "trainer", ".", "_teardown", "(", ")", "launcher", "=", "trainer", ".", "strategy", ".", "launcher", "if", "isinstance", "(", "launcher", ",", "_SubprocessScriptLauncher", ")", ":", "launcher", ".", "kill", "(", "_get_sigkill_signal", "(", ")", ")", "sys", ".", "exit", "(", "1", ")", "except", "BaseException", "as", "exception", ":", "_interrupt", "(", "trainer", ",", "exception", ")", "trainer", ".", "_teardown", "(", ")", "trainer", ".", "state", ".", "stage", "=", "None", "raise"], "docstring": "r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\r\n    as all errors should funnel through them.\r\n\r\n    Args:\r\n        trainer_fn: one of (fit, validate, test, predict)\r\n        *args: positional arguments to be passed to the `trainer_fn`\r\n        **kwargs: keyword arguments to be passed to `trainer_fn`\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "error", "handling", "intended", "to", "be", "used", "only", "for", "main", "trainer", "function", "entry", "points", "fit", "validate", "test", "predict", "as", "all", "errors", "should", "funnel", "through", "them", "args", "trainer_fn", "one", "of", "fit", "validate", "test", "predict", "args", "positional", "arguments", "to", "be", "passed", "to", "the", "trainer_fn", "kwargs", "keyword", "arguments", "to", "be", "passed", "to", "trainer_fn"], "docstring_summary": "r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\call.py", "partition": "valid", "function_type": "function", "start_line": 35, "end_line": 72, "hash": "d2c72d9d29abd164b57f6314406f13dc", "complexity": 6, "parameters": ["trainer", "trainer_fn", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "_call_callbacks_state_dict", "original_string": "def _call_callbacks_state_dict(trainer: \"pl.Trainer\") -> dict[str, dict]:\r\n    \"\"\"Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by\r\n    `Callback.state_key`.\"\"\"\r\n    callback_state_dicts = {}\r\n    for callback in trainer.callbacks:\r\n        state_dict = callback.state_dict()\r\n        if state_dict:\r\n            callback_state_dicts[callback.state_key] = state_dict\r\n    return callback_state_dicts", "language": "python", "code": "def _call_callbacks_state_dict(trainer: \"pl.Trainer\") -> dict[str, dict]:\r\n    \"\"\"Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by\r\n    `Callback.state_key`.\"\"\"\r\n    callback_state_dicts = {}\r\n    for callback in trainer.callbacks:\r\n        state_dict = callback.state_dict()\r\n        if state_dict:\r\n            callback_state_dicts[callback.state_key] = state_dict\r\n    return callback_state_dicts", "code_tokens": ["def", "_call_callbacks_state_dict", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "dict", "[", "str", ",", "dict", "]", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "model", "checkpoint", ",", "calls", "and", "returns", "every", "callback", "'", "s", "`", "state_dict", "`", ",", "keyed", "by", "`", "Callback", ".", "state_key", "`", ".", "\"", "\"", "\"", "callback_state_dicts", "=", "{", "}", "for", "callback", "in", "trainer", ".", "callbacks", ":", "state_dict", "=", "callback", ".", "state_dict", "(", ")", "if", "state_dict", ":", "callback_state_dicts", "[", "callback", ".", "state_key", "]", "=", "state_dict", "return", "callback_state_dicts"], "docstring": "Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by\r\n    `Callback.state_key`.", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "calls", "and", "returns", "every", "callback", "s", "state_dict", "keyed", "by", "callback", "state_key"], "docstring_summary": "Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\call.py", "partition": "valid", "function_type": "function", "start_line": 234, "end_line": 242, "hash": "25f8baf0959dc8e102bdc700f488bf66", "complexity": 3, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "_call_callbacks_on_save_checkpoint", "original_string": "def _call_callbacks_on_save_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.\"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_save_checkpoint\"\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_save_checkpoint\"):\r\n            callback.on_save_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        # restore current_fx when nested context\r\n        pl_module._current_fx_name = prev_fx_name", "language": "python", "code": "def _call_callbacks_on_save_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.\"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_save_checkpoint\"\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_save_checkpoint\"):\r\n            callback.on_save_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        # restore current_fx when nested context\r\n        pl_module._current_fx_name = prev_fx_name", "code_tokens": ["def", "_call_callbacks_on_save_checkpoint", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "model", "checkpoint", ",", "calls", "every", "callback", "'", "s", "`", "on_save_checkpoint", "`", "hook", ".", "\"", "\"", "\"", "pl_module", "=", "trainer", ".", "lightning_module", "if", "pl_module", ":", "prev_fx_name", "=", "pl_module", ".", "_current_fx_name", "pl_module", ".", "_current_fx_name", "=", "\"", "on_save_checkpoint", "\"", "for", "callback", "in", "trainer", ".", "callbacks", ":", "with", "trainer", ".", "profiler", ".", "profile", "(", "f", "\"", "[", "Callback", "]", "{", "callback", ".", "state_key", "}", ".", "on_save_checkpoint", "\"", ")", ":", "callback", ".", "on_save_checkpoint", "(", "trainer", ",", "trainer", ".", "lightning_module", ",", "checkpoint", ")", "if", "pl_module", ":", "pl_module", ".", "_current_fx_name", "=", "prev_fx_name"], "docstring": "Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "calls", "every", "callback", "s", "on_save_checkpoint", "hook"], "docstring_summary": "Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\call.py", "partition": "valid", "function_type": "function", "start_line": 245, "end_line": 258, "hash": "0fee5384bf48330402f6340e10ef8e46", "complexity": 5, "parameters": ["trainer", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "_call_callbacks_on_load_checkpoint", "original_string": "def _call_callbacks_on_load_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint.\r\n\r\n    Calls every callback's `on_load_checkpoint` hook. We have a dedicated function for this rather than using\r\n    `_call_callback_hooks` because we have special logic for getting callback_states.\r\n\r\n    \"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_load_checkpoint\"\r\n\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    is_legacy_ckpt = Version(checkpoint[\"pytorch-lightning_version\"]) < Version(\"1.5.0dev\")\r\n    current_callbacks_keys = {cb._legacy_state_key if is_legacy_ckpt else cb.state_key for cb in trainer.callbacks}\r\n    difference = callback_states.keys() - current_callbacks_keys\r\n    if difference:\r\n        rank_zero_warn(\r\n            \"Be aware that when using `ckpt_path`,\"\r\n            \" callbacks used to create the checkpoint need to be provided during `Trainer` instantiation.\"\r\n            f\" Please add the following callbacks: {list(difference)}.\",\r\n        )\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_load_checkpoint\"):\r\n            callback.on_load_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        # restore current_fx when nested context\r\n        pl_module._current_fx_name = prev_fx_name", "language": "python", "code": "def _call_callbacks_on_load_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint.\r\n\r\n    Calls every callback's `on_load_checkpoint` hook. We have a dedicated function for this rather than using\r\n    `_call_callback_hooks` because we have special logic for getting callback_states.\r\n\r\n    \"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_load_checkpoint\"\r\n\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    is_legacy_ckpt = Version(checkpoint[\"pytorch-lightning_version\"]) < Version(\"1.5.0dev\")\r\n    current_callbacks_keys = {cb._legacy_state_key if is_legacy_ckpt else cb.state_key for cb in trainer.callbacks}\r\n    difference = callback_states.keys() - current_callbacks_keys\r\n    if difference:\r\n        rank_zero_warn(\r\n            \"Be aware that when using `ckpt_path`,\"\r\n            \" callbacks used to create the checkpoint need to be provided during `Trainer` instantiation.\"\r\n            f\" Please add the following callbacks: {list(difference)}.\",\r\n        )\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_load_checkpoint\"):\r\n            callback.on_load_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        # restore current_fx when nested context\r\n        pl_module._current_fx_name = prev_fx_name", "code_tokens": ["def", "_call_callbacks_on_load_checkpoint", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "model", "checkpoint", ".", "Calls", "every", "callback", "'", "s", "`", "on_load_checkpoint", "`", "hook", ".", "We", "have", "a", "dedicated", "function", "for", "this", "rather", "than", "using", "`", "_call_callback_hooks", "`", "because", "we", "have", "special", "logic", "for", "getting", "callback_states", ".", "\"", "\"", "\"", "pl_module", "=", "trainer", ".", "lightning_module", "if", "pl_module", ":", "prev_fx_name", "=", "pl_module", ".", "_current_fx_name", "pl_module", ".", "_current_fx_name", "=", "\"", "on_load_checkpoint", "\"", "callback_states", ":", "Optional", "[", "dict", "[", "Union", "[", "type", ",", "str", "]", ",", "dict", "]", "]", "=", "checkpoint", ".", "get", "(", "\"", "callbacks", "\"", ")", "if", "callback_states", "is", "None", ":", "return", "is_legacy_ckpt", "=", "Version", "(", "checkpoint", "[", "\"", "pytorch", "-", "lightning_version", "\"", "]", ")", "<", "Version", "(", "\"", "1", ".", "5", ".", "0dev", "\"", ")", "current_callbacks_keys", "=", "{", "cb", ".", "_legacy_state_key", "if", "is_legacy_ckpt", "else", "cb", ".", "state_key", "for", "cb", "in", "trainer", ".", "callbacks", "}", "difference", "=", "callback_states", ".", "keys", "(", ")", "-", "current_callbacks_keys", "if", "difference", ":", "rank_zero_warn", "(", "\"", "Be", "aware", "that", "when", "using", "`", "ckpt_path", "`", ",", "\"", "\"", "callbacks", "used", "to", "create", "the", "checkpoint", "need", "to", "be", "provided", "during", "`", "Trainer", "`", "instantiation", ".", "\"", "f", "\"", "Please", "add", "the", "following", "callbacks", ":", "{", "list", "(", "difference", ")", "}", ".", "\"", ",", ")", "for", "callback", "in", "trainer", ".", "callbacks", ":", "with", "trainer", ".", "profiler", ".", "profile", "(", "f", "\"", "[", "Callback", "]", "{", "callback", ".", "state_key", "}", ".", "on_load_checkpoint", "\"", ")", ":", "callback", ".", "on_load_checkpoint", "(", "trainer", ",", "trainer", ".", "lightning_module", ",", "checkpoint", ")", "if", "pl_module", ":", "pl_module", ".", "_current_fx_name", "=", "prev_fx_name"], "docstring": "Called when loading a model checkpoint.\r\n\r\n    Calls every callback's `on_load_checkpoint` hook. We have a dedicated function for this rather than using\r\n    `_call_callback_hooks` because we have special logic for getting callback_states.", "docstring_tokens": ["called", "when", "loading", "a", "model", "checkpoint", "calls", "every", "callback", "s", "on_load_checkpoint", "hook", "we", "have", "a", "dedicated", "function", "for", "this", "rather", "than", "using", "_call_callback_hooks", "because", "we", "have", "special", "logic", "for", "getting", "callback_states"], "docstring_summary": "Called when loading a model checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\call.py", "partition": "valid", "function_type": "function", "start_line": 261, "end_line": 294, "hash": "8f482283227dd23929cbd38538a6f49d", "complexity": 9, "parameters": ["trainer", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "_call_callbacks_load_state_dict", "original_string": "def _call_callbacks_load_state_dict(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint, calls every callback's `load_state_dict`.\"\"\"\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    for callback in trainer.callbacks:\r\n        state = callback_states.get(callback.state_key, callback_states.get(callback._legacy_state_key))\r\n        if state:\r\n            state = deepcopy(state)\r\n            callback.load_state_dict(state)", "language": "python", "code": "def _call_callbacks_load_state_dict(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint, calls every callback's `load_state_dict`.\"\"\"\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    for callback in trainer.callbacks:\r\n        state = callback_states.get(callback.state_key, callback_states.get(callback._legacy_state_key))\r\n        if state:\r\n            state = deepcopy(state)\r\n            callback.load_state_dict(state)", "code_tokens": ["def", "_call_callbacks_load_state_dict", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "model", "checkpoint", ",", "calls", "every", "callback", "'", "s", "`", "load_state_dict", "`", ".", "\"", "\"", "\"", "callback_states", ":", "Optional", "[", "dict", "[", "Union", "[", "type", ",", "str", "]", ",", "dict", "]", "]", "=", "checkpoint", ".", "get", "(", "\"", "callbacks", "\"", ")", "if", "callback_states", "is", "None", ":", "return", "for", "callback", "in", "trainer", ".", "callbacks", ":", "state", "=", "callback_states", ".", "get", "(", "callback", ".", "state_key", ",", "callback_states", ".", "get", "(", "callback", ".", "_legacy_state_key", ")", ")", "if", "state", ":", "state", "=", "deepcopy", "(", "state", ")", "callback", ".", "load_state_dict", "(", "state", ")"], "docstring": "Called when loading a model checkpoint, calls every callback's `load_state_dict`.", "docstring_tokens": ["called", "when", "loading", "a", "model", "checkpoint", "calls", "every", "callback", "s", "load_state_dict"], "docstring_summary": "Called when loading a model checkpoint, calls every callback's `load_state_dict`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\call.py", "partition": "valid", "function_type": "function", "start_line": 297, "end_line": 308, "hash": "dd04cdd18253715dae5d6e8aca2ea3b7", "complexity": 4, "parameters": ["trainer", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "func_name": "_verify_loop_configurations", "original_string": "def _verify_loop_configurations(trainer: \"pl.Trainer\") -> None:\r\n    r\"\"\"Checks that the model is configured correctly before the run is started.\r\n\r\n    Args:\r\n        trainer: Lightning Trainer. Its `lightning_module` (the model) to check the configuration.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n\r\n    if trainer.state.fn is None:\r\n        raise ValueError(\"Unexpected: Trainer state fn must be set before validating loop configuration.\")\r\n    if trainer.state.fn == TrainerFn.FITTING:\r\n        __verify_train_val_loop_configuration(trainer, model)\r\n        __verify_manual_optimization_support(trainer, model)\r\n    elif trainer.state.fn == TrainerFn.VALIDATING:\r\n        __verify_eval_loop_configuration(model, \"val\")\r\n    elif trainer.state.fn == TrainerFn.TESTING:\r\n        __verify_eval_loop_configuration(model, \"test\")\r\n    elif trainer.state.fn == TrainerFn.PREDICTING:\r\n        __verify_eval_loop_configuration(model, \"predict\")\r\n\r\n    __verify_configure_model_configuration(model)\r\n    __warn_dataloader_iter_limitations(model)", "language": "python", "code": "def _verify_loop_configurations(trainer: \"pl.Trainer\") -> None:\r\n    r\"\"\"Checks that the model is configured correctly before the run is started.\r\n\r\n    Args:\r\n        trainer: Lightning Trainer. Its `lightning_module` (the model) to check the configuration.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n\r\n    if trainer.state.fn is None:\r\n        raise ValueError(\"Unexpected: Trainer state fn must be set before validating loop configuration.\")\r\n    if trainer.state.fn == TrainerFn.FITTING:\r\n        __verify_train_val_loop_configuration(trainer, model)\r\n        __verify_manual_optimization_support(trainer, model)\r\n    elif trainer.state.fn == TrainerFn.VALIDATING:\r\n        __verify_eval_loop_configuration(model, \"val\")\r\n    elif trainer.state.fn == TrainerFn.TESTING:\r\n        __verify_eval_loop_configuration(model, \"test\")\r\n    elif trainer.state.fn == TrainerFn.PREDICTING:\r\n        __verify_eval_loop_configuration(model, \"predict\")\r\n\r\n    __verify_configure_model_configuration(model)\r\n    __warn_dataloader_iter_limitations(model)", "code_tokens": ["def", "_verify_loop_configurations", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Checks", "that", "the", "model", "is", "configured", "correctly", "before", "the", "run", "is", "started", ".", "Args", ":", "trainer", ":", "Lightning", "Trainer", ".", "Its", "`", "lightning_module", "`", "(", "the", "model", ")", "to", "check", "the", "configuration", ".", "\"", "\"", "\"", "model", "=", "trainer", ".", "lightning_module", "if", "trainer", ".", "state", ".", "fn", "is", "None", ":", "raise", "ValueError", "(", "\"", "Unexpected", ":", "Trainer", "state", "fn", "must", "be", "set", "before", "validating", "loop", "configuration", ".", "\"", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "__verify_train_val_loop_configuration", "(", "trainer", ",", "model", ")", "__verify_manual_optimization_support", "(", "trainer", ",", "model", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "\"", "val", "\"", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "TESTING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "\"", "test", "\"", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "PREDICTING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "\"", "predict", "\"", ")", "__verify_configure_model_configuration", "(", "model", ")", "__warn_dataloader_iter_limitations", "(", "model", ")"], "docstring": "r\"\"\"Checks that the model is configured correctly before the run is started.\r\n\r\n    Args:\r\n        trainer: Lightning Trainer. Its `lightning_module` (the model) to check the configuration.\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "checks", "that", "the", "model", "is", "configured", "correctly", "before", "the", "run", "is", "started", "args", "trainer", "lightning", "trainer", "its", "lightning_module", "the", "model", "to", "check", "the", "configuration"], "docstring_summary": "r\"\"\"Checks that the model is configured correctly before the run is started.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\configuration_validator.py", "partition": "valid", "function_type": "function", "start_line": 23, "end_line": 45, "hash": "9e2999a8b34d9cafceae53e2b305a83e", "complexity": 6, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "func_name": "__warn_dataloader_iter_limitations", "original_string": "def __warn_dataloader_iter_limitations(model: \"pl.LightningModule\") -> None:\r\n    \"\"\"Check if `dataloader_iter is enabled`.\"\"\"\r\n    if any(\r\n        is_param_in_hook_signature(step_fn, \"dataloader_iter\", explicit=True)\r\n        for step_fn in (model.training_step, model.validation_step, model.predict_step, model.test_step)\r\n        if step_fn is not None\r\n    ):\r\n        rank_zero_warn(\r\n            \"You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the\"\r\n            \" `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch\"\r\n            \" consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index.\"\r\n            \" This will also not work well with gradient accumulation. This feature is very experimental and subject to\"\r\n            \" change. Here be dragons.\",\r\n            category=PossibleUserWarning,\r\n        )", "language": "python", "code": "def __warn_dataloader_iter_limitations(model: \"pl.LightningModule\") -> None:\r\n    \"\"\"Check if `dataloader_iter is enabled`.\"\"\"\r\n    if any(\r\n        is_param_in_hook_signature(step_fn, \"dataloader_iter\", explicit=True)\r\n        for step_fn in (model.training_step, model.validation_step, model.predict_step, model.test_step)\r\n        if step_fn is not None\r\n    ):\r\n        rank_zero_warn(\r\n            \"You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the\"\r\n            \" `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch\"\r\n            \" consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index.\"\r\n            \" This will also not work well with gradient accumulation. This feature is very experimental and subject to\"\r\n            \" change. Here be dragons.\",\r\n            category=PossibleUserWarning,\r\n        )", "code_tokens": ["def", "__warn_dataloader_iter_limitations", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Check", "if", "`", "dataloader_iter", "is", "enabled", "`", ".", "\"", "\"", "\"", "if", "any", "(", "is_param_in_hook_signature", "(", "step_fn", ",", "\"", "dataloader_iter", "\"", ",", "explicit", "=", "True", ")", "for", "step_fn", "in", "(", "model", ".", "training_step", ",", "model", ".", "validation_step", ",", "model", ".", "predict_step", ",", "model", ".", "test_step", ")", "if", "step_fn", "is", "not", "None", ")", ":", "rank_zero_warn", "(", "\"", "You", "are", "using", "the", "`", "dataloader_iter", "`", "step", "flavor", ".", "If", "you", "consume", "the", "iterator", "more", "than", "once", "per", "step", ",", "the", "\"", "\"", "`", "batch_idx", "`", "argument", "in", "any", "hook", "that", "takes", "it", "will", "not", "match", "with", "the", "batch", "index", "of", "the", "last", "batch", "\"", "\"", "consumed", ".", "This", "might", "have", "unforeseen", "effects", "on", "callbacks", "or", "code", "that", "expects", "to", "get", "the", "correct", "index", ".", "\"", "\"", "This", "will", "also", "not", "work", "well", "with", "gradient", "accumulation", ".", "This", "feature", "is", "very", "experimental", "and", "subject", "to", "\"", "\"", "change", ".", "Here", "be", "dragons", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Check if `dataloader_iter is enabled`.", "docstring_tokens": ["check", "if", "dataloader_iter", "is", "enabled"], "docstring_summary": "Check if `dataloader_iter is enabled`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\configuration_validator.py", "partition": "valid", "function_type": "function", "start_line": 135, "end_line": 149, "hash": "d90326496f1c345cff76fd44bf3b6e29", "complexity": 4, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "fit", "original_string": "def fit(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> None:\r\n        r\"\"\"Runs the full optimization routine.\r\n\r\n        Args:\r\n            model: Model to fit.\r\n\r\n            train_dataloaders: An iterable or collection of iterables specifying training samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            val_dataloaders: An iterable or collection of iterables specifying validation samples.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of three special\r\n                keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\r\n                Otherwise, if there is no checkpoint file at the path, an exception is raised.\r\n\r\n                    - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - registry: the model will be downloaded from the Lightning Model Registry with following notations:\r\n\r\n                        - ``'registry'``: uses the latest/default version of default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")``\r\n                        - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\r\n                        - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\r\n                        - ``'registry:version:v2'``: uses the default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")`` and version 'v2'\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\r\n                2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\r\n                :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        \"\"\"\r\n        model = _maybe_unwrap_optimized(model)\r\n        self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(model, self.strategy)\r\n        self.state.fn = TrainerFn.FITTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.training = True\r\n        self.should_stop = False\r\n        call._call_and_handle_interrupt(\r\n            self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n        )", "language": "python", "code": "def fit(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> None:\r\n        r\"\"\"Runs the full optimization routine.\r\n\r\n        Args:\r\n            model: Model to fit.\r\n\r\n            train_dataloaders: An iterable or collection of iterables specifying training samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            val_dataloaders: An iterable or collection of iterables specifying validation samples.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of three special\r\n                keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\r\n                Otherwise, if there is no checkpoint file at the path, an exception is raised.\r\n\r\n                    - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - registry: the model will be downloaded from the Lightning Model Registry with following notations:\r\n\r\n                        - ``'registry'``: uses the latest/default version of default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")``\r\n                        - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\r\n                        - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\r\n                        - ``'registry:version:v2'``: uses the default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")`` and version 'v2'\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\r\n                2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\r\n                :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        \"\"\"\r\n        model = _maybe_unwrap_optimized(model)\r\n        self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(model, self.strategy)\r\n        self.state.fn = TrainerFn.FITTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.training = True\r\n        self.should_stop = False\r\n        call._call_and_handle_interrupt(\r\n            self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n        )", "code_tokens": ["def", "fit", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Runs", "the", "full", "optimization", "routine", ".", "Args", ":", "model", ":", "Model", "to", "fit", ".", "train_dataloaders", ":", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "training", "samples", ".", "Alternatively", ",", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "train_dataloader", "`", "hook", ".", "val_dataloaders", ":", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", ".", "datamodule", ":", "A", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "train_dataloader", "`", "hook", ".", "ckpt_path", ":", "Path", "/", "URL", "of", "the", "checkpoint", "from", "which", "training", "is", "resumed", ".", "Could", "also", "be", "one", "of", "three", "special", "keywords", "`", "`", "\"", "last", "\"", "`", "`", ",", "`", "`", "\"", "hpc", "\"", "`", "`", "and", "`", "`", "\"", "registry", "\"", "`", "`", ".", "Otherwise", ",", "if", "there", "is", "no", "checkpoint", "file", "at", "the", "path", ",", "an", "exception", "is", "raised", ".", "-", "best", ":", "the", "best", "model", "checkpoint", "from", "the", "previous", "`", "`", "trainer", ".", "fit", "`", "`", "call", "will", "be", "loaded", "-", "last", ":", "the", "last", "model", "checkpoint", "from", "the", "previous", "`", "`", "trainer", ".", "fit", "`", "`", "call", "will", "be", "loaded", "-", "registry", ":", "the", "model", "will", "be", "downloaded", "from", "the", "Lightning", "Model", "Registry", "with", "following", "notations", ":", "-", "`", "`", "'", "registry", "'", "`", "`", ":", "uses", "the", "latest", "/", "default", "version", "of", "default", "model", "set", "with", "`", "`", "Trainer", "(", ".", ".", ".", ",", "model_registry", "=", "\"", "my", "-", "model", "\"", ")", "`", "`", "-", "`", "`", "'", "registry", ":", "model", "-", "name", "'", "`", "`", ":", "uses", "the", "latest", "/", "default", "version", "of", "this", "model", "`", "model", "-", "name", "`", "-", "`", "`", "'", "registry", ":", "model", "-", "name", ":", "version", ":", "v2", "'", "`", "`", ":", "uses", "the", "specific", "version", "'", "v2", "'", "of", "the", "model", "`", "model", "-", "name", "`", "-", "`", "`", "'", "registry", ":", "version", ":", "v2", "'", "`", "`", ":", "uses", "the", "default", "model", "set", "with", "`", "`", "Trainer", "(", ".", ".", ".", ",", "model_registry", "=", "\"", "my", "-", "model", "\"", ")", "`", "`", "and", "version", "'", "v2", "'", "Raises", ":", "TypeError", ":", "If", "`", "`", "model", "`", "`", "is", "not", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "for", "torch", "version", "less", "than", "2", ".", "0", ".", "0", "and", "if", "`", "`", "model", "`", "`", "is", "not", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "or", ":", "class", ":", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "for", "torch", "versions", "greater", "than", "or", "equal", "to", "2", ".", "0", ".", "0", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "\"", "\"", "\"", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "model", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "FITTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "training", "=", "True", "self", ".", "should_stop", "=", "False", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_fit_impl", ",", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ",", "ckpt_path", ")"], "docstring": "r\"\"\"Runs the full optimization routine.\r\n\r\n        Args:\r\n            model: Model to fit.\r\n\r\n            train_dataloaders: An iterable or collection of iterables specifying training samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            val_dataloaders: An iterable or collection of iterables specifying validation samples.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of three special\r\n                keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\r\n                Otherwise, if there is no checkpoint file at the path, an exception is raised.\r\n\r\n                    - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - registry: the model will be downloaded from the Lightning Model Registry with following notations:\r\n\r\n                        - ``'registry'``: uses the latest/default version of default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")``\r\n                        - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\r\n                        - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\r\n                        - ``'registry:version:v2'``: uses the default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")`` and version 'v2'\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\r\n                2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\r\n                :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "runs", "the", "full", "optimization", "routine", "args", "model", "model", "to", "fit", "train_dataloaders", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "training", "samples", "alternatively", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "train_dataloader", "hook", "val_dataloaders", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", "datamodule", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "train_dataloader", "hook", "ckpt_path", "path", "url", "of", "the", "checkpoint", "from", "which", "training", "is", "resumed", "could", "also", "be", "one", "of", "three", "special", "keywords", "last", "hpc", "and", "registry", "otherwise", "if", "there", "is", "no", "checkpoint", "file", "at", "the", "path", "an", "exception", "is", "raised", "best", "the", "best", "model", "checkpoint", "from", "the", "previous", "trainer", "fit", "call", "will", "be", "loaded", "last", "the", "last", "model", "checkpoint", "from", "the", "previous", "trainer", "fit", "call", "will", "be", "loaded", "registry", "the", "model", "will", "be", "downloaded", "from", "the", "lightning", "model", "registry", "with", "following", "notations", "registry", "uses", "the", "latest", "default", "version", "of", "default", "model", "set", "with", "trainer", "model_registry", "my", "model", "registry", "model", "name", "uses", "the", "latest", "default", "version", "of", "this", "model", "model", "name", "registry", "model", "name", "version", "v2", "uses", "the", "specific", "version", "v2", "of", "the", "model", "model", "name", "registry", "version", "v2", "uses", "the", "default", "model", "set", "with", "trainer", "model_registry", "my", "model", "and", "version", "v2", "raises", "typeerror", "if", "model", "is", "not", "class", "lightning", "pytorch", "core", "lightningmodule", "for", "torch", "version", "less", "than", "2", "0", "0", "and", "if", "model", "is", "not", "class", "lightning", "pytorch", "core", "lightningmodule", "or", "class", "torch", "_dynamo", "optimizedmodule", "for", "torch", "versions", "greater", "than", "or", "equal", "to", "2", "0", "0", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders"], "docstring_summary": "r\"\"\"Runs the full optimization routine.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 521, "end_line": 576, "hash": "dbd8efe41ccafc6fc7ef23de09eceadd", "complexity": 1, "parameters": ["model", "train_dataloaders", "LightningDataModule]]", "val_dataloaders", "datamodule", "ckpt_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "validate", "original_string": "def validate(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the validation set.\r\n\r\n        Args:\r\n            model: The model to validate.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying validation samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to validate. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the validation results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\r\n            The length of the list corresponds to the number of validation dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.validate()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.VALIDATING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.validating = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "language": "python", "code": "def validate(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the validation set.\r\n\r\n        Args:\r\n            model: The model to validate.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying validation samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to validate. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the validation results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\r\n            The length of the list corresponds to the number of validation dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.validate()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.VALIDATING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.validating = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "code_tokens": ["def", "validate", "(", "self", ",", "model", ":", "Optional", "[", "\"", "pl", ".", "LightningModule", "\"", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", ")", "-", ">", "_EVALUATE_OUTPUT", ":", "r", "\"", "\"", "\"", "Perform", "one", "evaluation", "epoch", "over", "the", "validation", "set", ".", "Args", ":", "model", ":", "The", "model", "to", "validate", ".", "dataloaders", ":", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", ".", "Alternatively", ",", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "val_dataloader", "`", "hook", ".", "ckpt_path", ":", "Either", "`", "`", "\"", "best", "\"", "`", "`", ",", "`", "`", "\"", "last", "\"", "`", "`", ",", "`", "`", "\"", "hpc", "\"", "`", "`", ",", "`", "`", "\"", "registry", "\"", "`", "`", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "validate", ".", "If", "`", "`", "None", "`", "`", "and", "the", "model", "instance", "was", "passed", ",", "use", "the", "current", "weights", ".", "Otherwise", ",", "the", "best", "model", "checkpoint", "from", "the", "previous", "`", "`", "trainer", ".", "fit", "`", "`", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", ".", "verbose", ":", "If", "True", ",", "prints", "the", "validation", "results", ".", "datamodule", ":", "A", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "val_dataloader", "`", "hook", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "Returns", ":", "List", "of", "dictionaries", "with", "metrics", "logged", "during", "the", "validation", "phase", ",", "e", ".", "g", ".", ",", "in", "model", "-", "or", "callback", "hooks", "like", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "LightningModule", ".", "validation_step", "`", "etc", ".", "The", "length", "of", "the", "list", "corresponds", "to", "the", "number", "of", "validation", "dataloaders", "used", ".", "Raises", ":", "TypeError", ":", "If", "no", "`", "`", "model", "`", "`", "is", "passed", "and", "there", "was", "no", "`", "`", "LightningModule", "`", "`", "passed", "in", "the", "previous", "run", ".", "If", "`", "`", "model", "`", "`", "passed", "is", "not", "`", "LightningModule", "`", "or", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", ".", "MisconfigurationException", ":", "If", "both", "`", "`", "dataloaders", "`", "`", "and", "`", "`", "datamodule", "`", "`", "are", "passed", ".", "Pass", "only", "one", "of", "these", ".", "RuntimeError", ":", "If", "a", "compiled", "`", "`", "model", "`", "`", "is", "passed", "and", "the", "strategy", "is", "not", "supported", ".", "\"", "\"", "\"", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "validate", "(", ")", "`", "requires", "a", "`", "LightningModule", "`", "when", "it", "hasn", "'", "t", "been", "passed", "in", "a", "previous", "run", "\"", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "VALIDATING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "validating", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_validate_impl", ",", "model", ",", "dataloaders", ",", "ckpt_path", ",", "verbose", ",", "datamodule", ")"], "docstring": "r\"\"\"Perform one evaluation epoch over the validation set.\r\n\r\n        Args:\r\n            model: The model to validate.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying validation samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to validate. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the validation results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\r\n            The length of the list corresponds to the number of validation dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "perform", "one", "evaluation", "epoch", "over", "the", "validation", "set", "args", "model", "the", "model", "to", "validate", "dataloaders", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", "alternatively", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "val_dataloader", "hook", "ckpt_path", "either", "best", "last", "hpc", "registry", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "validate", "if", "none", "and", "the", "model", "instance", "was", "passed", "use", "the", "current", "weights", "otherwise", "the", "best", "model", "checkpoint", "from", "the", "previous", "trainer", "fit", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", "verbose", "if", "true", "prints", "the", "validation", "results", "datamodule", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "val_dataloader", "hook", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "returns", "list", "of", "dictionaries", "with", "metrics", "logged", "during", "the", "validation", "phase", "e", "g", "in", "model", "or", "callback", "hooks", "like", "meth", "lightning", "pytorch", "lightningmodule", "validation_step", "etc", "the", "length", "of", "the", "list", "corresponds", "to", "the", "number", "of", "validation", "dataloaders", "used", "raises", "typeerror", "if", "no", "model", "is", "passed", "and", "there", "was", "no", "lightningmodule", "passed", "in", "the", "previous", "run", "if", "model", "passed", "is", "not", "lightningmodule", "or", "torch", "_dynamo", "optimizedmodule", "misconfigurationexception", "if", "both", "dataloaders", "and", "datamodule", "are", "passed", "pass", "only", "one", "of", "these", "runtimeerror", "if", "a", "compiled", "model", "is", "passed", "and", "the", "strategy", "is", "not", "supported"], "docstring_summary": "r\"\"\"Perform one evaluation epoch over the validation set.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 618, "end_line": 679, "hash": "2f25172adcb1aba27991b82d5604d41b", "complexity": 3, "parameters": ["model", "dataloaders", "LightningDataModule]]", "ckpt_path", "verbose", "datamodule"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "test", "original_string": "def test(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\r\n        test set until you want to.\r\n\r\n        Args:\r\n            model: The model to test.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying test samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to test. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the test results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\r\n            The length of the list corresponds to the number of test dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.test()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.TESTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.testing = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "language": "python", "code": "def test(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\r\n        test set until you want to.\r\n\r\n        Args:\r\n            model: The model to test.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying test samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to test. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the test results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\r\n            The length of the list corresponds to the number of test dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.test()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.TESTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.testing = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "code_tokens": ["def", "test", "(", "self", ",", "model", ":", "Optional", "[", "\"", "pl", ".", "LightningModule", "\"", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", ")", "-", ">", "_EVALUATE_OUTPUT", ":", "r", "\"", "\"", "\"", "Perform", "one", "evaluation", "epoch", "over", "the", "test", "set", ".", "It", "'", "s", "separated", "from", "fit", "to", "make", "sure", "you", "never", "run", "on", "your", "test", "set", "until", "you", "want", "to", ".", "Args", ":", "model", ":", "The", "model", "to", "test", ".", "dataloaders", ":", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "test", "samples", ".", "Alternatively", ",", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "test_dataloader", "`", "hook", ".", "ckpt_path", ":", "Either", "`", "`", "\"", "best", "\"", "`", "`", ",", "`", "`", "\"", "last", "\"", "`", "`", ",", "`", "`", "\"", "hpc", "\"", "`", "`", ",", "`", "`", "\"", "registry", "\"", "`", "`", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "test", ".", "If", "`", "`", "None", "`", "`", "and", "the", "model", "instance", "was", "passed", ",", "use", "the", "current", "weights", ".", "Otherwise", ",", "the", "best", "model", "checkpoint", "from", "the", "previous", "`", "`", "trainer", ".", "fit", "`", "`", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", ".", "verbose", ":", "If", "True", ",", "prints", "the", "test", "results", ".", "datamodule", ":", "A", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "test_dataloader", "`", "hook", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "Returns", ":", "List", "of", "dictionaries", "with", "metrics", "logged", "during", "the", "test", "phase", ",", "e", ".", "g", ".", ",", "in", "model", "-", "or", "callback", "hooks", "like", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "LightningModule", ".", "test_step", "`", "etc", ".", "The", "length", "of", "the", "list", "corresponds", "to", "the", "number", "of", "test", "dataloaders", "used", ".", "Raises", ":", "TypeError", ":", "If", "no", "`", "`", "model", "`", "`", "is", "passed", "and", "there", "was", "no", "`", "`", "LightningModule", "`", "`", "passed", "in", "the", "previous", "run", ".", "If", "`", "`", "model", "`", "`", "passed", "is", "not", "`", "LightningModule", "`", "or", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", ".", "MisconfigurationException", ":", "If", "both", "`", "`", "dataloaders", "`", "`", "and", "`", "`", "datamodule", "`", "`", "are", "passed", ".", "Pass", "only", "one", "of", "these", ".", "RuntimeError", ":", "If", "a", "compiled", "`", "`", "model", "`", "`", "is", "passed", "and", "the", "strategy", "is", "not", "supported", ".", "\"", "\"", "\"", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "test", "(", ")", "`", "requires", "a", "`", "LightningModule", "`", "when", "it", "hasn", "'", "t", "been", "passed", "in", "a", "previous", "run", "\"", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "TESTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "testing", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_test_impl", ",", "model", ",", "dataloaders", ",", "ckpt_path", ",", "verbose", ",", "datamodule", ")"], "docstring": "r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\r\n        test set until you want to.\r\n\r\n        Args:\r\n            model: The model to test.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying test samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to test. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the test results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\r\n            The length of the list corresponds to the number of test dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "perform", "one", "evaluation", "epoch", "over", "the", "test", "set", "it", "s", "separated", "from", "fit", "to", "make", "sure", "you", "never", "run", "on", "your", "test", "set", "until", "you", "want", "to", "args", "model", "the", "model", "to", "test", "dataloaders", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "test", "samples", "alternatively", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "test_dataloader", "hook", "ckpt_path", "either", "best", "last", "hpc", "registry", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "test", "if", "none", "and", "the", "model", "instance", "was", "passed", "use", "the", "current", "weights", "otherwise", "the", "best", "model", "checkpoint", "from", "the", "previous", "trainer", "fit", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", "verbose", "if", "true", "prints", "the", "test", "results", "datamodule", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "test_dataloader", "hook", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "returns", "list", "of", "dictionaries", "with", "metrics", "logged", "during", "the", "test", "phase", "e", "g", "in", "model", "or", "callback", "hooks", "like", "meth", "lightning", "pytorch", "lightningmodule", "test_step", "etc", "the", "length", "of", "the", "list", "corresponds", "to", "the", "number", "of", "test", "dataloaders", "used", "raises", "typeerror", "if", "no", "model", "is", "passed", "and", "there", "was", "no", "lightningmodule", "passed", "in", "the", "previous", "run", "if", "model", "passed", "is", "not", "lightningmodule", "or", "torch", "_dynamo", "optimizedmodule", "misconfigurationexception", "if", "both", "dataloaders", "and", "datamodule", "are", "passed", "pass", "only", "one", "of", "these", "runtimeerror", "if", "a", "compiled", "model", "is", "passed", "and", "the", "strategy", "is", "not", "supported"], "docstring_summary": "r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 728, "end_line": 790, "hash": "7f03bba3c5b28b4c71096b3b99d42778", "complexity": 3, "parameters": ["model", "dataloaders", "LightningDataModule]]", "ckpt_path", "verbose", "datamodule"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "predict", "original_string": "def predict(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        return_predictions: Optional[bool] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> Optional[_PREDICT_OUTPUT]:\r\n        r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to\r\n        perform distributed and batched predictions. Logging is disabled in the predict hooks.\r\n\r\n        Args:\r\n            model: The model to predict with.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying predict samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            return_predictions: Whether to return predictions.\r\n                ``True`` by default except when an accelerator that spawns processes is used (not supported).\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to predict. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.predict()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.PREDICTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.predicting = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n        )", "language": "python", "code": "def predict(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        return_predictions: Optional[bool] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> Optional[_PREDICT_OUTPUT]:\r\n        r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to\r\n        perform distributed and batched predictions. Logging is disabled in the predict hooks.\r\n\r\n        Args:\r\n            model: The model to predict with.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying predict samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            return_predictions: Whether to return predictions.\r\n                ``True`` by default except when an accelerator that spawns processes is used (not supported).\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to predict. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            # do we still have a reference from a previous call?\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.predict()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.PREDICTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.predicting = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n        )", "code_tokens": ["def", "predict", "(", "self", ",", "model", ":", "Optional", "[", "\"", "pl", ".", "LightningModule", "\"", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", "return_predictions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "r", "\"", "\"", "\"", "Run", "inference", "on", "your", "data", ".", "This", "will", "call", "the", "model", "forward", "function", "to", "compute", "predictions", ".", "Useful", "to", "perform", "distributed", "and", "batched", "predictions", ".", "Logging", "is", "disabled", "in", "the", "predict", "hooks", ".", "Args", ":", "model", ":", "The", "model", "to", "predict", "with", ".", "dataloaders", ":", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "predict", "samples", ".", "Alternatively", ",", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "predict_dataloader", "`", "hook", ".", "datamodule", ":", "A", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "that", "defines", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "DataHooks", ".", "predict_dataloader", "`", "hook", ".", "return_predictions", ":", "Whether", "to", "return", "predictions", ".", "`", "`", "True", "`", "`", "by", "default", "except", "when", "an", "accelerator", "that", "spawns", "processes", "is", "used", "(", "not", "supported", ")", ".", "ckpt_path", ":", "Either", "`", "`", "\"", "best", "\"", "`", "`", ",", "`", "`", "\"", "last", "\"", "`", "`", ",", "`", "`", "\"", "hpc", "\"", "`", "`", ",", "`", "`", "\"", "registry", "\"", "`", "`", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "predict", ".", "If", "`", "`", "None", "`", "`", "and", "the", "model", "instance", "was", "passed", ",", "use", "the", "current", "weights", ".", "Otherwise", ",", "the", "best", "model", "checkpoint", "from", "the", "previous", "`", "`", "trainer", ".", "fit", "`", "`", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "Returns", ":", "Returns", "a", "list", "of", "dictionaries", ",", "one", "for", "each", "provided", "dataloader", "containing", "their", "respective", "predictions", ".", "Raises", ":", "TypeError", ":", "If", "no", "`", "`", "model", "`", "`", "is", "passed", "and", "there", "was", "no", "`", "`", "LightningModule", "`", "`", "passed", "in", "the", "previous", "run", ".", "If", "`", "`", "model", "`", "`", "passed", "is", "not", "`", "LightningModule", "`", "or", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", ".", "MisconfigurationException", ":", "If", "both", "`", "`", "dataloaders", "`", "`", "and", "`", "`", "datamodule", "`", "`", "are", "passed", ".", "Pass", "only", "one", "of", "these", ".", "RuntimeError", ":", "If", "a", "compiled", "`", "`", "model", "`", "`", "is", "passed", "and", "the", "strategy", "is", "not", "supported", ".", "See", ":", "ref", ":", "`", "Lightning", "inference", "section", "<", "deploy", "/", "production_basic", ":", "Predict", "step", "with", "your", "LightningModule", ">", "`", "for", "more", ".", "\"", "\"", "\"", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "predict", "(", ")", "`", "requires", "a", "`", "LightningModule", "`", "when", "it", "hasn", "'", "t", "been", "passed", "in", "a", "previous", "run", "\"", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "PREDICTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "predicting", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_predict_impl", ",", "model", ",", "dataloaders", ",", "datamodule", ",", "return_predictions", ",", "ckpt_path", ")"], "docstring": "r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to\r\n        perform distributed and batched predictions. Logging is disabled in the predict hooks.\r\n\r\n        Args:\r\n            model: The model to predict with.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying predict samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            return_predictions: Whether to return predictions.\r\n                ``True`` by default except when an accelerator that spawns processes is used (not supported).\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to predict. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "run", "inference", "on", "your", "data", "this", "will", "call", "the", "model", "forward", "function", "to", "compute", "predictions", "useful", "to", "perform", "distributed", "and", "batched", "predictions", "logging", "is", "disabled", "in", "the", "predict", "hooks", "args", "model", "the", "model", "to", "predict", "with", "dataloaders", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "predict", "samples", "alternatively", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "predict_dataloader", "hook", "datamodule", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "that", "defines", "the", "class", "lightning", "pytorch", "core", "hooks", "datahooks", "predict_dataloader", "hook", "return_predictions", "whether", "to", "return", "predictions", "true", "by", "default", "except", "when", "an", "accelerator", "that", "spawns", "processes", "is", "used", "not", "supported", "ckpt_path", "either", "best", "last", "hpc", "registry", "or", "path", "to", "the", "checkpoint", "you", "wish", "to", "predict", "if", "none", "and", "the", "model", "instance", "was", "passed", "use", "the", "current", "weights", "otherwise", "the", "best", "model", "checkpoint", "from", "the", "previous", "trainer", "fit", "call", "will", "be", "loaded", "if", "a", "checkpoint", "callback", "is", "configured", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "returns", "returns", "a", "list", "of", "dictionaries", "one", "for", "each", "provided", "dataloader", "containing", "their", "respective", "predictions", "raises", "typeerror", "if", "no", "model", "is", "passed", "and", "there", "was", "no", "lightningmodule", "passed", "in", "the", "previous", "run", "if", "model", "passed", "is", "not", "lightningmodule", "or", "torch", "_dynamo", "optimizedmodule", "misconfigurationexception", "if", "both", "dataloaders", "and", "datamodule", "are", "passed", "pass", "only", "one", "of", "these", "runtimeerror", "if", "a", "compiled", "model", "is", "passed", "and", "the", "strategy", "is", "not", "supported", "see", "ref", "lightning", "inference", "section", "deploy", "production_basic", "predict", "step", "with", "your", "lightningmodule", "for", "more"], "docstring_summary": "r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 839, "end_line": 902, "hash": "fb7096bb2aa2a7b0eb17a6c5d1cf696b", "complexity": 3, "parameters": ["model", "dataloaders", "LightningDataModule]]", "datamodule", "return_predictions", "ckpt_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "_teardown", "original_string": "def _teardown(self) -> None:\r\n        \"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\r\n        those are handled by :meth:`_call_teardown_hook`.\"\"\"\r\n        self.strategy.teardown()\r\n        loop = self._active_loop\r\n        # loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\r\n        if loop is not None:\r\n            loop.teardown()\r\n        self._logger_connector.teardown()\r\n        self._signal_connector.teardown()", "language": "python", "code": "def _teardown(self) -> None:\r\n        \"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\r\n        those are handled by :meth:`_call_teardown_hook`.\"\"\"\r\n        self.strategy.teardown()\r\n        loop = self._active_loop\r\n        # loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\r\n        if loop is not None:\r\n            loop.teardown()\r\n        self._logger_connector.teardown()\r\n        self._signal_connector.teardown()", "code_tokens": ["def", "_teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "is", "the", "Trainer", "'", "s", "internal", "teardown", ",", "unrelated", "to", "the", "`", "teardown", "`", "hooks", "in", "LightningModule", "and", "Callback", ";", "those", "are", "handled", "by", ":", "meth", ":", "`", "_call_teardown_hook", "`", ".", "\"", "\"", "\"", "self", ".", "strategy", ".", "teardown", "(", ")", "loop", "=", "self", ".", "_active_loop", "if", "loop", "is", "not", "None", ":", "loop", ".", "teardown", "(", ")", "self", ".", "_logger_connector", ".", "teardown", "(", ")", "self", ".", "_signal_connector", ".", "teardown", "(", ")"], "docstring": "This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\r\n        those are handled by :meth:`_call_teardown_hook`.", "docstring_tokens": ["this", "is", "the", "trainer", "s", "internal", "teardown", "unrelated", "to", "the", "teardown", "hooks", "in", "lightningmodule", "and", "callback", "those", "are", "handled", "by", "meth", "_call_teardown_hook"], "docstring_summary": "This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1047, "end_line": 1056, "hash": "59c2ade2b7d6df358ae409f845ec7869", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "init_module", "original_string": "def init_module(self, empty_init: Optional[bool] = None) -> Generator:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in the Trainer.\r\n\r\n        The parameters and tensors get created on the device and with the right data type right away without wasting\r\n        memory being allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        if is_overridden(\"model_sharded_context\", self.strategy, parent=Strategy):\r\n            # warning instead of error so that code changes are not required when changing strategies\r\n            # this is a limitation because processes are not expected to have been launched when this is called\r\n            rank_zero_warn(\r\n                f\"`trainer.init_module` cannot fully support proper instantiation of your model with the\"\r\n                f\" `{type(self.strategy).__name__}` strategy. Please instantiate your model inside the\"\r\n                f\"`LightningModule.configure_model` hook instead\",\r\n                # ideally we would check if `configure_model` is already overridden, but we don't have a reliable\r\n                # reference to the model yet\r\n                category=PossibleUserWarning,\r\n            )\r\n        with self.strategy.tensor_init_context(empty_init=empty_init):\r\n            yield", "language": "python", "code": "def init_module(self, empty_init: Optional[bool] = None) -> Generator:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in the Trainer.\r\n\r\n        The parameters and tensors get created on the device and with the right data type right away without wasting\r\n        memory being allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        if is_overridden(\"model_sharded_context\", self.strategy, parent=Strategy):\r\n            # warning instead of error so that code changes are not required when changing strategies\r\n            # this is a limitation because processes are not expected to have been launched when this is called\r\n            rank_zero_warn(\r\n                f\"`trainer.init_module` cannot fully support proper instantiation of your model with the\"\r\n                f\" `{type(self.strategy).__name__}` strategy. Please instantiate your model inside the\"\r\n                f\"`LightningModule.configure_model` hook instead\",\r\n                # ideally we would check if `configure_model` is already overridden, but we don't have a reliable\r\n                # reference to the model yet\r\n                category=PossibleUserWarning,\r\n            )\r\n        with self.strategy.tensor_init_context(empty_init=empty_init):\r\n            yield", "code_tokens": ["def", "init_module", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have", "the", "right", "data", "type", "depending", "on", "the", "precision", "setting", "in", "the", "Trainer", ".", "The", "parameters", "and", "tensors", "get", "created", "on", "the", "device", "and", "with", "the", "right", "data", "type", "right", "away", "without", "wasting", "memory", "being", "allocated", "unnecessarily", ".", "Args", ":", "empty_init", ":", "Whether", "to", "initialize", "the", "model", "with", "empty", "weights", "(", "uninitialized", "memory", ")", ".", "If", "`", "`", "None", "`", "`", ",", "the", "strategy", "will", "decide", ".", "Some", "strategies", "may", "not", "support", "all", "options", ".", "Set", "this", "to", "`", "`", "True", "`", "`", "if", "you", "are", "loading", "a", "checkpoint", "into", "a", "large", "model", ".", "\"", "\"", "\"", "if", "is_overridden", "(", "\"", "model_sharded_context", "\"", ",", "self", ".", "strategy", ",", "parent", "=", "Strategy", ")", ":", "rank_zero_warn", "(", "f", "\"", "`", "trainer", ".", "init_module", "`", "cannot", "fully", "support", "proper", "instantiation", "of", "your", "model", "with", "the", "\"", "f", "\"", "`", "{", "type", "(", "self", ".", "strategy", ")", ".", "__name__", "}", "`", "strategy", ".", "Please", "instantiate", "your", "model", "inside", "the", "\"", "f", "\"", "`", "LightningModule", ".", "configure_model", "`", "hook", "instead", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "with", "self", ".", "strategy", ".", "tensor_init_context", "(", "empty_init", "=", "empty_init", ")", ":", "yield"], "docstring": "Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in the Trainer.\r\n\r\n        The parameters and tensors get created on the device and with the right data type right away without wasting\r\n        memory being allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.", "docstring_tokens": ["tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have", "the", "right", "data", "type", "depending", "on", "the", "precision", "setting", "in", "the", "trainer", "the", "parameters", "and", "tensors", "get", "created", "on", "the", "device", "and", "with", "the", "right", "data", "type", "right", "away", "without", "wasting", "memory", "being", "allocated", "unnecessarily", "args", "empty_init", "whether", "to", "initialize", "the", "model", "with", "empty", "weights", "uninitialized", "memory", "if", "none", "the", "strategy", "will", "decide", "some", "strategies", "may", "not", "support", "all", "options", "set", "this", "to", "true", "if", "you", "are", "loading", "a", "checkpoint", "into", "a", "large", "model"], "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1120, "end_line": 1145, "hash": "713707d00b3cb41d0a153a7309212f0c", "complexity": 3, "parameters": ["empty_init"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "print", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Print", "something", "only", "on", "the", "first", "process", ".", "If", "running", "on", "multiple", "machines", ",", "it", "will", "print", "from", "the", "first", "process", "in", "each", "machine", ".", "Arguments", "passed", "to", "this", "method", "are", "forwarded", "to", "the", "Python", "built", "-", "in", ":", "func", ":", "`", "print", "`", "function", ".", "\"", "\"", "\"", "if", "self", ".", "local_rank", "=", "=", "0", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.", "docstring_tokens": ["print", "something", "only", "on", "the", "first", "process", "if", "running", "on", "multiple", "machines", "it", "will", "print", "from", "the", "first", "process", "in", "each", "machine", "arguments", "passed", "to", "this", "method", "are", "forwarded", "to", "the", "python", "built", "in", "func", "print", "function"], "docstring_summary": "Print something only on the first process. If running on multiple machines, it will print from the first", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1147, "end_line": 1155, "hash": "ff9696e07f80fb3dff2f761ac6b1a32d", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "device_ids", "original_string": "def device_ids(self) -> list[int]:\r\n        \"\"\"List of device indexes per node.\"\"\"\r\n        devices = (\r\n            self.strategy.parallel_devices\r\n            if isinstance(self.strategy, ParallelStrategy)\r\n            else [self.strategy.root_device]\r\n        )\r\n        assert devices is not None\r\n        device_ids = []\r\n        for idx, device in enumerate(devices):\r\n            if isinstance(device, torch.device):\r\n                device_ids.append(device.index or idx)\r\n            elif isinstance(device, int):\r\n                device_ids.append(device)\r\n        return device_ids", "language": "python", "code": "def device_ids(self) -> list[int]:\r\n        \"\"\"List of device indexes per node.\"\"\"\r\n        devices = (\r\n            self.strategy.parallel_devices\r\n            if isinstance(self.strategy, ParallelStrategy)\r\n            else [self.strategy.root_device]\r\n        )\r\n        assert devices is not None\r\n        device_ids = []\r\n        for idx, device in enumerate(devices):\r\n            if isinstance(device, torch.device):\r\n                device_ids.append(device.index or idx)\r\n            elif isinstance(device, int):\r\n                device_ids.append(device)\r\n        return device_ids", "code_tokens": ["def", "device_ids", "(", "self", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "List", "of", "device", "indexes", "per", "node", ".", "\"", "\"", "\"", "devices", "=", "(", "self", ".", "strategy", ".", "parallel_devices", "if", "isinstance", "(", "self", ".", "strategy", ",", "ParallelStrategy", ")", "else", "[", "self", ".", "strategy", ".", "root_device", "]", ")", "assert", "devices", "is", "not", "None", "device_ids", "=", "[", "]", "for", "idx", ",", "device", "in", "enumerate", "(", "devices", ")", ":", "if", "isinstance", "(", "device", ",", "torch", ".", "device", ")", ":", "device_ids", ".", "append", "(", "device", ".", "index", "or", "idx", ")", "elif", "isinstance", "(", "device", ",", "int", ")", ":", "device_ids", ".", "append", "(", "device", ")", "return", "device_ids"], "docstring": "List of device indexes per node.", "docstring_tokens": ["list", "of", "device", "indexes", "per", "node"], "docstring_summary": "List of device indexes per node.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1198, "end_line": 1212, "hash": "6549af5065dcf07dd453c4d18a4b4f01", "complexity": 6, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "model", "original_string": "def model(self) -> Optional[torch.nn.Module]:\r\n        \"\"\"The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\r\n\r\n        To access the pure LightningModule, use\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.\r\n\r\n        \"\"\"\r\n        return self.strategy.model", "language": "python", "code": "def model(self) -> Optional[torch.nn.Module]:\r\n        \"\"\"The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\r\n\r\n        To access the pure LightningModule, use\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.\r\n\r\n        \"\"\"\r\n        return self.strategy.model", "code_tokens": ["def", "model", "(", "self", ")", "-", ">", "Optional", "[", "torch", ".", "nn", ".", "Module", "]", ":", "\"", "\"", "\"", "The", "LightningModule", ",", "but", "possibly", "wrapped", "into", "DataParallel", "or", "DistributedDataParallel", ".", "To", "access", "the", "pure", "LightningModule", ",", "use", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "lightning_module", "`", "instead", ".", "\"", "\"", "\"", "return", "self", ".", "strategy", ".", "model"], "docstring": "The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\r\n\r\n        To access the pure LightningModule, use\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.", "docstring_tokens": ["the", "lightningmodule", "but", "possibly", "wrapped", "into", "dataparallel", "or", "distributeddataparallel", "to", "access", "the", "pure", "lightningmodule", "use", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "lightning_module", "instead"], "docstring_summary": "The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1245, "end_line": 1252, "hash": "3ae8ea82d80a3314bcbca7827497a80d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"The directory for the current experiment. Use this to save images to, etc...\r\n\r\n        .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n         .. code-block:: python\r\n\r\n             def training_step(self, batch, batch_idx):\r\n                 img = ...\r\n                 save_img(img, self.trainer.log_dir)\r\n\r\n        \"\"\"\r\n        if len(self.loggers) > 0:\r\n            if not isinstance(self.loggers[0], (TensorBoardLogger, CSVLogger)):\r\n                dirpath = self.loggers[0].save_dir\r\n            else:\r\n                dirpath = self.loggers[0].log_dir\r\n        else:\r\n            dirpath = self.default_root_dir\r\n\r\n        dirpath = self.strategy.broadcast(dirpath)\r\n        return dirpath", "language": "python", "code": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"The directory for the current experiment. Use this to save images to, etc...\r\n\r\n        .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n         .. code-block:: python\r\n\r\n             def training_step(self, batch, batch_idx):\r\n                 img = ...\r\n                 save_img(img, self.trainer.log_dir)\r\n\r\n        \"\"\"\r\n        if len(self.loggers) > 0:\r\n            if not isinstance(self.loggers[0], (TensorBoardLogger, CSVLogger)):\r\n                dirpath = self.loggers[0].save_dir\r\n            else:\r\n                dirpath = self.loggers[0].log_dir\r\n        else:\r\n            dirpath = self.default_root_dir\r\n\r\n        dirpath = self.strategy.broadcast(dirpath)\r\n        return dirpath", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "The", "directory", "for", "the", "current", "experiment", ".", "Use", "this", "to", "save", "images", "to", ",", "etc", ".", ".", ".", ".", ".", "note", ":", ":", "You", "must", "call", "this", "on", "all", "processes", ".", "Failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "img", "=", ".", ".", ".", "save_img", "(", "img", ",", "self", ".", "trainer", ".", "log_dir", ")", "\"", "\"", "\"", "if", "len", "(", "self", ".", "loggers", ")", ">", "0", ":", "if", "not", "isinstance", "(", "self", ".", "loggers", "[", "0", "]", ",", "(", "TensorBoardLogger", ",", "CSVLogger", ")", ")", ":", "dirpath", "=", "self", ".", "loggers", "[", "0", "]", ".", "save_dir", "else", ":", "dirpath", "=", "self", ".", "loggers", "[", "0", "]", ".", "log_dir", "else", ":", "dirpath", "=", "self", ".", "default_root_dir", "dirpath", "=", "self", ".", "strategy", ".", "broadcast", "(", "dirpath", ")", "return", "dirpath"], "docstring": "The directory for the current experiment. Use this to save images to, etc...\r\n\r\n        .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n         .. code-block:: python\r\n\r\n             def training_step(self, batch, batch_idx):\r\n                 img = ...\r\n                 save_img(img, self.trainer.log_dir)", "docstring_tokens": ["the", "directory", "for", "the", "current", "experiment", "use", "this", "to", "save", "images", "to", "etc", "note", "you", "must", "call", "this", "on", "all", "processes", "failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever", "code", "block", "python", "def", "training_step", "self", "batch", "batch_idx", "img", "save_img", "img", "self", "trainer", "log_dir"], "docstring_summary": "The directory for the current experiment. Use this to save images to, etc...", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1259, "end_line": 1280, "hash": "90237c305209b8c4ed0e102d61fc06e4", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "is_global_zero", "original_string": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this process is the global zero in multi-node training.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                if self.trainer.is_global_zero:\r\n                    print(\"in node 0, accelerator 0\")\r\n\r\n        \"\"\"\r\n        return self.strategy.is_global_zero", "language": "python", "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this process is the global zero in multi-node training.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                if self.trainer.is_global_zero:\r\n                    print(\"in node 0, accelerator 0\")\r\n\r\n        \"\"\"\r\n        return self.strategy.is_global_zero", "code_tokens": ["def", "is_global_zero", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Whether", "this", "process", "is", "the", "global", "zero", "in", "multi", "-", "node", "training", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "if", "self", ".", "trainer", ".", "is_global_zero", ":", "print", "(", "\"", "in", "node", "0", ",", "accelerator", "0", "\"", ")", "\"", "\"", "\"", "return", "self", ".", "strategy", ".", "is_global_zero"], "docstring": "Whether this process is the global zero in multi-node training.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                if self.trainer.is_global_zero:\r\n                    print(\"in node 0, accelerator 0\")", "docstring_tokens": ["whether", "this", "process", "is", "the", "global", "zero", "in", "multi", "node", "training", "code", "block", "python", "def", "training_step", "self", "batch", "batch_idx", "if", "self", "trainer", "is_global_zero", "print", "in", "node", "0", "accelerator", "0"], "docstring_summary": "Whether this process is the global zero in multi-node training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1283, "end_line": 1293, "hash": "09104bbf72aa8c66c9c9a72f38ec98b5", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "enable_validation", "original_string": "def enable_validation(self) -> bool:\r\n        \"\"\"Check if we should run validation during training.\"\"\"\r\n        return (\r\n            self.fit_loop.epoch_loop.val_loop._data_source.is_defined()\r\n            and is_overridden(\"validation_step\", self.lightning_module)\r\n            and self.limit_val_batches > 0\r\n        )", "language": "python", "code": "def enable_validation(self) -> bool:\r\n        \"\"\"Check if we should run validation during training.\"\"\"\r\n        return (\r\n            self.fit_loop.epoch_loop.val_loop._data_source.is_defined()\r\n            and is_overridden(\"validation_step\", self.lightning_module)\r\n            and self.limit_val_batches > 0\r\n        )", "code_tokens": ["def", "enable_validation", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "we", "should", "run", "validation", "during", "training", ".", "\"", "\"", "\"", "return", "(", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_data_source", ".", "is_defined", "(", ")", "and", "is_overridden", "(", "\"", "validation_step", "\"", ",", "self", ".", "lightning_module", ")", "and", "self", ".", "limit_val_batches", ">", "0", ")"], "docstring": "Check if we should run validation during training.", "docstring_tokens": ["check", "if", "we", "should", "run", "validation", "during", "training"], "docstring_summary": "Check if we should run validation during training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1302, "end_line": 1308, "hash": "c9e1791896d198ef928aeba7b40fb047", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "default_root_dir", "original_string": "def default_root_dir(self) -> str:\r\n        \"\"\"The default location to save artifacts of loggers, checkpoints etc.\r\n\r\n        It is used as a fallback if logger or checkpoint callback do not define specific save paths.\r\n\r\n        \"\"\"\r\n        if _is_local_file_protocol(self._default_root_dir):\r\n            return os.path.normpath(os.path.expanduser(self._default_root_dir))\r\n        return self._default_root_dir", "language": "python", "code": "def default_root_dir(self) -> str:\r\n        \"\"\"The default location to save artifacts of loggers, checkpoints etc.\r\n\r\n        It is used as a fallback if logger or checkpoint callback do not define specific save paths.\r\n\r\n        \"\"\"\r\n        if _is_local_file_protocol(self._default_root_dir):\r\n            return os.path.normpath(os.path.expanduser(self._default_root_dir))\r\n        return self._default_root_dir", "code_tokens": ["def", "default_root_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "default", "location", "to", "save", "artifacts", "of", "loggers", ",", "checkpoints", "etc", ".", "It", "is", "used", "as", "a", "fallback", "if", "logger", "or", "checkpoint", "callback", "do", "not", "define", "specific", "save", "paths", ".", "\"", "\"", "\"", "if", "_is_local_file_protocol", "(", "self", ".", "_default_root_dir", ")", ":", "return", "os", ".", "path", ".", "normpath", "(", "os", ".", "path", ".", "expanduser", "(", "self", ".", "_default_root_dir", ")", ")", "return", "self", ".", "_default_root_dir"], "docstring": "The default location to save artifacts of loggers, checkpoints etc.\r\n\r\n        It is used as a fallback if logger or checkpoint callback do not define specific save paths.", "docstring_tokens": ["the", "default", "location", "to", "save", "artifacts", "of", "loggers", "checkpoints", "etc", "it", "is", "used", "as", "a", "fallback", "if", "logger", "or", "checkpoint", "callback", "do", "not", "define", "specific", "save", "paths"], "docstring_summary": "The default location to save artifacts of loggers, checkpoints etc.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1311, "end_line": 1319, "hash": "eab8895c35cae6103578c8fb707d5603", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "early_stopping_callback", "original_string": "def early_stopping_callback(self) -> Optional[EarlyStopping]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.early_stopping_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "language": "python", "code": "def early_stopping_callback(self) -> Optional[EarlyStopping]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.early_stopping_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "code_tokens": ["def", "early_stopping_callback", "(", "self", ")", "-", ">", "Optional", "[", "EarlyStopping", "]", ":", "\"", "\"", "\"", "The", "first", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "early_stopping", ".", "EarlyStopping", "`", "callback", "in", "the", "Trainer", ".", "callbacks", "list", ",", "or", "`", "`", "None", "`", "`", "if", "it", "doesn", "'", "t", "exist", ".", "\"", "\"", "\"", "callbacks", "=", "self", ".", "early_stopping_callbacks", "return", "callbacks", "[", "0", "]", "if", "len", "(", "callbacks", ")", ">", "0", "else", "None"], "docstring": "The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.", "docstring_tokens": ["the", "first", "class", "lightning", "pytorch", "callbacks", "early_stopping", "earlystopping", "callback", "in", "the", "trainer", "callbacks", "list", "or", "none", "if", "it", "doesn", "t", "exist"], "docstring_summary": "The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1322, "end_line": 1326, "hash": "a210340c788fb989de6a5d70db834c1c", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "early_stopping_callbacks", "original_string": "def early_stopping_callbacks(self) -> list[EarlyStopping]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\r\n        Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, EarlyStopping)]", "language": "python", "code": "def early_stopping_callbacks(self) -> list[EarlyStopping]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\r\n        Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, EarlyStopping)]", "code_tokens": ["def", "early_stopping_callbacks", "(", "self", ")", "-", ">", "list", "[", "EarlyStopping", "]", ":", "\"", "\"", "\"", "A", "list", "of", "all", "instances", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "early_stopping", ".", "EarlyStopping", "`", "found", "in", "the", "Trainer", ".", "callbacks", "list", ".", "\"", "\"", "\"", "return", "[", "c", "for", "c", "in", "self", ".", "callbacks", "if", "isinstance", "(", "c", ",", "EarlyStopping", ")", "]"], "docstring": "A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\r\n        Trainer.callbacks list.", "docstring_tokens": ["a", "list", "of", "all", "instances", "of", "class", "lightning", "pytorch", "callbacks", "early_stopping", "earlystopping", "found", "in", "the", "trainer", "callbacks", "list"], "docstring_summary": "A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1329, "end_line": 1332, "hash": "515e4ca67c7276addf9a1066f7bc3174", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "checkpoint_callback", "original_string": "def checkpoint_callback(self) -> Optional[Checkpoint]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.checkpoint_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "language": "python", "code": "def checkpoint_callback(self) -> Optional[Checkpoint]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.checkpoint_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "code_tokens": ["def", "checkpoint_callback", "(", "self", ")", "-", ">", "Optional", "[", "Checkpoint", "]", ":", "\"", "\"", "\"", "The", "first", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "model_checkpoint", ".", "ModelCheckpoint", "`", "callback", "in", "the", "Trainer", ".", "callbacks", "list", ",", "or", "`", "`", "None", "`", "`", "if", "it", "doesn", "'", "t", "exist", ".", "\"", "\"", "\"", "callbacks", "=", "self", ".", "checkpoint_callbacks", "return", "callbacks", "[", "0", "]", "if", "len", "(", "callbacks", ")", ">", "0", "else", "None"], "docstring": "The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.", "docstring_tokens": ["the", "first", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "callback", "in", "the", "trainer", "callbacks", "list", "or", "none", "if", "it", "doesn", "t", "exist"], "docstring_summary": "The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1335, "end_line": 1339, "hash": "c3b297ad2a19e922696c2514615fea0a", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "checkpoint_callbacks", "original_string": "def checkpoint_callbacks(self) -> list[Checkpoint]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\r\n        the Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, Checkpoint)]", "language": "python", "code": "def checkpoint_callbacks(self) -> list[Checkpoint]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\r\n        the Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, Checkpoint)]", "code_tokens": ["def", "checkpoint_callbacks", "(", "self", ")", "-", ">", "list", "[", "Checkpoint", "]", ":", "\"", "\"", "\"", "A", "list", "of", "all", "instances", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "model_checkpoint", ".", "ModelCheckpoint", "`", "found", "in", "the", "Trainer", ".", "callbacks", "list", ".", "\"", "\"", "\"", "return", "[", "c", "for", "c", "in", "self", ".", "callbacks", "if", "isinstance", "(", "c", ",", "Checkpoint", ")", "]"], "docstring": "A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\r\n        the Trainer.callbacks list.", "docstring_tokens": ["a", "list", "of", "all", "instances", "of", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "found", "in", "the", "trainer", "callbacks", "list"], "docstring_summary": "A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1342, "end_line": 1345, "hash": "8bac284ef2bc9ec37da499844d1ecbc0", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "progress_bar_callback", "original_string": "def progress_bar_callback(self) -> Optional[ProgressBar]:\r\n        \"\"\"An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\r\n        Trainer.callbacks list, or ``None`` if one doesn't exist.\"\"\"\r\n        for c in self.callbacks:\r\n            if isinstance(c, ProgressBar):\r\n                return c\r\n        return None", "language": "python", "code": "def progress_bar_callback(self) -> Optional[ProgressBar]:\r\n        \"\"\"An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\r\n        Trainer.callbacks list, or ``None`` if one doesn't exist.\"\"\"\r\n        for c in self.callbacks:\r\n            if isinstance(c, ProgressBar):\r\n                return c\r\n        return None", "code_tokens": ["def", "progress_bar_callback", "(", "self", ")", "-", ">", "Optional", "[", "ProgressBar", "]", ":", "\"", "\"", "\"", "An", "instance", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "progress", ".", "progress_bar", ".", "ProgressBar", "`", "found", "in", "the", "Trainer", ".", "callbacks", "list", ",", "or", "`", "`", "None", "`", "`", "if", "one", "doesn", "'", "t", "exist", ".", "\"", "\"", "\"", "for", "c", "in", "self", ".", "callbacks", ":", "if", "isinstance", "(", "c", ",", "ProgressBar", ")", ":", "return", "c", "return", "None"], "docstring": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\r\n        Trainer.callbacks list, or ``None`` if one doesn't exist.", "docstring_tokens": ["an", "instance", "of", "class", "lightning", "pytorch", "callbacks", "progress", "progress_bar", "progressbar", "found", "in", "the", "trainer", "callbacks", "list", "or", "none", "if", "one", "doesn", "t", "exist"], "docstring_summary": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1348, "end_line": 1354, "hash": "5c3107ec03eb17fc4b5d9b2876151205", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "ckpt_path", "original_string": "def ckpt_path(self) -> Optional[_PATH]:\r\n        \"\"\"Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\r\n\r\n        ``None`` otherwise.\r\n\r\n        \"\"\"\r\n        return self._checkpoint_connector._ckpt_path", "language": "python", "code": "def ckpt_path(self) -> Optional[_PATH]:\r\n        \"\"\"Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\r\n\r\n        ``None`` otherwise.\r\n\r\n        \"\"\"\r\n        return self._checkpoint_connector._ckpt_path", "code_tokens": ["def", "ckpt_path", "(", "self", ")", "-", ">", "Optional", "[", "_PATH", "]", ":", "\"", "\"", "\"", "Set", "to", "the", "path", "/", "URL", "of", "a", "checkpoint", "loaded", "via", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "fit", "`", ",", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "validate", "`", ",", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "test", "`", ",", "or", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "predict", "`", ".", "`", "`", "None", "`", "`", "otherwise", ".", "\"", "\"", "\"", "return", "self", ".", "_checkpoint_connector", ".", "_ckpt_path"], "docstring": "Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\r\n\r\n        ``None`` otherwise.", "docstring_tokens": ["set", "to", "the", "path", "url", "of", "a", "checkpoint", "loaded", "via", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "fit", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "validate", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "test", "or", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "predict", "none", "otherwise"], "docstring_summary": "Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1357, "end_line": 1366, "hash": "6d4e90327489ce6f40dbebb4ec65cb00", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "ckpt_path", "original_string": "def ckpt_path(self, ckpt_path: Optional[_PATH]) -> None:\r\n        \"\"\"Allows you to manage which checkpoint is loaded statefully.\r\n\r\n        .. code-block:: python\r\n\r\n            trainer = Trainer()\r\n            trainer.ckpt_path = \"my/checkpoint/file.ckpt\"\r\n            trainer.fit(model)\r\n            ...\r\n\r\n            # you will be in charge of resetting this\r\n            trainer.ckpt_path = None\r\n            trainer.test(model)\r\n\r\n        \"\"\"\r\n        self._checkpoint_connector._ckpt_path = ckpt_path\r\n        self._checkpoint_connector._user_managed = bool(ckpt_path)", "language": "python", "code": "def ckpt_path(self, ckpt_path: Optional[_PATH]) -> None:\r\n        \"\"\"Allows you to manage which checkpoint is loaded statefully.\r\n\r\n        .. code-block:: python\r\n\r\n            trainer = Trainer()\r\n            trainer.ckpt_path = \"my/checkpoint/file.ckpt\"\r\n            trainer.fit(model)\r\n            ...\r\n\r\n            # you will be in charge of resetting this\r\n            trainer.ckpt_path = None\r\n            trainer.test(model)\r\n\r\n        \"\"\"\r\n        self._checkpoint_connector._ckpt_path = ckpt_path\r\n        self._checkpoint_connector._user_managed = bool(ckpt_path)", "code_tokens": ["def", "ckpt_path", "(", "self", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Allows", "you", "to", "manage", "which", "checkpoint", "is", "loaded", "statefully", ".", ".", ".", "code", "-", "block", ":", ":", "python", "trainer", "=", "Trainer", "(", ")", "trainer", ".", "ckpt_path", "=", "\"", "my", "/", "checkpoint", "/", "file", ".", "ckpt", "\"", "trainer", ".", "fit", "(", "model", ")", ".", ".", ".", "trainer", ".", "ckpt_path", "=", "None", "trainer", ".", "test", "(", "model", ")", "\"", "\"", "\"", "self", ".", "_checkpoint_connector", ".", "_ckpt_path", "=", "ckpt_path", "self", ".", "_checkpoint_connector", ".", "_user_managed", "=", "bool", "(", "ckpt_path", ")"], "docstring": "Allows you to manage which checkpoint is loaded statefully.\r\n\r\n        .. code-block:: python\r\n\r\n            trainer = Trainer()\r\n            trainer.ckpt_path = \"my/checkpoint/file.ckpt\"\r\n            trainer.fit(model)\r\n            ...\r\n\r\n            # you will be in charge of resetting this\r\n            trainer.ckpt_path = None\r\n            trainer.test(model)", "docstring_tokens": ["allows", "you", "to", "manage", "which", "checkpoint", "is", "loaded", "statefully", "code", "block", "python", "trainer", "trainer", "trainer", "ckpt_path", "my", "checkpoint", "file", "ckpt", "trainer", "fit", "model", "you", "will", "be", "in", "charge", "of", "resetting", "this", "trainer", "ckpt_path", "none", "trainer", "test", "model"], "docstring_summary": "Allows you to manage which checkpoint is loaded statefully.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1369, "end_line": 1385, "hash": "a1a25fb0d6d4e9e5f9cf5724cf256d4d", "complexity": 1, "parameters": ["ckpt_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self, filepath: _PATH, weights_only: bool = False, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        r\"\"\"Runs routine to create a checkpoint.\r\n\r\n        This method needs to be called on all processes in case the selected strategy is handling distributed\r\n        checkpointing.\r\n\r\n        Args:\r\n            filepath: Path where checkpoint is saved.\r\n            weights_only: If ``True``, will only save the model weights.\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        Raises:\r\n            AttributeError:\r\n                If the model is not attached to the Trainer before calling this method.\r\n\r\n        \"\"\"\r\n        if self.model is None:\r\n            raise AttributeError(\r\n                \"Saving a checkpoint is only possible if a model is attached to the Trainer. Did you call\"\r\n                \" `Trainer.save_checkpoint()` before calling `Trainer.{fit,validate,test,predict}`?\"\r\n            )\r\n        with self.profiler.profile(\"save_checkpoint\"):\r\n            checkpoint = self._checkpoint_connector.dump_checkpoint(weights_only)\r\n            self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\r\n            self.strategy.barrier(\"Trainer.save_checkpoint\")", "language": "python", "code": "def save_checkpoint(\r\n        self, filepath: _PATH, weights_only: bool = False, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        r\"\"\"Runs routine to create a checkpoint.\r\n\r\n        This method needs to be called on all processes in case the selected strategy is handling distributed\r\n        checkpointing.\r\n\r\n        Args:\r\n            filepath: Path where checkpoint is saved.\r\n            weights_only: If ``True``, will only save the model weights.\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        Raises:\r\n            AttributeError:\r\n                If the model is not attached to the Trainer before calling this method.\r\n\r\n        \"\"\"\r\n        if self.model is None:\r\n            raise AttributeError(\r\n                \"Saving a checkpoint is only possible if a model is attached to the Trainer. Did you call\"\r\n                \" `Trainer.save_checkpoint()` before calling `Trainer.{fit,validate,test,predict}`?\"\r\n            )\r\n        with self.profiler.profile(\"save_checkpoint\"):\r\n            checkpoint = self._checkpoint_connector.dump_checkpoint(weights_only)\r\n            self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\r\n            self.strategy.barrier(\"Trainer.save_checkpoint\")", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ",", "weights_only", ":", "bool", "=", "False", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Runs", "routine", "to", "create", "a", "checkpoint", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", "in", "case", "the", "selected", "strategy", "is", "handling", "distributed", "checkpointing", ".", "Args", ":", "filepath", ":", "Path", "where", "checkpoint", "is", "saved", ".", "weights_only", ":", "If", "`", "`", "True", "`", "`", ",", "will", "only", "save", "the", "model", "weights", ".", "storage_options", ":", "parameter", "for", "how", "to", "save", "to", "storage", ",", "passed", "to", "`", "`", "CheckpointIO", "`", "`", "plugin", "Raises", ":", "AttributeError", ":", "If", "the", "model", "is", "not", "attached", "to", "the", "Trainer", "before", "calling", "this", "method", ".", "\"", "\"", "\"", "if", "self", ".", "model", "is", "None", ":", "raise", "AttributeError", "(", "\"", "Saving", "a", "checkpoint", "is", "only", "possible", "if", "a", "model", "is", "attached", "to", "the", "Trainer", ".", "Did", "you", "call", "\"", "\"", "`", "Trainer", ".", "save_checkpoint", "(", ")", "`", "before", "calling", "`", "Trainer", ".", "{", "fit", ",", "validate", ",", "test", ",", "predict", "}", "`", "?", "\"", ")", "with", "self", ".", "profiler", ".", "profile", "(", "\"", "save_checkpoint", "\"", ")", ":", "checkpoint", "=", "self", ".", "_checkpoint_connector", ".", "dump_checkpoint", "(", "weights_only", ")", "self", ".", "strategy", ".", "save_checkpoint", "(", "checkpoint", ",", "filepath", ",", "storage_options", "=", "storage_options", ")", "self", ".", "strategy", ".", "barrier", "(", "\"", "Trainer", ".", "save_checkpoint", "\"", ")"], "docstring": "r\"\"\"Runs routine to create a checkpoint.\r\n\r\n        This method needs to be called on all processes in case the selected strategy is handling distributed\r\n        checkpointing.\r\n\r\n        Args:\r\n            filepath: Path where checkpoint is saved.\r\n            weights_only: If ``True``, will only save the model weights.\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        Raises:\r\n            AttributeError:\r\n                If the model is not attached to the Trainer before calling this method.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "runs", "routine", "to", "create", "a", "checkpoint", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "in", "case", "the", "selected", "strategy", "is", "handling", "distributed", "checkpointing", "args", "filepath", "path", "where", "checkpoint", "is", "saved", "weights_only", "if", "true", "will", "only", "save", "the", "model", "weights", "storage_options", "parameter", "for", "how", "to", "save", "to", "storage", "passed", "to", "checkpointio", "plugin", "raises", "attributeerror", "if", "the", "model", "is", "not", "attached", "to", "the", "trainer", "before", "calling", "this", "method"], "docstring_summary": "r\"\"\"Runs routine to create a checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1387, "end_line": 1413, "hash": "892d239834b8ac497e7fff91ead057a8", "complexity": 3, "parameters": ["filepath", "weights_only", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "sanity_checking", "original_string": "def sanity_checking(self) -> bool:\r\n        \"\"\"Whether sanity checking is running.\r\n\r\n        Useful to disable some hooks, logging or callbacks during the sanity checking.\r\n\r\n        \"\"\"\r\n        return self.state.stage == RunningStage.SANITY_CHECKING", "language": "python", "code": "def sanity_checking(self) -> bool:\r\n        \"\"\"Whether sanity checking is running.\r\n\r\n        Useful to disable some hooks, logging or callbacks during the sanity checking.\r\n\r\n        \"\"\"\r\n        return self.state.stage == RunningStage.SANITY_CHECKING", "code_tokens": ["def", "sanity_checking", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Whether", "sanity", "checking", "is", "running", ".", "Useful", "to", "disable", "some", "hooks", ",", "logging", "or", "callbacks", "during", "the", "sanity", "checking", ".", "\"", "\"", "\"", "return", "self", ".", "state", ".", "stage", "=", "=", "RunningStage", ".", "SANITY_CHECKING"], "docstring": "Whether sanity checking is running.\r\n\r\n        Useful to disable some hooks, logging or callbacks during the sanity checking.", "docstring_tokens": ["whether", "sanity", "checking", "is", "running", "useful", "to", "disable", "some", "hooks", "logging", "or", "callbacks", "during", "the", "sanity", "checking"], "docstring_summary": "Whether sanity checking is running.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1472, "end_line": 1478, "hash": "13ac4c9693d04326106cfbd379dc1856", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "received_sigterm", "original_string": "def received_sigterm(self) -> bool:\r\n        \"\"\"Whether a ``signal.SIGTERM`` signal was received.\r\n\r\n        For example, this can be checked to exit gracefully.\r\n\r\n        \"\"\"\r\n        return self._signal_connector.received_sigterm", "language": "python", "code": "def received_sigterm(self) -> bool:\r\n        \"\"\"Whether a ``signal.SIGTERM`` signal was received.\r\n\r\n        For example, this can be checked to exit gracefully.\r\n\r\n        \"\"\"\r\n        return self._signal_connector.received_sigterm", "code_tokens": ["def", "received_sigterm", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Whether", "a", "`", "`", "signal", ".", "SIGTERM", "`", "`", "signal", "was", "received", ".", "For", "example", ",", "this", "can", "be", "checked", "to", "exit", "gracefully", ".", "\"", "\"", "\"", "return", "self", ".", "_signal_connector", ".", "received_sigterm"], "docstring": "Whether a ``signal.SIGTERM`` signal was received.\r\n\r\n        For example, this can be checked to exit gracefully.", "docstring_tokens": ["whether", "a", "signal", "sigterm", "signal", "was", "received", "for", "example", "this", "can", "be", "checked", "to", "exit", "gracefully"], "docstring_summary": "Whether a ``signal.SIGTERM`` signal was received.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1488, "end_line": 1494, "hash": "884eceeda9b35350d0da8349973ababd", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "global_step", "original_string": "def global_step(self) -> int:\r\n        \"\"\"The number of optimizer steps taken (does not reset each epoch).\r\n\r\n        This includes multiple optimizers (if enabled).\r\n\r\n        \"\"\"\r\n        return self.fit_loop.epoch_loop.global_step", "language": "python", "code": "def global_step(self) -> int:\r\n        \"\"\"The number of optimizer steps taken (does not reset each epoch).\r\n\r\n        This includes multiple optimizers (if enabled).\r\n\r\n        \"\"\"\r\n        return self.fit_loop.epoch_loop.global_step", "code_tokens": ["def", "global_step", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "number", "of", "optimizer", "steps", "taken", "(", "does", "not", "reset", "each", "epoch", ")", ".", "This", "includes", "multiple", "optimizers", "(", "if", "enabled", ")", ".", "\"", "\"", "\"", "return", "self", ".", "fit_loop", ".", "epoch_loop", ".", "global_step"], "docstring": "The number of optimizer steps taken (does not reset each epoch).\r\n\r\n        This includes multiple optimizers (if enabled).", "docstring_tokens": ["the", "number", "of", "optimizer", "steps", "taken", "does", "not", "reset", "each", "epoch", "this", "includes", "multiple", "optimizers", "if", "enabled"], "docstring_summary": "The number of optimizer steps taken (does not reset each epoch).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1501, "end_line": 1507, "hash": "5b11f02df04e01c2ba1235291ce95004", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "train_dataloader", "original_string": "def train_dataloader(self) -> Optional[TRAIN_DATALOADERS]:\r\n        \"\"\"The training dataloader(s) used during ``trainer.fit()``.\"\"\"\r\n        if (combined_loader := self.fit_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def train_dataloader(self) -> Optional[TRAIN_DATALOADERS]:\r\n        \"\"\"The training dataloader(s) used during ``trainer.fit()``.\"\"\"\r\n        if (combined_loader := self.fit_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "Optional", "[", "TRAIN_DATALOADERS", "]", ":", "\"", "\"", "\"", "The", "training", "dataloader", "(", "s", ")", "used", "during", "`", "`", "trainer", ".", "fit", "(", ")", "`", "`", ".", "\"", "\"", "\"", "if", "(", "combined_loader", ":", "=", "self", ".", "fit_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The training dataloader(s) used during ``trainer.fit()``.", "docstring_tokens": ["the", "training", "dataloader", "s", "used", "during", "trainer", "fit"], "docstring_summary": "The training dataloader(s) used during ``trainer.fit()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1536, "end_line": 1540, "hash": "26f8494ea8307934cea46fcf4c5c8c19", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "val_dataloaders", "original_string": "def val_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if (combined_loader := self.fit_loop.epoch_loop.val_loop._combined_loader) is not None or (\r\n            combined_loader := self.validate_loop._combined_loader\r\n        ) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def val_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if (combined_loader := self.fit_loop.epoch_loop.val_loop._combined_loader) is not None or (\r\n            combined_loader := self.validate_loop._combined_loader\r\n        ) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "val_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "\"", "\"", "\"", "The", "validation", "dataloader", "(", "s", ")", "used", "during", "`", "`", "trainer", ".", "fit", "(", ")", "`", "`", "or", "`", "`", "trainer", ".", "validate", "(", ")", "`", "`", ".", "\"", "\"", "\"", "if", "(", "combined_loader", ":", "=", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_combined_loader", ")", "is", "not", "None", "or", "(", "combined_loader", ":", "=", "self", ".", "validate_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.", "docstring_tokens": ["the", "validation", "dataloader", "s", "used", "during", "trainer", "fit", "or", "trainer", "validate"], "docstring_summary": "The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1543, "end_line": 1549, "hash": "0f9d956ebba4a020cd19c6797d772444", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "test_dataloaders", "original_string": "def test_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The test dataloader(s) used during ``trainer.test()``.\"\"\"\r\n        if (combined_loader := self.test_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def test_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The test dataloader(s) used during ``trainer.test()``.\"\"\"\r\n        if (combined_loader := self.test_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "test_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "\"", "\"", "\"", "The", "test", "dataloader", "(", "s", ")", "used", "during", "`", "`", "trainer", ".", "test", "(", ")", "`", "`", ".", "\"", "\"", "\"", "if", "(", "combined_loader", ":", "=", "self", ".", "test_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The test dataloader(s) used during ``trainer.test()``.", "docstring_tokens": ["the", "test", "dataloader", "s", "used", "during", "trainer", "test"], "docstring_summary": "The test dataloader(s) used during ``trainer.test()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1552, "end_line": 1556, "hash": "91bec7414f9b917a72815cac77bce2f2", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "predict_dataloaders", "original_string": "def predict_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The prediction dataloader(s) used during ``trainer.predict()``.\"\"\"\r\n        if (combined_loader := self.predict_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def predict_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The prediction dataloader(s) used during ``trainer.predict()``.\"\"\"\r\n        if (combined_loader := self.predict_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "predict_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "\"", "\"", "\"", "The", "prediction", "dataloader", "(", "s", ")", "used", "during", "`", "`", "trainer", ".", "predict", "(", ")", "`", "`", ".", "\"", "\"", "\"", "if", "(", "combined_loader", ":", "=", "self", ".", "predict_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The prediction dataloader(s) used during ``trainer.predict()``.", "docstring_tokens": ["the", "prediction", "dataloader", "s", "used", "during", "trainer", "predict"], "docstring_summary": "The prediction dataloader(s) used during ``trainer.predict()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1559, "end_line": 1563, "hash": "5257425905ab22a421b05b873a996ac6", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "num_sanity_val_batches", "original_string": "def num_sanity_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\"\"\"\r\n        max_batches = self.fit_loop.epoch_loop.val_loop.max_batches\r\n        # re-compute the `min` in case this is called outside the sanity-checking stage\r\n        return [min(self.num_sanity_val_steps, batches) for batches in max_batches]", "language": "python", "code": "def num_sanity_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\"\"\"\r\n        max_batches = self.fit_loop.epoch_loop.val_loop.max_batches\r\n        # re-compute the `min` in case this is called outside the sanity-checking stage\r\n        return [min(self.num_sanity_val_steps, batches) for batches in max_batches]", "code_tokens": ["def", "num_sanity_val_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "\"", "\"", "\"", "The", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "the", "sanity", "-", "checking", "part", "of", "`", "`", "trainer", ".", "fit", "(", ")", "`", "`", ".", "\"", "\"", "\"", "max_batches", "=", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "max_batches", "return", "[", "min", "(", "self", ".", "num_sanity_val_steps", ",", "batches", ")", "for", "batches", "in", "max_batches", "]"], "docstring": "The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.", "docstring_tokens": ["the", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "the", "sanity", "checking", "part", "of", "trainer", "fit"], "docstring_summary": "The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1571, "end_line": 1575, "hash": "0b9788bced050e3673a92ffcefe2810f", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "num_val_batches", "original_string": "def num_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if self.state.fn == TrainerFn.VALIDATING:\r\n            return self.validate_loop.max_batches\r\n        # if no trainer.fn is set, assume fit's validation\r\n        # use the protected access, because it shouldn't return the sanity_val batches\r\n        return self.fit_loop.epoch_loop.val_loop._max_batches", "language": "python", "code": "def num_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if self.state.fn == TrainerFn.VALIDATING:\r\n            return self.validate_loop.max_batches\r\n        # if no trainer.fn is set, assume fit's validation\r\n        # use the protected access, because it shouldn't return the sanity_val batches\r\n        return self.fit_loop.epoch_loop.val_loop._max_batches", "code_tokens": ["def", "num_val_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "\"", "\"", "\"", "The", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "`", "`", "trainer", ".", "fit", "(", ")", "`", "`", "or", "`", "`", "trainer", ".", "validate", "(", ")", "`", "`", ".", "\"", "\"", "\"", "if", "self", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "return", "self", ".", "validate_loop", ".", "max_batches", "return", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_max_batches"], "docstring": "The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.", "docstring_tokens": ["the", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "trainer", "fit", "or", "trainer", "validate"], "docstring_summary": "The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1578, "end_line": 1584, "hash": "8bbcc8fcc0faf67487580b153df346e1", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "loggers", "original_string": "def loggers(self) -> list[Logger]:\r\n        \"\"\"The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\r\n\r\n        .. code-block:: python\r\n\r\n            for logger in trainer.loggers:\r\n                logger.log_metrics({\"foo\": 1.0})\r\n\r\n        \"\"\"\r\n        return self._loggers", "language": "python", "code": "def loggers(self) -> list[Logger]:\r\n        \"\"\"The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\r\n\r\n        .. code-block:: python\r\n\r\n            for logger in trainer.loggers:\r\n                logger.log_metrics({\"foo\": 1.0})\r\n\r\n        \"\"\"\r\n        return self._loggers", "code_tokens": ["def", "loggers", "(", "self", ")", "-", ">", "list", "[", "Logger", "]", ":", "\"", "\"", "\"", "The", "list", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "loggers", ".", "logger", ".", "Logger", "`", "used", ".", ".", ".", "code", "-", "block", ":", ":", "python", "for", "logger", "in", "trainer", ".", "loggers", ":", "logger", ".", "log_metrics", "(", "{", "\"", "foo", "\"", ":", "1", ".", "0", "}", ")", "\"", "\"", "\"", "return", "self", ".", "_loggers"], "docstring": "The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\r\n\r\n        .. code-block:: python\r\n\r\n            for logger in trainer.loggers:\r\n                logger.log_metrics({\"foo\": 1.0})", "docstring_tokens": ["the", "list", "of", "class", "lightning", "pytorch", "loggers", "logger", "logger", "used", "code", "block", "python", "for", "logger", "in", "trainer", "loggers", "logger", "log_metrics", "foo", "1", "0"], "docstring_summary": "The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1633, "end_line": 1642, "hash": "8910e2df3768c9413928869d66c59f73", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "callback_metrics", "original_string": "def callback_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics available to callbacks.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                self.log(\"a_val\", 2.0)\r\n\r\n\r\n            callback_metrics = trainer.callback_metrics\r\n            assert callback_metrics[\"a_val\"] == 2.0\r\n\r\n        \"\"\"\r\n        return self._logger_connector.callback_metrics", "language": "python", "code": "def callback_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics available to callbacks.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                self.log(\"a_val\", 2.0)\r\n\r\n\r\n            callback_metrics = trainer.callback_metrics\r\n            assert callback_metrics[\"a_val\"] == 2.0\r\n\r\n        \"\"\"\r\n        return self._logger_connector.callback_metrics", "code_tokens": ["def", "callback_metrics", "(", "self", ")", "-", ">", "_OUT_DICT", ":", "\"", "\"", "\"", "The", "metrics", "available", "to", "callbacks", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "self", ".", "log", "(", "\"", "a_val", "\"", ",", "2", ".", "0", ")", "callback_metrics", "=", "trainer", ".", "callback_metrics", "assert", "callback_metrics", "[", "\"", "a_val", "\"", "]", "=", "=", "2", ".", "0", "\"", "\"", "\"", "return", "self", ".", "_logger_connector", ".", "callback_metrics"], "docstring": "The metrics available to callbacks.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                self.log(\"a_val\", 2.0)\r\n\r\n\r\n            callback_metrics = trainer.callback_metrics\r\n            assert callback_metrics[\"a_val\"] == 2.0", "docstring_tokens": ["the", "metrics", "available", "to", "callbacks", "code", "block", "python", "def", "training_step", "self", "batch", "batch_idx", "self", "log", "a_val", "2", "0", "callback_metrics", "trainer", "callback_metrics", "assert", "callback_metrics", "a_val", "2", "0"], "docstring_summary": "The metrics available to callbacks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1649, "end_line": 1662, "hash": "bb7c497715bfc2586a1206b12b412712", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "logged_metrics", "original_string": "def logged_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics sent to the loggers.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.logged_metrics", "language": "python", "code": "def logged_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics sent to the loggers.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.logged_metrics", "code_tokens": ["def", "logged_metrics", "(", "self", ")", "-", ">", "_OUT_DICT", ":", "\"", "\"", "\"", "The", "metrics", "sent", "to", "the", "loggers", ".", "This", "includes", "metrics", "logged", "via", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "log", "`", "with", "the", ":", "paramref", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "log", ".", "logger", "`", "argument", "set", ".", "\"", "\"", "\"", "return", "self", ".", "_logger_connector", ".", "logged_metrics"], "docstring": "The metrics sent to the loggers.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.", "docstring_tokens": ["the", "metrics", "sent", "to", "the", "loggers", "this", "includes", "metrics", "logged", "via", "meth", "lightning", "pytorch", "core", "lightningmodule", "log", "with", "the", "paramref", "lightning", "pytorch", "core", "lightningmodule", "log", "logger", "argument", "set"], "docstring_summary": "The metrics sent to the loggers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1665, "end_line": 1672, "hash": "0fa37579ac96001d6aa957118c207924", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "progress_bar_metrics", "original_string": "def progress_bar_metrics(self) -> _PBAR_DICT:\r\n        \"\"\"The metrics sent to the progress bar.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.progress_bar_metrics", "language": "python", "code": "def progress_bar_metrics(self) -> _PBAR_DICT:\r\n        \"\"\"The metrics sent to the progress bar.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.progress_bar_metrics", "code_tokens": ["def", "progress_bar_metrics", "(", "self", ")", "-", ">", "_PBAR_DICT", ":", "\"", "\"", "\"", "The", "metrics", "sent", "to", "the", "progress", "bar", ".", "This", "includes", "metrics", "logged", "via", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "log", "`", "with", "the", ":", "paramref", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "log", ".", "prog_bar", "`", "argument", "set", ".", "\"", "\"", "\"", "return", "self", ".", "_logger_connector", ".", "progress_bar_metrics"], "docstring": "The metrics sent to the progress bar.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.", "docstring_tokens": ["the", "metrics", "sent", "to", "the", "progress", "bar", "this", "includes", "metrics", "logged", "via", "meth", "lightning", "pytorch", "core", "lightningmodule", "log", "with", "the", "paramref", "lightning", "pytorch", "core", "lightningmodule", "log", "prog_bar", "argument", "set"], "docstring_summary": "The metrics sent to the progress bar.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1675, "end_line": 1682, "hash": "bc8130fd4053d76ca91dd253edc0d2c8", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "estimated_stepping_batches", "original_string": "def estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        # infinite training\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        # iterable dataset\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps", "language": "python", "code": "def estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        # infinite training\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        # iterable dataset\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps", "code_tokens": ["def", "estimated_stepping_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "r", "\"", "\"", "\"", "The", "estimated", "number", "of", "batches", "that", "will", "`", "`", "optimizer", ".", "step", "(", ")", "`", "`", "during", "training", ".", "This", "accounts", "for", "gradient", "accumulation", "and", "the", "current", "trainer", "configuration", ".", "This", "might", "be", "used", "when", "setting", "up", "your", "training", "dataloader", ",", "if", "it", "hasn", "'", "t", "been", "set", "up", "already", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "configure_optimizers", "(", "self", ")", ":", "optimizer", "=", ".", ".", ".", "stepping_batches", "=", "self", ".", "trainer", ".", "estimated_stepping_batches", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "OneCycleLR", "(", "optimizer", ",", "max_lr", "=", "1e", "-", "3", ",", "total_steps", "=", "stepping_batches", ")", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "Raises", ":", "MisconfigurationException", ":", "If", "estimated", "stepping", "batches", "cannot", "be", "computed", "due", "to", "different", "`", "accumulate_grad_batches", "`", "at", "different", "epochs", ".", "\"", "\"", "\"", "if", "self", ".", "max_epochs", "=", "=", "-", "1", ":", "return", "float", "(", "\"", "inf", "\"", ")", "if", "self", ".", "max_steps", "=", "=", "-", "1", "else", "self", ".", "max_steps", "if", "self", ".", "train_dataloader", "is", "None", ":", "rank_zero_info", "(", "\"", "Loading", "`", "train_dataloader", "`", "to", "estimate", "number", "of", "stepping", "batches", ".", "\"", ")", "self", ".", "fit_loop", ".", "setup_data", "(", ")", "total_batches", "=", "self", ".", "num_training_batches", "if", "total_batches", "=", "=", "float", "(", "\"", "inf", "\"", ")", ":", "return", "self", ".", "max_steps", "assert", "self", ".", "max_epochs", "is", "not", "None", "max_estimated_steps", "=", "math", ".", "ceil", "(", "total_batches", "/", "self", ".", "accumulate_grad_batches", ")", "*", "max", "(", "self", ".", "max_epochs", ",", "1", ")", "max_estimated_steps", "=", "min", "(", "max_estimated_steps", ",", "self", ".", "max_steps", ")", "if", "self", ".", "max_steps", "!", "=", "-", "1", "else", "max_estimated_steps", "return", "max_estimated_steps"], "docstring": "r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "the", "estimated", "number", "of", "batches", "that", "will", "optimizer", "step", "during", "training", "this", "accounts", "for", "gradient", "accumulation", "and", "the", "current", "trainer", "configuration", "this", "might", "be", "used", "when", "setting", "up", "your", "training", "dataloader", "if", "it", "hasn", "t", "been", "set", "up", "already", "code", "block", "python", "def", "configure_optimizers", "self", "optimizer", "stepping_batches", "self", "trainer", "estimated_stepping_batches", "scheduler", "torch", "optim", "lr_scheduler", "onecyclelr", "optimizer", "max_lr", "1e", "3", "total_steps", "stepping_batches", "return", "optimizer", "scheduler", "raises", "misconfigurationexception", "if", "estimated", "stepping", "batches", "cannot", "be", "computed", "due", "to", "different", "accumulate_grad_batches", "at", "different", "epochs"], "docstring_summary": "r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\trainer.py", "partition": "valid", "function_type": "class_method", "class_name": "Trainer", "start_line": 1696, "end_line": 1734, "hash": "851bb06a6047644d55f6c529f29fe60e", "complexity": 6, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]] = None,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        sync_batchnorm: bool = False,\r\n        benchmark: Optional[bool] = None,\r\n        use_distributed_sampler: bool = True,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n    ) -> None:\r\n        \"\"\"The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other\r\n        components such as the Accelerator and Precision plugins.\r\n\r\n            A. accelerator flag could be:\r\n                1. accelerator class\r\n                2. accelerator str\r\n                3. accelerator auto\r\n\r\n            B. strategy flag could be:\r\n                1. strategy class\r\n                2. strategy str registered with StrategyRegistry\r\n\r\n            C. plugins flag could be:\r\n                1. precision class (should be removed, and precision flag should allow user pass classes)\r\n                2. checkpoint_io class\r\n                3. cluster_environment class\r\n\r\n        priorities which to take when:\r\n            A. Class > str\r\n            B. Strategy > Accelerator/precision/plugins\r\n\r\n        \"\"\"\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        _set_torch_flags(deterministic=deterministic, benchmark=benchmark)\r\n\r\n        # 1. Parsing flags\r\n        # Get registered strategies, built-in accelerators and precision plugins\r\n        _register_external_accelerators_and_strategies()\r\n        self._registered_strategies = StrategyRegistry.available_strategies()\r\n        self._accelerator_types = AcceleratorRegistry.available_accelerators()\r\n\r\n        # Raise an exception if there are conflicts between flags\r\n        # Set each valid flag to `self._x_flag` after validation\r\n        self._strategy_flag: Union[Strategy, str] = \"auto\"\r\n        self._accelerator_flag: Union[Accelerator, str] = \"auto\"\r\n        self._precision_flag: _PRECISION_INPUT_STR = \"32-true\"\r\n        self._precision_plugin_flag: Optional[Precision] = None\r\n        self._cluster_environment_flag: Optional[Union[ClusterEnvironment, str]] = None\r\n        self._parallel_devices: list[Union[int, torch.device, str]] = []\r\n        self._layer_sync: Optional[LayerSync] = TorchSyncBatchNorm() if sync_batchnorm else None\r\n        self.checkpoint_io: Optional[CheckpointIO] = None\r\n\r\n        self._check_config_and_set_final_flags(\r\n            strategy=strategy,\r\n            accelerator=accelerator,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            sync_batchnorm=sync_batchnorm,\r\n        )\r\n\r\n        # 2. Instantiate Accelerator\r\n        # handle `auto`, `None` and `gpu`\r\n        if self._accelerator_flag == \"auto\":\r\n            self._accelerator_flag = self._choose_auto_accelerator()\r\n        elif self._accelerator_flag == \"gpu\":\r\n            self._accelerator_flag = self._choose_gpu_accelerator_backend()\r\n\r\n        self._check_device_config_and_set_final_flags(devices=devices, num_nodes=num_nodes)\r\n        self._set_parallel_devices_and_init_accelerator()\r\n\r\n        # 3. Instantiate ClusterEnvironment\r\n        self.cluster_environment: ClusterEnvironment = self._choose_and_init_cluster_environment()\r\n\r\n        # 4. Instantiate Strategy - Part 1\r\n        if self._strategy_flag == \"auto\":\r\n            self._strategy_flag = self._choose_strategy()\r\n        # In specific cases, ignore user selection and fall back to a different strategy\r\n        self._check_strategy_and_fallback()\r\n        self._init_strategy()\r\n\r\n        # 5. Instantiate Precision Plugin\r\n        self.precision_plugin = self._check_and_init_precision()\r\n\r\n        # 6. Instantiate Strategy - Part 2\r\n        self._lazy_init_strategy()", "language": "python", "code": "def __init__(\r\n        self,\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]] = None,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        sync_batchnorm: bool = False,\r\n        benchmark: Optional[bool] = None,\r\n        use_distributed_sampler: bool = True,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n    ) -> None:\r\n        \"\"\"The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other\r\n        components such as the Accelerator and Precision plugins.\r\n\r\n            A. accelerator flag could be:\r\n                1. accelerator class\r\n                2. accelerator str\r\n                3. accelerator auto\r\n\r\n            B. strategy flag could be:\r\n                1. strategy class\r\n                2. strategy str registered with StrategyRegistry\r\n\r\n            C. plugins flag could be:\r\n                1. precision class (should be removed, and precision flag should allow user pass classes)\r\n                2. checkpoint_io class\r\n                3. cluster_environment class\r\n\r\n        priorities which to take when:\r\n            A. Class > str\r\n            B. Strategy > Accelerator/precision/plugins\r\n\r\n        \"\"\"\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        _set_torch_flags(deterministic=deterministic, benchmark=benchmark)\r\n\r\n        # 1. Parsing flags\r\n        # Get registered strategies, built-in accelerators and precision plugins\r\n        _register_external_accelerators_and_strategies()\r\n        self._registered_strategies = StrategyRegistry.available_strategies()\r\n        self._accelerator_types = AcceleratorRegistry.available_accelerators()\r\n\r\n        # Raise an exception if there are conflicts between flags\r\n        # Set each valid flag to `self._x_flag` after validation\r\n        self._strategy_flag: Union[Strategy, str] = \"auto\"\r\n        self._accelerator_flag: Union[Accelerator, str] = \"auto\"\r\n        self._precision_flag: _PRECISION_INPUT_STR = \"32-true\"\r\n        self._precision_plugin_flag: Optional[Precision] = None\r\n        self._cluster_environment_flag: Optional[Union[ClusterEnvironment, str]] = None\r\n        self._parallel_devices: list[Union[int, torch.device, str]] = []\r\n        self._layer_sync: Optional[LayerSync] = TorchSyncBatchNorm() if sync_batchnorm else None\r\n        self.checkpoint_io: Optional[CheckpointIO] = None\r\n\r\n        self._check_config_and_set_final_flags(\r\n            strategy=strategy,\r\n            accelerator=accelerator,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            sync_batchnorm=sync_batchnorm,\r\n        )\r\n\r\n        # 2. Instantiate Accelerator\r\n        # handle `auto`, `None` and `gpu`\r\n        if self._accelerator_flag == \"auto\":\r\n            self._accelerator_flag = self._choose_auto_accelerator()\r\n        elif self._accelerator_flag == \"gpu\":\r\n            self._accelerator_flag = self._choose_gpu_accelerator_backend()\r\n\r\n        self._check_device_config_and_set_final_flags(devices=devices, num_nodes=num_nodes)\r\n        self._set_parallel_devices_and_init_accelerator()\r\n\r\n        # 3. Instantiate ClusterEnvironment\r\n        self.cluster_environment: ClusterEnvironment = self._choose_and_init_cluster_environment()\r\n\r\n        # 4. Instantiate Strategy - Part 1\r\n        if self._strategy_flag == \"auto\":\r\n            self._strategy_flag = self._choose_strategy()\r\n        # In specific cases, ignore user selection and fall back to a different strategy\r\n        self._check_strategy_and_fallback()\r\n        self._init_strategy()\r\n\r\n        # 5. Instantiate Precision Plugin\r\n        self.precision_plugin = self._check_and_init_precision()\r\n\r\n        # 6. Instantiate Strategy - Part 2\r\n        self._lazy_init_strategy()", "code_tokens": ["def", "__init__", "(", "self", ",", "devices", ":", "Union", "[", "list", "[", "int", "]", ",", "str", ",", "int", "]", "=", "\"", "auto", "\"", ",", "num_nodes", ":", "int", "=", "1", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", "=", "\"", "auto", "\"", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", "=", "\"", "auto", "\"", ",", "plugins", ":", "Optional", "[", "Union", "[", "_PLUGIN_INPUT", ",", "Iterable", "[", "_PLUGIN_INPUT", "]", "]", "]", "=", "None", ",", "precision", ":", "Optional", "[", "_PRECISION_INPUT", "]", "=", "None", ",", "sync_batchnorm", ":", "bool", "=", "False", ",", "benchmark", ":", "Optional", "[", "bool", "]", "=", "None", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "deterministic", ":", "Optional", "[", "Union", "[", "bool", ",", "_LITERAL_WARN", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "The", "AcceleratorConnector", "parses", "several", "Trainer", "arguments", "and", "instantiates", "the", "Strategy", "including", "other", "components", "such", "as", "the", "Accelerator", "and", "Precision", "plugins", ".", "A", ".", "accelerator", "flag", "could", "be", ":", "1", ".", "accelerator", "class", "2", ".", "accelerator", "str", "3", ".", "accelerator", "auto", "B", ".", "strategy", "flag", "could", "be", ":", "1", ".", "strategy", "class", "2", ".", "strategy", "str", "registered", "with", "StrategyRegistry", "C", ".", "plugins", "flag", "could", "be", ":", "1", ".", "precision", "class", "(", "should", "be", "removed", ",", "and", "precision", "flag", "should", "allow", "user", "pass", "classes", ")", "2", ".", "checkpoint_io", "class", "3", ".", "cluster_environment", "class", "priorities", "which", "to", "take", "when", ":", "A", ".", "Class", ">", "str", "B", ".", "Strategy", ">", "Accelerator", "/", "precision", "/", "plugins", "\"", "\"", "\"", "self", ".", "use_distributed_sampler", "=", "use_distributed_sampler", "_set_torch_flags", "(", "deterministic", "=", "deterministic", ",", "benchmark", "=", "benchmark", ")", "_register_external_accelerators_and_strategies", "(", ")", "self", ".", "_registered_strategies", "=", "StrategyRegistry", ".", "available_strategies", "(", ")", "self", ".", "_accelerator_types", "=", "AcceleratorRegistry", ".", "available_accelerators", "(", ")", "self", ".", "_strategy_flag", ":", "Union", "[", "Strategy", ",", "str", "]", "=", "\"", "auto", "\"", "self", ".", "_accelerator_flag", ":", "Union", "[", "Accelerator", ",", "str", "]", "=", "\"", "auto", "\"", "self", ".", "_precision_flag", ":", "_PRECISION_INPUT_STR", "=", "\"", "32", "-", "true", "\"", "self", ".", "_precision_plugin_flag", ":", "Optional", "[", "Precision", "]", "=", "None", "self", ".", "_cluster_environment_flag", ":", "Optional", "[", "Union", "[", "ClusterEnvironment", ",", "str", "]", "]", "=", "None", "self", ".", "_parallel_devices", ":", "list", "[", "Union", "[", "int", ",", "torch", ".", "device", ",", "str", "]", "]", "=", "[", "]", "self", ".", "_layer_sync", ":", "Optional", "[", "LayerSync", "]", "=", "TorchSyncBatchNorm", "(", ")", "if", "sync_batchnorm", "else", "None", "self", ".", "checkpoint_io", ":", "Optional", "[", "CheckpointIO", "]", "=", "None", "self", ".", "_check_config_and_set_final_flags", "(", "strategy", "=", "strategy", ",", "accelerator", "=", "accelerator", ",", "precision", "=", "precision", ",", "plugins", "=", "plugins", ",", "sync_batchnorm", "=", "sync_batchnorm", ",", ")", "if", "self", ".", "_accelerator_flag", "=", "=", "\"", "auto", "\"", ":", "self", ".", "_accelerator_flag", "=", "self", ".", "_choose_auto_accelerator", "(", ")", "elif", "self", ".", "_accelerator_flag", "=", "=", "\"", "gpu", "\"", ":", "self", ".", "_accelerator_flag", "=", "self", ".", "_choose_gpu_accelerator_backend", "(", ")", "self", ".", "_check_device_config_and_set_final_flags", "(", "devices", "=", "devices", ",", "num_nodes", "=", "num_nodes", ")", "self", ".", "_set_parallel_devices_and_init_accelerator", "(", ")", "self", ".", "cluster_environment", ":", "ClusterEnvironment", "=", "self", ".", "_choose_and_init_cluster_environment", "(", ")", "if", "self", ".", "_strategy_flag", "=", "=", "\"", "auto", "\"", ":", "self", ".", "_strategy_flag", "=", "self", ".", "_choose_strategy", "(", ")", "self", ".", "_check_strategy_and_fallback", "(", ")", "self", ".", "_init_strategy", "(", ")", "self", ".", "precision_plugin", "=", "self", ".", "_check_and_init_precision", "(", ")", "self", ".", "_lazy_init_strategy", "(", ")"], "docstring": "The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other\r\n        components such as the Accelerator and Precision plugins.\r\n\r\n            A. accelerator flag could be:\r\n                1. accelerator class\r\n                2. accelerator str\r\n                3. accelerator auto\r\n\r\n            B. strategy flag could be:\r\n                1. strategy class\r\n                2. strategy str registered with StrategyRegistry\r\n\r\n            C. plugins flag could be:\r\n                1. precision class (should be removed, and precision flag should allow user pass classes)\r\n                2. checkpoint_io class\r\n                3. cluster_environment class\r\n\r\n        priorities which to take when:\r\n            A. Class > str\r\n            B. Strategy > Accelerator/precision/plugins", "docstring_tokens": ["the", "acceleratorconnector", "parses", "several", "trainer", "arguments", "and", "instantiates", "the", "strategy", "including", "other", "components", "such", "as", "the", "accelerator", "and", "precision", "plugins", "a", "accelerator", "flag", "could", "be", "1", "accelerator", "class", "2", "accelerator", "str", "3", "accelerator", "auto", "b", "strategy", "flag", "could", "be", "1", "strategy", "class", "2", "strategy", "str", "registered", "with", "strategyregistry", "c", "plugins", "flag", "could", "be", "1", "precision", "class", "should", "be", "removed", "and", "precision", "flag", "should", "allow", "user", "pass", "classes", "2", "checkpoint_io", "class", "3", "cluster_environment", "class", "priorities", "which", "to", "take", "when", "a", "class", "str", "b", "strategy", "accelerator", "precision", "plugins"], "docstring_summary": "The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_AcceleratorConnector", "start_line": 75, "end_line": 162, "hash": "3f52574404494b739668f8013f604ebd", "complexity": 5, "parameters": ["devices", "str", "int]", "num_nodes", "accelerator", "Accelerator]", "strategy", "Strategy]", "plugins", "Iterable[_PLUGIN_INPUT]]]", "precision", "sync_batchnorm", "benchmark", "use_distributed_sampler", "deterministic", "_LITERAL_WARN]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "_choose_auto_accelerator", "original_string": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if HPUAccelerator.is_available():\r\n                return \"hpu\"\r\n        return _select_auto_accelerator()", "language": "python", "code": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if HPUAccelerator.is_available():\r\n                return \"hpu\"\r\n        return _select_auto_accelerator()", "code_tokens": ["def", "_choose_auto_accelerator", "(", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Choose", "the", "accelerator", "type", "(", "str", ")", "based", "on", "availability", ".", "\"", "\"", "\"", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", "if", "HPUAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "hpu", "\"", "return", "_select_auto_accelerator", "(", ")"], "docstring": "Choose the accelerator type (str) based on availability.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability"], "docstring_summary": "Choose the accelerator type (str) based on availability.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_AcceleratorConnector", "start_line": 332, "end_line": 339, "hash": "f74263987e5811c4b237e6e55f10090c", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "_check_strategy_and_fallback", "original_string": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if (\r\n            strategy_flag in FSDPStrategy.get_registered_strategies() or type(self._strategy_flag) is FSDPStrategy\r\n        ) and not (self._accelerator_flag in (\"cuda\", \"gpu\") or isinstance(self._accelerator_flag, CUDAAccelerator)):\r\n            raise ValueError(\r\n                f\"The strategy `{FSDPStrategy.strategy_name}` requires a GPU accelerator, but received \"\r\n                f\"`accelerator={self._accelerator_flag!r}`. Please set `accelerator='cuda'`, `accelerator='gpu'`,\"\r\n                \" or pass a `CUDAAccelerator()` instance to use FSDP.\"\r\n            )\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Trainer(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Trainer(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "language": "python", "code": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if (\r\n            strategy_flag in FSDPStrategy.get_registered_strategies() or type(self._strategy_flag) is FSDPStrategy\r\n        ) and not (self._accelerator_flag in (\"cuda\", \"gpu\") or isinstance(self._accelerator_flag, CUDAAccelerator)):\r\n            raise ValueError(\r\n                f\"The strategy `{FSDPStrategy.strategy_name}` requires a GPU accelerator, but received \"\r\n                f\"`accelerator={self._accelerator_flag!r}`. Please set `accelerator='cuda'`, `accelerator='gpu'`,\"\r\n                \" or pass a `CUDAAccelerator()` instance to use FSDP.\"\r\n            )\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Trainer(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Trainer(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "code_tokens": ["def", "_check_strategy_and_fallback", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", ",", "and", "we", "need", "to", "fall", "back", "to", "a", "different", "choice", "depending", "on", "other", "parameters", "or", "the", "environment", ".", "\"", "\"", "\"", "strategy_flag", "=", "\"", "\"", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", "else", "self", ".", "_strategy_flag", "if", "(", "strategy_flag", "in", "FSDPStrategy", ".", "get_registered_strategies", "(", ")", "or", "type", "(", "self", ".", "_strategy_flag", ")", "is", "FSDPStrategy", ")", "and", "not", "(", "self", ".", "_accelerator_flag", "in", "(", "\"", "cuda", "\"", ",", "\"", "gpu", "\"", ")", "or", "isinstance", "(", "self", ".", "_accelerator_flag", ",", "CUDAAccelerator", ")", ")", ":", "raise", "ValueError", "(", "f", "\"", "The", "strategy", "`", "{", "FSDPStrategy", ".", "strategy_name", "}", "`", "requires", "a", "GPU", "accelerator", ",", "but", "received", "\"", "f", "\"", "`", "accelerator", "=", "{", "self", ".", "_accelerator_flag", "!", "r", "}", "`", ".", "Please", "set", "`", "accelerator", "=", "'", "cuda", "'", "`", ",", "`", "accelerator", "=", "'", "gpu", "'", "`", ",", "\"", "\"", "or", "pass", "a", "`", "CUDAAccelerator", "(", ")", "`", "instance", "to", "use", "FSDP", ".", "\"", ")", "if", "strategy_flag", "in", "_DDP_FORK_ALIASES", "and", "\"", "fork", "\"", "not", "in", "torch", ".", "multiprocessing", ".", "get_all_start_methods", "(", ")", ":", "raise", "ValueError", "(", "f", "\"", "You", "selected", "`", "Trainer", "(", "strategy", "=", "'", "{", "strategy_flag", "}", "'", ")", "`", "but", "process", "forking", "is", "not", "supported", "on", "this", "\"", "f", "\"", "platform", ".", "We", "recommend", "`", "Trainer", "(", "strategy", "=", "'", "ddp_spawn", "'", ")", "`", "instead", ".", "\"", ")", "if", "strategy_flag", ":", "self", ".", "_strategy_flag", "=", "strategy_flag"], "docstring": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.", "docstring_tokens": ["checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", "and", "we", "need", "to", "fall", "back", "to", "a", "different", "choice", "depending", "on", "other", "parameters", "or", "the", "environment"], "docstring_summary": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_AcceleratorConnector", "start_line": 446, "end_line": 467, "hash": "5346db8105e0cb538c3347b8b61b7d68", "complexity": 9, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "_init_strategy", "original_string": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = StrategyRegistry.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "language": "python", "code": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = StrategyRegistry.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "code_tokens": ["def", "_init_strategy", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Instantiate", "the", "Strategy", "given", "depending", "on", "the", "setting", "of", "`", "`", "_strategy_flag", "`", "`", ".", "\"", "\"", "\"", "assert", "isinstance", "(", "self", ".", "_strategy_flag", ",", "(", "str", ",", "Strategy", ")", ")", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "str", ")", ":", "self", ".", "strategy", "=", "StrategyRegistry", ".", "get", "(", "self", ".", "_strategy_flag", ")", "else", ":", "self", ".", "strategy", "=", "self", ".", "_strategy_flag"], "docstring": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "docstring_tokens": ["instantiate", "the", "strategy", "given", "depending", "on", "the", "setting", "of", "_strategy_flag"], "docstring_summary": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_AcceleratorConnector", "start_line": 469, "end_line": 476, "hash": "2b77d851b7251266bca261404fe90eb7", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "_validate_precision_choice", "original_string": "def _validate_precision_choice(self) -> None:\r\n        \"\"\"Validate the combination of choices for precision, AMP type, and accelerator.\"\"\"\r\n        if isinstance(self._precision_plugin_flag, BitsandbytesPrecision) and not isinstance(\r\n            self.accelerator, CUDAAccelerator\r\n        ):\r\n            raise RuntimeError(\"Bitsandbytes is only supported on CUDA GPUs.\")\r\n        mp_precision_supported = (\"32-true\", \"bf16-mixed\", \"bf16-true\", \"16-true\")\r\n        if (\r\n            isinstance(self._strategy_flag, ModelParallelStrategy)\r\n            and self._precision_flag not in mp_precision_supported\r\n        ):\r\n            raise ValueError(\r\n                f\"The `ModelParallelStrategy` does not support `Fabric(..., precision={self._precision_flag!r})`.\"\r\n                f\" Choose a different precision among: {', '.join(mp_precision_supported)}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and self._precision_flag not in (\r\n                \"16-mixed\",\r\n                \"bf16-mixed\",\r\n                \"32-true\",\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"`Trainer(accelerator='hpu', precision={self._precision_flag!r})` is not supported.\"\r\n                )", "language": "python", "code": "def _validate_precision_choice(self) -> None:\r\n        \"\"\"Validate the combination of choices for precision, AMP type, and accelerator.\"\"\"\r\n        if isinstance(self._precision_plugin_flag, BitsandbytesPrecision) and not isinstance(\r\n            self.accelerator, CUDAAccelerator\r\n        ):\r\n            raise RuntimeError(\"Bitsandbytes is only supported on CUDA GPUs.\")\r\n        mp_precision_supported = (\"32-true\", \"bf16-mixed\", \"bf16-true\", \"16-true\")\r\n        if (\r\n            isinstance(self._strategy_flag, ModelParallelStrategy)\r\n            and self._precision_flag not in mp_precision_supported\r\n        ):\r\n            raise ValueError(\r\n                f\"The `ModelParallelStrategy` does not support `Fabric(..., precision={self._precision_flag!r})`.\"\r\n                f\" Choose a different precision among: {', '.join(mp_precision_supported)}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and self._precision_flag not in (\r\n                \"16-mixed\",\r\n                \"bf16-mixed\",\r\n                \"32-true\",\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"`Trainer(accelerator='hpu', precision={self._precision_flag!r})` is not supported.\"\r\n                )", "code_tokens": ["def", "_validate_precision_choice", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Validate", "the", "combination", "of", "choices", "for", "precision", ",", "AMP", "type", ",", "and", "accelerator", ".", "\"", "\"", "\"", "if", "isinstance", "(", "self", ".", "_precision_plugin_flag", ",", "BitsandbytesPrecision", ")", "and", "not", "isinstance", "(", "self", ".", "accelerator", ",", "CUDAAccelerator", ")", ":", "raise", "RuntimeError", "(", "\"", "Bitsandbytes", "is", "only", "supported", "on", "CUDA", "GPUs", ".", "\"", ")", "mp_precision_supported", "=", "(", "\"", "32", "-", "true", "\"", ",", "\"", "bf16", "-", "mixed", "\"", ",", "\"", "bf16", "-", "true", "\"", ",", "\"", "16", "-", "true", "\"", ")", "if", "(", "isinstance", "(", "self", ".", "_strategy_flag", ",", "ModelParallelStrategy", ")", "and", "self", ".", "_precision_flag", "not", "in", "mp_precision_supported", ")", ":", "raise", "ValueError", "(", "f", "\"", "The", "`", "ModelParallelStrategy", "`", "does", "not", "support", "`", "Fabric", "(", ".", ".", ".", ",", "precision", "=", "{", "self", ".", "_precision_flag", "!", "r", "}", ")", "`", ".", "\"", "f", "\"", "Choose", "a", "different", "precision", "among", ":", "{", "'", ",", "'", ".", "join", "(", "mp_precision_supported", ")", "}", ".", "\"", ")", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", "if", "isinstance", "(", "self", ".", "accelerator", ",", "HPUAccelerator", ")", "and", "self", ".", "_precision_flag", "not", "in", "(", "\"", "16", "-", "mixed", "\"", ",", "\"", "bf16", "-", "mixed", "\"", ",", "\"", "32", "-", "true", "\"", ",", ")", ":", "raise", "MisconfigurationException", "(", "f", "\"", "`", "Trainer", "(", "accelerator", "=", "'", "hpu", "'", ",", "precision", "=", "{", "self", ".", "_precision_flag", "!", "r", "}", ")", "`", "is", "not", "supported", ".", "\"", ")"], "docstring": "Validate the combination of choices for precision, AMP type, and accelerator.", "docstring_tokens": ["validate", "the", "combination", "of", "choices", "for", "precision", "amp", "type", "and", "accelerator"], "docstring_summary": "Validate the combination of choices for precision, AMP type, and accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_AcceleratorConnector", "start_line": 522, "end_line": 548, "hash": "5fa8a200cac38c6079b3181d70ea6fc9", "complexity": 8, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "_register_external_accelerators_and_strategies", "original_string": "def _register_external_accelerators_and_strategies() -> None:\r\n    \"\"\"Registers all known strategies in other packages.\"\"\"\r\n    if _habana_available_and_importable():\r\n        from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n        # TODO: Prevent registering multiple times\r\n        if \"hpu\" not in AcceleratorRegistry:\r\n            HPUAccelerator.register_accelerators(AcceleratorRegistry)\r\n        if \"hpu_parallel\" not in StrategyRegistry:\r\n            HPUParallelStrategy.register_strategies(StrategyRegistry)\r\n        if \"hpu_single\" not in StrategyRegistry:\r\n            SingleHPUStrategy.register_strategies(StrategyRegistry)", "language": "python", "code": "def _register_external_accelerators_and_strategies() -> None:\r\n    \"\"\"Registers all known strategies in other packages.\"\"\"\r\n    if _habana_available_and_importable():\r\n        from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n        # TODO: Prevent registering multiple times\r\n        if \"hpu\" not in AcceleratorRegistry:\r\n            HPUAccelerator.register_accelerators(AcceleratorRegistry)\r\n        if \"hpu_parallel\" not in StrategyRegistry:\r\n            HPUParallelStrategy.register_strategies(StrategyRegistry)\r\n        if \"hpu_single\" not in StrategyRegistry:\r\n            SingleHPUStrategy.register_strategies(StrategyRegistry)", "code_tokens": ["def", "_register_external_accelerators_and_strategies", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Registers", "all", "known", "strategies", "in", "other", "packages", ".", "\"", "\"", "\"", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", ",", "HPUParallelStrategy", ",", "SingleHPUStrategy", "if", "\"", "hpu", "\"", "not", "in", "AcceleratorRegistry", ":", "HPUAccelerator", ".", "register_accelerators", "(", "AcceleratorRegistry", ")", "if", "\"", "hpu_parallel", "\"", "not", "in", "StrategyRegistry", ":", "HPUParallelStrategy", ".", "register_strategies", "(", "StrategyRegistry", ")", "if", "\"", "hpu_single", "\"", "not", "in", "StrategyRegistry", ":", "SingleHPUStrategy", ".", "register_strategies", "(", "StrategyRegistry", ")"], "docstring": "Registers all known strategies in other packages.", "docstring_tokens": ["registers", "all", "known", "strategies", "in", "other", "packages"], "docstring_summary": "Registers all known strategies in other packages.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "partition": "valid", "function_type": "function", "start_line": 650, "end_line": 661, "hash": "ada14e03740a3ca7e91547bf4c613925", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "func_name": "_attach_model_callbacks", "original_string": "def _attach_model_callbacks(self) -> None:\r\n        \"\"\"Attaches the callbacks defined in the model.\r\n\r\n        If a callback returned by the model's configure_callback method has the same type as one or several\r\n        callbacks already present in the trainer callbacks list, it will replace them.\r\n        In addition, all :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks\r\n        will be pushed to the end of the list, ensuring they run last.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        model_callbacks = call._call_lightning_module_hook(trainer, \"configure_callbacks\")\r\n        if not model_callbacks:\r\n            return\r\n\r\n        model_callbacks = [model_callbacks] if not isinstance(model_callbacks, Sequence) else model_callbacks\r\n        model_callback_types = {type(c) for c in model_callbacks}\r\n        trainer_callback_types = {type(c) for c in trainer.callbacks}\r\n        # edge case: if an unmodified callback was added, the logic below would filter it\r\n        trainer_callback_types.discard(Callback)\r\n        # exclude trainer callbacks of the same class or subclass\r\n        override_types = set()\r\n        for model_cb in model_callback_types:\r\n            for trainer_cb in trainer_callback_types:\r\n                if issubclass(model_cb, trainer_cb):\r\n                    override_types.add(trainer_cb)\r\n                    break\r\n        if override_types:\r\n            rank_zero_info(\r\n                \"The following callbacks returned in `LightningModule.configure_callbacks` will override\"\r\n                \" existing callbacks passed to Trainer:\"\r\n                f\" {', '.join(sorted(t.__name__ for t in override_types))}\"\r\n            )\r\n        # remove all callbacks with a type that occurs in model callbacks\r\n        all_callbacks = [c for c in trainer.callbacks if type(c) not in override_types]\r\n        all_callbacks.extend(model_callbacks)\r\n        all_callbacks = _CallbackConnector._reorder_callbacks(all_callbacks)\r\n        # TODO: connectors refactor: move callbacks list to connector and do not write Trainer state\r\n        trainer.callbacks = all_callbacks", "language": "python", "code": "def _attach_model_callbacks(self) -> None:\r\n        \"\"\"Attaches the callbacks defined in the model.\r\n\r\n        If a callback returned by the model's configure_callback method has the same type as one or several\r\n        callbacks already present in the trainer callbacks list, it will replace them.\r\n        In addition, all :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks\r\n        will be pushed to the end of the list, ensuring they run last.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        model_callbacks = call._call_lightning_module_hook(trainer, \"configure_callbacks\")\r\n        if not model_callbacks:\r\n            return\r\n\r\n        model_callbacks = [model_callbacks] if not isinstance(model_callbacks, Sequence) else model_callbacks\r\n        model_callback_types = {type(c) for c in model_callbacks}\r\n        trainer_callback_types = {type(c) for c in trainer.callbacks}\r\n        # edge case: if an unmodified callback was added, the logic below would filter it\r\n        trainer_callback_types.discard(Callback)\r\n        # exclude trainer callbacks of the same class or subclass\r\n        override_types = set()\r\n        for model_cb in model_callback_types:\r\n            for trainer_cb in trainer_callback_types:\r\n                if issubclass(model_cb, trainer_cb):\r\n                    override_types.add(trainer_cb)\r\n                    break\r\n        if override_types:\r\n            rank_zero_info(\r\n                \"The following callbacks returned in `LightningModule.configure_callbacks` will override\"\r\n                \" existing callbacks passed to Trainer:\"\r\n                f\" {', '.join(sorted(t.__name__ for t in override_types))}\"\r\n            )\r\n        # remove all callbacks with a type that occurs in model callbacks\r\n        all_callbacks = [c for c in trainer.callbacks if type(c) not in override_types]\r\n        all_callbacks.extend(model_callbacks)\r\n        all_callbacks = _CallbackConnector._reorder_callbacks(all_callbacks)\r\n        # TODO: connectors refactor: move callbacks list to connector and do not write Trainer state\r\n        trainer.callbacks = all_callbacks", "code_tokens": ["def", "_attach_model_callbacks", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Attaches", "the", "callbacks", "defined", "in", "the", "model", ".", "If", "a", "callback", "returned", "by", "the", "model", "'", "s", "configure_callback", "method", "has", "the", "same", "type", "as", "one", "or", "several", "callbacks", "already", "present", "in", "the", "trainer", "callbacks", "list", ",", "it", "will", "replace", "them", ".", "In", "addition", ",", "all", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "model_checkpoint", ".", "ModelCheckpoint", "`", "callbacks", "will", "be", "pushed", "to", "the", "end", "of", "the", "list", ",", "ensuring", "they", "run", "last", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "model_callbacks", "=", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "configure_callbacks", "\"", ")", "if", "not", "model_callbacks", ":", "return", "model_callbacks", "=", "[", "model_callbacks", "]", "if", "not", "isinstance", "(", "model_callbacks", ",", "Sequence", ")", "else", "model_callbacks", "model_callback_types", "=", "{", "type", "(", "c", ")", "for", "c", "in", "model_callbacks", "}", "trainer_callback_types", "=", "{", "type", "(", "c", ")", "for", "c", "in", "trainer", ".", "callbacks", "}", "trainer_callback_types", ".", "discard", "(", "Callback", ")", "override_types", "=", "set", "(", ")", "for", "model_cb", "in", "model_callback_types", ":", "for", "trainer_cb", "in", "trainer_callback_types", ":", "if", "issubclass", "(", "model_cb", ",", "trainer_cb", ")", ":", "override_types", ".", "add", "(", "trainer_cb", ")", "break", "if", "override_types", ":", "rank_zero_info", "(", "\"", "The", "following", "callbacks", "returned", "in", "`", "LightningModule", ".", "configure_callbacks", "`", "will", "override", "\"", "\"", "existing", "callbacks", "passed", "to", "Trainer", ":", "\"", "f", "\"", "{", "'", ",", "'", ".", "join", "(", "sorted", "(", "t", ".", "__name__", "for", "t", "in", "override_types", ")", ")", "}", "\"", ")", "all_callbacks", "=", "[", "c", "for", "c", "in", "trainer", ".", "callbacks", "if", "type", "(", "c", ")", "not", "in", "override_types", "]", "all_callbacks", ".", "extend", "(", "model_callbacks", ")", "all_callbacks", "=", "_CallbackConnector", ".", "_reorder_callbacks", "(", "all_callbacks", ")", "trainer", ".", "callbacks", "=", "all_callbacks"], "docstring": "Attaches the callbacks defined in the model.\r\n\r\n        If a callback returned by the model's configure_callback method has the same type as one or several\r\n        callbacks already present in the trainer callbacks list, it will replace them.\r\n        In addition, all :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks\r\n        will be pushed to the end of the list, ensuring they run last.", "docstring_tokens": ["attaches", "the", "callbacks", "defined", "in", "the", "model", "if", "a", "callback", "returned", "by", "the", "model", "s", "configure_callback", "method", "has", "the", "same", "type", "as", "one", "or", "several", "callbacks", "already", "present", "in", "the", "trainer", "callbacks", "list", "it", "will", "replace", "them", "in", "addition", "all", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "callbacks", "will", "be", "pushed", "to", "the", "end", "of", "the", "list", "ensuring", "they", "run", "last"], "docstring_summary": "Attaches the callbacks defined in the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CallbackConnector", "start_line": 172, "end_line": 210, "hash": "fd85567962122afa93c9dc7187810697", "complexity": 12, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "func_name": "_reorder_callbacks", "original_string": "def _reorder_callbacks(callbacks: list[Callback]) -> list[Callback]:\r\n        \"\"\"Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks\r\n        to the end of the list. The sequential order within the group of checkpoint callbacks is preserved, as well as\r\n        the order of all other callbacks.\r\n\r\n        Args:\r\n            callbacks: A list of callbacks.\r\n\r\n        Return:\r\n            A new list in which the first elements are tuner specific callbacks and last elements are ModelCheckpoints\r\n            if there were any present in the input.\r\n\r\n        \"\"\"\r\n        tuner_callbacks: list[Callback] = []\r\n        other_callbacks: list[Callback] = []\r\n        checkpoint_callbacks: list[Callback] = []\r\n\r\n        for cb in callbacks:\r\n            if isinstance(cb, (BatchSizeFinder, LearningRateFinder)):\r\n                tuner_callbacks.append(cb)\r\n            elif isinstance(cb, Checkpoint):\r\n                checkpoint_callbacks.append(cb)\r\n            else:\r\n                other_callbacks.append(cb)\r\n\r\n        return tuner_callbacks + other_callbacks + checkpoint_callbacks", "language": "python", "code": "def _reorder_callbacks(callbacks: list[Callback]) -> list[Callback]:\r\n        \"\"\"Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks\r\n        to the end of the list. The sequential order within the group of checkpoint callbacks is preserved, as well as\r\n        the order of all other callbacks.\r\n\r\n        Args:\r\n            callbacks: A list of callbacks.\r\n\r\n        Return:\r\n            A new list in which the first elements are tuner specific callbacks and last elements are ModelCheckpoints\r\n            if there were any present in the input.\r\n\r\n        \"\"\"\r\n        tuner_callbacks: list[Callback] = []\r\n        other_callbacks: list[Callback] = []\r\n        checkpoint_callbacks: list[Callback] = []\r\n\r\n        for cb in callbacks:\r\n            if isinstance(cb, (BatchSizeFinder, LearningRateFinder)):\r\n                tuner_callbacks.append(cb)\r\n            elif isinstance(cb, Checkpoint):\r\n                checkpoint_callbacks.append(cb)\r\n            else:\r\n                other_callbacks.append(cb)\r\n\r\n        return tuner_callbacks + other_callbacks + checkpoint_callbacks", "code_tokens": ["def", "_reorder_callbacks", "(", "callbacks", ":", "list", "[", "Callback", "]", ")", "-", ">", "list", "[", "Callback", "]", ":", "\"", "\"", "\"", "Moves", "all", "the", "tuner", "specific", "callbacks", "at", "the", "beginning", "of", "the", "list", "and", "all", "the", "`", "ModelCheckpoint", "`", "callbacks", "to", "the", "end", "of", "the", "list", ".", "The", "sequential", "order", "within", "the", "group", "of", "checkpoint", "callbacks", "is", "preserved", ",", "as", "well", "as", "the", "order", "of", "all", "other", "callbacks", ".", "Args", ":", "callbacks", ":", "A", "list", "of", "callbacks", ".", "Return", ":", "A", "new", "list", "in", "which", "the", "first", "elements", "are", "tuner", "specific", "callbacks", "and", "last", "elements", "are", "ModelCheckpoints", "if", "there", "were", "any", "present", "in", "the", "input", ".", "\"", "\"", "\"", "tuner_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "other_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "checkpoint_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "for", "cb", "in", "callbacks", ":", "if", "isinstance", "(", "cb", ",", "(", "BatchSizeFinder", ",", "LearningRateFinder", ")", ")", ":", "tuner_callbacks", ".", "append", "(", "cb", ")", "elif", "isinstance", "(", "cb", ",", "Checkpoint", ")", ":", "checkpoint_callbacks", ".", "append", "(", "cb", ")", "else", ":", "other_callbacks", ".", "append", "(", "cb", ")", "return", "tuner_callbacks", "+", "other_callbacks", "+", "checkpoint_callbacks"], "docstring": "Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks\r\n        to the end of the list. The sequential order within the group of checkpoint callbacks is preserved, as well as\r\n        the order of all other callbacks.\r\n\r\n        Args:\r\n            callbacks: A list of callbacks.\r\n\r\n        Return:\r\n            A new list in which the first elements are tuner specific callbacks and last elements are ModelCheckpoints\r\n            if there were any present in the input.", "docstring_tokens": ["moves", "all", "the", "tuner", "specific", "callbacks", "at", "the", "beginning", "of", "the", "list", "and", "all", "the", "modelcheckpoint", "callbacks", "to", "the", "end", "of", "the", "list", "the", "sequential", "order", "within", "the", "group", "of", "checkpoint", "callbacks", "is", "preserved", "as", "well", "as", "the", "order", "of", "all", "other", "callbacks", "args", "callbacks", "a", "list", "of", "callbacks", "return", "a", "new", "list", "in", "which", "the", "first", "elements", "are", "tuner", "specific", "callbacks", "and", "last", "elements", "are", "modelcheckpoints", "if", "there", "were", "any", "present", "in", "the", "input"], "docstring_summary": "Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CallbackConnector", "start_line": 213, "end_line": 238, "hash": "b7f395acdf70d2d7cdf7195e7ddc85a2", "complexity": 4, "parameters": ["callbacks"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "resume_start", "original_string": "def resume_start(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:\r\n\r\n        1. from HPC weights if `checkpoint_path` is ``None`` and on SLURM or passed keyword `\"hpc\"`.\r\n        2. from fault-tolerant auto-saved checkpoint if found\r\n        3. from `checkpoint_path` file if provided\r\n        4. don't restore\r\n\r\n        \"\"\"\r\n        self._ckpt_path = checkpoint_path\r\n        if not checkpoint_path:\r\n            log.debug(\"`checkpoint_path` not specified. Skipping checkpoint loading.\")\r\n            return\r\n\r\n        rank_zero_info(f\"Restoring states from the checkpoint path at {checkpoint_path}\")\r\n        with pl_legacy_patch():\r\n            loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\r\n        self._loaded_checkpoint = _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)", "language": "python", "code": "def resume_start(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:\r\n\r\n        1. from HPC weights if `checkpoint_path` is ``None`` and on SLURM or passed keyword `\"hpc\"`.\r\n        2. from fault-tolerant auto-saved checkpoint if found\r\n        3. from `checkpoint_path` file if provided\r\n        4. don't restore\r\n\r\n        \"\"\"\r\n        self._ckpt_path = checkpoint_path\r\n        if not checkpoint_path:\r\n            log.debug(\"`checkpoint_path` not specified. Skipping checkpoint loading.\")\r\n            return\r\n\r\n        rank_zero_info(f\"Restoring states from the checkpoint path at {checkpoint_path}\")\r\n        with pl_legacy_patch():\r\n            loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\r\n        self._loaded_checkpoint = _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)", "code_tokens": ["def", "resume_start", "(", "self", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Attempts", "to", "pre", "-", "load", "the", "checkpoint", "file", "to", "memory", ",", "with", "the", "source", "path", "determined", "in", "this", "priority", ":", "1", ".", "from", "HPC", "weights", "if", "`", "checkpoint_path", "`", "is", "`", "`", "None", "`", "`", "and", "on", "SLURM", "or", "passed", "keyword", "`", "\"", "hpc", "\"", "`", ".", "2", ".", "from", "fault", "-", "tolerant", "auto", "-", "saved", "checkpoint", "if", "found", "3", ".", "from", "`", "checkpoint_path", "`", "file", "if", "provided", "4", ".", "don", "'", "t", "restore", "\"", "\"", "\"", "self", ".", "_ckpt_path", "=", "checkpoint_path", "if", "not", "checkpoint_path", ":", "log", ".", "debug", "(", "\"", "`", "checkpoint_path", "`", "not", "specified", ".", "Skipping", "checkpoint", "loading", ".", "\"", ")", "return", "rank_zero_info", "(", "f", "\"", "Restoring", "states", "from", "the", "checkpoint", "path", "at", "{", "checkpoint_path", "}", "\"", ")", "with", "pl_legacy_patch", "(", ")", ":", "loaded_checkpoint", "=", "self", ".", "trainer", ".", "strategy", ".", "load_checkpoint", "(", "checkpoint_path", ")", "self", ".", "_loaded_checkpoint", "=", "_pl_migrate_checkpoint", "(", "loaded_checkpoint", ",", "checkpoint_path", ")"], "docstring": "Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:\r\n\r\n        1. from HPC weights if `checkpoint_path` is ``None`` and on SLURM or passed keyword `\"hpc\"`.\r\n        2. from fault-tolerant auto-saved checkpoint if found\r\n        3. from `checkpoint_path` file if provided\r\n        4. don't restore", "docstring_tokens": ["attempts", "to", "pre", "load", "the", "checkpoint", "file", "to", "memory", "with", "the", "source", "path", "determined", "in", "this", "priority", "1", "from", "hpc", "weights", "if", "checkpoint_path", "is", "none", "and", "on", "slurm", "or", "passed", "keyword", "hpc", "2", "from", "fault", "tolerant", "auto", "saved", "checkpoint", "if", "found", "3", "from", "checkpoint_path", "file", "if", "provided", "4", "don", "t", "restore"], "docstring_summary": "Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 66, "end_line": 83, "hash": "54110e56ba64b0f0378680321d276e78", "complexity": 3, "parameters": ["checkpoint_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "_select_ckpt_path", "original_string": "def _select_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Called by the ``Trainer`` to select the checkpoint path source.\"\"\"\r\n        if self._user_managed:\r\n            if ckpt_path:\r\n                rank_zero_warn(\r\n                    f\"`trainer.ckpt_path = {self._ckpt_path!r}` was called but then you\"\r\n                    f\" passed `trainer.fit(ckpt_path={ckpt_path!r})`. The latter will be loaded.\"\r\n                )\r\n                # reset the previous path\r\n                self._ckpt_path = None\r\n                self._user_managed = False\r\n                ckpt_path = self._parse_ckpt_path(\r\n                    state_fn,\r\n                    ckpt_path,\r\n                    model_provided=model_provided,\r\n                    model_connected=model_connected,\r\n                )\r\n            else:\r\n                ckpt_path = self._ckpt_path\r\n        else:\r\n            ckpt_path = self._parse_ckpt_path(\r\n                state_fn,\r\n                ckpt_path,\r\n                model_provided=model_provided,\r\n                model_connected=model_connected,\r\n            )\r\n        return ckpt_path", "language": "python", "code": "def _select_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Called by the ``Trainer`` to select the checkpoint path source.\"\"\"\r\n        if self._user_managed:\r\n            if ckpt_path:\r\n                rank_zero_warn(\r\n                    f\"`trainer.ckpt_path = {self._ckpt_path!r}` was called but then you\"\r\n                    f\" passed `trainer.fit(ckpt_path={ckpt_path!r})`. The latter will be loaded.\"\r\n                )\r\n                # reset the previous path\r\n                self._ckpt_path = None\r\n                self._user_managed = False\r\n                ckpt_path = self._parse_ckpt_path(\r\n                    state_fn,\r\n                    ckpt_path,\r\n                    model_provided=model_provided,\r\n                    model_connected=model_connected,\r\n                )\r\n            else:\r\n                ckpt_path = self._ckpt_path\r\n        else:\r\n            ckpt_path = self._parse_ckpt_path(\r\n                state_fn,\r\n                ckpt_path,\r\n                model_provided=model_provided,\r\n                model_connected=model_connected,\r\n            )\r\n        return ckpt_path", "code_tokens": ["def", "_select_ckpt_path", "(", "self", ",", "state_fn", ":", "TrainerFn", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "model_provided", ":", "bool", ",", "model_connected", ":", "bool", ")", "-", ">", "Optional", "[", "_PATH", "]", ":", "\"", "\"", "\"", "Called", "by", "the", "`", "`", "Trainer", "`", "`", "to", "select", "the", "checkpoint", "path", "source", ".", "\"", "\"", "\"", "if", "self", ".", "_user_managed", ":", "if", "ckpt_path", ":", "rank_zero_warn", "(", "f", "\"", "`", "trainer", ".", "ckpt_path", "=", "{", "self", ".", "_ckpt_path", "!", "r", "}", "`", "was", "called", "but", "then", "you", "\"", "f", "\"", "passed", "`", "trainer", ".", "fit", "(", "ckpt_path", "=", "{", "ckpt_path", "!", "r", "}", ")", "`", ".", "The", "latter", "will", "be", "loaded", ".", "\"", ")", "self", ".", "_ckpt_path", "=", "None", "self", ".", "_user_managed", "=", "False", "ckpt_path", "=", "self", ".", "_parse_ckpt_path", "(", "state_fn", ",", "ckpt_path", ",", "model_provided", "=", "model_provided", ",", "model_connected", "=", "model_connected", ",", ")", "else", ":", "ckpt_path", "=", "self", ".", "_ckpt_path", "else", ":", "ckpt_path", "=", "self", ".", "_parse_ckpt_path", "(", "state_fn", ",", "ckpt_path", ",", "model_provided", "=", "model_provided", ",", "model_connected", "=", "model_connected", ",", ")", "return", "ckpt_path"], "docstring": "Called by the ``Trainer`` to select the checkpoint path source.", "docstring_tokens": ["called", "by", "the", "trainer", "to", "select", "the", "checkpoint", "path", "source"], "docstring_summary": "Called by the ``Trainer`` to select the checkpoint path source.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 85, "end_line": 113, "hash": "9ff094c6b069756629577d7c5d7b4e00", "complexity": 3, "parameters": ["state_fn", "ckpt_path", "model_provided", "model_connected"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "resume_end", "original_string": "def resume_end(self) -> None:\r\n        \"\"\"Signal the connector that all states have resumed and memory for the checkpoint object can be released.\"\"\"\r\n        assert self.trainer.state.fn is not None\r\n        if self._ckpt_path:\r\n            message = \"Restored all states\" if self.trainer.state.fn == TrainerFn.FITTING else \"Loaded model weights\"\r\n            rank_zero_info(f\"{message} from the checkpoint at {self._ckpt_path}\")\r\n\r\n        # free memory\r\n        self._loaded_checkpoint = {}\r\n        torch.cuda.empty_cache()\r\n\r\n        # wait for all to catch up\r\n        self.trainer.strategy.barrier(\"_CheckpointConnector.resume_end\")", "language": "python", "code": "def resume_end(self) -> None:\r\n        \"\"\"Signal the connector that all states have resumed and memory for the checkpoint object can be released.\"\"\"\r\n        assert self.trainer.state.fn is not None\r\n        if self._ckpt_path:\r\n            message = \"Restored all states\" if self.trainer.state.fn == TrainerFn.FITTING else \"Loaded model weights\"\r\n            rank_zero_info(f\"{message} from the checkpoint at {self._ckpt_path}\")\r\n\r\n        # free memory\r\n        self._loaded_checkpoint = {}\r\n        torch.cuda.empty_cache()\r\n\r\n        # wait for all to catch up\r\n        self.trainer.strategy.barrier(\"_CheckpointConnector.resume_end\")", "code_tokens": ["def", "resume_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Signal", "the", "connector", "that", "all", "states", "have", "resumed", "and", "memory", "for", "the", "checkpoint", "object", "can", "be", "released", ".", "\"", "\"", "\"", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "if", "self", ".", "_ckpt_path", ":", "message", "=", "\"", "Restored", "all", "states", "\"", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "else", "\"", "Loaded", "model", "weights", "\"", "rank_zero_info", "(", "f", "\"", "{", "message", "}", "from", "the", "checkpoint", "at", "{", "self", ".", "_ckpt_path", "}", "\"", ")", "self", ".", "_loaded_checkpoint", "=", "{", "}", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "self", ".", "trainer", ".", "strategy", ".", "barrier", "(", "\"", "_CheckpointConnector", ".", "resume_end", "\"", ")"], "docstring": "Signal the connector that all states have resumed and memory for the checkpoint object can be released.", "docstring_tokens": ["signal", "the", "connector", "that", "all", "states", "have", "resumed", "and", "memory", "for", "the", "checkpoint", "object", "can", "be", "released"], "docstring_summary": "Signal the connector that all states have resumed and memory for the checkpoint object can be released.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 218, "end_line": 230, "hash": "b4c26dde96398593dc3bae6abde888f1", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore", "original_string": "def restore(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\r\n        state-restore, in this priority:\r\n\r\n        1. from HPC weights if found\r\n        2. from `checkpoint_path` file if provided\r\n        3. don't restore\r\n\r\n        All restored states are listed in return value description of `dump_checkpoint`.\r\n\r\n        Args:\r\n            checkpoint_path: Path to a PyTorch Lightning checkpoint file.\r\n\r\n        \"\"\"\r\n        self.resume_start(checkpoint_path)\r\n\r\n        # restore module states\r\n        self.restore_datamodule()\r\n        self.restore_model()\r\n\r\n        # restore callback states\r\n        self.restore_callbacks()\r\n\r\n        # restore training state\r\n        self.restore_training_state()\r\n        self.resume_end()", "language": "python", "code": "def restore(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\r\n        state-restore, in this priority:\r\n\r\n        1. from HPC weights if found\r\n        2. from `checkpoint_path` file if provided\r\n        3. don't restore\r\n\r\n        All restored states are listed in return value description of `dump_checkpoint`.\r\n\r\n        Args:\r\n            checkpoint_path: Path to a PyTorch Lightning checkpoint file.\r\n\r\n        \"\"\"\r\n        self.resume_start(checkpoint_path)\r\n\r\n        # restore module states\r\n        self.restore_datamodule()\r\n        self.restore_model()\r\n\r\n        # restore callback states\r\n        self.restore_callbacks()\r\n\r\n        # restore training state\r\n        self.restore_training_state()\r\n        self.resume_end()", "code_tokens": ["def", "restore", "(", "self", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Attempt", "to", "restore", "everything", "at", "once", "from", "a", "'", "PyTorch", "-", "Lightning", "checkpoint", "'", "file", "through", "file", "-", "read", "and", "state", "-", "restore", ",", "in", "this", "priority", ":", "1", ".", "from", "HPC", "weights", "if", "found", "2", ".", "from", "`", "checkpoint_path", "`", "file", "if", "provided", "3", ".", "don", "'", "t", "restore", "All", "restored", "states", "are", "listed", "in", "return", "value", "description", "of", "`", "dump_checkpoint", "`", ".", "Args", ":", "checkpoint_path", ":", "Path", "to", "a", "PyTorch", "Lightning", "checkpoint", "file", ".", "\"", "\"", "\"", "self", ".", "resume_start", "(", "checkpoint_path", ")", "self", ".", "restore_datamodule", "(", ")", "self", ".", "restore_model", "(", ")", "self", ".", "restore_callbacks", "(", ")", "self", ".", "restore_training_state", "(", ")", "self", ".", "resume_end", "(", ")"], "docstring": "Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\r\n        state-restore, in this priority:\r\n\r\n        1. from HPC weights if found\r\n        2. from `checkpoint_path` file if provided\r\n        3. don't restore\r\n\r\n        All restored states are listed in return value description of `dump_checkpoint`.\r\n\r\n        Args:\r\n            checkpoint_path: Path to a PyTorch Lightning checkpoint file.", "docstring_tokens": ["attempt", "to", "restore", "everything", "at", "once", "from", "a", "pytorch", "lightning", "checkpoint", "file", "through", "file", "read", "and", "state", "restore", "in", "this", "priority", "1", "from", "hpc", "weights", "if", "found", "2", "from", "checkpoint_path", "file", "if", "provided", "3", "don", "t", "restore", "all", "restored", "states", "are", "listed", "in", "return", "value", "description", "of", "dump_checkpoint", "args", "checkpoint_path", "path", "to", "a", "pytorch", "lightning", "checkpoint", "file"], "docstring_summary": "Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 232, "end_line": 257, "hash": "fe6acda406a2833a5351cc1d5acf81c4", "complexity": 1, "parameters": ["checkpoint_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_datamodule", "original_string": "def restore_datamodule(self) -> None:\r\n        \"\"\"Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        datamodule = trainer.datamodule\r\n        if datamodule is not None and datamodule.__class__.__qualname__ in self._loaded_checkpoint:\r\n            call._call_lightning_datamodule_hook(\r\n                trainer, \"load_state_dict\", self._loaded_checkpoint[datamodule.__class__.__qualname__]\r\n            )", "language": "python", "code": "def restore_datamodule(self) -> None:\r\n        \"\"\"Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        datamodule = trainer.datamodule\r\n        if datamodule is not None and datamodule.__class__.__qualname__ in self._loaded_checkpoint:\r\n            call._call_lightning_datamodule_hook(\r\n                trainer, \"load_state_dict\", self._loaded_checkpoint[datamodule.__class__.__qualname__]\r\n            )", "code_tokens": ["def", "restore_datamodule", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "hooks", "on", "the", "datamodule", "to", "give", "it", "a", "chance", "to", "restore", "its", "state", "from", "the", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "trainer", "=", "self", ".", "trainer", "datamodule", "=", "trainer", ".", "datamodule", "if", "datamodule", "is", "not", "None", "and", "datamodule", ".", "__class__", ".", "__qualname__", "in", "self", ".", "_loaded_checkpoint", ":", "call", ".", "_call_lightning_datamodule_hook", "(", "trainer", ",", "\"", "load_state_dict", "\"", ",", "self", ".", "_loaded_checkpoint", "[", "datamodule", ".", "__class__", ".", "__qualname__", "]", ")"], "docstring": "Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.", "docstring_tokens": ["calls", "hooks", "on", "the", "datamodule", "to", "give", "it", "a", "chance", "to", "restore", "its", "state", "from", "the", "checkpoint"], "docstring_summary": "Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 259, "end_line": 269, "hash": "d4c2ed1bcd62b35370377515db6352a7", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_model", "original_string": "def restore_model(self) -> None:\r\n        \"\"\"Restores a model's weights from a PyTorch Lightning checkpoint.\r\n\r\n        Hooks are called first to give the LightningModule a chance to modify the contents, then finally the model gets\r\n        updated with the loaded weights.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # hook: give user access to checkpoint if needed.\r\n        call._call_lightning_module_hook(self.trainer, \"on_load_checkpoint\", self._loaded_checkpoint)\r\n\r\n        # restore model state_dict\r\n        self.trainer.strategy.load_model_state_dict(\r\n            self._loaded_checkpoint,\r\n            strict=self.trainer.lightning_module.strict_loading,\r\n        )", "language": "python", "code": "def restore_model(self) -> None:\r\n        \"\"\"Restores a model's weights from a PyTorch Lightning checkpoint.\r\n\r\n        Hooks are called first to give the LightningModule a chance to modify the contents, then finally the model gets\r\n        updated with the loaded weights.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # hook: give user access to checkpoint if needed.\r\n        call._call_lightning_module_hook(self.trainer, \"on_load_checkpoint\", self._loaded_checkpoint)\r\n\r\n        # restore model state_dict\r\n        self.trainer.strategy.load_model_state_dict(\r\n            self._loaded_checkpoint,\r\n            strict=self.trainer.lightning_module.strict_loading,\r\n        )", "code_tokens": ["def", "restore_model", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "a", "model", "'", "s", "weights", "from", "a", "PyTorch", "Lightning", "checkpoint", ".", "Hooks", "are", "called", "first", "to", "give", "the", "LightningModule", "a", "chance", "to", "modify", "the", "contents", ",", "then", "finally", "the", "model", "gets", "updated", "with", "the", "loaded", "weights", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "call", ".", "_call_lightning_module_hook", "(", "self", ".", "trainer", ",", "\"", "on_load_checkpoint", "\"", ",", "self", ".", "_loaded_checkpoint", ")", "self", ".", "trainer", ".", "strategy", ".", "load_model_state_dict", "(", "self", ".", "_loaded_checkpoint", ",", "strict", "=", "self", ".", "trainer", ".", "lightning_module", ".", "strict_loading", ",", ")"], "docstring": "Restores a model's weights from a PyTorch Lightning checkpoint.\r\n\r\n        Hooks are called first to give the LightningModule a chance to modify the contents, then finally the model gets\r\n        updated with the loaded weights.", "docstring_tokens": ["restores", "a", "model", "s", "weights", "from", "a", "pytorch", "lightning", "checkpoint", "hooks", "are", "called", "first", "to", "give", "the", "lightningmodule", "a", "chance", "to", "modify", "the", "contents", "then", "finally", "the", "model", "gets", "updated", "with", "the", "loaded", "weights"], "docstring_summary": "Restores a model's weights from a PyTorch Lightning checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 271, "end_line": 288, "hash": "296c239bfb04a5dee31819fe6f8d9803", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_training_state", "original_string": "def restore_training_state(self) -> None:\r\n        \"\"\"Restore the trainer state from the pre-loaded checkpoint.\r\n\r\n        This includes the precision settings, loop progress, optimizer states and learning rate scheduler states.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore precision plugin (scaler etc.)\r\n        self.restore_precision_plugin_state()\r\n\r\n        # restore loops and their progress\r\n        self.restore_loops()\r\n\r\n        assert self.trainer.state.fn is not None\r\n        if self.trainer.state.fn == TrainerFn.FITTING:\r\n            # restore optimizers and schedulers state\r\n            self.restore_optimizers_and_schedulers()", "language": "python", "code": "def restore_training_state(self) -> None:\r\n        \"\"\"Restore the trainer state from the pre-loaded checkpoint.\r\n\r\n        This includes the precision settings, loop progress, optimizer states and learning rate scheduler states.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore precision plugin (scaler etc.)\r\n        self.restore_precision_plugin_state()\r\n\r\n        # restore loops and their progress\r\n        self.restore_loops()\r\n\r\n        assert self.trainer.state.fn is not None\r\n        if self.trainer.state.fn == TrainerFn.FITTING:\r\n            # restore optimizers and schedulers state\r\n            self.restore_optimizers_and_schedulers()", "code_tokens": ["def", "restore_training_state", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restore", "the", "trainer", "state", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "This", "includes", "the", "precision", "settings", ",", "loop", "progress", ",", "optimizer", "states", "and", "learning", "rate", "scheduler", "states", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "self", ".", "restore_precision_plugin_state", "(", ")", "self", ".", "restore_loops", "(", ")", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "restore_optimizers_and_schedulers", "(", ")"], "docstring": "Restore the trainer state from the pre-loaded checkpoint.\r\n\r\n        This includes the precision settings, loop progress, optimizer states and learning rate scheduler states.", "docstring_tokens": ["restore", "the", "trainer", "state", "from", "the", "pre", "loaded", "checkpoint", "this", "includes", "the", "precision", "settings", "loop", "progress", "optimizer", "states", "and", "learning", "rate", "scheduler", "states"], "docstring_summary": "Restore the trainer state from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 290, "end_line": 308, "hash": "406e31011e9f86ff5102b379eb4511c4", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_precision_plugin_state", "original_string": "def restore_precision_plugin_state(self) -> None:\r\n        \"\"\"Restore the precision plugin state from the pre-loaded checkpoint.\"\"\"\r\n        prec_plugin = self.trainer.precision_plugin\r\n        prec_plugin.on_load_checkpoint(self._loaded_checkpoint)\r\n        if prec_plugin.__class__.__qualname__ in self._loaded_checkpoint:\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[prec_plugin.__class__.__qualname__])\r\n\r\n        # old checkpoints compatibility\r\n        if \"native_amp_scaling_state\" in self._loaded_checkpoint and isinstance(prec_plugin, MixedPrecision):\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[\"native_amp_scaling_state\"])", "language": "python", "code": "def restore_precision_plugin_state(self) -> None:\r\n        \"\"\"Restore the precision plugin state from the pre-loaded checkpoint.\"\"\"\r\n        prec_plugin = self.trainer.precision_plugin\r\n        prec_plugin.on_load_checkpoint(self._loaded_checkpoint)\r\n        if prec_plugin.__class__.__qualname__ in self._loaded_checkpoint:\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[prec_plugin.__class__.__qualname__])\r\n\r\n        # old checkpoints compatibility\r\n        if \"native_amp_scaling_state\" in self._loaded_checkpoint and isinstance(prec_plugin, MixedPrecision):\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[\"native_amp_scaling_state\"])", "code_tokens": ["def", "restore_precision_plugin_state", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restore", "the", "precision", "plugin", "state", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "\"", "\"", "\"", "prec_plugin", "=", "self", ".", "trainer", ".", "precision_plugin", "prec_plugin", ".", "on_load_checkpoint", "(", "self", ".", "_loaded_checkpoint", ")", "if", "prec_plugin", ".", "__class__", ".", "__qualname__", "in", "self", ".", "_loaded_checkpoint", ":", "prec_plugin", ".", "load_state_dict", "(", "self", ".", "_loaded_checkpoint", "[", "prec_plugin", ".", "__class__", ".", "__qualname__", "]", ")", "if", "\"", "native_amp_scaling_state", "\"", "in", "self", ".", "_loaded_checkpoint", "and", "isinstance", "(", "prec_plugin", ",", "MixedPrecision", ")", ":", "prec_plugin", ".", "load_state_dict", "(", "self", ".", "_loaded_checkpoint", "[", "\"", "native_amp_scaling_state", "\"", "]", ")"], "docstring": "Restore the precision plugin state from the pre-loaded checkpoint.", "docstring_tokens": ["restore", "the", "precision", "plugin", "state", "from", "the", "pre", "loaded", "checkpoint"], "docstring_summary": "Restore the precision plugin state from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 310, "end_line": 319, "hash": "0a4d41b2755abbf99f7f57861453fe4f", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_callbacks", "original_string": "def restore_callbacks(self) -> None:\r\n        \"\"\"Restores all callbacks from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        call._call_callbacks_on_load_checkpoint(trainer, self._loaded_checkpoint)\r\n        call._call_callbacks_load_state_dict(trainer, self._loaded_checkpoint)", "language": "python", "code": "def restore_callbacks(self) -> None:\r\n        \"\"\"Restores all callbacks from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        call._call_callbacks_on_load_checkpoint(trainer, self._loaded_checkpoint)\r\n        call._call_callbacks_load_state_dict(trainer, self._loaded_checkpoint)", "code_tokens": ["def", "restore_callbacks", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "all", "callbacks", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callbacks_on_load_checkpoint", "(", "trainer", ",", "self", ".", "_loaded_checkpoint", ")", "call", ".", "_call_callbacks_load_state_dict", "(", "trainer", ",", "self", ".", "_loaded_checkpoint", ")"], "docstring": "Restores all callbacks from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "all", "callbacks", "from", "the", "pre", "loaded", "checkpoint"], "docstring_summary": "Restores all callbacks from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 321, "end_line": 328, "hash": "1cce48331eaa3190ef175a67d547be18", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_loops", "original_string": "def restore_loops(self) -> None:\r\n        \"\"\"Restores the loop progress from the pre-loaded checkpoint.\r\n\r\n        Calls hooks on the loops to give it a chance to restore its state from the checkpoint.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        fit_loop = self.trainer.fit_loop\r\n        assert self.trainer.state.fn is not None\r\n        state_dict = self._loaded_checkpoint.get(\"loops\")\r\n        if state_dict is not None:\r\n            if self.trainer.state.fn == TrainerFn.FITTING:\r\n                fit_loop.load_state_dict(state_dict[\"fit_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.VALIDATING:\r\n                self.trainer.validate_loop.load_state_dict(state_dict[\"validate_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.TESTING:\r\n                self.trainer.test_loop.load_state_dict(state_dict[\"test_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.PREDICTING:\r\n                self.trainer.predict_loop.load_state_dict(state_dict[\"predict_loop\"])\r\n\r\n        if self.trainer.state.fn != TrainerFn.FITTING:\r\n            return\r\n\r\n        # crash if max_epochs is lower then the current epoch from the checkpoint\r\n        if (\r\n            self.trainer.max_epochs != -1\r\n            and self.trainer.max_epochs is not None\r\n            and self.trainer.current_epoch > self.trainer.max_epochs\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"You restored a checkpoint with current_epoch={self.trainer.current_epoch},\"\r\n                f\" but you have set Trainer(max_epochs={self.trainer.max_epochs}).\"\r\n            )", "language": "python", "code": "def restore_loops(self) -> None:\r\n        \"\"\"Restores the loop progress from the pre-loaded checkpoint.\r\n\r\n        Calls hooks on the loops to give it a chance to restore its state from the checkpoint.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        fit_loop = self.trainer.fit_loop\r\n        assert self.trainer.state.fn is not None\r\n        state_dict = self._loaded_checkpoint.get(\"loops\")\r\n        if state_dict is not None:\r\n            if self.trainer.state.fn == TrainerFn.FITTING:\r\n                fit_loop.load_state_dict(state_dict[\"fit_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.VALIDATING:\r\n                self.trainer.validate_loop.load_state_dict(state_dict[\"validate_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.TESTING:\r\n                self.trainer.test_loop.load_state_dict(state_dict[\"test_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.PREDICTING:\r\n                self.trainer.predict_loop.load_state_dict(state_dict[\"predict_loop\"])\r\n\r\n        if self.trainer.state.fn != TrainerFn.FITTING:\r\n            return\r\n\r\n        # crash if max_epochs is lower then the current epoch from the checkpoint\r\n        if (\r\n            self.trainer.max_epochs != -1\r\n            and self.trainer.max_epochs is not None\r\n            and self.trainer.current_epoch > self.trainer.max_epochs\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"You restored a checkpoint with current_epoch={self.trainer.current_epoch},\"\r\n                f\" but you have set Trainer(max_epochs={self.trainer.max_epochs}).\"\r\n            )", "code_tokens": ["def", "restore_loops", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "the", "loop", "progress", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "Calls", "hooks", "on", "the", "loops", "to", "give", "it", "a", "chance", "to", "restore", "its", "state", "from", "the", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "fit_loop", "=", "self", ".", "trainer", ".", "fit_loop", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "state_dict", "=", "self", ".", "_loaded_checkpoint", ".", "get", "(", "\"", "loops", "\"", ")", "if", "state_dict", "is", "not", "None", ":", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "fit_loop", ".", "load_state_dict", "(", "state_dict", "[", "\"", "fit_loop", "\"", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "self", ".", "trainer", ".", "validate_loop", ".", "load_state_dict", "(", "state_dict", "[", "\"", "validate_loop", "\"", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "TESTING", ":", "self", ".", "trainer", ".", "test_loop", ".", "load_state_dict", "(", "state_dict", "[", "\"", "test_loop", "\"", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "PREDICTING", ":", "self", ".", "trainer", ".", "predict_loop", ".", "load_state_dict", "(", "state_dict", "[", "\"", "predict_loop", "\"", "]", ")", "if", "self", ".", "trainer", ".", "state", ".", "fn", "!", "=", "TrainerFn", ".", "FITTING", ":", "return", "if", "(", "self", ".", "trainer", ".", "max_epochs", "!", "=", "-", "1", "and", "self", ".", "trainer", ".", "max_epochs", "is", "not", "None", "and", "self", ".", "trainer", ".", "current_epoch", ">", "self", ".", "trainer", ".", "max_epochs", ")", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "restored", "a", "checkpoint", "with", "current_epoch", "=", "{", "self", ".", "trainer", ".", "current_epoch", "}", ",", "\"", "f", "\"", "but", "you", "have", "set", "Trainer", "(", "max_epochs", "=", "{", "self", ".", "trainer", ".", "max_epochs", "}", ")", ".", "\"", ")"], "docstring": "Restores the loop progress from the pre-loaded checkpoint.\r\n\r\n        Calls hooks on the loops to give it a chance to restore its state from the checkpoint.", "docstring_tokens": ["restores", "the", "loop", "progress", "from", "the", "pre", "loaded", "checkpoint", "calls", "hooks", "on", "the", "loops", "to", "give", "it", "a", "chance", "to", "restore", "its", "state", "from", "the", "checkpoint"], "docstring_summary": "Restores the loop progress from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 330, "end_line": 364, "hash": "a846cb0df12add1d085b226a03a01514", "complexity": 11, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_optimizers_and_schedulers", "original_string": "def restore_optimizers_and_schedulers(self) -> None:\r\n        \"\"\"Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        if self.trainer.strategy.lightning_restore_optimizer:\r\n            # validation\r\n            if \"optimizer_states\" not in self._loaded_checkpoint:\r\n                raise KeyError(\r\n                    \"Trying to restore optimizer state but checkpoint contains only the model.\"\r\n                    \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n                )\r\n            self.restore_optimizers()\r\n\r\n        if \"lr_schedulers\" not in self._loaded_checkpoint:\r\n            raise KeyError(\r\n                \"Trying to restore learning rate scheduler state but checkpoint contains only the model.\"\r\n                \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n            )\r\n        self.restore_lr_schedulers()", "language": "python", "code": "def restore_optimizers_and_schedulers(self) -> None:\r\n        \"\"\"Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        if self.trainer.strategy.lightning_restore_optimizer:\r\n            # validation\r\n            if \"optimizer_states\" not in self._loaded_checkpoint:\r\n                raise KeyError(\r\n                    \"Trying to restore optimizer state but checkpoint contains only the model.\"\r\n                    \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n                )\r\n            self.restore_optimizers()\r\n\r\n        if \"lr_schedulers\" not in self._loaded_checkpoint:\r\n            raise KeyError(\r\n                \"Trying to restore learning rate scheduler state but checkpoint contains only the model.\"\r\n                \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n            )\r\n        self.restore_lr_schedulers()", "code_tokens": ["def", "restore_optimizers_and_schedulers", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "the", "optimizers", "and", "learning", "rate", "scheduler", "states", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "if", "self", ".", "trainer", ".", "strategy", ".", "lightning_restore_optimizer", ":", "if", "\"", "optimizer_states", "\"", "not", "in", "self", ".", "_loaded_checkpoint", ":", "raise", "KeyError", "(", "\"", "Trying", "to", "restore", "optimizer", "state", "but", "checkpoint", "contains", "only", "the", "model", ".", "\"", "\"", "This", "is", "probably", "due", "to", "`", "ModelCheckpoint", ".", "save_weights_only", "`", "being", "set", "to", "`", "True", "`", ".", "\"", ")", "self", ".", "restore_optimizers", "(", ")", "if", "\"", "lr_schedulers", "\"", "not", "in", "self", ".", "_loaded_checkpoint", ":", "raise", "KeyError", "(", "\"", "Trying", "to", "restore", "learning", "rate", "scheduler", "state", "but", "checkpoint", "contains", "only", "the", "model", ".", "\"", "\"", "This", "is", "probably", "due", "to", "`", "ModelCheckpoint", ".", "save_weights_only", "`", "being", "set", "to", "`", "True", "`", ".", "\"", ")", "self", ".", "restore_lr_schedulers", "(", ")"], "docstring": "Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "optimizers", "and", "learning", "rate", "scheduler", "states", "from", "the", "pre", "loaded", "checkpoint"], "docstring_summary": "Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 366, "end_line": 385, "hash": "2e2eb63571be030fcd2a02fe7d6ec67c", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_optimizers", "original_string": "def restore_optimizers(self) -> None:\r\n        \"\"\"Restores the optimizer states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore the optimizers\r\n        self.trainer.strategy.load_optimizer_state_dict(self._loaded_checkpoint)", "language": "python", "code": "def restore_optimizers(self) -> None:\r\n        \"\"\"Restores the optimizer states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore the optimizers\r\n        self.trainer.strategy.load_optimizer_state_dict(self._loaded_checkpoint)", "code_tokens": ["def", "restore_optimizers", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "the", "optimizer", "states", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "self", ".", "trainer", ".", "strategy", ".", "load_optimizer_state_dict", "(", "self", ".", "_loaded_checkpoint", ")"], "docstring": "Restores the optimizer states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "optimizer", "states", "from", "the", "pre", "loaded", "checkpoint"], "docstring_summary": "Restores the optimizer states from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 387, "end_line": 393, "hash": "66a9b1d72cfa4ed196eb1321f401acd2", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "restore_lr_schedulers", "original_string": "def restore_lr_schedulers(self) -> None:\r\n        \"\"\"Restores the learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore the lr schedulers\r\n        lr_schedulers = self._loaded_checkpoint[\"lr_schedulers\"]\r\n        for config, lrs_state in zip(self.trainer.lr_scheduler_configs, lr_schedulers):\r\n            config.scheduler.load_state_dict(lrs_state)", "language": "python", "code": "def restore_lr_schedulers(self) -> None:\r\n        \"\"\"Restores the learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        # restore the lr schedulers\r\n        lr_schedulers = self._loaded_checkpoint[\"lr_schedulers\"]\r\n        for config, lrs_state in zip(self.trainer.lr_scheduler_configs, lr_schedulers):\r\n            config.scheduler.load_state_dict(lrs_state)", "code_tokens": ["def", "restore_lr_schedulers", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "the", "learning", "rate", "scheduler", "states", "from", "the", "pre", "-", "loaded", "checkpoint", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "lr_schedulers", "=", "self", ".", "_loaded_checkpoint", "[", "\"", "lr_schedulers", "\"", "]", "for", "config", ",", "lrs_state", "in", "zip", "(", "self", ".", "trainer", ".", "lr_scheduler_configs", ",", "lr_schedulers", ")", ":", "config", ".", "scheduler", ".", "load_state_dict", "(", "lrs_state", ")"], "docstring": "Restores the learning rate scheduler states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "learning", "rate", "scheduler", "states", "from", "the", "pre", "loaded", "checkpoint"], "docstring_summary": "Restores the learning rate scheduler states from the pre-loaded checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 395, "end_line": 403, "hash": "dbd835579800f0eba4b1e1885c796cbf", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "dump_checkpoint", "original_string": "def dump_checkpoint(self, weights_only: bool = False) -> dict:\r\n        \"\"\"Creating a model checkpoint dictionary object from various component states.\r\n\r\n        Args:\r\n            weights_only: saving model weights only\r\n        Return:\r\n            structured dictionary: {\r\n                'epoch':                     training epoch\r\n                'global_step':               training global step\r\n                'pytorch-lightning_version': The version of PyTorch Lightning that produced this checkpoint\r\n                'callbacks':                 \"callback specific state\"[] # if not weights_only\r\n                'optimizer_states':          \"PT optim's state_dict\"[]   # if not weights_only\r\n                'lr_schedulers':             \"PT sched's state_dict\"[]   # if not weights_only\r\n                'state_dict':                Model's state_dict (e.g. network weights)\r\n                precision_plugin.__class__.__qualname__:  precision plugin state_dict # if not weights_only\r\n                CHECKPOINT_HYPER_PARAMS_NAME:\r\n                CHECKPOINT_HYPER_PARAMS_KEY:\r\n                CHECKPOINT_HYPER_PARAMS_TYPE:\r\n                something_cool_i_want_to_save: anything you define through model.on_save_checkpoint\r\n                LightningDataModule.__class__.__qualname__: pl DataModule's state\r\n            }\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        model = trainer.lightning_module\r\n        datamodule = trainer.datamodule\r\n\r\n        checkpoint = {\r\n            # the epoch and global step are saved for compatibility but they are not relevant for restoration\r\n            \"epoch\": trainer.current_epoch,\r\n            \"global_step\": trainer.global_step,\r\n            \"pytorch-lightning_version\": pl.__version__,\r\n            \"state_dict\": self._get_lightning_module_state_dict(),\r\n            \"loops\": self._get_loops_state_dict(),\r\n        }\r\n\r\n        if not weights_only:\r\n            # dump callbacks\r\n            checkpoint[\"callbacks\"] = call._call_callbacks_state_dict(trainer)\r\n\r\n            optimizer_states = []\r\n            for i, optimizer in enumerate(trainer.optimizers):\r\n                # Rely on accelerator to dump optimizer state\r\n                optimizer_state = trainer.strategy.optimizer_state(optimizer)\r\n                optimizer_states.append(optimizer_state)\r\n\r\n            checkpoint[\"optimizer_states\"] = optimizer_states\r\n\r\n            # dump lr schedulers\r\n            lr_schedulers = []\r\n            for config in trainer.lr_scheduler_configs:\r\n                lr_schedulers.append(config.scheduler.state_dict())\r\n            checkpoint[\"lr_schedulers\"] = lr_schedulers\r\n\r\n            # precision plugin\r\n            prec_plugin = trainer.precision_plugin\r\n            prec_plugin_state_dict = prec_plugin.state_dict()\r\n            if prec_plugin_state_dict:\r\n                checkpoint[prec_plugin.__class__.__qualname__] = prec_plugin_state_dict\r\n            prec_plugin.on_save_checkpoint(checkpoint)\r\n\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container\r\n\r\n        # dump hyper-parameters\r\n        for obj in (model, datamodule):\r\n            if obj and obj.hparams:\r\n                if hasattr(obj, \"_hparams_name\"):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_NAME] = obj._hparams_name\r\n                # dump arguments\r\n                if _OMEGACONF_AVAILABLE and isinstance(obj.hparams, Container):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = obj.hparams\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_TYPE] = type(obj.hparams)\r\n                else:\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = dict(obj.hparams)\r\n\r\n        # dump stateful datamodule\r\n        if datamodule is not None:\r\n            datamodule_state_dict = call._call_lightning_datamodule_hook(trainer, \"state_dict\")\r\n            if datamodule_state_dict:\r\n                checkpoint[datamodule.__class__.__qualname__] = datamodule_state_dict\r\n\r\n        # on_save_checkpoint hooks\r\n        if not weights_only:\r\n            # if state is returned from callback's on_save_checkpoint\r\n            # it overrides the returned state from callback's state_dict\r\n            # support for returning state in on_save_checkpoint\r\n            # will be removed in v1.8\r\n            call._call_callbacks_on_save_checkpoint(trainer, checkpoint)\r\n        call._call_lightning_module_hook(trainer, \"on_save_checkpoint\", checkpoint)\r\n        return checkpoint", "language": "python", "code": "def dump_checkpoint(self, weights_only: bool = False) -> dict:\r\n        \"\"\"Creating a model checkpoint dictionary object from various component states.\r\n\r\n        Args:\r\n            weights_only: saving model weights only\r\n        Return:\r\n            structured dictionary: {\r\n                'epoch':                     training epoch\r\n                'global_step':               training global step\r\n                'pytorch-lightning_version': The version of PyTorch Lightning that produced this checkpoint\r\n                'callbacks':                 \"callback specific state\"[] # if not weights_only\r\n                'optimizer_states':          \"PT optim's state_dict\"[]   # if not weights_only\r\n                'lr_schedulers':             \"PT sched's state_dict\"[]   # if not weights_only\r\n                'state_dict':                Model's state_dict (e.g. network weights)\r\n                precision_plugin.__class__.__qualname__:  precision plugin state_dict # if not weights_only\r\n                CHECKPOINT_HYPER_PARAMS_NAME:\r\n                CHECKPOINT_HYPER_PARAMS_KEY:\r\n                CHECKPOINT_HYPER_PARAMS_TYPE:\r\n                something_cool_i_want_to_save: anything you define through model.on_save_checkpoint\r\n                LightningDataModule.__class__.__qualname__: pl DataModule's state\r\n            }\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        model = trainer.lightning_module\r\n        datamodule = trainer.datamodule\r\n\r\n        checkpoint = {\r\n            # the epoch and global step are saved for compatibility but they are not relevant for restoration\r\n            \"epoch\": trainer.current_epoch,\r\n            \"global_step\": trainer.global_step,\r\n            \"pytorch-lightning_version\": pl.__version__,\r\n            \"state_dict\": self._get_lightning_module_state_dict(),\r\n            \"loops\": self._get_loops_state_dict(),\r\n        }\r\n\r\n        if not weights_only:\r\n            # dump callbacks\r\n            checkpoint[\"callbacks\"] = call._call_callbacks_state_dict(trainer)\r\n\r\n            optimizer_states = []\r\n            for i, optimizer in enumerate(trainer.optimizers):\r\n                # Rely on accelerator to dump optimizer state\r\n                optimizer_state = trainer.strategy.optimizer_state(optimizer)\r\n                optimizer_states.append(optimizer_state)\r\n\r\n            checkpoint[\"optimizer_states\"] = optimizer_states\r\n\r\n            # dump lr schedulers\r\n            lr_schedulers = []\r\n            for config in trainer.lr_scheduler_configs:\r\n                lr_schedulers.append(config.scheduler.state_dict())\r\n            checkpoint[\"lr_schedulers\"] = lr_schedulers\r\n\r\n            # precision plugin\r\n            prec_plugin = trainer.precision_plugin\r\n            prec_plugin_state_dict = prec_plugin.state_dict()\r\n            if prec_plugin_state_dict:\r\n                checkpoint[prec_plugin.__class__.__qualname__] = prec_plugin_state_dict\r\n            prec_plugin.on_save_checkpoint(checkpoint)\r\n\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container\r\n\r\n        # dump hyper-parameters\r\n        for obj in (model, datamodule):\r\n            if obj and obj.hparams:\r\n                if hasattr(obj, \"_hparams_name\"):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_NAME] = obj._hparams_name\r\n                # dump arguments\r\n                if _OMEGACONF_AVAILABLE and isinstance(obj.hparams, Container):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = obj.hparams\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_TYPE] = type(obj.hparams)\r\n                else:\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = dict(obj.hparams)\r\n\r\n        # dump stateful datamodule\r\n        if datamodule is not None:\r\n            datamodule_state_dict = call._call_lightning_datamodule_hook(trainer, \"state_dict\")\r\n            if datamodule_state_dict:\r\n                checkpoint[datamodule.__class__.__qualname__] = datamodule_state_dict\r\n\r\n        # on_save_checkpoint hooks\r\n        if not weights_only:\r\n            # if state is returned from callback's on_save_checkpoint\r\n            # it overrides the returned state from callback's state_dict\r\n            # support for returning state in on_save_checkpoint\r\n            # will be removed in v1.8\r\n            call._call_callbacks_on_save_checkpoint(trainer, checkpoint)\r\n        call._call_lightning_module_hook(trainer, \"on_save_checkpoint\", checkpoint)\r\n        return checkpoint", "code_tokens": ["def", "dump_checkpoint", "(", "self", ",", "weights_only", ":", "bool", "=", "False", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "Creating", "a", "model", "checkpoint", "dictionary", "object", "from", "various", "component", "states", ".", "Args", ":", "weights_only", ":", "saving", "model", "weights", "only", "Return", ":", "structured", "dictionary", ":", "{", "'", "epoch", "'", ":", "training", "epoch", "'", "global_step", "'", ":", "training", "global", "step", "'", "pytorch", "-", "lightning_version", "'", ":", "The", "version", "of", "PyTorch", "Lightning", "that", "produced", "this", "checkpoint", "'", "callbacks", "'", ":", "\"", "callback", "specific", "state", "\"", "[", "]", "'", "optimizer_states", "'", ":", "\"", "PT", "optim", "'", "s", "state_dict", "\"", "[", "]", "'", "lr_schedulers", "'", ":", "\"", "PT", "sched", "'", "s", "state_dict", "\"", "[", "]", "'", "state_dict", "'", ":", "Model", "'", "s", "state_dict", "(", "e", ".", "g", ".", "network", "weights", ")", "precision_plugin", ".", "__class__", ".", "__qualname__", ":", "precision", "plugin", "state_dict", "CHECKPOINT_HYPER_PARAMS_NAME", ":", "CHECKPOINT_HYPER_PARAMS_KEY", ":", "CHECKPOINT_HYPER_PARAMS_TYPE", ":", "something_cool_i_want_to_save", ":", "anything", "you", "define", "through", "model", ".", "on_save_checkpoint", "LightningDataModule", ".", "__class__", ".", "__qualname__", ":", "pl", "DataModule", "'", "s", "state", "}", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "model", "=", "trainer", ".", "lightning_module", "datamodule", "=", "trainer", ".", "datamodule", "checkpoint", "=", "{", "\"", "epoch", "\"", ":", "trainer", ".", "current_epoch", ",", "\"", "global_step", "\"", ":", "trainer", ".", "global_step", ",", "\"", "pytorch", "-", "lightning_version", "\"", ":", "pl", ".", "__version__", ",", "\"", "state_dict", "\"", ":", "self", ".", "_get_lightning_module_state_dict", "(", ")", ",", "\"", "loops", "\"", ":", "self", ".", "_get_loops_state_dict", "(", ")", ",", "}", "if", "not", "weights_only", ":", "checkpoint", "[", "\"", "callbacks", "\"", "]", "=", "call", ".", "_call_callbacks_state_dict", "(", "trainer", ")", "optimizer_states", "=", "[", "]", "for", "i", ",", "optimizer", "in", "enumerate", "(", "trainer", ".", "optimizers", ")", ":", "optimizer_state", "=", "trainer", ".", "strategy", ".", "optimizer_state", "(", "optimizer", ")", "optimizer_states", ".", "append", "(", "optimizer_state", ")", "checkpoint", "[", "\"", "optimizer_states", "\"", "]", "=", "optimizer_states", "lr_schedulers", "=", "[", "]", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ":", "lr_schedulers", ".", "append", "(", "config", ".", "scheduler", ".", "state_dict", "(", ")", ")", "checkpoint", "[", "\"", "lr_schedulers", "\"", "]", "=", "lr_schedulers", "prec_plugin", "=", "trainer", ".", "precision_plugin", "prec_plugin_state_dict", "=", "prec_plugin", ".", "state_dict", "(", ")", "if", "prec_plugin_state_dict", ":", "checkpoint", "[", "prec_plugin", ".", "__class__", ".", "__qualname__", "]", "=", "prec_plugin_state_dict", "prec_plugin", ".", "on_save_checkpoint", "(", "checkpoint", ")", "if", "_OMEGACONF_AVAILABLE", ":", "from", "omegaconf", "import", "Container", "for", "obj", "in", "(", "model", ",", "datamodule", ")", ":", "if", "obj", "and", "obj", ".", "hparams", ":", "if", "hasattr", "(", "obj", ",", "\"", "_hparams_name", "\"", ")", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_NAME", "]", "=", "obj", ".", "_hparams_name", "if", "_OMEGACONF_AVAILABLE", "and", "isinstance", "(", "obj", ".", "hparams", ",", "Container", ")", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_KEY", "]", "=", "obj", ".", "hparams", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_TYPE", "]", "=", "type", "(", "obj", ".", "hparams", ")", "else", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_KEY", "]", "=", "dict", "(", "obj", ".", "hparams", ")", "if", "datamodule", "is", "not", "None", ":", "datamodule_state_dict", "=", "call", ".", "_call_lightning_datamodule_hook", "(", "trainer", ",", "\"", "state_dict", "\"", ")", "if", "datamodule_state_dict", ":", "checkpoint", "[", "datamodule", ".", "__class__", ".", "__qualname__", "]", "=", "datamodule_state_dict", "if", "not", "weights_only", ":", "call", ".", "_call_callbacks_on_save_checkpoint", "(", "trainer", ",", "checkpoint", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_save_checkpoint", "\"", ",", "checkpoint", ")", "return", "checkpoint"], "docstring": "Creating a model checkpoint dictionary object from various component states.\r\n\r\n        Args:\r\n            weights_only: saving model weights only\r\n        Return:\r\n            structured dictionary: {\r\n                'epoch':                     training epoch\r\n                'global_step':               training global step\r\n                'pytorch-lightning_version': The version of PyTorch Lightning that produced this checkpoint\r\n                'callbacks':                 \"callback specific state\"[] # if not weights_only\r\n                'optimizer_states':          \"PT optim's state_dict\"[]   # if not weights_only\r\n                'lr_schedulers':             \"PT sched's state_dict\"[]   # if not weights_only\r\n                'state_dict':                Model's state_dict (e.g. network weights)\r\n                precision_plugin.__class__.__qualname__:  precision plugin state_dict # if not weights_only\r\n                CHECKPOINT_HYPER_PARAMS_NAME:\r\n                CHECKPOINT_HYPER_PARAMS_KEY:\r\n                CHECKPOINT_HYPER_PARAMS_TYPE:\r\n                something_cool_i_want_to_save: anything you define through model.on_save_checkpoint\r\n                LightningDataModule.__class__.__qualname__: pl DataModule's state\r\n            }", "docstring_tokens": ["creating", "a", "model", "checkpoint", "dictionary", "object", "from", "various", "component", "states", "args", "weights_only", "saving", "model", "weights", "only", "return", "structured", "dictionary", "epoch", "training", "epoch", "global_step", "training", "global", "step", "pytorch", "lightning_version", "the", "version", "of", "pytorch", "lightning", "that", "produced", "this", "checkpoint", "callbacks", "callback", "specific", "state", "if", "not", "weights_only", "optimizer_states", "pt", "optim", "s", "state_dict", "if", "not", "weights_only", "lr_schedulers", "pt", "sched", "s", "state_dict", "if", "not", "weights_only", "state_dict", "model", "s", "state_dict", "e", "g", "network", "weights", "precision_plugin", "__class__", "__qualname__", "precision", "plugin", "state_dict", "if", "not", "weights_only", "checkpoint_hyper_params_name", "checkpoint_hyper_params_key", "checkpoint_hyper_params_type", "something_cool_i_want_to_save", "anything", "you", "define", "through", "model", "on_save_checkpoint", "lightningdatamodule", "__class__", "__qualname__", "pl", "datamodule", "s", "state"], "docstring_summary": "Creating a model checkpoint dictionary object from various component states.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 412, "end_line": 502, "hash": "e909a69ae6629d6915c4b120ac0d0160", "complexity": 15, "parameters": ["weights_only"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "__max_ckpt_version_in_folder", "original_string": "def __max_ckpt_version_in_folder(dir_path: _PATH, name_key: str = \"ckpt_\") -> Optional[int]:\r\n        \"\"\"List up files in `dir_path` with `name_key`, then yield maximum suffix number.\r\n\r\n        Args:\r\n            dir_path: path of directory which may contain files whose name include `name_key`\r\n            name_key: file name prefix\r\n        Returns:\r\n            None if no-corresponding-file else maximum suffix number\r\n\r\n        \"\"\"\r\n        # check directory existence\r\n        fs, uri = url_to_fs(str(dir_path))\r\n        if not fs.exists(dir_path):\r\n            return None\r\n\r\n        # check corresponding file existence\r\n        files = [os.path.basename(f[\"name\"]) for f in fs.listdir(uri)]\r\n        files = [x for x in files if name_key in x]\r\n        if len(files) == 0:\r\n            return None\r\n\r\n        # extract suffix number\r\n        ckpt_vs = []\r\n        for name in files:\r\n            name = name.split(name_key)[-1]\r\n            name = re.sub(\"[^0-9]\", \"\", name)\r\n            ckpt_vs.append(int(name))\r\n\r\n        return max(ckpt_vs)", "language": "python", "code": "def __max_ckpt_version_in_folder(dir_path: _PATH, name_key: str = \"ckpt_\") -> Optional[int]:\r\n        \"\"\"List up files in `dir_path` with `name_key`, then yield maximum suffix number.\r\n\r\n        Args:\r\n            dir_path: path of directory which may contain files whose name include `name_key`\r\n            name_key: file name prefix\r\n        Returns:\r\n            None if no-corresponding-file else maximum suffix number\r\n\r\n        \"\"\"\r\n        # check directory existence\r\n        fs, uri = url_to_fs(str(dir_path))\r\n        if not fs.exists(dir_path):\r\n            return None\r\n\r\n        # check corresponding file existence\r\n        files = [os.path.basename(f[\"name\"]) for f in fs.listdir(uri)]\r\n        files = [x for x in files if name_key in x]\r\n        if len(files) == 0:\r\n            return None\r\n\r\n        # extract suffix number\r\n        ckpt_vs = []\r\n        for name in files:\r\n            name = name.split(name_key)[-1]\r\n            name = re.sub(\"[^0-9]\", \"\", name)\r\n            ckpt_vs.append(int(name))\r\n\r\n        return max(ckpt_vs)", "code_tokens": ["def", "__max_ckpt_version_in_folder", "(", "dir_path", ":", "_PATH", ",", "name_key", ":", "str", "=", "\"", "ckpt_", "\"", ")", "-", ">", "Optional", "[", "int", "]", ":", "\"", "\"", "\"", "List", "up", "files", "in", "`", "dir_path", "`", "with", "`", "name_key", "`", ",", "then", "yield", "maximum", "suffix", "number", ".", "Args", ":", "dir_path", ":", "path", "of", "directory", "which", "may", "contain", "files", "whose", "name", "include", "`", "name_key", "`", "name_key", ":", "file", "name", "prefix", "Returns", ":", "None", "if", "no", "-", "corresponding", "-", "file", "else", "maximum", "suffix", "number", "\"", "\"", "\"", "fs", ",", "uri", "=", "url_to_fs", "(", "str", "(", "dir_path", ")", ")", "if", "not", "fs", ".", "exists", "(", "dir_path", ")", ":", "return", "None", "files", "=", "[", "os", ".", "path", ".", "basename", "(", "f", "[", "\"", "name", "\"", "]", ")", "for", "f", "in", "fs", ".", "listdir", "(", "uri", ")", "]", "files", "=", "[", "x", "for", "x", "in", "files", "if", "name_key", "in", "x", "]", "if", "len", "(", "files", ")", "=", "=", "0", ":", "return", "None", "ckpt_vs", "=", "[", "]", "for", "name", "in", "files", ":", "name", "=", "name", ".", "split", "(", "name_key", ")", "[", "-", "1", "]", "name", "=", "re", ".", "sub", "(", "\"", "[", "^", "0", "-", "9", "]", "\"", ",", "\"", "\"", ",", "name", ")", "ckpt_vs", ".", "append", "(", "int", "(", "name", ")", ")", "return", "max", "(", "ckpt_vs", ")"], "docstring": "List up files in `dir_path` with `name_key`, then yield maximum suffix number.\r\n\r\n        Args:\r\n            dir_path: path of directory which may contain files whose name include `name_key`\r\n            name_key: file name prefix\r\n        Returns:\r\n            None if no-corresponding-file else maximum suffix number", "docstring_tokens": ["list", "up", "files", "in", "dir_path", "with", "name_key", "then", "yield", "maximum", "suffix", "number", "args", "dir_path", "path", "of", "directory", "which", "may", "contain", "files", "whose", "name", "include", "name_key", "name_key", "file", "name", "prefix", "returns", "none", "if", "no", "corresponding", "file", "else", "maximum", "suffix", "number"], "docstring_summary": "List up files in `dir_path` with `name_key`, then yield maximum suffix number.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "valid", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 516, "end_line": 544, "hash": "65609424cb1d79b253c32fc4841c36e3", "complexity": 7, "parameters": ["dir_path", "name_key"]}
