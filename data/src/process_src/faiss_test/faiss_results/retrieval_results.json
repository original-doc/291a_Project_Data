[
  {
    "query_id": 1,
    "query": "How to fix CUDA out of memory error in PyTorch Lightning?",
    "query_type": "debugging",
    "retrieved_docs": [
      {
        "func_name": "_check_bad_cuda_fork",
        "docstring_summary": "Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.",
        "class_name": "",
        "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py"
      },
      {
        "func_name": "setup_module",
        "docstring_summary": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.",
        "class_name": "DDPStrategy",
        "path": "src\\lightning\\fabric\\strategies\\ddp.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\ddp.py"
      },
      {
        "func_name": "setup_device",
        "docstring_summary": "Raises:",
        "class_name": "CUDAAccelerator",
        "path": "src\\lightning\\pytorch\\accelerators\\cuda.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py"
      },
      {
        "func_name": "setup_device",
        "docstring_summary": "Raises:",
        "class_name": "CUDAAccelerator",
        "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py"
      },
      {
        "func_name": "is_cuda_available",
        "docstring_summary": "Returns a bool indicating if CUDA is currently available.",
        "class_name": "",
        "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py"
      }
    ],
    "scores": [
      0.6161134979780009,
      0.5412681222635292,
      0.5324349029197095,
      0.5191388242333124,
      0.5104667304237297
    ],
    "latency": 0.017511367797851562,
    "method": "sentence-transformer"
  },
  {
    "query_id": 2,
    "query": "Why is my validation loss not decreasing?",
    "query_type": "debugging",
    "retrieved_docs": [
      {
        "func_name": "check_finite_loss",
        "docstring_summary": "Checks for finite loss value.",
        "class_name": "",
        "path": "src\\lightning\\pytorch\\loops\\utilities.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\utilities.py"
      },
      {
        "func_name": "on_validation_model_eval",
        "docstring_summary": "Called when the validation loop starts.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "on_validation_model_train",
        "docstring_summary": "Called when the validation loop ends.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "on_validation_end",
        "docstring_summary": "Save a checkpoint at the end of the validation stage.",
        "class_name": "ModelCheckpoint",
        "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py"
      },
      {
        "func_name": "backward",
        "docstring_summary": "Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      }
    ],
    "scores": [
      0.466209409222441,
      0.4466812862359771,
      0.44239606450034175,
      0.43962132836934736,
      0.4344520802961311
    ],
    "latency": 0.011999130249023438,
    "method": "sentence-transformer"
  },
  {
    "query_id": 3,
    "query": "RuntimeError with checkpoint loading",
    "query_type": "debugging",
    "retrieved_docs": [
      {
        "func_name": "load",
        "docstring_summary": "Loads a checkpoint from a given file into state.",
        "class_name": "MyCustomTrainer",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py"
      },
      {
        "func_name": "load_checkpoint",
        "docstring_summary": "Uses the base ``checkpoint_io`` to load the checkpoint.",
        "class_name": "_WrappingCheckpointIO",
        "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\wrapper.py"
      },
      {
        "func_name": "_load_distributed_checkpoint",
        "docstring_summary": "Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.",
        "class_name": "",
        "path": "src\\lightning\\fabric\\utilities\\load.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\load.py"
      },
      {
        "func_name": "_get_full_model_name",
        "docstring_summary": "Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.",
        "class_name": "NeptuneLogger",
        "path": "src\\lightning\\pytorch\\loggers\\neptune.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py"
      },
      {
        "func_name": "load_checkpoint",
        "docstring_summary": "Load the contents from a checkpoint and restore the state of the given objects.",
        "class_name": "ModelParallelStrategy",
        "path": "src\\lightning\\fabric\\strategies\\model_parallel.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py"
      }
    ],
    "scores": [
      0.5972624819291888,
      0.5657274852582123,
      0.5552120259156942,
      0.5422873649672715,
      0.5394913379585939
    ],
    "latency": 0.007000923156738281,
    "method": "sentence-transformer"
  },
  {
    "query_id": 4,
    "query": "What parameters does the Lightning Trainer accept?",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "__init__",
        "docstring_summary": "Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are",
        "class_name": "LightningCLI",
        "path": "src\\lightning\\pytorch\\cli.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py"
      },
      {
        "func_name": "on_train_end",
        "docstring_summary": "Called when training ends.",
        "class_name": "WeightAveraging",
        "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py"
      },
      {
        "func_name": "on_train_epoch_end",
        "docstring_summary": "Called when a training epoch ends.",
        "class_name": "WeightAveraging",
        "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py"
      },
      {
        "func_name": "connect",
        "docstring_summary": "Called by the Trainer to connect the strategy with the model.",
        "class_name": "Strategy",
        "path": "src\\lightning\\pytorch\\strategies\\strategy.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py"
      },
      {
        "func_name": "instantiate_trainer",
        "docstring_summary": "Instantiates the trainer.",
        "class_name": "LightningCLI",
        "path": "src\\lightning\\pytorch\\cli.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py"
      }
    ],
    "scores": [
      0.5275439186370908,
      0.5154608616101417,
      0.5073675460238767,
      0.5028024650399896,
      0.4988256326414304
    ],
    "latency": 0.0059964656829833984,
    "method": "sentence-transformer"
  },
  {
    "query_id": 5,
    "query": "How to use early stopping callback?",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "configure_callbacks",
        "docstring_summary": "Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      },
      {
        "func_name": "_run_early_stopping_check",
        "docstring_summary": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training.",
        "class_name": "EarlyStopping",
        "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\early_stopping.py"
      },
      {
        "func_name": "barrier",
        "docstring_summary": "Wait for all processes to enter this call.",
        "class_name": "Fabric",
        "path": "src\\lightning\\fabric\\fabric.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py"
      },
      {
        "func_name": "call",
        "docstring_summary": "r\"\"\"Trigger the callback methods with the given name and arguments.",
        "class_name": "Fabric",
        "path": "src\\lightning\\fabric\\fabric.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py"
      },
      {
        "func_name": "_after_closure",
        "docstring_summary": "Utility to share some code after the closure has been run.",
        "class_name": "Precision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py"
      }
    ],
    "scores": [
      0.4807504573618203,
      0.4800780339242001,
      0.4576099664023635,
      0.4557763289279959,
      0.4494277471088134
    ],
    "latency": 0.006992340087890625,
    "method": "sentence-transformer"
  },
  {
    "query_id": 6,
    "query": "Configure learning rate scheduler in Lightning",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "lr_scheduler_step",
        "docstring_summary": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      },
      {
        "func_name": "step_scheduler",
        "docstring_summary": "Steps the learning rate scheduler if necessary.",
        "class_name": "MyCustomTrainer",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py"
      },
      {
        "func_name": "configure_optimizers",
        "docstring_summary": "Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.",
        "class_name": "LightningCLI",
        "path": "src\\lightning\\pytorch\\cli.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py"
      },
      {
        "func_name": "lr_schedulers",
        "docstring_summary": "Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      },
      {
        "func_name": "train_loop",
        "docstring_summary": "The training loop running a single training epoch.",
        "class_name": "MyCustomTrainer",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py"
      }
    ],
    "scores": [
      0.561235012906533,
      0.5558868310226516,
      0.5415703213084341,
      0.526069023117421,
      0.5126799329995206
    ],
    "latency": 0.00700068473815918,
    "method": "sentence-transformer"
  },
  {
    "query_id": 7,
    "query": "Set up model checkpointing",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "after_save_checkpoint",
        "docstring_summary": "Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.",
        "class_name": "NeptuneLogger",
        "path": "src\\lightning\\pytorch\\loggers\\neptune.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py"
      },
      {
        "func_name": "after_save_checkpoint",
        "docstring_summary": "Called after model checkpoint callback saves a new checkpoint.",
        "class_name": "Logger",
        "path": "src\\lightning\\pytorch\\loggers\\logger.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\logger.py"
      },
      {
        "func_name": "after_save_checkpoint",
        "docstring_summary": "Called after model checkpoint callback saves a new checkpoint.",
        "class_name": "TensorBoardLogger",
        "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py"
      },
      {
        "func_name": "save_checkpoint",
        "docstring_summary": "Save model, optimizer, and other state to a checkpoint on disk.",
        "class_name": "ModelParallelStrategy",
        "path": "src\\lightning\\fabric\\strategies\\model_parallel.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py"
      },
      {
        "func_name": "save_checkpoint",
        "docstring_summary": "Save model, optimizer, and other state as a checkpoint file.",
        "class_name": "Strategy",
        "path": "src\\lightning\\fabric\\strategies\\strategy.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py"
      }
    ],
    "scores": [
      0.5916205157826326,
      0.5793379002726344,
      0.5793379002726344,
      0.5671725932966113,
      0.5565191177462795
    ],
    "latency": 0.006992340087890625,
    "method": "sentence-transformer"
  },
  {
    "query_id": 8,
    "query": "How to log metrics to tensorboard?",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "log_hyperparams",
        "docstring_summary": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the",
        "class_name": "TensorBoardLogger",
        "path": "src\\lightning\\fabric\\loggers\\tensorboard.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py"
      },
      {
        "func_name": "log_hyperparams",
        "docstring_summary": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the",
        "class_name": "TensorBoardLogger",
        "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py"
      },
      {
        "func_name": "experiment",
        "docstring_summary": "Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.",
        "class_name": "TensorBoardLogger",
        "path": "src\\lightning\\fabric\\loggers\\tensorboard.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py"
      },
      {
        "func_name": "log_dict",
        "docstring_summary": "Log multiple scalars at once to all loggers that were added to Fabric.",
        "class_name": "Fabric",
        "path": "src\\lightning\\fabric\\fabric.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py"
      },
      {
        "func_name": "log_metrics",
        "docstring_summary": "Log metrics (numeric values) in Neptune runs.",
        "class_name": "NeptuneLogger",
        "path": "src\\lightning\\pytorch\\loggers\\neptune.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py"
      }
    ],
    "scores": [
      0.5575968864047153,
      0.5575968864047153,
      0.5394548576115709,
      0.5387949530878363,
      0.5359976389187868
    ],
    "latency": 0.006998777389526367,
    "method": "sentence-transformer"
  },
  {
    "query_id": 9,
    "query": "Implement custom callback in PyTorch Lightning",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "configure_callbacks",
        "docstring_summary": "Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      },
      {
        "func_name": "_after_closure",
        "docstring_summary": "Utility to share some code after the closure has been run.",
        "class_name": "Precision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py"
      },
      {
        "func_name": "on_train_epoch_end",
        "docstring_summary": "Called when the train epoch ends.",
        "class_name": "Callback",
        "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py"
      },
      {
        "func_name": "on_train_epoch_end",
        "docstring_summary": "Called in the training loop at the very end of the epoch.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "experiment",
        "docstring_summary": "r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the",
        "class_name": "WandbLogger",
        "path": "src\\lightning\\pytorch\\loggers\\wandb.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py"
      }
    ],
    "scores": [
      0.540584824889923,
      0.5147626075021396,
      0.5114591500546709,
      0.5088043830187139,
      0.5035701635412871
    ],
    "latency": 0.009001016616821289,
    "method": "sentence-transformer"
  },
  {
    "query_id": 10,
    "query": "Multi-GPU training setup",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "_parse_gpu_ids",
        "docstring_summary": "Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.",
        "class_name": "",
        "path": "src\\lightning\\fabric\\utilities\\device_parser.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py"
      },
      {
        "func_name": "__init__",
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "class_name": "MyCustomTrainer",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py"
      },
      {
        "func_name": "setup_device",
        "docstring_summary": "Raises:",
        "class_name": "CUDAAccelerator",
        "path": "src\\lightning\\pytorch\\accelerators\\cuda.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py"
      },
      {
        "func_name": "setup_dataloaders",
        "docstring_summary": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each",
        "class_name": "Fabric",
        "path": "src\\lightning\\fabric\\fabric.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py"
      },
      {
        "func_name": "_sanitize_gpu_ids",
        "docstring_summary": "Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the",
        "class_name": "",
        "path": "src\\lightning\\fabric\\utilities\\device_parser.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py"
      }
    ],
    "scores": [
      0.509927950846703,
      0.5077257327353079,
      0.466448606071444,
      0.4647211470800088,
      0.46467954668555383
    ],
    "latency": 0.006997346878051758,
    "method": "sentence-transformer"
  },
  {
    "query_id": 11,
    "query": "Custom validation step implementation",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "val_step_context",
        "docstring_summary": "A contextmanager for the validation step.",
        "class_name": "Precision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py"
      },
      {
        "func_name": "_build_step_args_from_hook_kwargs",
        "docstring_summary": "Helper method to build args for `test_step` or `validation_step`.",
        "class_name": "_EvaluationLoop",
        "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py"
      },
      {
        "func_name": "on_validation_batch_end",
        "docstring_summary": "Called in the validation loop after the batch.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "validate_settings",
        "docstring_summary": "Validates settings configured in the script against the environment, and raises an exception if there is an",
        "class_name": "ClusterEnvironment",
        "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py"
      },
      {
        "func_name": "on_validation_batch_end",
        "docstring_summary": "Called when the validation batch ends.",
        "class_name": "Callback",
        "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py"
      }
    ],
    "scores": [
      0.526108664798117,
      0.4946870405399317,
      0.47047345278985736,
      0.4665255215956241,
      0.4573993486294009
    ],
    "latency": 0.0069963932037353516,
    "method": "sentence-transformer"
  },
  {
    "query_id": 12,
    "query": "Gradient accumulation example",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "clip_gradients",
        "docstring_summary": "Clips the gradients.",
        "class_name": "Precision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py"
      },
      {
        "func_name": "clip_gradients",
        "docstring_summary": "DeepSpeed handles gradient clipping internally.",
        "class_name": "DeepSpeedPrecision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py"
      },
      {
        "func_name": "on_before_optimizer_step",
        "docstring_summary": "Called before ``optimizer.step()``.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "clip_gradients_norm",
        "docstring_summary": "Clip gradients by norm.",
        "class_name": "XLAFSDPStrategy",
        "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py"
      },
      {
        "func_name": "clip_grad_by_norm",
        "docstring_summary": "Clip gradients by norm.",
        "class_name": "Precision",
        "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py"
      }
    ],
    "scores": [
      0.4812329869678381,
      0.47821006881895006,
      0.46813884210444073,
      0.45871738833955794,
      0.4585174539936118
    ],
    "latency": 0.00699615478515625,
    "method": "sentence-transformer"
  },
  {
    "query_id": 13,
    "query": "Difference between training_step and validation_step",
    "query_type": "conceptual",
    "retrieved_docs": [
      {
        "func_name": "on_validation_batch_end",
        "docstring_summary": "Called in the validation loop after the batch.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "on_validation_batch_end",
        "docstring_summary": "Called when the validation batch ends.",
        "class_name": "Callback",
        "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py"
      },
      {
        "func_name": "on_validation_batch_start",
        "docstring_summary": "Called when the validation batch begins.",
        "class_name": "Callback",
        "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py"
      },
      {
        "func_name": "on_validation_batch_start",
        "docstring_summary": "Called in the validation loop before anything happens for that batch.",
        "class_name": "ModelHooks",
        "path": "src\\lightning\\pytorch\\core\\hooks.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py"
      },
      {
        "func_name": "training_step",
        "docstring_summary": "A single training step, running forward and backward. The optimizer step is called separately, as this is",
        "class_name": "MyCustomTrainer",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py"
      }
    ],
    "scores": [
      0.5303937436031188,
      0.5168613166525169,
      0.5039444552645943,
      0.5007359622553154,
      0.4990107952934581
    ],
    "latency": 0.011060237884521484,
    "method": "sentence-transformer"
  },
  {
    "query_id": 14,
    "query": "How does automatic optimization work?",
    "query_type": "conceptual",
    "retrieved_docs": [
      {
        "func_name": "_setup_optimizer",
        "docstring_summary": "Performs setup for the optimizer, e.g., by wrapping it by another class.",
        "class_name": "Strategy",
        "path": "src\\lightning\\pytorch\\strategies\\strategy.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py"
      },
      {
        "func_name": "configure_optimizers",
        "docstring_summary": "r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      },
      {
        "func_name": "optimizer_step",
        "docstring_summary": "Hook to run the optimizer step.",
        "class_name": "Precision",
        "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py"
      },
      {
        "func_name": "optimizer_state",
        "docstring_summary": "Returns state of an optimizer.",
        "class_name": "Strategy",
        "path": "src\\lightning\\pytorch\\strategies\\strategy.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py"
      },
      {
        "func_name": "optimizers",
        "docstring_summary": "Returns the optimizer(s) that are being used during training. Useful for manual optimization.",
        "class_name": "LightningModule",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py"
      }
    ],
    "scores": [
      0.48297470588926195,
      0.46901310536073093,
      0.4657422231460883,
      0.46396906389134307,
      0.4636620946456547
    ],
    "latency": 0.008000373840332031,
    "method": "sentence-transformer"
  },
  {
    "query_id": 15,
    "query": "What is a LightningModule?",
    "query_type": "conceptual",
    "retrieved_docs": [
      {
        "func_name": "experiment",
        "docstring_summary": "r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the",
        "class_name": "CometLogger",
        "path": "src\\lightning\\pytorch\\loggers\\comet.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py"
      },
      {
        "func_name": "experiment",
        "docstring_summary": "r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the",
        "class_name": "WandbLogger",
        "path": "src\\lightning\\pytorch\\loggers\\wandb.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py"
      },
      {
        "func_name": "add_lightning_class_args",
        "docstring_summary": "Adds arguments from a lightning class to a nested key of the parser.",
        "class_name": "LightningArgumentParser",
        "path": "src\\lightning\\pytorch\\cli.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py"
      },
      {
        "func_name": "setup_environment",
        "docstring_summary": "Setup any processes or distributed connections.",
        "class_name": "Strategy",
        "path": "src\\lightning\\pytorch\\strategies\\strategy.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py"
      },
      {
        "func_name": "_replace_imports",
        "docstring_summary": "Replace imports of standalone package to lightning.",
        "class_name": "",
        "path": ".actions\\assistant.py",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py"
      }
    ],
    "scores": [
      0.4564626437055246,
      0.43906246336177296,
      0.43421543558155185,
      0.4298863978899581,
      0.42297825702426817
    ],
    "latency": 0.00699925422668457,
    "method": "sentence-transformer"
  }
]