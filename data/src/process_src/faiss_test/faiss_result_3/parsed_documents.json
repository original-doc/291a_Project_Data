[
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 0,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: state_key\nLanguage: python\nPartition: train\n\n--- Docstring ---\nIdentifier for the state of the callback.\n\n--- Code ---\ndef state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__\n\n--- Original String ---\ndef state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__",
    "func_name": "state_key",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Identifier for the state of the callback.",
    "code": "def state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__",
    "docstring_summary": "Identifier for the state of the callback."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 1,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: _legacy_state_key\nLanguage: python\nPartition: train\n\n--- Docstring ---\nState key for checkpoints saved prior to version 1.5.0.\n\n--- Code ---\ndef _legacy_state_key(self) -> type[\"Callback\"]:\r\n        \"\"\"State key for checkpoints saved prior to version 1.5.0.\"\"\"\r\n        return type(self)\n\n--- Original String ---\ndef _legacy_state_key(self) -> type[\"Callback\"]:\r\n        \"\"\"State key for checkpoints saved prior to version 1.5.0.\"\"\"\r\n        return type(self)",
    "func_name": "_legacy_state_key",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "State key for checkpoints saved prior to version 1.5.0.",
    "code": "def _legacy_state_key(self) -> type[\"Callback\"]:\r\n        \"\"\"State key for checkpoints saved prior to version 1.5.0.\"\"\"\r\n        return type(self)",
    "docstring_summary": "State key for checkpoints saved prior to version 1.5.0."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 2,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: _generate_state_key\nLanguage: python\nPartition: train\n\n--- Docstring ---\nFormats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\n\n--- Code ---\ndef _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"\n\n--- Original String ---\ndef _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"",
    "func_name": "_generate_state_key",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for",
    "code": "def _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"",
    "docstring_summary": "Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 3,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: setup\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when fit, validate, test, predict, or tune begins.\n\n--- Code ---\ndef setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\"\"\"\n\n--- Original String ---\ndef setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\"\"\"",
    "func_name": "setup",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when fit, validate, test, predict, or tune begins.",
    "code": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\"\"\"",
    "docstring_summary": "Called when fit, validate, test, predict, or tune begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 4,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: teardown\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when fit, validate, test, predict, or tune ends.\n\n--- Code ---\ndef teardown(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune ends.\"\"\"\n\n--- Original String ---\ndef teardown(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune ends.\"\"\"",
    "func_name": "teardown",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when fit, validate, test, predict, or tune ends.",
    "code": "def teardown(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune ends.\"\"\"",
    "docstring_summary": "Called when fit, validate, test, predict, or tune ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 5,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_fit_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when fit begins.\n\n--- Code ---\ndef on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit begins.\"\"\"\n\n--- Original String ---\ndef on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit begins.\"\"\"",
    "func_name": "on_fit_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when fit begins.",
    "code": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit begins.\"\"\"",
    "docstring_summary": "Called when fit begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 6,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_fit_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when fit ends.\n\n--- Code ---\ndef on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit ends.\"\"\"\n\n--- Original String ---\ndef on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit ends.\"\"\"",
    "func_name": "on_fit_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when fit ends.",
    "code": "def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit ends.\"\"\"",
    "docstring_summary": "Called when fit ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 7,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_sanity_check_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation sanity check starts.\n\n--- Code ---\ndef on_sanity_check_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check starts.\"\"\"\n\n--- Original String ---\ndef on_sanity_check_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check starts.\"\"\"",
    "func_name": "on_sanity_check_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation sanity check starts.",
    "code": "def on_sanity_check_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check starts.\"\"\"",
    "docstring_summary": "Called when the validation sanity check starts."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 8,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_sanity_check_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation sanity check ends.\n\n--- Code ---\ndef on_sanity_check_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check ends.\"\"\"\n\n--- Original String ---\ndef on_sanity_check_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check ends.\"\"\"",
    "func_name": "on_sanity_check_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation sanity check ends.",
    "code": "def on_sanity_check_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check ends.\"\"\"",
    "docstring_summary": "Called when the validation sanity check ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 9,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_batch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train batch begins.\n\n--- Code ---\ndef on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"\n\n--- Original String ---\ndef on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"",
    "func_name": "on_train_batch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train batch begins.",
    "code": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"",
    "docstring_summary": "Called when the train batch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 10,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train batch ends.\n\n--- Code ---\ndef on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"\n\n--- Original String ---\ndef on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"",
    "func_name": "on_train_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train batch ends.",
    "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"",
    "docstring_summary": "Called when the train batch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 11,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train epoch begins.\n\n--- Code ---\ndef on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch begins.\"\"\"\n\n--- Original String ---\ndef on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch begins.\"\"\"",
    "func_name": "on_train_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train epoch begins.",
    "code": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch begins.\"\"\"",
    "docstring_summary": "Called when the train epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 12,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train epoch ends.\n\n--- Code ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"\n\n--- Original String ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"",
    "func_name": "on_train_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train epoch ends.",
    "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"",
    "docstring_summary": "Called when the train epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 13,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the val epoch begins.\n\n--- Code ---\ndef on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch begins.\"\"\"\n\n--- Original String ---\ndef on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch begins.\"\"\"",
    "func_name": "on_validation_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the val epoch begins.",
    "code": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch begins.\"\"\"",
    "docstring_summary": "Called when the val epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 14,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the val epoch ends.\n\n--- Code ---\ndef on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch ends.\"\"\"\n\n--- Original String ---\ndef on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch ends.\"\"\"",
    "func_name": "on_validation_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the val epoch ends.",
    "code": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch ends.\"\"\"",
    "docstring_summary": "Called when the val epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 15,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test epoch begins.\n\n--- Code ---\ndef on_test_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch begins.\"\"\"\n\n--- Original String ---\ndef on_test_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch begins.\"\"\"",
    "func_name": "on_test_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test epoch begins.",
    "code": "def on_test_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch begins.\"\"\"",
    "docstring_summary": "Called when the test epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 16,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test epoch ends.\n\n--- Code ---\ndef on_test_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch ends.\"\"\"\n\n--- Original String ---\ndef on_test_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch ends.\"\"\"",
    "func_name": "on_test_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test epoch ends.",
    "code": "def on_test_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch ends.\"\"\"",
    "docstring_summary": "Called when the test epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 17,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the predict epoch begins.\n\n--- Code ---\ndef on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch begins.\"\"\"\n\n--- Original String ---\ndef on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch begins.\"\"\"",
    "func_name": "on_predict_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the predict epoch begins.",
    "code": "def on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch begins.\"\"\"",
    "docstring_summary": "Called when the predict epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 18,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the predict epoch ends.\n\n--- Code ---\ndef on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch ends.\"\"\"\n\n--- Original String ---\ndef on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch ends.\"\"\"",
    "func_name": "on_predict_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the predict epoch ends.",
    "code": "def on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch ends.\"\"\"",
    "docstring_summary": "Called when the predict epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 19,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_batch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation batch begins.\n\n--- Code ---\ndef on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"\n\n--- Original String ---\ndef on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"",
    "func_name": "on_validation_batch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation batch begins.",
    "code": "def on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"",
    "docstring_summary": "Called when the validation batch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 20,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation batch ends.\n\n--- Code ---\ndef on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"\n\n--- Original String ---\ndef on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"",
    "func_name": "on_validation_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation batch ends.",
    "code": "def on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"",
    "docstring_summary": "Called when the validation batch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 21,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_batch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test batch begins.\n\n--- Code ---\ndef on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"\n\n--- Original String ---\ndef on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"",
    "func_name": "on_test_batch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test batch begins.",
    "code": "def on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"",
    "docstring_summary": "Called when the test batch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 22,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test batch ends.\n\n--- Code ---\ndef on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"\n\n--- Original String ---\ndef on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"",
    "func_name": "on_test_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test batch ends.",
    "code": "def on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"",
    "docstring_summary": "Called when the test batch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 23,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_batch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the predict batch begins.\n\n--- Code ---\ndef on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"\n\n--- Original String ---\ndef on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"",
    "func_name": "on_predict_batch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the predict batch begins.",
    "code": "def on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"",
    "docstring_summary": "Called when the predict batch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 24,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the predict batch ends.\n\n--- Code ---\ndef on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"\n\n--- Original String ---\ndef on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"",
    "func_name": "on_predict_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the predict batch ends.",
    "code": "def on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"",
    "docstring_summary": "Called when the predict batch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 25,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train begins.\n\n--- Code ---\ndef on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train begins.\"\"\"\n\n--- Original String ---\ndef on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train begins.\"\"\"",
    "func_name": "on_train_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train begins.",
    "code": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train begins.\"\"\"",
    "docstring_summary": "Called when the train begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 26,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_train_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the train ends.\n\n--- Code ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train ends.\"\"\"\n\n--- Original String ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train ends.\"\"\"",
    "func_name": "on_train_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the train ends.",
    "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train ends.\"\"\"",
    "docstring_summary": "Called when the train ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 27,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation loop begins.\n\n--- Code ---\ndef on_validation_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop begins.\"\"\"\n\n--- Original String ---\ndef on_validation_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop begins.\"\"\"",
    "func_name": "on_validation_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation loop begins.",
    "code": "def on_validation_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop begins.\"\"\"",
    "docstring_summary": "Called when the validation loop begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 28,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_validation_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the validation loop ends.\n\n--- Code ---\ndef on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop ends.\"\"\"\n\n--- Original String ---\ndef on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop ends.\"\"\"",
    "func_name": "on_validation_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the validation loop ends.",
    "code": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop ends.\"\"\"",
    "docstring_summary": "Called when the validation loop ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 29,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test begins.\n\n--- Code ---\ndef on_test_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test begins.\"\"\"\n\n--- Original String ---\ndef on_test_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test begins.\"\"\"",
    "func_name": "on_test_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test begins.",
    "code": "def on_test_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test begins.\"\"\"",
    "docstring_summary": "Called when the test begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 30,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_test_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the test ends.\n\n--- Code ---\ndef on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test ends.\"\"\"\n\n--- Original String ---\ndef on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test ends.\"\"\"",
    "func_name": "on_test_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the test ends.",
    "code": "def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test ends.\"\"\"",
    "docstring_summary": "Called when the test ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 31,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the predict begins.\n\n--- Code ---\ndef on_predict_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict begins.\"\"\"\n\n--- Original String ---\ndef on_predict_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict begins.\"\"\"",
    "func_name": "on_predict_start",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the predict begins.",
    "code": "def on_predict_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict begins.\"\"\"",
    "docstring_summary": "Called when the predict begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 32,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_predict_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when predict ends.\n\n--- Code ---\ndef on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when predict ends.\"\"\"\n\n--- Original String ---\ndef on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when predict ends.\"\"\"",
    "func_name": "on_predict_end",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when predict ends.",
    "code": "def on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when predict ends.\"\"\"",
    "docstring_summary": "Called when predict ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 33,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_exception\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when any trainer execution is interrupted by an exception.\n\n--- Code ---\ndef on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Called when any trainer execution is interrupted by an exception.\"\"\"\n\n--- Original String ---\ndef on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Called when any trainer execution is interrupted by an exception.\"\"\"",
    "func_name": "on_exception",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when any trainer execution is interrupted by an exception.",
    "code": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Called when any trainer execution is interrupted by an exception.\"\"\"",
    "docstring_summary": "Called when any trainer execution is interrupted by an exception."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 34,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: state_dict\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when saving a checkpoint, implement to generate callback's ``state_dict``.\n\n--- Code ---\ndef state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}\n\n--- Original String ---\ndef state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}",
    "func_name": "state_dict",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when saving a checkpoint, implement to generate callback's ``state_dict``.",
    "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}",
    "docstring_summary": "Called when saving a checkpoint, implement to generate callback's ``state_dict``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 35,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: load_state_dict\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\n\n--- Code ---\ndef load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass\n\n--- Original String ---\ndef load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass",
    "func_name": "load_state_dict",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.",
    "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass",
    "docstring_summary": "Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 36,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_save_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\n\n--- Code ---\ndef on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\n\n--- Original String ---\ndef on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"",
    "func_name": "on_save_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.",
    "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"",
    "docstring_summary": "r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 37,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_load_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Called when loading a model checkpoint, use to reload state.\n\n--- Code ---\ndef on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\n\n--- Original String ---\ndef on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"",
    "func_name": "on_load_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Called when loading a model checkpoint, use to reload state.",
    "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"",
    "docstring_summary": "r\"\"\"Called when loading a model checkpoint, use to reload state."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 38,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_before_backward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled before ``loss.backward()``.\n\n--- Code ---\ndef on_before_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\"\"\"\n\n--- Original String ---\ndef on_before_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\"\"\"",
    "func_name": "on_before_backward",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called before ``loss.backward()``.",
    "code": "def on_before_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\"\"\"",
    "docstring_summary": "Called before ``loss.backward()``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 39,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_after_backward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled after ``loss.backward()`` and before optimizers are stepped.\n\n--- Code ---\ndef on_after_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\"\n\n--- Original String ---\ndef on_after_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\"",
    "func_name": "on_after_backward",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called after ``loss.backward()`` and before optimizers are stepped.",
    "code": "def on_after_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\"",
    "docstring_summary": "Called after ``loss.backward()`` and before optimizers are stepped."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 40,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_before_optimizer_step\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled before ``optimizer.step()``.\n\n--- Code ---\ndef on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"\n\n--- Original String ---\ndef on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"",
    "func_name": "on_before_optimizer_step",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called before ``optimizer.step()``.",
    "code": "def on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"",
    "docstring_summary": "Called before ``optimizer.step()``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "index": 41,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\callback.py\nFunction Name: on_before_zero_grad\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled before ``optimizer.zero_grad()``.\n\n--- Code ---\ndef on_before_zero_grad(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.zero_grad()``.\"\"\"\n\n--- Original String ---\ndef on_before_zero_grad(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.zero_grad()``.\"\"\"",
    "func_name": "on_before_zero_grad",
    "path": "src\\lightning\\pytorch\\callbacks\\callback.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called before ``optimizer.zero_grad()``.",
    "code": "def on_before_zero_grad(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.zero_grad()``.\"\"\"",
    "docstring_summary": "Called before ``optimizer.zero_grad()``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\early_stopping.py",
    "index": 42,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\early_stopping.py\nFunction Name: _run_early_stopping_check\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChecks whether the early stopping condition is met and if so tells the trainer to stop the training.\n\n--- Code ---\ndef _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        # stop every ddp process if any world process decides to stop\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)\n\n--- Original String ---\ndef _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        # stop every ddp process if any world process decides to stop\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)",
    "func_name": "_run_early_stopping_check",
    "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training.",
    "code": "def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        # stop every ddp process if any world process decides to stop\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)",
    "docstring_summary": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\early_stopping.py",
    "index": 43,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\early_stopping.py\nFunction Name: _improvement_message\nLanguage: python\nPartition: train\n\n--- Docstring ---\nFormats a log message that informs the user about an improvement in the monitored score.\n\n--- Code ---\ndef _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg\n\n--- Original String ---\ndef _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg",
    "func_name": "_improvement_message",
    "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Formats a log message that informs the user about an improvement in the monitored score.",
    "code": "def _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg",
    "docstring_summary": "Formats a log message that informs the user about an improvement in the monitored score."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 44,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: flatten_modules\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\n\n--- Code ---\ndef flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        # Capture all leaf modules as well as parent modules that have parameters directly themselves\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]\n\n--- Original String ---\ndef flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        # Capture all leaf modules as well as parent modules that have parameters directly themselves\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]",
    "func_name": "flatten_modules",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules",
    "code": "def flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        # Capture all leaf modules as well as parent modules that have parameters directly themselves\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]",
    "docstring_summary": "This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 45,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: filter_params\nLanguage: python\nPartition: train\n\n--- Docstring ---\nYields the `requires_grad` parameters of a given module or list of modules.\n\n--- Code ---\ndef filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param\n\n--- Original String ---\ndef filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param",
    "func_name": "filter_params",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Yields the `requires_grad` parameters of a given module or list of modules.",
    "code": "def filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param",
    "docstring_summary": "Yields the `requires_grad` parameters of a given module or list of modules."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 46,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: make_trainable\nLanguage: python\nPartition: train\n\n--- Docstring ---\nUnfreezes the parameters of the provided modules.\n\n--- Code ---\ndef make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True\n\n--- Original String ---\ndef make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True",
    "func_name": "make_trainable",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Unfreezes the parameters of the provided modules.",
    "code": "def make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True",
    "docstring_summary": "Unfreezes the parameters of the provided modules."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 47,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: freeze_module\nLanguage: python\nPartition: train\n\n--- Docstring ---\nFreezes the parameters of the provided module.\n\n--- Code ---\ndef freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False\n\n--- Original String ---\ndef freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False",
    "func_name": "freeze_module",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Freezes the parameters of the provided module.",
    "code": "def freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False",
    "docstring_summary": "Freezes the parameters of the provided module."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 48,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: freeze\nLanguage: python\nPartition: train\n\n--- Docstring ---\nFreezes the parameters of the provided modules.\n\n--- Code ---\ndef freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)\n\n--- Original String ---\ndef freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)",
    "func_name": "freeze",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Freezes the parameters of the provided modules.",
    "code": "def freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)",
    "docstring_summary": "Freezes the parameters of the provided modules."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 49,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: filter_on_optimizer\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function is used to exclude any parameter which already exists in this optimizer.\n\n--- Code ---\ndef filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params\n\n--- Original String ---\ndef filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params",
    "func_name": "filter_on_optimizer",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function is used to exclude any parameter which already exists in this optimizer.",
    "code": "def filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params",
    "docstring_summary": "This function is used to exclude any parameter which already exists in this optimizer."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 50,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: unfreeze_and_add_param_group\nLanguage: python\nPartition: train\n\n--- Docstring ---\nUnfreezes a module and adds its parameters to an optimizer.\n\n--- Code ---\ndef unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})\n\n--- Original String ---\ndef unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})",
    "func_name": "unfreeze_and_add_param_group",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Unfreezes a module and adds its parameters to an optimizer.",
    "code": "def unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})",
    "docstring_summary": "Unfreezes a module and adds its parameters to an optimizer."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 51,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: on_train_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the epoch begins.\n\n--- Code ---\ndef on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)\n\n--- Original String ---\ndef on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)",
    "func_name": "on_train_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the epoch begins.",
    "code": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)",
    "docstring_summary": "Called when the epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 52,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: finetune_function\nLanguage: python\nPartition: train\n\n--- Docstring ---\nOverride to add your unfreeze logic.\n\n--- Code ---\ndef finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override to add your unfreeze logic.\"\"\"\r\n        raise NotImplementedError\n\n--- Original String ---\ndef finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override to add your unfreeze logic.\"\"\"\r\n        raise NotImplementedError",
    "func_name": "finetune_function",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Override to add your unfreeze logic.",
    "code": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override to add your unfreeze logic.\"\"\"\r\n        raise NotImplementedError",
    "docstring_summary": "Override to add your unfreeze logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 53,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: freeze_before_training\nLanguage: python\nPartition: train\n\n--- Docstring ---\nOverride to add your freeze logic.\n\n--- Code ---\ndef freeze_before_training(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Override to add your freeze logic.\"\"\"\r\n        raise NotImplementedError\n\n--- Original String ---\ndef freeze_before_training(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Override to add your freeze logic.\"\"\"\r\n        raise NotImplementedError",
    "func_name": "freeze_before_training",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Override to add your freeze logic.",
    "code": "def freeze_before_training(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Override to add your freeze logic.\"\"\"\r\n        raise NotImplementedError",
    "docstring_summary": "Override to add your freeze logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 54,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: on_fit_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRaises:\n\n--- Code ---\ndef on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")\n\n--- Original String ---\ndef on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")",
    "func_name": "on_fit_start",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Raises:",
    "code": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")",
    "docstring_summary": "Raises:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "index": 55,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\finetuning.py\nFunction Name: finetune_function\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when the epoch begins.\n\n--- Code ---\ndef finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )\n\n--- Original String ---\ndef finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )",
    "func_name": "finetune_function",
    "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when the epoch begins.",
    "code": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )",
    "docstring_summary": "Called when the epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py",
    "index": 56,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py\nFunction Name: on_train_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nPerformns a configuration validation before training starts and raises errors for incompatible settings.\n\n--- Code ---\ndef on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )\n\n--- Original String ---\ndef on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )",
    "func_name": "on_train_start",
    "path": "src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Performns a configuration validation before training starts and raises errors for incompatible settings.",
    "code": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )",
    "docstring_summary": "Performns a configuration validation before training starts and raises errors for incompatible settings."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "index": 57,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\lr_monitor.py\nFunction Name: on_train_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled before training, determines unique names for all lr schedulers in the case of multiple of the same\n\n--- Code ---\ndef on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        # Find names for schedulers\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        # Find names for leftover optimizers\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        # Initialize for storing values\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}\n\n--- Original String ---\ndef on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        # Find names for schedulers\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        # Find names for leftover optimizers\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        # Initialize for storing values\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}",
    "func_name": "on_train_start",
    "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called before training, determines unique names for all lr schedulers in the case of multiple of the same",
    "code": "def on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        # Find names for schedulers\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        # Find names for leftover optimizers\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        # Initialize for storing values\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}",
    "docstring_summary": "Called before training, determines unique names for all lr schedulers in the case of multiple of the same"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "index": 58,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\lr_monitor.py\nFunction Name: _remap_keys\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function is used the remap the keys if param groups for a given optimizer increased.\n\n--- Code ---\ndef _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []\n\n--- Original String ---\ndef _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []",
    "func_name": "_remap_keys",
    "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function is used the remap the keys if param groups for a given optimizer increased.",
    "code": "def _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []",
    "docstring_summary": "This function is used the remap the keys if param groups for a given optimizer increased."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "index": 59,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\lr_monitor.py\nFunction Name: _extract_weight_decay\nLanguage: python\nPartition: train\n\n--- Docstring ---\nExtracts the weight decay statistics from a parameter group.\n\n--- Code ---\ndef _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}\n\n--- Original String ---\ndef _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}",
    "func_name": "_extract_weight_decay",
    "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Extracts the weight decay statistics from a parameter group.",
    "code": "def _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}",
    "docstring_summary": "Extracts the weight decay statistics from a parameter group."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 60,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: on_train_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSave checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\n\n--- Code ---\ndef on_train_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n    ) -> None:\r\n        \"\"\"Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\"\"\"\r\n        # Do not return early here because we may need to set deferral flags even\r\n        # if a save already happened at this global step. We'll enforce the skip\r\n        # just before actually saving below.\r\n        skip_due_to_state = self._should_skip_saving_checkpoint(trainer)\r\n        skip_batch = self._every_n_train_steps < 1 or (trainer.global_step % self._every_n_train_steps != 0)\r\n\r\n        train_time_interval = self._train_time_interval\r\n        skip_time = True\r\n        now = time.monotonic()\r\n        # Important: allow zero timedelta as a valid interval\r\n        if train_time_interval is not None:\r\n            prev_time_check = self._last_time_checked\r\n            skip_time = prev_time_check is None or (now - prev_time_check) < train_time_interval.total_seconds()\r\n            # in case we have time differences across ranks\r\n            # broadcast the decision on whether to checkpoint from rank 0 to avoid possible hangs\r\n            skip_time = trainer.strategy.broadcast(skip_time)\r\n\r\n        if skip_batch and skip_time:\r\n            return\r\n        if not skip_time:\r\n            self._last_time_checked = now\r\n\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        # If monitoring a metric that is not yet available (e.g., validation-only),\r\n        # defer saving until validation end so the metric is present.\r\n        if self.monitor is not None and self.monitor not in monitor_candidates:\r\n            # Defer both top-k and last to avoid blocking with `_last_global_step_saved`\r\n            self._defer_save_until_validation = True\r\n            return\r\n\r\n        # Even if the monitored key exists, it could be stale from a previous validation.\r\n        # If validation is scheduled to run right after this batch (e.g., last batch of epoch)\r\n        # and we are not saving at train epoch end, defer to `on_validation_end` to use fresh metrics.\r\n        if (\r\n            self.monitor is not None\r\n            and not self._should_save_on_train_epoch_end(trainer)\r\n            and getattr(trainer.fit_loop.epoch_loop.batch_progress, \"is_last_batch\", False)\r\n        ):\r\n            # Only defer if a validation loop is expected to run after this batch.\r\n            will_run_val = False\r\n            if getattr(trainer, \"enable_validation\", False):\r\n                num_val_batches = (\r\n                    sum(trainer.num_val_batches)\r\n                    if isinstance(trainer.num_val_batches, list)\r\n                    else trainer.num_val_batches\r\n                )\r\n                if num_val_batches and num_val_batches > 0:\r\n                    cve = trainer.check_val_every_n_epoch\r\n                    if cve is None or ((trainer.current_epoch + 1) % cve == 0):\r\n                        will_run_val = True\r\n\r\n            if will_run_val:\r\n                self._defer_save_until_validation = True\r\n                return\r\n\r\n        # Only proceed to save if not skipping due to trainer/callback state\r\n        if skip_due_to_state:\r\n            return\r\n\r\n        self._save_topk_checkpoint(trainer, monitor_candidates)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\n\n--- Original String ---\ndef on_train_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n    ) -> None:\r\n        \"\"\"Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\"\"\"\r\n        # Do not return early here because we may need to set deferral flags even\r\n        # if a save already happened at this global step. We'll enforce the skip\r\n        # just before actually saving below.\r\n        skip_due_to_state = self._should_skip_saving_checkpoint(trainer)\r\n        skip_batch = self._every_n_train_steps < 1 or (trainer.global_step % self._every_n_train_steps != 0)\r\n\r\n        train_time_interval = self._train_time_interval\r\n        skip_time = True\r\n        now = time.monotonic()\r\n        # Important: allow zero timedelta as a valid interval\r\n        if train_time_interval is not None:\r\n            prev_time_check = self._last_time_checked\r\n            skip_time = prev_time_check is None or (now - prev_time_check) < train_time_interval.total_seconds()\r\n            # in case we have time differences across ranks\r\n            # broadcast the decision on whether to checkpoint from rank 0 to avoid possible hangs\r\n            skip_time = trainer.strategy.broadcast(skip_time)\r\n\r\n        if skip_batch and skip_time:\r\n            return\r\n        if not skip_time:\r\n            self._last_time_checked = now\r\n\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        # If monitoring a metric that is not yet available (e.g., validation-only),\r\n        # defer saving until validation end so the metric is present.\r\n        if self.monitor is not None and self.monitor not in monitor_candidates:\r\n            # Defer both top-k and last to avoid blocking with `_last_global_step_saved`\r\n            self._defer_save_until_validation = True\r\n            return\r\n\r\n        # Even if the monitored key exists, it could be stale from a previous validation.\r\n        # If validation is scheduled to run right after this batch (e.g., last batch of epoch)\r\n        # and we are not saving at train epoch end, defer to `on_validation_end` to use fresh metrics.\r\n        if (\r\n            self.monitor is not None\r\n            and not self._should_save_on_train_epoch_end(trainer)\r\n            and getattr(trainer.fit_loop.epoch_loop.batch_progress, \"is_last_batch\", False)\r\n        ):\r\n            # Only defer if a validation loop is expected to run after this batch.\r\n            will_run_val = False\r\n            if getattr(trainer, \"enable_validation\", False):\r\n                num_val_batches = (\r\n                    sum(trainer.num_val_batches)\r\n                    if isinstance(trainer.num_val_batches, list)\r\n                    else trainer.num_val_batches\r\n                )\r\n                if num_val_batches and num_val_batches > 0:\r\n                    cve = trainer.check_val_every_n_epoch\r\n                    if cve is None or ((trainer.current_epoch + 1) % cve == 0):\r\n                        will_run_val = True\r\n\r\n            if will_run_val:\r\n                self._defer_save_until_validation = True\r\n                return\r\n\r\n        # Only proceed to save if not skipping due to trainer/callback state\r\n        if skip_due_to_state:\r\n            return\r\n\r\n        self._save_topk_checkpoint(trainer, monitor_candidates)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)",
    "func_name": "on_train_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`",
    "code": "def on_train_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n    ) -> None:\r\n        \"\"\"Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\"\"\"\r\n        # Do not return early here because we may need to set deferral flags even\r\n        # if a save already happened at this global step. We'll enforce the skip\r\n        # just before actually saving below.\r\n        skip_due_to_state = self._should_skip_saving_checkpoint(trainer)\r\n        skip_batch = self._every_n_train_steps < 1 or (trainer.global_step % self._every_n_train_steps != 0)\r\n\r\n        train_time_interval = self._train_time_interval\r\n        skip_time = True\r\n        now = time.monotonic()\r\n        # Important: allow zero timedelta as a valid interval\r\n        if train_time_interval is not None:\r\n            prev_time_check = self._last_time_checked\r\n            skip_time = prev_time_check is None or (now - prev_time_check) < train_time_interval.total_seconds()\r\n            # in case we have time differences across ranks\r\n            # broadcast the decision on whether to checkpoint from rank 0 to avoid possible hangs\r\n            skip_time = trainer.strategy.broadcast(skip_time)\r\n\r\n        if skip_batch and skip_time:\r\n            return\r\n        if not skip_time:\r\n            self._last_time_checked = now\r\n\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        # If monitoring a metric that is not yet available (e.g., validation-only),\r\n        # defer saving until validation end so the metric is present.\r\n        if self.monitor is not None and self.monitor not in monitor_candidates:\r\n            # Defer both top-k and last to avoid blocking with `_last_global_step_saved`\r\n            self._defer_save_until_validation = True\r\n            return\r\n\r\n        # Even if the monitored key exists, it could be stale from a previous validation.\r\n        # If validation is scheduled to run right after this batch (e.g., last batch of epoch)\r\n        # and we are not saving at train epoch end, defer to `on_validation_end` to use fresh metrics.\r\n        if (\r\n            self.monitor is not None\r\n            and not self._should_save_on_train_epoch_end(trainer)\r\n            and getattr(trainer.fit_loop.epoch_loop.batch_progress, \"is_last_batch\", False)\r\n        ):\r\n            # Only defer if a validation loop is expected to run after this batch.\r\n            will_run_val = False\r\n            if getattr(trainer, \"enable_validation\", False):\r\n                num_val_batches = (\r\n                    sum(trainer.num_val_batches)\r\n                    if isinstance(trainer.num_val_batches, list)\r\n                    else trainer.num_val_batches\r\n                )\r\n                if num_val_batches and num_val_batches > 0:\r\n                    cve = trainer.check_val_every_n_epoch\r\n                    if cve is None or ((trainer.current_epoch + 1) % cve == 0):\r\n                        will_run_val = True\r\n\r\n            if will_run_val:\r\n                self._defer_save_until_validation = True\r\n                return\r\n\r\n        # Only proceed to save if not skipping due to trainer/callback state\r\n        if skip_due_to_state:\r\n            return\r\n\r\n        self._save_topk_checkpoint(trainer, monitor_candidates)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)",
    "docstring_summary": "Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 61,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: on_train_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSave a checkpoint at the end of the training epoch.\n\n--- Code ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)\n\n--- Original String ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "func_name": "on_train_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Save a checkpoint at the end of the training epoch.",
    "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "docstring_summary": "Save a checkpoint at the end of the training epoch."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 62,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: on_validation_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSave a checkpoint at the end of the validation stage.\n\n--- Code ---\ndef on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            # If a step/time-triggered save was deferred due to a missing monitored metric,\r\n            # perform the save now that validation metrics are available.\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)\n\n--- Original String ---\ndef on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            # If a step/time-triggered save was deferred due to a missing monitored metric,\r\n            # perform the save now that validation metrics are available.\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "func_name": "on_validation_end",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Save a checkpoint at the end of the validation stage.",
    "code": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            # If a step/time-triggered save was deferred due to a missing monitored metric,\r\n            # perform the save now that validation metrics are available.\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "docstring_summary": "Save a checkpoint at the end of the validation stage."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 63,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: on_exception\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSave a checkpoint when an exception is raised.\n\n--- Code ---\ndef on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )\n\n--- Original String ---\ndef on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )",
    "func_name": "on_exception",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Save a checkpoint when an exception is raised.",
    "code": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )",
    "docstring_summary": "Save a checkpoint when an exception is raised."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 64,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: on_train_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nEnsure save_last=True is applied when training ends.\n\n--- Code ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)\n\n--- Original String ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "func_name": "on_train_end",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Ensure save_last=True is applied when training ends.",
    "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)",
    "docstring_summary": "Ensure save_last=True is applied when training ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 65,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: format_checkpoint_name\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGenerate a filename according to the defined template.\n\n--- Code ---\ndef format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name\n\n--- Original String ---\ndef format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name",
    "func_name": "format_checkpoint_name",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Generate a filename according to the defined template.",
    "code": "def format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name",
    "docstring_summary": "Generate a filename according to the defined template."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 66,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: __resolve_ckpt_dir\nLanguage: python\nPartition: train\n\n--- Docstring ---\nDetermines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\n\n--- Code ---\ndef __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            # short circuit if dirpath was passed to ModelCheckpoint\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            # if no loggers, use default_root_dir\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path\n\n--- Original String ---\ndef __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            # short circuit if dirpath was passed to ModelCheckpoint\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            # if no loggers, use default_root_dir\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path",
    "func_name": "__resolve_ckpt_dir",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to",
    "code": "def __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            # short circuit if dirpath was passed to ModelCheckpoint\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            # if no loggers, use default_root_dir\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path",
    "docstring_summary": "Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 67,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: to_yaml\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSaves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\n\n--- Code ---\ndef to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)\n\n--- Original String ---\ndef to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)",
    "func_name": "to_yaml",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML",
    "code": "def to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)",
    "docstring_summary": "Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 68,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: file_exists\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChecks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\n\n--- Code ---\ndef file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)\n\n--- Original String ---\ndef file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)",
    "func_name": "file_exists",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal",
    "code": "def file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)",
    "docstring_summary": "Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 69,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: _should_remove_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChecks if the previous checkpoint should be deleted.\n\n--- Code ---\ndef _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents\n\n--- Original String ---\ndef _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents",
    "func_name": "_should_remove_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Checks if the previous checkpoint should be deleted.",
    "code": "def _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents",
    "docstring_summary": "Checks if the previous checkpoint should be deleted."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "index": 70,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\model_checkpoint.py\nFunction Name: _remove_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalls the strategy to remove the checkpoint file.\n\n--- Code ---\ndef _remove_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\r\n        \"\"\"Calls the strategy to remove the checkpoint file.\"\"\"\r\n        trainer.strategy.remove_checkpoint(filepath)\n\n--- Original String ---\ndef _remove_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\r\n        \"\"\"Calls the strategy to remove the checkpoint file.\"\"\"\r\n        trainer.strategy.remove_checkpoint(filepath)",
    "func_name": "_remove_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Calls the strategy to remove the checkpoint file.",
    "code": "def _remove_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\r\n        \"\"\"Calls the strategy to remove the checkpoint file.\"\"\"\r\n        trainer.strategy.remove_checkpoint(filepath)",
    "docstring_summary": "Calls the strategy to remove the checkpoint file."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py",
    "index": 71,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\prediction_writer.py\nFunction Name: write_on_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nOverride with the logic to write a single batch.\n\n--- Code ---\ndef write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()\n\n--- Original String ---\ndef write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()",
    "func_name": "write_on_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Override with the logic to write a single batch.",
    "code": "def write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()",
    "docstring_summary": "Override with the logic to write a single batch."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py",
    "index": 72,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\prediction_writer.py\nFunction Name: write_on_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nOverride with the logic to write all batches.\n\n--- Code ---\ndef write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()\n\n--- Original String ---\ndef write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()",
    "func_name": "write_on_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Override with the logic to write all batches.",
    "code": "def write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()",
    "docstring_summary": "Override with the logic to write all batches."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 73,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: __init__\nLanguage: python\nPartition: train\n\n--- Docstring ---\nModel pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\n\n--- Code ---\ndef __init__(\r\n        self,\r\n        pruning_fn: Union[Callable, str],\r\n        parameters_to_prune: _PARAM_LIST = (),\r\n        parameter_names: Optional[list[str]] = None,\r\n        use_global_unstructured: bool = True,\r\n        amount: Union[int, float, Callable[[int], Union[int, float]]] = 0.5,\r\n        apply_pruning: Union[bool, Callable[[int], bool]] = True,\r\n        make_pruning_permanent: bool = True,\r\n        use_lottery_ticket_hypothesis: Union[bool, Callable[[int], bool]] = True,\r\n        resample_parameters: bool = False,\r\n        pruning_dim: Optional[int] = None,\r\n        pruning_norm: Optional[int] = None,\r\n        verbose: int = 0,\r\n        prune_on_train_epoch_end: bool = True,\r\n    ) -> None:\r\n        \"\"\"Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\r\n        parameters during training.\r\n\r\n        To learn more about pruning with PyTorch, please take a look at\r\n        `this tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. code-block:: python\r\n\r\n            parameters_to_prune = [(model.mlp_1, \"weight\"), (model.mlp_2, \"weight\")]\r\n\r\n            trainer = Trainer(\r\n                callbacks=[\r\n                    ModelPruning(\r\n                        pruning_fn=\"l1_unstructured\",\r\n                        parameters_to_prune=parameters_to_prune,\r\n                        amount=0.01,\r\n                        use_global_unstructured=True,\r\n                    )\r\n                ]\r\n            )\r\n\r\n        When ``parameters_to_prune`` is ``None``, ``parameters_to_prune`` will contain all parameters from the model.\r\n        The user can override ``filter_parameters_to_prune`` to filter any ``nn.Module`` to be pruned.\r\n\r\n        Args:\r\n\r\n            pruning_fn: Function from torch.nn.utils.prune module or your own PyTorch ``BasePruningMethod`` subclass.\r\n                Can also be string e.g. `\"l1_unstructured\"`. See pytorch docs for more details.\r\n\r\n            parameters_to_prune: List of tuples ``(nn.Module, \"parameter_name_string\")``.\r\n\r\n            parameter_names: List of parameter names to be pruned from the nn.Module.\r\n                Can either be ``\"weight\"`` or ``\"bias\"``.\r\n\r\n            use_global_unstructured: Whether to apply pruning globally on the model.\r\n                If ``parameters_to_prune`` is provided, global unstructured will be restricted on them.\r\n\r\n            amount: Quantity of parameters to prune:\r\n\r\n                - ``float``. Between 0.0 and 1.0. Represents the fraction of parameters to prune.\r\n                - ``int``. Represents the absolute number of parameters to prune.\r\n                - ``Callable``. For dynamic values. Will be called every epoch. Should return a value.\r\n\r\n            apply_pruning: Whether to apply pruning.\r\n\r\n                - ``bool``. Always apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            make_pruning_permanent: Whether to remove all reparameterization pre-hooks and apply masks\r\n                when training ends or the model is saved.\r\n\r\n            use_lottery_ticket_hypothesis: See `The lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`_:\r\n\r\n                - ``bool``. Whether to apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            resample_parameters: Used with ``use_lottery_ticket_hypothesis``. If True, the model parameters will\r\n                be resampled, otherwise, the exact original parameters will be used.\r\n\r\n            pruning_dim: If you are using a structured pruning method you need to specify the dimension.\r\n\r\n            pruning_norm: If you are using ``ln_structured`` you need to specify the norm.\r\n\r\n            verbose: Verbosity level. 0 to disable, 1 to log overall sparsity, 2 to log per-layer sparsity\r\n\r\n            prune_on_train_epoch_end: whether to apply pruning at the end of the training epoch.\r\n                If this is ``False``, then the check runs at the end of the validation epoch.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameter_names`` is neither ``\"weight\"`` nor ``\"bias\"``,\r\n                if the provided ``pruning_fn`` is not supported,\r\n                if ``pruning_dim`` is not provided when ``\"unstructured\"``,\r\n                if ``pruning_norm`` is not provided when ``\"ln_structured\"``,\r\n                if ``pruning_fn`` is neither ``str`` nor :class:`torch.nn.utils.prune.BasePruningMethod`, or\r\n                if ``amount`` is none of ``int``, ``float`` and ``Callable``.\r\n\r\n        \"\"\"\r\n\r\n        self._use_global_unstructured = use_global_unstructured\r\n        self._parameters_to_prune = parameters_to_prune\r\n        self._use_lottery_ticket_hypothesis = use_lottery_ticket_hypothesis\r\n        self._resample_parameters = resample_parameters\r\n        self._prune_on_train_epoch_end = prune_on_train_epoch_end\r\n        self._parameter_names = parameter_names or self.PARAMETER_NAMES\r\n        self._global_kwargs: dict[str, Any] = {}\r\n        self._original_layers: Optional[dict[int, _LayerRef]] = None\r\n        self._pruning_method_name: Optional[str] = None\r\n\r\n        for name in self._parameter_names:\r\n            if name not in self.PARAMETER_NAMES:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `parameter_names` name: {name} isn't in {self.PARAMETER_NAMES}\"\r\n                )\r\n\r\n        if isinstance(pruning_fn, str):\r\n            pruning_kwargs = {}\r\n            pruning_fn = pruning_fn.lower()\r\n            if pruning_fn not in _PYTORCH_PRUNING_FUNCTIONS:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `pruning_fn` {pruning_fn} isn't available in PyTorch's\"\r\n                    f\" built-in functions: {list(_PYTORCH_PRUNING_FUNCTIONS.keys())} \"\r\n                )\r\n            if pruning_fn.endswith(\"_structured\"):\r\n                if pruning_dim is None:\r\n                    raise MisconfigurationException(\r\n                        \"When requesting `structured` pruning, the `pruning_dim` should be provided.\"\r\n                    )\r\n                if pruning_fn == \"ln_structured\":\r\n                    if pruning_norm is None:\r\n                        raise MisconfigurationException(\r\n                            \"When requesting `ln_structured` pruning, the `pruning_norm` should be provided.\"\r\n                        )\r\n                    pruning_kwargs[\"n\"] = pruning_norm\r\n                pruning_kwargs[\"dim\"] = pruning_dim\r\n            pruning_fn = self._create_pruning_fn(pruning_fn, **pruning_kwargs)\r\n        elif self._is_pruning_method(pruning_fn):\r\n            if not use_global_unstructured:\r\n                raise MisconfigurationException(\r\n                    \"PyTorch `BasePruningMethod` is currently only supported with `use_global_unstructured=True`.\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                f\"`pruning_fn` is expected to be a str in {list(_PYTORCH_PRUNING_FUNCTIONS.keys())}\"\r\n                f\" or a PyTorch `BasePruningMethod`. Found: {pruning_fn}.\"\r\n                \" HINT: if passing a `BasePruningMethod`, pass the class, not an instance\"\r\n            )\r\n\r\n        # need to ignore typing here since pytorch base class does not define the PRUNING_TYPE attribute\r\n        if use_global_unstructured and pruning_fn.PRUNING_TYPE != \"unstructured\":  # type: ignore\r\n            raise MisconfigurationException(\r\n                'Only the \"unstructured\" PRUNING_TYPE is supported with `use_global_unstructured=True`.'\r\n                f\" Found method {pruning_fn} of type {pruning_fn.PRUNING_TYPE}. \"  # type: ignore[union-attr]\r\n            )\r\n\r\n        self.pruning_fn = pruning_fn\r\n        self._apply_pruning = apply_pruning\r\n        self._make_pruning_permanent = make_pruning_permanent\r\n\r\n        if not (isinstance(amount, (int, float)) or callable(amount)):\r\n            raise MisconfigurationException(\r\n                \"`amount` should be provided and be either an int, a float or Callable function.\"\r\n            )\r\n\r\n        self.amount = amount\r\n\r\n        if verbose not in (0, 1, 2):\r\n            raise MisconfigurationException(\"`verbose` must be any of (0, 1, 2)\")\r\n\r\n        self._verbose = verbose\n\n--- Original String ---\ndef __init__(\r\n        self,\r\n        pruning_fn: Union[Callable, str],\r\n        parameters_to_prune: _PARAM_LIST = (),\r\n        parameter_names: Optional[list[str]] = None,\r\n        use_global_unstructured: bool = True,\r\n        amount: Union[int, float, Callable[[int], Union[int, float]]] = 0.5,\r\n        apply_pruning: Union[bool, Callable[[int], bool]] = True,\r\n        make_pruning_permanent: bool = True,\r\n        use_lottery_ticket_hypothesis: Union[bool, Callable[[int], bool]] = True,\r\n        resample_parameters: bool = False,\r\n        pruning_dim: Optional[int] = None,\r\n        pruning_norm: Optional[int] = None,\r\n        verbose: int = 0,\r\n        prune_on_train_epoch_end: bool = True,\r\n    ) -> None:\r\n        \"\"\"Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\r\n        parameters during training.\r\n\r\n        To learn more about pruning with PyTorch, please take a look at\r\n        `this tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. code-block:: python\r\n\r\n            parameters_to_prune = [(model.mlp_1, \"weight\"), (model.mlp_2, \"weight\")]\r\n\r\n            trainer = Trainer(\r\n                callbacks=[\r\n                    ModelPruning(\r\n                        pruning_fn=\"l1_unstructured\",\r\n                        parameters_to_prune=parameters_to_prune,\r\n                        amount=0.01,\r\n                        use_global_unstructured=True,\r\n                    )\r\n                ]\r\n            )\r\n\r\n        When ``parameters_to_prune`` is ``None``, ``parameters_to_prune`` will contain all parameters from the model.\r\n        The user can override ``filter_parameters_to_prune`` to filter any ``nn.Module`` to be pruned.\r\n\r\n        Args:\r\n\r\n            pruning_fn: Function from torch.nn.utils.prune module or your own PyTorch ``BasePruningMethod`` subclass.\r\n                Can also be string e.g. `\"l1_unstructured\"`. See pytorch docs for more details.\r\n\r\n            parameters_to_prune: List of tuples ``(nn.Module, \"parameter_name_string\")``.\r\n\r\n            parameter_names: List of parameter names to be pruned from the nn.Module.\r\n                Can either be ``\"weight\"`` or ``\"bias\"``.\r\n\r\n            use_global_unstructured: Whether to apply pruning globally on the model.\r\n                If ``parameters_to_prune`` is provided, global unstructured will be restricted on them.\r\n\r\n            amount: Quantity of parameters to prune:\r\n\r\n                - ``float``. Between 0.0 and 1.0. Represents the fraction of parameters to prune.\r\n                - ``int``. Represents the absolute number of parameters to prune.\r\n                - ``Callable``. For dynamic values. Will be called every epoch. Should return a value.\r\n\r\n            apply_pruning: Whether to apply pruning.\r\n\r\n                - ``bool``. Always apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            make_pruning_permanent: Whether to remove all reparameterization pre-hooks and apply masks\r\n                when training ends or the model is saved.\r\n\r\n            use_lottery_ticket_hypothesis: See `The lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`_:\r\n\r\n                - ``bool``. Whether to apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            resample_parameters: Used with ``use_lottery_ticket_hypothesis``. If True, the model parameters will\r\n                be resampled, otherwise, the exact original parameters will be used.\r\n\r\n            pruning_dim: If you are using a structured pruning method you need to specify the dimension.\r\n\r\n            pruning_norm: If you are using ``ln_structured`` you need to specify the norm.\r\n\r\n            verbose: Verbosity level. 0 to disable, 1 to log overall sparsity, 2 to log per-layer sparsity\r\n\r\n            prune_on_train_epoch_end: whether to apply pruning at the end of the training epoch.\r\n                If this is ``False``, then the check runs at the end of the validation epoch.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameter_names`` is neither ``\"weight\"`` nor ``\"bias\"``,\r\n                if the provided ``pruning_fn`` is not supported,\r\n                if ``pruning_dim`` is not provided when ``\"unstructured\"``,\r\n                if ``pruning_norm`` is not provided when ``\"ln_structured\"``,\r\n                if ``pruning_fn`` is neither ``str`` nor :class:`torch.nn.utils.prune.BasePruningMethod`, or\r\n                if ``amount`` is none of ``int``, ``float`` and ``Callable``.\r\n\r\n        \"\"\"\r\n\r\n        self._use_global_unstructured = use_global_unstructured\r\n        self._parameters_to_prune = parameters_to_prune\r\n        self._use_lottery_ticket_hypothesis = use_lottery_ticket_hypothesis\r\n        self._resample_parameters = resample_parameters\r\n        self._prune_on_train_epoch_end = prune_on_train_epoch_end\r\n        self._parameter_names = parameter_names or self.PARAMETER_NAMES\r\n        self._global_kwargs: dict[str, Any] = {}\r\n        self._original_layers: Optional[dict[int, _LayerRef]] = None\r\n        self._pruning_method_name: Optional[str] = None\r\n\r\n        for name in self._parameter_names:\r\n            if name not in self.PARAMETER_NAMES:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `parameter_names` name: {name} isn't in {self.PARAMETER_NAMES}\"\r\n                )\r\n\r\n        if isinstance(pruning_fn, str):\r\n            pruning_kwargs = {}\r\n            pruning_fn = pruning_fn.lower()\r\n            if pruning_fn not in _PYTORCH_PRUNING_FUNCTIONS:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `pruning_fn` {pruning_fn} isn't available in PyTorch's\"\r\n                    f\" built-in functions: {list(_PYTORCH_PRUNING_FUNCTIONS.keys())} \"\r\n                )\r\n            if pruning_fn.endswith(\"_structured\"):\r\n                if pruning_dim is None:\r\n                    raise MisconfigurationException(\r\n                        \"When requesting `structured` pruning, the `pruning_dim` should be provided.\"\r\n                    )\r\n                if pruning_fn == \"ln_structured\":\r\n                    if pruning_norm is None:\r\n                        raise MisconfigurationException(\r\n                            \"When requesting `ln_structured` pruning, the `pruning_norm` should be provided.\"\r\n                        )\r\n                    pruning_kwargs[\"n\"] = pruning_norm\r\n                pruning_kwargs[\"dim\"] = pruning_dim\r\n            pruning_fn = self._create_pruning_fn(pruning_fn, **pruning_kwargs)\r\n        elif self._is_pruning_method(pruning_fn):\r\n            if not use_global_unstructured:\r\n                raise MisconfigurationException(\r\n                    \"PyTorch `BasePruningMethod` is currently only supported with `use_global_unstructured=True`.\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                f\"`pruning_fn` is expected to be a str in {list(_PYTORCH_PRUNING_FUNCTIONS.keys())}\"\r\n                f\" or a PyTorch `BasePruningMethod`. Found: {pruning_fn}.\"\r\n                \" HINT: if passing a `BasePruningMethod`, pass the class, not an instance\"\r\n            )\r\n\r\n        # need to ignore typing here since pytorch base class does not define the PRUNING_TYPE attribute\r\n        if use_global_unstructured and pruning_fn.PRUNING_TYPE != \"unstructured\":  # type: ignore\r\n            raise MisconfigurationException(\r\n                'Only the \"unstructured\" PRUNING_TYPE is supported with `use_global_unstructured=True`.'\r\n                f\" Found method {pruning_fn} of type {pruning_fn.PRUNING_TYPE}. \"  # type: ignore[union-attr]\r\n            )\r\n\r\n        self.pruning_fn = pruning_fn\r\n        self._apply_pruning = apply_pruning\r\n        self._make_pruning_permanent = make_pruning_permanent\r\n\r\n        if not (isinstance(amount, (int, float)) or callable(amount)):\r\n            raise MisconfigurationException(\r\n                \"`amount` should be provided and be either an int, a float or Callable function.\"\r\n            )\r\n\r\n        self.amount = amount\r\n\r\n        if verbose not in (0, 1, 2):\r\n            raise MisconfigurationException(\"`verbose` must be any of (0, 1, 2)\")\r\n\r\n        self._verbose = verbose",
    "func_name": "__init__",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks",
    "code": "def __init__(\r\n        self,\r\n        pruning_fn: Union[Callable, str],\r\n        parameters_to_prune: _PARAM_LIST = (),\r\n        parameter_names: Optional[list[str]] = None,\r\n        use_global_unstructured: bool = True,\r\n        amount: Union[int, float, Callable[[int], Union[int, float]]] = 0.5,\r\n        apply_pruning: Union[bool, Callable[[int], bool]] = True,\r\n        make_pruning_permanent: bool = True,\r\n        use_lottery_ticket_hypothesis: Union[bool, Callable[[int], bool]] = True,\r\n        resample_parameters: bool = False,\r\n        pruning_dim: Optional[int] = None,\r\n        pruning_norm: Optional[int] = None,\r\n        verbose: int = 0,\r\n        prune_on_train_epoch_end: bool = True,\r\n    ) -> None:\r\n        \"\"\"Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\r\n        parameters during training.\r\n\r\n        To learn more about pruning with PyTorch, please take a look at\r\n        `this tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. code-block:: python\r\n\r\n            parameters_to_prune = [(model.mlp_1, \"weight\"), (model.mlp_2, \"weight\")]\r\n\r\n            trainer = Trainer(\r\n                callbacks=[\r\n                    ModelPruning(\r\n                        pruning_fn=\"l1_unstructured\",\r\n                        parameters_to_prune=parameters_to_prune,\r\n                        amount=0.01,\r\n                        use_global_unstructured=True,\r\n                    )\r\n                ]\r\n            )\r\n\r\n        When ``parameters_to_prune`` is ``None``, ``parameters_to_prune`` will contain all parameters from the model.\r\n        The user can override ``filter_parameters_to_prune`` to filter any ``nn.Module`` to be pruned.\r\n\r\n        Args:\r\n\r\n            pruning_fn: Function from torch.nn.utils.prune module or your own PyTorch ``BasePruningMethod`` subclass.\r\n                Can also be string e.g. `\"l1_unstructured\"`. See pytorch docs for more details.\r\n\r\n            parameters_to_prune: List of tuples ``(nn.Module, \"parameter_name_string\")``.\r\n\r\n            parameter_names: List of parameter names to be pruned from the nn.Module.\r\n                Can either be ``\"weight\"`` or ``\"bias\"``.\r\n\r\n            use_global_unstructured: Whether to apply pruning globally on the model.\r\n                If ``parameters_to_prune`` is provided, global unstructured will be restricted on them.\r\n\r\n            amount: Quantity of parameters to prune:\r\n\r\n                - ``float``. Between 0.0 and 1.0. Represents the fraction of parameters to prune.\r\n                - ``int``. Represents the absolute number of parameters to prune.\r\n                - ``Callable``. For dynamic values. Will be called every epoch. Should return a value.\r\n\r\n            apply_pruning: Whether to apply pruning.\r\n\r\n                - ``bool``. Always apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            make_pruning_permanent: Whether to remove all reparameterization pre-hooks and apply masks\r\n                when training ends or the model is saved.\r\n\r\n            use_lottery_ticket_hypothesis: See `The lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`_:\r\n\r\n                - ``bool``. Whether to apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            resample_parameters: Used with ``use_lottery_ticket_hypothesis``. If True, the model parameters will\r\n                be resampled, otherwise, the exact original parameters will be used.\r\n\r\n            pruning_dim: If you are using a structured pruning method you need to specify the dimension.\r\n\r\n            pruning_norm: If you are using ``ln_structured`` you need to specify the norm.\r\n\r\n            verbose: Verbosity level. 0 to disable, 1 to log overall sparsity, 2 to log per-layer sparsity\r\n\r\n            prune_on_train_epoch_end: whether to apply pruning at the end of the training epoch.\r\n                If this is ``False``, then the check runs at the end of the validation epoch.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameter_names`` is neither ``\"weight\"`` nor ``\"bias\"``,\r\n                if the provided ``pruning_fn`` is not supported,\r\n                if ``pruning_dim`` is not provided when ``\"unstructured\"``,\r\n                if ``pruning_norm`` is not provided when ``\"ln_structured\"``,\r\n                if ``pruning_fn`` is neither ``str`` nor :class:`torch.nn.utils.prune.BasePruningMethod`, or\r\n                if ``amount`` is none of ``int``, ``float`` and ``Callable``.\r\n\r\n        \"\"\"\r\n\r\n        self._use_global_unstructured = use_global_unstructured\r\n        self._parameters_to_prune = parameters_to_prune\r\n        self._use_lottery_ticket_hypothesis = use_lottery_ticket_hypothesis\r\n        self._resample_parameters = resample_parameters\r\n        self._prune_on_train_epoch_end = prune_on_train_epoch_end\r\n        self._parameter_names = parameter_names or self.PARAMETER_NAMES\r\n        self._global_kwargs: dict[str, Any] = {}\r\n        self._original_layers: Optional[dict[int, _LayerRef]] = None\r\n        self._pruning_method_name: Optional[str] = None\r\n\r\n        for name in self._parameter_names:\r\n            if name not in self.PARAMETER_NAMES:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `parameter_names` name: {name} isn't in {self.PARAMETER_NAMES}\"\r\n                )\r\n\r\n        if isinstance(pruning_fn, str):\r\n            pruning_kwargs = {}\r\n            pruning_fn = pruning_fn.lower()\r\n            if pruning_fn not in _PYTORCH_PRUNING_FUNCTIONS:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `pruning_fn` {pruning_fn} isn't available in PyTorch's\"\r\n                    f\" built-in functions: {list(_PYTORCH_PRUNING_FUNCTIONS.keys())} \"\r\n                )\r\n            if pruning_fn.endswith(\"_structured\"):\r\n                if pruning_dim is None:\r\n                    raise MisconfigurationException(\r\n                        \"When requesting `structured` pruning, the `pruning_dim` should be provided.\"\r\n                    )\r\n                if pruning_fn == \"ln_structured\":\r\n                    if pruning_norm is None:\r\n                        raise MisconfigurationException(\r\n                            \"When requesting `ln_structured` pruning, the `pruning_norm` should be provided.\"\r\n                        )\r\n                    pruning_kwargs[\"n\"] = pruning_norm\r\n                pruning_kwargs[\"dim\"] = pruning_dim\r\n            pruning_fn = self._create_pruning_fn(pruning_fn, **pruning_kwargs)\r\n        elif self._is_pruning_method(pruning_fn):\r\n            if not use_global_unstructured:\r\n                raise MisconfigurationException(\r\n                    \"PyTorch `BasePruningMethod` is currently only supported with `use_global_unstructured=True`.\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                f\"`pruning_fn` is expected to be a str in {list(_PYTORCH_PRUNING_FUNCTIONS.keys())}\"\r\n                f\" or a PyTorch `BasePruningMethod`. Found: {pruning_fn}.\"\r\n                \" HINT: if passing a `BasePruningMethod`, pass the class, not an instance\"\r\n            )\r\n\r\n        # need to ignore typing here since pytorch base class does not define the PRUNING_TYPE attribute\r\n        if use_global_unstructured and pruning_fn.PRUNING_TYPE != \"unstructured\":  # type: ignore\r\n            raise MisconfigurationException(\r\n                'Only the \"unstructured\" PRUNING_TYPE is supported with `use_global_unstructured=True`.'\r\n                f\" Found method {pruning_fn} of type {pruning_fn.PRUNING_TYPE}. \"  # type: ignore[union-attr]\r\n            )\r\n\r\n        self.pruning_fn = pruning_fn\r\n        self._apply_pruning = apply_pruning\r\n        self._make_pruning_permanent = make_pruning_permanent\r\n\r\n        if not (isinstance(amount, (int, float)) or callable(amount)):\r\n            raise MisconfigurationException(\r\n                \"`amount` should be provided and be either an int, a float or Callable function.\"\r\n            )\r\n\r\n        self.amount = amount\r\n\r\n        if verbose not in (0, 1, 2):\r\n            raise MisconfigurationException(\"`verbose` must be any of (0, 1, 2)\")\r\n\r\n        self._verbose = verbose",
    "docstring_summary": "Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 74,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: filter_parameters_to_prune\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function can be overridden to control which module to prune.\n\n--- Code ---\ndef filter_parameters_to_prune(self, parameters_to_prune: _PARAM_LIST = ()) -> _PARAM_LIST:\r\n        \"\"\"This function can be overridden to control which module to prune.\"\"\"\r\n        return parameters_to_prune\n\n--- Original String ---\ndef filter_parameters_to_prune(self, parameters_to_prune: _PARAM_LIST = ()) -> _PARAM_LIST:\r\n        \"\"\"This function can be overridden to control which module to prune.\"\"\"\r\n        return parameters_to_prune",
    "func_name": "filter_parameters_to_prune",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function can be overridden to control which module to prune.",
    "code": "def filter_parameters_to_prune(self, parameters_to_prune: _PARAM_LIST = ()) -> _PARAM_LIST:\r\n        \"\"\"This function can be overridden to control which module to prune.\"\"\"\r\n        return parameters_to_prune",
    "docstring_summary": "This function can be overridden to control which module to prune."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 75,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: _create_pruning_fn\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function takes `pruning_fn`, a function name.\n\n--- Code ---\ndef _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        # save the function __name__ now because partial does not include it\r\n        # and there are issues setting the attribute manually in ddp.\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)\n\n--- Original String ---\ndef _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        # save the function __name__ now because partial does not include it\r\n        # and there are issues setting the attribute manually in ddp.\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)",
    "func_name": "_create_pruning_fn",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function takes `pruning_fn`, a function name.",
    "code": "def _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        # save the function __name__ now because partial does not include it\r\n        # and there are issues setting the attribute manually in ddp.\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)",
    "docstring_summary": "This function takes `pruning_fn`, a function name."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 76,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: make_pruning_permanent\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRemoves pruning buffers from any pruned modules.\n\n--- Code ---\ndef make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]\n\n--- Original String ---\ndef make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]",
    "func_name": "make_pruning_permanent",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Removes pruning buffers from any pruned modules.",
    "code": "def make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]",
    "docstring_summary": "Removes pruning buffers from any pruned modules."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 77,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: apply_lottery_ticket_hypothesis\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\n\n--- Code ---\ndef apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)\n\n--- Original String ---\ndef apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)",
    "func_name": "apply_lottery_ticket_hypothesis",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):",
    "code": "def apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)",
    "docstring_summary": "r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 78,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: apply_pruning\nLanguage: python\nPartition: train\n\n--- Docstring ---\nApplies pruning to ``parameters_to_prune``.\n\n--- Code ---\ndef apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)\n\n--- Original String ---\ndef apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)",
    "func_name": "apply_pruning",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Applies pruning to ``parameters_to_prune``.",
    "code": "def apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)",
    "docstring_summary": "Applies pruning to ``parameters_to_prune``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "index": 79,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\pruning.py\nFunction Name: sanitize_parameters_to_prune\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\n\n--- Code ---\ndef sanitize_parameters_to_prune(\r\n        pl_module: LightningModule, parameters_to_prune: _PARAM_LIST = (), parameter_names: Sequence[str] = ()\r\n    ) -> _PARAM_LIST:\r\n        \"\"\"This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\r\n        ``parameters_to_prune is None``, it will be generated with all parameters of the model.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameters_to_prune`` doesn't exist in the model, or\r\n                if ``parameters_to_prune`` is neither a list nor a tuple.\r\n\r\n        \"\"\"\r\n        parameters = parameter_names or ModelPruning.PARAMETER_NAMES\r\n\r\n        current_modules = [m for m in pl_module.modules() if not isinstance(m, _MODULE_CONTAINERS)]\r\n\r\n        if not parameters_to_prune:\r\n            parameters_to_prune = [\r\n                (m, p)\r\n                for p in parameters\r\n                for m in current_modules\r\n                if getattr(m, p, None) is not None and isinstance(getattr(m, p, None), nn.Parameter)\r\n            ]\r\n        elif (\r\n            isinstance(parameters_to_prune, (list, tuple))\r\n            and len(parameters_to_prune) > 0\r\n            and all(len(p) == 2 for p in parameters_to_prune)\r\n            and all(isinstance(a, nn.Module) and isinstance(b, str) for a, b in parameters_to_prune)\r\n        ):\r\n            missing_modules, missing_parameters = [], []\r\n            for module, name in parameters_to_prune:\r\n                if module not in current_modules:\r\n                    missing_modules.append(module)\r\n                    continue\r\n                if not hasattr(module, name):\r\n                    missing_parameters.append(name)\r\n\r\n            if missing_modules or missing_parameters:\r\n                raise MisconfigurationException(\r\n                    \"Some provided `parameters_to_prune` don't exist in the model.\"\r\n                    f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                \"The provided `parameters_to_prune` should either be list of tuple\"\r\n                \" with 2 elements: (nn.Module, parameter_name_to_prune) or None\"\r\n            )\r\n\r\n        return parameters_to_prune\n\n--- Original String ---\ndef sanitize_parameters_to_prune(\r\n        pl_module: LightningModule, parameters_to_prune: _PARAM_LIST = (), parameter_names: Sequence[str] = ()\r\n    ) -> _PARAM_LIST:\r\n        \"\"\"This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\r\n        ``parameters_to_prune is None``, it will be generated with all parameters of the model.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameters_to_prune`` doesn't exist in the model, or\r\n                if ``parameters_to_prune`` is neither a list nor a tuple.\r\n\r\n        \"\"\"\r\n        parameters = parameter_names or ModelPruning.PARAMETER_NAMES\r\n\r\n        current_modules = [m for m in pl_module.modules() if not isinstance(m, _MODULE_CONTAINERS)]\r\n\r\n        if not parameters_to_prune:\r\n            parameters_to_prune = [\r\n                (m, p)\r\n                for p in parameters\r\n                for m in current_modules\r\n                if getattr(m, p, None) is not None and isinstance(getattr(m, p, None), nn.Parameter)\r\n            ]\r\n        elif (\r\n            isinstance(parameters_to_prune, (list, tuple))\r\n            and len(parameters_to_prune) > 0\r\n            and all(len(p) == 2 for p in parameters_to_prune)\r\n            and all(isinstance(a, nn.Module) and isinstance(b, str) for a, b in parameters_to_prune)\r\n        ):\r\n            missing_modules, missing_parameters = [], []\r\n            for module, name in parameters_to_prune:\r\n                if module not in current_modules:\r\n                    missing_modules.append(module)\r\n                    continue\r\n                if not hasattr(module, name):\r\n                    missing_parameters.append(name)\r\n\r\n            if missing_modules or missing_parameters:\r\n                raise MisconfigurationException(\r\n                    \"Some provided `parameters_to_prune` don't exist in the model.\"\r\n                    f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                \"The provided `parameters_to_prune` should either be list of tuple\"\r\n                \" with 2 elements: (nn.Module, parameter_name_to_prune) or None\"\r\n            )\r\n\r\n        return parameters_to_prune",
    "func_name": "sanitize_parameters_to_prune",
    "path": "src\\lightning\\pytorch\\callbacks\\pruning.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If",
    "code": "def sanitize_parameters_to_prune(\r\n        pl_module: LightningModule, parameters_to_prune: _PARAM_LIST = (), parameter_names: Sequence[str] = ()\r\n    ) -> _PARAM_LIST:\r\n        \"\"\"This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\r\n        ``parameters_to_prune is None``, it will be generated with all parameters of the model.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameters_to_prune`` doesn't exist in the model, or\r\n                if ``parameters_to_prune`` is neither a list nor a tuple.\r\n\r\n        \"\"\"\r\n        parameters = parameter_names or ModelPruning.PARAMETER_NAMES\r\n\r\n        current_modules = [m for m in pl_module.modules() if not isinstance(m, _MODULE_CONTAINERS)]\r\n\r\n        if not parameters_to_prune:\r\n            parameters_to_prune = [\r\n                (m, p)\r\n                for p in parameters\r\n                for m in current_modules\r\n                if getattr(m, p, None) is not None and isinstance(getattr(m, p, None), nn.Parameter)\r\n            ]\r\n        elif (\r\n            isinstance(parameters_to_prune, (list, tuple))\r\n            and len(parameters_to_prune) > 0\r\n            and all(len(p) == 2 for p in parameters_to_prune)\r\n            and all(isinstance(a, nn.Module) and isinstance(b, str) for a, b in parameters_to_prune)\r\n        ):\r\n            missing_modules, missing_parameters = [], []\r\n            for module, name in parameters_to_prune:\r\n                if module not in current_modules:\r\n                    missing_modules.append(module)\r\n                    continue\r\n                if not hasattr(module, name):\r\n                    missing_parameters.append(name)\r\n\r\n            if missing_modules or missing_parameters:\r\n                raise MisconfigurationException(\r\n                    \"Some provided `parameters_to_prune` don't exist in the model.\"\r\n                    f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                \"The provided `parameters_to_prune` should either be list of tuple\"\r\n                \" with 2 elements: (nn.Module, parameter_name_to_prune) or None\"\r\n            )\r\n\r\n        return parameters_to_prune",
    "docstring_summary": "This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "index": 80,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py\nFunction Name: __init__\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\n\n--- Code ---\ndef __init__(\r\n        self,\r\n        swa_lrs: Union[float, list[float]],\r\n        swa_epoch_start: Union[int, float] = 0.8,\r\n        annealing_epochs: int = 10,\r\n        annealing_strategy: Literal[\"cos\", \"linear\"] = \"cos\",\r\n        avg_fn: Optional[_AVG_FN] = None,\r\n        device: Optional[Union[torch.device, str]] = torch.device(\"cpu\"),\r\n    ):\r\n        r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\r\n\r\n        Stochastic Weight Averaging was proposed in ``Averaging Weights Leads to\r\n        Wider Optima and Better Generalization`` by Pavel Izmailov, Dmitrii\r\n        Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\r\n        (UAI 2018).\r\n\r\n        This documentation is highly inspired by PyTorch's work on SWA.\r\n        The callback arguments follow the scheme defined in PyTorch's ``swa_utils`` package.\r\n\r\n        For a SWA explanation, please take a look\r\n        `here <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently not supported for multiple optimizers/schedulers.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently only supported on every epoch.\r\n\r\n        See also how to :ref:`enable it directly on the Trainer <advanced/training_tricks:Weight Averaging>`.\r\n\r\n        Arguments:\r\n\r\n            swa_lrs: The SWA learning rate to use:\r\n\r\n                - ``float``. Use this value for all parameter groups of the optimizer.\r\n                - ``List[float]``. A list values for each parameter group of the optimizer.\r\n\r\n            swa_epoch_start: If provided as int, the procedure will start from\r\n                the ``swa_epoch_start``-th epoch. If provided as float between 0 and 1,\r\n                the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch\r\n\r\n            annealing_epochs: number of epochs in the annealing phase (default: 10)\r\n\r\n            annealing_strategy: Specifies the annealing strategy (default: \"cos\"):\r\n\r\n                - ``\"cos\"``. For cosine annealing.\r\n                - ``\"linear\"`` For linear annealing\r\n\r\n            avg_fn: the averaging function used to update the parameters;\r\n                the function must take in the current value of the\r\n                :class:`AveragedModel` parameter, the current value of :attr:`model`\r\n                parameter and the number of models already averaged; if None,\r\n                equally weighted average is used (default: ``None``)\r\n\r\n            device: if provided, the averaged model will be stored on the ``device``.\r\n                When None is provided, it will infer the `device` from ``pl_module``.\r\n                (default: ``\"cpu\"``)\r\n\r\n        \"\"\"\r\n\r\n        err_msg = \"swa_epoch_start should be a >0 integer or a float between 0 and 1.\"\r\n        if isinstance(swa_epoch_start, int) and swa_epoch_start < 1:\r\n            raise MisconfigurationException(err_msg)\r\n        if isinstance(swa_epoch_start, float) and not (0 <= swa_epoch_start <= 1):\r\n            raise MisconfigurationException(err_msg)\r\n\r\n        wrong_type = not isinstance(swa_lrs, (float, list))\r\n        wrong_float = isinstance(swa_lrs, float) and swa_lrs <= 0\r\n        wrong_list = isinstance(swa_lrs, list) and not all(lr > 0 and isinstance(lr, float) for lr in swa_lrs)\r\n        if wrong_type or wrong_float or wrong_list:\r\n            raise MisconfigurationException(\"The `swa_lrs` should a positive float, or a list of positive floats\")\r\n\r\n        if avg_fn is not None and not callable(avg_fn):\r\n            raise MisconfigurationException(\"The `avg_fn` should be callable.\")\r\n\r\n        if device is not None and not isinstance(device, (torch.device, str)):\r\n            raise MisconfigurationException(f\"device is expected to be a torch.device or a str. Found {device}\")\r\n\r\n        self.n_averaged: Optional[Tensor] = None\r\n        self._swa_epoch_start = swa_epoch_start\r\n        self._swa_lrs = swa_lrs\r\n        self._annealing_epochs = annealing_epochs\r\n        self._annealing_strategy = annealing_strategy\r\n        self._avg_fn = avg_fn or self.avg_fn\r\n        self._device = device\r\n        self._model_contains_batch_norm: Optional[bool] = None\r\n        self._average_model: Optional[pl.LightningModule] = None\r\n        self._initialized = False\r\n        self._swa_scheduler: Optional[LRScheduler] = None\r\n        self._scheduler_state: Optional[dict] = None\r\n        self._init_n_averaged = 0\r\n        self._latest_update_epoch = -1\r\n        self.momenta: dict[nn.modules.batchnorm._BatchNorm, Optional[float]] = {}\r\n        self._max_epochs: int\n\n--- Original String ---\ndef __init__(\r\n        self,\r\n        swa_lrs: Union[float, list[float]],\r\n        swa_epoch_start: Union[int, float] = 0.8,\r\n        annealing_epochs: int = 10,\r\n        annealing_strategy: Literal[\"cos\", \"linear\"] = \"cos\",\r\n        avg_fn: Optional[_AVG_FN] = None,\r\n        device: Optional[Union[torch.device, str]] = torch.device(\"cpu\"),\r\n    ):\r\n        r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\r\n\r\n        Stochastic Weight Averaging was proposed in ``Averaging Weights Leads to\r\n        Wider Optima and Better Generalization`` by Pavel Izmailov, Dmitrii\r\n        Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\r\n        (UAI 2018).\r\n\r\n        This documentation is highly inspired by PyTorch's work on SWA.\r\n        The callback arguments follow the scheme defined in PyTorch's ``swa_utils`` package.\r\n\r\n        For a SWA explanation, please take a look\r\n        `here <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently not supported for multiple optimizers/schedulers.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently only supported on every epoch.\r\n\r\n        See also how to :ref:`enable it directly on the Trainer <advanced/training_tricks:Weight Averaging>`.\r\n\r\n        Arguments:\r\n\r\n            swa_lrs: The SWA learning rate to use:\r\n\r\n                - ``float``. Use this value for all parameter groups of the optimizer.\r\n                - ``List[float]``. A list values for each parameter group of the optimizer.\r\n\r\n            swa_epoch_start: If provided as int, the procedure will start from\r\n                the ``swa_epoch_start``-th epoch. If provided as float between 0 and 1,\r\n                the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch\r\n\r\n            annealing_epochs: number of epochs in the annealing phase (default: 10)\r\n\r\n            annealing_strategy: Specifies the annealing strategy (default: \"cos\"):\r\n\r\n                - ``\"cos\"``. For cosine annealing.\r\n                - ``\"linear\"`` For linear annealing\r\n\r\n            avg_fn: the averaging function used to update the parameters;\r\n                the function must take in the current value of the\r\n                :class:`AveragedModel` parameter, the current value of :attr:`model`\r\n                parameter and the number of models already averaged; if None,\r\n                equally weighted average is used (default: ``None``)\r\n\r\n            device: if provided, the averaged model will be stored on the ``device``.\r\n                When None is provided, it will infer the `device` from ``pl_module``.\r\n                (default: ``\"cpu\"``)\r\n\r\n        \"\"\"\r\n\r\n        err_msg = \"swa_epoch_start should be a >0 integer or a float between 0 and 1.\"\r\n        if isinstance(swa_epoch_start, int) and swa_epoch_start < 1:\r\n            raise MisconfigurationException(err_msg)\r\n        if isinstance(swa_epoch_start, float) and not (0 <= swa_epoch_start <= 1):\r\n            raise MisconfigurationException(err_msg)\r\n\r\n        wrong_type = not isinstance(swa_lrs, (float, list))\r\n        wrong_float = isinstance(swa_lrs, float) and swa_lrs <= 0\r\n        wrong_list = isinstance(swa_lrs, list) and not all(lr > 0 and isinstance(lr, float) for lr in swa_lrs)\r\n        if wrong_type or wrong_float or wrong_list:\r\n            raise MisconfigurationException(\"The `swa_lrs` should a positive float, or a list of positive floats\")\r\n\r\n        if avg_fn is not None and not callable(avg_fn):\r\n            raise MisconfigurationException(\"The `avg_fn` should be callable.\")\r\n\r\n        if device is not None and not isinstance(device, (torch.device, str)):\r\n            raise MisconfigurationException(f\"device is expected to be a torch.device or a str. Found {device}\")\r\n\r\n        self.n_averaged: Optional[Tensor] = None\r\n        self._swa_epoch_start = swa_epoch_start\r\n        self._swa_lrs = swa_lrs\r\n        self._annealing_epochs = annealing_epochs\r\n        self._annealing_strategy = annealing_strategy\r\n        self._avg_fn = avg_fn or self.avg_fn\r\n        self._device = device\r\n        self._model_contains_batch_norm: Optional[bool] = None\r\n        self._average_model: Optional[pl.LightningModule] = None\r\n        self._initialized = False\r\n        self._swa_scheduler: Optional[LRScheduler] = None\r\n        self._scheduler_state: Optional[dict] = None\r\n        self._init_n_averaged = 0\r\n        self._latest_update_epoch = -1\r\n        self.momenta: dict[nn.modules.batchnorm._BatchNorm, Optional[float]] = {}\r\n        self._max_epochs: int",
    "func_name": "__init__",
    "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.",
    "code": "def __init__(\r\n        self,\r\n        swa_lrs: Union[float, list[float]],\r\n        swa_epoch_start: Union[int, float] = 0.8,\r\n        annealing_epochs: int = 10,\r\n        annealing_strategy: Literal[\"cos\", \"linear\"] = \"cos\",\r\n        avg_fn: Optional[_AVG_FN] = None,\r\n        device: Optional[Union[torch.device, str]] = torch.device(\"cpu\"),\r\n    ):\r\n        r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\r\n\r\n        Stochastic Weight Averaging was proposed in ``Averaging Weights Leads to\r\n        Wider Optima and Better Generalization`` by Pavel Izmailov, Dmitrii\r\n        Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\r\n        (UAI 2018).\r\n\r\n        This documentation is highly inspired by PyTorch's work on SWA.\r\n        The callback arguments follow the scheme defined in PyTorch's ``swa_utils`` package.\r\n\r\n        For a SWA explanation, please take a look\r\n        `here <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently not supported for multiple optimizers/schedulers.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently only supported on every epoch.\r\n\r\n        See also how to :ref:`enable it directly on the Trainer <advanced/training_tricks:Weight Averaging>`.\r\n\r\n        Arguments:\r\n\r\n            swa_lrs: The SWA learning rate to use:\r\n\r\n                - ``float``. Use this value for all parameter groups of the optimizer.\r\n                - ``List[float]``. A list values for each parameter group of the optimizer.\r\n\r\n            swa_epoch_start: If provided as int, the procedure will start from\r\n                the ``swa_epoch_start``-th epoch. If provided as float between 0 and 1,\r\n                the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch\r\n\r\n            annealing_epochs: number of epochs in the annealing phase (default: 10)\r\n\r\n            annealing_strategy: Specifies the annealing strategy (default: \"cos\"):\r\n\r\n                - ``\"cos\"``. For cosine annealing.\r\n                - ``\"linear\"`` For linear annealing\r\n\r\n            avg_fn: the averaging function used to update the parameters;\r\n                the function must take in the current value of the\r\n                :class:`AveragedModel` parameter, the current value of :attr:`model`\r\n                parameter and the number of models already averaged; if None,\r\n                equally weighted average is used (default: ``None``)\r\n\r\n            device: if provided, the averaged model will be stored on the ``device``.\r\n                When None is provided, it will infer the `device` from ``pl_module``.\r\n                (default: ``\"cpu\"``)\r\n\r\n        \"\"\"\r\n\r\n        err_msg = \"swa_epoch_start should be a >0 integer or a float between 0 and 1.\"\r\n        if isinstance(swa_epoch_start, int) and swa_epoch_start < 1:\r\n            raise MisconfigurationException(err_msg)\r\n        if isinstance(swa_epoch_start, float) and not (0 <= swa_epoch_start <= 1):\r\n            raise MisconfigurationException(err_msg)\r\n\r\n        wrong_type = not isinstance(swa_lrs, (float, list))\r\n        wrong_float = isinstance(swa_lrs, float) and swa_lrs <= 0\r\n        wrong_list = isinstance(swa_lrs, list) and not all(lr > 0 and isinstance(lr, float) for lr in swa_lrs)\r\n        if wrong_type or wrong_float or wrong_list:\r\n            raise MisconfigurationException(\"The `swa_lrs` should a positive float, or a list of positive floats\")\r\n\r\n        if avg_fn is not None and not callable(avg_fn):\r\n            raise MisconfigurationException(\"The `avg_fn` should be callable.\")\r\n\r\n        if device is not None and not isinstance(device, (torch.device, str)):\r\n            raise MisconfigurationException(f\"device is expected to be a torch.device or a str. Found {device}\")\r\n\r\n        self.n_averaged: Optional[Tensor] = None\r\n        self._swa_epoch_start = swa_epoch_start\r\n        self._swa_lrs = swa_lrs\r\n        self._annealing_epochs = annealing_epochs\r\n        self._annealing_strategy = annealing_strategy\r\n        self._avg_fn = avg_fn or self.avg_fn\r\n        self._device = device\r\n        self._model_contains_batch_norm: Optional[bool] = None\r\n        self._average_model: Optional[pl.LightningModule] = None\r\n        self._initialized = False\r\n        self._swa_scheduler: Optional[LRScheduler] = None\r\n        self._scheduler_state: Optional[dict] = None\r\n        self._init_n_averaged = 0\r\n        self._latest_update_epoch = -1\r\n        self.momenta: dict[nn.modules.batchnorm._BatchNorm, Optional[float]] = {}\r\n        self._max_epochs: int",
    "docstring_summary": "r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "index": 81,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py\nFunction Name: reset_batch_norm_and_save_state\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAdapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\n\n--- Code ---\ndef reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0\n\n--- Original String ---\ndef reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0",
    "func_name": "reset_batch_norm_and_save_state",
    "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.",
    "code": "def reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0",
    "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "index": 82,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py\nFunction Name: reset_momenta\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAdapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\n\n--- Code ---\ndef reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]\n\n--- Original String ---\ndef reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]",
    "func_name": "reset_momenta",
    "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.",
    "code": "def reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]",
    "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "index": 83,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py\nFunction Name: update_parameters\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAdapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\n\n--- Code ---\ndef update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1\n\n--- Original String ---\ndef update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1",
    "func_name": "update_parameters",
    "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.",
    "code": "def update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1",
    "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "index": 84,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py\nFunction Name: avg_fn\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAdapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\n\n--- Code ---\ndef avg_fn(averaged_model_parameter: Tensor, model_parameter: Tensor, num_averaged: Tensor) -> Tensor:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\"\"\"\r\n        return averaged_model_parameter + (model_parameter - averaged_model_parameter) / (num_averaged + 1)\n\n--- Original String ---\ndef avg_fn(averaged_model_parameter: Tensor, model_parameter: Tensor, num_averaged: Tensor) -> Tensor:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\"\"\"\r\n        return averaged_model_parameter + (model_parameter - averaged_model_parameter) / (num_averaged + 1)",
    "func_name": "avg_fn",
    "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.",
    "code": "def avg_fn(averaged_model_parameter: Tensor, model_parameter: Tensor, num_averaged: Tensor) -> Tensor:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\"\"\"\r\n        return averaged_model_parameter + (model_parameter - averaged_model_parameter) / (num_averaged + 1)",
    "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "index": 85,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\timer.py\nFunction Name: start_time\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturn the start time of a particular stage (in seconds)\n\n--- Code ---\ndef start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]\n\n--- Original String ---\ndef start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]",
    "func_name": "start_time",
    "path": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Return the start time of a particular stage (in seconds)",
    "code": "def start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]",
    "docstring_summary": "Return the start time of a particular stage (in seconds)"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "index": 86,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\timer.py\nFunction Name: end_time\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturn the end time of a particular stage (in seconds)\n\n--- Code ---\ndef end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]\n\n--- Original String ---\ndef end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]",
    "func_name": "end_time",
    "path": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Return the end time of a particular stage (in seconds)",
    "code": "def end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]",
    "docstring_summary": "Return the end time of a particular stage (in seconds)"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "index": 87,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\timer.py\nFunction Name: time_elapsed\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturn the time elapsed for a particular stage (in seconds)\n\n--- Code ---\ndef time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset\n\n--- Original String ---\ndef time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset",
    "func_name": "time_elapsed",
    "path": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Return the time elapsed for a particular stage (in seconds)",
    "code": "def time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset",
    "docstring_summary": "Return the time elapsed for a particular stage (in seconds)"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "index": 88,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\timer.py\nFunction Name: time_remaining\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturn the time remaining for a particular stage (in seconds)\n\n--- Code ---\ndef time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None\n\n--- Original String ---\ndef time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None",
    "func_name": "time_remaining",
    "path": "src\\lightning\\pytorch\\callbacks\\timer.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Return the time remaining for a particular stage (in seconds)",
    "code": "def time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None",
    "docstring_summary": "Return the time remaining for a particular stage (in seconds)"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 89,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: should_update\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled after every optimizer step and after every training epoch to check whether the average model should\n\n--- Code ---\ndef should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None\n\n--- Original String ---\ndef should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None",
    "func_name": "should_update",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called after every optimizer step and after every training epoch to check whether the average model should",
    "code": "def should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None",
    "docstring_summary": "Called after every optimizer step and after every training epoch to check whether the average model should"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 90,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: setup\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when fit, validate, test, predict, or tune begins.\n\n--- Code ---\ndef setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            # If the configure_model hook is overridden, call it to create the layers before constructing the\r\n            # AveragedModel. However, sharding will not be done and a warning will be issued.\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )\n\n--- Original String ---\ndef setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            # If the configure_model hook is overridden, call it to create the layers before constructing the\r\n            # AveragedModel. However, sharding will not be done and a warning will be issued.\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )",
    "func_name": "setup",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when fit, validate, test, predict, or tune begins.",
    "code": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            # If the configure_model hook is overridden, call it to create the layers before constructing the\r\n            # AveragedModel. However, sharding will not be done and a warning will be issued.\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )",
    "docstring_summary": "Called when fit, validate, test, predict, or tune begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 91,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_train_batch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when a training batch ends.\n\n--- Code ---\ndef on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        # trainer.global_step is the number of optimizer steps taken so far, i.e. 1 after the first optimizer step. To\r\n        # make step_idx consistent with epoch_idx, we'll pass a zero-based index.\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step\n\n--- Original String ---\ndef on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        # trainer.global_step is the number of optimizer steps taken so far, i.e. 1 after the first optimizer step. To\r\n        # make step_idx consistent with epoch_idx, we'll pass a zero-based index.\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step",
    "func_name": "on_train_batch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when a training batch ends.",
    "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        # trainer.global_step is the number of optimizer steps taken so far, i.e. 1 after the first optimizer step. To\r\n        # make step_idx consistent with epoch_idx, we'll pass a zero-based index.\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step",
    "docstring_summary": "Called when a training batch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 92,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_train_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when a training epoch ends.\n\n--- Code ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch\n\n--- Original String ---\ndef on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch",
    "func_name": "on_train_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when a training epoch ends.",
    "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch",
    "docstring_summary": "Called when a training epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 93,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_train_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when training ends.\n\n--- Code ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)\n\n--- Original String ---\ndef on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)",
    "func_name": "on_train_end",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when training ends.",
    "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)",
    "docstring_summary": "Called when training ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 94,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_validation_epoch_start\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when a validation epoch begins.\n\n--- Code ---\ndef on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)\n\n--- Original String ---\ndef on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)",
    "func_name": "on_validation_epoch_start",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when a validation epoch begins.",
    "code": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)",
    "docstring_summary": "Called when a validation epoch begins."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 95,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_validation_epoch_end\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when a validation epoch ends.\n\n--- Code ---\ndef on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)\n\n--- Original String ---\ndef on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)",
    "func_name": "on_validation_epoch_end",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when a validation epoch ends.",
    "code": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)",
    "docstring_summary": "Called when a validation epoch ends."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 96,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: state_dict\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when saving a checkpoint.\n\n--- Code ---\ndef state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}\n\n--- Original String ---\ndef state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}",
    "func_name": "state_dict",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when saving a checkpoint.",
    "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}",
    "docstring_summary": "Called when saving a checkpoint."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 97,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: load_state_dict\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled when loading a checkpoint.\n\n--- Code ---\ndef load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]\n\n--- Original String ---\ndef load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]",
    "func_name": "load_state_dict",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Called when loading a checkpoint.",
    "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]",
    "docstring_summary": "Called when loading a checkpoint."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 98,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_save_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Called when saving a checkpoint.\n\n--- Code ---\ndef on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            # Truncate the \"module.\" prefix (the first 7 characters) from the names of the variables in the\r\n            # AveragedModel state.\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }\n\n--- Original String ---\ndef on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            # Truncate the \"module.\" prefix (the first 7 characters) from the names of the variables in the\r\n            # AveragedModel state.\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }",
    "func_name": "on_save_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Called when saving a checkpoint.",
    "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            # Truncate the \"module.\" prefix (the first 7 characters) from the names of the variables in the\r\n            # AveragedModel state.\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }",
    "docstring_summary": "r\"\"\"Called when saving a checkpoint."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "index": 99,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\callbacks\\weight_averaging.py\nFunction Name: on_load_checkpoint\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Called when loading a model checkpoint.\n\n--- Code ---\ndef on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            # The current model state has already been loaded from \"state_dict\" (which contains the average model\r\n            # weights) at this point, so overwriting \"state_dict\" in the checkpoint dictionary makes no difference. We\r\n            # have to reload the model state from \"current_model_state\".\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)\n\n--- Original String ---\ndef on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            # The current model state has already been loaded from \"state_dict\" (which contains the average model\r\n            # weights) at this point, so overwriting \"state_dict\" in the checkpoint dictionary makes no difference. We\r\n            # have to reload the model state from \"current_model_state\".\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)",
    "func_name": "on_load_checkpoint",
    "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Called when loading a model checkpoint.",
    "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            # The current model state has already been loaded from \"state_dict\" (which contains the average model\r\n            # weights) at this point, so overwriting \"state_dict\" in the checkpoint dictionary makes no difference. We\r\n            # have to reload the model state from \"current_model_state\".\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)",
    "docstring_summary": "r\"\"\"Called when loading a model checkpoint."
  }
]