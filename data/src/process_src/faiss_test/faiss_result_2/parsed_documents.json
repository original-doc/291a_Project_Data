[
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 0,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _get_supported_strategies\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\n\n--- Code ---\ndef _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]\n\n--- Original String ---\ndef _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]",
    "func_name": "_get_supported_strategies",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the",
    "code": "def _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]",
    "docstring_summary": "Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 1,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _run\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRun a Lightning Fabric script.\n\n--- Code ---\ndef _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)\n\n--- Original String ---\ndef _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)",
    "func_name": "_run",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Run a Lightning Fabric script.",
    "code": "def _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)",
    "docstring_summary": "Run a Lightning Fabric script."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 2,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _consolidate\nLanguage: python\nPartition: train\n\n--- Docstring ---\nConvert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\n\n--- Code ---\ndef _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)\n\n--- Original String ---\ndef _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)",
    "func_name": "_consolidate",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.",
    "code": "def _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)",
    "docstring_summary": "Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 3,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _set_env_variables\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSet the environment variables for the new processes.\n\n--- Code ---\ndef _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)\n\n--- Original String ---\ndef _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)",
    "func_name": "_set_env_variables",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Set the environment variables for the new processes.",
    "code": "def _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)",
    "docstring_summary": "Set the environment variables for the new processes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 4,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _get_num_processes\nLanguage: python\nPartition: train\n\n--- Docstring ---\nParse the `devices` argument to determine how many processes need to be launched on the current machine.\n\n--- Code ---\ndef _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0\n\n--- Original String ---\ndef _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0",
    "func_name": "_get_num_processes",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Parse the `devices` argument to determine how many processes need to be launched on the current machine.",
    "code": "def _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0",
    "docstring_summary": "Parse the `devices` argument to determine how many processes need to be launched on the current machine."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\cli.py",
    "index": 5,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\cli.py\nFunction Name: _torchrun_launch\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis will invoke `torchrun` programmatically to launch the given script in new processes.\n\n--- Code ---\ndef _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    # set a good default number of threads for OMP to avoid warnings being emitted to the user\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)\n\n--- Original String ---\ndef _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    # set a good default number of threads for OMP to avoid warnings being emitted to the user\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)",
    "func_name": "_torchrun_launch",
    "path": "src\\lightning\\fabric\\cli.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This will invoke `torchrun` programmatically to launch the given script in new processes.",
    "code": "def _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    # set a good default number of threads for OMP to avoid warnings being emitted to the user\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)",
    "docstring_summary": "This will invoke `torchrun` programmatically to launch the given script in new processes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\connector.py",
    "index": 6,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\connector.py\nFunction Name: _check_config_and_set_final_flags\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThis method checks:\n\n--- Code ---\ndef _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.fabric.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, dp, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._registered_accelerators\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._registered_accelerators)}.\"\r\n            )\r\n\r\n        # MPS accelerator is incompatible with DDP family of strategies. It supports single-device operation only.\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_dp_str = isinstance(strategy, str) and \"dp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_dp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_input = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_instance = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                else:\r\n                    raise TypeError(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise ValueError(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_input is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_input}` and `plugins={self._precision_instance}`. Choose one.\"\r\n                )\r\n\r\n        self._precision_input = \"32-true\" if precision_input is None else precision_input\r\n\r\n        # handle the case when the user passes in a strategy instance which has an accelerator, precision,\r\n        # checkpoint io or cluster env set up\r\n        # TODO: improve the error messages below\r\n        if isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise ValueError(\"accelerator set through both strategy class and accelerator flag, choose one\")\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision:\r\n                # [RFC] handle precision plugin set up conflict?\r\n                if self._precision_instance:\r\n                    raise ValueError(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_instance = self._strategy_flag._precision\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise ValueError(\"checkpoint_io set through both strategy class and plugins, choose one\")\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise ValueError(\"cluster_environment set through both strategy class and plugins, choose one\")\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise ValueError(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise ValueError(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices\n\n--- Original String ---\ndef _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.fabric.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, dp, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._registered_accelerators\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._registered_accelerators)}.\"\r\n            )\r\n\r\n        # MPS accelerator is incompatible with DDP family of strategies. It supports single-device operation only.\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_dp_str = isinstance(strategy, str) and \"dp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_dp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_input = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_instance = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                else:\r\n                    raise TypeError(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise ValueError(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_input is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_input}` and `plugins={self._precision_instance}`. Choose one.\"\r\n                )\r\n\r\n        self._precision_input = \"32-true\" if precision_input is None else precision_input\r\n\r\n        # handle the case when the user passes in a strategy instance which has an accelerator, precision,\r\n        # checkpoint io or cluster env set up\r\n        # TODO: improve the error messages below\r\n        if isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise ValueError(\"accelerator set through both strategy class and accelerator flag, choose one\")\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision:\r\n                # [RFC] handle precision plugin set up conflict?\r\n                if self._precision_instance:\r\n                    raise ValueError(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_instance = self._strategy_flag._precision\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise ValueError(\"checkpoint_io set through both strategy class and plugins, choose one\")\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise ValueError(\"cluster_environment set through both strategy class and plugins, choose one\")\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise ValueError(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise ValueError(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices",
    "func_name": "_check_config_and_set_final_flags",
    "path": "src\\lightning\\fabric\\connector.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "This method checks:",
    "code": "def _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.fabric.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, dp, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._registered_accelerators\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._registered_accelerators)}.\"\r\n            )\r\n\r\n        # MPS accelerator is incompatible with DDP family of strategies. It supports single-device operation only.\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_dp_str = isinstance(strategy, str) and \"dp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_dp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_input = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_instance = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                else:\r\n                    raise TypeError(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise ValueError(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_input is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_input}` and `plugins={self._precision_instance}`. Choose one.\"\r\n                )\r\n\r\n        self._precision_input = \"32-true\" if precision_input is None else precision_input\r\n\r\n        # handle the case when the user passes in a strategy instance which has an accelerator, precision,\r\n        # checkpoint io or cluster env set up\r\n        # TODO: improve the error messages below\r\n        if isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise ValueError(\"accelerator set through both strategy class and accelerator flag, choose one\")\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision:\r\n                # [RFC] handle precision plugin set up conflict?\r\n                if self._precision_instance:\r\n                    raise ValueError(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_instance = self._strategy_flag._precision\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise ValueError(\"checkpoint_io set through both strategy class and plugins, choose one\")\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise ValueError(\"cluster_environment set through both strategy class and plugins, choose one\")\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise ValueError(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise ValueError(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices",
    "docstring_summary": "This method checks:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\connector.py",
    "index": 7,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\connector.py\nFunction Name: _choose_auto_accelerator\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChoose the accelerator type (str) based on availability when ``accelerator='auto'``.\n\n--- Code ---\ndef _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"\n\n--- Original String ---\ndef _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"",
    "func_name": "_choose_auto_accelerator",
    "path": "src\\lightning\\fabric\\connector.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Choose the accelerator type (str) based on availability when ``accelerator='auto'``.",
    "code": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"",
    "docstring_summary": "Choose the accelerator type (str) based on availability when ``accelerator='auto'``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\connector.py",
    "index": 8,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\connector.py\nFunction Name: _check_strategy_and_fallback\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChecks edge cases when the strategy selection was a string input, and we need to fall back to a different\n\n--- Code ---\ndef _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        # Change fsdp to xla_fsdp if using TPU\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag\n\n--- Original String ---\ndef _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        # Change fsdp to xla_fsdp if using TPU\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag",
    "func_name": "_check_strategy_and_fallback",
    "path": "src\\lightning\\fabric\\connector.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different",
    "code": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        # Change fsdp to xla_fsdp if using TPU\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag",
    "docstring_summary": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\connector.py",
    "index": 9,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\connector.py\nFunction Name: _init_strategy\nLanguage: python\nPartition: train\n\n--- Docstring ---\nInstantiate the Strategy given depending on the setting of ``_strategy_flag``.\n\n--- Code ---\ndef _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag\n\n--- Original String ---\ndef _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag",
    "func_name": "_init_strategy",
    "path": "src\\lightning\\fabric\\connector.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.",
    "code": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag",
    "docstring_summary": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\connector.py",
    "index": 10,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\connector.py\nFunction Name: _lazy_init_strategy\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLazily set missing attributes on the previously instantiated strategy.\n\n--- Code ---\ndef _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        # TODO: should be moved to _check_strategy_and_fallback().\r\n        # Current test check precision first, so keep this check here to meet error order\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )\n\n--- Original String ---\ndef _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        # TODO: should be moved to _check_strategy_and_fallback().\r\n        # Current test check precision first, so keep this check here to meet error order\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )",
    "func_name": "_lazy_init_strategy",
    "path": "src\\lightning\\fabric\\connector.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Lazily set missing attributes on the previously instantiated strategy.",
    "code": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        # TODO: should be moved to _check_strategy_and_fallback().\r\n        # Current test check precision first, so keep this check here to meet error order\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )",
    "docstring_summary": "Lazily set missing attributes on the previously instantiated strategy."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 11,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe current device this process runs on.\n\n--- Code ---\ndef device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device\n\n--- Original String ---\ndef device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device",
    "func_name": "device",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The current device this process runs on.",
    "code": "def device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device",
    "docstring_summary": "The current device this process runs on."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 12,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: global_rank\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe global index of the current process across all devices and nodes.\n\n--- Code ---\ndef global_rank(self) -> int:\r\n        \"\"\"The global index of the current process across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"global_rank\", 0)\n\n--- Original String ---\ndef global_rank(self) -> int:\r\n        \"\"\"The global index of the current process across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"global_rank\", 0)",
    "func_name": "global_rank",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The global index of the current process across all devices and nodes.",
    "code": "def global_rank(self) -> int:\r\n        \"\"\"The global index of the current process across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"global_rank\", 0)",
    "docstring_summary": "The global index of the current process across all devices and nodes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 13,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: local_rank\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe index of the current process among the processes running on the local node.\n\n--- Code ---\ndef local_rank(self) -> int:\r\n        \"\"\"The index of the current process among the processes running on the local node.\"\"\"\r\n        return getattr(self._strategy, \"local_rank\", 0)\n\n--- Original String ---\ndef local_rank(self) -> int:\r\n        \"\"\"The index of the current process among the processes running on the local node.\"\"\"\r\n        return getattr(self._strategy, \"local_rank\", 0)",
    "func_name": "local_rank",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The index of the current process among the processes running on the local node.",
    "code": "def local_rank(self) -> int:\r\n        \"\"\"The index of the current process among the processes running on the local node.\"\"\"\r\n        return getattr(self._strategy, \"local_rank\", 0)",
    "docstring_summary": "The index of the current process among the processes running on the local node."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 14,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: node_rank\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe index of the current node.\n\n--- Code ---\ndef node_rank(self) -> int:\r\n        \"\"\"The index of the current node.\"\"\"\r\n        return getattr(self._strategy, \"node_rank\", 0)\n\n--- Original String ---\ndef node_rank(self) -> int:\r\n        \"\"\"The index of the current node.\"\"\"\r\n        return getattr(self._strategy, \"node_rank\", 0)",
    "func_name": "node_rank",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The index of the current node.",
    "code": "def node_rank(self) -> int:\r\n        \"\"\"The index of the current node.\"\"\"\r\n        return getattr(self._strategy, \"node_rank\", 0)",
    "docstring_summary": "The index of the current node."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 15,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: world_size\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe total number of processes running across all devices and nodes.\n\n--- Code ---\ndef world_size(self) -> int:\r\n        \"\"\"The total number of processes running across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"world_size\", 1)\n\n--- Original String ---\ndef world_size(self) -> int:\r\n        \"\"\"The total number of processes running across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"world_size\", 1)",
    "func_name": "world_size",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The total number of processes running across all devices and nodes.",
    "code": "def world_size(self) -> int:\r\n        \"\"\"The total number of processes running across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"world_size\", 1)",
    "docstring_summary": "The total number of processes running across all devices and nodes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 16,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: is_global_zero\nLanguage: python\nPartition: train\n\n--- Docstring ---\nWhether this rank is rank zero.\n\n--- Code ---\ndef is_global_zero(self) -> bool:\r\n        \"\"\"Whether this rank is rank zero.\"\"\"\r\n        return self._strategy.is_global_zero\n\n--- Original String ---\ndef is_global_zero(self) -> bool:\r\n        \"\"\"Whether this rank is rank zero.\"\"\"\r\n        return self._strategy.is_global_zero",
    "func_name": "is_global_zero",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Whether this rank is rank zero.",
    "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this rank is rank zero.\"\"\"\r\n        return self._strategy.is_global_zero",
    "docstring_summary": "Whether this rank is rank zero."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 17,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: loggers\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns all loggers passed to Fabric.\n\n--- Code ---\ndef loggers(self) -> list[Logger]:\r\n        \"\"\"Returns all loggers passed to Fabric.\"\"\"\r\n        return self._loggers\n\n--- Original String ---\ndef loggers(self) -> list[Logger]:\r\n        \"\"\"Returns all loggers passed to Fabric.\"\"\"\r\n        return self._loggers",
    "func_name": "loggers",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns all loggers passed to Fabric.",
    "code": "def loggers(self) -> list[Logger]:\r\n        \"\"\"Returns all loggers passed to Fabric.\"\"\"\r\n        return self._loggers",
    "docstring_summary": "Returns all loggers passed to Fabric."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 18,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: logger\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns the first logger in the list passed to Fabric, which is considered the main logger.\n\n--- Code ---\ndef logger(self) -> Logger:\r\n        \"\"\"Returns the first logger in the list passed to Fabric, which is considered the main logger.\"\"\"\r\n        return self._loggers[0]\n\n--- Original String ---\ndef logger(self) -> Logger:\r\n        \"\"\"Returns the first logger in the list passed to Fabric, which is considered the main logger.\"\"\"\r\n        return self._loggers[0]",
    "func_name": "logger",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns the first logger in the list passed to Fabric, which is considered the main logger.",
    "code": "def logger(self) -> Logger:\r\n        \"\"\"Returns the first logger in the list passed to Fabric, which is considered the main logger.\"\"\"\r\n        return self._loggers[0]",
    "docstring_summary": "Returns the first logger in the list passed to Fabric, which is considered the main logger."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 19,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: run\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAll the code inside this run method gets accelerated by Fabric.\n\n--- Code ---\ndef run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"\n\n--- Original String ---\ndef run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"",
    "func_name": "run",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "All the code inside this run method gets accelerated by Fabric.",
    "code": "def run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"",
    "docstring_summary": "All the code inside this run method gets accelerated by Fabric."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 20,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: setup\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Set up a model and its optimizers for accelerated training.\n\n--- Code ---\ndef setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        # Let accelerator/plugin wrap and connect the models and optimizers\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            # join both types in a tuple for API convenience\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module\n\n--- Original String ---\ndef setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        # Let accelerator/plugin wrap and connect the models and optimizers\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            # join both types in a tuple for API convenience\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module",
    "func_name": "setup",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Set up a model and its optimizers for accelerated training.",
    "code": "def setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        # Let accelerator/plugin wrap and connect the models and optimizers\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            # join both types in a tuple for API convenience\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module",
    "docstring_summary": "r\"\"\"Set up a model and its optimizers for accelerated training."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 21,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: setup_module\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Set up a model for accelerated training or inference.\n\n--- Code ---\ndef setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        # Let strategy wrap and connect the module alone\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module\n\n--- Original String ---\ndef setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        # Let strategy wrap and connect the module alone\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module",
    "func_name": "setup_module",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Set up a model for accelerated training or inference.",
    "code": "def setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        # Let strategy wrap and connect the module alone\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module",
    "docstring_summary": "r\"\"\"Set up a model for accelerated training or inference."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 22,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: setup_optimizers\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Set up one or more optimizers for accelerated training.\n\n--- Code ---\ndef setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)\n\n--- Original String ---\ndef setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)",
    "func_name": "setup_optimizers",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Set up one or more optimizers for accelerated training.",
    "code": "def setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)",
    "docstring_summary": "r\"\"\"Set up one or more optimizers for accelerated training."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 23,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: setup_dataloaders\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\n\n--- Code ---\ndef setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders\n\n--- Original String ---\ndef setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders",
    "func_name": "setup_dataloaders",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each",
    "code": "def setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders",
    "docstring_summary": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 24,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: _setup_dataloader\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Set up a single dataloader for accelerated training.\n\n--- Code ---\ndef _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            # the dataloader needs to be re-instantiated because we want to update the sampler\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        # add worker_init_fn for correct seeding in worker processes\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader\n\n--- Original String ---\ndef _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            # the dataloader needs to be re-instantiated because we want to update the sampler\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        # add worker_init_fn for correct seeding in worker processes\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader",
    "func_name": "_setup_dataloader",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Set up a single dataloader for accelerated training.",
    "code": "def _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            # the dataloader needs to be re-instantiated because we want to update the sampler\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        # add worker_init_fn for correct seeding in worker processes\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader",
    "docstring_summary": "r\"\"\"Set up a single dataloader for accelerated training."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 25,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: backward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\n\n--- Code ---\ndef backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                # requires to attach the current `DeepSpeedEngine` for the `_FabricOptimizer.step` call.\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False\n\n--- Original String ---\ndef backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                # requires to attach the current `DeepSpeedEngine` for the `_FabricOptimizer.step` call.\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False",
    "func_name": "backward",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.",
    "code": "def backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                # requires to attach the current `DeepSpeedEngine` for the `_FabricOptimizer.step` call.\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False",
    "docstring_summary": "r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 26,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: clip_gradients\nLanguage: python\nPartition: train\n\n--- Docstring ---\nClip the gradients of the model to a given max value or max norm.\n\n--- Code ---\ndef clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")\n\n--- Original String ---\ndef clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")",
    "func_name": "clip_gradients",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Clip the gradients of the model to a given max value or max norm.",
    "code": "def clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")",
    "docstring_summary": "Clip the gradients of the model to a given max value or max norm."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 27,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: autocast\nLanguage: python\nPartition: train\n\n--- Docstring ---\nA context manager to automatically convert operations for the chosen precision.\n\n--- Code ---\ndef autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()\n\n--- Original String ---\ndef autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()",
    "func_name": "autocast",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "A context manager to automatically convert operations for the chosen precision.",
    "code": "def autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()",
    "docstring_summary": "A context manager to automatically convert operations for the chosen precision."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 28,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: to_device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\n\n--- Code ---\ndef to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)\n\n--- Original String ---\ndef to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)",
    "func_name": "to_device",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on",
    "code": "def to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)",
    "docstring_summary": "r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 29,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: print\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\n\n--- Code ---\ndef print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)\n\n--- Original String ---\ndef print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)",
    "func_name": "print",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first",
    "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)",
    "docstring_summary": "r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 30,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: barrier\nLanguage: python\nPartition: train\n\n--- Docstring ---\nWait for all processes to enter this call.\n\n--- Code ---\ndef barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)\n\n--- Original String ---\ndef barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)",
    "func_name": "barrier",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Wait for all processes to enter this call.",
    "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)",
    "docstring_summary": "Wait for all processes to enter this call."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 31,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: broadcast\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Send a tensor from one process to all others.\n\n--- Code ---\ndef broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)\n\n--- Original String ---\ndef broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)",
    "func_name": "broadcast",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Send a tensor from one process to all others.",
    "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)",
    "docstring_summary": "r\"\"\"Send a tensor from one process to all others."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 32,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: all_gather\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGather tensors or collections of tensors from multiple processes.\n\n--- Code ---\ndef all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)\n\n--- Original String ---\ndef all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)",
    "func_name": "all_gather",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gather tensors or collections of tensors from multiple processes.",
    "code": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)",
    "docstring_summary": "Gather tensors or collections of tensors from multiple processes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 33,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: all_reduce\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReduce tensors or collections of tensors from multiple processes.\n\n--- Code ---\ndef all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)\n\n--- Original String ---\ndef all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)",
    "func_name": "all_reduce",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Reduce tensors or collections of tensors from multiple processes.",
    "code": "def all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)",
    "docstring_summary": "Reduce tensors or collections of tensors from multiple processes."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 34,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: rank_zero_first\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\n\n--- Code ---\ndef rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()\n\n--- Original String ---\ndef rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()",
    "func_name": "rank_zero_first",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when",
    "code": "def rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()",
    "docstring_summary": "r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 35,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: no_backward_sync\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\n\n--- Code ---\ndef no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)\n\n--- Original String ---\ndef no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)",
    "func_name": "no_backward_sync",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.",
    "code": "def no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)",
    "docstring_summary": "r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 36,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: sharded_model\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\n\n--- Code ---\ndef sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()\n\n--- Original String ---\ndef sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()",
    "func_name": "sharded_model",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.",
    "code": "def sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()",
    "docstring_summary": "r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 37,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: init_tensor\nLanguage: python\nPartition: train\n\n--- Docstring ---\nTensors that you instantiate under this context manager will be created on the device right away and have\n\n--- Code ---\ndef init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()\n\n--- Original String ---\ndef init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()",
    "func_name": "init_tensor",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Tensors that you instantiate under this context manager will be created on the device right away and have",
    "code": "def init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()",
    "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 38,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: init_module\nLanguage: python\nPartition: train\n\n--- Docstring ---\nInstantiate the model and its parameters under this context manager to reduce peak memory usage.\n\n--- Code ---\ndef init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)\n\n--- Original String ---\ndef init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)",
    "func_name": "init_module",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Instantiate the model and its parameters under this context manager to reduce peak memory usage.",
    "code": "def init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)",
    "docstring_summary": "Instantiate the model and its parameters under this context manager to reduce peak memory usage."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 39,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: save\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Save checkpoint contents to a file.\n\n--- Code ---\ndef save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()\n\n--- Original String ---\ndef save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()",
    "func_name": "save",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Save checkpoint contents to a file.",
    "code": "def save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()",
    "docstring_summary": "r\"\"\"Save checkpoint contents to a file."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 40,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: load\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLoad a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\n\n--- Code ---\ndef load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\r\n            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder\n\n--- Original String ---\ndef load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\r\n            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder",
    "func_name": "load",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)",
    "code": "def load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\r\n            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder",
    "docstring_summary": "Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 41,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: load_raw\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLoad the state of a module or optimizer from a single state-dict file.\n\n--- Code ---\ndef load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)\n\n--- Original String ---\ndef load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)",
    "func_name": "load_raw",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Load the state of a module or optimizer from a single state-dict file.",
    "code": "def load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)",
    "docstring_summary": "Load the state of a module or optimizer from a single state-dict file."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 42,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: launch\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLaunch and initialize all the processes needed for distributed execution.\n\n--- Code ---\ndef launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)\n\n--- Original String ---\ndef launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)",
    "func_name": "launch",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Launch and initialize all the processes needed for distributed execution.",
    "code": "def launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)",
    "docstring_summary": "Launch and initialize all the processes needed for distributed execution."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 43,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: call\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Trigger the callback methods with the given name and arguments.\n\n--- Code ---\ndef call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r\n            # TODO(fabric): handle the following signatures\r\n            # method(self, fabric|trainer, x, y=1)\r\n            # method(self, fabric|trainer, *args, x, y=1)\r\n            # method(self, *args, y=1)\r\n            # method(self, *args, **kwargs)\r\n\n--- Original String ---\ndef call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r\n            # TODO(fabric): handle the following signatures\r\n            # method(self, fabric|trainer, x, y=1)\r\n            # method(self, fabric|trainer, *args, x, y=1)\r\n            # method(self, *args, y=1)\r\n            # method(self, *args, **kwargs)",
    "func_name": "call",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Trigger the callback methods with the given name and arguments.",
    "code": "def call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r\n            # TODO(fabric): handle the following signatures\r\n            # method(self, fabric|trainer, x, y=1)\r\n            # method(self, fabric|trainer, *args, x, y=1)\r\n            # method(self, *args, y=1)\r\n            # method(self, *args, **kwargs)",
    "docstring_summary": "r\"\"\"Trigger the callback methods with the given name and arguments."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 44,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: log\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLog a scalar to all loggers that were added to Fabric.\n\n--- Code ---\ndef log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)\n\n--- Original String ---\ndef log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)",
    "func_name": "log",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Log a scalar to all loggers that were added to Fabric.",
    "code": "def log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)",
    "docstring_summary": "Log a scalar to all loggers that were added to Fabric."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 45,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: log_dict\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLog multiple scalars at once to all loggers that were added to Fabric.\n\n--- Code ---\ndef log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)\n\n--- Original String ---\ndef log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)",
    "func_name": "log_dict",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Log multiple scalars at once to all loggers that were added to Fabric.",
    "code": "def log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)",
    "docstring_summary": "Log multiple scalars at once to all loggers that were added to Fabric."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\fabric.py",
    "index": 46,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\fabric.py\nFunction Name: seed_everything\nLanguage: python\nPartition: train\n\n--- Docstring ---\nr\"\"\"Helper function to seed everything without explicitly importing Lightning.\n\n--- Code ---\ndef seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            # Lightning sets `workers=False` by default to avoid breaking reproducibility, but since this is a new\r\n            # release, we can afford to do it.\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)\n\n--- Original String ---\ndef seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            # Lightning sets `workers=False` by default to avoid breaking reproducibility, but since this is a new\r\n            # release, we can afford to do it.\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)",
    "func_name": "seed_everything",
    "path": "src\\lightning\\fabric\\fabric.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "r\"\"\"Helper function to seed everything without explicitly importing Lightning.",
    "code": "def seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            # Lightning sets `workers=False` by default to avoid breaking reproducibility, but since this is a new\r\n            # release, we can afford to do it.\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)",
    "docstring_summary": "r\"\"\"Helper function to seed everything without explicitly importing Lightning."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 47,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: __init__\nLanguage: python\nPartition: train\n\n--- Docstring ---\nFabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\n\n--- Code ---\ndef __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        # imitate the class of the wrapped object to make isinstance checks work\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})\n\n--- Original String ---\ndef __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        # imitate the class of the wrapped object to make isinstance checks work\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})",
    "func_name": "__init__",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer",
    "code": "def __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        # imitate the class of the wrapped object to make isinstance checks work\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})",
    "docstring_summary": "FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 48,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: __init__\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\n\n--- Code ---\ndef __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True\n\n--- Original String ---\ndef __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True",
    "func_name": "__init__",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast",
    "code": "def __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True",
    "docstring_summary": "The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 49,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: forward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCasts all inputs to the right precision and handles autocast for operations in the module forward method.\n\n--- Code ---\ndef forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output\n\n--- Original String ---\ndef forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output",
    "func_name": "forward",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Casts all inputs to the right precision and handles autocast for operations in the module forward method.",
    "code": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output",
    "docstring_summary": "Casts all inputs to the right precision and handles autocast for operations in the module forward method."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 50,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: mark_forward_method\nLanguage: python\nPartition: train\n\n--- Docstring ---\nMark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\n\n--- Code ---\ndef mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)\n\n--- Original String ---\ndef mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)",
    "func_name": "mark_forward_method",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).",
    "code": "def mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)",
    "docstring_summary": "Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP)."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 51,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: _wrap_method_with_module_call_tracker\nLanguage: python\nPartition: train\n\n--- Docstring ---\nTracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\n\n--- Code ---\ndef _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method\n\n--- Original String ---\ndef _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method",
    "func_name": "_wrap_method_with_module_call_tracker",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by",
    "code": "def _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method",
    "docstring_summary": "Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 52,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: __init__\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\n\n--- Code ---\ndef __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0\n\n--- Original String ---\ndef __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0",
    "func_name": "__init__",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the",
    "code": "def __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0",
    "docstring_summary": "The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 53,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: _unwrap_compiled\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRemoves the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\n\n--- Code ---\ndef _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None\n\n--- Original String ---\ndef _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None",
    "func_name": "_unwrap_compiled",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.",
    "code": "def _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None",
    "docstring_summary": "Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 54,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: is_wrapped\nLanguage: python\nPartition: train\n\n--- Docstring ---\nChecks if an object was set up by Fabric.\n\n--- Code ---\ndef is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))\n\n--- Original String ---\ndef is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))",
    "func_name": "is_wrapped",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Checks if an object was set up by Fabric.",
    "code": "def is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))",
    "docstring_summary": "Checks if an object was set up by Fabric."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\wrappers.py",
    "index": 55,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\wrappers.py\nFunction Name: _capture_compile_kwargs\nLanguage: python\nPartition: train\n\n--- Docstring ---\nWraps the ``torch.compile`` function and captures the compile arguments.\n\n--- Code ---\ndef _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n    # Limitation: Currently, the global compile config does not get captured on a per-model basis.\r\n    # PyTorch will resolve this in the future: https://github.com/pytorch/pytorch/issues/116575\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            # either torch.compile is being applied as a decorator or we're compiling something else\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture\n\n--- Original String ---\ndef _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n    # Limitation: Currently, the global compile config does not get captured on a per-model basis.\r\n    # PyTorch will resolve this in the future: https://github.com/pytorch/pytorch/issues/116575\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            # either torch.compile is being applied as a decorator or we're compiling something else\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture",
    "func_name": "_capture_compile_kwargs",
    "path": "src\\lightning\\fabric\\wrappers.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Wraps the ``torch.compile`` function and captures the compile arguments.",
    "code": "def _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n    # Limitation: Currently, the global compile config does not get captured on a per-model basis.\r\n    # PyTorch will resolve this in the future: https://github.com/pytorch/pytorch/issues/116575\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            # either torch.compile is being applied as a decorator or we're compiling something else\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture",
    "docstring_summary": "Wraps the ``torch.compile`` function and captures the compile arguments."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 56,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: setup_device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCreate and prepare the device for the current process.\n\n--- Code ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"Create and prepare the device for the current process.\"\"\"\n\n--- Original String ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"Create and prepare the device for the current process.\"\"\"",
    "func_name": "setup_device",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Create and prepare the device for the current process.",
    "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"Create and prepare the device for the current process.\"\"\"",
    "docstring_summary": "Create and prepare the device for the current process."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 57,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: teardown\nLanguage: python\nPartition: train\n\n--- Docstring ---\nClean up any state created by the accelerator.\n\n--- Code ---\ndef teardown(self) -> None:\r\n        \"\"\"Clean up any state created by the accelerator.\"\"\"\n\n--- Original String ---\ndef teardown(self) -> None:\r\n        \"\"\"Clean up any state created by the accelerator.\"\"\"",
    "func_name": "teardown",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Clean up any state created by the accelerator.",
    "code": "def teardown(self) -> None:\r\n        \"\"\"Clean up any state created by the accelerator.\"\"\"",
    "docstring_summary": "Clean up any state created by the accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 58,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: parse_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAccelerator device parsing logic.\n\n--- Code ---\ndef parse_devices(devices: Any) -> Any:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\n\n--- Original String ---\ndef parse_devices(devices: Any) -> Any:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"",
    "func_name": "parse_devices",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Accelerator device parsing logic.",
    "code": "def parse_devices(devices: Any) -> Any:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"",
    "docstring_summary": "Accelerator device parsing logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 59,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: get_parallel_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets parallel devices for the Accelerator.\n\n--- Code ---\ndef get_parallel_devices(devices: Any) -> Any:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\n\n--- Original String ---\ndef get_parallel_devices(devices: Any) -> Any:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"",
    "func_name": "get_parallel_devices",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets parallel devices for the Accelerator.",
    "code": "def get_parallel_devices(devices: Any) -> Any:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"",
    "docstring_summary": "Gets parallel devices for the Accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 60,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: auto_device_count\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGet the device count when set to auto.\n\n--- Code ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the device count when set to auto.\"\"\"\n\n--- Original String ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the device count when set to auto.\"\"\"",
    "func_name": "auto_device_count",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Get the device count when set to auto.",
    "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the device count when set to auto.\"\"\"",
    "docstring_summary": "Get the device count when set to auto."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "index": 61,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\accelerator.py\nFunction Name: is_available\nLanguage: python\nPartition: train\n\n--- Docstring ---\nDetect if the hardware is available.\n\n--- Code ---\ndef is_available() -> bool:\r\n        \"\"\"Detect if the hardware is available.\"\"\"\n\n--- Original String ---\ndef is_available() -> bool:\r\n        \"\"\"Detect if the hardware is available.\"\"\"",
    "func_name": "is_available",
    "path": "src\\lightning\\fabric\\accelerators\\accelerator.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Detect if the hardware is available.",
    "code": "def is_available() -> bool:\r\n        \"\"\"Detect if the hardware is available.\"\"\"",
    "docstring_summary": "Detect if the hardware is available."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 62,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: setup_device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRaises:\n\n--- Code ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")\n\n--- Original String ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")",
    "func_name": "setup_device",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Raises:",
    "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")",
    "docstring_summary": "Raises:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 63,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: parse_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAccelerator device parsing logic.\n\n--- Code ---\ndef parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)\n\n--- Original String ---\ndef parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)",
    "func_name": "parse_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Accelerator device parsing logic.",
    "code": "def parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)",
    "docstring_summary": "Accelerator device parsing logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 64,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: get_parallel_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets parallel devices for the Accelerator.\n\n--- Code ---\ndef get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices\n\n--- Original String ---\ndef get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices",
    "func_name": "get_parallel_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets parallel devices for the Accelerator.",
    "code": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices",
    "docstring_summary": "Gets parallel devices for the Accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 65,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: auto_device_count\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGet the devices when set to auto.\n\n--- Code ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1\n\n--- Original String ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1",
    "func_name": "auto_device_count",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Get the devices when set to auto.",
    "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1",
    "docstring_summary": "Get the devices when set to auto."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 66,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: is_available\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCPU is always available for execution.\n\n--- Code ---\ndef is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True\n\n--- Original String ---\ndef is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True",
    "func_name": "is_available",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "CPU is always available for execution.",
    "code": "def is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True",
    "docstring_summary": "CPU is always available for execution."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "index": 67,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cpu.py\nFunction Name: _parse_cpu_cores\nLanguage: python\nPartition: train\n\n--- Docstring ---\nParses the cpu_cores given in the format as accepted by the ``devices`` argument in the\n\n--- Code ---\ndef _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores\n\n--- Original String ---\ndef _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores",
    "func_name": "_parse_cpu_cores",
    "path": "src\\lightning\\fabric\\accelerators\\cpu.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the",
    "code": "def _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores",
    "docstring_summary": "Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 68,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: setup_device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRaises:\n\n--- Code ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)\n\n--- Original String ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)",
    "func_name": "setup_device",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Raises:",
    "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)",
    "docstring_summary": "Raises:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 69,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: parse_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAccelerator device parsing logic.\n\n--- Code ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)\n\n--- Original String ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)",
    "func_name": "parse_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Accelerator device parsing logic.",
    "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)",
    "docstring_summary": "Accelerator device parsing logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 70,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: get_parallel_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets parallel devices for the Accelerator.\n\n--- Code ---\ndef get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]\n\n--- Original String ---\ndef get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]",
    "func_name": "get_parallel_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets parallel devices for the Accelerator.",
    "code": "def get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]",
    "docstring_summary": "Gets parallel devices for the Accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 71,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: auto_device_count\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGet the devices when set to auto.\n\n--- Code ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()\n\n--- Original String ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()",
    "func_name": "auto_device_count",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Get the devices when set to auto.",
    "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()",
    "docstring_summary": "Get the devices when set to auto."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 72,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: find_usable_cuda_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns a list of all available and usable CUDA GPU devices.\n\n--- Code ---\ndef find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            # exit early if we found the right number of GPUs\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices\n\n--- Original String ---\ndef find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            # exit early if we found the right number of GPUs\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices",
    "func_name": "find_usable_cuda_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns a list of all available and usable CUDA GPU devices.",
    "code": "def find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            # exit early if we found the right number of GPUs\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices",
    "docstring_summary": "Returns a list of all available and usable CUDA GPU devices."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 73,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: _get_all_visible_cuda_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns a list of all visible CUDA GPU devices.\n\n--- Code ---\ndef _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))\n\n--- Original String ---\ndef _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))",
    "func_name": "_get_all_visible_cuda_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns a list of all visible CUDA GPU devices.",
    "code": "def _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))",
    "docstring_summary": "Returns a list of all visible CUDA GPU devices."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 74,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: num_cuda_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns the number of available CUDA devices.\n\n--- Code ---\ndef num_cuda_devices() -> int:\r\n    \"\"\"Returns the number of available CUDA devices.\"\"\"\r\n    return torch.cuda.device_count()\n\n--- Original String ---\ndef num_cuda_devices() -> int:\r\n    \"\"\"Returns the number of available CUDA devices.\"\"\"\r\n    return torch.cuda.device_count()",
    "func_name": "num_cuda_devices",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns the number of available CUDA devices.",
    "code": "def num_cuda_devices() -> int:\r\n    \"\"\"Returns the number of available CUDA devices.\"\"\"\r\n    return torch.cuda.device_count()",
    "docstring_summary": "Returns the number of available CUDA devices."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "index": 75,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\cuda.py\nFunction Name: is_cuda_available\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns a bool indicating if CUDA is currently available.\n\n--- Code ---\ndef is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py\r\n    return torch.cuda.is_available()\n\n--- Original String ---\ndef is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py\r\n    return torch.cuda.is_available()",
    "func_name": "is_cuda_available",
    "path": "src\\lightning\\fabric\\accelerators\\cuda.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns a bool indicating if CUDA is currently available.",
    "code": "def is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py\r\n    return torch.cuda.is_available()",
    "docstring_summary": "Returns a bool indicating if CUDA is currently available."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 76,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: setup_device\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRaises:\n\n--- Code ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")\n\n--- Original String ---\ndef setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")",
    "func_name": "setup_device",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Raises:",
    "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")",
    "docstring_summary": "Raises:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 77,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: parse_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAccelerator device parsing logic.\n\n--- Code ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)\n\n--- Original String ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)",
    "func_name": "parse_devices",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Accelerator device parsing logic.",
    "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)",
    "docstring_summary": "Accelerator device parsing logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 78,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: get_parallel_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets parallel devices for the Accelerator.\n\n--- Code ---\ndef get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]\n\n--- Original String ---\ndef get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]",
    "func_name": "get_parallel_devices",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets parallel devices for the Accelerator.",
    "code": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]",
    "docstring_summary": "Gets parallel devices for the Accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 79,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: auto_device_count\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGet the devices when set to auto.\n\n--- Code ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1\n\n--- Original String ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1",
    "func_name": "auto_device_count",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Get the devices when set to auto.",
    "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1",
    "docstring_summary": "Get the devices when set to auto."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 80,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: is_available\nLanguage: python\nPartition: train\n\n--- Docstring ---\nMPS is only available on a machine with the ARM-based Apple Silicon processors.\n\n--- Code ---\ndef is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")\n\n--- Original String ---\ndef is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")",
    "func_name": "is_available",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "MPS is only available on a machine with the ARM-based Apple Silicon processors.",
    "code": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")",
    "docstring_summary": "MPS is only available on a machine with the ARM-based Apple Silicon processors."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\mps.py",
    "index": 81,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\mps.py\nFunction Name: _get_all_available_mps_gpus\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns:\n\n--- Code ---\ndef _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []\n\n--- Original String ---\ndef _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []",
    "func_name": "_get_all_available_mps_gpus",
    "path": "src\\lightning\\fabric\\accelerators\\mps.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns:",
    "code": "def _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []",
    "docstring_summary": "Returns:"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\registry.py",
    "index": 82,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\registry.py\nFunction Name: register\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRegisters a accelerator mapped to a name and with required metadata.\n\n--- Code ---\ndef register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register\n\n--- Original String ---\ndef register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register",
    "func_name": "register",
    "path": "src\\lightning\\fabric\\accelerators\\registry.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Registers a accelerator mapped to a name and with required metadata.",
    "code": "def register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register",
    "docstring_summary": "Registers a accelerator mapped to a name and with required metadata."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\registry.py",
    "index": 83,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\registry.py\nFunction Name: get\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalls the registered accelerator with the required parameters and returns the accelerator object.\n\n--- Code ---\ndef get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))\n\n--- Original String ---\ndef get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))",
    "func_name": "get",
    "path": "src\\lightning\\fabric\\accelerators\\registry.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Calls the registered accelerator with the required parameters and returns the accelerator object.",
    "code": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))",
    "docstring_summary": "Calls the registered accelerator with the required parameters and returns the accelerator object."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\registry.py",
    "index": 84,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\registry.py\nFunction Name: remove\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRemoves the registered accelerator by name.\n\n--- Code ---\ndef remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered accelerator by name.\"\"\"\r\n        self.pop(name)\n\n--- Original String ---\ndef remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered accelerator by name.\"\"\"\r\n        self.pop(name)",
    "func_name": "remove",
    "path": "src\\lightning\\fabric\\accelerators\\registry.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Removes the registered accelerator by name.",
    "code": "def remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered accelerator by name.\"\"\"\r\n        self.pop(name)",
    "docstring_summary": "Removes the registered accelerator by name."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\registry.py",
    "index": 85,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\registry.py\nFunction Name: available_accelerators\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturns a set of registered accelerators.\n\n--- Code ---\ndef available_accelerators(self) -> set[str]:\r\n        \"\"\"Returns a set of registered accelerators.\"\"\"\r\n        return set(self.keys())\n\n--- Original String ---\ndef available_accelerators(self) -> set[str]:\r\n        \"\"\"Returns a set of registered accelerators.\"\"\"\r\n        return set(self.keys())",
    "func_name": "available_accelerators",
    "path": "src\\lightning\\fabric\\accelerators\\registry.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Returns a set of registered accelerators.",
    "code": "def available_accelerators(self) -> set[str]:\r\n        \"\"\"Returns a set of registered accelerators.\"\"\"\r\n        return set(self.keys())",
    "docstring_summary": "Returns a set of registered accelerators."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\registry.py",
    "index": 86,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\registry.py\nFunction Name: call_register_accelerators\nLanguage: python\nPartition: train\n\n--- Docstring ---\nLegacy.\n\n--- Code ---\ndef call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)\n\n--- Original String ---\ndef call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)",
    "func_name": "call_register_accelerators",
    "path": "src\\lightning\\fabric\\accelerators\\registry.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Legacy.",
    "code": "def call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)",
    "docstring_summary": "Legacy."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\xla.py",
    "index": 87,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\xla.py\nFunction Name: parse_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nAccelerator device parsing logic.\n\n--- Code ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_tpu_devices(devices)\n\n--- Original String ---\ndef parse_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_tpu_devices(devices)",
    "func_name": "parse_devices",
    "path": "src\\lightning\\fabric\\accelerators\\xla.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Accelerator device parsing logic.",
    "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_tpu_devices(devices)",
    "docstring_summary": "Accelerator device parsing logic."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\xla.py",
    "index": 88,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\xla.py\nFunction Name: get_parallel_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets parallel devices for the Accelerator.\n\n--- Code ---\ndef get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        # list of devices is not supported, just a specific index, fine to access [0]\r\n        return [torch.device(\"xla\", devices[0])]\r\n        # we cannot create `xla_device` here because processes have not been spawned yet (this is called in the\r\n        # accelerator connector init). However, there doesn't seem to be a problem with instantiating `torch.device`.\r\n        # it will be replaced with `xla_device` (also a torch.device`, but with extra logic) in the strategy\r\n\n--- Original String ---\ndef get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        # list of devices is not supported, just a specific index, fine to access [0]\r\n        return [torch.device(\"xla\", devices[0])]\r\n        # we cannot create `xla_device` here because processes have not been spawned yet (this is called in the\r\n        # accelerator connector init). However, there doesn't seem to be a problem with instantiating `torch.device`.\r\n        # it will be replaced with `xla_device` (also a torch.device`, but with extra logic) in the strategy",
    "func_name": "get_parallel_devices",
    "path": "src\\lightning\\fabric\\accelerators\\xla.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets parallel devices for the Accelerator.",
    "code": "def get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        # list of devices is not supported, just a specific index, fine to access [0]\r\n        return [torch.device(\"xla\", devices[0])]\r\n        # we cannot create `xla_device` here because processes have not been spawned yet (this is called in the\r\n        # accelerator connector init). However, there doesn't seem to be a problem with instantiating `torch.device`.\r\n        # it will be replaced with `xla_device` (also a torch.device`, but with extra logic) in the strategy",
    "docstring_summary": "Gets parallel devices for the Accelerator."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\xla.py",
    "index": 89,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\xla.py\nFunction Name: auto_device_count\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGet the devices when set to auto.\n\n--- Code ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)\n\n--- Original String ---\ndef auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)",
    "func_name": "auto_device_count",
    "path": "src\\lightning\\fabric\\accelerators\\xla.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Get the devices when set to auto.",
    "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)",
    "docstring_summary": "Get the devices when set to auto."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\accelerators\\xla.py",
    "index": 90,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\accelerators\\xla.py\nFunction Name: _parse_tpu_devices\nLanguage: python\nPartition: train\n\n--- Docstring ---\nParses the TPU devices given in the format as accepted by the\n\n--- Code ---\ndef _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices\n\n--- Original String ---\ndef _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices",
    "func_name": "_parse_tpu_devices",
    "path": "src\\lightning\\fabric\\accelerators\\xla.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Parses the TPU devices given in the format as accepted by the",
    "code": "def _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices",
    "docstring_summary": "Parses the TPU devices given in the format as accepted by the"
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 91,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: name\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets the name of the experiment.\n\n--- Code ---\ndef name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name\n\n--- Original String ---\ndef name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name",
    "func_name": "name",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets the name of the experiment.",
    "code": "def name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name",
    "docstring_summary": "Gets the name of the experiment."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 92,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: version\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets the version of the experiment.\n\n--- Code ---\ndef version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version\n\n--- Original String ---\ndef version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version",
    "func_name": "version",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets the version of the experiment.",
    "code": "def version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version",
    "docstring_summary": "Gets the version of the experiment."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 93,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: root_dir\nLanguage: python\nPartition: train\n\n--- Docstring ---\nGets the save directory where the versioned CSV experiments are saved.\n\n--- Code ---\ndef root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the versioned CSV experiments are saved.\"\"\"\r\n        return self._root_dir\n\n--- Original String ---\ndef root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the versioned CSV experiments are saved.\"\"\"\r\n        return self._root_dir",
    "func_name": "root_dir",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Gets the save directory where the versioned CSV experiments are saved.",
    "code": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the versioned CSV experiments are saved.\"\"\"\r\n        return self._root_dir",
    "docstring_summary": "Gets the save directory where the versioned CSV experiments are saved."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 94,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: log_dir\nLanguage: python\nPartition: train\n\n--- Docstring ---\nThe log directory for this run.\n\n--- Code ---\ndef log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)\n\n--- Original String ---\ndef log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)",
    "func_name": "log_dir",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "The log directory for this run.",
    "code": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)",
    "docstring_summary": "The log directory for this run."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 95,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: experiment\nLanguage: python\nPartition: train\n\n--- Docstring ---\nActual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\n\n--- Code ---\ndef experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment\n\n--- Original String ---\ndef experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment",
    "func_name": "experiment",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.",
    "code": "def experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment",
    "docstring_summary": "Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 96,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: log_metrics\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRecord metrics.\n\n--- Code ---\ndef log_metrics(self, metrics_dict: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Record metrics.\"\"\"\r\n\r\n        def _handle_value(value: Union[Tensor, Any]) -> Any:\r\n            if isinstance(value, Tensor):\r\n                return value.item()\r\n            return value\r\n\r\n        if step is None:\r\n            step = len(self.metrics)\r\n\r\n        metrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n        metrics[\"step\"] = step\r\n        self.metrics.append(metrics)\n\n--- Original String ---\ndef log_metrics(self, metrics_dict: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Record metrics.\"\"\"\r\n\r\n        def _handle_value(value: Union[Tensor, Any]) -> Any:\r\n            if isinstance(value, Tensor):\r\n                return value.item()\r\n            return value\r\n\r\n        if step is None:\r\n            step = len(self.metrics)\r\n\r\n        metrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n        metrics[\"step\"] = step\r\n        self.metrics.append(metrics)",
    "func_name": "log_metrics",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Record metrics.",
    "code": "def log_metrics(self, metrics_dict: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Record metrics.\"\"\"\r\n\r\n        def _handle_value(value: Union[Tensor, Any]) -> Any:\r\n            if isinstance(value, Tensor):\r\n                return value.item()\r\n            return value\r\n\r\n        if step is None:\r\n            step = len(self.metrics)\r\n\r\n        metrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n        metrics[\"step\"] = step\r\n        self.metrics.append(metrics)",
    "docstring_summary": "Record metrics."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 97,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: save\nLanguage: python\nPartition: train\n\n--- Docstring ---\nSave recorded metrics into files.\n\n--- Code ---\ndef save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            # we need to re-write the file if the keys (header) change\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                # only write the header if we're writing a fresh file\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset\r\n\n--- Original String ---\ndef save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            # we need to re-write the file if the keys (header) change\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                # only write the header if we're writing a fresh file\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset",
    "func_name": "save",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Save recorded metrics into files.",
    "code": "def save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            # we need to re-write the file if the keys (header) change\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                # only write the header if we're writing a fresh file\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset",
    "docstring_summary": "Save recorded metrics into files."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "index": 98,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\csv_logs.py\nFunction Name: _record_new_keys\nLanguage: python\nPartition: train\n\n--- Docstring ---\nRecords new keys that have not been logged before.\n\n--- Code ---\ndef _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys\n\n--- Original String ---\ndef _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys",
    "func_name": "_record_new_keys",
    "path": "src\\lightning\\fabric\\loggers\\csv_logs.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Records new keys that have not been logged before.",
    "code": "def _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys",
    "docstring_summary": "Records new keys that have not been logged before."
  },
  {
    "label": "src_data",
    "file": "src\\lightning\\fabric\\loggers\\logger.py",
    "index": 99,
    "raw_text": "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\fabric\\loggers\\logger.py\nFunction Name: name\nLanguage: python\nPartition: train\n\n--- Docstring ---\nReturn the experiment name.\n\n--- Code ---\ndef name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name.\"\"\"\n\n--- Original String ---\ndef name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name.\"\"\"",
    "func_name": "name",
    "path": "src\\lightning\\fabric\\loggers\\logger.py",
    "repo": "pytorch-lightning",
    "language": "python",
    "partition": "train",
    "docstring": "Return the experiment name.",
    "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name.\"\"\"",
    "docstring_summary": "Return the experiment name."
  }
]