[
  {
    "query_id": 1,
    "query": "How to fix CUDA out of memory error in PyTorch Lightning?",
    "query_type": "debugging",
    "retrieved_docs": [
      {
        "func_name": "suggested_max_num_workers",
        "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 284
      },
      {
        "func_name": "suggested_max_num_workers",
        "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 3048
      },
      {
        "func_name": "suggested_max_num_workers",
        "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 1963
      },
      {
        "func_name": "init_module",
        "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 1611
      },
      {
        "func_name": "init_module",
        "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 2589
      }
    ],
    "scores": [
      0.7989990506810936,
      0.7989990506810936,
      0.7433984703476603,
      0.6859833486167949,
      0.6859833486167949
    ],
    "latency": 0.04627728462219238,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 2,
    "query": "Why is my validation loss not decreasing?",
    "query_type": "debugging",
    "retrieved_docs": [
      {
        "func_name": "post_training_step",
        "docstring_summary": "This hook is deprecated.",
        "path": "src\\lightning\\pytorch\\strategies\\strategy.py",
        "index": 861
      },
      {
        "func_name": "num_sanity_val_batches",
        "docstring_summary": "re-compute the `min` in case this is called outside the sanity-checking stage",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 1316
      },
      {
        "func_name": "suggestion",
        "docstring_summary": "This will propose a suggestion for an initial learning rate based on the point with the steepest negative",
        "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py",
        "index": 3767
      },
      {
        "func_name": "suggestion",
        "docstring_summary": "This will propose a suggestion for an initial learning rate based on the point with the steepest negative",
        "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py",
        "index": 1003
      },
      {
        "func_name": "init_validation_tqdm",
        "docstring_summary": "The train progress bar doesn't exist in `trainer.validate()`",
        "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py",
        "index": 1197
      }
    ],
    "scores": [
      0.6879689635962952,
      0.6705685270465256,
      0.59421068,
      0.59421068,
      0.5927306
    ],
    "latency": 0.046465158462524414,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 3,
    "query": "What parameters does the Lightning Trainer accept?",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "launch",
        "docstring_summary": "Launches processes that run the given function in parallel.",
        "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py",
        "index": 2568
      },
      {
        "func_name": "launch",
        "docstring_summary": "Launches processes that run the given function in parallel.",
        "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py",
        "index": 1598
      },
      {
        "func_name": "init_module",
        "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 1611
      },
      {
        "func_name": "advance",
        "docstring_summary": "Performs the training step for manual optimization.",
        "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py",
        "index": 1573
      },
      {
        "func_name": "advance",
        "docstring_summary": "Performs the training step for manual optimization.",
        "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py",
        "index": 2443
      }
    ],
    "scores": [
      0.6455795426500507,
      0.6455795426500507,
      0.6359623248437922,
      0.6282703870416656,
      0.6282703870416656
    ],
    "latency": 0.050171852111816406,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 4,
    "query": "How to use early stopping callback?",
    "query_type": "api_usage",
    "retrieved_docs": [
      {
        "func_name": "done",
        "docstring_summary": "early stopping",
        "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py",
        "index": 1266
      },
      {
        "func_name": "done",
        "docstring_summary": "Evaluates when to leave the loop. early stopping",
        "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py",
        "index": 1563
      },
      {
        "func_name": "done",
        "docstring_summary": "Evaluates when to leave the loop. early stopping",
        "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py",
        "index": 2417
      },
      {
        "func_name": "finalize",
        "docstring_summary": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using",
        "path": "src\\lightning\\pytorch\\loggers\\comet.py",
        "index": 3394
      },
      {
        "func_name": "finalize",
        "docstring_summary": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using",
        "path": "src\\lightning\\pytorch\\loggers\\comet.py",
        "index": 630
      }
    ],
    "scores": [
      0.91956951,
      0.8737373639517392,
      0.8737373639517392,
      0.6130135832458591,
      0.6130135832458591
    ],
    "latency": 0.04206490516662598,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 5,
    "query": "Implement custom callback in PyTorch Lightning",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "creates_processes_externally",
        "docstring_summary": "LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.",
        "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py",
        "index": 2896
      },
      {
        "func_name": "creates_processes_externally",
        "docstring_summary": "LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.",
        "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py",
        "index": 132
      },
      {
        "func_name": "creates_processes_externally",
        "docstring_summary": "LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.",
        "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py",
        "index": 1811
      },
      {
        "func_name": "progress_bar_callback",
        "docstring_summary": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 923
      },
      {
        "func_name": "progress_bar_callback",
        "docstring_summary": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the",
        "path": "src\\lightning\\pytorch\\trainer\\trainer.py",
        "index": 3687
      }
    ],
    "scores": [
      0.8536788233437226,
      0.8536788233437226,
      0.8536788233437226,
      0.6430378319645753,
      0.6430378319645753
    ],
    "latency": 0.03058314323425293,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 6,
    "query": "Multi-GPU training setup",
    "query_type": "implementation",
    "retrieved_docs": [
      {
        "func_name": "reset",
        "docstring_summary": "when restarting, if we are running `validate` or `test` twice, since there's no concept of `max_epochs` we need to reset the current state when the loop has finished running some users want validation shuffling based on the training progress set the per-da",
        "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py",
        "index": 1252
      },
      {
        "func_name": "finalize",
        "docstring_summary": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using",
        "path": "src\\lightning\\pytorch\\loggers\\comet.py",
        "index": 1538
      },
      {
        "func_name": "finalize",
        "docstring_summary": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using",
        "path": "src\\lightning\\pytorch\\loggers\\comet.py",
        "index": 2309
      },
      {
        "func_name": "reset",
        "docstring_summary": "Resets the internal state of the loop. when restarting, if we are running `validate` or `test` twice, since there's no concept of `max_epochs` we need to reset the current state when the loop has finished running some users want validation shuffling based ",
        "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py",
        "index": 2362
      },
      {
        "func_name": "reset",
        "docstring_summary": "Resets the internal state of the loop. when restarting, if we are running `validate` or `test` twice, since there's no concept of `max_epochs` we need to reset the current state when the loop has finished running some users want validation shuffling based ",
        "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py",
        "index": 1549
      }
    ],
    "scores": [
      0.5737964059077483,
      0.5731801749077482,
      0.5731801749077482,
      0.5728057726410298,
      0.5728057726410298
    ],
    "latency": 0.041596174240112305,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 7,
    "query": "Difference between training_step and validation_step",
    "query_type": "conceptual",
    "retrieved_docs": [
      {
        "func_name": "validation_step",
        "docstring_summary": "if you have one val dataloader: if you have multiple val dataloaders: CASE 1: A single validation dataset implement your own log 6 example images or generated text... or whatever calculate acc log the outputs! CASE 2: multiple validation dataloaders datalo",
        "path": "src\\lightning\\pytorch\\core\\module.py",
        "index": 1215
      },
      {
        "func_name": "on_train_batch_end",
        "docstring_summary": "Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps` Do not return early here because we may need to set deferral flags even if a save already happened at this global step. We'll enforce the skip just before actually saving ",
        "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
        "index": 2124
      },
      {
        "func_name": "on_train_batch_end",
        "docstring_summary": "Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps` Do not return early here because we may need to set deferral flags even if a save already happened at this global step. We'll enforce the skip just before actually saving ",
        "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
        "index": 1482
      },
      {
        "func_name": "on_train_batch_end",
        "docstring_summary": "Do not return early here because we may need to set deferral flags even if a save already happened at this global step. We'll enforce the skip just before actually saving below. Important: allow zero timedelta as a valid interval in case we have time diffe",
        "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py",
        "index": 1185
      },
      {
        "func_name": "experiment",
        "docstring_summary": "log metrics log images",
        "path": "src\\lightning\\pytorch\\loggers\\neptune.py",
        "index": 1244
      }
    ],
    "scores": [
      0.7113351430218244,
      0.620191666193404,
      0.620191666193404,
      0.6194054325640939,
      0.600699582
    ],
    "latency": 0.04711103439331055,
    "method": "graphcodebert-hybrid"
  },
  {
    "query_id": 8,
    "query": "How does automatic optimization work?",
    "query_type": "conceptual",
    "retrieved_docs": [
      {
        "func_name": "sized_len",
        "docstring_summary": "try getting the length",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 1143
      },
      {
        "func_name": "experiment",
        "docstring_summary": "log metrics log images",
        "path": "src\\lightning\\pytorch\\loggers\\neptune.py",
        "index": 1244
      },
      {
        "func_name": "_should_accumulate",
        "docstring_summary": "Lightning steps on the final batch but the strategy might not",
        "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py",
        "index": 1269
      },
      {
        "func_name": "suggested_max_num_workers",
        "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 284
      },
      {
        "func_name": "suggested_max_num_workers",
        "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on",
        "path": "src\\lightning\\fabric\\utilities\\data.py",
        "index": 3048
      }
    ],
    "scores": [
      0.59406816,
      0.59302579,
      0.5922721,
      0.5909500099999999,
      0.5909500099999999
    ],
    "latency": 0.04317784309387207,
    "method": "graphcodebert-hybrid"
  }
]