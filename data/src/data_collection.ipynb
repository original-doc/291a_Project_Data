{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be7982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "GITHUB_TOKEN = \"\"\n",
    "API_URL = \"https://api.github.com/graphql\"\n",
    "OWNER = \"Lightning-AI\"\n",
    "REPO = \"pytorch-lightning\"\n",
    "OUTPUT_FILE = \"../final_data/lightning_discussions_answered.json\"\n",
    "\n",
    "def get_discussions():\n",
    "    query = \"\"\"\n",
    "    query($cursor: String) {\n",
    "      repository(owner: \"%s\", name: \"%s\") {\n",
    "        discussions(first: 100, after: $cursor, orderBy: {field: CREATED_AT, direction: DESC}) {\n",
    "          pageInfo {\n",
    "            hasNextPage\n",
    "            endCursor\n",
    "          }\n",
    "          nodes {\n",
    "            title\n",
    "            url\n",
    "            createdAt\n",
    "            answerChosenAt\n",
    "            author {\n",
    "              login}\n",
    "            bodyText\n",
    "            answer {\n",
    "              author {\n",
    "                login\n",
    "              }\n",
    "              bodyText\n",
    "            }\n",
    "            comments(first: 20) {\n",
    "              nodes {\n",
    "                author {\n",
    "                  login\n",
    "                }\n",
    "                bodyText\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % (OWNER, REPO)\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\"}\n",
    "\n",
    "    all_discussions = []\n",
    "    cursor = None\n",
    "\n",
    "    while True:\n",
    "        response = requests.post(API_URL, json={\"query\": query, \"variables\": {\"cursor\": cursor}}, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        discussions_data = (\n",
    "            data.get(\"data\", {})\n",
    "            .get(\"repository\", {})\n",
    "            .get(\"discussions\", {})\n",
    "        )\n",
    "\n",
    "        if not discussions_data:\n",
    "            print(\"No discussions found or API limit reached.\")\n",
    "            break\n",
    "\n",
    "        nodes = discussions_data.get(\"nodes\", [])\n",
    "        all_discussions.extend(nodes)\n",
    "\n",
    "        if len(all_discussions) >= 1000:\n",
    "            break\n",
    "\n",
    "        page_info = discussions_data.get(\"pageInfo\", {})\n",
    "        if not page_info.get(\"hasNextPage\"):\n",
    "            break\n",
    "\n",
    "        cursor = page_info.get(\"endCursor\")\n",
    "\n",
    "    return all_discussions\n",
    "\n",
    "# Filter for answered, which means the asker approved the answer\n",
    "data = get_discussions()\n",
    "answered = []\n",
    "ctr = 0\n",
    "for discussion in data:\n",
    "    is_answered = discussion[\"answer\"] is not None\n",
    "    comments = discussion.get(\"comments\", {}).get(\"nodes\", [])\n",
    "\n",
    "#  [\"thanks\", \"appreciate\", \"great\", \"excellent\", \"perfect\"]\n",
    "    # heuristic: check for positive replies confirming the solution worked\n",
    "    good_keywords = [\"worked\", \"thank you\", \"good answer\", \"fixed\", \"solved\"]\n",
    "    has_positive_reply = any(\n",
    "        any(k in c[\"bodyText\"].lower() for k in good_keywords)\n",
    "        for c in comments\n",
    "    )\n",
    "\n",
    "    if is_answered and has_positive_reply:\n",
    "        discussion[\"label\"] = \"discussion\"\n",
    "        discussion[\"file\"] = \"../final_data/discussions.json\"\n",
    "        discussion[\"index\"] = ctr\n",
    "        ctr += 1\n",
    "        del discussion[\"comments\"]\n",
    "        answered.append(discussion)\n",
    "\n",
    "\n",
    "# --- Keep only the most recent 50 ---\n",
    "answered = answered[:50]\n",
    "\n",
    "# --- Save to JSON ---\n",
    "with open(\"../final_data/discussions.json\", \"w\") as f:\n",
    "    json.dump(answered, f, indent=2)\n",
    "\n",
    "print(len(answered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9db0d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Training is taking a long time. How do I speed up the training for multiple datasets?', 'query_type': 'debugging', 'relevant_docs': [{'file': '../data/final_data/discussions.json', 'index': 1, 'text': '', 'score': 7}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 57, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 58, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 59, 'text': '', 'score': 9}]}\n",
      "{'query': 'What do I need to pass into LightningCLI() to get it working? Also, please give an example.', 'query_type': 'api_usage', 'relevant_docs': [{'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 6, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 7, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 4, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 1, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 2, 'text': '', 'score': 9}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 3, 'text': '', 'score': 9}, {'file': '../data/final_data/discussions.json', 'index': 0, 'text': '', 'score': 6}, {'file': '../data/final_data/discussions.json', 'index': 18, 'text': '', 'score': 6}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 5, 'text': '', 'score': 7}]}\n",
      "{'query': 'For defining a training step function, when should I use manual backward and what issues can arrise from using it?', 'query_type': 'api_usage', 'relevant_docs': [{'file': '../data/final_data/discussions.json', 'index': 20, 'text': '', 'score': 7}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 20, 'text': '', 'score': 8}, {'file': '../data/final_data/lightning_docs_cleaned.json', 'index': 28, 'text': '', 'score': 6}]}\n",
      "{'query': \"init_meta_context() isn't available. What are some alternitives that have similar functionality?\", 'query_type': 'debugging', 'relevant_docs': [{'file': '../data/final_data/discussions.json', 'index': 3, 'text': '', 'score': 7}, {'file': '../data/final_data/src_filtered_data.json', 'index': 1346, 'text': '', 'score': 7}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "doc_links = []\n",
    "doc_dir = \"../final_data\"\n",
    "doc_jsons = []\n",
    "# handle documentation json\n",
    "with open(os.path.join(doc_dir, \"lightning_docs_cleaned.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_jsons.append(data)\n",
    "    doc_links += [d[\"url_html\"] for d in data]\n",
    "    doc_docs = [f\"{d['title']}\\n{d['text']}\" for d in data]\n",
    "\n",
    "# handle discussions json\n",
    "with open(os.path.join(doc_dir, \"discussions.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"url\"] for d in data]\n",
    "    disc_docs = [\n",
    "        f\"{d.get('title','')}\\n{d.get('bodyText','')}\\nAnswer: {d.get('answer', {}).get('bodyText','')}\"\n",
    "        for d in data\n",
    "    ]\n",
    "\n",
    "# handle src code json\n",
    "with open(os.path.join(doc_dir, \"src_filtered_data.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"file\"] for d in data]\n",
    "    src_docs = [f\"{d['text']}\" for d in data]\n",
    "\n",
    "with open(\"../../requests/richa_requests.json\") as f:\n",
    "    queries = json.load(f)\n",
    "    for q in queries:\n",
    "        print(q)\n",
    "        for d in q[\"relevant_docs\"]:\n",
    "            \n",
    "            if \"discussions.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = disc_docs[d[\"index\"]]\n",
    "            elif \"docs_cleaned.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = doc_docs[d[\"index\"]]\n",
    "            elif \"src_filtered_data.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = src_docs[d[\"index\"]]\n",
    "    json.dump(queries, open(\"../../requests/final_requests.json\", \"a\"), indent=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
