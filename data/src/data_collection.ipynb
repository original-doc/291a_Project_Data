{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be7982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "GITHUB_TOKEN = \"\"\n",
    "API_URL = \"https://api.github.com/graphql\"\n",
    "OWNER = \"Lightning-AI\"\n",
    "REPO = \"pytorch-lightning\"\n",
    "OUTPUT_FILE = \"../final_data/lightning_discussions_answered.json\"\n",
    "\n",
    "def get_discussions():\n",
    "    query = \"\"\"\n",
    "    query($cursor: String) {\n",
    "      repository(owner: \"%s\", name: \"%s\") {\n",
    "        discussions(first: 100, after: $cursor, orderBy: {field: CREATED_AT, direction: DESC}) {\n",
    "          pageInfo {\n",
    "            hasNextPage\n",
    "            endCursor\n",
    "          }\n",
    "          nodes {\n",
    "            title\n",
    "            url\n",
    "            createdAt\n",
    "            answerChosenAt\n",
    "            author {\n",
    "              login}\n",
    "            bodyText\n",
    "            answer {\n",
    "              author {\n",
    "                login\n",
    "              }\n",
    "              bodyText\n",
    "            }\n",
    "            comments(first: 20) {\n",
    "              nodes {\n",
    "                author {\n",
    "                  login\n",
    "                }\n",
    "                bodyText\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % (OWNER, REPO)\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\"}\n",
    "\n",
    "    all_discussions = []\n",
    "    cursor = None\n",
    "\n",
    "    while True:\n",
    "        response = requests.post(API_URL, json={\"query\": query, \"variables\": {\"cursor\": cursor}}, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        discussions_data = (\n",
    "            data.get(\"data\", {})\n",
    "            .get(\"repository\", {})\n",
    "            .get(\"discussions\", {})\n",
    "        )\n",
    "\n",
    "        if not discussions_data:\n",
    "            print(\"No discussions found or API limit reached.\")\n",
    "            break\n",
    "\n",
    "        nodes = discussions_data.get(\"nodes\", [])\n",
    "        all_discussions.extend(nodes)\n",
    "\n",
    "        if len(all_discussions) >= 1000:\n",
    "            break\n",
    "\n",
    "        page_info = discussions_data.get(\"pageInfo\", {})\n",
    "        if not page_info.get(\"hasNextPage\"):\n",
    "            break\n",
    "\n",
    "        cursor = page_info.get(\"endCursor\")\n",
    "\n",
    "    return all_discussions\n",
    "\n",
    "# Filter for answered, which means the asker approved the answer\n",
    "data = get_discussions()\n",
    "answered = []\n",
    "ctr = 0\n",
    "for discussion in data:\n",
    "    is_answered = discussion[\"answer\"] is not None\n",
    "    comments = discussion.get(\"comments\", {}).get(\"nodes\", [])\n",
    "\n",
    "#  [\"thanks\", \"appreciate\", \"great\", \"excellent\", \"perfect\"]\n",
    "    # heuristic: check for positive replies confirming the solution worked\n",
    "    good_keywords = [\"worked\", \"thank you\", \"good answer\", \"fixed\", \"solved\"]\n",
    "    has_positive_reply = any(\n",
    "        any(k in c[\"bodyText\"].lower() for k in good_keywords)\n",
    "        for c in comments\n",
    "    )\n",
    "\n",
    "    if is_answered and has_positive_reply:\n",
    "        discussion[\"label\"] = \"discussion\"\n",
    "        discussion[\"file\"] = \"../final_data/discussions.json\"\n",
    "        discussion[\"index\"] = ctr\n",
    "        ctr += 1\n",
    "        del discussion[\"comments\"]\n",
    "        answered.append(discussion)\n",
    "\n",
    "\n",
    "# --- Keep only the most recent 50 ---\n",
    "answered = answered[:50]\n",
    "\n",
    "# --- Save to JSON ---\n",
    "with open(\"../final_data/discussions.json\", \"w\") as f:\n",
    "    json.dump(answered, f, indent=2)\n",
    "\n",
    "print(len(answered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9db0d519",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m     doc_jsons\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m----> 9\u001b[0m     doc_links \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_html\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     10\u001b[0m     doc_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# handle discussions json\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m     doc_jsons\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m----> 9\u001b[0m     doc_links \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl_html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     10\u001b[0m     doc_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# handle discussions json\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'url_html'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "doc_links = []\n",
    "doc_dir = \"../final_data\"\n",
    "doc_jsons = []\n",
    "# handle documentation json\n",
    "with open(os.path.join(doc_dir, \"docs.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_jsons.append(data)\n",
    "    doc_links += [d[\"url_html\"] for d in data]\n",
    "    doc_docs = [f\"{d['title']}\\n{d['text']}\" for d in data]\n",
    "\n",
    "# handle discussions json\n",
    "with open(os.path.join(doc_dir, \"discussions.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"url\"] for d in data]\n",
    "    disc_docs = [\n",
    "        f\"{d.get('title','')}\\n{d.get('bodyText','')}\\nAnswer: {d.get('answer', {}).get('bodyText','')}\"\n",
    "        for d in data\n",
    "    ]\n",
    "\n",
    "# handle src code json\n",
    "with open(os.path.join(doc_dir, \"src_filtered_data.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"file\"] for d in data]\n",
    "    src_docs = [f\"{d['text']}\" for d in data]\n",
    "\n",
    "with open(\"../../requests/richa_requests.json\") as f:\n",
    "    queries = json.load(f)\n",
    "    for q in queries:\n",
    "        print(q)\n",
    "        for d in q[\"relevant_docs\"]:\n",
    "            \n",
    "            if \"discussions.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = disc_docs[d[\"index\"]]\n",
    "            elif \"docs_cleaned.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = doc_docs[d[\"index\"]]\n",
    "            elif \"src_filtered_data.json\" in d[\"file\"]:\n",
    "                d[\"text\"] = src_docs[d[\"index\"]]\n",
    "    json.dump(queries, open(\"../../requests/final_requests.json\", \"w\"), indent=2)\n",
    "\n",
    "with open(\"../../requests/docs_request.json\") as f:\n",
    "    queries = json.load(f)\n",
    "    json.dump(queries, open(\"../../requests/final_requests.json\", \"a\"), indent=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
