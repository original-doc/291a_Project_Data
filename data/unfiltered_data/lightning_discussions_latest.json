[
  {
    "title": "DDP training and storing rank specific info in checkpoints",
    "body": "I'm working on preserving state between start/stop of training runs in a manner that guarantees reproducible results. That is, I'd like to be able to stop my training at any given checkpoint, then restart the training from that checkpoint and finish to completion, and have these results match (exactly) the results obtained from a single continuous run. I've been able to do this on single node setups by storing the outputs of \r\n\r\n- `torch.get_rng_state()` \r\n- `torch.cuda.get_rng_state()`\r\n- `np.random.get_state()`\r\n- `random.getstate()`\r\n\r\nwithin the model checkpoint, and using the corresponding `set` method upon loading the checkpoint. I've been performing the save/load routines within a custom `pytorch_lightning.callbacks.Callback` by overriding the `on_save_checkpoint` and `on_load_checkpoint` appropriately. \r\n\r\nI'm now trying to perform the same checkpoint save/load procedure using a multi-node setup, with a DDP strategy. My attempt was to append the global-rank-specific rng states to the checkpoint dictionary, which I had thought would then be saved appropriately. However, when I executed the code, the only rng state that is preserved within the checkpoint dictionary, is the rank 0 state. Can someone please advise on how to preserve the rng states from other ranks within the checkpoint in a DDP setup? As a higher level question: if there is a better way to preserve these states between training runs rather than checkpoint storage and re-instantiation, that information would also be welcome.\r\n\r\nThe main Callback save routine I'm using is posted below. I've then been checking the contents of the saved checkpoint dictionary by using a manual `torch.load()` call.\r\n\r\npython version: 3.9.12\r\npytorch version: 2.2.0+cu121\r\npytorch_lightning version: 2.2.0\r\n\r\n```\r\nclass CustomCallback(ptl.callbacks.Callback):\r\n    def on_save_checkpoint(self, trainer, module, checkpoint):\r\n        # get random states\r\n        state = {\r\n            'torch': torch.get_rng_state().cpu(),\r\n            'cuda': torch.cuda.get_rng_state().cpu() if torch.cuda.is_available() else None,\r\n            'numpy': np.random.get_state(),\r\n            'random': random.getstate(),\r\n        }\r\n        rank = trainer.global_rank\r\n        checkpoint[f'state_{rank}'] = state.  # note: this key never appears in the saved checkpoint except for rank 0\r\n\r\n        # note: this code *does* execute, I do see the saved data for each rank, \r\n        # but I'd rather store it cleanly in the checkpoint file\r\n        torch.save(state, f'rng_state_{rank}.pt')\r\n\r\n    def on_load_checkpoint(self, trainer, module, checkpoint):\r\n        # pass for now, easy enough to update if I get the on_save_ method working appropriately\r\n        pass\r\n```\r\n      ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/21097",
    "createdAt": "2025-08-19T16:56:02Z",
    "updatedAt": "2025-08-26T16:11:20Z",
    "closedAt": "2025-08-26T16:11:13Z",
    "isAnswered": true,
    "author": {
      "login": "bardsleypt"
    },
    "answer": {
      "body": "After extensive doc-searching and even more extensive trial-and-error, I believe I have a good understanding of this issue.  Unfortunately, unless there is a flag within `pytorch`, `ddp`, `lightning`, or `CUDA` governing GPU scheduling determinism that I don't know about, my main problem of forcing _exact_ reproducibility seems impossible (or at least largely impractical) for reasons I'll summarize below. I am moving on from this problem, by simply avoiding the model stop/restart process I mentioned above via other methods. But I'm posting the information I have discovered in case it helps anyone with a similar problem.\r\n\r\nFor starters, my second comment is more-or-less correct. Each rank (i.e., each process) gets its own instantiation of the entire training run (thus all object instantiations including trainers, dataloaders, callbacks, etc.), but under the hood `ddp` + `lightning` is only letting a given process see a subset of the training data. During a single batch on a given rank, the data is put through the forward pass of the lightning module, after which the backward pass is called. The backward pass computes gradients locally (on the given rank/process) but crucially as soon as the backward pass completes, an `all-reduce` routine is called deep within the `ddp` + `lightning` source code to aggregate and synchronize all gradients across all ranks. This way, when it is time to make the optimization step, all processes have the same gradient values (i.e., the mean of the gradients over all ranks).\r\n\r\nI was able to determine the gradients are the first location I encountered a discrepancy between a continuous training run and a run that involved a stop-checkpoint-restart at a given epoch (epoch 3 in my case). At this breakpoint, the two training runs had different gradients going into the next optimization step (epoch 3 update step), where the discrepancy was O(10^-11). After a lot of additional model hacking to debug the local gradients across each rank, I determined this discrepancy came purely from the order in which the gradients are synchronized (i.e., floating point arithmetic does not obey commutativity of addition perfectly). For example, on the continuous training run with 4 ranks (GPUs), the gradients were aggregated in an order [0, 1, 2, 3], while on the restarted training run the gradients were aggregated in an order [1, 2, 3, 0] (or possibly a similar ordering, but certainly not the same order). I verified this by manually combining all of the local gradients from the restarted training run in different orders until I was able to reproduce the aggregated gradient from both the continuous training run and the restarted training run. That is, I could get different aggregated gradient values purely from the order in which I performed the addition/averaging of the local gradients, one corresponding to the restarted run and another corresponding to the continuous run. This indicated to me that the order in which the GPUs are aggregating their gradients is not the same between my two training runs, though it is still deterministic (i.e., the restarted training run always gave the same discrepancy from the continuous training run). \r\n\r\nShort of controlling the exact scheduling of GPU processes, and/or rewriting my model code to perform the aggregation/optimization steps manually, it seems _exact_ reproducibility between these two runs is not possible. If anyone does stumble across this and has more information and better ideas on this, I'd be happy to learn more here.\r\n\r\n## Debugging local gradients\r\nHere is little more information for anyone encountering similar problems. These are the steps I had to take within my `lightning` module and training code to debug and output the local (i.e., per process or per rank) gradients. I didn't find this process all that intuitive nor well-documented, so hopefully this helps someone.\r\n\r\nThe synchronization of the gradients happens almost immediately in the training process, and is handled using some form of  hook/observer/subscriber pattern. I'm not sure the specifics here, but it is definitely opaque to the high-level pytorch-lightning user. This means by the time one is at a callback such as `on_before_optimizer_step` or even `on_after_backward`, the gradients have already been synchronized and the local gradients are lost. To access the local gradients then, one needs to compute them in a manual fashion without the mentioned synchornization hook in place, store them, and then output them whenever convenient. Here is my approach:\r\n\r\n```\r\nclass MyModel(ptl.LightningModule):\r\n    def __init__(self, *args, **kwargs):\r\n        ...\r\n        self.automatic_optimization = False     # this allows calls to manual_backwards() but changes the train_step functionality\r\n        self.local_grads = [] # storage of local gradients\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ...\r\n       loss = self.forward(*args, **kwargs)\r\n        \r\n        with self.trainer.strategy.model.no_sync():  # prevents immediate aggregation of gradients \r\n            self.manual_backward(loss, retain_graph=True).   # debug backward pass to populate local gradients\r\n            self.local_grads = [p.grad.clone().detach() if p.grad is not None else None for p in self.parameters()]\r\n\r\n        opt = self.optimizers()\r\n        opt.zero_grad()  # clear any/all gradients from non-synced step above\r\n        self.manual_backward(loss)  # actual backward pass used for gradients in optimization step\r\n        opt.zero_grad()\r\n        opt.step()\r\n        \r\n        return loss\r\n```\r\nOnce the local gradients are cloned, detached, and stored inside of `local_grads`, they can be accessed from any of the various callbacks (either lightning-module callback hooks or lighting-callback hooks). Since the object is instantiated on each rank/process, each rank/process will store its own copy of the local gradients. In this manner, I was able to pull the local gradients together (after saving them to disk) for each rank, and reassemble them in different orders to achieve slightly varying aggregated gradient values.\r\n",
      "author": {
        "login": "bardsleypt"
      },
      "createdAt": "2025-08-26T16:11:13Z"
    }
  },
  {
    "title": "Dynamically Setting out_dim Based on LightningDataModule in LightningCLI",
    "body": "### Dynamically Setting `out_dim` Based on `LightningDataModule` in LightningCLI\r\n\r\nHi all,\r\n\r\nI\u2019m looking for a way to dynamically set the `out_dim` parameter in my `LightningModule`. Currently, this is passed explicitly via the command line, but I want to avoid that and instead have it automatically set based on the `num_of_labels` from the `LightningDataModule`. The problem is that `num_of_labels` is determined inside the `setup` method of the `DataModule`, not during its initialization.\r\n\r\nI tried using `link_arguments` to link the `num_of_labels` from the `DataModule` to the `out_dim` in the `LightningModule`, but it doesn\u2019t seem to work for this use case.\r\n\r\nI\u2019ve created a minimal example of the code below to illustrate the issue:\r\n_**Note**_ :  **`num_of_labels` can only be determined in `setup` method of the datamodule in below case, and not in constructor**.\r\n\r\n```python\r\nimport torch\r\nfrom lightning.pytorch import LightningDataModule, LightningModule\r\nimport torch.nn.functional as F\r\nfrom lightning.pytorch.cli import LightningCLI\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass MyLightningDataModule(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self._num_of_labels = None\r\n        self._feature_vector_size = None\r\n\r\n    def prepare_data(self):\r\n        pass\r\n\r\n    def setup(self, stage=None):\r\n        self._num_of_labels = 10\r\n        self._feature_vector_size = 20\r\n\r\n    @property\r\n    def num_of_labels(self):\r\n        return self._num_of_labels\r\n\r\n    @property\r\n    def feature_vector_size(self):\r\n        return self._feature_vector_size\r\n\r\n    def train_dataloader(self):\r\n        assert self.feature_vector_size is not None, \"feature_vector_size must be set\"\r\n        # Dummy dataset for example purposes\r\n        x = torch.randn(100, self.feature_vector_size)\r\n        y = torch.randn(100, self._num_of_labels)\r\n        dataset = TensorDataset(x, y)\r\n        return DataLoader(dataset, batch_size=32)\r\n\r\n\r\nclass MyLightningModel(LightningModule):\r\n    def __init__(self, input_dim: int, out_dim: int):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        assert out_dim is not None, \"out_dim must be set\"\r\n        assert input_dim is not None, \"input_dim must be set\"\r\n        # self.fc1 = torch.nn.Linear(20, 10)\r\n        self.fc1 = torch.nn.Linear(input_dim, out_dim)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def forward(self, x):\r\n        return self.fc1(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        loss = F.mse_loss(self.forward(x), y)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.num_of_labels', 'model.out_dim', apply_on='instantiate')\r\n        parser.link_arguments(\"data.feature_vector_size\", \"model.input_dim\", apply_on=\"instantiate\")\r\n\r\n        parser.link_arguments(\r\n            \"data.num_of_labels\", \"trainer.callbacks.init_args.num_labels\", apply_on=\"instantiate\"\r\n        )\r\n\r\n        # Further I need `num_of_labels` for `torchmetrics.MetricCollection`, commented as of now\r\n        # command: fit --model.train_metrics=\"path_to_yaml_file\"\r\n        # for kind in (\"train\", \"val\", \"test\"):\r\n        #     for average in (\"micro-f1\", \"macro-f1\"):\r\n        #         parser.link_arguments(\r\n        #             'data.num_of_labels',\r\n        #             f\"model.init_args.{kind}_metrics.init_args.metrics.{average}.init_args.num_labels\",\r\n        #             apply_on=\"instantiate\"\r\n        #         )\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli = MyCLI(MyLightningModel, MyLightningDataModule,)\r\n\r\n```\r\n\r\nAs you can see, I\u2019m trying to use `link_arguments` to link `num_of_labels` to `out_dim`, but since `num_of_labels` is determined in the `setup` method (not the constructor), I\u2019m running into issues. \r\n\r\nHas anyone encountered a similar situation or found a way to dynamically set the `out_dim` based on the data module? Is there a better way to achieve this, or am I missing something?\r\n\r\nAny feedback or suggestions would be greatly appreciated!\r\n\r\nThanks in advance!\r\n\r\n### Environment\r\n\r\n- **PyTorch Lightning**: 2.1.2\r\n- **Python**: 3.10.14\r\n- **Torch**: 2.5.1\r\n- **jsonargparse**: 4.31.0\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20602",
    "createdAt": "2025-02-25T17:37:12Z",
    "updatedAt": "2025-02-26T23:00:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "aditya0by0"
    },
    "answer": {
      "body": "You have two options:\r\n\r\n1. **[preferred]** In the data module, change the `setup` method so that if it is called multiple time, only the first run does the required logic. Then change the `num_of_labels` property so that it calls `setup` before accessing the `_num_of_labels` attribute.\r\n2. Create a link applied on instantiation with source the entire `data` instance (instead of an attribute of it) and a `compute_fn` that first runs `setup` and then accesses the `num_of_labels` property.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2025-02-25T18:31:21Z"
    }
  },
  {
    "title": "Avoid deepspeed plugin converting the whole model",
    "body": "I am using deepspeed plugin in lightning to train my model. I want the first part of my model to be float32, while the second part to be bfloat16 (the optimizer only trains the first part). However, I found lightning will convert the whole model to float32 if I do not specify the precision. How to keep my pre-defined model dtype untouched?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20543",
    "createdAt": "2025-01-11T01:26:45Z",
    "updatedAt": "2025-01-11T07:13:36Z",
    "closedAt": "2025-01-11T07:13:26Z",
    "isAnswered": true,
    "author": {
      "login": "Boltzmachine"
    },
    "answer": {
      "body": "I found you should implement the dtype conversion in your `LightningModule`, and avoid `DeepSpeedStrategy` from converting your module.\r\n\r\n```python\r\nfrom lightning.pytorch.plugins import DeepSpeedPrecision\r\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\r\nfrom typing_extensions import override\r\n\r\nclass DeepSpeedPrecisionWithoutModuleConversion(DeepSpeedPrecision):\r\n    @override\r\n    def convert_module(self, module):\r\n        return module\r\n```\r\n\r\nand pass to the trainer as\r\n```python\r\ntrainer = Trainer(\r\n    ...,\r\n    DeepSpeedStrategy(stage=2, precision_plugin=DeepSpeedPrecisionWithoutModuleConversion('32-true'))\r\n)\r\n```",
      "author": {
        "login": "Boltzmachine"
      },
      "createdAt": "2025-01-11T07:13:26Z"
    }
  },
  {
    "title": "Alternate \"prediction\" loops",
    "body": "I am hoping to implement an additional loop through the Trainer in order to leverage Lightnings auto*magic* handling of dataloaders and GPUs. Specifically, I want to run batches through the attribution methods from [Captum](https://captum.ai/). My first attempt was to hijack the `predition_step` of the LightningModule with a simple class attribute `self.calculate_attributes` switch:\r\n\r\n```python3\r\n    def predict_step(self, batch, batch_idx):\r\n        if self.calculate_attributes:\r\n            return self.attribution_step(batch, batch_idx)\r\n        else:\r\n            data, target = batch\r\n            return self.model(data)\r\n    \r\n    def attribution_step(self, batch, batch_idx):\r\n        data, target = batch\r\n        batch_size = data.shape[0]\r\n        baselines = torch.zeros_like(data)\r\n        attribution = self.explainer.attribute(data, baselines, target=target, internal_batch_size=batch_size)\r\n        return attribution, target\r\n```\r\n\r\nBut this has run into issues because gradients are required, and, I believe, the prediction loop disables them. I tried to get around with the `@torch.enable_grad()` decorator and explicit `.requires_grad=True` calls, but that is not working. For every attempt I get the error `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`. I have no issue running the explainers on their own on the same data. I've been looking at the depreciated [Loops interface](https://lightning.ai/docs/pytorch/LTS/extensions/loops.html) but haven't made any progress there either.\r\n\r\nIs there a proper way to implement this? Any suggestions would be appreciated.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20318",
    "createdAt": "2024-10-04T01:34:39Z",
    "updatedAt": "2024-10-04T03:58:51Z",
    "closedAt": "2024-10-04T03:58:47Z",
    "isAnswered": true,
    "author": {
      "login": "kaboroevich"
    },
    "answer": {
      "body": "Setting `Trainer(inference_mode=False)` was the answer\r\n\r\n#15925 #15765",
      "author": {
        "login": "kaboroevich"
      },
      "createdAt": "2024-10-04T03:58:47Z"
    }
  },
  {
    "title": "Saving conformer checkpoint and resuming train progress from checkpoint",
    "body": "Confused about saving checkpoint of encoder-decoder models in lightning AI. Is the saving checkpoint technique same for all models?\r\n```py\r\n  checkpoint_callback = ModelCheckpoint(\r\n      monitor='val_loss',\r\n      dirpath=\"./saved_checkpoint/\",\r\n      filename='model-{epoch:02d}-{val_wer:.2f}',\r\n      save_top_k=3,        # 3 Checkpoints\r\n      mode='min'\r\n  )\r\n```\r\n\r\nBut how to load the checkpoint again to resume the training? Is the same as normal architecture models like giving `ckpt_path` in `trainer()` \r\n```py\r\n\r\n  ckpt_path = args.checkpoint_path if args.checkpoint_path else None\r\n  trainer.fit(speech_trainer, data_module, ckpt_path=ckpt_path)   \r\n  trainer.validate(speech_trainer, data_module)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20286",
    "createdAt": "2024-09-18T03:29:55Z",
    "updatedAt": "2024-09-19T15:48:02Z",
    "closedAt": "2024-09-19T15:47:58Z",
    "isAnswered": true,
    "author": {
      "login": "LuluW8071"
    },
    "answer": {
      "body": "Found the answer the code above is correct ",
      "author": {
        "login": "LuluW8071"
      },
      "createdAt": "2024-09-19T15:47:58Z"
    }
  },
  {
    "title": "Clarifying how `log` with `sync_dist` and `on_epoch` from the `training_step` works?",
    "body": "Hello, [the documentation about `log`](https://lightning.ai/docs/pytorch/stable/extensions/logging.html#logging-from-a-lightningmodule) doesn't seem to specify quite what happens with a couple combinations of settings. Notably, I'm interested in calculating a metric on each step (inside `training_step`), with the values reduced at the end of the epoch (using `on_step=False, on_epoch=True`). However, when running with DDP, I would like to also reduce across the DDP group. But, for performance, I only want this reduction to happen at the end of the epoch, not on each step. If I set `sync_dist` does the sync happen on each step before accumulating? Or does it happen during the reduction at the end of the epoch since this is when the logging occurs? If it happens on each step, is there a good builtin a way to have this sync happen on the values which were already accumulated? To note, using the `on_train_epoch_end` function will not work well in my standard case, as a single epoch would will contain billions of values if not accumulated during the training step and kept as separate values. Thank you for your time!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20123",
    "createdAt": "2024-07-24T04:00:00Z",
    "updatedAt": "2025-09-02T13:34:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "golmschenk"
    },
    "answer": {
      "body": " > The sync_dist, sync_dist_group and reduce_fx flags from self.log(...) don\u2019t affect the metric logging in any manner. The metric class contains its own distributed synchronization logic.\r\n \r\n Ref: https://lightning.ai/docs/torchmetrics/v1.8.1/pages/lightning.html\r\n\r\n",
      "author": {
        "login": "lokesh-vr-17773"
      },
      "createdAt": "2025-09-02T07:19:43Z"
    }
  },
  {
    "title": "deterministic torch.random.random_split",
    "body": "I am using \r\n\r\n```\r\ntorch.utils.data.random_split()\r\n```\r\nto create train, val,test split, however, I am testing the data on two different models which were trained pytroch lightning, but it is not splitting deterministically while I am setting seed_everything:42 \r\n\r\nHow can I address this? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19944",
    "createdAt": "2024-06-04T16:11:01Z",
    "updatedAt": "2024-06-14T12:33:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mshooter"
    },
    "answer": {
      "body": "What do you mean it does \"not splitting deterministically\"? This function is part of Pytorch and has nothing to do with Lightning.\r\n\r\nFrom the [Pytorch docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split):\r\n```python\r\ngenerator = torch.Generator().manual_seed(42)\r\nrandom_split(range(10), [3, 7], generator=generator)\r\n```",
      "author": {
        "login": "adosar"
      },
      "createdAt": "2024-06-08T19:57:04Z"
    }
  },
  {
    "title": "set state in LightningModule using LightningDataModule after setup",
    "body": "I would like to be able to set an attribute on my `LightningModule` model object after the trainer has called `setup(stage='fit')` on the datamodule, but before the sanity validation loop and training loops begin (the reason is this: the datamodule prepares a certain tokenizer in its `prepare_data` method, and then loads this tokenizer in its `setup` method, and my model needs access to the tokenizer). I searched through the model hooks and callbacks and it's not clear if there's a hook that gets called at some point between the datamodule setup and the start of the validation/fit loops. How would I go about doing this?   ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19735",
    "createdAt": "2024-04-03T22:21:57Z",
    "updatedAt": "2024-04-04T22:04:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "abefrandsen"
    },
    "answer": {
      "body": "Ok I found a way to do it. I have something like the following in the datamodule. This is functional but I'd be interested if there a better pattern I can employ here.\r\n```\r\nclass MyDM(L.LightningDataModule):\r\n    ...\r\n    def setup(self, stage):\r\n        self._load_tokenizer()\r\n        if hasattr(self, \"trainer\"): # here I communicate the tokenizer to the model\r\n            self.trainer.lightning_module.tokenizer= self.tokenizer\r\n```",
      "author": {
        "login": "abefrandsen"
      },
      "createdAt": "2024-04-04T22:04:38Z"
    }
  },
  {
    "title": "ValueError(f\"{func} does not have a return type annotation\") when implementing LightningCLI",
    "body": "Dear All,\r\n\r\nI am trying to implement the LightningCLI class, but it tells me my model does not have any Type annotation, although I added class function type return type annotation.\r\n\r\nWhat am I doing wrong?\r\n\r\nThank you for your input!\r\n\r\nLG\r\n\r\nMax",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19637",
    "createdAt": "2024-03-15T14:13:40Z",
    "updatedAt": "2024-03-17T16:15:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "djtschke"
    },
    "answer": {
      "body": "Hi Mauvilsa,\r\n\r\nthank you for your reply. While trying to come up with a minimal reproduction example I actually found the solution to my problem:\r\nLightningCLI() does not take the instance of the model as a parameter but the class itself (as I guess instantiation is then done by the pipeline itself). To be more clear:\r\n\r\nWrong / Not Working:\r\n```\r\nfrom lightning.pytorch.cli import LightningCLI\r\nfrom MyFancyModel import MyFancyModelClass\r\nfrom MyFancyDataModule import MyFancyDataModuleClass\r\n\r\nmodel = MyFancyModelClass()\r\ndatamodule = MyFancyDataModuleClass()\r\n\r\nLightningCLI = LightningCLI(model, datamodule)\r\n```\r\n\r\n**Correct / Working**:\r\n```\r\nfrom lightning.pytorch.cli import LightningCLI\r\nfrom MyFancyModel import MyFancyModelClass\r\nfrom MyFancyDataModule import MyFancyDataModuleClass\r\n\r\nLightningCLI = LightningCLI(MyFancyModelClass, MyFancyDataModuleClass)\r\n```\r\n\r\nHope that also helps others!\r\n\r\nCheers,\r\n\r\nMax",
      "author": {
        "login": "djtschke"
      },
      "createdAt": "2024-03-17T16:14:58Z"
    }
  },
  {
    "title": "confusions about load_from_checkpoint() and save_hyperparameters()",
    "body": "according to \r\n\r\n> https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html,\r\n\r\nThere is a model like this:\r\n\r\n```\r\nclass Encoder(L.LightningModule):\r\n    ...\r\n\r\nclass Decoder(L.LightningModule):\r\n    ...\r\n\r\nclass Autoencoder(L.LightningModule):\r\n    def __init__(self, encoder, decoder, *args, **kwargs):\r\n        self.save_hyperparameters(ignore=['encoder', 'decoder'])\r\n        self.encoder=encoder\r\n        self.encoder.freeze()\r\n        self.decoder=decoder\r\n        ...\r\n\r\n# training code\r\nencoder = Encoder.load_from_checkpoint(\"encoder.ckpt\")\r\ndecoder = Decoder(some hyperparameters)\r\nautoencoder = Autoencoder(encoder, decoder)\r\ntrainer.fit(autoencoder, datamodule)\r\n\r\n```\r\nWe assume that the autoencoder has been stored in the `autoencoder.ckpt` file. There are three key points I am curious about:\r\n\r\n1. Does the `autoencoder.ckpt` file include both the `encoder` and `decoder` weights?\r\n2. If `autoencoder.ckpt` contains the `encoder` weights, how can I import the weights from `encoder.ckpt` into the `autoencoder` without them being overwritten?\r\n3. If `autoencoder.ckpt` does not include the `decoder` weights, what is the procedure to save the `decoder` weights separately?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19312",
    "createdAt": "2024-01-18T16:40:55Z",
    "updatedAt": "2024-01-28T04:19:39Z",
    "closedAt": "2024-01-28T03:50:19Z",
    "isAnswered": true,
    "author": {
      "login": "KasuganoLove"
    },
    "answer": {
      "body": "I got my answer from\r\n\r\n> https://lightning.ai/forums/t/confusions-about-load-from-checkpoint-and-save-hyperparameters/4881/2",
      "author": {
        "login": "KasuganoLove"
      },
      "createdAt": "2024-01-28T03:50:19Z"
    }
  },
  {
    "title": "How to load the weight weight from checkpoint while we didn't self.save_hyparams(.) ?",
    "body": "As the title, i wonder what I should do to only load the model weight as the starting point of finetuneing.\r\nFirstly, I can not call self.hyparams(.) to save the model arguments, or it will stuck train.fit procedure **without any error message**.  So, I need to first initialize the random weight modelA and modelB, and hope the checkpoint will automatically load the pretrained weight in it when I call `Dummy_framework.load_from_checkpoint(.)`.\r\n**Otherwise, I will get the error about `xxx.__init__() missing xxx required positional arguments: modelA, modelB, ...`**\r\n\r\nSecondly, I try to trust the automatic saving mechanism (without any callback, any self.hyparams, torch.save something), and   I can find the checkpoint under the folder. So, I don't know what the checkpoint contains, but I assume it contains everything including model weights.\r\n\r\nwhen I execute the following code, it doesn't give any error and run smoothly, but I afraid the pre-trained weight doesn't be loaded in modelA and modelB.\r\n```\r\ndummy_modelA, dummy_modelB = get_rand_init_weight_models(cfger.model_params)\r\ntmp = Dummy_framework.load_from_checkpoint(\r\n    cfger.test_params['PATH'], modelA=dummy_modelA, modelB=dummy_modelB,\r\n    **cfger.train_params  # dummy args\r\n)\r\ntrainer.fit(tmp, dataloaders=tra_ld)\r\n```\r\n\r\nSince the torch-lightning document described : \r\n```\r\n# if you train and save the model like this it will use these values when loading\r\n# the weights. But you can overwrite this\r\nLitModel(in_dim=32, out_dim=10)\r\n\r\n# uses in_dim=32, out_dim=10\r\nmodel = LitModel.load_from_checkpoint(PATH)\r\n\r\n# uses in_dim=128, out_dim=10\r\nmodel = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\r\n```\r\nSo, will the pretrained model be overrides by **the random init modelA, modelB weights** ?\r\nAs I did this : \r\n```\r\nDummy_framework.load_from_checkpoint(cfger.test_params['PATH'], \r\n# I feed the dummy model into it, to prevent the error message I described in First point.\r\nmodelA=dummy_modelA, modelB=dummy_modelB\r\n)\r\n```\r\n\r\nBesides, if I want to conduct finetuneing, how can I **only load** the pretrained weight (modelA, modelB) and discard the other info, including lr states, epochs states, ... ? \r\n\r\nAny suggestion will be appreciated!!\r\n\r\n( PS. the PL document in Checkpoint Loading seems not helpful in this case\r\n\r\n\r\n ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19290",
    "createdAt": "2024-01-16T07:38:22Z",
    "updatedAt": "2024-02-02T08:34:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "HuangChiEn"
    },
    "answer": {
      "body": "> So, will the pretrained model be overrides by the random init modelA, modelB weights ?\r\n\r\nI think no.\r\nThe lightning module `load_from_checkpoint` method is like this:\r\n```\r\n@_restricted_classmethod\r\ndef load_from_checkpoint(\r\n    cls,\r\n    checkpoint_path: Union[_PATH, IO],\r\n    map_location: _MAP_LOCATION_TYPE = None,\r\n    hparams_file: Optional[_PATH] = None,\r\n    strict: bool = True,\r\n    **kwargs: Any,\r\n) -> Self:\r\n```\r\nand the discription of `strict` is: ```strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict.```\r\nSince the your lightning module `ckpt` contains both two models (you can print your ckpt to check this), and default `strict` is `True`, the `load_from_checkpoint` method will strictly load ```every keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict.```",
      "author": {
        "login": "KasuganoLove"
      },
      "createdAt": "2024-01-28T04:11:41Z"
    }
  },
  {
    "title": "How to exclude model `__init__` parameters from the lightning CLI?",
    "body": "I have a model that looks something like this:\r\n```python\r\nclass MyModel(L.LightningModule):\r\n    def __init__(\r\n        self,\r\n        data_mean: float = 0,\r\n        data_std: float = 1,\r\n        learning_rate: float = 1e-3,\r\n    ) -> None:\r\n        super().__init__()\r\n        self.data_mean = data_mean\r\n        self.data_std = data_std\r\n        self.learning_rate = learning_rate\r\n        ...\r\n\r\n    def training_step(self, batch):\r\n        normalized_output = self(batch)\r\n        denormalized_output = normalized_output * self.data_std + self.data_mean\r\n        ...\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n\r\n    ...\r\n```\r\nHere, `learning_rate` is an actual hyperparameter that one might want to set via a CLI argument. But `data_mean` and `data_std` are supposed to be calculated from the training data. These parameters are used to obtain a model that outputs values in real units, while the trainable parameters of the model are optimized to predict outputs with zero mean and unit standard deviation.\r\n\r\nMy main script looks something like this:\r\n```python\r\ndef main() -> None:\r\n    cli = LightningCLI(MyModel, MyDataModule, run=False)\r\n\r\n    data = MyDataModule(\"data\")\r\n    data.setup()\r\n    model = MyModel(\r\n        data_mean=data.train_mean.item(),\r\n        data_std=data.train_std.item(),\r\n    )\r\n    trainer = L.Trainer()\r\n    trainer.fit(model, data, ...)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nHere, the idea would be to access the CLI arguments through the `cli` object and use them to implement my custom logic. But of course, the `data_mean` and `data_std` parameters end up in the config file and CLI help, where they don't belong.\r\n\r\nIs there a way to exclude such parameters from the inferred CLI options or am I using `LightningCLI` wrong?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19245",
    "createdAt": "2024-01-08T14:43:57Z",
    "updatedAt": "2024-01-09T14:15:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "NiklasKappel"
    },
    "answer": {
      "body": "A parameter that is in `__init__` is something that should be specified by the user and not computed. If you compute something based on init params, then internally give it a different name since it is not the same as what gets provided.\r\n\r\nWhat you need is argument linking. Have a look at https://github.com/Lightning-AI/pytorch-lightning/discussions/13403#discussioncomment-3174904, also [cli argument linking](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_expert.html#argument-linking) and [jsonargparse argument linking](https://jsonargparse.readthedocs.io/en/v4.27.1/#argument-linking). You could link the entire data module. Though, it might be cleaner to link a `@property` that provides whatever needs computing.\r\n\r\nNote. When an argument is linked, it doesn't show up in the config. Since the value is derived instead of provided.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2024-01-09T13:56:43Z"
    }
  },
  {
    "title": "No Progress bar showing in Slurm output file",
    "body": "The title explains exactly the problem I have. \r\nI would like to see the TQDM progress bar updates in the slurm output log. \r\nIt works on my computer and I get the progress bar in my terminal. \r\n\r\nHowever, when I run a sbatch script of the model I do not see the progress bar updates in the slurm output file or somewhere else in the logs directory.  I tried both the tqdm and rich progress bar but with both I do not see any output.\r\n\r\nIs this the intended behaviour for running in a slurm system? \r\nIf so it there a way to get tqdm to output to the slurm output file or maybe a way to redirect it to another file.\r\n\r\nI made sure I am using the latest Ligthning and tqdm version.  I also search everywhere on the lightning docs and github for a similar question but i could not find any. \r\n\r\nAny help or solutions are appreciated. \r\nThank you.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19227",
    "createdAt": "2024-01-01T18:24:18Z",
    "updatedAt": "2024-07-24T04:08:37Z",
    "closedAt": "2024-01-02T11:42:18Z",
    "isAnswered": true,
    "author": {
      "login": "Cing2"
    },
    "answer": {
      "body": "Okay, I found my problem. It was a mistake from my part, I am sorry for the issue.\r\nI was using the pytorch ligthning + hydra template to construct my code and I used hydra to change the progress bar which was default set to rich to tqdm, however, I did this wrong and thus I was still using the rich progress bar on the server. This one does not provide intermediate step updates. \r\n",
      "author": {
        "login": "Cing2"
      },
      "createdAt": "2024-01-02T11:42:18Z"
    }
  },
  {
    "title": "how to use `find_usable_cuda_devices` in lightning cli config.yaml?",
    "body": "for example\r\n```\r\ntrainer:\r\n  accelerator: gpu\r\n  devices: [0]\r\n```\r\nI want to use find_usable_cuda_devices(1) instead of choosing one avaliable gpu manually",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18622",
    "createdAt": "2023-09-24T06:51:38Z",
    "updatedAt": "2023-09-26T14:24:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Jackiexiao"
    },
    "answer": {
      "body": "Unfortunately that is not supported. Though, you could do the following workaround. First subclass the trainer:\r\n\r\n```python\r\nimport re\r\nfrom lightning.fabric.accelerators import find_usable_cuda_devices\r\n\r\nclass CustomTrainer(Trainer):\r\n    def __init__(self, *args, **kwargs):\r\n        devices = kwargs.get(\"devices\")\r\n        if (\r\n            isinstance(devices, str)\r\n            and re.match(\"^find_usable_cuda_devices\\([0-9]*\\)$\", devices)\r\n        ):\r\n            kwargs[\"devices\"] = eval(devices)\r\n        super().__init__(*args, **kwargs)\r\n```\r\n\r\nThen provide `trainer_class=CustomTrainer` when instantiating the `LightningCLI` class. In the config.yaml then it could be written as `devices: find_usable_cuda_devices(1)`.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-09-26T05:08:04Z"
    }
  },
  {
    "title": "LightningCLI: callbacks based on model",
    "body": "I'm trying to use `LightningCLI` to replace TorchGeo's [train.py](https://github.com/microsoft/torchgeo/blob/b30a8b939325be2a73ffbd61332e506966700393/train.py) script. I'm absolutely loving it so far, but have encountered one thing I'm not sure how to translate. The performance metric we monitor in our ModelCheckpoint/EarlyStopping callbacks depends on which model we select (e.g., classification: 'val_loss', object detection: 'val_map', SSL: 'train_loss'). What would be the most idiomatic way to handle this?\r\n\r\nMy first thought was to override `LightningCLI.instantiate_trainer` so that we can add callbacks based on parsed model config. Is there a cleaner way to do this? Ideally it would be possible for the user to override this but it's not a requirement.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18480",
    "createdAt": "2023-09-05T00:50:36Z",
    "updatedAt": "2023-09-05T13:56:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "adamjstewart"
    },
    "answer": {
      "body": "Maybe you can config callbacks for model directly through [`LightningModule.configure_callbacks`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_callbacks)",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2023-09-05T04:00:46Z"
    }
  },
  {
    "title": "fabric and scheduler",
    "body": "How do I use fabric with a scheduler. The fabric API doesn't seem to have a `fabric.setup_scheduler()` function.\r\nThanks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18359",
    "createdAt": "2023-08-21T13:52:53Z",
    "updatedAt": "2023-12-13T14:56:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pfeatherstone"
    },
    "answer": {
      "body": "@pfeatherstone There is no special need to setup the scheduler. You can just use it side by side with your optimizer and step it normally. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2023-08-28T22:33:04Z"
    }
  },
  {
    "title": "load checkpoint resume from CLI, different learning rate",
    "body": "Hi how do I load the weights of the lightning module? but change learning rate using CLI ? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18339",
    "createdAt": "2023-08-18T07:28:53Z",
    "updatedAt": "2023-08-23T09:10:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mshooter"
    },
    "answer": {
      "body": "It depends on what you implement. What `LightningCLI` does is expose the parameters in `__init__` as configurable from command line and config files. By `self.learning_rate` do you mean that you added an init parameter for the learning rate?\r\n\r\nNote that the CLIs have a help that explains how to use it, i.e. `python cli.py --help`. Also, look at the CLI documentation. The use of the automatic `configure_optimizers`, both for optimizers and schedulers is explained [here](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_intermediate_2.html#multiple-optimizers). Though, if you have defined a learning rate parameter then it seems you are not doing that. A more advanced way of making optimizers and schedulers configurable (via dependency injection) is explained in [multiple-optimizers-and-schedulers](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_3.html#multiple-optimizers-and-schedulers).",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-08-18T19:26:14Z"
    }
  },
  {
    "title": "Why does Lightning AI Fabric gives warning to training with precision = \"bf16-mixed\" instead of \"bf16\"",
    "body": "It gives the following warning: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead! I have seen many articles training the model in pure bf16 and getting similar accuracy.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18263",
    "createdAt": "2023-08-08T20:20:06Z",
    "updatedAt": "2024-01-11T12:44:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "basujindal"
    },
    "answer": {
      "body": "Hi, previously, bf16 was a shorthand notation for bf16-mixed. To clearly distinguish between mixed precision and pure bf16 we introduced the notion of `bf16-mixed` and `bf16-true` while just `bf16` still is a deprecated alias for `bf16-mixed`",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2023-08-11T14:15:16Z"
    }
  },
  {
    "title": "VSCode and jsonargparse error",
    "body": "Hi everyone, \r\n\r\nI recently switchted to VSCode from PyCharm and I am facing currently an issue. If I try to debug my `main.py` file, which uses the `LightningCLI` (`DiffLightningCLi` inherits from it and just overwrites `add_arguments_to_parser`):\r\n\r\n```python\r\nimport sys\r\nimport os\r\n\r\nfrom adiff.utils import train_util\r\nimport adiff.model.lightning.base_model\r\n\r\n\r\ndef cli_main():\r\n    # Start Training/Testing based on the args\r\n\r\n      data_dir = f'/home/{os.getlogin()}/Data/datasets'\r\n      logging_dir = f'/home/{os.getlogin()}/Data/lightning_logs'\r\n      default_cfg = [\"config/debug_cfg.yaml\"]\r\n      cmdline_args = [\r\n          \"fit\", \r\n              \"--model\", \"DiffGen\",\r\n              '--task', 'GEN',\r\n              '--data_dir', f'{data_dir}', \r\n              '--logging_dir', f'{logging_dir}',\r\n              '--data.dimensions', '2',\r\n              '--num_workers', '0',\r\n              '--data.batch_size=5',\r\n              '--data.dataset=cifar-10',\r\n              '--data.in_channels=3',\r\n              '--model.out_channels=3',\r\n              '--trainer.logger=False',\r\n              '--data.num_train_subj=10',\r\n              '--data.num_val_subj=10',\r\n              '--model.diffusion_mean_type=EPSILON',\r\n              '--model.diffusion_loss_type=SIMPLE',\r\n              '--model.diffusion_var_type=FIXED_LARGE',\r\n              '--model.ema_decay=0.9999',\r\n              '--trainer.max_steps=700000',\r\n              ]\r\n    subcommands = ('fit', 'test', 'validate', 'predict')\r\n    parser_kwargs = {subcommand: {'default_config_files': default_cfg} for subcommand in subcommands}\r\n\r\n    cli = train_util.DiffLightningCLI(\r\n            model_class=adiff.model.lightning.base_model.BaseModel,\r\n            subclass_mode_model=True,\r\n            run=run,\r\n            seed_everything_default=123,\r\n            parser_kwargs=parser_kwargs,\r\n            save_config_kwargs={\"overwrite\": True,\r\n                                \"multifile\": True},\r\n            args=cmdline_args)\r\n\r\n    # Shutdown\r\n    sys.exit()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import warnings\r\n    from lightning_fabric.utilities.warnings import PossibleUserWarning\r\n\r\n    with warnings.catch_warnings():\r\n        warnings.filterwarnings(\r\n            action='ignore',\r\n            category=PossibleUserWarning,\r\n            message='The dataloader,')\r\n        cli_main()\r\n\r\n```\r\n**I am aware that passing in the args like this is not recommended and is also only used in debugging and not in the final `main.py`**. Running/debugging the script using the `launch.json` configuration of VSCode \r\n```json\r\n{\r\n    \"version\": \"0.2.0\",\r\n    \"configurations\": [\r\n        {\r\n            \"name\": \"main.py\",\r\n            \"type\": \"python\",\r\n            \"request\": \"launch\",\r\n            \"program\": \"${workspaceFolder}/ADiff/adiff/main.py\",\r\n            \"console\": \"integratedTerminal\",\r\n            \"justMyCode\": false,\r\n            \"redirectOutput\": false,\r\n        }\r\n    ]\r\n}\r\n```\r\nI get the following error message:\r\n\r\n```\r\nmain.py [options] fit: error: Parser key \"data\":\r\n  Not a valid subclass of LightningDataModule. Got value: NestedArg(key='dimensions', val='2')\r\n  Subclass types expect one of:\r\n  - a class path (str)\r\n  - a dict with class_path entry\r\n  - a dict without class_path but with init_args entry (class path given previously)\r\n ```\r\n\r\nThe command under the hood in VSCode uses `cd /home/u24417317/Code ; /usr/bin/env /home/u24417317/.conda/envs/phd39/bin/python /home/u24417317/.vscode-server/extensions/ms-python.python-2023.12.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 60221 -- /home/u24417317/Code/ADiff/adiff/main.py` so the correct file is executed.\r\n\r\nHowever, if I run the exact same file using python directly without VSCode, the code runs as intended (as it was running on PyCharm).\r\n```\r\n(phd39) u24417317@LabServer:~/Code/ADiff/adiff$ python main.py\r\nGlobal seed set to 123\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\n```\r\nI am debugging remotely using ssh (I am running the same file with and without VSCode), made sure that the exact same conda environment is utilised with the following packages:\r\n\r\njsonargparse: 4.22.1\r\nlightning-utilities: 0.8.0\r\npytorch-lightning: 2.0.4\r\n\r\nIt seems to me like a VSCode issue, hence why I am aware that this might not be the best place to ask that question but maybe someone has encountered a similar problem or could share their working `launch.json` configuration.\r\n\r\nI appreciate any input!\r\nNico\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/18025",
    "createdAt": "2023-07-08T06:11:44Z",
    "updatedAt": "2023-07-09T22:08:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nicoloesch"
    },
    "answer": {
      "body": "@mauvilsa I figured it out. It turned out that my default configuration file was not found due to an error in the `launch.json` file of VSCode. I provided the configuration as a relative path to the `main.py` file. However, the `cwd` was set to the `${workspaceFolder}`, which is two folders above it (as I want to have all my different projects in the same workspace). This lead to not finding the respective default configuration file in `_get_default_config_files` of `jsonargparse/core.py`. \r\nYour suggestion of having a more distinct error message could be helpful in the future but is included in the current error messages if one has more experience with the `jsonargparse` module.\r\n\r\nThank you for your help! \r\nNico\r\n\r\n-------------\r\nSolution: Setting\r\n`\"cwd\": \"${workspaceFolder}/<path_to_main_file>\"` in `launch.json`",
      "author": {
        "login": "nicoloesch"
      },
      "createdAt": "2023-07-09T22:08:08Z"
    }
  },
  {
    "title": "How can I use a weighted loss with LightningCLI?",
    "body": "I would like to use a weighted loss function in a LightningModule model. The weights can be computed after initializing the train dataset of the LightningDataModule, ergo I only have access to the weights after DataModule.setup(). However, LightningCLI only calls the respective init methods. I would need to setup the DataModule before initializing the model. So, what is the best solution to achieve this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17953",
    "createdAt": "2023-06-30T12:24:25Z",
    "updatedAt": "2023-07-01T15:14:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "maxfreu"
    },
    "answer": {
      "body": "You can use an argument link applied on instantiation, see [argument-linking](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_expert.html#argument-linking). This could be done in several ways, depending on when/where you want `setup()` to be called. One possibility could be that the `LightningModule` gets the instance of the `LightningDataModule`. This is something like:\r\n\r\n```python\r\nclass MyModule(LightningModule):\r\n    def __init__(self, data_module: LightningDataModule):\r\n        # data setup could be called here or later\r\n        ...\r\n\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments(\"data\", \"model.data_module\", apply_on=\"instantiate\")\r\n\r\ncli = MyLightningCLI(MyModule, MyDataModule)\r\n```\r\n\r\nOther possibilities are:\r\n- The `LightningDataModule` implements a `@property` that automatically triggers setup and gives the data needed for the weighted loss. The `LightningModule.__init__` has a parameter to receive this data. Then you create an instantiation link from the data property to the module parameter.\r\n- The `LightningModule.__init__` has a parameter to get the data needed for the weighted loss. Then create an instantiation link with source the `LightningDataModule` instance and a [`compute_fn`](https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentLinking.link_arguments) function that takes care of `setup()` and returns the required data.\r\n\r\nThe link target would need `.init_args.` if the `CLI` uses subclass mode for the `LightningModule`, e.g., `parser.link_arguments(\"data\", \"model.init_args.data_module\", apply_on=\"instantiate\")`.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-06-30T21:53:46Z"
    }
  },
  {
    "title": "LightningCLI: Set interval for lr_scheduler in config.yaml",
    "body": "Hi everyone,\r\n\r\nI recently changed over to `LightningCLI` from the regular `argparser`. In order to change `lr_scheduler` and `optimizer` from the command-line, I added the respective keys to the `config.yaml`\r\n```yaml\r\noptimizer:\r\n  class_path: torch.optim.Adam\r\n  init_args:\r\n    lr: 1e-5\r\n    weight_decay: 0.0\r\nlr_scheduler:\r\n  class_path: adiff.utils.callbacks.WarmUpScheduler\r\n  init_args:\r\n    warmup: 20\r\n```\r\n\r\nHowever, `.step()` of the `lr_scheduler` is only called every epoch. **I want it to be called every step**. \r\n\r\nIn `configure_optimizers` of a `LightningModule`, one can set the `interval` of the scheduler. Is this also possible through the `CLI` interface/ in the `yaml`-config and if so, how?  I tried multiple different versions but none of them was parse-able.\r\n\r\nThank you for any help!\r\nNico",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17868",
    "createdAt": "2023-06-19T02:46:38Z",
    "updatedAt": "2023-06-19T06:15:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nicoloesch"
    },
    "answer": {
      "body": "See https://github.com/Lightning-AI/lightning/issues/15340#issuecomment-1434560283",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-06-19T05:50:39Z"
    }
  },
  {
    "title": "Anyone know why the Fault-Tolerant Training document was removed from the latest pytorch-lighting document?",
    "body": "Anyone know why the Fault-Tolerant Training document was removed from the latest pytorch-lighting document? It seems to have existed at least as of version 1.7.7.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17811",
    "createdAt": "2023-06-12T05:40:08Z",
    "updatedAt": "2023-06-12T07:21:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "Hey, this feature was removes with PL 2.0.0 as it was having to high impact on complexity and thereby training speed. You can still manually do this by saving the random seed of your dataloaders/datasets and restoring them upon loading. ",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2023-06-12T06:46:36Z"
    }
  },
  {
    "title": "Hook for processing input data in LightningCLI",
    "body": "Hi, I have a model that accepts a couple of files as input and I want to run prediction on it. Since I know the file structure, I'm not passing the paths directly via Argparse for now, I just provide directory and get all required paths programatically. I want to integrate my current approach with LightningCLI, but I don't see a place to somehow parse/process input arguments. \r\n\r\nLightningCLI implementation:\r\n```\r\nfrom pytorch_lightning.cli import LightningCLI, LightningArgumentParser\r\n\r\nclass FeatureExtractorCLI(LightningCLI):\r\n\r\n    def add_arguments_to_parser(self, parser: LightningArgumentParser):\r\n        parser.add_argument(\"--input_dir\", help=\"Input directory\", required=True)\r\n\r\n        # TODO transform input_dir into 3 new file parameters that will be used in data module\r\n\r\n    def collect_files_from_input_dir(input_dir):\r\n        file1 = None\r\n        file2 = None\r\n        file3 = None\r\n        # some code scanning input_dir to get particular file paths...\r\n        return file1, file2, file3\r\n```\r\n\r\nData module that operates on separate files:\r\n```\r\nclass DataModule(pl.LightningDataModule):\r\n\r\n    def __init__(self, file1, file2, file3):\r\n        super().__init__()\r\n        self._file1 = file1\r\n        self._file2 = file2\r\n        self._file3 = file3\r\n```\r\n\r\nIs there a way to combine these two approaches or I should not mixed them? I tried overriding \"parse_arguments\" in LightningCLI, but unfortunately \"args\" are null at the time. Is there a better place to hook?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17594",
    "createdAt": "2023-05-08T14:50:31Z",
    "updatedAt": "2023-05-14T21:01:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "machur"
    },
    "answer": {
      "body": "My recommendation would be that you think of `LightningCLI` as a class that automatically exposes signature parameters in a CLI. However, the logic to run in most cases is not implemented as part of the CLI class. The parameters come from other classes that you can even use without the CLI.\r\n\r\nIf you want to load data from a directory, then implement a data module that receives as parameter that directory. This could be a subclass of your module that gets files. Something like:\r\n\r\n```python\r\nclass FilesData(pl.LightningDataModule):\r\n    def __init__(self, file1, file2, file3):\r\n        super().__init__()\r\n        self._file1 = file1\r\n        self._file2 = file2\r\n        self._file3 = file3\r\n\r\nclass DirectoryData(FilesData):\r\n    def __init__(self, input_dir: os.PathLike):\r\n        file1, file2, file3 = self.get_files_from_dir(input_dir)\r\n        super().__init__(file1, file2, file3)\r\n```\r\n\r\nThen to `LightningCLI` you can provide `datamodule_class=FilesData` and `subclass_mode_data=True` so that both `FilesData` and `DirectoryData` can be used from the command line.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-05-11T19:46:25Z"
    }
  },
  {
    "title": "Multiple Sequential trainings slows down speed",
    "body": "Hi.\r\nI have a task where I need to run a training script multiple time with a for-loop like this:\r\n```\r\nfor dataset in datasets:\r\n      ... Training script with datamodule, model, Trainer Init and trainer.fit()\r\n```\r\nHowever, after each loop the the training it self slows down incrementally (epoch / sec). I am thinking it is because I need to reset something but I have not been able to find that information. I am using Torch-cpu 2.0.0\r\n\r\nAny idea on what I am doing wrong? :(",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17490",
    "createdAt": "2023-04-27T07:43:47Z",
    "updatedAt": "2023-04-27T09:05:42Z",
    "closedAt": "2023-04-27T09:05:36Z",
    "isAnswered": true,
    "author": {
      "login": "HadiSDev"
    },
    "answer": {
      "body": "I tried to remove the neptune logger and model saving functionality and it seemed to have solved the issue",
      "author": {
        "login": "HadiSDev"
      },
      "createdAt": "2023-04-27T09:05:36Z"
    }
  },
  {
    "title": "Possible to change starting epoch?",
    "body": "Hey everyone!\r\n\r\nSo a project I've been helping out a bit on, https://github.com/34j/so-vits-svc-fork , is manually loading model checkpoints upon training start / initialization.\r\n\r\nCurrently, it uses a bit of a hacky way to set the current epoch to continue from:\r\nhttps://github.com/34j/so-vits-svc-fork/blob/main/src/so_vits_svc_fork/train.py#L170\r\n```py\r\ndef on_train_start(self) -> None:\r\n    if not self.tuning:\r\n        self.set_current_epoch(self._temp_epoch) # This is being loaded from the model\r\n        total_batch_idx = self.current_epoch * len(self.trainer.train_dataloader)\r\n        self.set_total_batch_idx(total_batch_idx)\r\n        global_step = total_batch_idx * self.optimizers_count\r\n        self.set_global_step(global_step)\r\n\r\ndef set_current_epoch(self, epoch: int):\r\n    LOG.info(f\"Setting current epoch to {epoch}\")\r\n    self.trainer.fit_loop.epoch_progress.current.completed = epoch\r\n    assert self.current_epoch == epoch, f\"{self.current_epoch} != {epoch}\"\r\n\r\ndef set_global_step(self, global_step: int):\r\n    LOG.info(f\"Setting global step to {global_step}\")\r\n    self.trainer.fit_loop.epoch_loop.manual_optimization.optim_step_progress.total.completed = (\r\n        global_step\r\n    )\r\n    self.trainer.fit_loop.epoch_loop.automatic_optimization.optim_progress.optimizer.step.total.completed = (\r\n        global_step\r\n    )\r\n    assert self.global_step == global_step, f\"{self.global_step} != {global_step}\"\r\n\r\ndef set_total_batch_idx(self, total_batch_idx: int):\r\n    LOG.info(f\"Setting total batch idx to {total_batch_idx}\")\r\n    self.trainer.fit_loop.epoch_loop.batch_progress.total.ready = (\r\n        total_batch_idx + 1\r\n    )\r\n    self.trainer.fit_loop.epoch_loop.batch_progress.total.completed = (\r\n        total_batch_idx\r\n    )\r\n    assert (\r\n        self.total_batch_idx == total_batch_idx + 1\r\n    ), f\"{self.total_batch_idx} != {total_batch_idx + 1}\"\r\n\r\n@property\r\ndef total_batch_idx(self) -> int:\r\n    return self.trainer.fit_loop.epoch_loop.total_batch_idx + 1\r\n```\r\n\r\nHowever, this breaks support for `max_epochs`, as I feel it's not overriding all necessary variables internally.\r\n\r\nIs there a way to properly override the current epoch it \"starts\" (or in this case, continues) training from?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17396",
    "createdAt": "2023-04-16T17:03:22Z",
    "updatedAt": "2023-06-27T10:59:09Z",
    "closedAt": "2023-04-19T11:39:53Z",
    "isAnswered": true,
    "author": {
      "login": "Lordmau5"
    },
    "answer": {
      "body": "After looking at the Lightning code, I managed to fix it by also setting `self.trainer.fit_loop.epoch_progress.current.processed` to a specific epoch \ud83d\ude01 ",
      "author": {
        "login": "Lordmau5"
      },
      "createdAt": "2023-04-19T11:39:53Z"
    }
  },
  {
    "title": "How to do fit and test at the same time with Lightning CLI ?",
    "body": "Hi, I am wondering how to `fit` and `test` my model with a single CLI?\r\nCurrently, i am just using:\r\n```\r\npython main.py fit ...\r\npython main.py test ckpt_path\r\n```\r\nThis is not very convenient for me since and the log dir is unknown before `fit`(v_num),so i cannot wrap the two commands above into a script. And i have to find the best ckpt_path myself. Is there any better workaround here ?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17300",
    "createdAt": "2023-04-07T06:52:39Z",
    "updatedAt": "2023-04-07T07:07:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Hannibal046"
    },
    "answer": {
      "body": "Instead of having a CLI with subcommands, you can use the [instantiation only mode](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_3.html#instantiation-only-mode) and call test right after fit.\r\n\r\nHowever, a fair warning. The test set should be used as few times as possible. Measuring performance on the test set too often is a bad practice because you end up optimizing on the test. So, technically it is better to use the test subcommand giving explicitly a checkpoint (only one among many you may have) and not plan to run the test for every fit you do.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-04-07T07:04:08Z"
    }
  },
  {
    "title": "Why is it necessary to specify subcommands in the yaml config for the model in CLI mode?",
    "body": "I am currently upgrading from `1.9.4` to `2.x` of `pytorch_lightning`, which includes the removal of the legacy `argparse` (e.g. `Trainer.from_argparse_args`). As such I have set up the following test envrionment mimicking my project structure, however with simple modules. \r\n\r\n### Test Project Structure\r\nIn my bigger project, I have an (partial) abstract base class inheriting from `LightningModule`, which implements general functionality required for both child modules (hence why I specify here `ChildModuleA` and `ChildModuleB`).\r\n\r\n```python\r\nfrom pytorch_lightning.demos.boring_classes import BoringModel\r\n\r\nclass BaseModel(BoringModel):\r\n    def __init__(self,\r\n                 base: int):\r\n        super().__init__()\r\n        self._base = base\r\n\r\nclass ChildModelA(BaseModel):\r\n    def __init__(self,\r\n                 base: int,\r\n                 child: Optional[int] = None):\r\n        super().__init__(base)\r\n        if child is None:\r\n            child = \"None\"\r\n        self._child = child\r\n        print(f\"A,base| {self._base}\")\r\n        print(f\"A,child| {self._child}\")\r\n\r\nclass ChildModelB(BaseModel):\r\n    def __init__(self,\r\n                 base: int,\r\n                 child: Optional[int] = None):\r\n        super().__init__(base)\r\n        if child is None:\r\n            child = \"Null\"\r\n        self._child = child\r\n        print(f\"B,base| {self._base}\")\r\n        print(f\"B,child| {self._child}\")\r\n```\r\nand the instantiation of the CLI as follows:\r\n\r\n```python\r\n    cli = LightningCLI(model_class=test_lightning.BaseModel,\r\n                       datamodule_class=BoringDataModule,\r\n                       subclass_mode_model=True,\r\n                       parser_kwargs={'default_config_files': [\"test_cfg.yaml\"]})\r\n```\r\nThe `test_cfg.yaml` follows the documentation and is defined as follows:\r\n\r\n```yaml\r\nmodel:\r\n  class_path: test_lightning.base_model.BaseModel\r\n  init_args:\r\n    base: 2\r\n    child: 5\r\n```\r\n\r\n### Problem\r\nHowever, this configuration works **ONLY** when `run=False` within the `CLI` (and omitting the subcommand for execution). As soon as I try to run the file with `python main.py fit --model ChildModelA` I receive the following error message:\r\n\r\n`argparse.ArgumentError: Problem in default config file \"test_cfg.yaml\": \"subcommand\" is required but not given or its value is None.`\r\n\r\n### Question\r\nMy question now is: **Why do I need to specify the subcommands within the `.yaml` file resulting in an exact duplicate for both `fit` (and all other subcommands)?** \r\n\r\nWith the following `test_cfg.yaml`, I can run the command (but the duplication is quite obvious, which becomes even more of a problem with my models and the larger amount of hparams...):\r\n```yaml\r\nfit:\r\n  model:\r\n    class_path: test_lightning.base_model.BaseModel\r\n    init_args:\r\n      base: 1\r\n      child: 2\r\ntest:\r\n  model:\r\n    class_path: test_lightning.base_model.BaseModel\r\n    init_args:\r\n      base: 1\r\n      child: 2\r\n```\r\n\r\nWouldn't it suffice to have a single configuration that applies to all subcommands and only have the subcommands **IF** an individual behaviour is wanted?\r\n\r\nI am clearly doing something wrong here but I was not able to distinguish which behaviour is required based on the [Documentation](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli.html).\r\n\r\nI appreciate any help or a hint to the right direction!\r\n\r\nCheers,\r\nNico\r\n \r\n \r\n### Envrionment\r\n\r\n- `pytorch_lighting: 2.0.0` (installed with [extra])\r\n- `python: 3.9`\r\n- `torch: 1.13.1` (intention to upgrade to 2.x)\r\n- `jsonargparse: 4.20.0`\r\n- `lightning-utilities: 0.8.0`\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17209",
    "createdAt": "2023-03-27T21:21:44Z",
    "updatedAt": "2023-04-03T04:54:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nicoloesch"
    },
    "answer": {
      "body": "There are many possibilities to provide settings to a CLI, and having one yaml where you can specify all subcommand is just one of them. With `run=True` you can give to the CLI a single config that does not require a subcommand as:\r\n\r\n```shell\r\npython main.py fit --config=test_cfg.yaml\r\n```\r\n\r\nNote that in the command, the config is given **after** `fit`. The subcommand is already known, so the config is expected to not specify it. If the command is\r\n\r\n\r\n```shell\r\npython main.py --config=test_cfg.yaml fit\r\n```\r\n\r\ni.e. the config is given **before** `fit`, then the config must specify the subcommand. Similarly it is possible to specify `default_config_files` for each subcommand (see [lightning_cli_advanced_2.html#set-default-config-files](https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_2.html#set-default-config-files)), in which case the respective default configs are expected to be like with `run=False`.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2023-03-28T06:23:14Z"
    }
  },
  {
    "title": "TypeError: on_validation_end() missing 1 required positional argument: 'outputs'",
    "body": "Hello, \r\n\r\nWhen I train my model, I get this error:\r\n\r\nTypeError: on_validation_end() missing 1 required positional argument: 'outputs'\r\n\r\nThis is the function inside my model class:\r\n\r\n```\r\ndef on_validation_end(self, outputs):\r\n     loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\r\n     metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\r\n     self.log('val_loss', loss)\r\n     self.log('val_metric', metric)\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17182",
    "createdAt": "2023-03-23T22:20:17Z",
    "updatedAt": "2024-05-08T09:01:41Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dimgag"
    },
    "answer": {
      "body": "hey @dimgag! I had the same problem recently. As of `lightning>=2.0.0` you have to define the `validation_step_outputs` in the init clause of your module to store the validation outputs of each epoch. Here's an example (from [this PR](https://github.com/Lightning-AI/lightning/pull/16520)).\r\n\r\n```python\r\n class MyLightningModule(L.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.validation_step_outputs = []\r\n\r\n     def validation_step(self, ...):\r\n         loss = ...\r\n         self.validation_step_outputs.append(loss)\r\n         return loss\r\n\r\n    def on_validation_epoch_end(self):\r\n        epoch_average = torch.stack(self.validation_step_outputs).mean()\r\n        self.log(\"validation_epoch_average\", epoch_average)\r\n        self.validation_step_outputs.clear()  # free memory\r\n```",
      "author": {
        "login": "AzulGarza"
      },
      "createdAt": "2023-04-05T20:20:58Z"
    }
  },
  {
    "title": "on_training_epoch_end is never called",
    "body": "I'm training a CNN and I wanted to log some metrics at the end of each training epoch. However, I've noticed that `on_training_epoch_end` is never called while `on_validation_epoch_end` works just fine Here's an excerpt of the model containing those two:\r\n```\r\ndef training_step(self, batch):\r\n          images, labels = batch \r\n          pred = self(images)\r\n          train_loss = F.cross_entropy(pred, labels)\r\n          correct=pred.argmax(dim=1).eq(labels).sum().item()\r\n          total=len(labels)\r\n          batch_dictionary={\r\n                \"loss\": train_loss,\r\n                \"correct\": correct,\r\n                \"total\": total\r\n          }\r\n          self.training_step_outputs.append(batch_dictionary)\r\n          return batch_dictionary\r\n          \r\n\r\n      def validation_step(self, batch, batch_idx):\r\n          images, labels = batch \r\n          pred = self(images)\r\n          val_loss = F.cross_entropy(pred, labels)\r\n          correct=pred.argmax(dim=1).eq(labels).sum().item()\r\n          total=len(labels)\r\n          val_acc = correct/total\r\n          batch_dictionary={\r\n                \"loss\": val_loss,\r\n                \"acc\": val_acc,\r\n                \"correct\": correct,\r\n                \"total\": total\r\n          }\r\n          self.validation_step_outputs.append(batch_dictionary)\r\n          return batch_dictionary\r\n          \r\n      def on_training_epoch_end(self):\r\n          print('training epoch')\r\n          outputs = self.training_step_outputs;\r\n          batch_losses = [x['loss'] for x in outputs]\r\n          epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\r\n          epoch_acc = sum([x['acc'] for x in outputs])/len(outputs)\r\n          print(\"Training accuracy : \", epoch_acc)\r\n          print(\"Training loss : \", epoch_loss)\r\n          self.training_step_outputs.clear()  # free memory\r\n          \r\n      def on_validation_epoch_end(self):\r\n          outputs = self.validation_step_outputs;\r\n          batch_losses = [x['loss'] for x in outputs]\r\n          epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\r\n          epoch_acc = sum([x['acc'] for x in outputs])/len(outputs)\r\n          print(\"\\nValidation accuracy : \", epoch_acc)\r\n          print(\"Validation loss : \", epoch_loss)\r\n          val_acc.append(epoch_acc)\r\n          self.validation_step_outputs.clear()  # free memory\r\n```\r\n\r\nI've looked around and couldn't find any solutions to as to why this is happening or how to fix it.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17146",
    "createdAt": "2023-03-20T05:43:55Z",
    "updatedAt": "2023-03-21T14:11:46Z",
    "closedAt": "2023-03-21T14:11:42Z",
    "isAnswered": true,
    "author": {
      "login": "bavShehata"
    },
    "answer": {
      "body": "If any one finds themselves here, the correct hook is `on_train_epoch_end` and not `on_training_epoch_end` as that was a mistake in the docs as answered [here on their forums](https://lightning.ai/forums/t/on-training-epoch-end-does-not-get-called/2436)",
      "author": {
        "login": "bavShehata"
      },
      "createdAt": "2023-03-21T14:11:42Z"
    }
  },
  {
    "title": "gradient optimizer for PyTorch Temporal Fusion Transformer (TFT)",
    "body": "Dear Community,\r\n\r\nI am using the PyTorch TemporalFusionTransformer implementation along with the PyTorch lightning Trainer. (https://pytorch-forecasting.readthedocs.io/en/latest/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html + https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)\r\n\r\nUnfortuantely, i can't find any information about the default learning rate optimizer is used. Does anyone have any idea?\r\n\r\nAs far as I understand it correctly, the authors of TFT use the Adam optimizer with learning_rate and max_gradient_norm:\r\nhttps://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py\r\n\r\nBest regards\r\n\r\nStefan",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17036",
    "createdAt": "2023-03-11T12:41:42Z",
    "updatedAt": "2023-03-22T18:22:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "grosestq"
    },
    "answer": {
      "body": "I finaly found it :)\r\nPytorch uses the Ranger optimizer as default.",
      "author": {
        "login": "grosestq"
      },
      "createdAt": "2023-03-22T18:22:29Z"
    }
  },
  {
    "title": "Does pytorch lightning divide the loss by number of gradient accumulation steps?",
    "body": "For example, in this code, does the Trainer divide the loss by gradient accumulation steps (i.e. 7)?\r\n\r\n```py\r\n# Accumulate gradients for 7 batches\r\ntrainer = Trainer(accumulate_grad_batches=7)\r\n```\r\n\r\nI want to know this in order to understand how to set learning rate correctly. I saw that huggingface accelerate package does divide the loss: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation\r\n```py\r\n- loss = loss / gradient_accumulation_steps  # this is the line that does divide the loss\r\n  accelerator.backward(loss)\r\n- if (index+1) % gradient_accumulation_steps == 0:\r\n  optimizer.step()\r\n  scheduler.step()\r\n  optimizer.zero_grad()\r\n```\r\n\r\nIf the loss is not being divided, it will cause the gradient to become bigger and that has implication on whether you should multiply the learning rate by gradient accumulation steps.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17035",
    "createdAt": "2023-03-11T11:46:43Z",
    "updatedAt": "2025-02-25T16:37:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "offchan42"
    },
    "answer": {
      "body": "https://github.com/Lightning-AI/lightning/blob/bb861cba7e2a4597c56def506f0a64c9a30b9e8a/src/lightning/pytorch/core/module.py#L1038-L1054\r\n\r\nNow I know why my model doesn't converge :) ",
      "author": {
        "login": "JuanFMontesinos"
      },
      "createdAt": "2023-03-21T08:37:09Z"
    }
  },
  {
    "title": "Current best practices to initialize massive (50B parameter+) models",
    "body": "Hi, I am working with GPT-style models and need to intitialize a model at the GPT-3 scale.  Unfortunately, this means the model will run out of memory during initialization on CPU (or take an eternity to initialize layer-by-layer on cpu before shipping to GPU).  In vanilla Pytorch I solved this using FSDP by initializing my models on the \"meta\" device, with full initialization on GPU afterward.  What is the current best, most performant method to accomplish this with lightning?\r\n\r\nNote: I found this, which references an init_meta_context(), but my pytorch-lightning (v1.9.0) has no such functionality:\r\nhttps://devblog.pytorchlightning.ai/experiment-with-billion-parameter-models-faster-using-deepspeed-and-meta-tensors-2e9c255edd71",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16944",
    "createdAt": "2023-03-03T15:44:50Z",
    "updatedAt": "2023-03-09T15:15:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "azton"
    },
    "answer": {
      "body": "Hi! The `init_meta_context` functionality was replaced with a [`torchdistx`](https://github.com/pytorch/torchdistx) integration in https://github.com/Lightning-AI/lightning/pull/13868. You can do the following:\r\n\r\n```python\r\nfrom torchdistx.deferred_init import deferred_init\r\n\r\nmodel = deferred_init(YourLightningModule)\r\n```\r\n\r\nAnd we'll materialize it for you in the Trainer. This is very experimental, and you might encounter installation issues.\r\n\r\nIn the long term, we'll adopt the fake tensor mode from PyTorch: https://github.com/Lightning-AI/lightning/issues/16448.\r\n\r\nOtherwise, for a stable(r) solution, you can use the DeepSpeed integration: https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed-zero-stage-3",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2023-03-09T13:53:53Z"
    }
  },
  {
    "title": "Lightning in Jupyter notebook server on SLURM",
    "body": "pytorch_lightning v1.9.2\r\n\r\nIs there a way to bypass/disable automatic Slurm environment detection and instantiation? I'm starting a jupyter notebook server through a Slurm batch and the Trainer seems to be getting confused\r\n\r\nHere are the various combinations of parameters and configurations that I have attempted with no success:\r\n\r\nstrategy=\"dp\" in notebook or in a python training script run from terminal in server - works but pytorch only sees `--cpus-per-task` number of cpus, regardless of the value of `--ntasks-per-node`, so if I have 6 cpus per task and 8 tasks, then pytorch dataloader complains if I have more than `num_workers=6`\r\n\r\nstrategy='ddp' in a python training script run from terminal in server - hangs on `INFO:lightning_fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8`. Doesn't matter how many devices are specified, attempts to spin up `--ntasks-per-node` number of threads\r\n\r\nstrategy='ddp_notebook' in notebook - complains that whichever port is randomly (or manually) chosen is already in use: \r\n```\r\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:21545 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:21545 (errno: 98 - Address already in use).\r\n```\r\n\r\nWhen I specify `--ntasks` instead of `--ntasks-per-node` in my slurm batch script, lightning complains about the Slurm environment, so I know that it's being called even though I haven't specified it as a plugin for Trainer initialization. I've tried overwriting the environment after the Trainer is instantiated, but get the same socket bind error, so my only guess is that I need to prevent SLURMEnvironment from being called at all",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16846",
    "createdAt": "2023-02-23T00:12:34Z",
    "updatedAt": "2023-02-23T23:02:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "devtronslab"
    },
    "answer": {
      "body": "I figured it out. In case anyone else is attempting to follow me into this soul-sucking morass of frustration here's how you solve it.\r\n\r\nIf you're attempting to run a jupyter notebook server on a slurm-provisioned instance and use lightning with strategy `ddp_notebook`:\r\n - in `sbatch`, `salloc`, or `srun` \r\n   - `nodes=1`, I'm not sure it's possible to run a jupyter notebook server distributed over multiple nodes, so this is 1\r\n   - `cpus-per-task=<total number of cpus desired>`, pytorch dataloader will only be able to see this many processors, _not_ this value times the number of tasks. So, if you have 4 tasks and 5 cpus-per-task, pytorch will only utilize 5 cpus\r\n   - `ntasks-per-node=1`, setting this to 1 doesn't seem to affect anything, and as mentioned above `cpus-per-task` needs to be the total number of cpus, so this needs to be 1\r\n   - my system uses `gres=gpu:<total number of gpus>`, I haven't tried any other ways of specifying total number of gpus\r\n - in your notebook \r\n```\r\ntrainer = pl.Trainer(accelerator='gpu', devices=NUM_GPUS, strategy='ddp_notebook', \r\n                     plugins=[pl.plugins.environments.LightningEnvironment()])\r\n```\r\n - profit",
      "author": {
        "login": "devtronslab"
      },
      "createdAt": "2023-02-23T23:02:50Z"
    }
  },
  {
    "title": "Is there an easy way to test checkpoints every epoch and draw a line chart using a wandb or tensorboard logger?",
    "body": "I have trained my model and saved checkpoints every epoch. After training, I realize that the validation result which is a line chart is not reliable because the size of validation set is too small. Thus, I want to use a larger validation set to validate all the checkpoints and draw a line chart. I have a solution to this problem which is to validate every checkpoint in separate runs then draw a line chart using matplotlib. However, the solution is not graceful. Is there an easy way such as a parameter of Trainer or rewriting methods of LightningModule to do this job?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16762",
    "createdAt": "2023-02-15T06:03:25Z",
    "updatedAt": "2023-02-15T10:05:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "guzy0324"
    },
    "answer": {
      "body": "Instantiating 1 trainer and validating all the checkpoints solves my problem. And the epoch and global step of training time can be resumed by [this way](https://github.com/Lightning-AI/lightning/issues/16207#issuecomment-1431053171).",
      "author": {
        "login": "guzy0324"
      },
      "createdAt": "2023-02-15T10:05:22Z"
    }
  },
  {
    "title": "Correct behavior when using precision=16 with Tensor-Cores",
    "body": "I have notices that setting `precision=16` in the Trainer raises a warning on the cluster I am using: \r\n\r\n```\r\nYou are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores.  \r\nTo properly utilize them, you should set \r\n`torch.set_float32_matmul_precision('medium' | 'high')`  \r\nwhich will trade-off precision for performance. \r\nFor more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\r\n```\r\n\r\nDoes this means that I should avoid `precision=16` ? Where should i set the suggested command: \r\n`torch.set_float32_matmul_precision('medium' | 'high')` in the PL script ? \r\n\r\nThanks ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16698",
    "createdAt": "2023-02-09T09:29:41Z",
    "updatedAt": "2025-04-25T21:24:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SerezD"
    },
    "answer": {
      "body": "@SerezD Good question.\r\nThis setting is independent of the \"precision\" setting in the Trainer. It is regarding how matrix multiplications are performed in general. Note that precision=16 conceptually means \"mixed precision\", so not everything is in float-16 precision. So you can potentially benefit from this setting regardless. \r\n\r\n> Where should i set the suggested command\r\n\r\nThis is a global setting, so you can put it anywhere you want. But of course, preferably before training starts :) \r\n\r\n\r\nJust a heads up: For future questions, consider posting them in our new Forum over at [lightning.ai/forums](https://lightning.ai/forums/). We are slowly beginning the migration away from GH discussions.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2023-02-10T02:12:44Z"
    }
  },
  {
    "title": "What happens if the sampler of the training dataloader has a varying size for each epoch ?",
    "body": "Hi,\r\n\r\nI'm currently writing my own sampler of data and because of my requirements, for some epochs there are fewer or more data sampled.\r\n\r\nthe code roughly looks like this:\r\n```python\r\nclass MySampler(Sampler):\r\n    def __init__(\r\n        self,\r\n        data_source,\r\n        max_num_samples,\r\n    ) -> None:\r\n        super().__init__(data_source)\r\n        self.max_num_samples = max_num_samples\r\n        self.num_samples=0\r\n\r\n    def __iter__(self) -> List[Any]:\r\n        g = torch.Generator()\r\n        g.manual_seed(self.seed + self.epoch)\r\n\r\n        indices = [None for i in range(self.max_num_samples)]\r\n        global_idx = 0\r\n        for idx in range(torch.randint(self.max_num_windows, (1), generator=g)):\r\n            indices[global_idx] = self.data_source[idx]\r\n            global_idx +=1\r\n        \r\n        indices = indices[: global_idx + 1]\r\n        self.num_samples = len(indices)\r\n\r\n        return iter(indices)\r\n\r\n    def __len__(self) -> int:\r\n        return self.num_samples\r\n```\r\n\r\nIf I leave ```self.num_samples=0``` lightning does not perform training and says there are no training samples, probably because of some initialization inside lightning. I changed it to `self.num_samples = max_num_samples` to make it work, but my questions would be:\r\n- Is it ok to have a varying number of batches at each epoch without using iterable datasets?\r\n- What is initialized behind the scenes that lightning needs ? I assume that it will try to compute the number of training steps which will be wrong in my case.\r\n\r\nThanks in advance. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16652",
    "createdAt": "2023-02-06T16:43:23Z",
    "updatedAt": "2023-02-10T07:27:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "juliendenize"
    },
    "answer": {
      "body": "Hi @juliendenize \r\n\r\n> Is it ok to have a varying number of batches at each epoch without using iterable datasets?\r\n\r\n- It is ok if training with single-device/single-GPU training. \r\n- It is NOT ok if training with DDP in general (your training loop will fall out of sync and eventually hang)\r\n- It is ok if training with DDP AND you can guarantee that each process/GPU has the same epoch length. If the length changes from epoch N to N + 1, it has to change the same way in all processes.\r\n\r\nIf you aren't intending to train with DDP, you should be good. If you do, and since you have a custom sampler, you will have to make your sample distributed (let me know if you need details on this).\r\n\r\n> What is initialized behind the scenes that lightning needs ? I assume that it will try to compute the number of training steps which will be wrong in my case.\r\n\r\nI think all we do is call len() on the dataloader to determine the number. Since you implement that as well in the sampler, that should be correct. Every epoch, the loop will call `__iter__` on the dataloader, and this will invoke your `__iter__` on the sampler.\r\nLet me know if you need references to the code.\r\n\r\nLet me know if this helps.\r\n\r\nJust a heads up: For future questions, consider posting them in our new Forum over at [lightning.ai/forums](https://lightning.ai/forums/). We are slowly beginning the migration away from GH discussions.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2023-02-10T02:44:02Z"
    }
  },
  {
    "title": "Passing string value for custom profiler to the trainer.",
    "body": "I have been working on a custom profiler installable package for which I want to pass string option to the trainer. However trainer has a check in _init_profiler that allows string values to only be one of \"simple\", \"advanced\", \"pytorch\", or \"xla\". \r\nIs there a way to do this in an external package without modifying the trainer in PyTorch Lightning ?\r\n\r\nError: raise MisconfigurationException(\r\nlightning_lite.utilities.exceptions.MisconfigurationException: When passing string value for the `profiler` parameter of `Trainer`, it can only be one of ['simple', 'advanced', 'pytorch', 'xla']",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16563",
    "createdAt": "2023-01-30T15:21:23Z",
    "updatedAt": "2023-02-01T09:56:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ankitgola005"
    },
    "answer": {
      "body": "Dear @assassin1991,\r\n\r\nUnfortunately, PyTorch Lightning doesn't support passing new profiler strings as those are already registered in the framework.  \r\n\r\nHere are several options:\r\n1) Pass the profiler directly to the Trainer. Simpler solution IMO especially if you are exposing arguments to the profiler.\r\n2) Patch the Trainer internal to register your profiler string but this might break if internal changes\r\n3) Open a PR enabling users to register new profiler names. If your profiler subclasses PyTorch Lightning profilers and register a static `name`, I believe it is possible for the Trainer to search all the available subclass of the BaseProfiler and perform automatic resolution of the Profiler class.\r\n\r\nBest,\r\nT.C \r\n\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2023-02-01T09:04:30Z"
    }
  },
  {
    "title": "Exclude specific nn.Parameter from checkpoint saving",
    "body": "I have certain parameters registered as nn.Parameter (inside an nn.Module inside a LightningModule) that I would like to be excluded from the checkpoint. How can I accomplish this in Lightning?\r\n\r\nI realize I could avoid using nn.Parameter and simply place the tensor on the correct device. However, letting Lightning handle the device placement is much easier in this context, and it also records their size for the model summary. Most of the time these are non-learnable paramters, but in some cases I want to choose whether or not to save learnable parameters in the checkpoint. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16409",
    "createdAt": "2023-01-17T18:44:54Z",
    "updatedAt": "2023-01-18T19:48:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "RS-Coop"
    },
    "answer": {
      "body": "I figured out how to accomplish this by deleting the Parameters in question from the state dict inside `LightningModule.on_save_checkpoint()`. ",
      "author": {
        "login": "RS-Coop"
      },
      "createdAt": "2023-01-18T19:48:09Z"
    }
  },
  {
    "title": "Getting ValueError Error when using ModelCheckpoint with auto_insert_metric_name=False",
    "body": "Hi, I understand that `auto_insert_metric_name=False` when metric names contain /. \r\nFor my case, I set checkpoints as follows.\r\n\r\n```\r\n pl.callbacks.ModelCheckpoint(monitor='epoch/val/loss', mode='min',\r\n                                                       auto_insert_metric_name=False,\r\n                                                       filename='epoch={epoch:02d}-loss={epoch/val/loss:.2f}')\r\n```\r\n\r\nBut encounter a problem which is `ValueError: Only '.' or '[' may follow ']' in format field specifier` at `pytorch_lightning/callbacks/model_checkpoint.py\", line 515`\r\n\r\nWondering if I missed anything on this feature?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16385",
    "createdAt": "2023-01-16T17:22:36Z",
    "updatedAt": "2023-07-23T01:44:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "quy-ng"
    },
    "answer": {
      "body": "Hi @dongchirua, I found this issue by debugging pytorch_lightning/callbacks/model_checkpoint.py #L524\r\n```python\r\n    def _format_checkpoint_name(\r\n        cls,\r\n        filename: Optional[str],\r\n        metrics: Dict[str, Tensor],\r\n        prefix: str = \"\",\r\n        auto_insert_metric_name: bool = True,\r\n    ) -> str:\r\n        if not filename:\r\n            # filename is not set, use default name\r\n            filename = \"{epoch}\" + cls.CHECKPOINT_JOIN_CHAR + \"{step}\"\r\n\r\n        # check and parse user passed keys in the string\r\n        groups = re.findall(r\"(\\{.*?)[:\\}]\", filename)\r\n        if len(groups) >= 0:\r\n            for group in groups:\r\n                name = group[1:]\r\n\r\n                if auto_insert_metric_name:\r\n                    filename = filename.replace(group, name + \"={\" + name)\r\n\r\n                # support for dots: https://stackoverflow.com/a/7934969\r\n                filename = filename.replace(group, f\"{{0[{name}]\")\r\n\r\n                if name not in metrics:\r\n                    metrics[name] = torch.tensor(0)\r\n            filename = filename.format(metrics)\r\n\r\n        if prefix:\r\n            filename = cls.CHECKPOINT_JOIN_CHAR.join([prefix, filename])\r\n\r\n        return filename\r\n```\r\nRegex `groups` will contain \"{epoch\" and \"{epoch/val/loss\". In `filename = filename.replace(group, f\"{{0[{name}]\")`, the pattern `\"{epoch\"` will replace **BOTH** the first epoch and epoch of `'epoch/val/loss'` which results in `'epoch={0[epoch]:02d}-loss={0[epoch]/val/loss:.2f}'`. Format on this string will incur an error since designating floating point through `\":.2f\"` requires proper bracket open-close.\r\nThis is **NOT** expected, but we rather expect `'epoch={0[epoch]:02d}-loss={0[epoch/val/loss]:.2f}'`.\r\n\r\nFor me I just hacked into pytorch-lightning source code by changing`for group in groups[::-1`, tracking `\"epoch/val/loss\"` first. This is just a hack which does NOT essentially solve the problem and not a desired solution as well. I think changing not only including the opening bracket {, but also closing bracket } in regex pattern may solve the issue\r\n",
      "author": {
        "login": "1pha"
      },
      "createdAt": "2023-03-14T12:11:37Z"
    }
  },
  {
    "title": "Training dies with multiprocessing error",
    "body": "Hey,\r\n\r\nmy training dies after exactly 24 epoches always with the same multiprocessing error. Any idea what this\r\nI am using an RTX 3090 (NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8), Ubuntu 22.04\r\nFull log and conda environment attached.\r\nInterestingly I can reproduce the very same behavior on a different machine; almost the same error, also after 24 epochs, this machine has 2x 2080Ti.\r\n\r\nAny idea on how to investigate it? Or does it seem like core PyTorch and is best asked in their respective forum?\r\n\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/home/riesgroup/deploy/mac-ries26/decode/decode/neuralfitter/train/train.py\", line 81, in train\r\n    trainer.fit(\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 603, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\r\n    self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\r\n    results = self._run_stage()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\r\n    self._run_train()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1200, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.on_advance_end()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 251, in on_advance_end\r\n    self._run_validation()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 310, in _run_validation\r\n    self.val_loop.run()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\r\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 121, in advance\r\n    batch = next(data_fetcher)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py\", line 184, in __next__\r\n    return self.fetching_function()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py\", line 265, in fetching_function\r\n    self._fetch_next_batch(self.dataloader_iter)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py\", line 280, in _fetch_next_batch\r\n    batch = next(iterator)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\r\n    data = self._next_data()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1316, in _next_data\r\n    idx, data = self._get_data()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1282, in _get_data\r\n    success, data = self._try_get_data()\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1120, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/multiprocessing/queues.py\", line 122, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \"/home/riesgroup/mambaforge/envs/decode_dev_lighting186/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 322, in rebuild_storage_filename\r\n    storage = torch.UntypedStorage._new_shared_filename_cpu(manager, handle, size)\r\nRuntimeError: unable to mmap 160 bytes from file </torch_1282240_2656777504_296>: Cannot allocate memory (12)\r\n```\r\n[env.txt](https://github.com/Lightning-AI/lightning/files/10343510/env.txt)\r\n[decode_error.log](https://github.com/Lightning-AI/lightning/files/10343511/decode_error.log)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16241",
    "createdAt": "2023-01-04T11:10:25Z",
    "updatedAt": "2023-01-23T15:15:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Haydnspass"
    },
    "answer": {
      "body": "Custom Collate_FN error. Unrelated",
      "author": {
        "login": "Haydnspass"
      },
      "createdAt": "2023-01-23T15:15:38Z"
    }
  },
  {
    "title": "Proper way to shut down sub-processes in a dataloader",
    "body": "Hello,\r\n\r\nI have a dataloader (with `num_workers=0`) that creates a number of sub-processes to fill a queue. What is the proper way to signal the dataloader to shut down its processes when the training loop is executed by pytorch lightning?\r\n\r\nI can iterate the dataloader manually like this and everything works fine:\r\n\r\n```python\r\n\r\nclass MyIterableDataset():\r\n  def __iter__(self):\r\n    try:\r\n      processes = [mp.Process(...) for _ in range(8)]\r\n      yield ...\r\n    finally:\r\n      process.terminate()\r\n      process.join()\r\n\r\ndataloader = Dataloader(MyIterableDataset())\r\n\r\nfor epoch in range(2):\r\n  for batch in dataloader:\r\n    pass\r\n```\r\n\r\nI simply use the `try` `finally` pattern to shut down the sub-processes after each epoch in `finally`. But when I run a loop with pytorch lightning (via `trainer.fit`, for example), I get a BrokenPipe error, because the `finally` block is never executed. Pytorch lightning shuts the loop down from externally, i.e., it never reaches `finally`. What is the mechanism used by pytorch lightning to stop the iteration (at the end of an epoch or when reaching `limit_train_batches` etc.)? I tried to figure it out from the source code but did not manage.\r\n\r\nThanks\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16214",
    "createdAt": "2022-12-29T17:04:25Z",
    "updatedAt": "2022-12-30T16:41:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bask0"
    },
    "answer": {
      "body": "**The issue was not related to pytorch lightning**\r\n\r\nTurns out the BrokenPipe was caused by me shutting down the queue manager before shutting down the processes",
      "author": {
        "login": "bask0"
      },
      "createdAt": "2022-12-30T16:41:32Z"
    }
  },
  {
    "title": "lightningCLI : how to set a list parameters in commandline?",
    "body": "I'm using lightningCLI, I wanna to know , if trainer.plugins is a list such as ['AlpsLightningEnvironment', 'AntCheckpointIO'], how can i set the commandline ? (i set --trainer.plugins ['AlpsLightningEnvironment', 'AntCheckpointIO'] got error)\r\n\r\n\r\none more question, I want set callbacks of ModelCheckpoint like this : \"--trainer.callbacks ModelCheckpoint --trainer.callbacks.ModelCheckpoint.dirpath tmp_dir/torch_train_v2.pt\", but got trainer_init error. how to set path correctly?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16210",
    "createdAt": "2022-12-27T14:08:04Z",
    "updatedAt": "2022-12-28T03:29:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MasterGG"
    },
    "answer": {
      "body": "The CLI parses values as yaml/json. You need to make sure that what you give is valid and not incorrectly interpreted by your shell. Probably you will need to add quotes so that your shell does not split things into different arguments, i.e. `--trainer.plugins '[\"AlpsLightningEnvironment\", \"AntCheckpointIO\"]'`. Not sure if those values are correct, but the point is adding quotes so that the entire list is considered a single argument.\r\n\r\nExplanation about how to provide init args of a subclass in a list from command line is here [trainer-callbacks-and-arguments-with-class-type](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type).\r\n\r\nNote that as arguments become complex, it is preferable that you use a config file and get some help from `--print_config`, see [prepare-a-config-file-for-the-cli](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_advanced.html#prepare-a-config-file-for-the-cli).\r\n",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-12-27T14:28:34Z"
    }
  },
  {
    "title": "eval every n_steps",
    "body": "How to run the **validation** every **n_steps**? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16179",
    "createdAt": "2022-12-22T17:56:17Z",
    "updatedAt": "2022-12-26T05:25:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "charlottecaucheteux"
    },
    "answer": {
      "body": "```python\r\n# default used by the Trainer\r\ntrainer = Trainer(val_check_interval=1.0)\r\n\r\n# check validation set 4 times during a training epoch\r\ntrainer = Trainer(val_check_interval=0.25)\r\n\r\n# check validation set every 1000 training batches in the current epoch\r\ntrainer = Trainer(val_check_interval=1000)\r\n\r\n# check validation set every 1000 training batches across complete epochs or during iteration-based training\r\n# use this when using iterableDataset and your dataset has no length\r\n# (ie: production cases with streaming data)\r\ntrainer = Trainer(val_check_interval=1000, check_val_every_n_epoch=None)\r\n```\r\n\r\nSee also: https://pytorch-lightning.readthedocs.io/en/1.8.3/common/trainer.html#val-check-interval",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-12-26T05:25:25Z"
    }
  },
  {
    "title": "How to keep track of all outputs during training?",
    "body": "Dear lightning discussion channel,\r\n\r\nthank you for building such an amazing tool and developing environment.\r\n\r\nI am currently trying to build a simple callback that would keep hold of all outputs and targets of a model during training. I was able to build a rough draft, but I am facing the problem that the images are not fed in a fixed order (shuffle = True), so I wonder if there is a way to find the image index in the dataset.\r\n\r\nMy current approach is defining the following function:\r\n\r\n```python\r\ndef on_train_batch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\r\n        ...\r\n        self.outputs[trainer.current_epoch, batch_size * batch_idx: (batch_idx + 1) * batch_size] = outputs[\"outputs\"]\r\n        self.targets[trainer.current_epoch, batch_size * batch_idx: (batch_idx + 1) * batch_size] = outputs[\"targets\"]\r\n```\r\n\r\nHowever, due to shuffling the `self.targets` is not the same among epochs. Is there a way to know the index of each image in the original dataset/subset object?\r\n\r\nI am still thinking if there is a simple way to do this. I though about hashing the value of each image, but data augmentation makes such action unreliable.\r\n\r\nHopefully you can help me! ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16127",
    "createdAt": "2022-12-20T08:30:00Z",
    "updatedAt": "2023-02-05T06:53:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Pedrexus"
    },
    "answer": {
      "body": "In the end, the solution was simply to modify the dataset class dynamically:\r\n\r\n```python\r\n  def index_return_wrapper(dataset_cls: type) -> type:\r\n      class IndexReturnWrapper(dataset_cls):\r\n          def __getitem__(self, index: int) -> Tuple[Any, Any, int]:\r\n              return super().__getitem__(index) + (index,)\r\n      return IndexReturnWrapper\r\n```",
      "author": {
        "login": "Pedrexus"
      },
      "createdAt": "2022-12-25T00:52:30Z"
    }
  },
  {
    "title": "MLFlow UI and save_dir specification",
    "body": "Hello,\r\n\r\nWhen I use the MLFlowLogger without setting `save_dir` argument, then run `mlflow ui`, I have no issue to display artefacts.\r\nIn `mlruns/438809872721685038/d0b0373f48c549f3b36e97f5017dda71/meta.yml,` I see\r\n`artifact_uri: file:./mlruns/438809872721685038/d0b0373f48c549f3b36e97f5017dda71/artifacts`\r\n\r\nWhen I set `save_dir` (here \"experiments/mlruns\"), then run `cd experiments;mlflow ui`, I don't see the artefacts.\r\nIn `experiments/mlruns/933121669842833043/64cd5a18fe71423d8c9423a320ee7b91/meta.yml`, I see\r\n`artifact_uri: file:experiments/mlruns/933121669842833043/64cd5a18fe71423d8c9423a320ee7b91/artifacts`\r\n\r\nI fix it by setting the `tracking_uri` to set an absolute path, like this :\r\n```\r\nsave_dir = \"experiments/mlruns\"\r\npl.loggers.mlflow.MLFlowLogger(\r\n    save_dir=save_dir,\r\n    experiment_name=experiment_name,\r\n    run_name=run_name,\r\n    tracking_uri=f\"file://{save_dir}\"\r\n)\r\n```\r\n\r\nIs there a better way to fix it ?\r\nIs it a good idea to open an issue about it ?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16112",
    "createdAt": "2022-12-19T09:39:19Z",
    "updatedAt": "2022-12-26T10:21:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": null,
    "answer": {
      "body": "@NicolasNeovision It would be great if you could submit a bug report by creating a GitHub issue with a minimal script and env detail. Thank you \u26a1",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-12-26T06:13:58Z"
    }
  },
  {
    "title": "correct way to launch on slurm clusters?",
    "body": "I have been using lightning on a single node and it works very well. Thanks for the great project.\r\n\r\nI am not sure what's the best practices to launch jobs on slurm clusters when lightning is used in the codebase. I am also using hydra and submitit. It seems the hydra submitit launcher plugin does not work well because submitit launcher and lightning are both trying to spawn multiple subprocesses for DDP workers.\r\n\r\nWhat do people find as the best practice in multi-node training? Any comments or suggestions are great appreciated.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/16004",
    "createdAt": "2022-12-10T18:21:39Z",
    "updatedAt": "2024-02-28T16:22:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "min-xu-ai"
    },
    "answer": {
      "body": "Never mind. I found the doc that describes this. I guess using submitit is just not supported at the moment.",
      "author": {
        "login": "min-xu-ai"
      },
      "createdAt": "2022-12-11T18:09:44Z"
    }
  },
  {
    "title": "Linking arguments after loading a data module?",
    "body": "I have a lightning module (model) that needs an argument to come from the DataModule (eg. batch_size). This works great when I explicitly define the data module that needs to be used: \r\n\r\n```python\r\nclass demo_cli(LightningCLI):\r\n     def add_arguments_to_parser(self, parser):\r\n            parser.link_arguments(\"data.batch_size\", \"model.batch_size\")\r\n\r\ncli = demo_cli(LightningModule, DataModule) \r\n```\r\nIf I however have multiple data modules, and attempt this, there appears to be no way to link arguments from DataModules loaded dynamically via the command line / config files as described in this [section of the docs](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_advanced_3.html#multiple-models-and-or-datasets). Any ideas?\r\n```python\r\ncli = demo_cli(LightningModule, subclass_mode_data=True) \r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15613",
    "createdAt": "2022-11-10T00:59:28Z",
    "updatedAt": "2022-11-18T22:17:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rohanshad"
    },
    "answer": {
      "body": "The `link_argument`'s source and target keys follow the same structure as the config files. If only the data is in subclass mode, not the model, then the link would be as:\r\n\r\n```python\r\nparser.link_arguments(\"data.init_args.batch_size\", \"model.batch_size\")\r\n```",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-11-11T17:40:01Z"
    }
  },
  {
    "title": "Running test stage after training stage using LightningCLI",
    "body": "Hello,\r\n\r\nI'm using LightningCLI for training a model. I want to run training, validation and testing all in the same experiment to have them all logged in one place, so that they are easily accessible by the logger (mlflow). The validation stage happens after each epoch and the parameters are logged. The testing stage does not take place after training automatically.\r\n\r\nI know that I can use fit,validate,test,predict,tune as the documentation says.\r\n\r\n`$ python trainer.py --help\r\nusage: trainer.py [-h] [--config CONFIG] [--print_config [={comments,skip_null}+]] {fit,validate,test,predict,tune} ...\r\n`\r\n\r\nAfter the training is over the execution of the experiment is terminated and I have manually execute the code and specify the path to the checkpoint. Therefore, a new experiment is created and the test metrics are logged independently as a separate experiment. I want to run the test stage right after training is over with the current model and avoid running a new experiment for testing. I couldn't find a way to do this with the Lightning CLI. How can I run the test stage right after training is over?\r\n\r\nThanks,",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15541",
    "createdAt": "2022-11-04T22:10:01Z",
    "updatedAt": "2022-11-10T14:12:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SinaSarparast"
    },
    "answer": {
      "body": "You can [Customize the LightningCLI](https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_expert.html#customize-the-lightningcli) `after_fit`, here is an [example](https://github.com/tshu-w/lightning-template/blob/cacec51c9035d614e9a92a158bc1ef683e7b5b3e/src/utils/lit_cli.py#L54-L100).\r\nOr implement a [callback](https://github.com/tshu-w/lightning-template/blob/main/src/callbacks/metric.py) and add it to LightningCLI by default.",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-11-05T05:38:59Z"
    }
  },
  {
    "title": "Loss discrepancies when using the trainer",
    "body": "I'm encountering a weird training issue where my loss increases dramatically after one sample (like from 8 to 9.1188e+11) when I return the loss at the end of my training_step() function. However when I don't return anything (comment-out return(loss)), the loss remains stable for all of the samples.\r\n\r\nHas anyone seen anything like this? Perhaps I am misunderstanding how lightning performs the training. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15442",
    "createdAt": "2022-10-31T21:57:32Z",
    "updatedAt": "2022-10-31T22:45:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "martinez-zacharya"
    },
    "answer": {
      "body": "Nevermind, my learning rate was set to 1e5 instead of 1e-5.",
      "author": {
        "login": "martinez-zacharya"
      },
      "createdAt": "2022-10-31T22:19:34Z"
    }
  },
  {
    "title": "Handling extreme cases in validation sanity check",
    "body": "I'm using the default settings for the validation sanity check and think it is a useful feature.\r\n\r\nHowever, because the validation sanity check only processes a tiny amount of data, there might be some extreme cases occuring. Specifially, I'm using torchmetric's ConfusionMatrix, and most of the time not all of the existing classes are represented in the first few batches. Therefore the computation of the confusion matrix results in a warning:\r\n\r\n`UserWarning: 2 NaN values found in confusion matrix have been replaced with zeros.`\r\n\r\nI get this for every training run and it's a bit annoying. What's the best way to prevent this warning from occuring in such cases? I don't want to filter this warning globally, because if this happens in the actual validation it is definitely an issue. So I wondered how such extreme cases in the validation sanity check are supposed to be handled. Is it somehow possible to run some code before and after this sanity check, to disable this warning temporarily?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15381",
    "createdAt": "2022-10-28T09:38:07Z",
    "updatedAt": "2022-10-31T14:37:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ptoews"
    },
    "answer": {
      "body": "Hi @ptoews `on_train_start()` runs right after sanity-checking, so you can filter out the warning until sanity check finishes and remove the warning filter in `on_train_start()` hook so that you get the warning during the actual validations. FYI, here's the execution order of hooks:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.7.7/common/lightning_module.html#hooks",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-10-30T20:18:37Z"
    }
  },
  {
    "title": "error with customizing my progress par",
    "body": "I am  trying to  use RichProgressBar ! \r\nwhen i run the following code , I get this error \r\n\r\n```python\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\nfrom rich.progress import Progress\r\nmodel = ButterflyGAN()\r\ntrainer = pl.Trainer( max_epochs=100, gpus=1,callbacks=[RichProgressBar(refresh_rate=1,\r\n                                            theme=RichProgressBarTheme(description='black'\r\n                                                                        ,progressbar = '#6206E0'\r\n                                                                        ,progress_bar_finished='#6206E0'\r\n                                                                        ,progress_bar_pulse='#6206E0',\r\n                                                                        batch_size='white',\r\n                                                                        time='grey54',\r\n                                                                        processing_speed='grey70',\r\n                                                                        metrics='white'),console_kwargs=None)])#gpus=1\r\ntrainer.fit(model, buttefly_train_dataloader)\r\n```\r\n\r\n```\r\nNameError: name'RichProgressBarTheme' is not defined \r\n```\r\n\r\nany help\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15378",
    "createdAt": "2022-10-28T01:08:46Z",
    "updatedAt": "2022-10-31T14:21:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "abdelkareemkobo"
    },
    "answer": {
      "body": "You need to import `RichProgressBar` before instantiating the class.\r\n\r\n```python\r\nfrom pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\r\n```\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/1.7.7/visualize/logging_expert.html?highlight=RichProgressBarTheme#use-the-richprogressbar",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-10-28T20:17:26Z"
    }
  },
  {
    "title": "torch.set_grad_enabled fails to work in test_step",
    "body": "I wrote the following code in test_step:\r\n```python\r\nwith torch.set_grad_enabled(True):\r\n    x = torch.zeros(4, requires_grad=True)\r\n    print(x * 2)\r\n```\r\nbut only got a regular tensor (without grad_fn):\r\n```python\r\ntensor([0., 0., 0., 0.])\r\n```\r\n\r\nThis seems very weird. However, the same code works fine in validation_step, giving:\r\n```python\r\ntensor([0., 0., 0., 0.], grad_fn=<MulBackward0>)\r\n```\r\n\r\nI'm using the latest version 1.7.7. Also note that 1.6.1 works fine without such a problem. I think there are some changes about the test routine but I can't find any related changelog. Is this the expected behavior? Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15326",
    "createdAt": "2022-10-26T12:04:57Z",
    "updatedAt": "2022-10-27T08:22:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bennyguo"
    },
    "answer": {
      "body": "@bennyguo It may be related to #14497. With Lightning 1.8 release, you will be able to disable PyTorch's inference mode (which is enabled by Lightning by default):\r\n\r\n```python\r\n# default used by the Trainer\r\ntrainer = Trainer(inference_mode=True)\r\n\r\n# Use `torch.no_grad` instead\r\ntrainer = Trainer(inference_mode=False)\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-10-26T21:36:21Z"
    }
  },
  {
    "title": "Why does the distributed training get stuck here and doesn't move.",
    "body": "![image](https://user-images.githubusercontent.com/29114869/197446417-e23a8625-5900-48f4-955d-40abfeaf3c29.png)\r\n\r\nI have requested two GPUs on slurm cluster for distributed training, but the program does not move?\r\n\r\nWhen I use only one GPU, the model is trained normally.\r\n\r\n```\r\n        tb_logger = pl_loggers.TensorBoardLogger(save_dir=save_log_path)\r\n        checkpoint_callback = ModelCheckpoint(monitor='val_f1',\r\n                                              dirpath=\"{}\".format(save_log_path),\r\n                                              filename='best_{}'.format(index_times),\r\n                                              save_top_k=1,\r\n                                              mode='max',\r\n                                              save_weights_only=True,\r\n                                              save_last=False)\r\n\r\n        early_stop_callback = EarlyStopping(monitor=\"val_f1\", min_delta=0.00, patience=args.patience,\r\n                                            verbose=False, mode=\"max\")\r\n\r\n        trainer = Trainer(devices=\"auto\", accelerator=\"auto\",\r\n                          logger=tb_logger, log_every_n_steps=50,\r\n                          flush_logs_every_n_steps=50, callbacks=[checkpoint_callback, early_stop_callback],\r\n                          max_epochs=args.epochs)\r\n        trainer.fit(model)\r\n        result = trainer.test(model, ckpt_path='best')\r\n```\r\n\r\n\r\n```\r\nclass BERT_baseline(pl.LightningModule):\r\n    def __init__(self, args, lr):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path=args.path)\r\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=args.path)\r\n        self.CE_loss = nn.CrossEntropyLoss()\r\n        self.lr = lr\r\n        self.args = args\r\n        self.metric = load_metric(\"seqeval\")\r\n        self.label_name = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\r\n        self.num_labels = len(self.label_name)\r\n        self.fc = nn.Sequential(nn.Linear(self.bert.config.hidden_size, self.num_labels))\r\n\r\n    def forward(self, inputs=None, labels=None, label_mask=None):\r\n        bert_out = self.bert(**inputs)[0]  # batch * seq-length * hidden\r\n        logits = self.fc(bert_out)  # batch_size *  seq_length * num_lables\r\n        if labels is not None:  # \u8bad\u7ec3\r\n            # Only keep active parts of the loss\r\n            active_loss = label_mask.view(-1) == 1\r\n            active_logits = logits.view(-1, self.num_labels)[active_loss]  # \u9884\u6d4b\u7684\u7ed3\u679c\r\n            active_labels = labels.view(-1)[active_loss]  # \u771f\u5b9e\u7684\u7ed3\u679c\r\n            loss = self.CE_loss(active_logits, active_labels)\r\n            return loss\r\n        else:  # \u6d4b\u8bd5\u96c6\r\n            return bert_out, logits\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        bert_inputs = batch[0]\r\n        real_labels = batch[1]\r\n        label_mask = batch[2]\r\n        loss = self(inputs=bert_inputs, labels=real_labels, label_mask=label_mask)\r\n        loss_metirc = {\"loss\": loss}\r\n        self.log_dict(loss_metirc, on_step=True, prog_bar=True, logger=True, on_epoch=True)\r\n        return loss_metirc\r\n\r\n    def training_step_end(self, training_step_outputs):  # \u4e00\u4e2abatch\u7ed3\u675f\r\n        pass\r\n\r\n    def training_epoch_end(self, training_step_outputs):  # \u4e00\u4e2aepoch\r\n        pass\r\n\r\n    def share_val_step(self, batch):\r\n        bert_inputs = batch[0]\r\n        real_labels = batch[1]  # batch * seq\r\n        hidden, logits = self(inputs=bert_inputs)\r\n        y_pred_label = logits.argmax(dim=-1)\r\n        result_dict = self.compute_metrics(labels=real_labels, predictions=y_pred_label)\r\n        return result_dict['f1'], result_dict['precision'], result_dict['recall']\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        f1, precision,  recall = self.share_val_step(batch)\r\n        metrics = {'val_f1': f1, 'val_precision': precision, 'val_recall': recall}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n    def validation_step_end(self, val_step_outputs):\r\n        pass\r\n\r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        pass\r\n\r\n    def test_step(self, batch, batch_idx, dataloader_idx):\r\n        f1, precision,  recall = self.share_val_step(batch)\r\n        metrics = {'f1': f1, 'precision': precision, 'recall': recall}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n    def test_step_end(self, output_results):\r\n        pass\r\n\r\n    def test_epoch_end(self, test_step_outputs):\r\n        pass\r\n\r\n    def configure_optimizers(self):\r\n        if self.args.freeze != -1:\r\n            no_grad = [\"embeddings\"] + [\"layer.\" + str(layer_i) + \".\" for layer_i in range(12)\r\n                                        if layer_i < self.args.freeze]\r\n            for n, p in self.bert.named_parameters():\r\n                p.requires_grad = False if any(nd in n for nd in no_grad) else True\r\n\r\n        no_decay = ['bias', 'LayerNorm.weight']\r\n        optimizer_grouped_parameters = [  # \u5c06\u6743\u91cd\u8870\u51cf\u5e94\u7528\u4e8e\u9664\u4e86'\u504f\u79fb'\u8fd9\u4e9b\u4e4b\u5916\u7684\u6240\u6709\u53c2\u6570\r\n            # n\u662f\u6a21\u578b\u6bcf\u5c42\u540d\u5b57\uff0cp\u662f\u6bcf\u5c42\u53c2\u6570\uff0cmodel\u662f\u6a21\u578b\u540d\u5b57\r\n            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\r\n             'weight_decay': self.args.weight_decay},\r\n            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\r\n             'weight_decay': 0.0}\r\n        ]\r\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.lr, weight_decay=self.args.weight_decay)\r\n        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9, last_epoch=-1)\r\n        scheduler = get_linear_schedule_with_warmup(optimizer,\r\n                                                    num_warmup_steps=self.args.warmup_steps,\r\n                                                    num_training_steps=self.args.total_steps)\r\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\r\n        return [optimizer], [scheduler]\r\n\r\n    def train_dataloader(self):\r\n        train_data = Ner_Dataset(language=self.args.source, desc='train')\r\n        self.print('======================== \u8bad\u7ec3\u96c6\u6709\uff1a{}====================='.format(len(train_data)))\r\n        return DataLoader(train_data, batch_size=self.args.batch_size,\r\n                          shuffle=True, collate_fn=self.collate)\r\n\r\n    def val_dataloader(self):\r\n        val_data = Ner_Dataset(language=self.args.source, desc='valid')\r\n        self.print('======================== \u9a8c\u8bc1\u96c6\u6709\uff1a{}====================='.format(len(val_data)))\r\n        return DataLoader(val_data, batch_size=self.args.batch_size,\r\n                          shuffle=False, collate_fn=self.collate)\r\n\r\n    def test_dataloader(self):\r\n        data_list = [Ner_Dataset(language=one, desc='test') for one in self.args.target_list]\r\n        return [DataLoader(one, batch_size=self.args.batch_size, shuffle=False, collate_fn=self.collate)\r\n                for one in data_list]\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15266",
    "createdAt": "2022-10-24T06:10:32Z",
    "updatedAt": "2023-07-27T15:40:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "When I switch the communication method from **\u2018nccl\u2019** to **'gloo'**, it works. I don't know what the problem is, but I hope I can help you.\r\n```\r\n        ddp = DDPStrategy(process_group_backend='gloo')\r\n        trainer = Trainer(devices=\"auto\", accelerator=\"auto\", strategy=ddp,\r\n                          logger=tb_logger, log_every_n_steps=50,\r\n                          flush_logs_every_n_steps=50, callbacks=[checkpoint_callback, early_stop_callback],\r\n                          max_epochs=args.epochs)\r\n```",
      "author": {
        "login": "Struggle-Forever"
      },
      "createdAt": "2022-10-24T13:11:51Z"
    }
  },
  {
    "title": "Are there any good examples or tutorials out there for making Lightning Apps?",
    "body": "I didn't really understand how the following App was made. Is there a good example or tutorial somewhere?\r\n\r\nLightning App Gallery https://lightning.ai/apps",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15255",
    "createdAt": "2022-10-23T03:58:23Z",
    "updatedAt": "2022-10-24T01:05:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "to get started you can look at: https://lightning.ai/lightning-docs/#get-started\r\nand once you are familiar with the core structure, I'd suggest going over this tutorial: https://lightning.ai/lightning-docs/get_started/lightning_apps_intro.html\r\n\r\nlater on once you have good understanding of the components, you can pick any project you like from the gallary and explore the code-base and try to re-build it.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-10-23T20:32:43Z"
    }
  },
  {
    "title": "How to preserve dataset order when using DDP?",
    "body": "I need to be able to preserve the order in which the data is fed to the model when training in multiple GPUS.\r\nAccording to https://github.com/Lightning-AI/lightning/discussions/13342 each GPU gets a consecutive fraction of the dataset, so if I have 2GPUs, the first one will get the first half of the dataset and the other one will get the second half of the dataset.\r\nI need to preserve the order and don't know how to overwrite the dataset-splitting logic. Any advice?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15164",
    "createdAt": "2022-10-18T09:28:25Z",
    "updatedAt": "2022-11-12T09:25:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "for that you'd need to write a custom [DistributedSampler](https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler) and pass it to the dataloader and set `Trainer(replace_sampler_ddp=False)`",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-10-18T18:13:27Z"
    }
  },
  {
    "title": "Accuracy/loss is not improving when train huggingface transformer bert",
    "body": "## Summary\r\nLoss does not decrease and accuracy/F1-score is not improving during training HuggingFace Transformer BertForSequenceClassification with Pytorch-Lightning\r\n\r\n## Issue\r\nHello PTL team, previously, I trained huggingface bert model with my own trainer code. To improve code quality and implement MLOps system, I\u2019m trying to train huggingface\u2019s transformers Bert with pytorch lightning. \r\n\r\nWhen I train BertForSequenceClassification in the transformers with PTL, however, Loss, accuracy, and even f1 score seems to not improve during a training phase. I think there are some bugs in the optimizer or back-propagation in my code, but I can\u2019t find any problems. my question is, what is the problem with my code?\r\n\r\n## my assumtions:\r\n1. configure_optimizer is not correctly configured\r\n2. huggingface sequence classification module\r\n3. segment_ids should not be passes during training step\r\n4. etc.\r\n\r\n## package versions\r\npython==3.7\r\npytorch==1.11.0\r\npytorch-lightning == 1.7.7\r\ntransformers == 4.2.2\r\ntorchmetrics == up-to-date\r\n\r\n## code snippet\r\n```python\r\nimport os\r\nimport csv\r\nimport tqdm\r\nfrom typing import *\r\nimport pandas as pd\r\n\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\nimport torchmetrics.functional as F\r\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\r\nfrom transformers.modeling_outputs import SequenceClassifierOutput\r\nfrom transformers import BertForSequenceClassification, BertTokenizer, InputExample, InputFeatures, AutoConfig, get_linear_schedule_with_warmup\r\n\r\n\r\ndef get_input_example(guid: int, text_a: str, label: str) -> Tuple[InputExample, str]:\r\n    input_example = InputExample(\r\n        guid=guid,\r\n        text_a=text_a,\r\n        text_b=None,\r\n        label=label\r\n    )\r\n    return input_example, label\r\n\r\n\r\ndef add_examples(\r\n        texts_or_text_and_labels: Union[List[str], str],\r\n        text_index: int,\r\n        label_index: int,\r\n        label_dict: Dict[str, int] = None,\r\n        remove_top: bool = False\r\n) -> Tuple[List[InputExample], Dict[str, int]]:\r\n    examples = list()\r\n    labels = list()\r\n\r\n    tmp = []\r\n    if isinstance(texts_or_text_and_labels, str):\r\n        with open(texts_or_text_and_labels, 'r') as f:\r\n            if texts_or_text_and_labels.endswith('csv'):\r\n                delimiter = ','\r\n            elif texts_or_text_and_labels.endswith('tsv'):\r\n                delimiter='\\t'\r\n\r\n            reader = csv.reader(f, delimiter=delimiter, quotechar='\"')\r\n            for idx, line in enumerate(tqdm.tqdm(reader)):\r\n                if remove_top is True and idx == 0:\r\n                    pass\r\n                else:\r\n                    tmp.append(line)\r\n        texts_or_text_and_labels = tmp\r\n\r\n    for line in tqdm.tqdm(texts_or_text_and_labels):\r\n        text_a = line[text_index]\r\n        label = line[label_index]\r\n\r\n        input_example, label = get_input_example(guid=line[0], text_a=text_a, label=label)\r\n        examples.append(input_example)\r\n        if label_dict is None:\r\n            labels.append(label)\r\n    if label_dict is None:\r\n        label_dict = {i: idx for idx, i in enumerate(list(set(labels)))}\r\n    return examples, label_dict\r\n\r\n\r\nclass BertDataset(Dataset):\r\n    def __init__(self, examples, tokenizer, label_dict, max_length):\r\n        self.examples = examples\r\n        self.tokenizer = tokenizer\r\n        self.label_dict = label_dict\r\n        self.max_length = max_length\r\n\r\n    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length) -> None:\r\n        \"\"\"\r\n        Truncates a sequence pair in place to the maximum length.\r\n        This is a simple heuristic which will always truncate the longer sequence\r\n        one token at a time. This makes more sense than truncating an equal percent\r\n        of tokens from each, since if one sequence is very short then each token\r\n        that's truncated likely contains more information than a longer sequence.\r\n        \"\"\"\r\n        while True:\r\n            total_length = len(tokens_a) + len(tokens_b)\r\n            if total_length <= max_length - 3:\r\n                break\r\n            if len(tokens_a) > len(tokens_b):\r\n                tokens_a.pop()\r\n            else:\r\n                tokens_b.pop()\r\n\r\n    def __len__(self):\r\n        return len(self.examples)\r\n\r\n    def __getitem__(\r\n            self,\r\n            idx: int,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\r\n\r\n        def _tokens_and_segment_id(token_a: List[str], token_b: List[str] = None) -> Tuple[Any, List[int]]:\r\n            tokens = ['[CLS]'] + token_a + ['[SEP]']  # See in 1-1. Section in /docs/Appendix.md\r\n            token_type_ids = [0] * len(tokens)  # for more information of 138-145 lines\r\n            if token_b:\r\n                tokens += token_b + ['[SEP]']\r\n                token_type_ids += [1] * (len(token_b) + 1)\r\n            return tokens, token_type_ids\r\n\r\n        text_a = self.examples[idx].text_a\r\n        text_b = self.examples[idx].text_b\r\n        label = self.examples[idx].label\r\n\r\n            #   Convert texts into tokens\r\n        tokens_a = self.tokenizer.tokenize(text_a)\r\n        tokens_b = None\r\n        if text_b:\r\n            tokens_b = self.tokenizer.tokenize(text_b)\r\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\r\n            # length is less than the specified length.\r\n            # Account for [CLS], [SEP], [SEP] with '- 3'\r\n            self._truncate_seq_pair(tokens_a, tokens_b, self.max_length)\r\n        else:\r\n            if len(tokens_a) > self.max_length - 2:\r\n                tokens_a = tokens_a[:(self.max_length - 2)]\r\n\r\n        tokens, token_type_ids = _tokens_and_segment_id(tokens_a, tokens_b)\r\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\r\n        label_ids = self.label_dict[label]\r\n\r\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n        # tokens are attended to.\r\n        attention_mask = [1] * len(input_ids)\r\n\r\n        # Zero-pad up to the sequence length.\r\n        padding = [0] * (self.max_length - len(input_ids))\r\n        input_ids += padding\r\n        attention_mask += padding\r\n        token_type_ids += padding\r\n\r\n        assert len(input_ids) == self.max_length\r\n        assert len(attention_mask) == self.max_length\r\n        assert len(token_type_ids) == self.max_length\r\n\r\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\r\n        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\r\n        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\r\n        labels = torch.tensor(label_ids, dtype=torch.long)\r\n        return input_ids, attention_mask, token_type_ids, labels\r\n\r\n\r\nclass BertAccTestModel(pl.LightningModule):\r\n    def __init__(self, num_classes: int):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.config = AutoConfig.from_pretrained('klue/bert-base', num_labels=num_classes)\r\n        self.model = BertForSequenceClassification.from_pretrained('klue/bert-base', config=self.config)\r\n        self.tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\r\n        self.num_classes = num_classes\r\n\r\n    def forward(self, init_ids, input_mask, segment_ids) -> SequenceClassifierOutput:\r\n        outputs = self.model(init_ids, input_mask, segment_ids)\r\n        return outputs\r\n\r\n    def info(self, dictionary: dict) -> None:\r\n        r\"\"\"\r\n        Logging information from dictionary.\r\n        Args:\r\n            dictionary (dict): dictionary contains information.\r\n        \"\"\"\r\n        for key, value in dictionary.items():\r\n            self.log(key, value, prog_bar=True, sync_dist=True)\r\n\r\n    def training_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'train_loss': loss,\r\n            'train_acc': top1_acc,\r\n            'train_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def validation_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'val_loss': loss,\r\n            'val_acc': top1_acc,\r\n            'val_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def test_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'test_loss': loss,\r\n            'test_acc': top1_acc,\r\n            'test_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self(init_ids, input_mask, segment_ids, label_ids)\r\n        return torch.argmax(outputs.logits)\r\n\r\n    def configure_optimizers(self):\r\n        model = self.model\r\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n        optimizer_grouped_parameters = [\r\n            {\r\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n            {\r\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n        ]\r\n        optim = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\r\n        scheduler = get_linear_schedule_with_warmup(\r\n            optim,\r\n            num_warmup_steps=500,\r\n            num_training_steps=self.trainer.estimated_stepping_batches,\r\n        )\r\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\r\n        return [optim], [scheduler]\r\n\r\n\r\ndef main():\r\n    root = '/'.join(os.getcwd().split('/'))\r\n    print(root)\r\n    tokenizer = BertTokenizer.from_pretrained('klue/bert-base', do_lower_case=False)\r\n\r\n    train_examples, label_dict = add_examples(os.path.join(root, 'data/training_merged_1d.tsv'), text_index=2, label_index=7,  remove_top=True)\r\n    eval_examples, _ = add_examples(os.path.join(root, 'data/validation.tsv'), text_index=2, label_index=7, label_dict=label_dict)\r\n    test_examples, _ = add_examples(os.path.join(root, 'data/test.tsv'), text_index=2, label_index=7, label_dict=label_dict, remove_top=True)\r\n    train_dataset = BertDataset(train_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n    eval_dataset = BertDataset(eval_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n    test_dataset = BertDataset(test_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n\r\n    print(len(train_dataset), len(eval_dataset), len(test_dataset))\r\n    trn_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=4, sampler=RandomSampler(train_dataset))\r\n    eval_dataloader = DataLoader(eval_dataset, batch_size=64, num_workers=4, sampler=SequentialSampler(eval_dataset))\r\n    test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=4, sampler=SequentialSampler(test_dataset))\r\n    inference_label_dict = {v: k for k, v in label_dict.items()}\r\n\r\n    model = BertAccTestModel(num_classes= len(label_dict))\r\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        save_last=True,\r\n        save_weights_only=True,\r\n        monitor='val_f1',\r\n        mode='max',\r\n        dirpath=os.path.join(root, 'weights'),\r\n        filename='pytorch_model'\r\n    )\r\n    trainer = pl.Trainer(gpus=4, max_epochs=10, accelerator='cuda', strategy='ddp', precision=32, callbacks=[checkpoint_callback])\r\n    trainer.fit(model, train_dataloaders=trn_dataloader, val_dataloaders=eval_dataloader)\r\n    trainer.test(model, test_dataloader)\r\n\r\n    with open(os.path.join(root, 'weights', 'labels.dict'), 'w') as f:\r\n        import json\r\n        json.dump(inference_label_dict, f)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/15044",
    "createdAt": "2022-10-08T13:48:41Z",
    "updatedAt": "2022-10-10T03:30:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Klassikcat"
    },
    "answer": {
      "body": "## Updates\r\nI found that DDP was the problem. Accuracy and loss improve when I train my model on single-GPU, It seems to add_examples code was a problem because they make a label list which is not a fixed index. \r\n",
      "author": {
        "login": "Klassikcat"
      },
      "createdAt": "2022-10-10T03:30:30Z"
    }
  },
  {
    "title": "Training with dual (optimizer+scheduler) only one learning rate is updated over training steps",
    "body": "I am using two optimizers and two schedules in my PL model:\r\n```python\r\ndef configure_optimizers(self):\r\n    # optimizers\r\n    opt1 = torch.optim.AdamW(\r\n        self.encoder_1.parameters(), \r\n        lr=self.hparams.lr_1,\r\n        weight_decay=self.hparams.weight_decay\r\n    )\r\n\r\n    opt2 = torch.optim.AdamW(\r\n        self.encoder_2.parameters(),\r\n        lr=self.hparams.lr_2,\r\n        weight_decay=self.hparams.weight_decay\r\n    )\r\n\r\n    step_size_up = round(0.07 * self.trainer.estimated_stepping_batches)\r\n\r\n    schdlr_1 = torch.optim.lr_scheduler.CyclicLR(opt1, mode='triangular2',\r\n                                                       base_lr=self.hparams.base_lr,\r\n                                                       max_lr=self.hparams.max_lr, step_size_up=step_size_up\r\n                                                       )\r\n    schdlr_2 = torch.optim.lr_scheduler.CyclicLR(opt2, mode='triangular2',\r\n                                                       base_lr=self.hparams.base_lr,\r\n                                                       max_lr=self.hparams.max_lr, step_size_up=step_size_up\r\n                                                       )\r\n\r\n    return (\r\n        {\"optimizer\": opt1, \"lr_scheduler\": schdlr_1, \"frequency\": self.hparams.frequency_1},\r\n        {\"optimizer\": opt2, \"lr_scheduler\": schdlr_2, \"frequency\": self.hparams.frequency_2},\r\n    )\r\n```\r\nHowever, inspecting the loss over the training steps revealed that only one learning rate was updated:\r\n\r\n![lr_scheduler](https://user-images.githubusercontent.com/11181748/193428975-fdd09902-ecd5-4277-aa4e-db17505f7dd3.png)\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14970",
    "createdAt": "2022-10-01T21:31:52Z",
    "updatedAt": "2022-10-03T13:37:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "celsofranssa"
    },
    "answer": {
      "body": "It happend because I've forgotten about specifying the scheduler's interval.\r\nThe correct desired config is something like:\r\n\r\n```python\r\nreturn (\r\n            {\"optimizer\": opt_1, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-1\"}, \"frequency\": 1},\r\n            {\"optimizer\": opt_2, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-2\"}, \"frequency\": 1}\r\n)\r\n```",
      "author": {
        "login": "celsofranssa"
      },
      "createdAt": "2022-10-03T13:36:40Z"
    }
  },
  {
    "title": "Access best checkpoint when using early stopping",
    "body": "I use the early stopping callback like this \r\n\r\n```python\r\n    trainer = pl.Trainer(\r\n        strategy=strategy,\r\n        accelerator=accelerator,\r\n        devices=devices,\r\n        deterministic=True,\r\n        max_epochs=max_epochs,\r\n        callbacks=[EarlyStopping(monitor=\"rougeL_fmeasure\", mode=\"max\", patience=5)],\r\n    )\r\n```\r\n\r\nBut this just creates a single checkpoint like this `'epoch=8-step=4500.ckpt'` epoch 8 is when the training was stopped, and epoch 3 is actually the best epoch. How can I access the checkpoint with the best metric?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14931",
    "createdAt": "2022-09-29T00:02:28Z",
    "updatedAt": "2022-10-08T16:18:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vikigenius"
    },
    "answer": {
      "body": "Hello @vikigenius,\r\n\r\nI usually specify how my PL model checkpoints, as shown in the code snippet below:\r\n```python\r\ntrainer = pl.Trainer(\r\n            [...] # remain trainer params\r\n            callbacks=[\r\n\r\n                self.get_model_checkpoint_callback(),  # checkpoint_callback\r\n                self.get_early_stopping_callback(),  # early_stopping_callback\r\n                [...] # other callbacks\r\n            ],\r\n            deterministic=True\r\n        )\r\n\r\n\r\n\r\ndef get_model_checkpoint_callback(self):\r\n    return ModelCheckpoint(\r\n        monitor=self.params.val_metric,\r\n        dirpath=self.params.model_checkpoint.dir,\r\n        filename=f\"{self.params.model.name}_{self.params.data.name}_{self.params.fold}\",\r\n        save_top_k=self.params.save_top_k, # the best k models according to the monitor\r\n        save_weights_only=self.params.save_weights_only,\r\n        mode=self.params.mode\r\n    )\r\n\r\n\r\ndef get_early_stopping_callback(self,):\r\n    return EarlyStopping(\r\n        monitor=self.params.val_metric,\r\n        patience=self.params.trainer.patience,\r\n        min_delta=self.params.trainer.min_delta,\r\n        mode=self.params.mode\r\n    )\r\n```\r\n\r\nI hope this could help you.",
      "author": {
        "login": "celsofranssa"
      },
      "createdAt": "2022-10-08T13:05:59Z"
    }
  },
  {
    "title": "Batch size mismatch during loss computation",
    "body": "Hello, \r\n\r\nI have a pre-trained model that I had trained from scratch using Densenet Architecture with size output  2048. \r\nI am trying to use my pretrained model for finetuning/transfer learning on a downstream task.\r\nHowever, I keep having size mismatch issues at the loss computation line and loss explosion. I am thinking it has something to do with batch size not in tune with each other.\r\nPlease see a minimal reproduction of my code below:\r\n\r\n\r\n```python\r\nclass DownstreamTask(pl.LightningModule):\r\n    def __init__(self, pre_model, lr=LR):\r\n        super().__init__()\r\n        self.network = pre_model\r\n        self.fc = nn.Sequential(nn.Linear(2048,22)) \r\n        self.learning_rate = lr\r\n\r\n    def forward(self, x):\r\n        features = self.gaze_network(x)\r\n        features = features.view(features.size(0), -1)\r\n        gaze = self.fc(features)\r\n        return gaze\r\n\r\n    def training_step(self, batch):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.l1_loss(y_hat, y)\r\n        self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n        return {'loss': loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n\r\nmodel = DenseNet()\r\nmodel.load_state_dict(torch.load(PATH))\r\nmodel.eval()\r\n\r\ntrain_loader = DataLoader(TrainLoader(data_dir, batch_size, num_workers), batch_size=batch_size, shuffle=True, num_workers=int(num_workers))\r\n\r\nlearner = learner = DownstreamTask(model)\r\ntrainer = pl.Trainer(accelerator='gpu', devices=num_gpus, max_epochs=epochs, strategy='ddp', num_nodes=num_nodes,)\r\n\r\ntrainer.fit(learner, train_loader)\r\n```\r\n\r\nHere is the error I got:\r\n\r\n```\r\n/trainer.py:44: UserWarning: Using a target size (torch.Size([10, 2])) that is different to the input size (torch.Size([490, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\r\n  loss = F.l1_loss(y_hat, y)\r\n\r\nFile \"/usersDownstream/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 79, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"/users/trainer.py\", line 44, in training_step\r\n    loss = F.l1_loss(y_hat, y)\r\n  File \"/users/Downstream/lib/python3.10/site-packages/torch/nn/functional.py\", line 3248, in l1_loss\r\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\r\n  File \"/users/Downstream/lib/python3.10/site-packages/torch/functional.py\", line 73, in broadcast_tensors\r\n    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\r\nRuntimeError: The size of tensor a (490) must match the size of tensor b (10) at non-singleton dimension 0\r\n```\r\n\r\nI have tried to resize/reshape but nothing seems to work. It looks like there is a batch size mismatch. \r\nI am seriously confused, I will appreciate any help. Many thanks.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14812",
    "createdAt": "2022-09-20T18:21:21Z",
    "updatedAt": "2022-09-30T07:56:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Estabi"
    },
    "answer": {
      "body": "@Estabi I believe this is irrelevant to PL. Have you checked the sizes of `y_hat` and `y` match? See PyTorch docs:\u3000https://pytorch.org/docs/1.12/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-09-27T08:34:41Z"
    }
  },
  {
    "title": "Adversarial training with lightning",
    "body": "Hello!\r\n\r\nI'm attempting to do some simple adversarial training with lightning but I'm running in some issues for the testing part.\r\nMy model is a lightning module with a base model and an attack model (from torchattacks).\r\nDuring adversarial training adversarial images are first computed with the attack and then used for the training.\r\nDuring validation and test both clean and adversarial images are used.\r\nSince gradients are required to compute adversarial images I simply enable gradients in validation_step / testing_step:\r\n```python\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n```\r\n\r\nThis works fine during training (when doing trainer.fit(model), but fails during testing (trainer.test(model)), with \r\n```python\r\n\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\n\r\nI checked similar problems and the solutions was to enable gradients (which I did) or remove automatic optimization but I would like to retain the possibility of accumulating gradients, and since trainer.fit works fine I don't see why I would need to do manual optimization.\r\n\r\nI did some digging and the enabling of gradients seems to work:\r\nAll the model parameters.requires_grad and adv_images.requires_grad are True in the attack code:\r\n[https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgd.py](https://github.com/Harry24k/adversarial-attacks-pytorch/blob/712144154f580ff8a1f492bde6c154f14e8556a6/torchattacks/attacks/pgd.py#L62)\r\nbut outputs.requires_grad is False. even if I had a second (probably redundant) with torch.enable_grad() in the attack when computing outputs.\r\nUsing torch.set_grad_enabled(True) does not change this.\r\n\r\n\r\nSame thing happens if I do trainer.validate(model). Also when using other attacks than PGD.\r\n\r\nAny idea why is that and how I can fix it?\r\nWhat is intriguing is that it works when doing trainer.fit...\r\n\r\nReproducible in [colab](https://colab.research.google.com/drive/1GKriw-4OB0vNndW2pmb8Man-zMvL0FzV#scrollTo=LU6Nj3Vh_KeS) \r\n\r\nOr full script to reproduce:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom torch import nn\r\nimport torch\r\nfrom torchvision.datasets import MNIST\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.transforms as T\r\nimport torchattacks\r\n\r\nclass adv_model(pl.LightningModule):\r\n    def __init__(self,\r\n                 model,\r\n                 attack=None,\r\n                 loaders=None,\r\n                 loss_fn=nn.CrossEntropyLoss(),\r\n                 optim=\"AdamW\",\r\n                 clean=False,\r\n                 lr=0.01\r\n                 ):\r\n        super().__init__()\r\n        self.model = model\r\n        self.loss_fn = loss_fn\r\n        self.loaders = loaders\r\n        self.atk = attack\r\n        self.clean = clean\r\n        self.lr = lr\r\n        if optim is None:\r\n            self.optim = torch.optim.AdamW\r\n        elif optim == \"AdamW\":\r\n            self.optim = torch.optim.AdamW\r\n        elif optim == \"Adam\":\r\n            self.optim = torch.optim.Adam\r\n        elif optim == \"SGD\":\r\n            self.optim = torch.optim.SGD\r\n        else:\r\n            raise ValueError(f\"Optim should be in '[AdamW, Adam, SGD]', not {optim}\")\r\n\r\n    def forward(self, x, clean=None):\r\n        return self.model(x)\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        imgs, labels = batch\r\n        if not self.clean:\r\n            imgs = self.atk(imgs, labels)\r\n        logits = self.model(imgs)\r\n        loss = self.loss_fn(logits, labels)\r\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\r\n        acc = (logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\r\n\r\n        return {\"loss\": loss, \"acc\": acc}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        imgs, labels = batch\r\n\r\n        clean_logits = self.model(imgs)\r\n        clean_loss = self.loss_fn(clean_logits, labels)\r\n        clean_acc = (clean_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log(\"clean_val_loss\", clean_loss, prog_bar=True)\r\n        self.log('clean_val_acc', clean_acc, prog_bar=True)\r\n        if self.clean:\r\n            return clean_loss, clean_acc\r\n        # computing adversarial accuracy and loss\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n        adv_logits = self.model(adv_img)\r\n        adv_loss = self.loss_fn(adv_logits, labels)\r\n        self.log(\"adv_val_loss\", adv_loss, prog_bar=True)\r\n        adv_acc = (adv_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log('adv_val_acc', adv_acc, prog_bar=True)\r\n\r\n        return clean_loss, clean_acc, adv_loss, adv_acc\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        imgs, labels = batch\r\n\r\n        clean_logits = self.model(imgs)\r\n        clean_loss = self.loss_fn(clean_logits, labels)\r\n        clean_acc = (clean_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n\r\n        self.log(\"clean_test_loss\", clean_loss, prog_bar=True)\r\n        self.log('clean_test_acc', clean_acc, prog_bar=True)\r\n        if self.clean:\r\n            return clean_loss, clean_acc\r\n        # computing adversarial accuracy and loss\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n        adv_logits = self.model(adv_img)\r\n        adv_loss = self.loss_fn(adv_logits, labels)\r\n        self.log(\"adv_test_loss\", adv_loss, prog_bar=True)\r\n        adv_acc = (adv_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log('adv_test_acc', adv_acc, prog_bar=True)\r\n\r\n        return clean_loss, clean_acc, adv_loss, adv_acc\r\n\r\n    def configure_optimizers(self):\r\n        optim = self.optim\r\n        if issubclass(optim, torch.optim.SGD):\r\n            if self.lr is not None:\r\n                return optim(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=1e-4)\r\n            else:\r\n                return optim(self.model.parameters(), momentum=0.9, weight_decay=1e-4)\r\n        elif issubclass(optim, (torch.optim.Adam, torch.optim.AdamW)):\r\n            if self.lr is not None:\r\n                return optim(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\r\n            else:\r\n                return optim(self.model.parameters(), weight_decay=1e-4)\r\n        else:\r\n            return self.optim\r\n\r\n    def train_dataloader(self):\r\n        return self.loaders[0]\r\n\r\n    def val_dataloader(self):\r\n\r\n        return self.loaders[1]\r\n\r\n    def test_dataloader(self):\r\n        return self.loaders[2]\r\n\r\n\r\ntrainer = pl.Trainer(accelerator=\"gpu\",\r\n                     max_epochs=3,\r\n                     val_check_interval=1.0,\r\n\r\n                     )\r\n\r\nbase_model = torch.nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),\r\n                                 nn.Linear(256, 256), nn.ReLU(),\r\n                                 nn.Linear(256, 10))\r\n\r\ntrain_set = MNIST(root=\"./\",\r\n                  transform=T.ToTensor(),\r\n                  download=True,\r\n                  train=True\r\n                  )\r\n\r\ntest_set = MNIST(root=\"./\",\r\n                 transform=T.ToTensor(),\r\n                 download=True,\r\n                 train=False\r\n                 )\r\n\r\ntrain_loader = DataLoader(train_set, batch_size=100, shuffle=True, num_workers=2)\r\ntest_loader = DataLoader(test_set, batch_size=1000, shuffle=False, num_workers=2)\r\nval_loader = DataLoader(test_set, batch_size=1000, shuffle=False, num_workers=2)\r\nloaders = (train_loader, val_loader, test_loader)\r\n\r\natk = torchattacks.PGD(model=base_model.cuda(), steps=10)\r\nmodel = adv_model(base_model,\r\n                  loaders=loaders,\r\n                  attack=atk,\r\n                  clean=False,\r\n                  optim=\"Adam\")\r\n\r\n\r\ntrainer.fit(model)\r\n\r\ntrainer.test(model)\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14782",
    "createdAt": "2022-09-19T14:17:00Z",
    "updatedAt": "2023-11-28T14:59:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sergedurand"
    },
    "answer": {
      "body": "Answering to myself:\r\nAfter more digging, it seems that it is the use of torch.inference_mode that is the cause of the issue.\r\nUsing torch.no_grad is not enough to get out of inference_mode.\r\nIn fact getting out of inference_mode with e.g with torch.inference_mode(mode=False) or a decorator is not enough, I then have a problem\r\n with Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.\r\n \r\n For now the solution I have is to change the function\r\n ```python\r\n @contextmanager\r\ndef _evaluation_context(accelerator: Accelerator) -> Generator:\r\n    # inference mode is not supported with gloo backend (#9431),\r\n    # and HPU & TPU accelerators.\r\n    context_manager_class = (\r\n        torch.inference_mode\r\n        if not (dist.is_initialized() and dist.get_backend() == \"gloo\")\r\n        and not isinstance(accelerator, HPUAccelerator)\r\n        and not isinstance(accelerator, TPUAccelerator)\r\n        else torch.no_grad\r\n    )\r\n    with context_manager_class():\r\n        yield\r\n```\r\nto always use torch.no_grad (l2794 in trainer.py). \r\nor use older pytorch/lightning versions...\r\n\r\nIf there is a simple alternative to use in the test_step or some parameters to force the use of no_grad instead of inference_mode I'm all ears.",
      "author": {
        "login": "sergedurand"
      },
      "createdAt": "2022-09-19T14:44:45Z"
    }
  },
  {
    "title": "Passing transforms to datamodule from LightningCLI example",
    "body": "Would appreciate if someone has an example of passing a list of transforms with arguments to a Lightning DataModule from the LightningCLI.\r\n\r\nI'm thinking of a syntax similar to [this](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type) where one can pass multiple callables from the CLI. \r\n\r\nEDIT: I managed to pass a single transform by adding the `transforms: Optional[Callable] = None,` typing hint to my datamodule `__init__()` but I get an unrecognized argument error when using the `data.transforms+=my.transforms.Transform` syntax in the CLI.\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14607",
    "createdAt": "2022-09-08T13:06:37Z",
    "updatedAt": "2022-10-20T02:19:50Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cgoliver"
    },
    "answer": {
      "body": "You should use list, for more details, see https://jsonargparse.readthedocs.io/en/stable/#type-hints",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-09-08T14:48:05Z"
    }
  },
  {
    "title": "Question about logging metric object with `on_step=True`",
    "body": "I have a hard time understanding how logging value through metric object works when using `on_step=True`.\r\n\r\nI have these metrics in my lightning module:\r\n```python\r\n# for calculating and averaging accuracy across batches\r\nself.train_acc = Accuracy()\r\n\r\n# for averaging loss across batches\r\nself.train_loss = MeanMetric()\r\n```\r\n\r\nI'm logging metrics on step like this:\r\n```python\r\ndef training_step(self, batch: Any, batch_idx: int):\r\n    loss, preds, targets = self.step(batch)\r\n\r\n    # update metrics\r\n    self.train_loss(loss)\r\n    self.train_acc(preds, targets)\r\n    \r\n    self.log(\"train/loss\", self.train_loss, on_step=True, on_epoch=False, prog_bar=True)\r\n    self.log(\"train/acc\", self.train_acc, on_step=True, on_epoch=False, prog_bar=True)\r\n```\r\n\r\nQuestion: I'm seeing metric values on the progress bar on each step. Are these values actually **accuracy and loss only from current step**, or maybe these values are **average loss and average accuracy from all steps so far** in current epoch?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14590",
    "createdAt": "2022-09-07T22:00:43Z",
    "updatedAt": "2022-09-08T15:09:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ashleve"
    },
    "answer": {
      "body": "> Question: I'm seeing metric values on the progress bar on each step. Are these values actually **accuracy and loss only from current step**,\r\n\r\nYes, the value of `train/loss=...` shown in the progress bar is calculated based on each step. However, note that the value of `loss=...` (shown by default) at the very left of your progress bar (not `train/loss=...`) is smoothened across 20 steps. (Related to this RFC #9372)",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-09-08T00:50:37Z"
    }
  },
  {
    "title": "Save wandb config file as hyperparameter",
    "body": "I was wondering if there is some way to save a WandB config file using the `save_hyperparameters()` call in the Trainer. I am using WandB for logging purposes and to perform parameter sweeps etc. I initialize a WandB run with a config file, which is passed on to the Trainer. And I would like to save that config to the checkpoints I save using ModelCheckpoint callback. I know WandB saves the config that is used for a certain run with it in the same folder, but I would like them to be in the checkpoint as well. Is there some way to do this? Because it looks like the WandB config isn't allowed in the `save_hyperparameter()` at the moment...\r\nSo shortened in pseudo code I have something like this:\r\n\r\nA main file with:\r\n```\r\ndef main(default_config): # default_config is read from a file\r\n    ...\r\n    wandb.init(config=default_config)\r\n    ...\r\n    config = wandb.config\r\n    ...\r\n    model = LitTrainer(netG=generator, netD=discriminator, config=config)\r\n    trainer = pl.Trainer(...)\r\n    trainer.fit(model)\r\n```\r\n\r\nAnd then a trainer file with:\r\n```\r\nclass LitTrainer(pl.LightningModule):\r\n    def __init__(self,\r\n                 netG, netD,\r\n                 config,\r\n                 ):\r\n    super().__init__()\r\n    self.save_hyperparameters(config)\r\n    ...\r\n```\r\nAnd in the rest of the trainer all kinds of parameters are an attribute of that config. But when I try to save that config using the `save_hyperparameters`, it returns the following: `ValueError: Unsupported config type of <class 'wandb.sdk.wandb_config.Config'>.`\r\nAnd first trying to convert the config back to a dict before passing it to `save_hyperparameters` doesn't seem to work as well.\r\nDoes anyone know how to do this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14561",
    "createdAt": "2022-09-06T14:24:51Z",
    "updatedAt": "2022-09-07T13:41:21Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rienboonstoppel"
    },
    "answer": {
      "body": "Apparently it has something to do with the config from WandB. The issue is that WandB config is a managed object so it really can't cleanly be deepcopy'ed. Using the static_config from WandB is serializable and works just fine!",
      "author": {
        "login": "rienboonstoppel"
      },
      "createdAt": "2022-09-07T13:41:17Z"
    }
  },
  {
    "title": "How to pass a type(not an object) to a class with lightningcli?",
    "body": "I want to pass a type(not an object) to a subclass of LightningDataModule.  I did the following, but it seemed lightningcli tried to initialize an object and complain required fileds were missing. How can I do that?\r\n\r\n```\r\nclass TxtDataModule(LightningDataModule):\r\n    def \\__init\\__(\r\n        self,\r\n        domain_dataset: Dataset,\r\n        ...\r\n    ):\r\n       dataset_obj = domain_dataset(....)\r\n       ...\r\n```\r\n\r\nin yaml\r\n```\r\ndata:\r\n  class_path: src.training.data_module.data_module_txt.TxtDataModule\r\n  init_args:\r\n    domain_dataset: src.training.semantic_sim.dataset_txt.TxtDataset_LoadFromFile  # a subclass of Dataset\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14552",
    "createdAt": "2022-09-06T06:20:10Z",
    "updatedAt": "2022-09-07T02:05:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "calvinzhan"
    },
    "answer": {
      "body": "You can change `domain_dataset` type hint from `Dataset` to `Type`:\r\n\r\nHere is a demo:\r\n```python\r\nfrom typing import Type\r\nfrom jsonargparse import CLI\r\n\r\nclass Class1:\r\n    def __init__(self) -> None:\r\n        print(\"Class1\")\r\n\r\nclass Class2:\r\n    def __init__(self) -> None:\r\n        print(\"Class2\")\r\n\r\ndef test_fn(\r\n    class_type: Type = Class1,\r\n):\r\n    print(class_type)\r\n    class_type()\r\n\r\nif __name__ == \"__main__\":\r\n    CLI(test_fn)\r\n```\r\n\r\n```console\r\npython test.py --class_type '__main__.Class2'\r\n```",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-09-06T07:14:17Z"
    }
  },
  {
    "title": "How to pass in a function with lightningcli configration?",
    "body": "Suppose I have a subclass of LightningDataModule\r\n\r\nclass TxtDataModule(LightningDataModule):\r\n    def \\__init\\__(\r\n        self,\r\n        line_processor,  # line_processor is a function \r\n        ...\r\n    ):\r\n       line_processor(....)\r\n       ....\r\n\r\nHow can I configure TxtDataModule to pass in a function in yaml configuration?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14530",
    "createdAt": "2022-09-04T13:36:27Z",
    "updatedAt": "2022-09-06T06:20:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "calvinzhan"
    },
    "answer": {
      "body": "You may find [this](https://github.com/Lightning-AI/lightning/discussions/13613#discussioncomment-3174735) helpful.",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-09-05T02:10:57Z"
    }
  },
  {
    "title": "In Manual Optimization mode, loss does not converge when the Train Batch Size is 1",
    "body": "This is very confusing that if the train batch size is set to 1 and Automatic Optimization mode is used, accumulate_grad_batches=32, train loss can converge normally. However, using Manual Optimization mode with automatic_optimization=False, the train loss fails to converge. Is there any other code I'm ignoring in PyTorch Lightning?\r\n \r\nThe code used is as follows:\r\n```python\r\n     def training_step(self, batch, batch_idx, optimizer_idx=None):\r\n            opt = self.optimizers()\r\n            loss = gen_helper()\r\n            self.log('train/loss', loss, prog_bar=True, logger=True)\r\n            self.manual_backward(loss)\r\n            if (batch_idx+1) % self.hparams.accumulate_grad_batches == 0:\r\n                opt.step()\r\n                opt.zero_grad()\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14526",
    "createdAt": "2022-09-04T04:22:10Z",
    "updatedAt": "2022-09-06T00:38:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "OPilgrim"
    },
    "answer": {
      "body": "@OPilgrim Your `training_step` looks good to me.\r\n\r\nHowever, please note that your logged value `\"train/loss\"` is being logged regardless of whether the weights are being updated in the iteration, which will make it look like a convergence issue.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-09-04T12:56:45Z"
    }
  },
  {
    "title": "Isn't here duplicate computation for gan example?",
    "body": "Hi,\r\nIt seems there is duplicate computation for [gan example](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/basic-gan.html).\r\nOne here:\r\n```python\r\n# train generator\r\n        if optimizer_idx == 0:\r\n\r\n            # generate images\r\n            self.generated_imgs = self(z) ## \uff0c<-- first\r\n```\r\nThe other here:\r\n```\r\n# train discriminator\r\n        if optimizer_idx == 1:\r\n....\r\n\r\nfake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake) <--- repeat\r\n```\r\n\r\nAnd also the sampling seems also duplicate:\r\n```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        imgs, _ = batch\r\n\r\n        # sample noise\r\n        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\r\n        z = z.type_as(imgs)\r\n```\r\nI do think this is unreasonable.\r\n\r\nSo how to avoid this unnecessary computation in lightning, I know there are some tricks to achieve, such as using a `self.some_state` to hold the values and the reuse them. But they seems no elegant. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14506",
    "createdAt": "2022-09-03T07:41:14Z",
    "updatedAt": "2022-09-03T11:16:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "wztdream"
    },
    "answer": {
      "body": "@wztdream Yes, it is redundant. I see a few options:\r\n\r\n1. Store the computed result (the noise and generated images) in some attribute and reuse it for the next optimizer indexed `optimizer_idx==1`.\r\n2. Use manual optimization.\r\n   - Docs: https://pytorch-lightning.readthedocs.io/en/stable/model/manual_optimization.html?highlight=manual%20optimization#manual-optimization\r\n   - Example: https://github.com/akihironitta/gist/blob/5f29d7aa8712e2f16c929b1384b555dd2e53ae9f/pl_gan_manual_optimization/main.py\r\n3. (Use the loop API. There's an example of yielding training step using the loop API: https://github.com/Lightning-AI/lightning/blob/master/examples/pl_loops/yielding_training_step.py, but I'd recommend the first two above because this one needs a bit more work.)",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-09-03T08:57:44Z"
    }
  },
  {
    "title": "Using data from training_epoch_end in validation_epoch_end.",
    "body": "I'm running a metric learning model, and I'd like to use embeddings from the training step for KNN lookups in the validation step. I tried to do so with the following code. \r\n\r\n```\r\ndef training_epoch_end(self, outputs):\r\n        embeddings = []\r\n        for output in outputs:\r\n            embeddings.append(output[\"embeddings\"])\r\n        self.training_embeddings = torch.cat(embeddings, dim=0)\r\n        print(\"Finished training epoch\")\r\n        return super().training_epoch_end(outputs)\r\n\r\ndef validation_epoch_end(self, outputs):\r\n      train_embeddings = self.train_embeddings\r\n      #Use train embeddings...\r\n      self.train_embeddings = None #Delete embeddings for future use.\r\n```\r\n\r\nHowever, self.train_embeddings is either None, or uninitialized, when validation_epoch_end executes. Is there a way to communicate between these two methods, or send information from both to a final callback? \r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14491",
    "createdAt": "2022-09-01T23:51:37Z",
    "updatedAt": "2022-11-08T05:22:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "amdson"
    },
    "answer": {
      "body": "Just saw other answers explaining the order of train_epoch_end and val_epoch_end hooks. This issue was solved by moving any code needing both validation and training data to \"on_train_epoch_end\". ",
      "author": {
        "login": "amdson"
      },
      "createdAt": "2022-09-02T00:14:33Z"
    }
  },
  {
    "title": "How to switch data loaders between epochs while using multiple optimizer",
    "body": "Hi,\r\n\r\nI have two datasets and each dataset will have a corresponding optimizer.\r\n\r\nRight now I am updating the model parameters per batch per dataset. But how can I iterate `datasetA` for an epoch first and then iterate an epoch for `datasetB`? \r\n\r\n- Train an epoch on the datasetA using `optimizer A`\r\n- Train an epoch on the datasetB using `optimizer B`\r\n\r\nThis is what I have for iterating multiple datasets per batch.\r\n```python\r\nclass Mymodel(LightningModule):\r\n  def configure_optimizers(self):\r\n      optimizer = optim.Adam(\r\n          filter(\r\n              lambda p: p.requires_grad,\r\n              self.parameters()),\r\n          lr=self.lr,)\r\n      scheduler = optim.lr_scheduler.ReduceLROnPlateau(\r\n          optimizer,\r\n      )\r\n      scheduler_config = {\r\n          \"scheduler\": scheduler,\r\n          \"monitor\": \"val/rec/epoch_loss\",\r\n          \"interval\": \"epoch\",\r\n          \"frequency\": 1,\r\n      }\r\n      pre_optimizer = optim.SGD(\r\n            filter(\r\n                lambda p: p.requires_grad,\r\n                self.parameters()),\r\n            lr=self.lr_for_other_modules,)\r\n      optimizers = [optimizer, pre_optimizer]\r\n      return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\r\n  \r\n  def training_step(self, batch, batch_idx, optimizer_idx):\r\n       if optimizer_idx == 0:\r\n          # do forward on datasetA\r\n          inputs = batch[\"datasetA\"]\r\n        if self.is_use_some_modules and optimizer_idx == 1:\r\n          # do something else \r\n          inputs = batch[\"datasetB\"]\r\n        return ...\r\n\r\nclass Datamodule(LightningDataModule):\r\n  def train_dataloader(self):\r\n      ...\r\n      return {\"datasetA\": A_loader, \"datasetB\": B_loader}\r\n\r\n```\r\n\r\nI found a similar question here #3336 and the suggested solution is reloading data loaders every epoch. \r\nI wonder if it is safe to do this as I have multiple optimizers.\r\n\r\n```python\r\nclass Datamodule(LightningDataModule):\r\n  def train_dataloader(self):\r\n      if self.current_epoch % 2 == 0:\r\n          A_loader = ...\r\n          return A_loader\r\n      else:\r\n          B_loader = ...\r\n          return B_loader\r\n\r\nclass Mymodel(LightningModule):\r\n  def training_step(self, batch, batch_idx, optimizer_idx):\r\n      if self.current_epoch % 2 == 0 and optimizer_idx == 0:\r\n           inputs = batch[\"datasetA\"]\r\n      elif self.current_epoch % 2 == 1 and optimizer_idx == 1:\r\n          inputs = batch[\"datasetB\"]\r\n      return ...\r\n``` \r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14420",
    "createdAt": "2022-08-27T12:21:14Z",
    "updatedAt": "2022-08-29T09:14:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "icedpanda"
    },
    "answer": {
      "body": "one simple solution is to use `reload_dataloaders_every_epoch=1` and configure `training_step` based on `current_epoch`.\r\n\r\n```py\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n    if (self.current_epoch % 2 == 0 and optimizer_idx == 0) or (self.current_epoch % 2 == 1 and optimizer_idx == 1):\r\n        ...\r\n        return loss\r\n```\r\n\r\nin other cases of `training_step`, it will return `None`, which will not make any updates to the other optimizer. You might get a warning, but that's fine.\r\nyour solution is fine as well, but it will load the data from both the dataloader which is not required in each epoch.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-28T17:29:16Z"
    }
  },
  {
    "title": "Segmentation Fault While Import pytorch_lightning",
    "body": "![image](https://user-images.githubusercontent.com/29114869/187025003-ffbc1742-7c8d-4635-9a37-d26842e0b0eb.png)\r\npytorch-lightning             1.7.3",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14419",
    "createdAt": "2022-08-27T09:52:12Z",
    "updatedAt": "2022-08-27T10:55:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "For anyone seeing this issue, see #11663.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-08-27T10:55:39Z"
    }
  },
  {
    "title": "Proper way to do contrastive learning with DDP & PT-Lightning",
    "body": "I want to use DDP and experiment with contrastive losses. Since DDP processes each subset of the data independently, negative examples that could be used to increase the contrastive power cannot be taken into account using automatic optimization. Suppose I am training with 2 GPU's and each GPU sees a mini-batch of size 4. This leads to missing signal between (x1, x5), (x1, x6), (x1, x7), etc... since x1-x4 are on GPU1 and x5-x8 are on GPU2. \r\n\r\nWhat is the recommended method to account for this in PT-Lightning?\r\nOne approach seems to be to use the `on_train_batch_end()` callback, and 1) gather the outputs from all GPUs, 2) compute the loss on rank=0, and 3) distribute that loss back to each GPU.\r\n\r\nAfter computing loss, I'm unclear as to the mechanics for how to distribute that loss computed on rank=0 back to all of the GPU's so that the gradients are synced. Is this something that happens automatically under the hood, or do I need to do something w.r.t. manual optimization?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14390",
    "createdAt": "2022-08-25T15:50:33Z",
    "updatedAt": "2024-04-26T01:28:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kkarrancsu"
    },
    "answer": {
      "body": "@kkarrancsu You are definitely on the right track here. In the LightningModule, you have [this method](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=all_gather#all-gather) for gathering a tensor from all processes:\r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor)\r\n```\r\n\r\nWhat you want is to back-propagate through this all_gather function, and this is possible if you set \r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor, sync_grad=True)\r\n```\r\n\r\nIn your case, your `training_step` method could look something like this:\r\n```py\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        outputs = self(batch)\r\n        ...\r\n\r\n        all_outputs = self.all_gather(outputs, sync_grads=True)\r\n\r\n        loss = contrastive_loss_fn(all_outputs, ...)\r\n        return loss\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2022-08-28T13:13:50Z"
    }
  },
  {
    "title": "`RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location`",
    "body": "RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.\r\n\r\nmy model is transformer.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14377",
    "createdAt": "2022-08-24T01:27:09Z",
    "updatedAt": "2024-01-08T22:51:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "fine i solved this question\r\n\r\nthis is the solution:\r\n\r\nchange PositionalEncoding to this:\r\n\r\n```Ptyhon\r\nclass PositionalEncoding(nn.Module):\r\n\r\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n        super(PositionalEncoding, self).__init__()\r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        pe = torch.zeros(max_len, d_model)\r\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n        pe[:, 0::2] = torch.sin(position * div_term)\r\n        pe[:, 1::2] = torch.cos(position * div_term)\r\n        pe = pe.unsqueeze(0).transpose(0, 1)\r\n        self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))\r\n\r\n    def forward(self, x):\r\n        x = x + self.pe[:x.size(0), :]\r\n        return self.dropout(x)\r\n```",
      "author": {
        "login": "morestart"
      },
      "createdAt": "2022-08-24T01:29:49Z"
    }
  },
  {
    "title": "Is it possible to pass arguments to callbacks in the LightningCLI?",
    "body": "For example, if I wanted to set the option for `write_interval` in the [BasePredictionWriter](https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/callbacks/prediction_writer.html#BasePredictionWriter) from the LightningCLI's `trainer.callbacks` flag, would there be a syntax for this? As far as I can tell, one can only pass the name of the callback class.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14339",
    "createdAt": "2022-08-22T06:39:11Z",
    "updatedAt": "2022-09-08T13:22:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cgoliver"
    },
    "answer": {
      "body": "The syntax is described in https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type. As command line arguments would be:\r\n```shell\r\n$ python ... \\\r\n    --trainer.callbacks+=BasePredictionWriter \\\r\n    --trainer.callbacks.write_interval=batch \\\r\n    ...\r\n```\r\nOr in a config file:\r\n```yaml\r\ntrainer:\r\n  callbacks:\r\n    - class_path: BasePredictionWriter\r\n      init_args:\r\n        write_interval: batch\r\n```",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-08-22T19:46:01Z"
    }
  },
  {
    "title": "`ImportError: cannot import name 'Mapping' from 'collections'`",
    "body": "I cant import lightning package:  ImportError: cannot import name 'Mapping' from 'collections'. In my Python 3.10 Mapping accessible by 'collections.abc' i.e 'from collections.abc import Mapping' not 'from collections import Mapping'. May be  someone knows how fix this issue. Import lightining package trying run next command 'from collections import Mapping'",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14329",
    "createdAt": "2022-08-21T02:12:05Z",
    "updatedAt": "2022-08-22T09:34:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Mpotrue"
    },
    "answer": {
      "body": "Would you mind providing the full error message and your environment detail?",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-08-21T12:19:43Z"
    }
  },
  {
    "title": "Why does validation epoch end before train epoch?",
    "body": "It seems that validation_epoch_end gets called and only after that train_epoch_end gets called. Why is that?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14318",
    "createdAt": "2022-08-19T14:34:56Z",
    "updatedAt": "2022-08-22T10:02:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "march-o"
    },
    "answer": {
      "body": "When you run `trainer.fit(...)`, there're two calls to `validation_step()` followed by a call to `validation_epoch_end()` before any calls to `training_step()` by default.\r\n\r\nThe order of all hooks are documented at: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\r\n```python\r\n# the sanity check runs here <--- mind that here's **SANITY CHECK**\r\n\r\non_train_start()\r\nfor epoch in epochs:\r\n    fit_loop()\r\non_train_end()\r\n```\r\n\r\nYou can disable it by specifying `Trainer(num_sanity_val_steps=0)`: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/trainer.html#num-sanity-val-steps\r\n\r\n",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-08-20T09:49:28Z"
    }
  },
  {
    "title": "Where are model predictions saved by LightningCLI?",
    "body": "I am trying to understand how the `predict` subcommand works with the LightningCLI.\r\n\r\nAs far as I can tell, it would be calling the lightning module's `predict_step` method. However, I don't know what happens after this. I see there is a `return_predictions` flag but not sure where or whether the output is stored somewhere. I am not finding it in the checkpoint directory.\r\n\r\nThanks in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14316",
    "createdAt": "2022-08-19T12:37:38Z",
    "updatedAt": "2022-08-22T06:32:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cgoliver"
    },
    "answer": {
      "body": "Hi, the return of subcommand is not saved in LightningCLI.\r\nhttps://github.com/Lightning-AI/lightning/blob/0ca3b5aa1b16667cc2d006c3833f4953b5706e72/src/pytorch_lightning/cli.py#L623-L630\r\n\r\nYou can instance CLI with `run=False` or Hack the `LightningCLI` directly to make sure it return results.\r\n```python\r\ncli = LightningCLI(run=False)\r\ncli.trainer.predict(...)\r\n```",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-08-20T11:41:57Z"
    }
  },
  {
    "title": "Does LightningCLI support user-defined subcommands?",
    "body": "I would like to define some new subcommands along with the default ones and be able to attach some function calls to it. The documentation does not seem to show this use case so I'm wondering if this is possible.\r\n\r\nFor example, to visualize the output of some model.\r\n\r\n```\r\npython main.py visualize --checkpoint=/my/checkpoing.ckpt\r\n```\r\n\r\nOr would it be best to just customize the behaviour of some existing subcommands as described [here](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_expert.html#customize-the-lightningcli)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14314",
    "createdAt": "2022-08-19T10:48:17Z",
    "updatedAt": "2022-08-25T11:54:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cgoliver"
    },
    "answer": {
      "body": "To define new subcommands I would recommend the following. First you subclass `Trainer` and add methods for each of the subcommands you want. These new methods should be usable independent of the CLI. Then subclass `LightningCLI`, in the `__init__` make your new trainer the default and to add the subcommands simply override/extend https://github.com/Lightning-AI/lightning/blob/7a617ec90e1566c763be8ac7a200af1e4025412c/src/pytorch_lightning/cli.py#L425-L434",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-08-22T19:58:33Z"
    }
  },
  {
    "title": "Iterate over all validation data while using limit_val_batches",
    "body": "I have 10,000s of images for training a semantic segmentation model.\r\n\r\nI am therefore using `limit_val_batches` (along with `val_check_interval`) to limit the number of valuation batches per epoch\r\n\r\nI however note (i.e [Pytorch Lightning limit_val_batches and val_check_interval behavior - Stack Overflow](https://stackoverflow.com/questions/68658917/pytorch-lightning-limit-val-batches-and-val-check-interval-behavior)) that when using `limit_val_batches=N`, the first `N` batches from the dataloader are returned for each validation iteration. Training therefore only every sees the first \u2018N\u2019 validation batches when using `limit_val_batches` and never the rest that makes up the bulk of validation examples\r\n\r\nRather than using the same \u2018N\u2019 batches starting at index 0 for each validation epoch, I would like the dataloader when using `limit_val_batches` to sequentially chunk through all the validation data (e.g 0-19 batches the first epoch and then 20-39 the next... and so on)\r\n\r\nHow would I go about implementing this behavior with limit_val_batches? or is this not the expected thing to do?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14309",
    "createdAt": "2022-08-18T22:01:55Z",
    "updatedAt": "2022-08-31T20:25:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SPlanzer"
    },
    "answer": {
      "body": "Thanks @tshu-w. \r\n\r\nIn that case, what is the strategy when your validation dataset (or training set) has 10,000 plus images in it and you don't want validation to validate against all these images every epoch, but rather chunk through the dataset each epoch a small bit at a time in a sequential manner? ",
      "author": {
        "login": "SPlanzer"
      },
      "createdAt": "2022-08-19T03:13:30Z"
    }
  },
  {
    "title": "Why validation_step starts before training epoch ends?",
    "body": "As the pic is shown below, `validation_step` started before training epoch 0 ended.\r\n\r\n![image](https://user-images.githubusercontent.com/49632327/185279513-7f376958-79aa-45e4-b5ea-940623d9d88e.png)\r\n\r\nIt seems that the validation progress bar rolls forward together with the training progress bar. How could I separate them (i.e. the training epoch first ends, and then validation starts) ?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14269",
    "createdAt": "2022-08-18T02:31:20Z",
    "updatedAt": "2022-08-18T07:38:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MooreManor"
    },
    "answer": {
      "body": "Hi, the global process bar contains training + validation, this doesn't mean validation_step started before training epoch 0 ended.\r\nhttps://github.com/Lightning-AI/lightning/issues/12623",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-08-18T03:03:08Z"
    }
  },
  {
    "title": "About average in progress bar with on_epoch",
    "body": "`self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)`\r\nIf the calculated `loss` is averaged on a batch,\r\n\r\nthen, is `val_loss` with `on_epoch` an average of the averaged loss?\r\nSo, double-averaged? Or does it take `batch_size` into account for averaging?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14240",
    "createdAt": "2022-08-17T07:53:27Z",
    "updatedAt": "2025-01-09T17:51:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Nokimann"
    },
    "answer": {
      "body": "it takes batch_size into account for averaging.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-17T13:24:05Z"
    }
  },
  {
    "title": "TypeError: validation_step() missing 1 required positional argument: 'dataloader_nb'",
    "body": "```python\r\ndef validation_step(self, batch, batch_nb, dataloader_nb, vis=False, save=True, mesh_save_dir=None):\r\n```\r\n\r\nMy pytorch-lightning version is 1.6.0, but the code is written with pytorch-lightning 1.1.8. And when I ran the code, I met the bug `validation_step() missing 1 required positional argument: 'dataloader_nb'`.\r\n\r\nHow can I  fix it and pass parameters to `validation_step() `?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14238",
    "createdAt": "2022-08-17T07:27:46Z",
    "updatedAt": "2022-08-23T11:06:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MooreManor"
    },
    "answer": {
      "body": "if you are using a single val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx):\r\n```\r\n\r\nif you are using multiple val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx):\r\n```\r\n\r\nfor consistency, you can just do\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-17T13:26:46Z"
    }
  },
  {
    "title": "get same loss on different GPU device",
    "body": "Hi, \r\n\r\nI tried to set up my model training with 4 gpus using Lightning DDPPlugin. During my debugging process, I found that the data loaded are different on different devices, but a variable I printed out in the loss calculation seems to have the same value on different gpus devices (same up to 4 digits). Is this normal? If so, what is the reason? If not, what might go wrong with my network/dataloader? I really appreciate your comments and help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14194",
    "createdAt": "2022-08-13T00:47:37Z",
    "updatedAt": "2022-08-15T03:59:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dichencd"
    },
    "answer": {
      "body": "you can manually check out the targets and outputs used to compute the loss.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-14T17:57:03Z"
    }
  },
  {
    "title": "does each worker (num_workers>1) has its own instance of the Dataset object?",
    "body": "I am currently working on a code where the dataset has one property that\r\n\r\n1. has a custom Dataset which has an attribute at `self.items_served` that counts how many items have been served. It is initialized at 0 in the `init() `method and uptadted every time `get_item()` is called\r\n2. on my LightningModule's ` init()`, I initialize one instance of this custom Dataset and assign it to an attribute of the class: d= `Dataset(args)` and then `self.train_dataset=d`\r\n3. on my LightningModule's `train_dataloader()`, I return a dataloader which has` self.train_dataset` as dataset and numworkers = 10\r\n4. on my LightningModule's `training_step()`, I try to check the attribute at `self.train_dataset.items_served` and it is always 0, it does not update.\r\n\r\nHowever, everything works fine if I use only one worker (numworkers=0), so I guess I a doing something wrong  and/or I don't understand how dataloader works with multiple workers\r\n\r\nAny suggestion on how to tackle this problem?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14166",
    "createdAt": "2022-08-11T13:13:52Z",
    "updatedAt": "2022-08-17T10:29:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "It's because when you use `num_workers>  0`, the data is loaded, and the `collate_fn` is called inside the worker and that fetched data is pushed to the main process. So all the updates are happening inside the sub-worker process, but `training_step` runs on the main process, so no effect is reflected there.\r\n\r\npotential sol you can try: https://stackoverflow.com/questions/63460992/how-do-i-fix-the-dataset-to-return-desired-output-pytorch",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-14T18:09:17Z"
    }
  },
  {
    "title": "resume_form_checkpoint with different hyperparameters",
    "body": "Hi,\r\nI am training a classifier with a pretrained backbone. I froze the backbone and only trained a linear classifier for some epochs. Now I want to continue training with an unfrozen backbone and a smaller learning rate. \r\n\r\nCan this be done by just just using the `resume_from_checkpoint` argument in the trainer or will the encoder still be frozen?\r\nIf it is not possible what would the simplest way to achieve my goal be?\r\n\r\nThanks  ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14144",
    "createdAt": "2022-08-10T14:21:01Z",
    "updatedAt": "2022-08-16T11:26:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sirtris"
    },
    "answer": {
      "body": "> Can this be done by just just using the resume_from_checkpoint argument in the trainer or will the encoder still be frozen?\r\n\r\nno, since Trainer doesn't know what your backbone is and it doesn't freeze any module manually.\r\nyou can check out [BackboneFinetuning](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.BackboneFinetuning.html) for your usecase.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-16T07:34:49Z"
    }
  },
  {
    "title": "How to register a (repeatedly) sampled random tensor?",
    "body": "In my code, with each batch, I sample a random tensor. However, when I try porting to GPU, I get an error that I'm trying to multiply a tensor on CPU with a tensor on GPU. The [documentation](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/accelerator_prepare.html) states\r\n\r\n> The LightningModule knows what device it is on. You can access the reference via self.device. Sometimes it is necessary to store tensors as module attributes. However, if they are not parameters they will remain on the CPU even if the module gets moved to a new device. To prevent that and remain device agnostic, register the tensor as a buffer in your modules\u2019 __init__ method with register_buffer().\r\n\r\nThe example given is:\r\n\r\n```\r\nclass LitModel(LightningModule):\r\n    def __init__(self):\r\n        ...\r\n        self.register_buffer(\"sigma\", torch.eye(3))\r\n        # you can now access self.sigma anywhere in your module\r\n```\r\n\r\nUsing this example, suppose I want to randomly sample a 3x3 matrix `sigma` with each batch. How do I properly register this tensor?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/14131",
    "createdAt": "2022-08-10T02:49:42Z",
    "updatedAt": "2022-08-11T05:49:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "RylanSchaeffer"
    },
    "answer": {
      "body": "I just found this answer from 2020: https://stackoverflow.com/questions/63660624/normal-distribution-sampling-in-pytorch-lightning\r\n\r\nIs the best solution currently to specify the device?",
      "author": {
        "login": "RylanSchaeffer"
      },
      "createdAt": "2022-08-10T02:52:14Z"
    }
  },
  {
    "title": "Collective mismatch at end of training epoch",
    "body": "I\u2019m facing an issue where training a lightning module with DDP on >4 GPUs gets stuck at end of first training epoch (I made sure there is no validation epoch). This doesn\u2019t occur with 2 GPUs.\r\n\r\nI made sure that the dataset is balanced, and that the total batch size is equal to number of GPUs.\r\n\r\nDetecting unused parameters is on. There are unused parameters (and that\u2019s intentional).\r\n\r\nI obtained a stack traces with TORCH_CPP_LOG_LEVEL=INFO and TORCH_DISTRIBUTED_DEBUG=DETAIL.\r\n\r\nI\u2019m having difficulty understanding these stack traces, since they include >10 layers of PyTorch Lightning calls, and I don\u2019t have a good enough understanding of Lighting\u2019s internals. Perhaps someone can glance at this and get a sense for what are the top possible causes?\r\n\r\nStack trace from rank 7:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/solr/app/main_lightning.py\", line 164, in <module>\r\n    trainer.fit(model, datamodule=datamodule)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 294, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in _call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in on_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 381, in _save_topk_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 658, in _save_none_monitor_checkpoint\r\n    filepath = self._get_metric_interpolated_filepath_name(monitor_candidates, trainer)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 619, in _get_metric_interpolated_filepath_name\r\n    while self.file_exists(filepath, trainer) and filepath != del_filepath:\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 720, in file_exists\r\n    return trainer.strategy.broadcast(exists)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/ddp.py\", line 319, in broadcast\r\n    torch.distributed.broadcast_object_list(obj, src, group=_group.WORLD)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py\", line 1869, in broadcast_object_list\r\n    broadcast(object_sizes_tensor, src=src, group=group)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py\", line 1187, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\nRuntimeError: Detected mismatch between collectives on ranks. Rank 7 is running inconsistent collective: CollectiveFingerPrint(OpType=BROADCAST, TensorShape=[1], TensorDtypes=Long, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))\r\n```\r\n\r\nStack trace from rank 2 (ranks 0,1,3,4,5,6 are also similar):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/solr/app/main_lightning.py\", line 164, in <module>\r\n    trainer.fit(model, datamodule=datamodule)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\r\n    batch_output = self.batch_loop.run(batch, batch_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\r\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 203, in advance\r\n    result = self._run_optimization(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 256, in _run_optimization\r\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 369, in _optimizer_step\r\n    self.trainer._call_lightning_module_hook(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/lightning.py\", line 1646, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/optimizer.py\", line 168, in step\r\n    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/ddp.py\", line 286, in optimizer_step\r\n    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 193, in optimizer_step\r\n    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 155, in optimizer_step\r\n    return optimizer.step(closure=closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/rmsprop.py\", line 96, in step\r\n    loss = closure()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 140, in _wrap_closure\r\n    closure_result = closure()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 148, in __call__\r\n    self._result = self.closure(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 143, in closure\r\n    self._backward_fn(step_output.closure_loss)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 311, in backward_fn\r\n    self.trainer._call_strategy_hook(\"backward\", loss, optimizer, opt_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 168, in backward\r\n    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 80, in backward\r\n    model.backward(closure_loss, optimizer, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/lightning.py\", line 1391, in backward\r\n    loss.backward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 363, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 173, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: Detected mismatch between collectives on ranks. Rank 2 is running inconsistent collective: CollectiveFingerPrint(OpType=ALLREDUCE, TensorShape=[283125], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13997",
    "createdAt": "2022-08-03T05:12:48Z",
    "updatedAt": "2024-10-19T17:38:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "valtsblukis"
    },
    "answer": {
      "body": "Found the issue. Even with find_unused_parameters=True, there needs to be at least one used parameter every training step.\r\n\r\nI had a unique case where for some batches no parameters were used. This caused ranks to lose sync. My guess for why this happens is as follows: The ranks with used parameters would get stuck on allreduce in the backwards hook, waiting for the rank with no used parameters to catch up. However, the rank with no used parameters doesn't hit a backwards hook, and instead proceeds to the next training step, when it eventually joins up with the allreduce operation. Since it has now done one more training step than the other ranks, it will run out of data at the end of the epoch earlier. When this happens, it proceeds to save the model checkpoint, while the other ranks are still waiting on gradient allreduce.\r\n\r\nThe quick workaround was to add a dummy parameter to the model and simply return it in place of the loss in this special case.",
      "author": {
        "login": "valtsblukis"
      },
      "createdAt": "2022-08-08T20:20:07Z"
    }
  },
  {
    "title": "Change the scheduler interval in CLI",
    "body": "I just trying to use `StepLR`  in my `.yaml` config. \r\n```yaml\r\nlr_scheduler:\r\n  class_path: StepLR\r\n  init_args:\r\n    step_size : 10\r\n    gamma : 0.5\r\n```\r\nI guess the CLI will set the `interval` as \"epoch\", but I want to use the \"step\" to control my learning rate.  I do not find anything about it in the docs. How can achieve it just in my `.yaml` file?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13975",
    "createdAt": "2022-08-02T03:28:28Z",
    "updatedAt": "2022-08-03T12:26:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ForJadeForest"
    },
    "answer": {
      "body": "This is not possible from the yaml file. If you need to customize the learning rate scheduler configuration, you can do so by overriding:\r\n\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        return {\r\n            \"optimizer\": optimizer,\r\n            \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\"},\r\n        }\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-08-03T11:48:11Z"
    }
  },
  {
    "title": "Access model weights during training",
    "body": "Hello and thanks for this amazing library. \r\n\r\nWhen training models I find useful checking which parameters change after backpropagation. \r\n\r\nIn vanilla Pytorch I would do something like\r\n\r\n```\r\nparameters_before = model.parameters()\r\n\r\n...\r\n\r\nloss.backward()\r\noptimizer.step()\r\n...\r\nparameters_after = model.parameters()\r\n\r\n[print( (parameter_before != parameter_after).mean() ) for parameter_before, parameter_after\r\nin zip(parameters_before, parameters_after)]\r\n\r\n```\r\n\r\nAnd I should see a bunch of ones. Some zeroes can indicate branches of the network where the gradients are not flowing back. \r\n\r\nI was thinking of a callback that does the same within PL. After reading the docs, I have came up with this:\r\n\r\n```\r\nclass CheckParamsUpdatedCallback(Callback):\r\n    def __init__(self):\r\n        self.prms_for_check = dict.fromkeys(('before', 'after'))\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        self.prms_for_check['before'] = pl_module.parameters()\r\n\r\n    def on_validation_epoch_start(self, trainer, pl_module):\r\n        self.prms_for_check['after'] = pl_module.parameters()\r\n        for v in self.prms_for_check.values():\r\n            if v is None:  # handles validation sanity check\r\n                return None\r\n        print([(pbef != paft).float().mean().item()\r\n               for pbef, paft in zip(self.prms_for_check['before'], self.prms_for_check['after'])])\r\n```\r\n\r\n\r\nNevertheless, I believe that pl_module.parameters() is not returning the actual parameters used in the optimization, as I am always getting zeroes printed out, even if other indicators (loss/metrics) suggest the model is being trained well.\r\n\r\nHow can I access the model parameters during training process?\r\n\r\nThanks a lot and best wishes,\r\n\r\nVictor",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13966",
    "createdAt": "2022-08-01T15:49:08Z",
    "updatedAt": "2022-08-02T16:42:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vokcow"
    },
    "answer": {
      "body": "> And I should see a bunch of ones.\r\n\r\nyou should see or you did see. I think the parameters saved in the dict are references to the same object hence you can't see any difference there.\r\n\r\n> pl_module.parameters()\r\n\r\nalso, this is a generator.\r\n\r\nyou can deepcopy the state_dict instead to verify it.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-08-02T10:49:54Z"
    }
  },
  {
    "title": "ddp_sharded crash during model save",
    "body": "I am trying to train a big styleGAN model on 4 v100, I used the `ddp_sharded` strategy,\r\nAt the end of the first epoch, the training ends with an error.\r\nI think it came from the model checkpointing at the end of each epoch, do you have maybe a solution to perform the checkpointing without this error?\r\nI also have issues visualizing how the model is shared between the multiple gpus.\r\n\r\n```\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 294, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in _call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in on_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 381, in _save_topk_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 661, in _save_none_monitor_checkpoint\r\n    self._save_checkpoint(trainer, filepath)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in _save_checkpoint\r\n    trainer.save_checkpoint(filepath, self.save_weights_only)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 2467, in save_checkpoint\r\n    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 444, in save_checkpoint\r\n    _checkpoint = self.dump_checkpoint(weights_only)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 380, in dump_checkpoint\r\n    optimizer_state = self.trainer.strategy.optimizer_state(optimizer)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/strategies/sharded.py\", line 117, in optimizer_state\r\n    optimizer.consolidate_state_dict()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/fairscale/optim/oss.py\", line 364, in consolidate_state_dict\r\n    dist.broadcast_object_list(\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1840, in broadcast_object_list\r\n    object_list[i] = _tensor_to_object(obj_view, obj_size)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1532, in _tensor_to_object\r\n    return _unpickler(io.BytesIO(buf)).load()\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/storage.py\", line 161, in _load_from_bytes\r\n    return torch.load(io.BytesIO(b))\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 608, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 787, in _legacy_load\r\n    result = unpickler.load()\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 743, in persistent_load\r\n    deserialized_objects[root_key] = restore_location(obj, location)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 175, in default_restore_location\r\n    result = fn(storage, location)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 155, in _cuda_deserialize\r\n    return storage_type(obj.size())\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 606, in _lazy_new\r\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\nRuntimeError: CUDA error: out of memory\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13951",
    "createdAt": "2022-08-01T01:09:16Z",
    "updatedAt": "2022-09-12T08:03:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MaugrimEP"
    },
    "answer": {
      "body": "Ok, DeepSpeed with one optimizer is working quite well with only little changes in the code.\r\n\r\nI still have an issue, when training on 1 node with 4 GPUs everything works fine. When switching to 2 nodes with 4 GPUs per node, I get CUDA out of memory which is interesting?\r\n\r\nIf anyone has the same problem with 2 optimizers, I just did the simple solution to have 1 optimizer and set either the Generator or the Discriminator `requires_grad` to True or False.\r\nThe other limitation of DeepSpeed is that, I think, you can only do one `.step()` per training step on the optimizer.\r\n```\r\n\tdef configure_optimizers(self):\r\n\t\toptimizer = torch.optim.Adam(\r\n\t\t\t[\r\n\t\t\t\t{'params': self.generators.parameters(), 'lr': self.lr * g_reg_ratio, 'betas': (0 ** g_reg_ratio, 0.99 ** g_reg_ratio)},\r\n\t\t\t\t{'params': self.discriminator.parameters(), 'lr': self.lr * d_reg_ratio, 'betas': (0 ** d_reg_ratio, 0.99 ** d_reg_ratio)},\r\n\t\t\t],\r\n\t\t)\r\n\t\treturn optimizer\r\n\r\n\tdef _set_grad_step(self, is_generator: bool):\r\n\t\tfor p in self.generators.parameters():\r\n\t\t\tp.requires_grad = is_generator\r\n\t\tfor p in self.discriminator.parameters():\r\n\t\t\tp.requires_grad = not is_generator\r\n\r\n\tdef single_training_step(self, batch, batch_idx):\r\n\t\tif batch_idx % 2 == 0:\r\n\t\t\tself._set_grad_step(is_generator=False)\r\n\t\t\treturn self.training_step_discriminator(reals=batch, batch_idx=batch_idx)\r\n\t\telse:\r\n\t\t\tself._set_grad_step(is_generator=True)\r\n\t\t\treturn self.training_step_generator(reals=batch, batch_idx=batch_idx)\r\n```",
      "author": {
        "login": "MaugrimEP"
      },
      "createdAt": "2022-09-12T07:56:46Z"
    }
  },
  {
    "title": "limit_train_batches",
    "body": "Hi,\r\nwhen I use the `limit_train_batches` flag in the trainer and set it to `0.1` does it use the same batches in each epoch or does it randomly sample 10% of the images (in my case) from the training data?\r\n\r\nThx",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13905",
    "createdAt": "2022-07-28T12:10:33Z",
    "updatedAt": "2022-08-02T09:41:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sirtris"
    },
    "answer": {
      "body": "It completely depends on your dataloader's arg `shuffle=True|False` (or your sampler specifically).",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-31T21:38:29Z"
    }
  },
  {
    "title": "Do not apply fp16 to certain key returned by Dataset",
    "body": "My Dataset returns additional `doc_id` key which is passed to the model to log loss on each document individually within a batch. The problem is that `doc_id` has some large numbers over > 1000. Due to numerical precision issue with fp16, some `doc_id` will have wrong values when I access them during `validation_step`. When running with full-precision, the `doc_id`s are all correct.\r\n\r\nIs there anyway I can avoid applying fp16 to `doc_id`? This shouldn't be a problem with the model as `doc_id` is never used to update the model's parameter. With vanilla PyTorch I think this should be pretty straight forward, however with Lightning it is unclear exactly when the fp16 is getting applied.\r\n\r\nThank you in advance.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13866",
    "createdAt": "2022-07-26T19:59:14Z",
    "updatedAt": "2022-08-16T10:35:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MattYoon"
    },
    "answer": {
      "body": "I just found out that this happened because I tried to stack float16 `doc_id` with float32 `loss` to a single tensor. The `doc_id` values were actually correct in `validation_step`, it was corrupted when the stack operation was applied.",
      "author": {
        "login": "MattYoon"
      },
      "createdAt": "2022-08-16T10:35:17Z"
    }
  },
  {
    "title": "About the loss on the progress bar",
    "body": "\r\n![image](https://user-images.githubusercontent.com/29114869/180907549-d7619959-d6c8-4cd3-8417-04ab91dacccc.png)\r\n\r\nThe loss in the red box indicates? \r\nMy understanding: loss_step denotes the loss of the current step (batch-level), loss_epoch denotes the loss of an epoch. So, the loss in the red box denotes ? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13842",
    "createdAt": "2022-07-26T02:14:06Z",
    "updatedAt": "2022-07-27T03:35:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "ideally it is just the step loss, but you might notice that it's different from `loss_step`. The reason is `loss` is a running loss with certain window.\r\n\r\nThere is a [tracking issue](https://github.com/Lightning-AI/lightning/issues/9372), that will remove the averaging window.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-26T10:01:37Z"
    }
  },
  {
    "title": "Custom callback to stop training in DDP.",
    "body": "I'm looking to create a callback that, given some external \"signal\", will stop training & save a checkpoint so that training can be resumed later.\r\n\r\nHere is the code so far. The \"signal\" in this case is a file that i create.\r\n\r\n```python\r\nimport os\r\nfrom pathlib import Path\r\nfrom pytorch_lightning.callbacks import Callback\r\n\r\nclass SignalStopCallback(Callback):\r\n    '''Given signal, will stop training'''\r\n\r\n    def __init__(self, signal_fpath: Path, chkpoint_dir: Path):\r\n        super().__init__()\r\n        self.signal_fpath = signal_fpath\r\n        self.chkpoint_dir = chkpoint_dir\r\n\r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        if self.signal_fpath.exists():\r\n            print(f'Signal stop found, stopping at epoch {trainer.current_epoch}')\r\n            trainer.save_checkpoint(self.chkpoint_dir.joinpath('last.ckpt'))\r\n            trainer.should_stop = True\r\n            os.remove(self.signal_fpath)\r\n```\r\n\r\nI believe this should work in a single GPU training setting. However, in DDP (one node, 4 GPUs), I believe this will cause undefined behaviour as once the file is removed by one process, `self.signal_fpath.exists()` will be evaluated as `False` in the others.\r\n\r\nI could remedy this situation by having this function not delete the file, and delete it manually myself. But is there a simple way to have the desired functionality here? For example is there a way to set `trainer.should_stop` only on the rank 0 process (by using `@rank_zero_only`, but have all other processes end safely?\r\n\r\nThanks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13834",
    "createdAt": "2022-07-25T16:12:03Z",
    "updatedAt": "2023-11-08T10:16:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "m-lyon"
    },
    "answer": {
      "body": "Hi @m-lyon,\r\n\r\nUse `trainer.strategy.barrier()` to ensure all processes are at the same line. It is strategy-agnostic, so when you're using a single device, for example, it will be just no-op.\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.strategies.Strategy.html#pytorch_lightning.strategies.Strategy.barrier",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-25T19:15:25Z"
    }
  },
  {
    "title": "Weighting different losses based on random initialization of network",
    "body": "I'm working on a model with multiple losses that I add together: an autoencoder reconstruction loss (mse) and a prediction loss (cross entropy).\r\n\r\nI now want to weight them based on the `reconstruction loss / prediction loss` ratio that is derived from a random initialization of the network (prior to training). I've thought about calculating these ratios using the `on_train_start` hook and then storing them in self. However, as far as I'm aware I do not have access to/cannot pass my dataloaders to `on_train_start`?\r\n\r\nAlternatively, I've also thought about creating a custom method in my model class that calculates the weights, stores them in self and then I would call that method manually after instantiating my model but prior to calling fit. Something like:\r\n```python\r\nclass MyModelClass(pl.LightningModule):\r\n    def __init__(self):\r\n        ...\r\n    \r\n    def calculate_loss_weights(self):\r\n        # calculate my ratio\r\n        self.reconstruction_prediction_ratio = ratio\r\n\r\n    def training_step(self, loaders):\r\n        # all my training code\r\n        return self.reconstruction_prediction_ratio * reconstruction loss + prediction_loss\r\n\r\ndata = MyDataClass()\r\nmodel = MyModelClass()\r\nmodel.calculate_loss_weights()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model, data)\r\n```\r\n\r\nHowever, I've realized that here too I don't know how to access my data loaders. \r\n\r\nIs there any way to implement this, and are there any best practices in terms of implementation I should stick to?\r\n\r\nTwo things to note:\r\n1. these weights would not change during training but remain fixed\r\n2. I have about 4 different dataloaders at the moment, most of which need to be passed to calculate the loss ratios",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13814",
    "createdAt": "2022-07-23T18:10:56Z",
    "updatedAt": "2022-08-03T03:58:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Michael-Geuenich"
    },
    "answer": {
      "body": "Hi @Michael-Geuenich, I think #8114 will help.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-25T11:28:16Z"
    }
  },
  {
    "title": "Callback Checkpointing never called",
    "body": "I am trying to save my Callback state along with my model at the end of each epoch.\r\nI tried reimplementing the example [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html#persisting-callback-state).\r\n\r\nI put print statement/breakpoint at the state_dict and load_state_dict method, and they are never called during my training. And the Counter state is not recovered.\r\n\r\nI tried to use checkpointing both with a callback ModelCheckpoint in the callback list of the trainer and with enable_checkpointing set to True (with or without the ModelCheckpoint) without it working.\r\n\r\nFor reference, other states are well loaded (for example the right epoch, the model weights etc).\r\n\r\nHere is a minimum working example:\r\n1) First train a model from scratch and save it\r\n2) put `loading = True`\r\n3) Rerun the script, in the training_step the value self.current_epoch et self.cpt are recovered but not the state of the Callback Counter.\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom pytorch_lightning import Callback, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom torch.nn import Parameter\r\nfrom torch.optim import Adam\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\n\r\nclass Dummy_dataset(Dataset):\r\n\tdef __init__(self, length):\r\n\t\tself.length = length\r\n\r\n\tdef __len__(self):\r\n\t\treturn self.length\r\n\r\n\tdef __getitem__(self, item):\r\n\t\treturn 1\r\n\r\n\r\nclass Dummy_Model(pl.LightningModule):\r\n\tdef __init__(self):\r\n\t\tsuper(Dummy_Model, self).__init__()\r\n\t\tself.cpt = Parameter(torch.tensor(0.), requires_grad=False)\r\n\r\n\tdef configure_optimizers(self):\r\n\t\treturn Adam([\r\n\t\t\t{'params': self.cpt},\r\n\t\t])\r\n\r\n\tdef forward(self, data):\r\n\t\treturn data\r\n\r\n\tdef training_step(self, data, batch_idx):\r\n\t\tself.cpt += batch_idx\r\n\t\tprint(f'\\n{self.current_epoch=} {self.cpt=}')\r\n\t\treturn None\r\n\r\n\tdef validation_step(self, data, batch_idx):\r\n\t\t\tpass\r\n\r\n\tdef test_step(self, data, batch_idx):\r\n\t\tpass\r\n\r\n\r\nclass Counter(Callback):\r\n\tdef __init__(self):\r\n\t\tself.state = {\"epochs\": 0, \"batches\": 0}\r\n\r\n\tdef on_train_epoch_end(self, *args, **kwargs):\r\n\t\tself.state['epochs'] += 1\r\n\t\tprint(self.state)\r\n\r\n\tdef on_train_batch_end(self, *args, **kwargs):\r\n\t\tself.state['batches'] += 1\r\n\r\n\tdef load_state_dict(self, state_dict):\r\n\t\tprint('load_state_dict')\r\n\t\tself.state.update(state_dict)\r\n\r\n\tdef state_dict(self):\r\n\t\tprint('state_dict')\r\n\t\treturn self.state.copy()\r\n\r\n\r\ntrain_dataloader = DataLoader(Dummy_dataset(length=10), batch_size=2)\r\nvalid_dataloader = DataLoader(Dummy_dataset(length=10), batch_size=2)\r\n\r\nmodel = Dummy_Model()\r\ncheckpoint_callback = ModelCheckpoint(\r\n\tdirpath='./',\r\n\tmonitor=None,\r\n\tverbose=True,\r\n\tsave_last=True,\r\n\tevery_n_epochs=1,\r\n)\r\nloading = False\r\ntrainer = Trainer(\r\n\tmax_epochs=300 if loading else 50,\r\n\tcallbacks=[Counter(), checkpoint_callback],\r\n\tenable_checkpointing=True,\r\n)\r\ntrainer.fit(\r\n\tmodel=model,\r\n\ttrain_dataloader=train_dataloader,\r\n\tval_dataloaders=valid_dataloader,\r\n\tckpt_path='last.ckpt' if loading else None,\r\n)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13800",
    "createdAt": "2022-07-22T14:29:40Z",
    "updatedAt": "2022-07-25T14:52:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MaugrimEP"
    },
    "answer": {
      "body": "what's your lightning version??",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-25T14:01:45Z"
    }
  },
  {
    "title": "How can I show my information on the progress bar\uff1f",
    "body": "The default process bar like this, \r\nEpoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10]\r\n\r\nIf i want to display acc metric on process bar like this, \r\nEpoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10, acc=0.99]\r\n\r\nWhat should I do\uff1f thx~",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13734",
    "createdAt": "2022-07-19T12:23:07Z",
    "updatedAt": "2022-07-19T12:25:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "xmy0916"
    },
    "answer": {
      "body": "I find it~ \r\nself.log(\"accuracy\", accuracy, prog_bar=True)",
      "author": {
        "login": "xmy0916"
      },
      "createdAt": "2022-07-19T12:25:30Z"
    }
  },
  {
    "title": "How to delete all the gradients in between operations",
    "body": "Hi! I am currently working on a project where, for a given trained model, I perform inference batches of inputs and compute (and store) the gradients of the output with respect to the inputs.\r\nMy code is something like this\r\n```py\r\ndef test_step(self, batch,batch_idx):\r\n   with torch.set_grad_enabled(True):\r\n      gradients_list=[]\r\n      for batch_of_inputs in batches:\r\n         batch_of_inputs.requires_grad_()\r\n         output=self(batch_of_inputs)\r\n         gradients = torch.autograd.grad(\r\n            outputs=output,\r\n            inputs=batch_of_inputs,\r\n            grad_outputs=torch.ones_like(output),\r\n            retain_graph=False,\r\n           )\r\n         gradients_list.append(gradients.detach_())\r\n```\r\nThe thing is that the used memory increases and increases until OOM error rises. I have tried to use\r\n`del gradients,batch_of_inputs,output`\r\nand the problem persisted.\r\nWhat would you suggest?\r\nThanks in advance",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13728",
    "createdAt": "2022-07-19T08:43:29Z",
    "updatedAt": "2022-07-20T13:05:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "> The thing is that the used memory increases and increases until OOM error rises\r\n\r\nif the OOM error is on GPU, you can move your gradients from GPU to CPU while storing them to release some GPU memory.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-19T10:49:27Z"
    }
  },
  {
    "title": "when and how the trainer or module move the data to gpu?",
    "body": "Automatically move data to GPU is fine.\r\nBut, not knowing what moves and what doesn't, When the batch in ```training_step(self, batch, batch_idx)``` is a dict, no movement will occur.\r\n\r\nAlso don't know when to move and how to move to GPU?\r\nI didn't find where to implement in the package!!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13713",
    "createdAt": "2022-07-18T15:31:38Z",
    "updatedAt": "2022-07-22T04:07:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "FutureWithoutEnding"
    },
    "answer": {
      "body": "Hi @FutureWithoutEnding\r\n\r\n**When the batch is transferred to device**\r\nI think the pseudocode in the docs explains when a batch is transferred to each device very well:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#hooks\r\n\r\n**What is transferred to device**\r\nSee the list of supported data structures in the documentation. If you have custom data structure, you need to override this hook in your `LightningModule`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#transfer-batch-to-device\r\n\r\nAlso, if you need to manually transfer tensors to device, you can utilise `self.device` so that your code stays hardware-agnostic. `your_tensor.to(self.device)`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/accelerators/gpu.html#init-tensors-using-type-as-and-register-buffer",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-19T06:13:32Z"
    }
  },
  {
    "title": "How to perform ungraceful shutdown",
    "body": "My training code is as such:\r\n\r\nself.trainer.fit(self, train_loader, dev_loader)\r\ntorch.save(self.model.state_dict(), path_model)\r\n\r\nHowever, when I press ctrl+C during training I get a \"attempting graceful shutdown\", and then the model is saved. I'd like to not save the model or perform any of the code after trainer.fit if keyboard interrupt is performed. \r\n\r\nThe doc says that \"The trainer will catch the KeyboardInterrupt and attempt a graceful shutdown, including running accelerator callback on_train_end to clean up memory. The trainer object will also set an attribute interrupted to True in such cases. If you have a callback which shuts down compute resources, for example, you can conditionally run the shutdown logic for only uninterrupted runs.\"\r\n\r\nHowever, when I try to access the \"interrupted\" attribute, I get \"no attribute 'interrupted' \". \r\n\r\nIs there a way to force the model to perform ungraceful shutdown or to quit after having release resources?\r\n\r\nThank you",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13707",
    "createdAt": "2022-07-18T12:17:16Z",
    "updatedAt": "2022-07-20T15:12:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "smolPixel"
    },
    "answer": {
      "body": "> I'd like to not ... perform any of the code after trainer.fit if keyboard interrupt is performed.\r\n\r\n1. Press C-c many times, or\r\n2. Override `on_exception` and reraise the exception there\r\n   - docs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html?highlight=on_exception#on-exception\r\n   - the error handling all happens at:\r\nhttps://github.com/Lightning-AI/lightning/blob/1d59b3f0ce185702d69eedef752363f63bc48012/src/pytorch_lightning/trainer/trainer.py#L649-L670\r\n\r\n> when I try to access the \"interrupted\" attribute, I get \"no attribute 'interrupted' \".\r\n\r\nHere's the trainer's property:\r\n```python\r\n# status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\r\ntrainer.state.status\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#state\r\n",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-19T06:48:01Z"
    }
  },
  {
    "title": "Add subcommands to LightningCLI",
    "body": "I am not sure if this is possible and I am missing how to do this. But I couldn't figure out how to do it from the docs.\r\n\r\nHow can I add my own subcommands by extending LightningCLI ?\r\n\r\nSay, my model code is part of an overall application I want to add a subcommand like `preprocess` for pre preparing data. Or `serve` for starting a dev webserver to serve the model etc ?\r\n\r\nIs this possible with the current API? Are there any plans for it?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13687",
    "createdAt": "2022-07-16T19:04:39Z",
    "updatedAt": "2023-09-12T14:05:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vikigenius"
    },
    "answer": {
      "body": "It is possible. First you would subclass `Trainer` and add methods for each of the subcommands you want. These new methods should be usable independent of the CLI. Then you would need to subclass `LightningCLI`, in the `__init__` make your new trainer the default and override https://github.com/Lightning-AI/lightning/blob/d4c7f91fec6f4cd8edf5124a5cdfac08a07c4bde/src/pytorch_lightning/utilities/cli.py#L534-L543",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-07-18T20:51:39Z"
    }
  },
  {
    "title": "`RuntimeError: Expected all tensors to be on the same device`",
    "body": "I am working on my first Lightning project and having an issue when I attempt to train on GPUs. When I train on the CPU using the accelerator='cpu' argument, the training and validation occurs with no problem. My workstation has two GPUs, so I set the accelerator='gpu', devices=2, and strategy='dp' (I've also tried 'ddp' with the same result). The data is being provided by a LightningDataModule that is pulling a custom Torch Dataset, and the Dataset is using a Pandas Dataframe. The Dataframe contains file names to Numpy files which are being loaded as follows:\r\n\r\n    def __getitem__(self, idx):\r\n        \r\n        x = self.df.ct[idx]\r\n        y = self.df.label[idx]\r\n\r\n        x = np.load(x)['arr_0']\r\n        y = np.loadtxt(y).flatten()\r\n\r\n        x = torch.from_numpy(x).type(torch.FloatTensor)\r\n        y = torch.from_numpy(y).type(torch.FloatTensor)\r\n\r\n        return x, y\r\n\r\nHowever, when I try to switch to GPUs for training, I receive the following error with strategy='dp':\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\r\n\r\nWith the strategy='ddp' this error is displayed:\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument mat1 in method wrapper_addmm)\r\n\r\nAny help would be appreciated.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13667",
    "createdAt": "2022-07-15T01:33:26Z",
    "updatedAt": "2022-07-16T07:12:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ddicostanzo"
    },
    "answer": {
      "body": "I figured it out. I had a mistake in my model generation where I was using a List for modules rather than a Pytorch class.\r\n\r\nI changed:\r\n`self.fc = []`\r\nto\r\n`self.fc = nn.Sequential()`\r\n",
      "author": {
        "login": "ddicostanzo"
      },
      "createdAt": "2022-07-15T12:14:00Z"
    }
  },
  {
    "title": "how to run horovod strategy?",
    "body": "**What I got is**: \r\nI run the horovod for the multi-gpus using the following command for the pytorch lightning 1.6.5.\r\n\r\n\r\n```python\r\npython pl_examples/basic_examples/mnist_examples/image_classifier_5_lightning_datamodule.py --trainer.accelerator 'gpu' --trainer.devices 4 --trainer.strategy 'horovod'\r\n```\r\n\r\n\r\nthe output is \r\n```python\r\nThu Jul 14 14:26:11 2022       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   35C    P0    34W /  70W |   1547MiB / 15360MiB |     37%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   31C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   31C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   32C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A     39649      C                                    1545MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nthe command line output is \r\n\r\n```python\r\n(base) ray@ip-172-31-36-78:~/horovod-gpu/lightning$ python pl_examples/basic_examples/mnist_examples/image_classifier_5_lightning_datamodule.py --trainer.accelerator 'gpu' --trainer.devices 4 --trainer.strategy 'horovod'\r\n\r\n\r\n                    ####\r\n                ###########\r\n             ####################\r\n         ############################\r\n    #####################################\r\n##############################################\r\n#########################  ###################\r\n#######################    ###################\r\n####################      ####################\r\n##################       #####################\r\n################        ######################\r\n#####################        #################\r\n######################     ###################\r\n#####################    #####################\r\n####################   #######################\r\n###################  #########################\r\n##############################################\r\n    #####################################\r\n         ############################\r\n             ####################\r\n                  ##########\r\n                     ####\r\n\r\nGlobal seed set to 42\r\n/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\n/home/ray/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\r\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\r\n  | Name     | Type     | Params\r\n--------------------------------------\r\n0 | model    | Net      | 1.2 M \r\n1 | test_acc | Accuracy | 0     \r\n--------------------------------------\r\n1.2 M     Trainable params\r\n0         Non-trainable params\r\n1.2 M     Total params\r\n4.800     Total estimated model params size (MB)\r\n/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nEpoch 0:   0%|                               \r\n```\r\n\r\n\r\n**What should be**: \r\n\r\nIt should be multiple gpus. How should I run? \r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13663",
    "createdAt": "2022-07-14T21:28:03Z",
    "updatedAt": "2022-07-15T01:22:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JiahaoYao"
    },
    "answer": {
      "body": "Try using `gpus` to specify number of devices to train on `--trainer.gpus=4`",
      "author": {
        "login": "treblenalto"
      },
      "createdAt": "2022-07-15T00:14:55Z"
    }
  },
  {
    "title": "Are we able to construct the model after DDP is initialized?",
    "body": "The current PL's logic seems to only ask people to first define and model, and then send it to the Trainer which automatically handles multi-gpus. However, right now I have a custom model which needs to be initialized on each GPUs. It means that I need to define the model after DDP is initialized on each GPU. Is there any way I can handle this in pytorch-lighting?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13644",
    "createdAt": "2022-07-13T18:15:07Z",
    "updatedAt": "2022-07-13T21:03:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MultiPath"
    },
    "answer": {
      "body": "https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#setup",
      "author": {
        "login": "MultiPath"
      },
      "createdAt": "2022-07-13T21:03:50Z"
    }
  },
  {
    "title": "monitoring metric and model saving",
    "body": "In pytorch-lightning,  we often monitor the metric at the current batch level\uff0c**validation_step**. Does this mean that the model parameters we save are optimal for the current batch rather than the whole validation set? Does this mean that in order to get the best model for the validation set, I have to monitor the metric in     **validation_epoch_end**?\r\n\r\n```\r\n    def share_val_step(self, batch):\r\n        graph_label = batch.y\r\n        graph_pred = self(data=batch)\r\n        y_pred_label = graph_pred.argmax(dim=-1)\r\n        f1, p, r, acc = envaulation(y_true=graph_label, y_pred=y_pred_label)\r\n        return torch.tensor(acc, device=self.device), torch.tensor(f1, device=self.device)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        f1, acc = self.share_val_step(batch)\r\n        metrics = {'val_f1': f1, 'val_acc': acc}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n    def validation_step_end(self, val_step_outputs):\r\n        pass\r\n\r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        pass\r\n\r\n    def test_step(self, batch, batch_idx, dataloader_idx):\r\n        f1, acc = self.share_val_step(batch)\r\n        metrics = {'test_f1': f1, 'test_acc': acc}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13633",
    "createdAt": "2022-07-13T11:22:17Z",
    "updatedAt": "2022-07-16T10:52:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "```py\r\nf1, acc = self.share_val_step(batch)\r\nmetrics = {'val_f1': f1, 'val_acc': acc}\r\nself.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n```\r\nhere you are using `on_epoch=True`, which means the metric will be aggregated across the validation set and will be used to monitor the checkpoints if you set `ModelCheckpoint(..., monitor='val_acc')`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-13T17:37:17Z"
    }
  },
  {
    "title": "the metric calculation for pytorch-lightning",
    "body": "Generally, in pytorch-lightning, we use the step method for metric calculation, and the callback function monitors the metric of the step. I am curious how pytorch-lightning gets the metric of the epoch? Is it obtained by averaging?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13630",
    "createdAt": "2022-07-13T02:15:21Z",
    "updatedAt": "2022-07-13T11:08:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "it takes the weighted average using the current batch size.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-13T08:47:09Z"
    }
  },
  {
    "title": "Warning during save_hyperparameter() gives misleading advice?",
    "body": "\r\n\r\nI try to understand / rectify a warning about saving my hyper parameters and would need some assistance please.\r\n\r\nI build a model this way:\r\n\r\n```py\r\nfrom pytorch_lightning.core.mixins import HyperparametersMixin\r\n\r\nclass MyModel(nn.Module, HyperparametersMixin):\r\n    def __init__(...):\r\n        super().__init__()\r\n        self.save_hyperparameters()  # Logs to self.hparams only, not to the logger (since there isn't any yet)\r\n\r\nclass MyModule(pl.LightningModule):\r\n    def __init__(model: nn.Module):\r\n        super().__init__()\r\n        self.save_hyperparameters(\"model\", logger=False)\r\n        self.save_hyperparameters()\r\n        self.save_hyperparameters(model.hparams)\r\n\r\nmodel = MyModel(...)\r\nmodule = MyModule(model)\r\n```\r\n\r\nThis works, and I can load a checkpoint with `model = MyModule.load_from_checkpoint(model_path_to_load_from)`\r\n\r\nBut I also get this warning during the initialization of `MyModule`: `Attribute 'model' is an instance of 'nn.Module' and is already saved during checkpointing. It is recommended to ignore them using 'self.save_hyperparameters(ignore=['model'])'`.\r\n\r\nSo I change the corresponding code in `MyModule` to:\r\n\r\n```py\r\nself.save_hyperparameters(ignore=[\"model\"])\r\nself.save_hyperparameters(model.hparams)\r\n```\r\n\r\nThe created checkpoint is marginally reduced by ~3KB, the checkpoint size is ~1MB.\r\n\r\nBut when I want to load the  checkpoint I get this error:\r\n\r\n```py\r\nFile \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 161, in load_from_checkpoint\r\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n  File \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 203, in _load_model_state\r\n    model = cls(**_cls_kwargs)\r\nTypeError: __init__() missing 1 required keyword-only argument: 'model'\r\n```\r\n\r\nWhich seems to indicate that I need to save `model` as a hyper parameter.\r\n\r\nWhat am I missing? What is the correct way to save the hyper parameters / model in the `pl.LightningModule`?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13615",
    "createdAt": "2022-07-12T09:48:33Z",
    "updatedAt": "2025-05-02T16:59:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hogru"
    },
    "answer": {
      "body": "the attributes that are not saved as hparams need to be passed explicitly. Considering you are using `load_from_checkpoint` API, you can use `model = MyModule.load_from_checkpoint(ckpt_path, model=model)`.\r\n\r\nIf you include it in the hparams, your checkpoints will be unnecessarily big and can create issues if you have large models.\r\n\r\nBy\r\n```\r\nis already saved during checkpointing.\r\n```\r\nit means the model weights are already saved in the checkpoint and are loaded using PyTorch API, not as hparams.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-12T12:56:54Z"
    }
  },
  {
    "title": "How to pass a reference to a function in yaml config file",
    "body": "If I want to use CLI to initialize datamodule object I should use:\r\n\r\n`python train.py  --config data.yaml`\r\n\r\n```\r\ndata:\r\n  class_path: foo.datasets.my_data\r\n  init_args:\r\n    data_dir: data\r\n```\r\n\r\nBut what if one the `init_arguments` is a function? How can I pass a reference to it like this? \r\n\r\n```\r\ndata:\r\n  class_path: foo.datasets.my_data\r\n  init_args:\r\n    data_dir: data\r\n    filter: foo.datasets.filter1\r\n```\r\n\r\n`foo.datasets.filter1` is a function.\r\n\r\nThank you for attention.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13613",
    "createdAt": "2022-07-12T07:54:35Z",
    "updatedAt": "2024-06-19T20:28:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kenenbek"
    },
    "answer": {
      "body": "This is supported in jsonargparse. Just use `Callable` as type hint, see its mention in the [docs](https://jsonargparse.readthedocs.io/en/stable/#type-hints). Basically you would have:\r\n```python\r\nclass MyData:\r\n    def __init__(self, data_dir: str, filter: Callable):\r\n        ...\r\n```\r\nThen the config could be as you had it:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter: foo.datasets.filter1\r\n```\r\nOr `filter` could be a class that once instantiated becomes callable, like:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter:\r\n    class_path: path.to.callable.class\r\n    init_args:\r\n      param1: val1\r\n      ...\r\n```",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-07-18T20:12:34Z"
    }
  },
  {
    "title": "when I configure callbacks in the model instead of in the Trainer function, will the model look for the best model or the current model when testing?",
    "body": "A question: when I configure callbacks in the model instead of in the Train function, will the model look for the best model or the current model when testing?\r\n\r\nFor example:  configure the   configure_callbacks in my model\r\n`\r\n```\r\n    def configure_callbacks(self):\r\n        early_stop_callback = EarlyStopping(monitor=\"val_f1\", min_delta=0.00, patience=self.args.patience,\r\n                                            verbose=False, mode=\"max\")\r\n        checkpoint_callback = ModelCheckpoint(monitor='val_f1',\r\n                                              dirpath=\"{}\".format(self.save_log_path),\r\n                                              filename='best_{}'.format(self.index_times),\r\n                                              save_top_k=1,\r\n                                              mode='max',\r\n                                              save_last=False)\r\n        return [checkpoint_callback, early_stop_callback]\r\n\r\n    def on_train_start(self):\r\n        self.print(\"Training is started!\")\r\n\r\n    def on_train_end(self):\r\n        self.print(\"Training is done!\")\r\n```\r\n\r\n`\r\n\r\nNot  configure the   configure_callbacks in the Trainer\r\n`\r\n        trainer = Trainer(devices=\"auto\", accelerator=\"auto\", logger=False, multiple_trainloader_mode='max_size_cycle',\r\n                          terminate_on_nan=True,\r\n                          logger=tb_logger, log_every_n_steps=1,\r\n                          flush_logs_every_n_steps=5,\r\n                          max_epochs=args.epochs)\r\n`\r\n\r\nWhen i perform the trainer  like this:\r\n`\r\n\r\n        trainer.fit(model)\r\n\r\n        result = trainer.test(model)\r\n`\r\n\r\nMy question is: Does the model use the parameters at the last epoch when testing, or the best parameters loaded?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13611",
    "createdAt": "2022-07-12T06:39:32Z",
    "updatedAt": "2022-07-13T11:07:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "> Does the model use the parameters at the last epoch when testing, or the best parameters loaded?\r\n\r\nHi @Struggle-Forever, yes, the `model` is in the last state, but not in the best state. I hope the following example in the documentation will explain it enough:\r\n\r\n```python\r\n# run full training\r\ntrainer.fit(model)\r\n\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test(ckpt_path=\"best\")\r\n\r\n# (2) test using a specific checkpoint\r\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\r\n\r\n# (3) test with an explicit model (will use this model and not load a checkpoint)\r\ntrainer.test(model)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.4/common/evaluation.html#test-after-fit",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-12T11:00:01Z"
    }
  },
  {
    "title": "where to add weight decay scheduler",
    "body": "I want to change weight decay during training, which hook should I overwrite, optimizer_step(), on_train_batch_end() or ... ?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13592",
    "createdAt": "2022-07-11T02:43:41Z",
    "updatedAt": "2022-07-11T13:03:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "FredZZD"
    },
    "answer": {
      "body": "@FredMushZhao I think either works if you want to schedule it on each step. Alternatively, you could use `on_train_epoch_start` if you want to update it on each epoch.\r\n\r\nFor reference on all available hooks: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#hooks",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-11T12:58:54Z"
    }
  },
  {
    "title": "where the new callbaclks located?",
    "body": "https://github.com/Lightning-AI/lightning/blob/master/src/pytorch_lightning/trainer/callback_hook.py\r\n\r\nthese callbacks are removed but where are the new locations for them? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13586",
    "createdAt": "2022-07-10T00:17:26Z",
    "updatedAt": "2022-07-14T03:16:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JiahaoYao"
    },
    "answer": {
      "body": "Hi @JiahaoYao These hooks were not meant to be called by users. (related to #10575)\r\n\r\nCan I ask what you're trying to find? Maybe, what you're looking for is user-facing `Callback`'s hooks? If so, they are located here in the codebase: https://github.com/Lightning-AI/lightning/blob/b59f80224843886459d54c828325683d770da746/src/pytorch_lightning/callbacks/callback.py",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-10T10:33:22Z"
    }
  },
  {
    "title": "Are model parameters after fitting the best or the last one?",
    "body": "When I use it like the following, does it use the best model by default? I remember that it defaults to the best model, but I can't find the documentation.\r\n\r\n```python\r\ntrainer.fit(model)\r\n\r\nresult = trainer.test(model)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13578",
    "createdAt": "2022-07-08T15:00:14Z",
    "updatedAt": "2022-07-13T11:11:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "have done",
      "author": {
        "login": "Struggle-Forever"
      },
      "createdAt": "2022-07-13T10:18:34Z"
    }
  },
  {
    "title": "How to predict on the test dataset using trainer.predict()?",
    "body": "Hi I have trained the model using trainer and was trying to use trainer.predict() method to predict on the datamodule. But it throws the following error:\r\n\r\n```\r\nMisconfigurationException                 Traceback (most recent call last)\r\nInput In [200], in <cell line: 1>()\r\n----> 1 trainer.predict(dataloaders=datamodule)\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1025, in Trainer.predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1000 r\"\"\"\r\n   1001 Run inference on your data.\r\n   1002 This will call the model forward function to compute predictions. Useful to perform distributed\r\n   (...)\r\n   1022     Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n   1023 \"\"\"\r\n   1024 self.strategy.model = model or self.lightning_module\r\n-> 1025 return self._call_and_handle_interrupt(\r\n   1026     self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n   1027 )\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:723, in Trainer._call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    721         return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\r\n    722     else:\r\n--> 723         return trainer_fn(*args, **kwargs)\r\n    724 # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    725 except KeyboardInterrupt as exception:\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1072, in Trainer._predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1066 self._ckpt_path = self.__set_ckpt_path(\r\n   1067     ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None\r\n   1068 )\r\n   1070 self._predicted_ckpt_path = self.ckpt_path  # TODO: remove in v1.8\r\n-> 1072 results = self._run(model, ckpt_path=self.ckpt_path)\r\n   1074 assert self.state.stopped\r\n   1075 self.predicting = False\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1160, in Trainer._run(self, model, ckpt_path)\r\n   1157 self._callback_connector._attach_model_callbacks()\r\n   1158 self._callback_connector._attach_model_logging_functions()\r\n-> 1160 verify_loop_configurations(self)\r\n   1162 # hook\r\n   1163 log.detail(f\"{self.__class__.__name__}: preparing data\")\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:48, in verify_loop_configurations(trainer)\r\n     46     __verify_eval_loop_configuration(trainer, model, \"test\")\r\n     47 elif trainer.state.fn == TrainerFn.PREDICTING:\r\n---> 48     __verify_eval_loop_configuration(trainer, model, \"predict\")\r\n     50 __verify_dp_batch_transfer_support(trainer, model)\r\n     51 _check_add_get_queue(model)\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:197, in __verify_eval_loop_configuration(trainer, model, stage)\r\n    193 # -----------------------------------\r\n    194 # verify model has an eval_dataloader\r\n    195 # -----------------------------------\r\n    196 if not has_loader:\r\n--> 197     raise MisconfigurationException(f\"No `{loader_name}()` method defined to run `Trainer.{trainer_method}`.\")\r\n    199 # predict_step is not required to be overridden\r\n    200 if stage == \"predict\":\r\n\r\nMisconfigurationException: No `predict_dataloader()` method defined to run `Trainer.predict`.\r\n```\r\nI have the following dataloader:\r\n```\r\nclass ProductDatasetModule(pl.LightningDataModule):\r\n    def __init__(self,train_data,test_data,tokenizer,train_transforms,test_transforms,batch_size):\r\n        super().__init__()\r\n        self.train_data = train_data\r\n        self.test_data = test_data\r\n        self.tokenizer = tokenizer\r\n        self.batch_size= batch_size\r\n        self.train_transforms = train_transforms\r\n        self.test_transforms = test_transforms\r\n        \r\n    def setup(self,stage=None):\r\n        self.train_dataset = ImageTextDataset(self.train_data,tokenizer=self.tokenizer,transforms=self.train_transforms)\r\n        self.test_dataset = ImageTextDataset(self.test_data,tokenizer=self.tokenizer,transforms=self.test_transforms)\r\n        \r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.train_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=True)\r\n    \r\n    def test_dataloader(self):\r\n        \r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n        \r\n```\r\n\r\nI have following model defined:\r\n\r\n```\r\nclass MLP(pl.LightningModule):\r\n    \"\"\"\r\n    Creates a MTL model with the encoder from \"arch\" and with dropout multiplier ps.\r\n    \"\"\"\r\n    def __init__(self, model,input_embedding_dim,output_dim,learning_rate,batch_size):\r\n        super(MLP,self).__init__()\r\n        self.encoder = model\r\n        for param in model.parameters():\r\n            param.requires_grad = False\r\n        self.batch_size=batch_size\r\n        self.projection = nn.Linear(input_embedding_dim, 256)\r\n        # self.layernorm = nn.LayerNorm()\r\n        self.gelu = nn.ReLU()\r\n        self.fc = nn.Linear(256, 256)\r\n        self.dropout = nn.Dropout(0.15)\r\n        self.fc2 = nn.Linear(256, 128)\r\n        self.fc3 = nn.Linear(128, 64)\r\n        # self.fc2 = nn.Linear(128, 128)\r\n        self.fc4 = nn.Linear(64,output_dim)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.learning_rate = learning_rate\r\n        self.train_acc = torchmetrics.Accuracy()\r\n        self.train_f1 = torchmetrics.F1Score(number_classes=25,\r\n        average=\"micro\")\r\n        self.train_auroc = torchmetrics.AUROC(number_classes=25,\r\n        average=\"micro\")\r\n        self.val_acc = torchmetrics.Accuracy()\r\n        self.val_f1 = torchmetrics.F1Score(number_classes=25,\r\n        average=\"micro\")\r\n        self.val_auroc = torchmetrics.AUROC(number_classes=25,\r\n        average=\"micro\")\r\n        #self.layer_norm = nn.LayerNorm(output_dim)\r\n\r\n    def forward(self,x):\r\n        image_features = self.encoder.image_encoder(x[\"image\"].to(CFG.device))\r\n        image_embeddings = self.encoder.image_projection(image_features)\r\n        text_features = self.encoder.text_encoder(\r\n        input_ids=x[\"input_ids\"], attention_mask=x[\"attention_mask\"])\r\n        text_embeddings = self.encoder.text_projection(text_features)\r\n        img_txt_embeddings = torch.cat((image_embeddings,text_embeddings),dim=1)\r\n        projected = self.projection(img_txt_embeddings)\r\n        # z = self.layernorm(z)\r\n        z= self.gelu(projected)\r\n        z= self.fc(z)\r\n        # z=self.layernorm(z)\r\n        z= self.gelu(z)\r\n        z= self.dropout(z)\r\n        z= self.fc2(z)\r\n        # z=self.layernorm(z)\r\n        z= self.gelu(z)\r\n        z= self.dropout(z)\r\n        z= self.fc3(z)\r\n        z= self.gelu(z)\r\n        z= self.fc4(z)\r\n        \r\n        \r\n        loss = self.criterion(z,x[\"label\"])\r\n        return loss,z\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        # accumulate and return metrics for logging\r\n        acc = self.train_acc(y_pred, y)\r\n        f1 = self.train_f1(y_pred, y)\r\n        # just accumulate\r\n        self.train_auroc.update(y_pred, y)\r\n        self.log(\"train_accuracy\", acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"train_f1\", f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True,logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        self.val_acc.update(y_pred, y)\r\n        self.val_f1.update(y_pred, y)\r\n        self.val_auroc.update(y_pred, y)\r\n        self.log(\"val_accuracy\",self.val_acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"val_f1\",self.val_f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"val_loss\", loss, prog_bar=True,on_step=True, on_epoch=True, logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def test_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        self.val_acc.update(y_pred, y)\r\n        self.val_f1.update(y_pred, y)\r\n        self.val_auroc.update(y_pred, y)\r\n        self.log(\"test_accuracy\",self.val_acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"test_f1\",self.val_f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True,logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(x)\r\n        return y_pred.softmax(dim=-1)\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.AdamW(self.parameters(),lr= self.learning_rate)\r\n        return optimizer\r\n```\r\n\r\nPlease, help to predict on the testdata. How I can leverage trainer to predict on the test data and get classification report for the predicted output?\r\n\r\nThanks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13568",
    "createdAt": "2022-07-07T16:47:39Z",
    "updatedAt": "2022-08-03T04:13:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "karndeepsingh"
    },
    "answer": {
      "body": "@karndeepsingh To use `Trainer.predict()`, You must have `predict_dataloader()` defined in your LightningModule or LightningDataModule as the error message states:\r\n```\r\nMisconfigurationException: No `predict_dataloader()` method defined to run `Trainer.predict`.\r\n```\r\n\r\nIf you'd like to run inference on your test set, you just need to define `predict_dataloader()` with your test set:\r\n```python\r\n    def predict_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-07T20:24:38Z"
    }
  },
  {
    "title": "How to prevent pytorch lightning from intercepting Ctrl-C?",
    "body": "Is there a way to make pytorch lightning **not** intercept ctrl-c, I would like to handle it myself.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13559",
    "createdAt": "2022-07-07T02:32:31Z",
    "updatedAt": "2022-07-07T06:07:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vedantroy"
    },
    "answer": {
      "body": "See #13560.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-07T02:56:33Z"
    }
  },
  {
    "title": "When I perform adversarial training, I find that the output of BERT is always NAN.",
    "body": "- [ ] When I perform adversarial training, I find that the output of BERT is always NAN.\r\n- [ ] \r\n![\u5fae\u4fe1\u622a\u56fe_20220705202316](https://user-images.githubusercontent.com/29114869/177455365-d167b314-5776-496a-8414-8485a40ac04a.png)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13546",
    "createdAt": "2022-07-05T12:42:00Z",
    "updatedAt": "2022-12-13T08:56:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Struggle-Forever"
    },
    "answer": {
      "body": "As far as I know, training with AMP (`precision=16`) can sometimes be unstable and lead to nan as you report.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-06T00:23:53Z"
    }
  },
  {
    "title": "how to get the \"epoch\" value in the \"validate_epoch_end\"",
    "body": "how to get the \"epoch\" value in the \"validate_epoch_end\" \r\n\r\nBecause I wanna use the \"epoch\" value as a  output datadir parameter.\r\n\r\nthank a lot.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13543",
    "createdAt": "2022-07-05T11:34:23Z",
    "updatedAt": "2022-07-05T19:40:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "emilyemliyM"
    },
    "answer": {
      "body": "you can get  epoch using self.current_epoch",
      "author": {
        "login": "Struggle-Forever"
      },
      "createdAt": "2022-07-05T12:47:10Z"
    }
  },
  {
    "title": "LightningCLI: Passing objects via link_arguments",
    "body": "Hi, \r\n\r\nis there a way to pass objects from a **LightningDataModule** to a **LightningModule** in the **LightningCLI** using **link_arguments**? I tried the following\r\n\r\n```\r\nclass TestModel(LightningModule):\r\n\r\n    def __init__(self, scaler: StandardScaler, *args: Any, **kwargs: Any):\r\n        super().__init__(*args, **kwargs)\r\n        print(scaler)\r\n\r\n\r\nclass TestDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.scaler = StandardScaler()\r\n\r\n\r\nclass TestCLI(LightningCLI):\r\n\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.scaler', 'model.scaler', apply_on='instantiate')\r\n\r\n\r\nTestCLI(TestModel, TestDataModule)\r\n```\r\n\r\nbut got a ` ValueError: Target key \"model.scaler\" must be for an individual argument`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13539",
    "createdAt": "2022-07-04T18:05:22Z",
    "updatedAt": "2022-07-19T11:47:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lneukom"
    },
    "answer": {
      "body": "The support for this has been implemented in [jsonargparse](https://github.com/omni-us/jsonargparse). There is no need for any change in pytorch-lightning. Just update the package, i.e. `pip install -U jsonargparse`, and your example should work.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-07-12T05:34:29Z"
    }
  },
  {
    "title": "DDP Hangs with TORCH_DISTRIBUTED_DEBUG = DETAIL",
    "body": "I'm not certain whether this is user error or a PyTorch/Lightning issue, so am posting a discussion instead.\r\n\r\nAdding the line `os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'` while using multiple GPUs and DDP causes the program to hang indefinitely.\r\n\r\nTo reproduce:\r\n\r\n```python\r\nimport argparse\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run(cl_args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Start changed code\r\n    import os\r\n    os.environ[\r\n        \"TORCH_DISTRIBUTED_DEBUG\"\r\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args(cl_args.split() if cl_args else None)\r\n    trainer = Trainer.from_argparse_args(args)\r\n    # End changed code\r\n\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run('--gpus 2 --strategy ddp')\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13503",
    "createdAt": "2022-07-02T00:31:58Z",
    "updatedAt": "2023-10-11T18:28:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kelvins64"
    },
    "answer": {
      "body": "Reading @akihironitta 's response and looking at the [documentation](https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug) again, I noticed that they set the environment variable prior to calling `mp.spawn`. Moving the `os.environ['TORCH_DISTRIBUTED_DEBUG] = 'DETAIL'` line outside of the main function prevented hanging.\r\n\r\n```python\r\nimport argparse\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n# Start changed code\r\nimport os\r\nos.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\r\n# End changed code\r\n\r\ndef run(cl_args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Start changed code\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args(cl_args.split() if cl_args else None)\r\n    trainer = Trainer.from_argparse_args(args)\r\n    # End changed code\r\n\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run('--gpus 2 --strategy ddp')\r\n```\r\nI presume this has to do with where the Trainer is forking the process. In summary, it seems one can \r\n- Set the environment variable via `os.environ` outside of the main function\r\n- Set the environment variable in the shell",
      "author": {
        "login": "kelvins64"
      },
      "createdAt": "2022-07-04T23:53:16Z"
    }
  },
  {
    "title": "Gradient accumulation and total number of training steps",
    "body": "When using `accumulate_grad_batches=K` with Lightning and training for a fixed number of steps `n` (instead of epochs), does Lightning either:\r\n- execute `K` forward passes and then one optimization step per step, resulting in a total of `n*K` batches and `n` optimization steps\r\n- or execute one forward pass per step for a total of `n` batches with an optimization step only every `K` steps (therefore only `n/K` optimization steps in total)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13457",
    "createdAt": "2022-06-29T22:09:26Z",
    "updatedAt": "2022-07-01T17:24:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "konstantinjdobler"
    },
    "answer": {
      "body": "if you mean `max_steps=n`, then 1.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-07-01T09:32:57Z"
    }
  },
  {
    "title": "Trying to create tensor with negative dimension with `ddp_sharded`",
    "body": "I'm trying to fine-tune a Transformer model (XLM-R) on multi-gpu, using the `ddp_sharded` strategy. The train works, but at the end of the first epoch I got this error\r\n\r\n```\r\nRuntimeError: Trying to create tensor with negative dimension -2061635393: [-2061635393]\r\n```\r\n\r\nI'm running the latest PyTorch Lightning, PyTorch 1.10, and I'm using two V100 on a Power9 based architecture. I've tried both with 16bit and 32bit precision. The optimizer I'm using is RAdam, from PyTorch.\r\n\r\nI can provide the code if needed.\r\n\r\n<details>\r\n  <summary>Here the complete stack trace</summary>\r\n  \r\n```\r\nTraceback (most recent call last):\r\n  File \"transformers_ner/train.py\", line 186, in main\r\n    train(conf)\r\n  File \"transformers_ner/train.py\", line 103, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 736, in\r\n_call_and_handle_interrupt\r\n    self.strategy.reconciliate_processes(traceback.format_exc())\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 451, in\r\nreconciliate_processes\r\n    raise DeadlockDetectedException(f\"DeadLock detected from rank: {self.global_rank} \\n {trace}\")\r\npytorch_lightning.utilities.exceptions.DeadlockDetectedException: DeadLock detected from rank: 0\r\n Traceback (most recent call last):\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 721, in\r\n_call_and_handle_interrupt\r\n    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 93,\r\nin launch\r\n    return function(*args, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 297, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in\r\n_call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in\r\non_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 379, in\r\n_save_topk_checkpoint\r\n    self._save_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 651, in\r\n_save_monitor_checkpoint\r\n    self._update_best_and_save(current, trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 702, in\r\n_update_best_and_save\r\n    self._save_checkpoint(trainer, filepath)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in\r\n_save_checkpoint\r\n    trainer.save_checkpoint(filepath, self.save_weights_only)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 2467, in save_checkpoint\r\n    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 444,\r\nin save_checkpoint\r\n    _checkpoint = self.dump_checkpoint(weights_only)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 380,\r\nin dump_checkpoint\r\n    optimizer_state = self.trainer.strategy.optimizer_state(optimizer)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/sharded.py\", line 117, in optimizer_state\r\n    optimizer.consolidate_state_dict()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/fairscale/optim/oss.py\", line 364, in consolidate_state_dict\r\n    dist.broadcast_object_list(\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1823, in\r\nbroadcast_object_list\r\n    object_tensor = torch.empty(\r\nRuntimeError: Trying to create tensor with negative dimension -2061635393: [-2061635393]\r\n```\r\n</details>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13418",
    "createdAt": "2022-06-27T15:55:13Z",
    "updatedAt": "2022-06-30T12:55:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Riccorl"
    },
    "answer": {
      "body": "Duplicate of #13431.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-30T01:07:31Z"
    }
  },
  {
    "title": "LightningCLI access DataModule methods in model constructor",
    "body": "Hello, I am switching my code to use LightningCLI for easier config/reproducibility. I want to access dataset examples when creating my model, but I don't want to have to make my data loading code explicitly available to both `LightningDataModule` and `LightningModule`.\r\n\r\nCurrently my project uses code similar to this:\r\n\r\n```python\r\ndata = MyLightningDataModule(...)\r\nmodel = MyLightningModule(data.input_dim, ...)\r\ntrainer.fit(model, datamodule=data)\r\n```\r\n\r\nwhere `data.input_dim` is roughly equivalent to `data.train_dataloader().dataset[0].shape[0]`. Basically, the width of the first layer of the model depends on the data generated by the DataModule. This is modeled after the API usage suggested in https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html#using-a-datamodule. I posted pseudocode to make it simpler but I can post the real thing if I didn't get the point across.\r\n\r\nThis is how I use LightningCLI:\r\n\r\n```python\r\ncli = LightningCLI(MyLightningModule, MyLightningDataModule)\r\n```\r\n\r\nBut it seems that when using LightningCLI, the LightningModule is instantiated before the LightningDataModule, and I can't figure out how to configure LightningCLI to make the LightningDataModule available to the LightningModule constructor.\r\n\r\nI've tried to make this happen by supplying datamodule as a parameter to my `LightningModule` following this API: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#models-with-multiple-submodules.\r\n\r\n```python\r\nclass MyLightningModule(pl.LightningModule):\r\n    def __init__(self, data: BigVulDatasetLineVDDataModule):\r\n        self.data = data\r\n        # instantiate model...\r\n\r\n    def train_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    def val_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    def test_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    # other LightningModule related methods...\r\n```\r\n\r\nAnd my config looks like this:\r\n```yaml\r\nmodel:\r\n  # model args...\r\n  data:\r\n    class_path: project.MyLightningDataModule\r\n    init_args:\r\n      # datamodule args...\r\n```\r\n\r\n...but it would be nice if I could keep these two classes independent. Is there any better way to achieve the functionality I want?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13403",
    "createdAt": "2022-06-24T13:18:44Z",
    "updatedAt": "2024-09-23T12:54:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bstee615"
    },
    "answer": {
      "body": "For this you use [link_arguments](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking) which requires to subclass `LightningCLI`. Something like:\r\n```python\r\nclass MyLightningModule(LightningModule):\r\n    def __init__(self, input_dim: int):\r\n        ...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.input_dim', 'model.input_dim', apply_on='instantiate')\r\n```\r\nIt might also be possible like your second example linking the entire data object. Though this needs the latest version of [jsonargparse](https://github.com/omni-us/jsonargparse):\r\n```python\r\nclass MyLightningModule(pl.LightningModule):\r\n    def __init__(self, data: BigVulDatasetLineVDDataModule):\r\n        self.data = data\r\n        # instantiate model...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data', 'model.data', apply_on='instantiate')\r\n```",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-07-18T20:41:20Z"
    }
  },
  {
    "title": "Training seems to pause every N steps",
    "body": "I am doing feature extraction using an efficientnet_b0 model. The training process works fine but it seems to pause every once in a while. I verified this using `nvidia-smi dmon`. There are spikes of a few seconds where the GPU utilization is anywhere between 50% and 100%, followed by a few seconds where the GPU utilization is 0%. \r\n\r\nRight now I am training with 4 Tesla T4, but I verified the same issue with a single GPU (T4 and V100).\r\nI am using a batch size of 200 (per GPU).  I have 48 CPUs and their usage is pretty low (I'd say 20-40%).\r\n\r\nI noticed the training pausing at epoch 48, 96, 144,... So it pauses every 48 steps.\r\n\r\nI thought that the pause were caused by logging so in my `Trainer` I set ` log_every_n_steps=500` and I also initialize my logger with `TensorBoardLogger(\"tb_logs\", name=\"vehicles\", max_queue=1000, flush_secs=120)`. I can that the processes pauses more frequently than 120 seconds.\r\n\r\nOriginally, I thought it was a PyTorch \"issue\". So I opened a post here https://discuss.pytorch.org/t/gpu-usage-is-not-constant-during-training/154718 . However I am wondering whether this could be caused by torch lightning.\r\n\r\nThank you\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13375",
    "createdAt": "2022-06-22T22:18:02Z",
    "updatedAt": "2025-03-27T06:09:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mfoglio"
    },
    "answer": {
      "body": "I analyzed the problem a little bit more. I noticed that I have 48 CPUs and 48 workers. That makes the training process pausing every 48 steps. If use 12 workers, the pause happens every 12 steps.\r\nI'd like to increase the number of workers but the RAM usage is crazy high. With 48 workers I am almost using all the 180Gb of RAM available. Is this normal for simply loading images of a few Kbytes?\r\nAny suggestion on how to speed this up?\r\n\r\nEDIT: I think I am facing this issue https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662 even though I am not entirely sure. My memory consumption is of about 100-150 gb right after the training starts. I tried to used a numpy array to store the huge list of integers containing the IDs of the record in the dataset. However, this didn't reduce the RAM usage.\r\nSuppose my dataset has a property `myobject` of type `MyObject`, and that `myobject` internally references a list of integers. Should I convert this list of integers to a numpy array too?",
      "author": {
        "login": "mfoglio"
      },
      "createdAt": "2022-06-23T20:01:34Z"
    }
  },
  {
    "title": "DDP - Synchronization on DGX - Use CPUs or GPU-to-GPU interconnect",
    "body": "Hi,\r\n\r\nI'm training a model on several GPUs with the DDP strategy, I'm using an Nvidia DGX where GPUs are interconnected with NVLinks. \r\n\r\nI want to know when the gradients are synchronized, does the communication use the NVLinks or GPU-to-GPU or use the CPUs to communicate?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13345",
    "createdAt": "2022-06-21T07:11:38Z",
    "updatedAt": "2022-06-22T07:27:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "KevinCrp"
    },
    "answer": {
      "body": "This thread in PyTorch forum may be what you find useful:\r\nhttps://discuss.pytorch.org/t/simple-code-example-with-nvlink-support/125304",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-21T20:00:03Z"
    }
  },
  {
    "title": "How to properly split train/val/test sets when using DDP and multiple GPUs",
    "body": "I trained a model using a single GPU. Now I am trying to use 4 GPUs and DDP. The problem is that the code seems to be executed 4 times, making the datasets split into training, validation, and test sets invalid.\r\nIn particular, in the `__main__` function, I call a function to initialize an instance of my custom data module (which extends `LightningDataModule`). In doing this, I take a list of all the records' IDs, and shuffle them to split them across training, validation and test sets. The problem is that each of the 4 processes shuffles the annotations at randoms (and therefore differently) and, as a consequence, elements that could be in the training set on GPU_0, could be in the test set of another GPU, effectively making the split invalid.\r\n\r\nFor example, suppose I have 4 records with IDs `[1,2,3,4]`.  What could happen is that:\r\n- On `GPU_0` the split is `training_set=[1,2]; validation_set=[3]; test_set=[4]`\r\n- On `GPU_1` the split is `training_set=[1,3]; validation_set=[2]; test_set=[4]`\r\n- Etc...\r\n\r\nHow do you suggest to fix this? How can I run the \"splitting\" only once? I'd like the fact that every time that I execute the code I get a different splits. I just want the splits to be consistent across all the GPUs.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13343",
    "createdAt": "2022-06-20T19:46:33Z",
    "updatedAt": "2022-10-20T16:49:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mfoglio"
    },
    "answer": {
      "body": "did you set `seed_everything(seed)` at the beginning of your main?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-06-21T11:22:03Z"
    }
  },
  {
    "title": "Multi-GPUs DDP - How the dataset is distributed accross the GPUs",
    "body": "Hi,\r\n\r\nI'm using some GPUs and the Distributed-Data-Parallel strategy and I want to know how the global dataset is split across all GPUs.\r\n\r\nIs it split iteratively: \r\n  * The N first graphs go-to GPU 0\r\n  * The N next go-to GPU 1\r\n  * ...\r\n  *  Until the end ?\r\n\r\nIs it randomly distributed?\r\n\r\nIs it another way?\r\n\r\nI looked at the DistributedSampler class, but I didn't find the answer.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13342",
    "createdAt": "2022-06-20T19:38:20Z",
    "updatedAt": "2022-06-22T07:16:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "KevinCrp"
    },
    "answer": {
      "body": "I believe [this line](https://github.com/pytorch/pytorch/blob/63c0bcf887736810dd2ca6e8b671439c67e58ed6/torch/utils/data/distributed.py#L118) in PyTorch code explains it all:\r\n```python\r\nindices = indices[self.rank:self.total_size:self.num_replicas]\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-21T20:13:14Z"
    }
  },
  {
    "title": "Placing a condition on ModelCheckpoint Callback",
    "body": "For example: How to save checkpoints ONLY when valset accuracy is greater than some specific threshold?\r\n\r\nIf this is not possible using parameters in ModelCheckpoint callback, how can we extend ModelCheckpoint to achieve this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13337",
    "createdAt": "2022-06-20T06:44:06Z",
    "updatedAt": "2022-06-23T04:08:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "skrish13"
    },
    "answer": {
      "body": "you can do:\r\n```py\r\nclass CustomModelCheckpoint(ModelCheckpoint):\r\n    def on_validation_end(self, trainer, pl_module):\r\n        score = trainer.callback_metrics[self.monitor]\r\n        if score > some_threshold:\r\n            super().on_validation_end(trainer, pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=[CustomModelCheckpoint(...)])\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-06-21T10:40:12Z"
    }
  },
  {
    "title": "Imagnet pretrained model: training accuracy to zero",
    "body": "Using the exact code from the [imagenet template](https://github.com/Lightning-AI/lightning/blob/master/examples/pl_domain_templates/imagenet.py), I add the following to the end of the script:\r\n\r\n```python \r\nif __name__ == \"__main__\":\r\n    import pytorch_lightning\r\n\r\n    model = ImageNetLightningModel(\r\n        data_path=\"path/to/imagenet/data\",\r\n        pretrained=True,\r\n        workers=8\r\n    )\r\n\r\n    trainer = pytorch_lightning.Trainer(\r\n        gpus=4\r\n    )\r\n\r\n    trainer.test(model)\r\n\r\n    trainer.fit(model)\r\n```\r\n\r\nIf I run `trainer.test(model)` I get the performance I expect from a pretrained imagenet model. However, when running `trainer.fit(model)` the training accuracy is zero. \r\n\r\nAre the weights getting reinitialized when training is started? If so, how do I prevent that from happening?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13334",
    "createdAt": "2022-06-19T16:39:00Z",
    "updatedAt": "2022-06-23T01:37:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jordanott"
    },
    "answer": {
      "body": "> Are the weights getting reinitialized when training is started? If so, how do I prevent that from happening?\r\n\r\nthey are not, can you check the loss and training logic inside the training step. Or maybe there is an issue on how you are calculating the accuracy.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-06-21T11:05:29Z"
    }
  },
  {
    "title": "What's the relationship between number of gpu and batch size (global batch size))",
    "body": "Yesterday, I saw the code [\ud83d\udd17](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/nlp/Question_Answering_Squad.ipynb#scrollTo=ref1qSonGNhP)\r\n, which given the comment at the last cell.\r\n> To improve the performance of the model, train with multi-GPU and a global batch size of 24. So if you use 8 GPUs with trainer.gpus=8, set model.train_ds.batch_size=3\r\n\r\nIn tensorflow, we need to calculate the global batch size in multi-gpu settings. Is the same concept also applied in the multi-gpu of trainer ?\r\n\r\nIf we setup `trainer = Trainer(**{'devices':4, 'accelerator':'gpu', 'strategy':'ddp'})` with the batch size 64, will the model benefit with the larger batch size (256), or the model only benefit with batch size (64) ?\r\n\r\n```\r\n# sample code snippet :\r\ntra_ld = DataLoader(train_dset, batch_size=32, ... )\r\ntrainer = Trainer(**{'devices':4, 'accelerator':'gpu', 'strategy':'ddp'})\r\ntrainer.fit(model, tra_ld)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13314",
    "createdAt": "2022-06-16T10:16:04Z",
    "updatedAt": "2022-06-17T07:36:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "HuangChiEn"
    },
    "answer": {
      "body": "Hi @HuangChiEn, in your case shown in the sample code, the actual batch size will be `128` (`32 * 4`).\r\n\r\nIt behaves differently depending on which strategy is used. You can read more about it\r\n- in our docs: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#batch-size\r\n- in PyTorch forum: https://discuss.pytorch.org/t/do-dataparallel-and-distributeddataparallel-affect-the-batch-size-and-gpu-memory-consumption/97194",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-16T11:13:11Z"
    }
  },
  {
    "title": "Detected call of `lr_scheduler.step()` before `optimizer.step()`",
    "body": "* `pytorch-lightning` version: `1.6.4`\r\n\r\n---\r\nWhen tuning the learning_rate using `lr_find`, I get:\r\n```\r\nUserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n```\r\nRelated Issue: #5587\r\nCode snippet is given below:\r\n\r\n```python\r\ndef tune_lr(\r\n        model: LightningModule,\r\n        tuning_params: Dict[str, Any] = None,\r\n        trainer_args: Dict[str, Any] = None\r\n):\r\n    if trainer_args is None:\r\n        trainer_args = dict()\r\n    if tuning_params is None:\r\n        tuning_params = dict()\r\n    dummy_trainer = Trainer(\r\n        **trainer_args\r\n    )\r\n    lr_finder = dummy_trainer.tuner.lr_find(model=model, **tuning_params)\r\n    lr = lr_finder.suggestion()\r\n    del dummy_trainer\r\n    return lr\r\n\r\nnet.hparams.lr = tune_lr(\r\n    model=net,\r\n    tuning_params={\r\n        \"mode\": \"exponential\",\r\n        \"datamodule\": data_module,\r\n        \"min_lr\": 1e-08,\r\n        \"max_lr\": 1.0\r\n    },\r\n    trainer_args={\r\n        \"callbacks\": [\r\n            StochasticWeightAveraging(swa_lrs=1e-2),\r\n            EarlyStopping(\r\n                monitor=\"Validation-Mean_Loss\",\r\n                mode=\"min\",\r\n                patience=10,\r\n                strict=True,\r\n                check_finite=True,\r\n                min_delta=1e-3,\r\n                check_on_train_epoch_end=False,\r\n            )\r\n        ],\r\n        \"accumulate_grad_batches\": 1,\r\n        \"check_val_every_n_epoch\": 10,\r\n        \"num_sanity_val_steps\": 0,\r\n        \"detect_anomaly\": False,\r\n        \"log_every_n_steps\": 1,\r\n        \"enable_progress_bar\": True,\r\n        \"precision\": 16,\r\n        \"sync_batchnorm\": False,\r\n        \"enable_model_summary\": False,\r\n        \"max_epochs\": max_epochs,\r\n        \"accelerator\": \"gpu\",\r\n        \"devices\": -1\r\n    }\r\n)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13300",
    "createdAt": "2022-06-15T12:04:22Z",
    "updatedAt": "2022-06-15T13:52:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "digital-idiot"
    },
    "answer": {
      "body": "@digital-idiot Can I see the trainer args as well?",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-15T12:33:38Z"
    }
  },
  {
    "title": "Bibtex Citation",
    "body": "Hello. I love PL!\r\n\r\nis there a bibtex version of this citation out there?\r\n\r\nThanks\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/CITATION.cff#L1\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13284",
    "createdAt": "2022-06-14T08:42:14Z",
    "updatedAt": "2022-06-15T13:50:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "noamsgl"
    },
    "answer": {
      "body": "@noamsgl You can copy it from \"Cite this repository\" in the side bar of the top page.\r\n\r\n![Screen Shot 2022-06-15 at 22 42 09](https://user-images.githubusercontent.com/20610905/173842305-2f939fa1-2bf6-4404-8890-fd693b4b3be2.png)",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-15T13:46:04Z"
    }
  },
  {
    "title": "Interesting behaviour/interaction between matplotlib and validation_epoch_end(self, outputs)",
    "body": "I am modifying some code taken out of the lightning docs and training a VAE to learn a represenation of time series thought to contain structure.  The validation outputs are reconstructions of the time series.  The following code is written with the intention of grabbing a random output and plotting it.\r\n\r\nTwo things happen that I don't understand. \r\n\r\n1 - It seems the plt object from the previous epoch is still in memory and the new data is plotted alongside the old.  I have tried deleting the data explicitly but to no avail... \r\n2 - Somehow both the input and the output are getting plotted (in the attached plot you see the learned noise, and the well formed actual singal I am trying to reconstruct. I have no idea where the input signal is coming from - is it embedded in the output tensor somewhere?\r\n \r\n    def validation_epoch_end(self, outputs):\r\n        if not self.save_images:\r\n            return\r\n        if not os.path.exists(self.save_path):\r\n            os.makedirs(self.save_path)\r\n        choice = random.choice(outputs)\r\n        output_sample = random.choice(choice[0])\r\n        plt.plot(output_sample.cpu())\r\n        plt.savefig(f\"{self.save_path}/epoch_{self.current_epoch}.png\")\r\n        print('this is def interesting')\r\n\r\n![epoch_0](https://user-images.githubusercontent.com/25708723/173474539-a50d4833-debe-4a69-a777-5aff7b555132.png)\r\n![epoch_1](https://user-images.githubusercontent.com/25708723/173474546-81e00883-e6e1-43eb-b2ac-de025024df34.png)\r\n![epoch_2](https://user-images.githubusercontent.com/25708723/173474554-d2e35885-4080-423c-97d5-1fb51e78e320.png)\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13282",
    "createdAt": "2022-06-14T01:35:34Z",
    "updatedAt": "2022-06-14T14:30:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nick-torenvliet"
    },
    "answer": {
      "body": "Creating a new figure before plotting?\r\n```python\r\nfig, ax = plt.subplots()\r\nax.set_xlim(0, 10)\r\nax.plot([1, 5], [0, 10])\r\n```",
      "author": {
        "login": "semaphore-egg"
      },
      "createdAt": "2022-06-14T14:21:53Z"
    }
  },
  {
    "title": "DDP: NCCL \" The server socket has failed to bind to...\"",
    "body": "Hi, I'm trying to use my Pytorch Lightning code in conjunction with [Jukebox](https://github.com/openai/jukebox) which has [its own set of routines for distributed training](https://github.com/openai/jukebox/blob/master/jukebox/utils/dist_utils.py) via the `torch.distributed.run` method.   I have read the [PyTorchLightning docs on torch.distributed](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html?highlight=torch.distributed.run#torch-distributed-elastic) and followed them as far as I know:\r\n\r\n```bash\r\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 --rdzv_id=31459 --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1 ./my_script.py --myarg1=thing  ...etc\r\n```\r\n\r\nIf I only ever run on 1 GPU there's no problem, but when I try to run on more than 1 GPU via DDP, then I get many errors from NCCL such as\r\n\r\n```\r\n[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).\r\n[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\r\n[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.\r\nCaught error during NCCL init (attempt 0 of 5): The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\r\n```\r\n\r\n\"Already in use\":  Presumably Jukebox, which it runs its own MPI initialization in the form of \r\n```Python\r\nfrom jukebox.utils.dist_utils import setup_dist_from_mpi\r\n...\r\nrank, local_rank, device = setup_dist_from_mpi()\r\n```\r\n(^^ This call is inside my TRAINER module, BTW, so it should be AFTER Lightning sets up the `init_process_group()`, right?) \r\n\r\n...Jukebox is trying to setup re-reserve the slots ALREADY setup/reserved by the Lightning Trainer when the [DDP Spawn](https://github.com/PyTorchLightning/pytorch-lightning/blob/c1f05021ff0093f720770a6065ab62a70c535add/pytorch_lightning/strategies/ddp_spawn.py#L160) routine in [PytorchLightning itself already calls ` torch.distributed.init_process_group()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/pytorch_lightning/utilities/distributed.py#L355) ; and rather than just polling the `os.` environment for keys like RANK and MASTER_ADDR, Jukebox is ignoring those for now. \r\n\r\n## My question:\r\nIf PyTorch Lightning is setting the `torch.distributed.dist` object already, then is there a way I can get access to it?  ( for interfacing with  the Jukebox code?)\r\n\r\n(or can I call `init_process_group()` myself or obtain the result from when PyTorch Lightning called it?)\r\n\r\nBecause right now, if I try NOT calling their MPI initialization routine `setup_dist_from_mpi()` and instead just communicate key values based on environment variables a la:\r\n\r\n```Python\r\n        rank, local_rank, device = os.getenv('RANK'), os.getenv('RANK'), self.device\r\n```\r\n\r\n...then whereever the Jukebox code calls something like `dist.barrier()`, then I get an error about \r\n```\r\nRuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1102, internal error, NCCL version 21.0.3\r\nncclInternalError: Internal check failed. This is either a bug in NCCL or due to memory corruption\r\n\r\n...\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group....\r\n``` \r\nBut I thought [Lightning was supposedly calling init_process_group() already?](https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/pytorch_lightning/utilities/distributed.py#L355)....? \r\nSo I'm confused.   Any tips?\r\n\r\n**UPDATE:** Before the call to the Jukebox stuff, I did check and the pytorch distributed `dist.is_available()` is True, so it looks like Lightning may have done something already by that point.  But in that case I'm confused about why we're still seeing that RuntimeError about process group not being initialized:\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13264",
    "createdAt": "2022-06-10T03:23:03Z",
    "updatedAt": "2024-05-17T11:19:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "drscotthawley"
    },
    "answer": {
      "body": "# MY SOLUTION: \r\n\r\nI think I've distilled it to two simple parts. Default values all ended up being ok, and no special environment-variable-setting proved necessary (e.g. I unset all the NCCL flags I'd tried earlier).  Two things:\r\n\r\n1. running as instructed with the `torch.distributed.run` as before.  Although the default values were fine. e.g. \r\n```\r\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\n\r\n2. Near the top of my Trainer init code (and before the Jukebox stuff), initialize :\r\n```Python\r\ndist.init_process_group(backend=\"nccl\")\r\n```\r\n no other parts were essential.   And I could either have the trainer strategy set to \"ddp\" or \"fsdp\" or nothing at all; made no difference. \r\n\r\n3. ALTHOUGH, one extra other thing that makes it go even faster: For some reason `OMP_NUM_THREADS` is not being set and so you see a warning message that it's getting set to 1 by default.  No need to leave that!   So my final, well-performing invocation looks like:\r\n```\r\nOMP_NUM_THREADS=12 python -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\none could also permanenty set `export OMP_NUM_THREADS=12` but I haven't bothered to do that yet. \r\n",
      "author": {
        "login": "drscotthawley"
      },
      "createdAt": "2022-06-10T05:56:49Z"
    }
  },
  {
    "title": "FP16 does not decrease much GPU memory",
    "body": "hi, I just tried mix precision training with precision=16 set in the trainer. I found the training speed does increase by around 30%, but the GPU memory merely decreases. Should it be half of the raw memory?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13252",
    "createdAt": "2022-06-08T12:48:00Z",
    "updatedAt": "2022-08-20T09:51:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "icoz69"
    },
    "answer": {
      "body": "Hi @icoz69 AFAIK, the memory usage depends on your model architecture, specifically, the ratio of the model size to the size of activations. This is because, with amp, your model always stays in fp32 while some operations in your model are done in fp16.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-09T00:55:47Z"
    }
  },
  {
    "title": "Conditionally have a single or multiple data Loaders.",
    "body": "Hi, I have a use case where I need to have a single validation dataloader or multiple validation dataloader based on a flag. My data module looks something like \r\n```python\r\nclass CodeXGlueDataModule(pl.LightningDataModule):\r\n    def __init__(self, args):\r\n        ...\r\n\r\n    def train_dataloader(self):\r\n        ...\r\n\r\n    def val_dataloader(self):\r\n        loader1 = ...\r\n\r\n        if flag:\r\n            loader2 = ...\r\n            loaders = [dataloader_ppl, dataloader_bleu]\r\n            return loaders\r\n        loaders = [dataloader_ppl, ]\r\n        return loaders\r\n```\r\nand my lightening model looks like\r\n```python\r\ndef validation_step(self, batch, batch_idx, dataloader_idx):\r\n    if dataloader_idx == 0:\r\n        ...\r\n        return {\"x\":x}\r\n\r\n    elif dataloader_idx == 1:\r\n        ...\r\n        return {\"y\":y}    \r\n```\r\n\r\nWhen I run the code with flag = True, it works fines but when I run it with flag = False, I get an error saying\r\n```\r\nvalidation_step() missing 1 required positional argument: 'dataloader_idx'\r\n```\r\nI understand that a single dataloader is not treated as a list and hence there is no dataloader_idx argument for the validation_step function but is there something I can do to make this work. \r\n\r\nThanks in Advance! ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13241",
    "createdAt": "2022-06-07T03:50:40Z",
    "updatedAt": "2022-06-07T18:04:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "prateeky2806"
    },
    "answer": {
      "body": "@prateeky2806 Making it an optional keyword argument will avoid the error?\r\n```diff\r\n-def validation_step(self, batch, batch_idx, dataloader_idx):\r\n-    if dataloader_idx == 0:\r\n+def validation_step(self, batch, batch_idx, dataloader_idx=None):\r\n+    if dataloader_idx == 0 or dataloader_idx is None:\r\n        ...\r\n\r\n    elif dataloader_idx == 1:\r\n        ...\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-07T04:13:30Z"
    }
  },
  {
    "title": "How to redirect output of rich progress bar to file?",
    "body": "When using rich progress bar with shell command like _python train.py > file.log 2>&1_, the file.log don't save any progress bar output. But check rich library [documents](https://rich.readthedocs.io/en/latest/console.html#error-console), it says rich will write to sys.stdout by default. Anyone know how to do that?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13229",
    "createdAt": "2022-06-04T10:26:20Z",
    "updatedAt": "2022-06-13T01:56:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ZeguanXiao"
    },
    "answer": {
      "body": "In fact, it does redirect to file.log and you can see them if you wait until all is over. I think maybe you can submit an issue and see if PL can keep flushing when running.",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2022-06-07T13:08:02Z"
    }
  },
  {
    "title": "RuntimeError: Trying to backward through the graph a second time",
    "body": "Hi Everyone, \r\n\r\nI'm trying to use the [torchdyn](https://github.com/DiffEqML/) library to solve a problem of mine, code is given here,  when I try to run this code I'm getting an error which says \r\n\r\n## Error:\r\n```\r\n\r\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already \r\n\r\nbeen freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify \r\n\r\nretain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after \r\n\r\ncalling backward.\r\n\r\n\r\n```\r\n\r\n\r\n## Code \r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torch.nn as nn\r\nimport copy\r\n\r\nfrom torch.autograd import grad\r\n\r\nimport torch\r\n\r\n\r\ndef set_initial_conditions(n_agents):\r\n    if n_agents == 12:\r\n        #px,py,qx,qy\r\n        x0 = torch.tensor([[0], [0.5], [-3], [5],\r\n                           [0], [0.5], [-3], [3],\r\n                           [0], [0.5], [-3], [1],\r\n                           [0], [-0.5], [-3], [-1],\r\n                           [0], [-0.5], [-3], [-3],\r\n                           [0], [-0.5], [-3], [-5],\r\n                           # second column\r\n                           [-0], [0.5], [3], [5],\r\n                           [-0], [0.5], [3], [3],\r\n                           [-0], [0.5], [3], [1],\r\n                           [0], [-0.5], [3], [-1],\r\n                           [0], [-0.5], [3], [-3],\r\n                           [0], [-0.5], [3], [-5],\r\n                           ])\r\n        xbar = torch.tensor([[0], [0], [3], [-5],\r\n                             [0], [0], [3], [-3],\r\n                             [0], [0], [3], [-1],\r\n                             [0], [0], [3], [1],\r\n                             [0], [0], [3], [3],\r\n                             [0], [0], [3], [5],\r\n                             # second column\r\n                             [0], [0], [-3], [-5],\r\n                             [0], [0], [-3], [-3],\r\n                             [0], [0], [-3], [-1],\r\n                             [0], [0], [-3], [1],\r\n                             [0], [0], [-3], [3],\r\n                             [0], [0], [-3], [5.0],\r\n                             ])\r\n    else:\r\n        x0 = (torch.rand(4*n_agents, 1)-0.5)*10\r\n        xbar = (torch.rand(4*n_agents, 1)-0.5)*10\r\n    return x0, xbar\r\n\r\n# X  = (J-R)*dV/dx + Fy + Gu\r\n# Y = G*dV/dx\r\n# forward = return (J-R)*dV/dx\r\n\r\nclass SystemEnv(nn.Module):\r\n    def __init__(self, V, K, n_agents=1, xbar=None, ctls=None, batch_size=1, **kwargs):\r\n        \"\"\" Initialize the environment. Here we represent the system.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.K = K\r\n        self.V = V\r\n        self.k = torch.tensor(1.0)\r\n        self.b = torch.tensor(0.2)\r\n        self.m = torch.tensor(1.0)\r\n        J = torch.tensor([[0, 0, -1, 0],\r\n                          [0, 0, 0, -1],\r\n                          [1., 0, 0, 0],\r\n                          [0, 1., 0, 0]])\r\n        R = torch.tensor([[self.b, 0, 0, 0],\r\n                          [0, self.b, 0, 0],\r\n                          [0, 0, 0, 0],\r\n                          [0, 0, 0, 0]])\r\n        #number of agents\r\n        self.n_agents = n_agents\r\n        #dimension of state space q,p = (2,2)\r\n        self.ni = 4\r\n        self.n = self.ni * n_agents\r\n        if ctls is None:\r\n            ctls = torch.ones(1, n_agents)\r\n            # n_of_inputs x n_masses\r\n        self.interconnection = ctls\r\n        self.J = torch.zeros((self.n, self.n))\r\n        self.R = torch.zeros((self.n, self.n))\r\n        for i in range(0, n_agents):\r\n            self.J[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = J\r\n            self.R[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = R\r\n        if xbar is None:\r\n            xbar = torch.zeros(self.n, 1)\r\n        self.xbar = xbar\r\n        self.B = torch.tensor([[1.0, 0], [0, 1.0]])\r\n        self.batch_size = batch_size\r\n\r\n    def g(self, t, x):\r\n        # g = torch.zeros((self.n, 2*int(self.interconnection.sum())))\r\n        # idx = 0\r\n        # for i, j in self.interconnection.nonzero():\r\n        #     g[(4*j), idx] = 1\r\n        #     g[(4*j)+1, idx+1] = 1\r\n        #     idx += 2\r\n        # return g\r\n        g_agent = torch.tensor([[1.0, 0], [0, 1.0], [0, 0], [0, 0]])\r\n        self.g_agent = copy.deepcopy(g_agent)\r\n        g = torch.zeros(0, 0)\r\n        for i in range(self.n_agents):\r\n            g = torch.block_diag(g, g_agent)\r\n        return g\r\n\r\n    def H(self, t, x):\r\n        delta_x = x - self.xbar\r\n        Q_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\r\n        Q = torch.zeros((self.n, self.n))\r\n        for i in range(self.n_agents):\r\n            Q[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = Q_agent\r\n        R_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\r\n        return 0.5 * F.linear(F.linear(delta_x.T, Q), delta_x.T)\r\n\r\n    def gradH(self, t, x):\r\n        x = x.requires_grad_(True)\r\n        return torch.autograd.grad(self.H(t, x), x, allow_unused=False, create_graph=True)[0]\r\n\r\n    def f(self, t, x):\r\n        dHdx = self.gradH(t, x)\r\n        return F.linear(dHdx.T, self.J - self.R)\r\n\r\n    def _dynamics(self, t, x, u):\r\n        # p = torch.cat(x[0::4], x[1::4], 0)\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).to(0)\r\n        p = torch.stack((x[:, 0::4], x[:, 1::4]), dim=2).view(x.shape[0], self.n_agents * 2).to(0)\r\n        # [p;q] = [J-R]*delV+[B,0]*u\r\n        delVq = self._energy_shaping(q)\r\n        # delVp = p from formulation\r\n        r1 = torch.stack((delVq, p), dim=2).to(0)\r\n        r1 = r1.view(x.shape[0], self.n_agents*4)\r\n        JR = (self.J - self.R).to(0)\r\n        # input, matrix\r\n        result  = torch.zeros(x.shape[0], self.n_agents*4).to(0)\r\n        u = u.view(x.shape[0], self.n_agents * 2).to(0)\r\n        for i in range(r1.shape[0]):\r\n            par1 = torch.matmul(JR, r1[i, :])\r\n            g = self.g(t, x).to(0)\r\n            # u\u03b8 = \u2212BInv*\u2207qV(q) \u2212 K\u2217(t, q, p)B*qdot\r\n            par2 = F.linear(g, u[i,:])\r\n            result[i,:] = torch.add(par1, par2).to(0)\r\n        return result\r\n\r\n    def _energy_shaping(self,q):\r\n        # dVdx = grad(self.V(q).sum(), q, create_graph=True)[0]\r\n        dVdx = grad(self.V(q).sum(), q, create_graph=True, retain_graph=True)[0]\r\n        return -1*dVdx\r\n\r\n    # def _energy(self,t,x):\r\n    #     Q_agent = torch.diag(torch.tensor([1 / 2*self.m, self.k]))\r\n    #     temp_x = torch.zeros(self.n_agents*2)\r\n    #     temp_p = torch.zeros(self.n_agents*2)\r\n    #     x_temp = x\r\n    #     x_temp = x.view(self.n_agents, 4)\r\n    #     for i in range(self.n_agents):\r\n    #         for j in range(len(x[i])):\r\n    #             temp_x[i]=(0.5*self.m)*()\r\n    #             temp_p[i] = (0.5 * self.m)\r\n    #\r\n    #     F.linear(Q_agent, torch.cat(torch.cdist(x[:,2:...],torch.zeros_like(x[:,2:...])),torch.cdist(x[...,:2]-xbar)))\r\n\r\n    def _damping_injection(self,x):\r\n        # x = [pdot, qdot]\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True)\r\n        Kmat = torch.diag(self.K(x.to(0)).ravel())\r\n        return -1*F.linear(Kmat, q.view(1, x.shape[0]*self.n_agents*2).to(0))\r\n\r\n    def forward(self, t, x):\r\n        # x = [p,q]\r\n        print(\"in forward\")\r\n        x = x.requires_grad_(True)\r\n        #batch_size, n_agents*4, n_agents = x\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True).to(0)\r\n        u1 = self._energy_shaping(q)\r\n        u1 = u1.view(x.shape[0]*self.n_agents*2, 1 )\r\n        u2 = self._damping_injection(x)\r\n        u = u1+u2\r\n        return  (self._dynamics(t,x,u),u)\r\n        # return self.f(t, x).T\r\n\r\nclass AugmentedDynamics(nn.Module):\r\n    # \"augmented\" vector field to take into account integral loss functions\r\n    def __init__(self, f, int_loss):\r\n        super().__init__()\r\n        self.f = f\r\n        self.int_loss = int_loss\r\n        self.nfe = 0.\r\n\r\n    def forward(self, t, x):\r\n        self.nfe += 1\r\n        x = x[:,:f.n_agents*4]\r\n        (dxdt,u) = self.f.forward(t, x)\r\n        dldt =   self.int_loss.int_loss(x, u, self.f)\r\n        return torch.cat([dxdt, dldt], 1).cpu()\r\n\r\n\r\n\r\nclass ControlEffort(nn.Module):\r\n    # control effort integral cost\r\n    def __init__(self, f, x0, xbar, dt):\r\n        super().__init__()\r\n        self.f = f\r\n\r\n        self.x = torch.cat((x0[0::4], x0[1::4]), dim=1)\r\n        self.x = self.x.repeat(f.batch_size, 1)\r\n        self.x = self.x.reshape(f.batch_size,f.n_agents*2)\r\n\r\n        self.xbar = torch.cat((xbar[0::4], xbar[1::4]), dim=1)\r\n        self.xbar = self.xbar.repeat(f.batch_size, 1)\r\n        self.xbar = self.xbar.reshape(batch_size, f.n_agents*2)\r\n\r\n        self.dt = dt\r\n\r\n    def forward(self, t, x):\r\n        with torch.set_grad_enabled(True):\r\n            q = torch.cat((x[2::4],x[3::4]),0).requires_grad_(True)\r\n            # q = torch.transpose(q, 0, 1)\r\n            u1 = torch.transpose(self.f._energy_shaping(q), 0, 1)\r\n            u2 = self.f._damping_injection(x).to(0)\r\n            u = u1+u2\r\n        return u\r\n\r\n    def int_loss(self, x, u, clsys):\r\n        x = x.reshape(x.shape[0],self.f.n_agents,2,2)\r\n        vel = torch.index_select(x, 2, torch.tensor([1]))\r\n        vel = vel.reshape(x.shape[0],self.f.n_agents*2)\r\n        self.x = self.x.cpu()+torch.mul(vel,self.dt)\r\n        self.x = self.x.to(0)\r\n        self.xbar = self.xbar.to(0)\r\n        self.u = u.reshape(self.f.batch_size,self.f.n_agents*2)\r\n        self.clsys = clsys\r\n        lx = self.f_loss_states().reshape(self.f.batch_size,1).to(0)\r\n        lu = self.f_loss_u().reshape(self.f.batch_size,1).to(0)\r\n        lca = self.f_loss_ca()\r\n        loss = lx+lu+lca\r\n        return loss.to(0)\r\n\r\n    def f_loss_states(self, test=False):\r\n        # clsys = SystemEnv\r\n        loss_function = nn.MSELoss(reduction='none')\r\n        xbar = self.clsys.xbar\r\n        # steps = t.shape[0]\r\n        if test:\r\n            gamma = 1\r\n        else:\r\n            gamma = 0.95\r\n        loss = loss_function(self.x, self.xbar)\r\n        loss = loss.view(self.f.batch_size, 2*self.f.n_agents)\r\n        loss = loss.sum(dim=1)\r\n        return loss\r\n\r\n    def f_loss_u(self):\r\n        loss_u = ((self.u*self.clsys.b) ** 2).sum(dim=1)\r\n        return loss_u\r\n\r\n\r\n    def f_loss_ca(self, min_dist=0.5):\r\n        steps = self.x.shape[0]\r\n        min_sec_dist = 1.4 * min_dist\r\n        # for i in range(steps):\r\n        #     for j in range(i+1, steps):\r\n        #         dist = torch.norm(self.x[i, :] - self.x[j, :])\r\n        #         if dist < min_sec_dist:\r\n        #             return torch.tensor(1e10)\r\n        # return torch.tensor(0)\r\n        loss_ca_ = torch.zeros(self.f.batch_size,1).to(0)\r\n        for i in range(self.x.shape[0]):\r\n            x = self.x[i,:].view(self.f.n_agents*2,1).to(0)\r\n            clsys = self.clsys\r\n            # collision avoidance:\r\n            # deltax = x[:, 2::4].repeat(1, 1, clsys.n // 4) - x[:, 2::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\r\n            deltax = x[0::2].repeat(1, clsys.n // 4) - x[0::2].transpose(0, 1).repeat( clsys.n // 4, 1)\r\n            # deltay = x[:, 3::4].repeat(1, 1, clsys.n // 4) - x[:, 3::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\r\n            deltay = x[1::2].repeat(1, clsys.n // 4) - x[1::2].transpose(0, 1).repeat(clsys.n // 4, 1)\r\n            distance_sq = deltax ** 2 + deltay ** 2\r\n            mask = torch.logical_not(torch.eye(clsys.n // 4)).unsqueeze(0).repeat(steps, 1, 1)\r\n            mask = mask.to(0)\r\n            loss_ca_[i,:] = (1 / (distance_sq + 1e-3) * (distance_sq.detach() < (min_sec_dist ** 2)) * mask).sum() / 2\r\n        return loss_ca_\r\n\r\n\r\nimport pytorch_lightning as pl\r\nimport torch.utils.data as data\r\n\r\ndef weighted_log_likelihood_loss(x, target, weight):\r\n    # weighted negative log likelihood loss\r\n    log_prob = target.log_prob(x)\r\n    weighted_log_p = weight * log_prob\r\n    return -torch.mean(weighted_log_p.sum(1))\r\n\r\nclass EnergyShapingLearner(pl.LightningModule):\r\n    def __init__(self, model: nn.Module, prior_dist, target_dist, t_span, sensitivity='autograd', n_agents=1):\r\n        super().__init__()\r\n        self.model = model\r\n        self.prior, self.target = prior_dist, target_dist\r\n        self.t_span = t_span\r\n        self.batch_size = batch_size\r\n        self.lr = 5e-3\r\n        self.n_agents = n_agents\r\n        self.weight = torch.ones(n_agents * 4).reshape(1, n_agents * 4)\r\n\r\n    def forward(self, x):\r\n        return self.model.odeint(x, self.t_span)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # sample a batch of initial conditions\r\n        # x0 = self.prior.sample((self.batch_size,))\r\n        n_agents = self.n_agents\r\n        x0 = torch.rand((self.batch_size,n_agents*4))\r\n        # x0, _ = set_initial_conditions(n_agents)\r\n        # Integrate the model\r\n        x0 = torch.cat([x0, torch.zeros(self.batch_size, 1)], -1).to(x0)\r\n        xs, xTl = self(x0)\r\n        xT, l = xTl[-1, :, :2], xTl[-1, :, -1:]\r\n\r\n        # Compute loss\r\n        # terminal_loss = weighted_log_likelihood_loss(xT, self.target, self.weight.to(xT))\r\n        integral_loss = torch.mean(l)\r\n        loss = 0 + 0.01 * integral_loss\r\n        return {'loss': loss.cpu()}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\r\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\r\n        return [optimizer], [scheduler]\r\n\r\n    def train_dataloader(self):\r\n        dummy_trainloader = data.DataLoader(\r\n            data.TensorDataset(torch.Tensor(1, 1), torch.Tensor(1, 1)),\r\n            batch_size=1)\r\n        return dummy_trainloader\r\n\r\n# # # # # # # # Parameters # # # # # # # #\r\nn_agents = 2  # agents are not interconnected (even when having a controller). Each of them acts independently\r\nt_end = 5\r\nsteps = 101\r\nmin_dist = 0.5  # min distance for collision avoidance\r\n# (px, py, qx, qy) - for each agent\r\nhdim = 64\r\nV = nn.Sequential(\r\n    nn.Linear(n_agents*2, hdim),\r\n    nn.Softplus(),\r\n    nn.Linear(hdim, hdim),\r\n    nn.Tanh(),\r\n    nn.Linear(hdim, 1))\r\nK = nn.Sequential(\r\n    nn.Linear(n_agents*4, hdim),\r\n    nn.Softplus(),\r\n    nn.Linear(hdim, (n_agents*2)),\r\n    nn.Softplus())\r\n\r\n\r\nfrom torch.distributions import Uniform, Normal\r\n\r\n\r\ndef prior_dist(q_min, q_max, p_min, p_max, device='cpu'):\r\n    # uniform \"prior\" distribution of initial conditions x(0)=[q(0),p(0)]\r\n    lb = torch.Tensor([q_min, p_min]).to(device)\r\n    ub = torch.Tensor([q_max, p_max]).to(device)\r\n    return Uniform(lb, ub)\r\n\r\n\r\ndef target_dist(mu, sigma, device='cpu'):\r\n    # normal target distribution of terminal states x(T)\r\n    mu, sigma = torch.Tensor(mu).reshape(1, 2).to(device), torch.Tensor(sigma).reshape(1, 2).to(device)\r\n    return Normal(mu, torch.sqrt(sigma))\r\n\r\ndef weighted_log_likelihood_loss(x, target, weight):\r\n    # weighted negative log likelihood loss\r\n    log_prob = target.log_prob(x)\r\n    weighted_log_p = weight * log_prob\r\n    return -torch.mean(weighted_log_p.sum(1))\r\n\r\nfrom torchdyn.models import ODEProblem\r\n\r\n# choose solver and sensitivity method\r\nsolver = 'rk4'\r\nsensitivity = 'autograd'\r\n\r\n# init to zero par.s of the final layer\r\n# for p in V[-1].parameters(): torch.nn.init.zeros_(p)\r\n# for p in K[-2].parameters(): torch.nn.init.zeros_(p)\r\n\r\n# define controlled system dynamics\r\nx0, xbar = set_initial_conditions(n_agents)\r\nbatch_size = 4\r\n\r\nf = SystemEnv(V.to(0), K.to(0), n_agents=2, xbar=xbar, batch_size = batch_size)\r\n\r\nt_span = torch.linspace(0, 3, 30)\r\ndt = t_span[1]-t_span[0]\r\naug_f = AugmentedDynamics(f, ControlEffort(f,x0,xbar,dt))\r\n# define time horizon\r\n\r\n\r\nprob = ODEProblem(aug_f, sensitivity=sensitivity, solver=solver)\r\n\r\n# train (it can be very slow on CPU)\r\n# (don't be scared if the loss starts very high)\r\nprior = prior_dist(-1, 1, -1, 1) # Uniform \"prior\" distribution of initial conditions x(0)\r\ntarget = target_dist([0, 0], [.001, .001]) # Normal target distribution for x(T)\r\nlearn = EnergyShapingLearner(prob, prior, target, t_span, batch_size, n_agents=n_agents)\r\n# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650).fit(learn)\r\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650)\r\ntrainer.fit(learn)\r\n\r\n```\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13219",
    "createdAt": "2022-06-03T07:59:20Z",
    "updatedAt": "2022-06-03T08:47:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "josyulakrishna"
    },
    "answer": {
      "body": "This can be resolved by using\r\n\r\n`return Variable(dyn).requires_grad_(True)` ,` x = Variable(x.data, requires_grad=True)`",
      "author": {
        "login": "josyulakrishna"
      },
      "createdAt": "2022-06-03T08:47:42Z"
    }
  },
  {
    "title": "Why does DDP mode continue the program in multiple process for longer than intended?",
    "body": "Hi all, \r\n\r\nI am using the following code to start a trainer with multrigpu.\r\n```python\r\npl.Trainer(accelerator=\"gpu\", devices=get_num_gpus(), strategy=\"ddp\")\r\n```\r\n\r\nand then I have this line of code: \r\n```python3\r\ninference_outputs = self.trainer.predict(self.embedding_model, inference_dataloader)\r\n\r\nprint(\"abc\")\r\n```\r\n\r\nWhat I am seeing is that the print(\"abc\" is being printed to the number of available devices while I would hav expect only the predict function to run on multiple gpu and processes and then finish before running the next line and gather all results into `inference_outputs`. \r\n\r\nAm I missing something? Is there a way to achieve what I just described?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13216",
    "createdAt": "2022-06-02T23:35:18Z",
    "updatedAt": "2022-08-03T03:42:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hfaghihi15"
    },
    "answer": {
      "body": "@hfaghihi15 That's how it is! With DDP, Lightning runs the whole script in its subprocesses as described in the doc here: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#distributed-data-parallel\r\n\r\n\r\n> This Lightning implementation of DDP calls your script under the hood multiple times with the correct environment variables:\r\n> ```shell\r\n> # example for 3 GPUs DDP\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=1 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=2 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> ```\r\n\r\n\r\n---\r\n\r\nIn case you want to run something in only one process, you can utilise the trainer property:\r\n```python\r\nif trainer.is_global_zero:\r\n    print(\"abc\")\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#is-global-zero",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-03T05:25:56Z"
    }
  },
  {
    "title": "Loading a large HuggingFace LM (T5-11B) using DeepSpeed",
    "body": "Hi,\r\n\r\nI'm recently trying to load the T5-11B model on GPUs using the Deepspeed framework. I'm using 5 Quadro RTX 8000 GPUs (48GB GPU memory each, node has 100 cores and 500GB RAM). I also varied the GPU numbers between [2, 10]. I've been getting an error \"RuntimeError: CUDA error: an illegal memory access was encountered\". This is different from the OOM errors I was getting initially. I don't know if there is some issue with my usage of the API or something else. I tried with T5-large and the code works fine. But I don't see what's the error with T5-11B, especially when I'm allocating so many GPUs already. A HuggingFace thread shows that T5-11B model can be loaded even on one GPU. So, I feel it should be doable here for sure. Any direction to debug further/ help would be super helpful. Below is a code sketch of how I used `deepspeed_stage_3`.\r\n\r\n```\r\nclass MyModel(LightningModule):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\t\tself.ptlm = T5ForConditionalGeneration.from_pretrained('t5-11b', low_cpu_mem_usage=True, torch_dtype=\"auto\")\r\n\t\t# Also tried without low_cpu_mem_usage and torch_dtype\r\n\r\n\tdef configure_sharded_model(self):\r\n\t\t# ptlm = auto_wrap(self.ptlm)\t# Also tried the auto_wrap function here\r\n\t\tself.reasoner = self.ptlm\r\n\r\n\tdef forward(self, x):\r\n\t\treturn self.reasoner(x)\r\n\r\n\tdef configure_optimizers(self):\r\n\t\treturn FusedAdam(self.reasoner.parameters(), lr=1e-5)\r\n\r\ndef run(args):\r\n\tmodel = MyModel()\r\n\ttrainer\t= Trainer.from_argparse_args(\r\n\t\targs,\r\n\t\tstrategy=\"deepspeed_stage_3\",\t# Also tried \"ddp_sharded\"\r\n\t\taccelerator=\"gpu\",\r\n\t\tprecision=16,\r\n\t)\r\n\r\n```\r\n\r\n\r\nThe most recent error trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/soumya/git/lr_dataset/src/main.py\", line 231, in <module>\r\n    trainer.fit(model, dm)\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 768, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 736, in _call_and_handle_interrupt\r\n    self._teardown()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1298, in _teardown\r\n    self.strategy.teardown()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 476, in teardown\r\n    torch.cuda.empty_cache()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/torch/cuda/memory.py\", line 114, in empty_cache\r\n    torch._C._cuda_emptyCache()\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\n``` ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13207",
    "createdAt": "2022-06-02T04:35:41Z",
    "updatedAt": "2024-01-14T10:33:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "soumyasanyal"
    },
    "answer": {
      "body": "A more whole answer here, we've added a function to `lightning-transformers` to help signal to HF that we'd like to enable sharding when loading their pre-trained weights (works only for DeepSpeed). Here is the code you need to add to your LightningModule! Note that the model has to be made in the `setup` function after the environment has been setup by Lightning.\r\n\r\n```bash\r\npip install lightning-transformers\r\n```\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom transformers import T5ForConditionalGeneration\r\nfrom lightning_transformers.utilities.deepspeed import enable_transformers_pretrained_deepspeed_sharding\r\n\r\n\r\nclass MyModel(pl.LightningModule):\r\n\r\n    def setup(self, stage: Optional[str] = None) -> None:\r\n        if not hasattr(self, \"ptlm\"):\r\n            enable_transformers_pretrained_deepspeed_sharding(self)\r\n            self.ptlm = T5ForConditionalGeneration.from_pretrained(\"t5-11b\")\r\n```",
      "author": {
        "login": "SeanNaren"
      },
      "createdAt": "2022-06-29T10:12:44Z"
    }
  },
  {
    "title": "Effective batch size in DDP",
    "body": "I have max batch size of 4 in single gpu. If 2 gpus are used, should I increase the batch size to 8 such that each gpu gets 4 batches. Or I just keep it as 4 and PL will load 2 four-batches data to 2 gpus? \r\nBased on the doc, \r\n```\r\nIn DDP, DDP_SPAWN, Deepspeed, DDP_SHARDED, or Horovod your effective batch size will be 7 * devices * num_nodes.\r\n```\r\nI think it is the second case?\r\n\r\nI also have another problem related to ddp training, which is posted on this link below. \r\nhttps://forums.pytorchlightning.ai/t/how-to-initialize-tensors-that-are-in-the-right-device-when-ddp-are-used/1708\r\n\r\nI post it here for convenience.\r\n\r\nI am incorporating  a pytorch based model into the pl framework for ddp training. \r\nI have a lightning model \r\n```python\r\nclass ZfoldLightning(pl.LightningModule):\r\n    def __init__(self, hparams):\r\n        ...\r\n        self.model = XFold(MODEL_PARAM)\r\n```\r\nwhich initializes the `XFold` model in `__init__`.\r\nHowever, the XFold model contains many 'to device' code like `b = torch.randn(1).to(a.device)`, which is not recommended by PL. \r\nI tried to increase the batch size and train this model on two device. this does not work. OOM error appears. Turns out even DDP is used, I can only use the same batch size as that of single gpu. I think the reason is that all the tensors are stored in one gpu no matter how many gpus are ultized.\r\n\r\nOne solution is to refactor those **to device code** and use the recommended usage `a.type_as(b)`. But there are to many of code to refactor.\r\nI am wondering if there are better solutions?\r\nAny helps?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13165",
    "createdAt": "2022-05-27T06:49:44Z",
    "updatedAt": "2024-03-09T06:13:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pengzhangzhi"
    },
    "answer": {
      "body": "I have solved my problem and find out that the answer is: each gpu get #batch_size batches. If you have batch_size of 2 and 2 gpus are utilized,  each gpu gets 2 batches and 4 batches in total are feed into a forward pass.",
      "author": {
        "login": "pengzhangzhi"
      },
      "createdAt": "2022-05-30T01:00:25Z"
    }
  },
  {
    "title": "wandb_logger.experiment.config has no attribute 'update' for GPUs > 1",
    "body": "Hi, There's a little snippet from the [WandB Logger Docs](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.loggers.WandbLogger.html?highlight=wandblogger) under \"Add other config parameters:\"\r\n\r\n```Python\r\nimport pytorch_lightning as pl\r\nwandb_logger = pl.loggers.WandbLogger(project='my_project')\r\nwandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\n\r\n# and then later I run...\r\ntrainer = pl.Trainer(gpus=args.num_gpus,...)\r\n```\r\n\r\nAnd when I use this in my full training code and only run the trainer on 1 GPU, then I have no problems.  It's fine. \r\n\r\nBut if I try to run on more than one GPU, then that config.update line above yields an error:\r\n\r\n```\r\n    wandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\nAttributeError: 'function' object has no attribute 'update'\r\n``` \r\n\r\nWhat's going on here, and how do I fix it?   Thanks. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13157",
    "createdAt": "2022-05-26T04:41:36Z",
    "updatedAt": "2022-05-26T05:23:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "drscotthawley"
    },
    "answer": {
      "body": "Update. Ok so according to [this issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/11380), the higher GPUs don't get \"real\" experiments?  \r\n\r\nSo my stupid hack which seems to work is just to check that the attribute exists! \ud83d\udcaf \r\n\r\n```Python\r\nif hasattr(wandb_logger.experiment.config, 'update'):\r\n    wandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\n```",
      "author": {
        "login": "drscotthawley"
      },
      "createdAt": "2022-05-26T05:23:22Z"
    }
  },
  {
    "title": "Why is 'log' keyword in the return parameter of training_step of LightningModule is not working?",
    "body": "I am seeing the following message in the Tensorboard:\r\n    **No dashboards are active for the current data set.** \r\n\r\nI use the following code\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                \r\n                # computes loss\r\n        \r\n                tensorboard_logs = {'train_loss': loss}\r\n        \r\n                return {\"loss\": loss, 'log': tensorboard_logs}\r\n\r\nI do the same in validation_epoch_end() as well.\r\n\r\nIs this functionality of logging through the 'log' keyword not supported anymore? Should we log explicitly using self.log() or something similar?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13115",
    "createdAt": "2022-05-20T09:19:04Z",
    "updatedAt": "2022-08-10T19:27:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kochark1"
    },
    "answer": {
      "body": "@kochark1 No, it isn't supported since 1.0.0 (#3681), and yes, you need to use `self.log(...)`. See details here: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-03T04:36:12Z"
    }
  },
  {
    "title": "Pass through **kwargs in yaml config file",
    "body": "Hi all,\r\n\r\nI'd like to pass through arguments in a yaml config file which are not declared with type hints in the classes receiving them, but are instead parsed out of **kwargs. Jsonargparse doesn't like this and complains that it needs type hints. Is there a simple way to do this? I'm aware I can define wrapper subclasses that declare the arguments in their `__init__` methods, but that seems like a hack.\r\n\r\nFor example, here's a snippet I'd like to use:\r\n```\r\nstrategy:                                                                       \r\n    class_path: pytorch_lightning.strategies.DDPStrategy                        \r\n    init_args:                                                                  \r\n        process_group_backend: gloo                                             \r\n        find_unused_parameters: false                                           \r\n```\r\n\r\nHere's what jsonargparse has to say about that:\r\n```\r\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_\r\nenvironment=None, checkpoint_io=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\" does not val\r\nidate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\r\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_environment=None, checkpoint_io\r\n=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\"\r\n  - Problem with given class_path \"pytorch_lightning.strategies.DDPStrategy\":\r\n    - 'Configuration check failed :: No action for destination key \"find_unused_parameters\" to check its value.'\r\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_environment=None, checkpoi\r\nnt_io=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\"\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13081",
    "createdAt": "2022-05-15T18:59:05Z",
    "updatedAt": "2022-08-01T16:56:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "wwbrannon"
    },
    "answer": {
      "body": "Hi! This is a known issue that is not implemented atm: you can see more info in #11574 and #11653",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-05-18T15:55:44Z"
    }
  },
  {
    "title": "How to save additional variables while checkpointing?",
    "body": "In vanilla pytorch, I save like this: \r\n```python\r\nif v_loss < best_val_loss:\r\n    print(\"Found better model, saving\")\r\n    model_save_path = f\"models/{sscat_id}/{attribute}/ts={time_stamp}/best.pth\"\r\n    best_val_loss = v_loss\r\n    torch.save(\r\n        {\r\n            \"epoch\": epoch,\r\n            \"model_state_dict\": model.state_dict(),\r\n            \"optim_state_dict\": optimizer.state_dict(),\r\n            \"report\": report,\r\n            \"label_encoder_dict\": label_encoder_dict,\r\n            \"inverse_label_encoder_dict\": {v: k for k, v in label_encoder_dict.items()},\r\n            \"weighted_f1\": weighted_f1,\r\n        },\r\n        model_save_path,\r\n```\r\n\r\n\r\nHow can I save extra keys like `report`, `label_encoder_dict` etc while using ModelCheckpoint callbacks? How do I save a variable inside the LightningModule instance? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13080",
    "createdAt": "2022-05-15T18:08:39Z",
    "updatedAt": "2022-08-21T16:47:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "arunpatro-meesho"
    },
    "answer": {
      "body": "you can use [on_save_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#on-save-checkpoint) hook inside LightningModule.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-05-16T06:15:59Z"
    }
  },
  {
    "title": "Cannot import auto_move_data decorator in Colab",
    "body": "Hi everyone! I am trying to import auto_move_data from pytorch_lightning.core.decorators, but I get an import error:\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\n[<ipython-input-3-22381b260381>](https://localhost:8080/#) in <module>()\r\n     26 from pytorch_lightning import LightningDataModule\r\n     27 from pytorch_lightning import LightningModule\r\n---> 28 from pytorch_lightning.core.decorators import auto_move_data\r\n     29 from pytorch_lightning import Trainer\r\n\r\nImportError: cannot import name 'auto_move_data' from 'pytorch_lightning.core.decorators' (/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py)\r\n```\r\n\r\nDoes anybody know how to solve this problem?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13078",
    "createdAt": "2022-05-15T12:53:20Z",
    "updatedAt": "2022-05-20T19:40:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MaxTeselkin"
    },
    "answer": {
      "body": "it's no longer present in the package.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-05-16T06:18:46Z"
    }
  },
  {
    "title": "Does PL train and validate at the same time?",
    "body": "I wasn't able to find this on github -> https://forums.pytorchlightning.ai/t/dose-pl-validate-and-train-at-the-same-time/1362/2\r\n\r\nI am logging step wise in training_step and validation_step\r\n\r\nbut i see this while training -> \r\n```\r\nEpoch 0:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 49/60 [00:59<00:13,  1.22s/it, loss=0.23, v_num=19, train_loss=0.0837] \r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/11 [00:00<?, ?it/s]\r\nEpoch 0:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 51/60 [01:05<00:11,  1.28s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  18%|\u2588\u258a        | 2/11 [00:05<00:20,  2.25s/it]\r\nEpoch 0:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 53/60 [01:05<00:08,  1.24s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  36%|\u2588\u2588\u2588\u258b      | 4/11 [00:07<00:09,  1.38s/it]\r\nEpoch 0:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 55/60 [01:09<00:06,  1.26s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 6/11 [00:09<00:06,  1.25s/it]\r\nEpoch 0:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 57/60 [01:09<00:03,  1.23s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 8/11 [00:10<00:02,  1.11it/s]\r\nEpoch 0:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 59/60 [01:13<00:01,  1.24s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:13<00:01,  1.01s/it]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [01:13<00:00,  1.23s/it, loss=0.23, v_num=19, train_loss=0.0837, val_loss=0.757]\r\n```\r\n\r\nHow can the model start validating before training has completed? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13077",
    "createdAt": "2022-05-15T04:36:11Z",
    "updatedAt": "2022-06-15T00:36:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "arunpatro-meesho"
    },
    "answer": {
      "body": "by default, it validates after each epoch given that you can configured validation using `validation_step` and `val_dataloader`.\r\ndo you need to validate the model right after training is done?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-05-15T05:17:52Z"
    }
  },
  {
    "title": "Set GradScaler's init_scale",
    "body": "Hi all,\r\n\r\nI'm using Lightning to train a model which encounters large gradient updates early in training. The default init_scale of 2**16 causes the gradients to overflow to inf in certain layers, which leads to NaNs, which leads to various kinds of suboptimal behavior. But I'd still like to use FP16 for the larger batch sizes.\r\n\r\nWriting out the training loop by hand and passing GradScaler a smaller init_scale avoids this problem. Is there a way to pass this value through the Trainer class? The documentation doesn't mention a way to customize the scaler.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13061",
    "createdAt": "2022-05-13T00:04:11Z",
    "updatedAt": "2022-07-07T11:41:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "wwbrannon"
    },
    "answer": {
      "body": "I think I figured this out - you need to pass a plugin to the trainer class that implements mixed precision, and give the plugin your preferred scaler.\r\n\r\nYaml:\r\n```\r\ntrainer:\r\n    precision: 16\r\n    amp_backend: 'native'\r\n    amp_level: null\r\n\r\n    plugins:\r\n        - class_path: pytorch_lightning.plugins.precision.NativeMixedPrecisionPlugin\r\n          init_args:\r\n              precision: 16\r\n              device: 'cuda'\r\n              scaler:\r\n                  class_path: torch.cuda.amp.GradScaler\r\n                  init_args:\r\n                      # the default scale of 2**16 overflows early in training\r\n                      # and makes the gradient unstable\r\n                      init_scale: 256\r\n```",
      "author": {
        "login": "wwbrannon"
      },
      "createdAt": "2022-05-13T06:05:42Z"
    }
  },
  {
    "title": "How to gather all validation_step_outputs at validation_epoch_end and run in rank_zero properly without deadlock?",
    "body": "Hi,\r\n\r\nI am trying to gather all the output and label pairs in the validation epoch end to run a simple validation process.\r\nFirst, all validation data will be separated into different devices (controlled by DDP).\r\nThe validation step is simple as follows:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        # use valid_metric\r\n        feat, label = batch\r\n        output = self.model(feat)\r\n        del batch, feat\r\n        torch.cuda.empty_cache()\r\n\r\n\r\n        for i, metric in enumerate(self.metrics):\r\n            metric.update(output[:,i], label[:,i])\r\n\r\n        return {\"label\": label, \"output\": output}\r\n```\r\n\r\n\r\nTo calculate an accurate metrics, I need to gather all outputs to a single device and log on rank 0 only as follows:\r\n\r\n```\r\n def validation_epoch_end(self, validation_step_outputs):\r\n        def _valid_epoch_end(validation_step_outputs):\r\n            enable_Flag = False\r\n            # AP refers to average_precision_score in torchmetrics\r\n            all_labels = list(map(itemgetter('label'), validation_step_outputs))\r\n            all_labels = torch.cat(all_labels).cpu().detach().numpy()\r\n            all_outputs = list(map(itemgetter('output'), validation_step_outputs))\r\n            all_outputs = torch.cat(all_outputs).cpu().detach().numpy()\r\n\r\n            AP = []\r\n            for i in range(1, 17+1):\r\n                AP.append(np.nan_to_num(average_precision_score(all_labels\r\n                                                                [:, i], all_outputs[:,  i])))\r\n            mAP = np.mean(AP)\r\n            tmp = EVENT_DICTIONARY_V2\r\n            tmp = tmp.copy()\r\n            for k, v in tmp.items():\r\n                #dict[\"kick-off\"] = AP[0]\r\n                tmp[k] = AP[v]\r\n            \r\n            self.log('Valid/mAP', mAP, logger=True, prog_bar=True,\r\n                     rank_zero_only=True if self.args.strategy != 'dp' and enable_Flag else False)\r\n\r\n            label_cls = (list(EVENT_DICTIONARY_V2.keys()))\r\n            zip_iterator = zip(label_cls, AP)\r\n            AP_dictionary = dict(zip_iterator)\r\n            self.log('Valid/AP', AP_dictionary, logger=True, prog_bar=False,\r\n                     rank_zero_only=True if self.args.strategy != 'dp' and enable_Flag else False)\r\n\r\n        _valid_epoch_end(validation_step_outputs)\r\n```\r\nThe program works fine. However, the metric result is always different from DP. Therefore, I need to gather all data first.\r\nHowever, the program doesn't even pass the validation step as the deadlock. Any good idea to modify this program so that I can gather ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13041",
    "createdAt": "2022-05-12T03:50:18Z",
    "updatedAt": "2023-04-10T10:02:11Z",
    "closedAt": "2023-03-26T11:29:17Z",
    "isAnswered": true,
    "author": {
      "login": "allanchan339"
    },
    "answer": {
      "body": "The solution is as follows:\r\nInstead of using \r\n```\r\nif self.trainer.is_global_zero:\r\n    all_val_outs = self.all_gather(...)\r\n```\r\nThe function above will hang as card 0 is trying to communicate other cards\r\n\r\nYou should write code in this way\r\n```\r\nall_val_out = self.all_gather(...)\r\n\r\nif self.trainer.is_global_zero:\r\n    # merge output and process\r\n\r\nself.trainer.strategy.barrier() #to let other cards to wait\r\n\r\n```",
      "author": {
        "login": "allanchan339"
      },
      "createdAt": "2023-03-26T11:29:17Z"
    }
  },
  {
    "title": "Model Pruning (Lottery Ticket Hypothesis) Not Reinitializing Weights",
    "body": "I am experimenting with model pruning in pytorch lightning. I noticed when pruning using the lottery ticket hypothesis (LTH), weights are not reset to the original initialization as proposed in the LTH paper and mentioned in the lightning docs. I reproduced the behaviour I faced in my own work using Tutorial 5: Transformers and Multi-Head Attention. Sharing this here to verify my analysis.\r\n\r\nCode I added:\r\n```python\r\nclass CheckPruningWeight(pl.Callback):\r\n  \r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        print(f\"\\nEPOCH {trainer.current_epoch} STARTING\")\r\n        for name in trainer.model.state_dict():\r\n          if 'input_net.1' in name:\r\n              print(name, trainer.model.state_dict()[name][:1])\r\n        print('\\n')\r\n\r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        print(f\"\\nEPOCH {trainer.current_epoch} ENDING\")\r\n        for name in trainer.model.state_dict():\r\n          if 'input_net.1' in name:\r\n              print(name, trainer.model.state_dict()[name][:1])\r\n```\r\n\r\nPruning callback:\r\n```python\r\n pruning_callback = pl.callbacks.ModelPruning(\r\n            pruning_fn=\"l1_unstructured\",\r\n            amount=0.2,\r\n            use_global_unstructured=True,\r\n            use_lottery_ticket_hypothesis=True,\r\n            verbose=1,\r\n            parameter_names=['weight'],\r\n            resample_parameters=False,\r\n            prune_on_train_epoch_end=False,\r\n        )\r\n```\r\nTrainer:\r\n```python\r\n trainer = pl.Trainer(\r\n        default_root_dir=root_dir,\r\n        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\r\n                   pruning_callback,\r\n                   CheckPruningWeight()],\r\n        gpus=1 if str(device).startswith(\"cuda\") else 0,\r\n        max_epochs=5,\r\n        gradient_clip_val=5,\r\n        progress_bar_refresh_rate=1,\r\n    )\r\n```\r\n\r\n\r\nTraining Snippet:\r\n```\r\n \r\n  | Name                | Type               | Params\r\n-----------------------------------------------------------\r\n0 | input_net           | Sequential         | 352   \r\n1 | positional_encoding | PositionalEncoding | 0     \r\n2 | transformer         | TransformerEncoder | 8.5 K \r\n3 | output_net          | Sequential         | 1.4 K \r\n-----------------------------------------------------------\r\n10.3 K    Trainable params\r\n0         Non-trainable params\r\n10.3 K    Total params\r\n0.041     Total estimated model params size (MB)\r\nEpoch 4: 10%\r\n40/398 [00:00<00:08, 41.01it/s, loss=0.294, v_num=15]\r\n\r\nEPOCH 0 STARTING\r\ninput_net.1.weight tensor([[ 0.0505, -0.1712,  0.3061, -0.2354,  0.3003, -0.0694,  0.2055, -0.0367,\r\n         -0.1696,  0.2973]])\r\ninput_net.1.bias tensor([0.1139])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 0/10346 (0.00%) -> 1990/10346 (19.23%)\r\n\r\nEPOCH 0 ENDING\r\ninput_net.1.bias tensor([0.1109])\r\ninput_net.1.weight_orig tensor([[ 0.0014, -0.0814,  0.2823, -0.3069,  0.2731, -0.1023,  0.2084, -0.0413,\r\n         -0.2384,  0.4107]])\r\ninput_net.1.weight_mask tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\r\n\r\nEPOCH 1 STARTING\r\ninput_net.1.bias tensor([0.1109])\r\ninput_net.1.weight_orig tensor([[ 0.0014, -0.0814,  0.2823, -0.3069,  0.2731, -0.1023,  0.2084, -0.0413,\r\n         -0.2384,  0.4107]])\r\ninput_net.1.weight_mask tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 1990/10346 (19.23%) -> 3582/10346 (34.62%)\r\n\r\nEPOCH 1 ENDING\r\ninput_net.1.bias tensor([0.1116])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0283,  0.1326, -0.1825,  0.2089, -0.0469,  0.0945, -0.0116,\r\n         -0.1252,  0.2888]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 1.]])\r\n\r\nEPOCH 2 STARTING\r\ninput_net.1.bias tensor([0.1116])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0283,  0.1326, -0.1825,  0.2089, -0.0469,  0.0945, -0.0116,\r\n         -0.1252,  0.2888]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 1.]])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 3582/10346 (34.62%) -> 4856/10346 (46.94%)\r\n\r\nEPOCH 2 ENDING\r\ninput_net.1.bias tensor([0.1114])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0274,  0.1090, -0.1602,  0.2130, -0.0471,  0.0802, -0.0114,\r\n         -0.1096,  0.2959]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 0., 0., 1., 1.]])\r\n\r\nEPOCH 3 STARTING\r\ninput_net.1.bias tensor([0.1114])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0274,  0.1090, -0.1602,  0.2130, -0.0471,  0.0802, -0.0114,\r\n         -0.1096,  0.2959]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 0., 0., 1., 1.]])\r\n```\r\n\r\nThe training snippet shows the original weight tensor changing at the start of each epoch, changing values to that of the end of the previous epoch. By right, LTH should revert weights start of every epoch to the original initialization (start of epoch 0). Thoughts or correction is much appreciated.  \r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13040",
    "createdAt": "2022-05-12T03:02:17Z",
    "updatedAt": "2022-06-13T19:47:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mune98"
    },
    "answer": {
      "body": "As I began implementing and comparing with my own custom pruning function, I came to the realization that weight_orig is the updated weight at the end of each epoch, not the original initialized weight. Printing the actual weight at the start of each epoch gives the original initialized weight. Had a different idea of what weight_orig meant, my bad! ",
      "author": {
        "login": "mune98"
      },
      "createdAt": "2022-05-12T09:15:01Z"
    }
  },
  {
    "title": "ddp help",
    "body": "Training a simple autoencoder as part of a larger project, and using it to get back up to speed on Lightning. Pytorch Lightning v 1.6.2, AMD Ryzen, 2 A6000s, Ubuntu 21.10, using DDP. I have 2 questions:\r\n\r\n\r\n1) During script startup, I warn the user if they are about to overwrite existing log files. When using DDP and both devices, pl asks this question twice, of course, once on each process. Where can I put this interaction to avoid doing it twice? I suppose I could put it in prepare_data() - but am hoping for something that feels more appropriate.\r\n\r\n2) Sometimes - but not always - calling trainer.test() immediately following training fails to find the checkpoint file. This happens with unchanged code and model - sometimes it works and sometimes it doesn't. Interestingly, when it fails, a message is displayed saying it is going to to use a file which **does** exists and is the best checkpoint, but it does not load it. Instead it fails twice, each time trying to load a different file name which **does not** exist. Guessing that each process is trying to run its own version of best checkpoint ... **Is there something I need to do/call before running test() to make sure this has all been resolved back to one process?** Or have I found a bug ...\r\n\r\nAny help appreciated.\r\n\r\nseth",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13032",
    "createdAt": "2022-05-10T22:34:55Z",
    "updatedAt": "2022-06-27T10:05:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sneiman"
    },
    "answer": {
      "body": "I resolved the first question. For those with a similar problem:\r\nThere is a set of rank_zero decorators which can be imported from utilities. The one I used is imported like so:\r\n```from   pytorch_lightning.utilities.rank_zero import rank_zero_only```\r\n\r\nPut user interaction into a function like so:\r\n```\r\n@rank_zero_only\r\ndef prelim(args):\r\n    # do as you please here - it will only run once from rank 0\r\n```\r\nCall it before the trainer.fit(), and it will only run once.\r\n\r\nSee [utilities docs](https://pytorch-lightning.readthedocs.io/en/stable/api_references.html#utilities-api) for details ...\r\n",
      "author": {
        "login": "sneiman"
      },
      "createdAt": "2022-05-12T14:28:10Z"
    }
  },
  {
    "title": "Is there a simple way to use old (PyTorch Lightning < 1.6) `global_step` update behavior?",
    "body": "I have 1 generator and 2 discriminators (`d_a` and `d_b`) in my `LightningModule` for *GAN* training. `d_a` will be updated every 2 steps and discriminator `d_b` will be updated every 3 steps. The generator will be pruned at step 10000.\r\n\r\nThe following code work in PyTorch Lightning < 1.6,\r\n\r\n```python\r\n\r\nclass PrunedModule(pl.LightningModule):\r\n    \"\"\"Apply pruning when self.global_step == 10000\"\"\"\r\n\r\n    def training_step_end(self, _):\r\n        if self.global_step == 10000:\r\n            # apply pruning\r\n\r\n\r\nclass PrunedGAN(PruningLightningModule):\r\n\r\n    def training_step(batch, batch_idx):\r\n        # ...\r\n        optimizer_g.step()\r\n\r\n        if self.global_step % 2 == 0:\r\n            # ...\r\n            # update d_a\r\n            optimizer_d_a.step()\r\n\r\n        if self.global_step % 3 == 0:\r\n            # ...\r\n            # update d_b\r\n            optimizer_d_b.step()\r\n```\r\n\r\nIn PyTorch Lightning 1.6, the `self.global_step` of `PrunedGAN` will be increased by 1, 2 or 3 in a single `training_step`.\r\n\r\nIf I use `self.register_buffer(\"my_global_step\", torch.tensor(0))` in `PrunedModule` and increase `self.my_global_step` by 1 in `training_step_end`, I still need to re-implement all classes inherited from `PrunedModule` and `ModelCheckpoint` to make sure that they use `self.my_global_step` instead of `self.global_step` or `trainer.global_step`. Besides, `my_global_step` will be moved to `cuda` in GPU training mode. It seems to be a bad idea to re-implement global step tracking by myself.\r\n\r\nIs there a simple way to use old (PyTorch Lightning < 1.6) `global_step` update behavior when using multiple optimizers?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13017",
    "createdAt": "2022-05-09T16:42:48Z",
    "updatedAt": "2022-08-03T13:08:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lecacosa"
    },
    "answer": {
      "body": "https://github.com/Lightning-AI/lightning/issues/13752#issuecomment-1190509385 might be relevant to your question.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-07-23T12:08:54Z"
    }
  },
  {
    "title": "Reload train dataloader when using limit_train_batches",
    "body": "Hi, I am using an IterableDataset that generates infinite samples and I want it to change its behaviour as the training progresses. To do so, I need to access information about the current training step from inside the Dataset.\r\nI tried passing the trainer to the dataset as an argument, however, when using multiple workers the trainer is not updated.\r\nAnother idea was to define virtual epochs of N steps. To do so, I passed limit_train_batches = N as an argument to the Trainer. Also, when an epoch finishs, I need to update the dataset variables. To do it I set reload_dataloader_every_n_epochs = 1 and I wrote a train_dataloader method in my Lightning Module that returns a new dataloader with the updated variables. However, it seems that the train_dataloader method isn't called if I set the limit_train_batches variable. Any idea on how to solve this problem?\r\n\r\nThanks in advance.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13001",
    "createdAt": "2022-05-06T18:12:26Z",
    "updatedAt": "2022-07-03T23:39:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mrpep"
    },
    "answer": {
      "body": "I finally could solve it. The problem was that I passed the train and validation dataloaders as arguments to ```trainer.fit()```, so ```train_dataloader()``` wasn't called. I ended up implementing ```__len__()``` in the IterableDataset instead of using ```limit_train_batches```\r\n",
      "author": {
        "login": "mrpep"
      },
      "createdAt": "2022-05-08T15:24:03Z"
    }
  },
  {
    "title": "Using trainer.fit(model, datamodule, ckpt_path=path) on compressed model",
    "body": "Task: Model Compression using [NNI](https://nni.readthedocs.io/). \r\n\r\nApproach: Loading a PyTorch Lightning trained model from a model checkpoint using `.load_checkpoint()`. \r\n\r\nProblem: After removing weights, my model class has reduced weights and has to be fine-tuned. As this is a compressed version of a trained model, I want to continue training with the optimizer state dict present in the checkpoint. If I try to fine-tune using \r\n```python\r\ntrainer = pl.Trainer(\r\n     gpus=self.gpus,\r\n     max_epochs=self.max_epochs,\r\n     callbacks=[checkpoint_callback],\r\n)\r\ntrainer.fit(self.model, datamodule, ckpt_path=ckpt_PATH)\r\n```\r\nThis gives an error as the model structure has changed (Conv2d filters have been reduced). Is there an approach that can be used that is easier?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12987",
    "createdAt": "2022-05-05T18:29:06Z",
    "updatedAt": "2022-05-31T01:10:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "shenoynikhil98"
    },
    "answer": {
      "body": "this is not possible with fit(..., ckpt_path=...) since this is partial resume and will load the model weights too. For you use-case you can do this maybe:\r\n\r\n```python\r\nclass OptimizerReload(Callback):\r\n    def __init__(self, ckpt_path):\r\n        self.ckpt_path = ckpt_path\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n        ckpt = torch.load(self.ckpt_path)\r\n        trainer.strategy.load_optimizer_state_dict(ckpt)\r\n```\r\n\r\nand, pass it to Trainer\r\n\r\n```python\r\ncb = OptimizerReload(ckpt_path)\r\ntrainer = Trainer(..., callbacks=[cb])\r\ntrainer.fit(model)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-05-08T09:54:28Z"
    }
  },
  {
    "title": "Specify Trainer strategy in CLI config?",
    "body": "Hi folks,\r\n\r\nI can't figure out how to give a Strategy class (rather than a string) as an argument to the trainer in a CLI config file. Is doing so supported?\r\n\r\nDoing this works fine:\r\n```\r\ntrainer:\r\n    ...\r\n    strategy: 'deepspeed_stage_2_offload'\r\n    ...\r\n```\r\n\r\nBut this gives a jsonargparse error:\r\n```\r\ntrainer:\r\n    ...\r\n    strategy:\r\n        class_path: pytorch_lightning.strategies.DeepSpeedStrategy\r\n        init_args:\r\n            stage: 2\r\n            offload_optimizer: True\r\n            logging_batch_size_per_gpu: 16\r\n    ...\r\n```\r\n\r\nThe error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/sharon/wbrannon/github/clip-graph/bin/trainer.py\", line 7, in <module>\r\n    cli = cl.LightningCLI(\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 552, in __init__\r\n    self.parse_arguments(self.parser)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 692, in parse_arguments\r\n    self.config = parser.parse_args()\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 268, in parse_args\r\n    return super().parse_args(*args, **kwargs)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/deprecated.py\", line 112, in patched_parse\r\n    cfg = parse_method(*args, _skip_check=_skip_check, **kwargs)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 366, in parse_args\r\n    self.error(str(ex), ex)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 983, in error\r\n    raise ParserError(message) from ex\r\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, s\r\ntage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000\r\n.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\" does not validate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\r\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\r\n  - Problem with given class_path \"pytorch_lightning.strategies.DeepSpeedStrategy\":\r\n    - Configuration check failed :: Parser key \"params_buffer_size\": Expected a <class 'int'> but got \"100000000.0\"\r\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12978",
    "createdAt": "2022-05-04T18:24:02Z",
    "updatedAt": "2022-06-10T19:00:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "wwbrannon"
    },
    "answer": {
      "body": "It's a bug! Will be fixed with https://github.com/PyTorchLightning/pytorch-lightning/pull/12989 which should be included in next week's bugfix release\r\n\r\nThanks for the report!",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-05-05T20:28:43Z"
    }
  },
  {
    "title": "How to refer to dataset attributes in CLI config file?",
    "body": "I'm writing a yaml config file for a pytorch-lightning CLI script. The file specifies model, data, trainer arguments and misc parameters. The model arguments depend on the dataset -- specifically, the dimension of the model's first layer needs to match what's in the data.\r\n\r\nDo I need to hardcode this value or is there a way to tell PL to discover it dynamically? Here's an example:\r\n```\r\nseed_everything: 42\r\nckpt_path: /path/to/ckpt\r\n\r\nmodel:\r\n    class_path: src.mymodel.litclass\r\n    init_args:\r\n        n_classes: 1024\r\n        key: 'target'\r\n        lr: 0.0005\r\n        weight_decay: 0\r\n        model:\r\n            class_path: src.mymodel.baseclass\r\n            init_args:\r\n                n_layers: 3\r\n                n_heads: 4\r\n                d_input: ??????\r\n                d_hidden: 64\r\n                d_feedforward: 128\r\n                p_dropout: 0.5\r\n\r\ndata:\r\n    class_path: src.mydataset.mydatamodule\r\n    init_args:\r\n        data_dir: ../data\r\n```\r\n\r\nHow should I fill in d_input?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12953",
    "createdAt": "2022-05-02T17:58:22Z",
    "updatedAt": "2022-06-10T19:00:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "wwbrannon"
    },
    "answer": {
      "body": "You can use argument linking: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking\r\nAnd have any initial default value\r\n\r\nOr variable interpolation with omegaconf: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#variable-interpolation",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-05-03T12:45:40Z"
    }
  },
  {
    "title": "Separate Trainer for `train()` and `test()`?",
    "body": "Python: 3.9.12\r\nPytorch: '1.11.0+cu102'\r\nPytorch-lightning: 1.6.1\r\n\r\nThe [docs for evaluation](https://pytorch-lightning.readthedocs.io/en/latest/common/evaluation_intermediate.html) say:\r\n> It is recommended to test with Trainer(devices=1) since distributed strategies such as DDP use [DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) internally, which replicates some samples to make sure all devices have same batch size in case of uneven inputs. This is helpful to make sure benchmarking for research papers is done the right way.\r\n\r\nSimilarly I get a warning during runtime:\r\n```\r\nDuring `trainer.test()`, it is recommended to use `Trainer(devices=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\r\n```\r\n\r\nI would like to use DDP for training on 4 GPUs, does this mean I create an entirely new Trainer for just val/test?\r\n\r\nMy train Trainer:\r\n```python\r\ntrainer = pl.Trainer(\r\n            max_epochs=self.max_epochs,\r\n            logger=logger,\r\n            num_nodes=self.num_nodes,\r\n            # use 1 processes if on cpu\r\n            devices=self.num_gpus if self.num_gpus else 1,\r\n            accelerator=\"gpu\" if self.num_gpus else \"cpu\",\r\n            strategy=DDPPlugin(find_unused_parameters=False)\r\n            if self.num_gpus > 1\r\n            else None,\r\n            enable_checkpointing=False,\r\n            callbacks=callbacks,\r\n            profiler=\"simple\",  # or \"advanced\" which is more granular\r\n            fast_dev_run=self.fast_dev_run,  # For debugging\r\n        )\r\ntrainer.fit(model, datamodule)\r\n```\r\nIf I want to then run testing would I then do something like:\r\n```python\r\ntest_trainer= pl.Trainer(\r\n            max_epochs=self.max_epochs,\r\n            logger=logger,\r\n            num_nodes=self.num_nodes,\r\n            # use 1 device for test\r\n            devices=1,\r\n            accelerator=\"gpu\" if self.num_gpus else \"cpu\",\r\n            strategy=DDPPlugin(find_unused_parameters=False)\r\n            if self.num_gpus > 1\r\n            else None,\r\n            enable_checkpointing=False,\r\n            callbacks=callbacks,\r\n            profiler=\"simple\",  # or \"advanced\" which is more granular\r\n            fast_dev_run=self.fast_dev_run,  # For debugging\r\n        )\r\ntest_trainer.test(model, test_dataloader)\r\n```\r\n\r\nAm I understanding correctly? I don't think it's possible to set the number of devices dynamically since there's no setter defined for the `num_devices` property in the Trainer class. I'm not sure if it makes sense to make a separate trainer, so I feel a bit confused.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12906",
    "createdAt": "2022-04-27T19:35:45Z",
    "updatedAt": "2022-06-13T22:08:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "davzaman"
    },
    "answer": {
      "body": "yes, if you care about the accuracy of the test metrics and don't want to include extra samples, this is the only way. Not just a separate trainer, if using DDP, you should create a separate script all together else the whole script, along with the `trainer.test` call will be launched on each device during `trainer.fit` call.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-29T18:53:34Z"
    }
  },
  {
    "title": "Skip validation for the first few epochs",
    "body": "Hi, PL.\r\n\r\nIs there any way to skip validation for the first few epochs (ex. 10)?\r\nI searched for an hour, but the only thing I found is check_val_every_n_epoch.\r\n\r\nBut the thing I want is different from this.\r\nI want to check every epoch after a certain epoch.\r\n\r\nSince I'm a beginner, any comment would be greatly appreciated.\r\n   ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12873",
    "createdAt": "2022-04-25T03:27:19Z",
    "updatedAt": "2022-06-19T11:17:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sangrockEG"
    },
    "answer": {
      "body": "Hi @sangrockEG! You can utilise `self.current_epoch` in your LightningModule:\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    if self.current_epoch <= 9:\r\n        return\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#current-epoch",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-04-25T18:52:40Z"
    }
  },
  {
    "title": "How to get different random minibatch orders?",
    "body": "Hi,\r\n\r\nI'm training DDP with 4 GPUs but noticed that if I rerun the experiment the first epoch has the **exact same** but random order of the minibatches as the previous experiment.\r\n\r\nHow can I make it so that each time I run the experiment I get a different random order?\r\n\r\nI'm using PL version is 1.4.5 and pytorch 1.10.0.\r\n\r\nThank you",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12867",
    "createdAt": "2022-04-23T15:11:29Z",
    "updatedAt": "2025-03-06T18:03:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jaak-s"
    },
    "answer": {
      "body": "Solved. Use `seed_everything(random_seed)`.",
      "author": {
        "login": "jaak-s"
      },
      "createdAt": "2022-04-23T16:26:56Z"
    }
  },
  {
    "title": "TypeError: 'function' object is not iterable",
    "body": "```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-303af62577c3> in <module>\r\n     65           n=30, d_model=50,\r\n     66           stage1_med_dim=50, stage1_out_dim=100, stage2_out_dim=200,\r\n---> 67           epoch=10\r\n     68           )\r\n\r\n<ipython-input-9-303af62577c3> in train_mct(config, n, d_model, stage1_med_dim, stage1_out_dim, stage2_out_dim, epoch)\r\n     49     trainer = Trainer(**kwargs)\r\n     50     trainer.fit(model, train_dataloaders=train_dataloader,\r\n---> 51                 val_dataloaders=val_dataloader)\r\n     52 \r\n     53 \r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\r\n    739             train_dataloaders = train_dataloader\r\n    740         self._call_and_handle_interrupt(\r\n--> 741             self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n    742         )\r\n    743 \r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    683         \"\"\"\r\n    684         try:\r\n--> 685             return trainer_fn(*args, **kwargs)\r\n    686         # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    687         except KeyboardInterrupt as exception:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    775         # TODO: ckpt_path only in v1.7\r\n    776         ckpt_path = ckpt_path or self.resume_from_checkpoint\r\n--> 777         self._run(model, ckpt_path=ckpt_path)\r\n    778 \r\n    779         assert self.state.stopped\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run(self, model, ckpt_path)\r\n   1197 \r\n   1198         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n-> 1199         self._dispatch()\r\n   1200 \r\n   1201         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _dispatch(self)\r\n   1277             self.training_type_plugin.start_predicting(self)\r\n   1278         else:\r\n-> 1279             self.training_type_plugin.start_training(self)\r\n   1280 \r\n   1281     def run_stage(self):\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py in start_training(self, trainer)\r\n    200     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n    201         # double dispatch to initiate the training loop\r\n--> 202         self._results = trainer.run_stage()\r\n    203 \r\n    204     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_stage(self)\r\n   1287         if self.predicting:\r\n   1288             return self._run_predict()\r\n-> 1289         return self._run_train()\r\n   1290 \r\n   1291     def _pre_training_routine(self):\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run_train(self)\r\n   1309             self.progress_bar_callback.disable()\r\n   1310 \r\n-> 1311         self._run_sanity_check(self.lightning_module)\r\n   1312 \r\n   1313         # enable train mode\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run_sanity_check(self, ref_model)\r\n   1373             # run eval step\r\n   1374             with torch.no_grad():\r\n-> 1375                 self._evaluation_loop.run()\r\n   1376 \r\n   1377             self.call_hook(\"on_sanity_check_end\")\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\base.py in run(self, *args, **kwargs)\r\n    143             try:\r\n    144                 self.on_advance_start(*args, **kwargs)\r\n--> 145                 self.advance(*args, **kwargs)\r\n    146                 self.on_advance_end()\r\n    147                 self.restarting = False\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py in advance(self, *args, **kwargs)\r\n    108         dl_max_batches = self._max_batches[dataloader_idx]\r\n    109 \r\n--> 110         dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n    111 \r\n    112         # store batch level output per dataloader\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\base.py in run(self, *args, **kwargs)\r\n    138         self.reset()\r\n    139 \r\n--> 140         self.on_run_start(*args, **kwargs)\r\n    141 \r\n    142         while not self.done:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py in on_run_start(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\r\n     84 \r\n     85         self._reload_dataloader_state_dict(data_fetcher)\r\n---> 86         self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n     87 \r\n     88     def advance(\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py in _update_dataloader_iter(data_fetcher, batch_idx)\r\n    119     if not isinstance(data_fetcher, DataLoaderIterDataFetcher):\r\n    120         # restore iteration\r\n--> 121         dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n    122     else:\r\n    123         dataloader_iter = iter(data_fetcher)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py in __iter__(self)\r\n    195             raise MisconfigurationException(\"The iterate hasn't been provided. HINT: Did you call setup function ?.\")\r\n    196         self.reset()\r\n--> 197         self.dataloader_iter = iter(self.dataloader)\r\n    198         self._apply_patch()\r\n    199         self.prefetching(self.prefetch_batches)\r\n```\r\nAnd here is my code.\r\n```\r\ndef train_mct(config, epoch):\r\n    # prepare dir for model path\r\n\r\n    # seed everything\r\n    seed_everything(seed=9876, workers=True)\r\n\r\n    def train_dataloader(self):\r\n        # expect to get train folder\r\n        dataset = load_train_data(df)\r\n        dataloader = DataLoader(dataset, batch_size=32, num_workers=0, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=True)\r\n\r\n        return dataloader\r\n\r\n    def val_dataloader(self):\r\n        # expect to get validation folder\r\n        dataset = load_train_data(df)\r\n        dataloader = DataLoader(dataset, num_workers=0, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=False)\r\n\r\n        return dataloader\r\n\r\n    model = MCT(config=config).float()\r\n\r\n    kwargs = {\r\n        \"max_epochs\": epoch,\r\n        \"enable_progress_bar\": False,\r\n        \"val_check_interval\": 1.0,\r\n        \"callbacks\": [\r\n            EarlyStopping(\r\n                monitor='training_loss',\r\n                patience=20, mode='min',\r\n                check_on_train_epoch_end=True)\r\n        ]\r\n    }\r\n\r\n    trainer = Trainer(**kwargs)\r\n    trainer.fit(model, train_dataloaders=train_dataloader,\r\n                val_dataloaders=val_dataloader)\r\n\r\nconfig = {\r\n    \"n_layers_s1\": 3,\r\n    \"n_layers_s2\": 3,\r\n    \"n_heads\": 5,\r\n    \"d_ff\": 1024,\r\n    \"lr\": 0.01,\r\n    \"app_weight\": 4,\r\n    \"signal_length\": 1500,\r\n    \"res_s0\": True\r\n}\r\ntrain_mct(config, epoch=10)\r\n```\r\nAnd advice on what happpend.\r\nThank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12844",
    "createdAt": "2022-04-22T02:10:33Z",
    "updatedAt": "2022-07-03T08:07:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "chriswangky"
    },
    "answer": {
      "body": "update\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader,\r\n                val_dataloaders=val_dataloader)\r\n```\r\nto\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader(),\r\n                val_dataloaders=val_dataloader())\r\n```\r\nsince `train_dataloader/val_dataloader` is a function in your case.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-22T08:49:13Z"
    }
  },
  {
    "title": "TypeError: validation_step() takes 2 positional arguments but 3 were given",
    "body": "When running my model I get the error message:`TypeError: validation_step() takes 2 positional arguments but 3 were given`\r\nHere is my code.\r\n```\r\ndef val_dataloader(self):\r\n        # expect to get train folder\r\n        dataset = load_train_data(self.val_data_path)\r\n        try:\r\n            num_workers = multiprocessing.cpu_count()\r\n        except:\r\n            num_workers = 1\r\n        dataloader = DataLoader(dataset, batch_size=1024, num_workers=num_workers, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=True)\r\n\r\n        return dataloader\r\n```\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        x_app = batch['feature'].float()\r\n        y_app = batch['app_label'].long()\r\n        y_tra, y_all, index = drop_na(batch)\r\n        y_hat_app, y_hat_tra, y_hat_all = self(x_app, index)\r\n\r\n        app_f1 = F1Score(num_classes=17)\r\n        tra_f1 = F1Score(num_classes=12)\r\n        all_f1 = F1Score(num_classes=6)\r\n        F1_app = app_f1(y_hat_app, y_app)\r\n        F1_tra = tra_f1(y_hat_tra, y_tra)\r\n        F1_all = all_f1(y_hat_all, y_all)\r\n        F1_score = (F1_app + F1_tra + F1_all) / 3.0\r\n        self.log('val_F1_app', F1_app, on_step=True, on_epoch=True)\r\n        self.log('val_F1_tra', F1_tra, on_step=True, on_epoch=True)\r\n        self.log('val_F1_all', F1_all, on_step=True, on_epoch=True)\r\n        self.log('val_F1_score', F1_score, on_step=True, on_epoch=True)\r\n\r\n        entropy_app = F.cross_entropy(y_hat_app, y_app)\r\n        entropy_tra = F.cross_entropy(y_hat_tra, y_tra)\r\n        entropy_all = F.cross_entropy(y_hat_all, y_all)\r\n        loss_weight = self.app_weight + 2 + 1\r\n        entropy = (self.app_weight * entropy_app + 2 *\r\n                   entropy_tra + 1*entropy_all) / loss_weight\r\n        self.log('val_app_loss', entropy_app, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('val_tra_loss', entropy_tra, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('val_all_loss', entropy_all, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('validation_loss', entropy, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n\r\n        return {\"val_loss\": entropy, \"val_F1score\": F1_score}\r\n```\r\n\r\nAny advice on what happened?\r\nThank you very much!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12838",
    "createdAt": "2022-04-21T16:10:06Z",
    "updatedAt": "2022-06-15T23:37:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "chriswangky"
    },
    "answer": {
      "body": "your code looks correct. Are you sure you are using a single validation dataloader?\r\nif yes, can you reproduce it using [BoringModel](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report/bug_report_model.ipynb)??",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-21T16:22:39Z"
    }
  },
  {
    "title": "TypeError: __init__() takes 1 positional argument but 3 were given",
    "body": "When I was training my model,  I got an error .\r\nTraceback is as follows:\r\n** \r\nFile \"/home/datasets/traffic_classification/ml/MCT.py\", line 355, in __init__\r\n self.fusion = Fusion_Block(n, d_model, self.n_layers_s1, self.n_layers_s2, self.n_heads, self.d_ff,\r\nFile \"/home/datasets/traffic_classification/ml/MCT.py\", line 313, in __init__\r\nself.c2trans = CNN_to_Trans(n, d_model)\r\nFile \"/usr/local/miniconda3/envs/deep_packet/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 92, in __init__\r\nsuper().__init__(*args, **kwargs)\r\nTypeError: __init__() takes 1 positional argument but 3 were given\r\n**\r\nHere is my code.\r\n```\r\nclass CNN_to_Trans(LightningModule):\r\n    def __int__(self, n, d_model):\r\n        super().__init__() \r\n        self.pooling = nn.AdaptiveAvgPool1d(n)\r\n        self.conv = nn.Conv1d(in_channels=n*d_model, out_channels=d_model, kernel_size=1,stride=1)\r\n        self.norm = nn.LayerNorm(d_model)\r\n        self.act_f = nn.ReLU()\r\n    def forward(self, x):\r\n        x = self.conv(x.transpose(1,2))\r\n        x = self.pooling(x).transpose(1,2)\r\n        x = self.norm(x)\r\n        x = self.act_f(x)\r\n        return x\r\n```\r\n\r\n```\r\nclass Fusion_Block(LightningModule):\r\n    def __init__(self, n, d_model, n_layers_s1, n_layers_s2, n_heads, d_ff, kernel_size, stage1_med_dim, stage1_out_dim, stage2_out_dim):\r\n        super(Fusion_Block, self).__init__()\r\n        ... ...\r\n        self.c2trans = CNN_to_Trans(n, d_model)\r\n        ... ...\r\n\r\n    def forward(...):\r\n        ... ...\r\n```\r\n\r\nThank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12831",
    "createdAt": "2022-04-21T06:02:17Z",
    "updatedAt": "2022-06-27T03:27:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "chriswangky"
    },
    "answer": {
      "body": "@chriswangky  Hi, is it a typo? `__int__` --> `__init__`",
      "author": {
        "login": "i-aki-y"
      },
      "createdAt": "2022-04-21T06:51:26Z"
    }
  },
  {
    "title": "LayerSummary.input_size returns '?'",
    "body": "Hi guys,\r\n\r\nI'm attempting to get to input and output shapes of each of my layers using the `input_size` and `output_size` properties. However, attempting to access either of these returns the value `?`, rather than the size array. Could anyone shine some light on why this might be happening? Thanks!\r\n\r\n<details><summary>Code Snippet</summary>\r\n<p>\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom scipy.io import wavfile\r\n\r\nclass CausalConv1d(torch.nn.Conv1d):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\r\n        self.__padding = (kernel_size - 1) * dilation\r\n\r\n        super(CausalConv1d, self).__init__(\r\n            in_channels,\r\n            out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=self.__padding,\r\n            dilation=dilation,\r\n            groups=groups,\r\n            bias=bias,\r\n        )\r\n\r\n    def forward(self, input):\r\n        result = super(CausalConv1d, self).forward(input)\r\n        if self.__padding != 0:\r\n            return result[:, :, : -self.__padding]\r\n        return result\r\n\r\n\r\ndef _conv_stack(dilations, in_channels, out_channels, kernel_size):\r\n    \"\"\"\r\n    Create stack of dilated convolutional layers, outlined in WaveNet paper:\r\n    https://arxiv.org/pdf/1609.03499.pdf\r\n    \"\"\"\r\n    return nn.ModuleList(\r\n        [\r\n            CausalConv1d(\r\n                in_channels=in_channels,\r\n                out_channels=out_channels,\r\n                dilation=d,\r\n                kernel_size=kernel_size,\r\n            )\r\n            for i, d in enumerate(dilations)\r\n        ]\r\n    )\r\n\r\n\r\nclass WaveNet(nn.Module):\r\n    def __init__(self, num_channels, dilation_depth, num_repeat, kernel_size=2):\r\n        super(WaveNet, self).__init__()\r\n        dilations = [2 ** d for d in range(dilation_depth)] * num_repeat\r\n        internal_channels = int(num_channels * 2)\r\n        self.hidden = _conv_stack(dilations, num_channels, internal_channels, kernel_size)\r\n        self.residuals = _conv_stack(dilations, num_channels, num_channels, 1)\r\n        self.input_layer = CausalConv1d(\r\n            in_channels=1,\r\n            out_channels=num_channels,\r\n            kernel_size=1,\r\n        )\r\n\r\n        self.linear_mix = nn.Conv1d(\r\n            in_channels=num_channels * dilation_depth * num_repeat,\r\n            out_channels=1,\r\n            kernel_size=1,\r\n        )\r\n        self.num_channels = num_channels\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        skips = []\r\n        out = self.input_layer(out)\r\n\r\n        for hidden, residual in zip(self.hidden, self.residuals):\r\n            x = out\r\n            out_hidden = hidden(x)\r\n\r\n            # gated activation\r\n            #   split (32,16,3) into two (16,16,3) for tanh and sigm calculations\r\n            out_hidden_split = torch.split(out_hidden, self.num_channels, dim=1)\r\n            out = torch.tanh(out_hidden_split[0]) * torch.sigmoid(out_hidden_split[1])\r\n\r\n            skips.append(out)\r\n\r\n            out = residual(out)\r\n            out = out + x[:, :, -out.size(2) :]\r\n\r\n        # modified \"postprocess\" step:\r\n        out = torch.cat([s[:, :, -out.size(2) :] for s in skips], dim=1)\r\n        out = self.linear_mix(out)\r\n        return out\r\n\r\n\r\ndef error_to_signal(y, y_pred):\r\n    \"\"\"\r\n    Error to signal ratio with pre-emphasis filter:\r\n    https://www.mdpi.com/2076-3417/10/3/766/htm\r\n    \"\"\"\r\n    y, y_pred = pre_emphasis_filter(y), pre_emphasis_filter(y_pred)\r\n    return (y - y_pred).pow(2).sum(dim=2) / (y.pow(2).sum(dim=2) + 1e-10)\r\n\r\n\r\ndef pre_emphasis_filter(x, coeff=0.95):\r\n    return torch.cat((x[:, :, 0:1], x[:, :, 1:] - coeff * x[:, :, :-1]), dim=2)\r\n\r\n\r\nclass SatNet(pl.LightningModule):\r\n    def __init__(self, hparams):\r\n        super(SatNet, self).__init__()\r\n        self.wavenet = WaveNet(\r\n            num_channels=hparams[\"num_channels\"],\r\n            dilation_depth=hparams[\"dilation_depth\"],\r\n            num_repeat=hparams[\"num_repeat\"],\r\n            kernel_size=hparams[\"kernel_size\"],\r\n        )\r\n        self.hparams.update(hparams)\r\n        self.save_hyperparameters()\r\n        self.test_ds = TensorDataset()\r\n\r\n    def prepare_data(self):\r\n\r\n        createTensorDataset = lambda x, y: TensorDataset(torch.from_numpy(x).unsqueeze(1), torch.from_numpy(y).unsqueeze(1))\r\n        \r\n        inRate, inData = wavfile.read(hparams[\"in_file\"])\r\n        outRate, outData = wavfile.read(hparams[\"out_file\"])\r\n        sampleTime = 0.1\r\n        sampleSize = int(inRate * sampleTime)\r\n        length = len(inData) - len(inData) % sampleSize\r\n\r\n        #Each row in this table represents the waveform samples seen in 0.1 seconds (4410 samples)\r\n        x = inData[:length].reshape((-1, sampleSize)).astype(np.float32)\r\n        y = outData[:length].reshape((-1, sampleSize)).astype(np.float32)\r\n\r\n        splitLocA = int(len(x) * 0.6)\r\n        splitLocB = int(len(x) * 0.8)\r\n\r\n        X_train, X_valid, X_test = np.split(x, [splitLocA, splitLocB])\r\n        y_train, y_valid, y_test = np.split(y, [splitLocA, splitLocB])\r\n\r\n        self.train_ds = createTensorDataset(X_train, y_train)\r\n        self.valid_ds = createTensorDataset(X_valid, y_valid)\r\n        self.test_ds = createTensorDataset(X_test, y_test)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.wavenet.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.train_ds,\r\n            shuffle=True,\r\n            batch_size=self.hparams.batch_size,\r\n            num_workers=4,\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.valid_ds, batch_size=self.hparams.batch_size, num_workers=4)\r\n\r\n    def forward(self, x):\r\n        return self.wavenet(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_pred = self.forward(x)\r\n        loss = error_to_signal(y[:, :, -y_pred.size(2) :], y_pred).mean()\r\n        self.log(\"loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_pred = self.forward(x)\r\n        loss = error_to_signal(y[:, :, -y_pred.size(2) :], y_pred).mean()\r\n        return loss\r\n\r\n    def validation_epoch_end(self, outs):\r\n        lossArray = []\r\n        for tensor in outs:\r\n            lossArray.append(tensor.item())\r\n        avg_loss = (np.asarray(lossArray)).mean()\r\n        self.log(\"avg_val_loss\", avg_loss)\r\n\r\n# Training\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor = \"avg_val_loss\",\r\n    dirpath = \"models\\\\newPLModelTest\\\\\",\r\n    filename = \"sample-mnist-{epoch:02d}-{avg_val_loss:.4f}\",\r\n    save_top_k = 3,\r\n    mode = \"min\"\r\n)\r\n\r\nhparams = {\r\n    \"in_file\": \"data\\\\y_input_data.wav\",\r\n    \"out_file\": \"data\\\\x_input_data.wav\",\r\n    \"num_channels\": 12,\r\n    \"dilation_depth\": 10,\r\n    \"num_repeat\": 1,\r\n    \"kernel_size\": 3,\r\n    \"learning_rate\": 3e-3,\r\n    \"batch_size\": 64\r\n}\r\n\r\nsatnet = SatNet(hparams)\r\n \r\ntrainer = pl.Trainer(gpus = -1, max_epochs = 1, callbacks = [checkpoint_callback])\r\ntrainer.fit(satnet)\r\ntrainer.save_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\n\r\n#Find layer in/out shapes\r\nfrom pytorch_lightning.utilities.model_summary import ModelSummary, LayerSummary\r\n\r\nmodel = SatNet.load_from_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\nlayers = list(model.named_modules())\r\n\r\nlayer = layers[4][1]\r\nsummary = LayerSummary(layer)\r\nprint (summary.in_size) #RETURNS '?'\r\n\r\n```\r\n\r\n</p>\r\n</details>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12827",
    "createdAt": "2022-04-20T19:50:06Z",
    "updatedAt": "2022-06-27T07:07:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ross-ca"
    },
    "answer": {
      "body": "when you do this:\r\n```py\r\nmodel = SatNet.load_from_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\n```\r\na new model is instantiated from scratch. Now since PyTorch modules are dynamic, they don't have input shape/output shape configured by default until a computation/forward pass is done through that layer when module is registered with `LayerSummary`. So you need to do a forward pass using a sample input to compute the required sizes. Check out the example linked in the docs.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.model_summary.html#pytorch_lightning.utilities.model_summary.LayerSummary",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-21T08:44:21Z"
    }
  },
  {
    "title": "How to loop over the entire dataset before training start",
    "body": "Hello,\r\nmy LightiningModule has an imputer which needs to be initialized from data (think about mean-imputation) before the training start. \r\n\r\n**Is there any ModelHook which allows me to perform a single pass on the entire dataset before the training start ?**   \r\n\r\nThanks in advance \r\n\r\n```\r\nclass Net(pl.LightningModule):\r\n\r\ndef __init__(self):\r\n      self.imputer = MyImputer()   # the imputer need to be initialized from the data\r\n      self.net = MyNet()\r\n\r\ndef training_step(self, bath, batch_idx):\r\n      x, y_hat = batch\r\n      x_imputed = self.imputer(x)\r\n      y = self.net(x_imputed)\r\n      loss = (y-y_hat).pow(2).sum()\r\n      return loss\r\n\r\n\r\ndef before_training(self, batch, batch_idx):\r\n     # How can I do this? Is there any ModelHook I can use/modify?\r\n     # I need to perform a full pass of the data to initialize the imputer before the training begins\r\n     self.imputer.initialize_from_data(batch)\r\n```\r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12809",
    "createdAt": "2022-04-19T21:19:33Z",
    "updatedAt": "2022-09-04T14:11:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dalessioluca"
    },
    "answer": {
      "body": "you can do\r\n```py\r\ndef on_train_start(self):\r\n    for batch in self.trainer.train_dataloader.loaders:\r\n        ...\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-20T08:55:42Z"
    }
  },
  {
    "title": "PyTorch Lightning (Trainable Params - Wrong)",
    "body": "I am employing MULTI-GPU training using pytorch lightning. The below output displays the model:\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\u250f\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503    \u2503 Name       \u2503 Type              \u2503 Params \u2503\r\n\u2521\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\r\n\u2502 0  \u2502 encoder    \u2502 Encoder           \u2502  2.0 M \u2502\r\n\u2502 1  \u2502 classifier \u2502 Sequential        \u2502  8.8 K \u2502\r\n\u2502 2  \u2502 criterion  \u2502 BCEWithLogitsLoss \u2502      0 \u2502\r\n\u2502 3  \u2502 train_acc  \u2502 Accuracy          \u2502      0 \u2502\r\n\u2502 4  \u2502 val_acc    \u2502 Accuracy          \u2502      0 \u2502\r\n\u2502 5  \u2502 train_auc  \u2502 AUROC             \u2502      0 \u2502\r\n\u2502 6  \u2502 val_auc    \u2502 AUROC             \u2502      0 \u2502\r\n\u2502 7  \u2502 train_f1   \u2502 F1Score           \u2502      0 \u2502\r\n\u2502 8  \u2502 val_f1     \u2502 F1Score           \u2502      0 \u2502\r\n\u2502 9  \u2502 train_mcc  \u2502 MatthewsCorrCoef  \u2502      0 \u2502\r\n\u2502 10 \u2502 val_mcc    \u2502 MatthewsCorrCoef  \u2502      0 \u2502\r\n\u2502 11 \u2502 train_sens \u2502 Recall            \u2502      0 \u2502\r\n\u2502 12 \u2502 val_sens   \u2502 Recall            \u2502      0 \u2502\r\n\u2502 13 \u2502 train_spec \u2502 Specificity       \u2502      0 \u2502\r\n\u2502 14 \u2502 val_spec   \u2502 Specificity       \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 2.0 M\r\nNon-trainable params: 0\r\n```\r\n\r\nI have set Encoder to be untrainable using the below code:\r\n\r\n```\r\nckpt = torch.load(chk_path)\r\nself.encoder.load_state_dict(ckpt['state_dict'])\r\nself.encoder.requires_grad = False\r\n```\r\n\r\nShouldn't ```trainable params``` be ```8.8 K``` rather than ```2.0 M``` ? \r\n\r\nMy optimizer is the following:\r\n```\r\noptimizer =  torch.optim.RMSprop(filter(lambda p: p.requires_grad, self.parameters()), lr =self.lr, weight_decay = self.weight_decay)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12808",
    "createdAt": "2022-04-19T16:48:19Z",
    "updatedAt": "2022-07-21T07:17:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "iamharsha1999"
    },
    "answer": {
      "body": "I don't think that is the right way to turn off the gradients:\r\nhttps://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-20T09:03:40Z"
    }
  },
  {
    "title": "DDP is not accelerating my training",
    "body": "Hi all,\r\n\r\nToday I tried to use DDP to accelerate my training. When not using it, I trained my model with batchsize=6 on 1 GPU. The iteration step per epoch is 6073 and the training time for the first epoch is 1h 29m. When using DDP with 1 node and 4 GPUs, the iteration step per epoch is still 6073 and the training time for the first epoch is 1h 9m. The batchsize was not changed. It seems that DDP is not working. I am using DDP as follows:\r\n`trainer = pl.Trainer(\r\n        accelerator=\"gpu\", devices=4, num_nodes=1, strategy=\"ddp\",\r\n        default_root_dir=models_save_path,`\r\nThe training log without DDP is:\r\n![image](https://user-images.githubusercontent.com/5463229/163566564-5af416b6-f9c5-43a3-8c83-377586f469c2.png)\r\nThe log with DDP is:\r\n![image](https://user-images.githubusercontent.com/5463229/163566586-79e26f6e-56cf-4321-b73b-aa7f8c0e32fa.png)\r\n\r\nAny suggestion?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12771",
    "createdAt": "2022-04-15T11:42:00Z",
    "updatedAt": "2022-06-07T14:22:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nian-liu"
    },
    "answer": {
      "body": "looks like in your case, DDP is not triggered for some reason since if you are not changing the batch_size and total batches in the progress bar should be reduced with DDP on 4 GPUs.\r\n\r\ndid you see any logs like this when you call `trainer.fit` ??\r\n```console\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\nInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\nInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-20T08:47:18Z"
    }
  },
  {
    "title": "TensorboardLogger not displaying metrics for LogisticRegression",
    "body": "I am currently building a big dataset and infrastructure using PyTorch Lightning. Unfortunately there is no precise information in the documentation.\r\n\r\nMy code can be seen as:\r\n```python\r\ndatamodule = DataModule(\r\n        transform=economy_average_vs_outcome,\r\n        download=False,\r\n        batch_size=2,\r\n        num_workers=4,\r\n    )\r\n# Preparing the data:\r\ndatamodule.prepare_data()\r\ndatamodule.setup()\r\nlogistic_regression = LogisticRegression(input_dim=2 * 39, num_classes=2)\r\nlogger = TensorBoardLogger(\"tb_logs\", name=\"Logistic Regression\")\r\ntrainer = pl.Trainer(\r\n        logger=logger,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        auto_select_gpus=True,\r\n        max_epochs=50,\r\n        log_every_n_steps=2,\r\n    )\r\n\r\n\r\n# Training the model:\r\ntrainer.fit(model=logistic_regression, datamodule=datamodule)\r\n```\r\n\r\nKeep in mind that this is sample code that is not final.\r\nIndependent of that there is no Tensorboard visualizations available.\r\n\r\n![image](https://user-images.githubusercontent.com/34846245/163412636-00dc7934-8cd6-49e6-8dc7-8a4dbca2257e.png)\r\n\r\n![image](https://user-images.githubusercontent.com/34846245/163412749-c3f96aa6-d2ed-4b6e-b2d0-1fe41eaefad4.png)\r\n\r\nThe code for ```LogisticRegression``` is imported from ```pl_bolts``` as follows:\r\n\r\n```\r\nfrom pl_bolts.models.regression import LogisticRegression\r\n```\r\n\r\nI should not be required to read the source code for the ```LogisticRegression``` to figure out what is the internal logging implementation.\r\n\r\nThe ```training_step``` is as follows coming from ```pl_bolts```:\r\n```python\r\n    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\r\n        x, y = batch\r\n\r\n        # flatten any input\r\n        x = x.view(x.size(0), -1)\r\n\r\n        y_hat = self.linear(x)\r\n\r\n        # PyTorch cross_entropy function combines log_softmax and nll_loss in single function\r\n        loss = F.cross_entropy(y_hat, y, reduction=\"sum\")\r\n\r\n        # L1 regularizer\r\n        if self.hparams.l1_strength > 0:\r\n            l1_reg = self.linear.weight.abs().sum()\r\n            loss += self.hparams.l1_strength * l1_reg\r\n\r\n        # L2 regularizer\r\n        if self.hparams.l2_strength > 0:\r\n            l2_reg = self.linear.weight.pow(2).sum()\r\n            loss += self.hparams.l2_strength * l2_reg\r\n\r\n        loss /= x.size(0)\r\n\r\n        tensorboard_logs = {\"train_ce_loss\": loss}\r\n        progress_bar_metrics = tensorboard_logs\r\n        return {\"loss\": loss, \"log\": tensorboard_logs, \"progress_bar\": progress_bar_metrics}\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12762",
    "createdAt": "2022-04-14T14:37:33Z",
    "updatedAt": "2022-04-21T20:11:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Kaszanas"
    },
    "answer": {
      "body": "the code in bolts is outdated and logging doesn't work like that anymore.\r\nyou need to do:\r\n```py\r\ndef training_step(self, batch, batch_idx):\r\n    ...\r\n    self.log(\"train_ce_loss\", loss, prog_bar=True)\r\n```\r\n\r\nmore info herE: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html#logging-from-a-lightningmodule",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-20T08:50:47Z"
    }
  },
  {
    "title": "How to use all the available GPUs",
    "body": "Hi all, I migrated my code to the latest version of lightnining and tried using `accelerator=auto` and `devices=-1` to use all available GPUs but I get:\r\n\r\nUserWarning: The flag `devices=-1` will be ignored, instead the device specific number 1\r\n\r\nWhat is the recommended way to use, by default, all GPUs (when present)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12755",
    "createdAt": "2022-04-14T08:14:14Z",
    "updatedAt": "2022-08-03T04:10:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mnslarcher"
    },
    "answer": {
      "body": "For anyone seeing this discussion, have a look at https://github.com/Lightning-AI/lightning/issues/12756#issuecomment-1106629824.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-08-03T04:10:30Z"
    }
  },
  {
    "title": "How can I get the final value of all the metrics that have been logged",
    "body": "At the moment, I am calling `trainer.fit(model=model, datamodule=datamodule)` and the training works fine.\r\nHowever, what I want to do is return the final value of all the metrics that have been logged during the training.\r\n\r\nIn my case, the training and validation steps looks as follows:\r\n\r\n```\r\ndef validation_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:\r\n        loss = torch.stack(outputs).mean()\r\n        self.log(\"val/loss\", loss)\r\n\r\ndef training_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:\r\n        loss = torch.stack(outputs).mean()\r\n        self.log(\"train/loss\", loss)\r\n```\r\n\r\nBasically, at the end of the training, I would like to query the final values of `train/loss` and `val/loss` and basically every other metrics that have been logged.\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12735",
    "createdAt": "2022-04-12T19:02:52Z",
    "updatedAt": "2022-06-08T12:50:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pamparana34"
    },
    "answer": {
      "body": "`trainer.callback_metrics` <- to access all the metrics\r\n`trainer.logged_metrics` <- to access only the logged metrics",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-13T11:47:42Z"
    }
  },
  {
    "title": "How to carry out validation loop on one single GPU",
    "body": "I have 8 gpus and during validation loop, I would like to only inference using one single GPU. I tried looking up documentation but I don't see any examples. Thanks!\r\n\r\nEnvironment:\r\n- Single machine\r\n- 8 GPUs\r\n- Pytorch Lightning 1.6.0\r\n- Strategy: DDP",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12721",
    "createdAt": "2022-04-11T23:03:40Z",
    "updatedAt": "2022-06-06T12:13:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vionwinnie"
    },
    "answer": {
      "body": "that is not possible.\r\n\r\nbut if you are calling `.validate` then you can configure your Trainer with devices=1\r\n\r\n```py\r\nTrainer(devices=1)\r\ntrainer.validate(...)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-12T08:05:53Z"
    }
  },
  {
    "title": "When I use the official sample to carry out back propagation manually, I make mistakes. First, there is no optimizer, and second, there is no attribute in the image",
    "body": "![image](https://user-images.githubusercontent.com/75936962/162600471-b3cb9237-f255-4d3c-828d-4150be132661.png)\r\n\r\n```python\r\nimport torch\r\nfrom torch import Tensor\r\nfrom pytorch_lightning import LightningModule\r\nclass Generator:\r\n    def __init__(self):\r\n        pass\r\n    def forward(self):\r\n        pass\r\n\r\nclass Discriminator:\r\n    def __init__(self):\r\n        pass\r\n    def forward(self):\r\n        pass\r\nclass SimpleGAN(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.G = Generator()\r\n        self.D = Discriminator()\r\n\r\n        # Important: This property activates manual optimization.\r\n        self.automatic_optimization = False\r\n\r\n    def sample_z(self, n) -> Tensor:\r\n        sample = self._Z.sample((n,))\r\n        return sample\r\n\r\n    def sample_G(self, n) -> Tensor:\r\n        z = self.sample_z(n)\r\n        return self.G(z)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # Implementation follows the PyTorch tutorial:\r\n        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\r\n        g_opt, d_opt = self.optimizers()\r\n\r\n        X, _ = batch\r\n        batch_size = X.shape[0]\r\n\r\n        real_label = torch.ones((batch_size, 1), device=self.device)\r\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\r\n\r\n        g_X = self.sample_G(batch_size)\r\n\r\n        ##########################\r\n        # Optimize Discriminator #\r\n        ##########################\r\n        d_x = self.D(X)\r\n        errD_real = self.criterion(d_x, real_label)\r\n\r\n        d_z = self.D(g_X.detach())\r\n        errD_fake = self.criterion(d_z, fake_label)\r\n\r\n        errD = errD_real + errD_fake\r\n\r\n        d_opt.zero_grad()\r\n        self.manual_backward(errD)\r\n        d_opt.step()\r\n\r\n        ######################\r\n        # Optimize Generator #\r\n        ######################\r\n        d_z = self.D(g_X)\r\n        errG = self.criterion(d_z, real_label)\r\n\r\n        g_opt.zero_grad()\r\n        self.manual_backward(errG)\r\n        g_opt.step()\r\n\r\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\r\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\r\n        return g_opt, d_opt\r\nbatch=torch.randn(3,2)\r\nbatch_idx=torch.ones(3)\r\nSimpleGAN().training_step(batch,batch_idx)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12683",
    "createdAt": "2022-04-10T03:40:54Z",
    "updatedAt": "2022-08-03T03:04:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Hou-jing"
    },
    "answer": {
      "body": "When I upgraded the version to the latest version, I solved this problem\u3002\r\nHowever, my running speed has been greatly affected. I used to have an epoch every 8 minutes, but now it has been delayed for a long time, and the data can't be loaded. I don't know why\r\nAnd this is the code\r\n[https://colab.research.google.com/drive/1dCP7-1xK48-PohGc8-RKx3Ne2HWd4Jkq#scrollTo=frTD9xWvBEUT]\r\n\r\n",
      "author": {
        "login": "Hou-jing"
      },
      "createdAt": "2022-04-10T11:47:53Z"
    }
  },
  {
    "title": "Trainer: loss stagnates, whereas custom train implementation continues converging ??",
    "body": "Hi,\r\nit would be great if you can help me unravel, what is a mystery to me.\r\n\r\n**Background**\r\nI have adapted a pretrained model for image regression. \r\n\r\n**Issue :**\r\nIf I finetune the model using the lightning trainer, the training loss stagnates at a value of ~10. However, in my pytorch training implementation training and validation loss become much less.\r\n\r\nCan you help me understand where my mistake is? Did I implement `.train_step` and `.forward` correctly?\r\n\r\nPLModule:\r\n```\r\nclass RGBYieldRegressor(LightningModule):\r\n    def __init__(self, optimizer:str = 'sgd', k:int = 0, lr:float = 0.001, momentum:float = 0.8, wd:float = 0.01, batch_size:int = 16, pretrained:bool = True):\r\n        super().__init__()\r\n\r\n        self.lr = lr\r\n        self.momentum = momentum\r\n        self.wd = wd\r\n        self.batch_size = batch_size\r\n        self.k = k\r\n\r\n        optimizers = {'adam': Adam, 'sgd': SGD}\r\n        self.optimizer = optimizers[optimizer]\r\n\r\n        self.criterion = nn.MSELoss(reduction='mean')\r\n\r\n        self.model_arch = model\r\n\r\n        num_target_classes = 1\r\n\r\n        self.model = models.resnet50(pretrained=pretrained)\r\n        num_filters = self.model.fc.in_features\r\n        self.model.fc = nn.Sequential(\r\n            nn.ReLU(),\r\n            nn.Linear(num_filters, num_target_classes))\r\n\r\n    def forward(self, x):\r\n        return torch.flatten(self.model(x))\r\n\r\n    def training_step(self, batch, batch_idx): # torch.autograd?\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n\r\n    def predicts_step(self, batch, batch_idx, dataloader_idx=0):\r\n        return self.model(batch).squeeze()\r\n\r\n    def configure_optimizers(self):\r\n        return self.optimizer(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.wd)\r\n\r\n```\r\n\r\nTrainer:\r\n```\r\ntrainer = Trainer(\r\n            max_epochs=50,  # general\r\n            num_sanity_val_steps=0,\r\n            devices=1,\r\n            accelerator=\"auto\",\r\n            callbacks=callbacks,\r\n            default_root_dir=this_output_dir,\r\n            weights_save_path=this_output_dir,\r\n            logger=logger,\r\n            num_processes=1,  \r\n        )\r\n        trainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\r\n```\r\n\r\nvs. pytorch training:\r\n```\r\nfor phase in ['train', 'val']:\r\n    if phase == 'train':\r\n        model.train()  # Set model to training mode\r\n    else:\r\n        model.eval()   # Set model to evaluate mode\r\n    running_loss = 0.0\r\n\r\n    # Iterate over data.\r\n    for inputs, labels in dataloaders[phase]:\r\n        inputs = inputs.to(device)\r\n        labels = labels.to(device)\r\n\r\n        # zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # forward\r\n        # track history if only in train\r\n        with torch.set_grad_enabled(phase == 'train'):\r\n            outputs = model(inputs)\r\n            loss = criterion(torch.flatten(outputs), labels.data)\r\n            if phase == 'train':\r\n                loss.backward()\r\n                optimizer.step()\r\n        # statistics\r\n        running_loss += loss.item() * inputs.size(0)\r\n\r\n    epoch_loss = running_loss / len(dataloaders[phase].dataset)\r\n    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12667",
    "createdAt": "2022-04-08T08:39:42Z",
    "updatedAt": "2022-08-19T13:35:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stillsen"
    },
    "answer": {
      "body": "update:\r\n```py\r\nloss = self.criterion(y, y_hat)\r\n```\r\nto\r\n```py\r\nloss = self.criterion(y_hat, y)\r\n```\r\neverywhere.\r\n\r\nalso:\r\n```py\r\ntrainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\r\n```\r\ncan be just\r\n```py\r\ntrainer.fit(lightningmodule, datamodule=datamodule)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-08T08:53:10Z"
    }
  },
  {
    "title": "model weight file corrupted when training on multi-gpus",
    "body": "When I trained on multi-GPU, the model weight file is corrupted. I guess the reason is that multi-gpus are saving the model weight at the same time. So how can I call the CheckpointCallback on a specific GPU?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12654",
    "createdAt": "2022-04-07T11:21:10Z",
    "updatedAt": "2022-04-08T01:19:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kleinzcy"
    },
    "answer": {
      "body": "checkpoints are saved only on global rank 0 which means only one of the GPUs will save the checkpoint even in the case of multi-node training. The issue might be something else.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-07T12:51:59Z"
    }
  },
  {
    "title": "hyper parameters not restored while resuming training",
    "body": "I call `save_hyperparameters()` in `__init__()`, and all hyper parameters sent to PL model are saved to checkpoint file. However, when i resume training from a checkpoint(call `trainer.fit(..., ckpt_path=checkpoint_file_path)`), the hyper parameters are not restored from checkpoint file and all of them keep initial values.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12639",
    "createdAt": "2022-04-06T09:48:58Z",
    "updatedAt": "2022-06-14T01:29:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Haojia521"
    },
    "answer": {
      "body": "hyperparameters are not restored by default because it allows users to update them if they want, using the checkpoint, while resuming.\r\n\r\nyou can do this:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(checkpoint_file_path)\r\ntrainer.fit(model, ..., ckpt_path=checkpoint_file_path)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-06T11:53:27Z"
    }
  },
  {
    "title": "Manually averaging metrics when logging",
    "body": "I have a metric from `torchmetric` as follows:\r\n\r\n```python\r\nAccuracy(\r\n    num_classes=self.model.out_channels,\r\n     average='none',\r\n     ignore_index=self.ignore_index\r\n)\r\n```\r\nObviously I can not log this, however I don't want to set average to any aggregation. I want to log its mean in `training_step` but want to preserve the class wise metric to till end of epoch where I display it to terminal. I want the metric to reset at epoch end only, so can't call `compute()` in training step.\r\n\r\nHow to solve this?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12636",
    "createdAt": "2022-04-06T07:57:27Z",
    "updatedAt": "2022-06-18T15:15:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "digital-idiot"
    },
    "answer": {
      "body": "From [this comment](https://github.com/PyTorchLightning/pytorch-lightning/issues/4396#issuecomment-717571905) I had the idea that:\r\n> if the .compute()method is called the internal state is reset.\r\n\r\nHowever, this behavior has changed, now calling `compute()` does not reset the state of the metrics. See [PR #5409](https://github.com/PyTorchLightning/pytorch-lightning/pull/5409)\r\n",
      "author": {
        "login": "digital-idiot"
      },
      "createdAt": "2022-04-06T14:23:34Z"
    }
  },
  {
    "title": "Loading model for prediction yields RuntimeError: Error(s) in loading state_dict",
    "body": "Hi,\r\nI want to use a pretrained ResNet or DenseNet with adjusted fc layer for image regression.\r\nAfter training, however, when I want to **load the finetuned DenseNet** for prediction I get a **RuntimeError**: Error(s) in loading state_dict.\r\nThis error does not occur for the ResNet.\r\nMy best guess is that this is somehow linked to checkpointing and that the value for `self.model_arch` might not be properly stored in the state_dict?\r\nBut I am not even sure if I need to tell the LightningModule what to save in the state_dict.\r\nAny ideas what might go wrong?\r\n\r\nBig Thanks already!\r\n\r\nLightningModule constructor:\r\n```\r\nclass ImageRegressor(LightningModule):\r\n    def __init__(self, optimizer:str = 'adam', k:int = 0, lr:float = 1e-3, batch_size:int = 16, pretrained:bool = True, tune_fc_only:bool = False, model: str = 'resnet50'):\r\n        super().__init__()\r\n        self.lr = lr\r\n        self.batch_size = batch_size\r\n        self.k = k\r\n        \r\n        optimizers = {'adam': Adam, 'sgd': SGD}\r\n        self.optimizer = optimizers[optimizer]\r\n        self.criterion = nn.MSELoss(reduction='mean')\r\n        self.model_arch = model\r\n        num_target_classes = 1\r\n\r\n        if self.model_arch == 'resnet50':\r\n            # init a pretrained resnet\r\n            self.model = models.resnet50(pretrained=pretrained)\r\n            num_filters = self.model.fc.in_features\r\n            self.model.fc = nn.Sequential(\r\n                nn.ReLU(),\r\n                nn.Linear(num_filters, num_target_classes))\r\n        elif self.model_arch == 'densenet':\r\n            print('setting up densenet')\r\n            self.model = models.densenet121(pretrained=pretrained)\r\n            num_filters = self.model.classifier.in_features\r\n            self.model.classifier = nn.Sequential(\r\n                nn.ReLU(),\r\n                nn.Linear(num_filters, num_target_classes))\r\n        if pretrained:\r\n            if tune_fc_only: # option to only tune the fully-connected layers\r\n                for child in list(self.model.children())[:-1]:\r\n                    for param in child.parameters():\r\n                        param.requires_grad = False\r\n```\r\n\r\nSaving is done automatically using this checkpoint:\r\n```\r\nModelCheckpoint(dirpath=this_output_dir,\r\n                                     filename='model_'+str(k)+'_{epoch}.pt',\r\n                                     monitor='val_loss')\r\n```\r\n\r\nloading checkpoint:\r\n```\r\nmodel = ImageRegressor(pretrained=True, tune_fc_only=True, model='densenet')\r\ntype(model).load_from_checkpoint(path)\r\n```\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12621",
    "createdAt": "2022-04-05T15:32:34Z",
    "updatedAt": "2022-08-20T11:38:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stillsen"
    },
    "answer": {
      "body": "should be:\r\n\r\n```py\r\nmodel = ImageRegressor.load_from_checkpoint(path, pretrained=True, tune_fc_only=True, model='densenet')\r\n```\r\n\r\ncheck out the examples and description here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#load-from-checkpoint\r\n\r\nslightly similar discussion: https://github.com/PyTorchLightning/pytorch-lightning/discussions/12399",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-04-05T17:31:20Z"
    }
  },
  {
    "title": "Option for disable tf32",
    "body": "Hi, is there a way in trainer to disable tf32 for ampere architecture? It's motivated by this discussion:https://discuss.pytorch.org/t/numerical-error-on-a100-gpus/148032/2\r\n\n\ncc @justusschock @kaushikb11 @awaelchli @borda @rohitgr7",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12601",
    "createdAt": "2022-04-01T23:06:14Z",
    "updatedAt": "2022-06-19T11:18:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dnnspark"
    },
    "answer": {
      "body": "Hi @dnnspark! Simply setting the flags in your script doesn't work?\r\n```python\r\ntorch.backends.cuda.matmul.allow_tf32 = False\r\ntorch.backends.cudnn.allow_tf32 = False\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-04-02T03:01:26Z"
    }
  },
  {
    "title": "Pattern for specifying modules in argument parser versus CLI",
    "body": "With the new experimental CLI, I've been trying to think of how to reconcile using the `add_model_specific_args` pattern with YAML configs; I've been digging through the docs and couldn't find the pieces I needed so hoping others could chime in. Basically I'd want to implement this once, and have backwards compatibility for both the `argparse` and new CLI routes, and I might be mistaken, but it feels that the two routes involve mutually exclusive patterns.\r\n\r\nLet's say we have an abstract `pl.LightningModule` that allows the user to choose an activation function:\r\n\r\n```python\r\nfrom ast import literal_eval\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\n\r\nclass LitModule(pl.LightningModule):\r\n    def __init__(self, input_dim: int, output_dim: int, activation: str):\r\n        # cross your fingers it resolves, something like `nn.SilU`\r\n        act_class = literal_eval(activation)\r\n        # instantiate the activation function object\r\n        self.model = nn.Sequential(nn.Linear(input_dim, output_dim), act_class())\r\n```\r\n\r\nWith `argparse`, the way I would set this up would be to implement `add_model_specific_args`:\r\n\r\n```python\r\n...\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"LitModule\")\r\n        parser.add_argument(\"--input_dim\", type=int)\r\n        parser.add_argument(\"--output_dim\", type=int)\r\n        parser.add_argument(\"--activation\", type=str, default=\"nn.SiLU\", help=\"Reference to activation function class in `torch.nn`.\")\r\n```\r\n\r\nAnd in my training script:\r\n\r\n```python\r\nfrom torch import nn\r\n\r\nparser = ArgumentParser()\r\nparser = LitModule.add_model_specific_args(parser)\r\n\r\nargs = parser.parse_args()\r\n\r\nmodel = LitModule(args)\r\n```\r\n\r\nWith the new CLI, the better option would be to use `nn.Module` for typing instead of `str` for the `activation` argument, i.e. redefining the `__init__` so we remove the possibility of running arbitrary code via `literal_eval` since it's taken care of by `jsonargparse`/`LightningCLI`:\r\n\r\n```python\r\nclass LitModule(pl.LightningModule):\r\n    def __init__(self, input_dim: int, output_dim: int, activation: nn.Module):\r\n        # don't need to use `literal_eval` anymore\r\n        self.model = nn.Sequential(nn.Linear(input_dim, output_dim), activation())\r\n```\r\n\r\n...and in my YAML config:\r\n\r\n```yaml\r\nmodel:\r\n   class_path: mymodule.LitModule\r\n   init_args:\r\n      input_dim: 8\r\n      output_dim: 2\r\n      activation: torch.nn.SiLU\r\n```\r\n\r\nI can't think of any way to implement this same pattern with `argparse`. While I prefer the CLI approach because of its potential for composability, it's experimental and sometimes you want more granular control as you get from a training script, i.e. putting the components together yourself, multiple optimizers, etc. which has a bit more flexibility. There might not be a one-size fits all, but I'd appreciate any feedback/discussion.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12574",
    "createdAt": "2022-04-01T19:59:48Z",
    "updatedAt": "2022-06-11T03:12:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "laserkelvin"
    },
    "answer": {
      "body": "If you truly want to have both, you could do `activation: Union[str, nn.Module]` and use the `literal_eval` only if an `str` is received. But even if you do this, there is little reason to keep using `add_model_specific_args` since with `LightningCLI` you could also give an `str` instead of the `class_path` and `init_args` pair.\r\n\r\nRegarding \"i.e. putting the components together yourself, multiple optimizers, etc. which has a bit more flexibility\" best if you give more details about what you want to do, and then see if it makes sense or not to use `LightningCLI`.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-05-10T20:58:43Z"
    }
  },
  {
    "title": "Training based on iterations",
    "body": "Hi, \r\nCould anyone advice me how to set up PyTorch lightning trainer to learn based on iterations instead of epochs? \r\n\r\nThank you! \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12511",
    "createdAt": "2022-03-29T17:39:17Z",
    "updatedAt": "2023-04-05T07:20:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mshooter"
    },
    "answer": {
      "body": "Hi @mshooter , you can use the `min_steps` and `max_steps `arguments on the Trainer to do training based on iterations instead of epochs. https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-steps",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-03-29T18:08:22Z"
    }
  },
  {
    "title": "Should I configure FP16, optimizers, batch_size in DeepSpeed config of Pytorch-Lightning?",
    "body": "My `deepspeed_zero2_config.json`:\r\n```json\r\n{\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"offload_optimizer\": {\r\n            \"device\": \"cpu\",\r\n            \"pin_memory\": true\r\n        },\r\n        \"allgather_partitions\": true,\r\n        \"allgather_bucket_size\": 2e8,\r\n        \"overlap_comm\": true,\r\n        \"reduce_scatter\": true,\r\n        \"reduce_bucket_size\": 2e8,\r\n        \"contiguous_gradients\": true\r\n    },\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"gradient_clipping\": \"auto\",\r\n    \"steps_per_print\": 2000,\r\n    \"train_batch_size\": \"auto\",\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"wall_clock_breakdown\": false\r\n}\r\n```\r\n\r\nI have some questions about how to configure DeepSpeed in Pytorch-Lightning:\r\n- I see that the [custom deepspeed config](https://pytorch-lightning.readthedocs.io/en/latest/advanced/model_parallel.html#custom-deepspeed-config) includes optimizer and scheduler. Should I add them in my config even I have configured in `Model.configure_optimizers`?\r\n  ```\r\n  You have not specified an optimizer or scheduler within the DeepSpeed config. Using `configure_optimizers` to define optimizer and scheduler.\r\n  ```\r\n- Should I add `fp16` config into deepspeed config json even I have passed `precision=\"bf16\"` in `pl.Trainer`?\r\n- Should I pass `logging_batch_size_per_gpu` to `pl.plugins.DeepSpeedPlugin` even I have configured batch_size in data loader?\r\n  ```\r\n  [2022-03-24 12:42:11,529] [WARNING] [deepspeed.py:630:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedPlugin(logging_batch_size_per_gpu=batch_size))`.\r\n  ```\r\n- It appears in log before training every time. Is that okey?\r\n  ```\r\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n  ninja: no work to do.\r\n  ```\r\n\r\nThanks a lot!\ud83d\ude0a",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12465",
    "createdAt": "2022-03-26T09:57:07Z",
    "updatedAt": "2022-06-08T12:24:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ShaneTian"
    },
    "answer": {
      "body": "yes, you don't need to set them inside config since this is done by Lightning already here if you set them in trainer and lightning module: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/strategies/deepspeed.py",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-26T14:14:28Z"
    }
  },
  {
    "title": "help defining new training_step() on a callback",
    "body": "Hi!\r\nI need to create a callback that once every N training steps performs the forward pass over the PL module and do some calculations.\r\nMy first approach has been to simply create a callback and then define a new training_step() within that callback that does my needed calculations. The problem is that this is calculations are not being executed.\r\nUsing the debugger I see that the callback is correctly initialized and correctly passed to the trainer, but it is not entering in this newly defined training step. Here is a minimal example of what I need to do\r\n![image](https://user-images.githubusercontent.com/95293295/159908838-815cb65d-10a7-40d5-821c-31ae7a1d3346.png)\r\nDo you have some insights on what I am doing wrong?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12442",
    "createdAt": "2022-03-24T11:41:51Z",
    "updatedAt": "2022-09-04T14:12:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "hey @malfonsoarquimea !\r\n\r\n`Callback.training_step` is not a hook so it won't be called automatically. For you use-case you can do something like:\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(..., every_n_train_steps):\r\n        self.every_n_train_steps = every_n_train_steps\r\n        ...\r\n    \r\n    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\r\n        if trainer.global_step % self.every_n_train_steps == 0:\r\n            outputs = pl_module(self.input)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-24T14:12:43Z"
    }
  },
  {
    "title": "Instantiate data augmentations through CLI",
    "body": "Hi,\r\nI want to implement a BYOL-like model using Lightning CLI, and one of the key aspects is to use specific data augmentations. However, I don't manage to indicate to the cli that I want to use these specific data augmentations. Here is a snippet of my code:\r\n***main.py***\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.utilities.cli import LightningCLI, MODEL_REGISTRY\r\nfrom pl_bolts.datamodules import CIFAR10DataModule\r\nfrom pl_bolts.models.self_supervised.simclr import SimCLRTrainDataTransform, SimCLREvalDataTransform\r\n\r\n\r\nclass DummyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc = nn.Linear(32*32*3, 10)\r\n        self.loss_fn = nn.MSELoss()\r\n\r\n    def shared_step(self, batch, batch_idx):\r\n        x, y = batch[0][:2]\r\n        z1 = self.fc(x.reshape(x.size(0), -1))\r\n        z2 = self.fc(y.reshape(y.size(0), -1))\r\n        return self.loss_fn(z1, z2)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.shared_step(batch, batch_idx)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        return self.shared_step(batch, batch_idx)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters())\r\n\r\n\r\n# THE COMMENTED LINES BELOW RUN PERFECTLY\r\n# trainer = pl.Trainer()\r\n# model = DummyModel()\r\n# dm = CIFAR10DataModule()\r\n# dm.train_transforms = SimCLRTrainDataTransform(32)\r\n# dm.val_transforms = SimCLREvalDataTransform(32)\r\n# trainer.fit(model, dm)\r\n\r\ncli = LightningCLI(DummyModel, CIFAR10DataModule, run=False)\r\n\r\n# not instantiated!\r\nprint(cli.config_init.data.train_transforms)\r\n```\r\nand here is my ***config.yaml***:\r\n```yaml\r\ndata:\r\n  train_transforms:\r\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLRTrainDataTransform\r\n    init_args:\r\n      input_height: 32\r\n  val_transforms:\r\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLREvalDataTransform\r\n    init_args:\r\n      input_height: 32\r\n```\r\n\r\nTo run the code, I run the following command:\r\n```\r\npython main.py --config config.yaml\r\n```\r\nand here is the error I get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alain/code/misc/examples/main.py\", line 42, in <module>\r\n    cli.trainer.fit(cli.model, cli.datamodule)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\r\n    self.prefetching(self.prefetch_batches)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\r\n    self._fetch_next_batch()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\r\n    batch = next(self.dataloader_iter)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    data = self._next_data()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\r\n    return self.dataset[self.indices[idx]]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\r\n    img = self.transform(img)\r\nTypeError: 'dict' object is not callable\r\n```\r\nWhat I understand from the error is that `LightningCLI` doesn't manage to interprete and instantiate the data augmentation model and understands it only as a dictionary with keys `class_path` and `init_args`. Does someone encounter a similar issue and/or knows how to solve it?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12424",
    "createdAt": "2022-03-23T16:18:04Z",
    "updatedAt": "2022-06-10T16:54:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "aRI0U"
    },
    "answer": {
      "body": "cc @carmocca ",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-24T14:13:24Z"
    }
  },
  {
    "title": "How to switch dataloader every n training steps",
    "body": "Hi everyone,\r\nIn my current setup, I would like to change the dataloader during a training epoch:\r\n\r\nThis is what I would like to achieve:\r\nstep 1.Train on dataset 1 for n batches\r\nstep 2.Train on dataset 2 for n batches\r\nstep 3.Go to step 1\r\n\r\nI found [this solution](https://forums.pytorchlightning.ai/t/how-to-switch-dataloaders-between-epochs-in-lightning/137/2) on the old forum but this only switches the dataset after each epoch.\r\n\r\nHere is my current attempt at switching it every n batches:\r\n```python\r\nclass SimpleModule(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = ...\r\n        self.batch_size = ...\r\n        self.change_every_n_batch = 20\r\n    \r\n    def train_dataloader(self):\r\n        self.current_dataset = (self.global_step // self.change_every_n_batch) % 2\r\n        if self.current_dataset == 0:\r\n            dataset = Dataset1()\r\n        elif self.current_dataset == 1:\r\n            dataset = Dataset2()\r\n\r\n        dataloader = DataLoader(dataset, batch_size=self.batch_size)\r\n        return dataloader\r\n    \r\n    def on_train_batch_end(self, outputs, batch, batch_idx):\r\n        new_dataset = (self.global_step // self.change_every_n_batch) % 2\r\n        if new_dataset != self.current_dataset:\r\n            self.trainer.reset_train_dataloader(self)\r\n```\r\n`train_dataloader()` is called as expected every 20 batches by `on_train_batch_end()` but the returned dataloader does not seem to be used during the training loop.\r\n\r\nAny idea what could be going wrong? Or do you have a solution for what I want to achieve?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12415",
    "createdAt": "2022-03-22T16:38:29Z",
    "updatedAt": "2025-07-13T10:00:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "matprst"
    },
    "answer": {
      "body": "hey @matprst !\r\n\r\nyou can set:\r\n- `limit_train_batches=n`. This will ensure that every training epoch will progress for only n batches\r\n- `reload_dataloaders_every_n_epochs=1`. this will ensure that train dataloader is reloaded after every epoch.\r\n\r\nand inside `train_dataloader`, flip the dataloader on each reload. something like:\r\n```py\r\ndef train_dataloader(self):\r\n    if self.some_flag:\r\n        dataset = Dataset1()\r\n    else:\r\n        dataset = Dataset2()\r\n\r\n    self.some_flag = not self.some_flag\r\n\r\n    return DataLoader(dataset, batch_size=self.batch_size)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-23T10:05:41Z"
    }
  },
  {
    "title": "Error with loading model checkpoint",
    "body": "Hi everyone. I was recently running a lightning model and saved a checkpoint to store the intermediate results. When I try to open the checkpoint, I get an error that positional arguments (used to initialize the lightning module) are not present. This wouldn't be a big deal but one of the positional arguments is the encoder (used for BarlowTwins training). I was worried if I loaded the model checkpoint with an encoder initialized with starting weights, this would overwrite the weight parameters stored in the checkpoint. See the error log and a block of code below. Any suggestions on how I can appropriately load this stored model to resume training?\r\n\r\n      model_ckpt = BarlowTwins.load_from_checkpoint('/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt')\r\n\r\n          Traceback (most recent call last):\r\n          File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 435, in <module>\r\n            main(default_config)\r\n          File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 427, in main\r\n            model_ckpt = BarlowTwins.load_from_checkpoint('/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt')\r\n          File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 156, in load_from_checkpoint\r\n            model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n          File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 198, in _load_model_state\r\n            model = cls(**_cls_kwargs)\r\n        TypeError: __init__() missing 5 required positional arguments: 'encoder', 'encoder_out_dim', 'num_training_samples', 'batch_size', and 'weight_decay'\r\n\r\n\r\noriginal model loaded with:\r\n\r\n        encoder = resnet18(zero_init_residual=True)\r\n    \r\n        model = BarlowTwins(\r\n            encoder=encoder,\r\n            encoder_out_dim=encoder_out_dim,\r\n            learning_rate = default_config['LR'],\r\n            weight_decay = default_config['WD'],\r\n            num_training_samples=262144,\r\n            batch_size=BATCH_SIZE,\r\n            z_dim=default_config['Z_DIM'],\r\n            lambda_coeff = default_config['LAMBDA'],\r\n            max_epochs=MAX_EPOCHS\r\n        )",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12399",
    "createdAt": "2022-03-21T18:53:40Z",
    "updatedAt": "2022-06-13T03:01:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dmandair"
    },
    "answer": {
      "body": "hey @dmandair !\r\n\r\ndid you call `self.save_hyperparameters()` inside your `LM.__init__`? else hyperparameters won't be saved inside the checkpoint and you might need to provide them again using `LMModel.load_from_checkpoint(..., encoder=encoder, encoder_out_dim=encoder_out_dim, ...)`.\r\n\r\nalso note that, if you are passing an `nn.Module` inside your LM and calling `self.save_hyperparameters()`, it will save that too inside your hparams, which is not a good thing considering that nn.Modules are saved inside checkpoint state_dict and might create issues for you. Ideally, you should ignore them using `self.save_hyperparameters(ignore=['encoder'])`. Check out this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/12068",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-22T10:14:34Z"
    }
  },
  {
    "title": "Lighting Module Loaded From Checkpoint Generates Different Output Each Time",
    "body": "I'm trying to gain some confidence in a model that seems to be training fine.\r\n\r\nAs a simple sanity check I'm trying to make sure I can load then test a checkpoint with the same input, expecting to be able to produce the same output each and every time (I'm using the same input and checkpoint each time so I expect the output to be the same).\r\n\r\nUnfortunately, I'm observing different output each time I reload the checkpoint.\r\n\r\nHere is the essence of what I'm doing:\r\n```\r\nfor n in range(2):\r\n        my_module = MyLightningModule.load_from_checkpoint(ckpt_path)\r\n\r\n        my_dataset = MyDataset()\r\n        batch = my_dataset.get_sanity_test_batch()  # confirmed to be the same batch every time\r\n\r\n        # this output is different every time (???)\r\n        output = my_module.model.generate(batch, max_length=some_length)\r\n```\r\n\r\n**It is also probably worth noting that the model trained/loaded by my_module is a hugginface T5 transformer ([_T5ForConditionalGeneration_](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration) )**\r\n\r\nPlease help me figure out how to ensure output is consistent after loading a trained checkpoint.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12397",
    "createdAt": "2022-03-21T15:50:43Z",
    "updatedAt": "2024-01-17T16:09:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "xsys-technology"
    },
    "answer": {
      "body": "Turns out that I was doing something a little different in the actual code than:\r\n`my_module = MyLightningModule.load_from_checkpoint(ckpt_path)`\r\n\r\nWhen I do this exactly, things work as expected :)",
      "author": {
        "login": "xsys-technology"
      },
      "createdAt": "2022-03-21T17:43:10Z"
    }
  },
  {
    "title": "Doesn't it support bf16 when using Deepspeed?",
    "body": "Hello.\r\n\r\nI am currently trying to train a model using Deepspeed zero-2.\r\nIn Deepspeed official repository, can use bf16 when using zero-2, but isn't it currently supported in PL?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12359",
    "createdAt": "2022-03-17T16:09:16Z",
    "updatedAt": "2022-08-20T11:37:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "toriving"
    },
    "answer": {
      "body": "if it's supported natively in deepspeed, PL should support it. Can you share some references from deepspeed?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-17T17:57:59Z"
    }
  },
  {
    "title": "How to evaluate every X steps?",
    "body": "What is the best practice for performing evaluation on the dev and/or test set/s every X steps and not every epoch end?\r\n\r\nIn the [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#) I saw the options of overriding `validation_epoch_end` and `test_epoch_end`, but could not find how to evaluate \"more frequently\".\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12349",
    "createdAt": "2022-03-16T20:58:52Z",
    "updatedAt": "2022-06-19T11:19:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zorikg"
    },
    "answer": {
      "body": "Hi @zorikg! You can configure it through the trainer flag `val_check_interval`. e.g.\r\n```python\r\n# check validation set 4 times during a training epoch\r\ntrainer = Trainer(val_check_interval=0.25)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#val-check-interval",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-03-17T01:46:25Z"
    }
  },
  {
    "title": "the process would be blocking in Validation step",
    "body": "Problem:\r\nI have a problem. when i train the model, the process would be blocking in validation step(or validation sanity check) but in the training step, it can work. And I debug in the pytorch-lightning , i found when loading data from validation dataloader it would be blocking. I am not sure what problem.\r\n\r\nEnvironment:\r\ndocker; the lastest pytorch-lighting;gpu a100\r\n\r\nlog:\r\nINFO Using validation DataLoader3DOffset with {}\r\nINFO Building Sampling Cache for Dataloder\r\nSampling Cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 1445.07it/s]\r\nINFO Using 5 num_processes and 2 num_cached_per_queue for augmentation.                                                                                                           | 0/2 [00:00<?, ?it/s]\r\nINFO VALIDATION KEYS:\r\n odict_keys(['case_0', 'case_7'])\r\nusing pin_memory on device 0\r\n\r\n---------------------------------------------------\r\n\r\nI test the validation step and it world jam \r\n\r\ncan you help me, thank you !!!!!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12329",
    "createdAt": "2022-03-15T01:47:26Z",
    "updatedAt": "2022-12-14T14:51:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lyingflatDDD"
    },
    "answer": {
      "body": "sorry,  It is my fault. now i solve this problem",
      "author": {
        "login": "lyingflatDDD"
      },
      "createdAt": "2022-03-17T06:57:29Z"
    }
  },
  {
    "title": "Torch accuracy and sklearn accuracy is v different",
    "body": "This is the code\r\n\r\n```python\r\n    def test_step(self,batch,batch_idx):\r\n        image,label=batch\r\n        pred = self(image)\r\n        loss=self.criterion(pred.flatten(),label.float()) #calculate loss\r\n        acc=self.metrics(pred.flatten(),label)#calculate accuracy\r\n        pred=torch.sigmoid(pred)\r\n        return {'loss':loss,'acc':acc,'label':label,'pred':pred}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n        acc=torch.stack([x[\"acc\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy().ravel()\r\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy().ravel()\r\n        pred=pred.astype(int)\r\n        print('torch acc',acc)\r\n        print(classification_report(label,pred))\r\n        print('sklearn',accuracy_score(label,pred))\r\n```\r\n\r\nThere is difference of 10-15% between accuracies obtained by torchmetrics and sklearn",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12311",
    "createdAt": "2022-03-12T11:42:43Z",
    "updatedAt": "2022-06-22T11:34:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "This is the solution\r\n```\r\ndef test_step(self,batch,batch_idx):\r\n        image,label=batch\r\n        pred = self(image)\r\n        \r\n        return {'label':label,'pred':pred}\r\n\r\ndef test_epoch_end(self, outputs):\r\n\r\n    label=torch.cat([x[\"label\"] for x in outputs])\r\n    pred=torch.cat([x[\"pred\"] for x in outputs])\r\n    acc=self.metrics(pred.flatten(),label)\r\n    pred=pred.detach().cpu().numpy().ravel()\r\n    label=label.detach().cpu().numpy().ravel()\r\n    \r\n    pred=np.where(pred>0.5,1,0).astype(int)\r\n    print('torch acc',acc)\r\n    print(classification_report(label,pred))\r\n    print('sklearn',accuracy_score(label,pred))\r\n\r\n\r\n```",
      "author": {
        "login": "talhaanwarch"
      },
      "createdAt": "2022-04-02T22:38:19Z"
    }
  },
  {
    "title": "overfit_batches duplicates entire train DataLoader causing out of memory",
    "body": "I would appreciate some help understanding the `overfit_batches` argument for the trainer.\r\n\r\nWhen overfitting batches, it seems that the entire train dataloader is deepcopied ([code in v1.5.10](https://github.com/PyTorchLightning/pytorch-lightning/blob/9ebdc52ec631df92ca7cfc1ba852801dd36d3864/pytorch_lightning/trainer/data_loading.py#L465)). In my case, this immediately results in my machine running out of RAM because of the size of this dataset. I also have many validation dataloaders, so I believe this compounds the issue. \r\n\r\nWill this behavior of copying the entire dataloader be removed in a future release? I believe some other mechanism of duplication is necessary to avoid copying the data that is not included in the batches that are being overfitted. \r\n\r\nI was trying to read #10877 to understand how the behavior would change, but I am unsure.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12272",
    "createdAt": "2022-03-08T16:47:25Z",
    "updatedAt": "2022-09-09T19:36:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jonathanking"
    },
    "answer": {
      "body": "hey @jonathanking !\r\n\r\nthe deepcopy for train dataloader to the validation dataloader has been removed from master and will be available in the next release soon.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-16T09:41:57Z"
    }
  },
  {
    "title": "If tuner.scale_batch_size() accepts a train_dataloader, why can't this be used independently of trainer.fit(model, datamodule)?",
    "body": "My training code (after a lot of setup) looks like this:\r\n\r\n```python\r\n\r\n# Create a trainer\r\ntrainer = pl.Trainer.from_argparse_args(argparse.Namespace(**dict_args),\r\n                                        callbacks=my_callbacks)\r\n# Look for largest batch size and optimal LR\r\ntuner = Tuner(trainer)\r\ntuner.scale_batch_size(\r\n    model,\r\n    train_dataloaders=data_module.get_descending_size_train_dataloader(),\r\n    init_val=1)\r\ntuner.lr_find(model, data_module.train_dataloader())\r\n\r\n# Train the model\r\ntrainer.fit(model, data_module)\r\ntrainer.test(model, data_module)\r\n\r\n\r\n```\r\n\r\nI thought making a data module was the best practice. Am I not allowed to pass a dataloader to both `trainer.fit` and `tuner.scale_batch_size`? It's not completely clear from the documentation. Is my _only_ option to re-incorporate my data module into my Lightning Module? It'd be great to have both!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12264",
    "createdAt": "2022-03-08T00:11:24Z",
    "updatedAt": "2022-08-01T11:51:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jonathanking"
    },
    "answer": {
      "body": "passing train_dataloader directly to batch_size scaling call will give you an exception error. But passing datamodule is allowed. The reason is that after each batch size scale iteration, we need to reinitialize the dataloader using the scaled value for batch_size param and if you pass in the dataloader itself, it won't be possible to reinitialize the dataloader as of now. Maybe we can add support for it in the future.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-08T09:46:02Z"
    }
  },
  {
    "title": "New error in Trainer started appearing recently in a previously running code.",
    "body": "When I create a Trainer and run Trainer.fit() I am now getting the following error:\r\n\r\n```\r\nraise TypeError(\"cannot assign '{}' as child module '{}' \"\r\nTypeError: cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected)\r\n```\r\n\r\nThis is a new error and this code was just working earlier. Do yall know what could be causing this issue?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12250",
    "createdAt": "2022-03-07T03:11:48Z",
    "updatedAt": "2023-07-27T08:45:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cwoolfo1"
    },
    "answer": {
      "body": "Do you have an attribute precision defined in your lightning module? If so, this is an improper override of the lightning module which is leading to this error: https://github.com/PyTorchLightning/pytorch-lightning/blob/49a4a36ad45b937dd0124ecfb08eb7400dbf3950/pytorch_lightning/core/lightning.py#L102-L103\r\n\r\n",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-03-08T18:58:16Z"
    }
  },
  {
    "title": "Unable to build docs locally",
    "body": "I am trying to build docs locally. I followed the steps [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md#documentation) but when I go to `http://docs/build/html/index.html` I get: \"This site can\u2019t be reached\"\r\n\r\nHere is the end of the output for when I run `make html`:\r\n```\r\ncopying images... [ 97%] _static/images/mnist_imgs/restart_runtime.png\r\ncopying images... [ 98%] _static/images/mnist_imgs/tpu_start.png\r\ncopying images... [ 99%] _static/images/mnist_imgs/tpu_fast.png\r\ncopying images... [100%] _static/images/general/PTL101_youtube_thumbnail.jpg\r\n\r\ncopying static files... done\r\ncopying extra files... done\r\ndumping search index in English (code: en)... done\r\ndumping object inventory... done\r\nbuild succeeded, 1 warning.\r\n\r\nThe HTML pages are in build/html.\r\nThe name of the builder is: htmlCopying sphinx_paramlinks stylesheet... done\r\n```\r\n\r\nIt looks all fine to me... ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12244",
    "createdAt": "2022-03-06T02:11:57Z",
    "updatedAt": "2022-08-03T03:00:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "daniellepintz"
    },
    "answer": {
      "body": "Hey @daniellepintz I think the URL should be something different. (the protocol shouldn't be http.)\r\n\r\nIn my env, the project root dir is:\r\n```\r\n/Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/\r\n```\r\nand the URL is:\r\n```\r\nfile:///Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/docs/build/html/index.html\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-03-06T17:48:15Z"
    }
  },
  {
    "title": "what does `step` mean in `max_steps` ?",
    "body": "I notice that `step` in PyTorch Lighting can mean `batch` or `optimizer.step()`.  So what does `step` mean in `max_steps` ? @rohitgr7\r\n\r\nThe following code maybe helpful:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L258-L260\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L323-L331\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L90-L93\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12220",
    "createdAt": "2022-03-04T02:24:28Z",
    "updatedAt": "2022-05-31T08:40:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yc1999"
    },
    "answer": {
      "body": "it means global_step. So if you have accumulation factors = 2 and max_steps = 10, then the total training batches that will be covered here will be 20 instead of 10. `global_step` indicate total optimization steps.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-04T08:47:00Z"
    }
  },
  {
    "title": "NCCL WARN Failed to open libibverbs.so[.1]",
    "body": "Just received qty 2 of A6000 and these are not compatible \r\nwith my existing docker file (lack of sm_86 support)\r\n```\r\nFROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime\r\nRUN pip install pytorch-lightning==1.0.7\r\n```\r\n\r\nSo upgraded my docker to \r\n```\r\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\r\nRUN pip install pytorch-lightning==1.5.10\r\n```\r\n\r\nI also made changed to my code the for the lightning braking change from \r\n```\r\ntrainer = pl.Trainer( gpus=[0,1],  \r\n        distributed_backend='ddp', , \r\n        ....\r\n```\r\n\r\nto \r\n\r\n```\r\ntrainer = pl.Trainer( gpus=[0,1],  \r\n        strategy='ddp', \r\n        ....\r\n```\r\n\r\nWhen I try to train it just stops.  So set env  NCCL_DEBUG=WARN \r\n [as per suggestion]( https://github.com/PyTorchLightning/pytorch-lightning/issues/9641)\r\nto get the following output:\r\n\r\n```\r\ninitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\ninitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n----------------------------------------------------------------------------------------------------\r\ndistributed_backend=nccl\r\nAll distributed processes registered. Starting with 2 processes\r\n----------------------------------------------------------------------------------------------------\r\n60b476048acc:22:22 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.10.3+cuda11.1\r\n60b476048acc:120:120 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n\r\n```\r\n\r\nSame happens when I try \r\n```\r\nFROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime \r\nFROM pytorch/pytorch:1.9.1-cuda11.1-cudnn8-runtime \r\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\r\n```\r\n\r\nMy old setup was 2xRTX Titan with nvlink while the new setup is 2xA6000 without a nvlink. [nvidia doc](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/nccl1.html) says that PCI is used but unclear if I need to do something to use this.\r\n\r\n[Distributed communication docs](https://pytorch.org/docs/stable/distributed.html) say \"NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA)\" .\r\n\r\nI suspect I am missing something about the breaking changes from pl 1.0 to 1.5. Would appreciate hints as to what to look for.\r\nIs NCCL something used in pl 1.0 or is this new to pl 1.5?\r\nDoes NCCL need to be installed?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12219",
    "createdAt": "2022-03-04T02:03:35Z",
    "updatedAt": "2022-06-17T02:26:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "johngrabner"
    },
    "answer": {
      "body": "Duplicate of #12235.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-03-06T17:51:17Z"
    }
  },
  {
    "title": "When are buffers moved to gpu?",
    "body": "I have an issue with a weighted mse function that I instantiate in the setup, with a buffer as parameter. Something like this:\r\n\r\n```python\r\n@torch.jit.script\r\ndef weighted_mse_func(weights, y, y_hat):\r\n    # weighted regression loss\r\n    reg_loss = torch.dot(weights,torch.mean(F.mse_loss(y_hat, y, reduction='none'), dim=0))\r\n    return reg_loss\r\n\r\ndef weighted_mse(weights):\r\n    def func(y, y_hat):\r\n        return weighted_mse_func(weights, y, y_hat)\r\n    return func\r\n\r\n\r\nclass model(pl.LightningModule):\r\n    def __init__(self, weights):\r\n        weights = torch.tensor(weights.copy(), dtype=self.dtype, device=self.device)\r\n        self.register_buffer(\"weights\", weights)\r\n    \r\n    def setup(self, stage):\r\n        super().setup(stage)\r\n        self.loss = weighted_mse(self.weights)\r\n```\r\n\r\nWhen initializing training on the GPU I get an error because `self.weights` is on CPU and not in GPU, if after the error I check the device of the buffer it's on GPU. So if I re-run the trainer, it works fine, also works fine if I call model.cuda() before training. What is going on? Why is the buffer not in GPU on the setup where it fails, but it is afterward? Is something asynchronous going on here?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12207",
    "createdAt": "2022-03-03T14:38:33Z",
    "updatedAt": "2022-06-08T05:59:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "grudloff"
    },
    "answer": {
      "body": "It seems that pytorch doesn't move buffers parameters in-place (like it is done for parameters), this results in references to buffers being useless if they are moved from one device to another. This issue is discussed in pytorch/pytorch#43815.",
      "author": {
        "login": "grudloff"
      },
      "createdAt": "2022-03-07T20:54:33Z"
    }
  },
  {
    "title": "Pytorch-lightning is not able to load the model checkpoint",
    "body": "Hi\r\nI first trained the model on kaggle on celeba:\r\nHere's the link to the notebook: https://www.kaggle.com/yashrathikaggle/resnet-gender-detection-with-98-16-accuracy/notebook\r\n\r\nWhile training the trained automatically checkpointed the best val_acc and train_loss. I downloaded the checkpoint on colab and tried to load the model to see the outputs. And it seems the model outputs are random. It doesn't looks like I was able to get the model.\r\n\r\nLink to colab: https://colab.research.google.com/drive/1E9eg3BkBQCsyPjmy1TZ3oR-kK7Ti4JXt#scrollTo=waqBK-6WaxrX\r\n\r\nPlease help if I am doing something wrong here.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12199",
    "createdAt": "2022-03-03T06:21:35Z",
    "updatedAt": "2022-06-24T14:03:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yashrathi-git"
    },
    "answer": {
      "body": "hey @yashrathi-git !\r\n\r\nyour code looks correct.\r\nWhy do you think that outputs are random?\r\nyou can also validate your model after loading using:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(...)\r\ntrainer = Trainer(...)\r\ntrainer.validate(model)  # or trainer.test(model)\r\n```\r\nif the above gives you the desired metrics then the model is loaded correctly.\r\n\r\none more thing, you might need to call `model.eval()` after loading the weights from the checkpoint.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-03-03T21:20:09Z"
    }
  },
  {
    "title": "fast_dev_run does not execute pl.LightningModule.test_step()",
    "body": "I may be misunderstanding something about the trainer argument `fast_dev_run`. When I provide `fast_dev_run=1` and I add a print statement in my LightningModule's `test_step` function, the print statement does not appear. In addition, I can see a progress bar for my training set and validation set, but no progress bar appears for the test set.\r\n\r\nIs `fast_dev_run` actually running n batches of my training set? I have passed in a DataModule to `trainer.fit()` that includes a test_dataloader.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12168",
    "createdAt": "2022-03-01T17:40:54Z",
    "updatedAt": "2022-06-08T18:08:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jonathanking"
    },
    "answer": {
      "body": "`fit` only runs training & validation, not testing.\r\n\r\n`trainer.test` runs the test_step",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-03-01T17:54:14Z"
    }
  },
  {
    "title": "Logging Multi-Label Metrics",
    "body": "Hey, I was wondering if there was a correct way to evaluate and log multi-class metrics at the end of an epoch, such that the metrics could be evaluated across multiple GPUs using ddp? \r\n\r\nTypically, if I was looking at a image classification problem I could use the following:\r\n\r\n```\r\nself.train_average_accuracy = torchmetrics.Accuracy(num_classes=self.num_classes)\r\n\r\nx, y = batch\r\noutputs = self(x)  # Evaluate logits.\r\nsigmoid_outputs = torch.sigmoid(outputs)  # Produce sigmoid outputs for multi-label metrics.\r\nloss = self.compute_loss(outputs, y)\r\naverage_acc = self.train_average_accuracy(sigmoid_outputs, y)\r\n```\r\n\r\nI could log the either with single GPU or multi-GPU accuracy using:\r\n\r\n```\r\nself.log(\r\n\"train_average_acc\",\r\naverage_acc,\r\non_step=False,\r\non_epoch=True,\r\nprog_bar=True,\r\n sync_dist=self.multi_gpu)\r\n```\r\n\r\nHowever, if I try modifying the accuracy metric to use multi-label accuracy using: \r\n```\r\ntorchmetrics.Accuracy(threshold=0.5, average=None, num_classes=self.num_classes)\r\n```\r\n\r\nI find that the metric is logged as nan, which is I'm guessing due to the tensor which contains the class wide accuracies not updating between batches. I've also tried to get around this I ended up passing `y` and `y_pred` into the `def validation_epoch_end(self, outputs)` and evaluating the metric using concatenated batches. However, I still find nan values logged. Is there any other way of logging the multi-label metrics other than, say iterating over the tensor containing the class level metrics?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12121",
    "createdAt": "2022-02-25T21:56:15Z",
    "updatedAt": "2022-05-30T20:39:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "deepbakes"
    },
    "answer": {
      "body": "As an update, I managed to develop a work around by iterating over the tensor that contained multi-label metrics that are evaluated on a class level. I.e, this function coulds do it\r\n\r\n```\r\n    for idx, metric_value in enumerate(class_metrics):  # Iterate over tensor elements.\r\n        if mode == \"debugging\":\r\n            print(f\"{phase}_{metric}_{class_labels[idx]}: {metric_value}\")\r\n\r\n        else:  # Note we cannot have forward slashes within class_labels when logging.\r\n            self.log(\r\n                f\"{phase}_{metric}_{class_labels[idx]}\",\r\n                metric_value,\r\n                on_step=False,\r\n                on_epoch=True,\r\n                sync_dist=self.multi_gpu,\r\n            )\r\n```\r\n\r\nWhere the metrics are evaluated on a class wise level using.\r\n\r\n```average_acc = self.train_average_accuracy(sigmoid_outputs, y)```\r\n\r\nand the logging function can be placed within the training or validation step using\r\n\r\n```log_class_metric(self, class_metrics=class_acc, metric=\"acc\", phase=\"train\")```",
      "author": {
        "login": "deepbakes"
      },
      "createdAt": "2022-02-28T11:06:28Z"
    }
  },
  {
    "title": "AttributeError: 'Trainer' object has no attribute 'run_evaluation'",
    "body": "I am getting the below error when running `trainer.fit`:\r\n`AttributeError: 'Trainer' object has no attribute 'run_evaluation'`\r\n\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"sdr_main.py\", line 81, in <module>\r\n    main()\r\n  File \"sdr_main.py\", line 28, in main\r\n    main_train(model_class_pointer, hyperparams,parser)\r\n  File \"sdr_main.py\", line 73, in main_train\r\n    trainer.fit(model)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1319, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in on_run_start\r\n    self.trainer.call_hook(\"on_train_start\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1495, in call_hook\r\n    callback_fx(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py\", line 138, in on_train_start\r\n    callback.on_train_start(self, self.lightning_module)\r\n  File \"/content/SDR/utils/pytorch_lightning_utils/callbacks.py\", line 10, in on_train_start\r\n    return trainer.run_evaluation()\r\nAttributeError: 'Trainer' object has no attribute 'run_evaluation'\r\n```\r\n\r\nMy `Trainer` object:\r\n```\r\ntrainer = pytorch_lightning.Trainer(\r\n    num_sanity_val_steps=2,\r\n    gradient_clip_val=hparams.max_grad_norm,\r\n    callbacks=[RunValidationOnStart()],\r\n    checkpoint_callback=ModelCheckpoint(\r\n        save_top_k=3,\r\n        save_last=True,\r\n        mode=\"min\" if \"acc\" not in hparams.metric_to_track else \"max\",\r\n        monitor=hparams.metric_to_track,\r\n        dirpath=model.hparams.hparams_dir,\r\n        filename=\"{epoch}\",\r\n        verbose=True,\r\n    ),\r\n    logger=logger,\r\n    max_epochs=hparams.max_epochs,\r\n    gpus=hparams.gpus,\r\n    strategy=\"dp\",\r\n    limit_val_batches=hparams.limit_val_batches,\r\n    limit_train_batches=hparams.limit_train_batches,\r\n    limit_test_batches=hparams.limit_test_batches,\r\n    check_val_every_n_epoch=hparams.check_val_every_n_epoch,\r\n    profiler=SimpleProfiler(),\r\n    accumulate_grad_batches=hparams.accumulate_grad_batches,\r\n    reload_dataloaders_every_epoch=True,\r\n    resume_from_checkpoint=hparams.resume_from_checkpoint,\r\n)\r\n```\r\n\r\nAny idea on how to fix this? My pytorch-lightning version is 1.5.10",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12097",
    "createdAt": "2022-02-24T17:40:47Z",
    "updatedAt": "2022-06-01T13:02:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hassiahk"
    },
    "answer": {
      "body": "hey !\r\n\r\nthis was removed in the previous release.\r\n\r\nYou can try:\r\n```py\r\ntrainer.validating = True\r\ntrainer.reset_val_dataloader()\r\ntrainer.val_loop.run()\r\ntrainer.training = True\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-25T07:56:58Z"
    }
  },
  {
    "title": "Save predictions in `test_step`",
    "body": "Hey,\r\n\r\nI set up an AutoEncoder for image reconstruction using the `LightningModule` (i.e. `my_model`). At inference, I currently run `pl.Trainer.test()` to obtain my test metrics, `pl.Trainer.predict()` to obtain my image predictions, and then in a 3rd loop save each prediction obtained from the `pl.Trainer.predict()`-step. This is awfully complicated and ideally I would like to directly save predictions in `test_step()`. I have the following `test_step()`.\r\n\r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    batch_hat = self(batch)\r\n    loss_test = nn.functional.mse_loss(batch_hat, batch)\r\n    self.log(\"loss_test\", loss_test, on_step=True, on_epoch=True, sync_dist=True)\r\n    \r\n    # I want to add something like this (NOT FUNCTIONAL)\r\n    filepath = trainer.datamodule.dataset_test.my_file_names[batch_idx]\r\n    my_save_function(batch_hat, filepath)\r\n\r\n    return loss_test\r\n```\r\n\r\nand I call `test_step()` with the conventional\r\n\r\n```python\r\ntrainer.test(my_model, my_datamodule)\r\n```\r\n, where `my_datamodule` is `LightningDataModule` and `trainer` is a `pl.Trainer()`.\r\n\r\n\r\n**Question**: How can I access the name of my samples in `batch` within `test_step()` that are stored in `my_datamodule`? \r\n**Goal**: I want to re-use the file names assigned to my input images for the predictions in `test_step()`.\r\n\r\nBest,\r\ndsethz",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12066",
    "createdAt": "2022-02-23T12:56:48Z",
    "updatedAt": "2022-05-31T03:17:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dsethz"
    },
    "answer": {
      "body": "hey @dsethz !\r\n\r\nyou can return the filenames from your `Dataset.__getitem__` which will be available inside the batch and you can access them inside `test_step` easily.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-23T14:25:27Z"
    }
  },
  {
    "title": "self.local_rank in LightningDataModule",
    "body": "Hi, I was recently reading this example from NVIDIA DALI: \r\nhttps://github.com/NVIDIA/DALI/blob/629c57592b9b4e91b8213e6c77c1af179f7dd079/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb\r\n\r\nI wanted to split the model and datamodule apart. In that case, how can I get `local_rank`, `global_rank` and `world_size` for datamodule's setup?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12056",
    "createdAt": "2022-02-23T01:02:15Z",
    "updatedAt": "2022-05-31T03:59:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "austinmw"
    },
    "answer": {
      "body": "hey @austinmw !\r\n\r\nyou can access them using `self.trainer.local_rank`, `self.trainer.global_rank` & `self.trainer.world_size`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-23T08:49:08Z"
    }
  },
  {
    "title": "AttributeError: 'Trainer' object has no attribute 'lr_find'",
    "body": "      6 t = pl.Trainer(gpus=[0])\r\n----> 7 lr_finder = t.lr_find(module)\r\n\r\nWhen I try to train a model, I got an error like;\r\n\r\n**\"AttributeError: 'Trainer' object has no attribute 'lr_find' \"**\r\n\r\nHow can I fix this?\r\nThanks,",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12043",
    "createdAt": "2022-02-22T13:44:50Z",
    "updatedAt": "2023-03-24T20:41:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ozgugoksu"
    },
    "answer": {
      "body": "hey @ozgugoksu \r\n\r\nits `trainer.tuner.lr_find`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-25T07:23:00Z"
    }
  },
  {
    "title": "Loading Lightning model in PyTorch",
    "body": "How to load a model saved in PyTorch Lightning in Vanilla PyTorch?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12041",
    "createdAt": "2022-02-22T12:32:14Z",
    "updatedAt": "2022-06-21T20:20:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rahulvigneswaran"
    },
    "answer": {
      "body": "check this out: https://pytorch-lightning.readthedocs.io/en/latest/common/production_inference.html#id1",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-22T13:08:52Z"
    }
  },
  {
    "title": "Logging Images during validation using Tensorboard Logger",
    "body": "Hi,\r\n\r\nI have successfully implemented the method to log images to tensorboard logger, except I run out of GPU memory soon as I accumulate images during the whole validation_step and by end of the validation round, I randomly select few images to log. This is not the best way to do it. Can someone point me how to do it properly where I don\u2019t consume too much of Memory. Thanks.\r\n\r\nThis is how my validation step looks:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        imgs, y_true = batch\r\n        y_pred = self(imgs)\r\n        val_loss = self.nn_criterion(y_pred, y_true)\r\n        self.log(\"val_loss\", val_loss)\r\n        \r\n        return {\"val_loss\": val_loss,\r\n                \"images\": imgs,\r\n                \"masks_pred\": y_pred,\r\n                \"true_masks\": y_true}\r\n```\r\n                \r\nOne can clearly see that I am accumulating tensors over the validation step. Since I am working with very large dataset, I run out of memory very soon. Thanks in advance.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12035",
    "createdAt": "2022-02-21T16:01:39Z",
    "updatedAt": "2022-08-03T03:17:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "muaali"
    },
    "answer": {
      "body": "hey @muaali !\r\n\r\n> by end of the validation round, I randomly select few images to log\r\n\r\nyou can log the images randomly inside `validation_step` itself to avoid accumulation that is creating memory overhead.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-21T17:09:52Z"
    }
  },
  {
    "title": "Clarification on reload_dataloaders_every_epoch",
    "body": "With a basic Datamodule like:\r\n```\r\nclass MyDM(pl.lightningDataModule):\r\n    def __init__(self,):\r\n        <init some stuff>\r\n\r\n    def setup(self, stage:typing.Optional[str] = None):\r\n        .... <sort out dataset etc>\r\n\r\n   def train_dataloader(self):\r\n         ....\r\n   etc etc\r\n\r\nmodel = MyModel()\r\ndata = MyDM()\r\ntrainer pl.Trainer(reload_dataloaders_every_n_epochs=5)\r\ntrainer.fit(model, data)\r\n```\r\nDoes the flag reload_dataloaders_every_n_epochs=N cause data.setup() to be called every reload. \r\nI expect my dataset to be constantly changing and am currently assuming that I can define anything I don't expect to be changing in __init__ and anything I will need to change every N epochs in setup. A quick look at the pl.trainer source code (specifically the reset_train_dataloader method) doesn't immediately elucidate this for me.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12023",
    "createdAt": "2022-02-20T17:29:28Z",
    "updatedAt": "2022-11-03T20:07:41Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SCMusson"
    },
    "answer": {
      "body": "no, it doesn't call setup at every reload, just the corresponding `_dataloader` hook. You can define the datasets in setup and access them inside dataloader_hooks or can initialize the corresponding dataset inside dataloader_hook as well.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-21T07:58:05Z"
    }
  },
  {
    "title": "How to call torch.distributed.get_rank() in model building phase",
    "body": "I implemented pytorch lightning-based learning as follows.\r\n\r\n------------------------------------------\r\n\r\ndm = build_datamodule(config)\r\nmodel = build_model(config)\r\n\r\ntrainer = Trainer(\r\n...\r\naccelerator=\"ddp\",\r\n...\r\n)\r\n\r\ntrainer.fit(model, dm)\r\n\r\n------------------------------------------\r\n\r\nIn this situation, in order to set different model parameters for each gpu process, distributed.get_rank() must be called at the stage of model building.\r\nHowever, the trainer.fit function doesn't seem to be able to implement this because it requires an already built model.\r\nI wonder if there is any other way to do this.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12017",
    "createdAt": "2022-02-20T06:11:34Z",
    "updatedAt": "2023-10-30T07:25:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "skjung0916"
    },
    "answer": {
      "body": "Hello, you are right that this currently isn't supported. I am working on adding this feature as part of this issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/11922\n\nCould you confirm that the issue and proposed solution meet your needs?",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-02-20T16:08:48Z"
    }
  },
  {
    "title": "import pytorch_lightning fails with ModuleNotFoundError: No module named 'tensorboard'",
    "body": "### BUG\r\nI finished installing pytorch_lightnin by pip but can not import pytorch_lightning.\r\n\r\n### Environment\r\n\r\n- CUDA: 11.2\r\n- GPU: v100 * 8\r\n- Packges:\r\n-OpenCC==1.1.0\r\n-pytorch-lightning==1.1.2\r\n-six==1.14.0\r\n-tensorboard==2.4.0\r\n-tensorboard-plugin-wit==1.7.0\r\n-threadpoolctl==2.1.0\r\n-tokenizers==0.9.4\r\n-torch==1.10.2+cu113\r\n-transformers==4.1.1\r\n-yacs\r\n-lxml\r\n\r\n- Error message\r\n![image](https://user-images.githubusercontent.com/47704881/154795220-cab9b91e-73df-4c4e-b446-53e17125568a.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/12002",
    "createdAt": "2022-02-19T09:31:39Z",
    "updatedAt": "2022-06-07T15:40:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Rafael8830"
    },
    "answer": {
      "body": "@Rafael8830 Could you try reinstalling it? If the issue still persists, could you provide more info about your environment?",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-02-20T11:02:19Z"
    }
  },
  {
    "title": "About `ModelCheckpoint` starting point",
    "body": "My `ModelCheckpoint` callback:\r\n```py\r\nckpt_callback = pl.callbacks.ModelCheckpoint(\r\n    filename=\"T5-{epoch}-{step}-{val_loss:.2f}-{val_ppl:.2f}\",\r\n    monitor=\"val_loss\",\r\n    save_top_k=-1,\r\n    every_n_train_steps=100\r\n)\r\n```\r\n\r\nQuestions:\r\n1. I got the checkpoints starting with **199** step:\r\n    <img width=\"406\" alt=\"image\" src=\"https://user-images.githubusercontent.com/42370681/154623035-6eac0ab8-ecbd-466d-aa61-3cce8ff09c7f.png\">\r\n    Why does **NOT** `pl` save the checkpoint in `99` step?\r\n2. Why is step not an integer multiple of 100? Because counting starts at 0? It is kind of weird.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11977",
    "createdAt": "2022-02-18T05:33:48Z",
    "updatedAt": "2022-08-29T13:52:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ShaneTian"
    },
    "answer": {
      "body": "hey @ShaneTian !\r\n\r\n1. maybe no validation loop is triggered after 99 steps, possibly because total training steps per epoch > 100?\r\n2. yes, it is zero-indexed.\r\n",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-21T08:22:43Z"
    }
  },
  {
    "title": "How to flag certain modules as non-deterministic",
    "body": "Hey,\r\n\r\n**Question**: How can I set a module/layer in my model-class to always be non-deterministic (irrespective of the `deterministic` flag in `pl.Trainer()`)?\r\n\r\n**Context**: I use `pl` to train a simple AutoEncoder that uses bilinear upscaling in the decoder part. For debugging, I use the `deterministic` flag of the `pl.Trainer()`. However, I receive the following error\r\n\r\n```\r\nRuntimeError: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\r\n```\r\n\r\nUnfortunately, the error does not hint at how to set the module to be non-deterministic and neither does the documentation.\r\n\r\nCheers\r\ndsethz",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11963",
    "createdAt": "2022-02-17T14:07:59Z",
    "updatedAt": "2022-06-17T06:53:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dsethz"
    },
    "answer": {
      "body": "Hi @dsethz!\r\n\r\nThe error comes from PyTorch, but not from Lightning, and I think it's not (shouldn't be) feasible even in pure PyTorch because the flag is for reproducibility and if you allow randomness in certain layers, you can't reproduce the same result anymore.\r\n\r\nhttps://pytorch.org/docs/stable/notes/randomness.html",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-03-11T18:04:56Z"
    }
  },
  {
    "title": "Logging multiple scalars to a single wandb chart",
    "body": "During `fit()` I want to log multiple scalars to a _single_ chart using the wandb logger. I am trying to achieve this with this example code:\r\n\r\n`self.log('val/top-k', {'1': 0.6, '5': 0.8, '10': 0.9})`\r\n\r\nOn wandb this creates _three_ charts with the headings `val/top-k.1` and `(...).5` and `(...).10` resp.\r\n\r\nAm I doing this wrong or do I need to call the wandb logger directly (which I try to avoid to keep the code logger agnostic).\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11960",
    "createdAt": "2022-02-17T11:53:03Z",
    "updatedAt": "2022-06-21T21:48:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hogru"
    },
    "answer": {
      "body": "hey @hogru!\r\n\r\ncan you share some references to how wandb plots multiple charts? I don't think there is a direct API. Personally, I used to log them individually and combine them in a single chart afterward. But if there is an option to do that via its API, then we can look more and think about adding support for it if it's not already there.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-17T15:46:51Z"
    }
  },
  {
    "title": "trainer.fit(strategy='ddp') executes code repeatedly",
    "body": " Hi everyone.\r\n\r\nI am trying to use 4 gpus in a single node to train my model with DDP strategy. But everytime I run **trainer.fit**, the whole bunch of codes are executed 4 times repeatedly, and it requires 4 times of CPU memory compared to a single GPU case.\r\n\r\nI am not sure whether it is intended behavior or not. I ran the following sample code. It trains MNIST data on 4 gpus.\r\n\r\n    import warnings\r\n    warnings.filterwarnings(\"ignore\")\r\n    \r\n    import os\r\n    import torch\r\n    from pytorch_lightning import LightningModule, Trainer\r\n    from torch import nn\r\n    from torch.nn import functional as F\r\n    from torch.utils.data import DataLoader#, random_split\r\n    from torchvision import transforms\r\n    from torchvision.datasets import MNIST\r\n\r\n    PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\r\n\r\n    class MNISTModel(LightningModule):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n        def forward(self, x):\r\n            return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n        def training_step(self, batch, batch_nb):\r\n            x, y = batch\r\n            loss = F.cross_entropy(self(x), y)\r\n            return loss\r\n\r\n        def configure_optimizers(self):\r\n            return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\n    if __name__ == '__main__':\r\n        print('Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\r\n        mnist_model = MNISTModel()\r\n\r\n        train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\r\n        train_loader = DataLoader(train_ds, batch_size=256)\r\n\r\n        trainer = Trainer(gpus=4, strategy='ddp', max_epochs=1, replace_sampler_ddp=True, num_nodes=1)\r\n        trainer.fit(mnist_model, train_loader)\r\n\r\n\r\nAnd I got the following output:\r\n\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    GPU available: True, used: True\r\n    TPU available: False, using: 0 TPU cores\r\n    IPU available: False, using: 0 IPUs\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\n    initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\n    initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n    ----------------------------------------------------------------------------------------------------\r\n    distributed_backend=nccl\r\n    All distributed processes registered. Starting with 4 processes\r\n    ----------------------------------------------------------------------------------------------------\r\n\r\nThe training is done well, but the thing is that 'Hello world!' is printed four times. My problem here is that train data is loaded four times also and it takes four times of CPU memory. I am not sure whether it is the intended behavior or am I doing something wrong?\r\n\r\nHow do you deal with DDP if train data is too large to be copied by multiple (=gpu num) times?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11938",
    "createdAt": "2022-02-16T06:45:54Z",
    "updatedAt": "2022-06-02T02:51:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "earendil25"
    },
    "answer": {
      "body": "hey @earendil25!\r\n\r\nthis is how DDP works exactly. To populate data across devices, DistributedSampler is added to avoid data duplication on each device and the model is wrapped around DistributedDataParallel to sync gradients. The command is launched on each device individually. Alternatively, you can also try DDP_Spawn, which creates spawn processes and won't execute the whole script on each device.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-17T15:36:26Z"
    }
  },
  {
    "title": "ValueError: Expected positive integer total_steps, but got -1",
    "body": "```python\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\r\n        print(self.trainer.max_steps)\r\n        lr_scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\r\n                                                             max_lr=self.lr,\r\n                                                             total_steps=self.trainer.max_steps,\r\n                                                             anneal_strategy='linear',\r\n                                                             cycle_momentum=False,\r\n                                                             pct_start=0.1),\r\n            'interval': 'step',\r\n            'frequency': 1\r\n        }\r\n        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler, \"monitor\": 'val_acc'}\r\n```\r\n\r\n```\r\nraise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\r\nValueError: Expected positive integer total_steps, but got -1\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11936",
    "createdAt": "2022-02-16T03:30:44Z",
    "updatedAt": "2022-07-25T13:06:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "ok i know the answer,",
      "author": {
        "login": "morestart"
      },
      "createdAt": "2022-02-16T04:38:11Z"
    }
  },
  {
    "title": "AttributeError: 'Trainer' object has no attribute 'running_sanity_check\"",
    "body": "Hi,\r\n\r\nI am trying to optimise the hyperparameters of my network using raytune. My implementation is pretty much based on this: \r\nhttps://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html#selecting-a-scheduler\r\n\r\nWhen I train my network using pre-set hyperparameters, it works smoothly. The problems come from the callback, so when I add the following line:\r\n`TuneReportCallback({\"loss\":\"val_loss\"}, on=\"validation_end\")`\r\n\r\nI get the following error:\r\n![Screenshot 2022-02-15 at 17 34 23](https://user-images.githubusercontent.com/57765003/154106370-c343c64c-b255-4bb7-86a3-9fdf2293492a.png)\r\n\r\nAnyone knows how to solve this??\r\nI don't think the problem is with my code as I haven't done anything different compared to the tutorial!\r\n\r\nMy code can be found here:\r\nhttps://github.com/annalauralerede/anomaly-detection/blob/main/lstmae_pl_opt.py",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11926",
    "createdAt": "2022-02-15T16:35:21Z",
    "updatedAt": "2022-06-03T13:00:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "annalauralerede"
    },
    "answer": {
      "body": "I think it dues to this : https://github.com/ray-project/ray/issues/20741\r\n\r\nAs of ray[tune]==1.10.0, either downgrade your pytorch-lightning to 1.4.\r\nOr upgrade your raytune to be compatible with pytorch-lightning 1.5+ ( the fix has been merged in this commit https://github.com/ray-project/ray/pull/20562).\r\n`$ sudo pip install ray==1.11.0rc0`\r\n\r\n",
      "author": {
        "login": "pvmilk"
      },
      "createdAt": "2022-02-17T13:11:50Z"
    }
  },
  {
    "title": "Validation step: error when trying to return object",
    "body": "Hi,\r\n\r\nAs part of my _validation_step_ in a AE I am trying to return an object containing input and reconstructed output.\r\n\r\n_validation_epoch_end_ does not get the accumulated outputs.\r\n\r\nHere's the code:\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        x, x_recon, _, _, _ = self.forward(batch)\r\n        outputs = {\r\n            'x': x,\r\n            'x_recon': x_recon\r\n        }\r\n        return outputs\r\n```\r\n\r\nthese are the shapes:\r\n`x.shape:  torch.Size([2, 1, 257, 63])`\r\n`x_recon.shape:  torch.Size([2, 1, 257, 63])`\r\n\r\nand this is what _validation_step_outputs_ looks like:\r\n`[{'x': tensor(0.0033, device='cuda:0'), 'x_recon': tensor(-0.0102, device='cuda:0')}]`\r\n\r\nAny idea why?\r\n\r\nThanks\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11916",
    "createdAt": "2022-02-14T15:05:30Z",
    "updatedAt": "2022-06-11T18:48:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mcomunita"
    },
    "answer": {
      "body": "hey @mcomunita!\r\n\r\nthis shouldn't happen. Quick ques: are you using DP?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-17T16:15:49Z"
    }
  },
  {
    "title": "Odd Performance Using Multi-GPU + Azure",
    "body": "I was wondering if anyone has observed odd performance when training multi-GPU models? I\u2019ve developed a script which trains a toy dataset (in this case the cats and dogs model), using a ResNet or EfficientNet. The script works fine locally on the GPU. However, when I move the script to the cloud and train using multiple GPUs strange things start to happen. The script trains fine on the cloud using 1 GPU, albeit slow as I was testing using a M60. However, if I run the same script on 4x K80 with ddp I find that the training process is around ~15% slower (which I\u2019m guessing is the difference between K80 and M60).\r\n\r\n![2022-02-13 11_23_24-Window](https://user-images.githubusercontent.com/98221950/153755313-7fee130e-315f-4e5d-96ac-578960bdd5a2.png)\r\n\r\nI checked GPU usage and the GPUs are all being used. However, model performance seems slower/worse than using just one GPU. Any ideas why this could be?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11905",
    "createdAt": "2022-02-13T13:27:26Z",
    "updatedAt": "2022-08-01T21:16:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "deepbakes"
    },
    "answer": {
      "body": "@deepbakes Could this be because of `benchmark=True` ?",
      "author": {
        "login": "rahulvigneswaran"
      },
      "createdAt": "2022-02-13T16:10:16Z"
    }
  },
  {
    "title": "lowest val/loss ckpt != highest val/Accuracy",
    "body": "Am using the following callback,\r\n```python\r\n checkpoint_callback = ModelCheckpoint(monitor='val/loss',\r\n                                        mode='min',\r\n                                        save_last=True,\r\n                                        filename=cfg.CALLBACKS.FILENAME,\r\n                                        auto_insert_metric_name=cfg.CALLBACKS.AUTO_INSERT_METRIC_NAME,\r\n                                        dirpath=LOGGER_DIR)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/37276766/153702380-cb8af624-4df0-4165-bcda-f27385bbb0aa.png)\r\n\r\nI am not sure what is going wrong. Am using `F.cross_entropy` for loss.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11896",
    "createdAt": "2022-02-12T07:51:13Z",
    "updatedAt": "2022-06-15T13:32:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rahulvigneswaran"
    },
    "answer": {
      "body": "- Lowest loss (say CE) will not always give you the highest accuracy. (https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy)\r\n- So, always use the accuracy (or desired metric at hand) to select the best model instead of loss.\r\n\r\n(This is probably common knowledge but this was news for me. Putting it out there so someone else doesn't bang their head for two days!)",
      "author": {
        "login": "rahulvigneswaran"
      },
      "createdAt": "2022-02-13T13:11:41Z"
    }
  },
  {
    "title": "Code not printing values in trained_epoch_end",
    "body": "My code is not printing `print('train acc loss',acc,loss)` in `trained_epoch_end` but its printing `print('val acc loss',acc,loss)` in `validation_epoch_end` \r\n```\r\nclass Model(LightningModule):\r\n  def __init__(self):\r\n    super(Model,self).__init__()\r\n    self.model=ResneT(21)\r\n    self.lr=1e-3\r\n    self.bs=128\r\n    self.worker=6\r\n    self.acc=torchmetrics.Accuracy()\r\n    self.creterion=nn.BCEWithLogitsLoss()\r\n    self.scheduler='lambda'\r\n  def forward(self,x):\r\n    x=self.model(x)\r\n    return x\r\n\r\n  def configure_optimizers(self):\r\n    opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\r\n    return opt\r\n\r\n  def train_dataloader(self):\r\n    dataset=DataReader(train_df)\r\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=True,\r\n                         pin_memory=True,collate_fn=collate_fn)\r\n    return dataloader\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    signal,label=batch\r\n    out=self(signal.float())\r\n    loss=self.creterion(out.flatten(),label.float().flatten())\r\n    acc=self.acc(out.flatten(),label.long().flatten())\r\n    return {'loss':loss,'acc':acc}\r\n\r\n  def trained_epoch_end(self,outputs):\r\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    self.log('train acc',acc)\r\n    self.log('train loss',loss)\r\n    print('train acc loss',acc,loss)\r\n\r\n  def val_dataloader(self):\r\n    dataset=DataReader(val_df)\r\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=False,\r\n                          pin_memory=True,\r\n                         collate_fn=collate_fn)\r\n    return dataloader\r\n\r\n  def validation_step(self,batch,batch_idx):\r\n    signal,label=batch\r\n    out=self(signal.float())\r\n    loss=self.creterion(out.flatten(),label.float().flatten())\r\n    acc=self.acc(out.flatten(),label.long().flatten())\r\n    return {'loss':loss,'acc':acc}\r\n\r\n  def validation_epoch_end(self,outputs):\r\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    print('val acc loss',self.current_epoch,acc,loss)\r\n    self.log('val acc',acc)\r\n    self.log('val loss',loss)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11851",
    "createdAt": "2022-02-10T19:14:03Z",
    "updatedAt": "2022-07-05T14:25:50Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "You need to implement `training_epoch_end`, not `trained_epoch_end`: https://github.com/PyTorchLightning/pytorch-lightning/blob/1515ef90ee2724bcba46e1434eb4b4f9719ebdd7/pytorch_lightning/core/lightning.py#L689",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-02-10T20:33:20Z"
    }
  },
  {
    "title": "Using Multiple Optimisers gives Index Error?",
    "body": "Hello,\r\nI am trying to build a model which uses multiple optimisers. When I try to train the model I get the error `validation_step() missing 1 required positional argument: 'optimizer_idx'`. I have reproduced this error on the BoringModel used for bug reports:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"train_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"valid_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"valid_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def test_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"test_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"test_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        opt_a = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        opt_b = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        return [opt_a, opt_b], []\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nWhat am I missing?\r\n\r\nThanks for the help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11846",
    "createdAt": "2022-02-10T15:33:10Z",
    "updatedAt": "2022-06-14T07:44:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "TheSeparatrix"
    },
    "answer": {
      "body": "Validation doesn't require optimizers. Try removing the \n`optimizer_idx` argument from your `validation_step` method definition ",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2022-02-10T15:41:59Z"
    }
  },
  {
    "title": "When doing `fit()`, `self.training` in `forward()` keeps turning into False?",
    "body": "Hi all, I tried to train a model with pl. And I just ran the `trainer.fit()` as below:\r\n```python\r\ntrainer.fit(model, train_dataloaders=model.train_dataloader(),\r\n                val_dataloaders=model.val_dataloader())\r\n```\r\nAnd I found that `model.training == False`, when it gets into `forward()`...\r\nIs there any solution or does anybody know the potential reason for this?\r\nOr does anyone know where can I find the source code for `training_step()` so that I can check and debug `forward()` in `fit()`?\r\n\r\nThank you very much.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11838",
    "createdAt": "2022-02-10T09:24:19Z",
    "updatedAt": "2022-06-25T14:01:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "HaFred"
    },
    "answer": {
      "body": "does it print `self.training = False` for all the training steps? maybe you might have checked it during the initial steps where val sanity check happens.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-10T09:47:36Z"
    }
  },
  {
    "title": "Remove parameters from autograd backward hook",
    "body": "Hello,\r\n\r\nI am trying to remove some layers from `DistributedDataParallel` to prevent them being synchronized between devices.\r\n\r\nI spent last 6 hours googling, and I have found out, that there's a attribute `_ddp_params_and_buffers_to_ignore` which can be set to module that is passed to `DistributedDataParallel` constructor. I've implemented custom strategy plugin to `Trainer`, I have checked that the parameters are then passed to a `parameters_to_ignore` attribute of the `DistributedDataParallel` but somehow if I check gradients, of the layer, they are always the same.\r\n\r\nIs there some simpler way to remove some layer / module from being synchronized between more devices in DDP strategy?\r\n\r\nThank you in advance for any help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11835",
    "createdAt": "2022-02-10T03:31:11Z",
    "updatedAt": "2022-06-23T12:28:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Honzys"
    },
    "answer": {
      "body": "Okay, It was my mistake, I deeply apologize for wasting your time there. The layer indeeds gets removed from the DistributedDataParallel (or rather not even getting there).\r\n\r\nBut I've found another error when trying to set the `_ddp_params_and_buffers_to_ignore` inside the `LightningModule`, so I've created issue here - https://github.com/PyTorchLightning/pytorch-lightning/issues/11844 .\r\n\r\nThank you anyway!",
      "author": {
        "login": "Honzys"
      },
      "createdAt": "2022-02-10T15:04:08Z"
    }
  },
  {
    "title": "Early Stopping Callback restore_best_weights",
    "body": "Hello there!\r\nWhen I do early stopping, I want to use the best model subsequently.\r\nReasonable Early Stopping Callbacks therefore should have the `restore_best_weights` parameter (like in Keras etc.).\r\nWhy is this missing from the lightning ES callback? What is the default behavior? If the default is not to restore the weights, how to do it in lightning?\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11787",
    "createdAt": "2022-02-07T11:58:26Z",
    "updatedAt": "2023-05-26T09:21:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "PascalIversen"
    },
    "answer": {
      "body": "Hi,\r\n\r\nlook at [ModelCheckpoint](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.ModelCheckpoint.html) callback. After the training, you can use its attribute `best_model_path` to restore the best model.\r\n\r\nHere is a simple code with EarlyStopping and ModelCheckpoint together (training is stopped when `val_loss` doesn't improve anymore).\r\n\r\n```Python\r\n# preparing my dataset\r\ndm = MyDataModule()\r\ndm.prepare_data()\r\ndm.setup()\r\n\r\n# initialize checkpoints\r\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\r\ncheckpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\r\n\r\n# initialize the trainer\r\ntrainer = pl.Trainer(\r\n    callbacks=[early_stop, checkpoint_callback],   # we use both checkpoints\r\n    max_epochs=100,\r\n    gpus=[0],\r\n    enable_checkpointing=True,\r\n)\r\n\r\n# fit the model\r\ntrainer.fit(\r\n    model,\r\n    train_dataloaders=dm.train_dataloader(),\r\n    val_dataloaders=dm.val_dataloader(),\r\n)\r\n\r\nprint(checkpoint_callback.best_model_path)   # prints path to the best model's checkpoint\r\nprint(checkpoint_callback.best_model_score) # and prints it score\r\nbest_model = MyModel.load_from_checkpoint(checkpoint_callback.best_model_path)\r\n\r\n# test only the best model\r\ntrainer.test(model=best_model, dataloaders=dm.val_dataloader())\r\n```",
      "author": {
        "login": "mpicek"
      },
      "createdAt": "2022-04-08T19:41:38Z"
    }
  },
  {
    "title": "Is it okay to feed optimizer to `configure_optimizers`",
    "body": "Hi all, is it okay to feed the optimizer that's been initialized outside this code to `pl.LightningModule`?\r\n```\r\ndef Model(pl.LightningModule):\r\n    def __init__(optimizer):\r\n        self.optimizer = optimizer\r\n    def configure_optimizers(self) -> Any:                                                                                      \r\n        optimizer = self.optimizer          # like this                                                                                    \r\n        scheduler = {                                                                                                               \r\n            'scheduler': LambdaLR(optimizer, self.lr_lambda),                                                                       \r\n            'interval': 'step',                                                                                                     \r\n            'frequency': 1,   \r\n        }                                                                                                                                                                                                                   \r\n    return [optimizer], [scheduler]\r\n```\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11783",
    "createdAt": "2022-02-07T11:01:43Z",
    "updatedAt": "2022-06-03T08:07:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sophia1488"
    },
    "answer": {
      "body": "yes, I think you can, but not a good practice, we recommend.\r\n\r\njust curious, why are you feeding it like that?\r\n",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-07T13:05:23Z"
    }
  },
  {
    "title": "Saving checkpoint, hparams & tfevents after training to separate folder",
    "body": "Thanks to **all the contributors of PyTorch Lightning** for a fantastic product! \r\n\r\nI want to save a checkpoint, hparams & tfevents after training finishes. I have written this callback:\r\n\r\n```python\r\nclass AfterTrainCheckpoint(pl.Callback):\r\n    \"\"\"\r\n    Callback for saving the checkpoint weights, hparams and tf.events after training finishes\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        print(f\"Saving final checkpoint...\")\r\n        # As we advance one step at end of training, we use `global_step - 1`\r\n        final_checkpoint_name = f\"final_models/final_step_{trainer.global_step - 1}.ckpt\"\r\n        final_hparams_name = f\"final_models/final_step_{trainer.global_step - 1}.yaml\"\r\n\r\n        trainer.save_checkpoint(final_checkpoint_name)\r\n        save_hparams_to_yaml(config_yaml=final_hparams_name, hparams=trainer.model.hparams)\r\n```\r\n\r\n1. Is this the best 'Lightning' way to achieve this?\r\n2. How can I save the final `events.out.tfevents` file to a new directory?\r\n3. Should I be setting `save_last=True`? seen in `ModelCheckpoint` in [the docs](https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/callbacks/model_checkpoint.html#ModelCheckpoint). I am slightly confused about \"monitor metrics logged during training/validation steps or end of epochs are not guaranteed to be available at this stage.\"",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11779",
    "createdAt": "2022-02-07T09:48:20Z",
    "updatedAt": "2022-06-09T15:09:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dispoth"
    },
    "answer": {
      "body": "hey @dispoth !!\r\n\r\n1. I'd say use `on_fit_end` instead, since the last checkpoint in the model checkpoint is saved in this hook, so it won't guarantee to have that ckpt when your callback calls it.\r\n2. you can copy the log files directly? the are available inside `trainer.log_dir`.\r\n3. yes, they will be available during both `on_train_end` and `on_fit_end`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-07T10:13:44Z"
    }
  },
  {
    "title": "Multi-GPU Tensor Initialization Question",
    "body": "The documentation advises the usage of '_type_as_' when initializing new tensors in multi-gpu settings:\r\n\r\n> [https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#init-tensors-using-type-as-and-register-buffer](url)\r\nWhen you need to create a new tensor, use type_as. This will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.\r\n\r\nThe example shows a case where a new tensor is initialized inside a LightningModule forward function:\r\n\r\n> def forward(self, x):\r\n         z = torch.Tensor(2, 3)\r\n         z = z.type_as(x)\r\n\r\nPresumably _x_ is a tensor that has already been initialized on the target gpu.\r\n\r\nMy question is what to do in the case where we want to initialize a new tensor on the target gpu, and we **do not** have access to a tensor that has already been initialized on the target gpu? \r\n\r\nFor example, how does one properly initialize a new tensor when it is created inside a Dataset constructor that is instantiated during LightningDataModule _setup()_?\r\n\r\n> class SomeDataModule(LightningDataModule):\r\n...\r\n    def setup(self, stage: Optional[str] = None):\r\n        if stage in (None, \"fit\"):\r\n            dataset = SomeDataset()\r\n...\r\n\r\nwhere:\r\n\r\n> class SomeDataset(Dataset):\r\n    def __init__(self):\r\n        self.some_tensor = torch.Tensor(2,3)\r\n\r\nWill using _type_as_ on the new tensor initialize the data on the target gpu?\r\n\r\n> self.some_tensor = self.some_tensor.type_as(self.some_tensor)\r\n\r\nOr is a different approach necessary? (e.g. [register_buffer()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer))\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11774",
    "createdAt": "2022-02-06T21:22:50Z",
    "updatedAt": "2022-06-10T16:58:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "xsys-technology"
    },
    "answer": {
      "body": "if it's part of the dataset, it's already moved to the target device when a batch is created while iterating over the dataset.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-06T21:59:58Z"
    }
  },
  {
    "title": "Confusion about NeptuneLogger.save_dir implementation",
    "body": "In the docstring it says the save_dir is None, but then why does it return a path? Should we change either the docstring, or the implementation here?\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/9d8faecdb2b873b52b95f2772f4bf48068a0af9a/pytorch_lightning/loggers/neptune.py#L516-L524",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11766",
    "createdAt": "2022-02-06T01:44:15Z",
    "updatedAt": "2022-06-10T19:01:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "daniellepintz"
    },
    "answer": {
      "body": "Hi @daniellepintz\r\n\r\nPrince Canuma here, a Data Scientist at Neptune.ai\r\n\r\nI will let the engineering team know about this,\r\n\r\nBy default, Neptune will create a '.neptune' folder inside the current working directory. In the case of https://github.com/PyTorchLightning/pytorch-lightning/pull/6867 it changes the model checkpoint path to be '.neptune' folder in case the user doesn't define his own path using ModelCheckpointCallback() for example.\r\nCheck this commit: https://github.com/PyTorchLightning/pytorch-lightning/commit/5ac80ece048c16f19e9c05b1873a08fe0a37be90\r\n\r\n> But still, a bit confused because I thought Neptune doesn't save anything locally.\r\n\r\nNeptune uses the '.neptune' folder to store metadata temporarily. For example, you track a run in offline mode or there is a network connectivity issue in which case neptune also automatically switches to offline mode and save the data to disk. Later you can synchronize the locally stored metadata with the servers using the neptune sync CLI command.\r\n\r\nDocs:\r\n\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune-sync",
      "author": {
        "login": "Blaizzy"
      },
      "createdAt": "2022-03-01T16:05:35Z"
    }
  },
  {
    "title": "CacheDataset with DDP and Multi-GPUs",
    "body": "We use CacheDataset [MONAI CacheDataset](https://docs.monai.io/en/stable/data.html#monai.data.CacheDataset) to speed up data loading. However, when combining the lightning module's standard training code with DDP strategy and multi-GPU environment, the cached dataset is not working as expected:\r\n\r\nIf provided with a full length of data in the CacheDataset, the initial epoch takes forever to load because each GPU will try to read in and cache ALL data, which is unnecessary because in DDP each GPU will only use a portion of the data.\r\n\r\nA workaround is mentioned in here [MONAI issue](https://github.com/Project-MONAI/MONAI/issues/1589), which mentioning to partition data before feeding into the CacheDataset:\r\n[MONAI Tutorial](https://github.com/Project-MONAI/tutorials/blob/master/acceleration/distributed_training/unet_training_smartcache.py#L120)\r\n\r\nHowever, if I make the partitioning in the [setup()](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#setup) function, the trainer will train for total_data_length // num_gpus samples each epoch instead of total_ data_length.\r\n\r\nAnd if I put the CacheDataset with full data length in the prepare_data function, the subprocess's object can't access the dataset instance (saved in self.x, which is not recommended).\r\n\r\nSo what's the best practical way to handle this? My gut feeling is that I should use the partitioned dataset on each GPU, and let the loader use the full length of dataset instead of part of it. Any suggestions?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11763",
    "createdAt": "2022-02-05T19:45:35Z",
    "updatedAt": "2024-01-24T06:20:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bill-yc-chen"
    },
    "answer": {
      "body": "hey @bill-yc-chen \r\n\r\nsince DDP executes scripts independently across devices, maybe try DDP_Spawn instead?\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html#sharing-datasets-across-process-boundaries",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-05T21:54:57Z"
    }
  },
  {
    "title": "Iterating over task for Continual Learning.",
    "body": "Hi everyone, I am new to PyTorch lightening and I am currently trying to implement a continual learning model in PyTorch lightening. \r\n\r\nI have multiple data loaders for different tasks and I want to train on all of these data loaders. After training on task1 with dataloader1 I want to update the parameters of the model which are going to be trained for task two. To do this, I have an attribute named current_task in my dataloader which decides the dataset from which the samples are generated for the current task. My datamodule looks something like this.\r\n```python\r\n\r\nclass RandSplitCIFAR100DataModule(LightningDataModule):\r\n    def __init__(self):\r\n        .....\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n    \r\n        # load datasets only if they're not loaded already\r\n        if not self.data_train and not self.data_val and not self.data_test:\r\n            self.data_train = datasets.CIFAR100(self.hparams.data_dir, train=True, transform=self.train_transforms)\r\n            self.data_val = datasets.CIFAR100(self.hparams.data_dir, train=False, transform=self.val_transforms)\r\n        \r\n        np.random.seed(self.hparams.seed)\r\n        perm = np.random.permutation(self.num_classes)\r\n        print(perm)\r\n\r\n        splits = [\r\n            (self.partition_datasetv4(self.data_train, perm[5 * i:5 * (i+1)]),\r\n            self.partition_datasetv4(self.data_val, perm[5 * i:5 * (i+1)]),)\r\n            for i in range(self.hparams.num_tasks)\r\n        ]\r\n\r\n        kwargs = {\"num_workers\": self.hparams.workers, \"pin_memory\": self.hparams.pin_memory}\r\n        self.loaders = [\r\n            (DataLoader(x[0], batch_size=self.hparams.batch_size, shuffle=True, **kwargs),\r\n            DataLoader(x[1], batch_size=self.hparams.test_batch_size, shuffle=False, **kwargs),)\r\n            for x in splits\r\n        ]\r\n\r\n    def update_task(self, i):\r\n        self.current_task = i\r\n        \r\n    def train_dataloader(self):\r\n        return self.loader[self.current_task][0]\r\n\r\n    def val_dataloader(self):\r\n        return self.loader[self.current_task][1]\r\n```\r\n\r\nNow I want to have a training loop that does something like this. \r\n```python\r\n\r\nfor task in range(num_tasks):\r\n    self.dataloder.update_task(task)\r\n\r\n    for n, p in model.named_parameters():\r\n        # change parameters to update\r\n    for epoch in range(max_epochs):\r\n        for batch in dataloader:\r\n            ....\r\n\r\n```\r\n\r\nI am currently not able to figure out how to go about this, I feel confident that lightening should be able to handle such cases but I am just not sure how to go about this. \r\n\r\nAny help is greatly appreciated!\r\nPrateek",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11724",
    "createdAt": "2022-02-03T17:06:53Z",
    "updatedAt": "2022-06-03T00:32:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "prateeky2806"
    },
    "answer": {
      "body": "well, there are multiple ways:\r\n\r\n1. if your max_epochs is consistent across all the tasks:\r\n```py\r\n\r\nclass LitModel(LightningModule):\r\n    def on_train_epoch_start(self):\r\n        if current_epoch == 0 or (current_epoch + 1) % self.trainer.reload_dataloaders_every_n_epochs == 0:\r\n            # update model parameters\r\n\r\n\r\nmax_epochs_n_tasks = max_epochs * n_tasks\r\ntrainer = Trainer(max_epochs=max_epochs_n_tasks, reload_dataloaders_every_n_epochs=max_epochs)\r\nmodel = LitModel()\r\n\r\n# inject the update task counter logic inside datamodule\r\ndm = RandSplitCIFAR100DataModule(...)\r\ntrainer.fit(model, datamodule=dm)\r\n```\r\n2. create an explicit loop\r\n\r\n```py\r\ndef init_trainer(...):\r\n    trainer = Trainer(max_epochs=max_epochs, ...)\r\n    return trainer\r\n    \r\ndatamodule = ...\r\nmodel = ...\r\nfor task in range(num_tasks):\r\n    # update params\r\n    datamodule.update_task(task)\r\n    trainer = init_trainer(...)\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nAlthough I'd suggest (1), even if your max_epochs differs for each task, it can easily be extended to support that too.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-03T18:01:11Z"
    }
  },
  {
    "title": "Enabling dropout during trainer.predict",
    "body": "I want to enable dropout during `.predict` and tried implementing the following:\r\n\r\n```\r\nmodel.eval() \r\nfor m in model.modules():\r\n    if m.__class__.__name__.startswith('Dropout'):\r\n        m.train()\r\n                \r\n...\r\n\r\ntrainer.predict(\r\n    model, dataloaders=data_loader, return_predictions=True\r\n)\r\n```\r\n\r\nIt seems like `.predict` is overriding this because I get identical predictions with different seeds. \r\n\r\nCan someone explain how to accomplish this, or point me to the relevant docs? (Couldn't find them & tried looking for while) \r\n\r\nThank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11710",
    "createdAt": "2022-02-03T01:14:30Z",
    "updatedAt": "2022-06-01T08:22:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "standard-aaron"
    },
    "answer": {
      "body": "hey @35ajstern !\r\n\r\nyou can enable this inside `predict_step` itself. Check this out: https://github.com/PyTorchLightning/pytorch-lightning/blob/f35e2210e240b443fd4dafed8fe2e30ee7d579ea/docs/source/common/production_inference.rst#prediction-api\r\n\r\nthis is part of a PR, will be available in the docs once merged.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-03T10:01:39Z"
    }
  },
  {
    "title": "Hook for Fully Formed Checkpoints",
    "body": "I want to create a hook that uploads checkpoints to cloud storage (e.g. AWS, Azure). I tried using the `on_save_checkpoint` hook as follows:\r\n\r\n```\r\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\r\n    checkpoint_bytes = io.BytesIO()\r\n    torch.save(checkpoint, checkpoint_bytes)\r\n    # Upload the BytesIO...\r\n```\r\n\r\nHowever, states for optimizers, learning rate schedulers, etc. are added to the checkpoint dict after `on_save_checkpoint` is called. Is there an elegant way to create a hook that receives **fully formed checkpoints**?\r\n\r\nedit: sorry, this is a duplicate -- GitHub was giving me errors when I posted",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11705",
    "createdAt": "2022-02-02T19:16:12Z",
    "updatedAt": "2022-06-19T11:19:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dcharatan"
    },
    "answer": {
      "body": "See #11704.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-02-03T00:27:51Z"
    }
  },
  {
    "title": "Hook for Fully Formed Checkpoints",
    "body": "I would like to create a hook that automatically uploads checkpoints to the cloud (e.g., AWS, Azure) when they're created. I tried using `on_save_checkpoint` roughly like this:\r\n\r\n```\r\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\r\n    checkpoint_bytes = io.BytesIO()\r\n    torch.save(checkpoint, checkpoint_bytes)\r\n    # Upload the BytesIO somewhere...\r\n```\r\n\r\nHowever, states for optimizers, schedulers, AMP, etc. are added after `on_save_checkpoint` hooks are called. Is there an elegant way to create a hook that receives the **fully formed checkpoint state**?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11704",
    "createdAt": "2022-02-02T19:11:46Z",
    "updatedAt": "2022-07-04T09:05:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dcharatan"
    },
    "answer": {
      "body": "hey @dcharatan !\r\n\r\nI'd rather suggest using the [remote filesystems](https://pytorch-lightning.readthedocs.io/en/latest/common/remote_fs.html#remote-filesystems). You can also specify the remote path inside `ModelCheckpoint`.\r\n\r\nor use [CheckpointIO](https://pytorch-lightning.readthedocs.io/en/latest/common/checkpointing.html#custom-checkpoint-io-plugin) plugin.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-02T19:39:51Z"
    }
  },
  {
    "title": "Gradient accumulation + DeepSpeed LR scheduler",
    "body": "How does gradient accumulation interact with DeepSpeed learning rate scheduling (e.g. the per-step warm-up scheduler)? Is the learning rate updated after every iteration, or only after the model weights are ultimately updated?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11686",
    "createdAt": "2022-02-01T05:24:27Z",
    "updatedAt": "2022-07-26T03:57:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "gahdritz"
    },
    "answer": {
      "body": "it considers the accumulation before doing lr_scheduler_step:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/loops/epoch/training_epoch_loop.py#L387-L390",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-01T10:30:17Z"
    }
  },
  {
    "title": "Can't import `tests.helpers.boring_model.BoringModelBoringModel` in colab",
    "body": "I am trying to import the BoringModel in this colab https://colab.research.google.com/drive/1wrPzif6zddJvdDgMnYsa05172Nv3WzWk?usp=sharing but am getting the error `ModuleNotFoundError: No module named 'tests.helpers'`\r\n\r\nAny ideas how to fix this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11681",
    "createdAt": "2022-01-31T22:20:32Z",
    "updatedAt": "2022-08-03T03:00:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "daniellepintz"
    },
    "answer": {
      "body": "with pip install it installs just the core-package (`pytorch_lightning`) and not any other directories. Since tests in this env will be just another package, not specific to pytorch_lightning.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-02-01T10:43:32Z"
    }
  },
  {
    "title": "Question about the log system",
    "body": "Hello, I have some question about the self.log function and batch_size during the trainer.\r\nIf I have two GPUs, and I want to train my model with batch_size 16 per GPU and I use DDP, so what's the number of batch_size in Datamodule and what's the number of batch_size in self.log, If I want to calculate my metrics correctly?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11670",
    "createdAt": "2022-01-31T10:38:50Z",
    "updatedAt": "2022-10-25T14:29:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "exiawsh"
    },
    "answer": {
      "body": "hey @exiawsh \r\n\r\nit should stay as batch_size for a single device only. With DDP if you set `batch_size=7`, then each device gets the batch of `batch_size=7`, and effective batch_size increases with the number of devices. Now if you want to log by accumulating metrics across devices, you need to set `sync_dist=True`. Check out the section here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-31T11:06:28Z"
    }
  },
  {
    "title": "Get batch\u2019s datapoints across all GPUs",
    "body": "Hello,\r\n\r\nI\u00b4m running my model in a cluster with multiples GPUs (2). My problem is that I would like to access all the datapoints in the batch (predictions and labels). Because I\u00b4m using more than 2 GPUs, my batch in divided between those two devices for parallelisation purposes, which means than when I access the data in the batch in eval/training, I\u00b4m getting just half the batch.\r\n\r\nHow could I obtain the complete batch and the predictions of the model that are divided among different devices/GPUs? @rohitgr7 suggested using self.all_gather, but after trying it on my LightningModule\u2019s forward method, I get just half the batch, that is, just the data stored in one of the two GPUs being used.\r\n\r\nThanks!\r\n\r\nPD:  may it be possible to access this info through \"validation_epoch_end\", \"test_epoch_end\", etc?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11667",
    "createdAt": "2022-01-31T10:06:46Z",
    "updatedAt": "2022-07-29T03:54:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "fmorenopino"
    },
    "answer": {
      "body": "if you are going to use the complete batch on the single GPU, then why use DDP?\r\n\r\nif you need predictions on a single device, you can rather gather all the predictions using `all_gather`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-31T10:36:19Z"
    }
  },
  {
    "title": "Disabling find_unused_parameters",
    "body": "When trying to disable `find_unused_parameters` in the trainer by doing the following, \r\n\r\n`strategy=DDPStrategy(find_unused_parameters=False)`\r\n\r\nAm being thrown an import error for `from pytorch_lightning.strategies import DDPStrategy`\r\n\r\nError: \r\n\r\n`No module named 'pytorch_lightning.strategies'`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11664",
    "createdAt": "2022-01-30T15:25:13Z",
    "updatedAt": "2023-03-20T07:51:23Z",
    "closedAt": "2023-03-20T07:51:23Z",
    "isAnswered": true,
    "author": {
      "login": "rahulvigneswaran"
    },
    "answer": {
      "body": "`pytorch_lightning.strategies` will be available in v1.6 release and is only available in master at the moment.\r\n\r\nFor now, you can use:\r\n```\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\ntrainer = pl.Trainer(\r\n    ...,\r\n    strategy=DDPPlugin(find_unused_parameters=False),\r\n)\r\n```\r\n\r\nSee the stable version of docs (not latest) here: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html?highlight=find_unused_parameters#when-using-ddp-plugins-set-find-unused-parameters-false",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-01-30T16:08:17Z"
    }
  },
  {
    "title": "How to access validation step outputs of complete epoch in a `on_validation_epoch_end` hook for a custom callback ?",
    "body": "I want to implement a custom callback which calculates a custom metric and needs all of the outputs from the complete epoch. Is there any way to pass all the outputs to `on_validation_epoch_end` hook of the callback ?\r\n\r\nHere's the pseudo-code of the setup\r\n\r\n```python\r\nclass FeedBackPrize(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        num_train_steps,\r\n        steps_per_epoch,\r\n        model_name: str = \"allenai/longformer-base-4096\",\r\n        lr: float = 1e-5,\r\n        num_labels: int = 16,\r\n        multi_sample_dropout=True,\r\n        step_scheduler_after: str = \"step\",\r\n    ):\r\n        super().__init__()\r\n        self.learning_rate = lr\r\n        self.model_name = model_name\r\n        self.multi_sample_dropout = multi_sample_dropout\r\n        self.num_train_steps = num_train_steps\r\n        self.num_labels = num_labels\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.step_scheduler_after = step_scheduler_after\r\n\r\n        hidden_dropout_prob: float = 0.1\r\n        layer_norm_eps: float = 1e-7\r\n\r\n        config = AutoConfig.from_pretrained(model_name)\r\n\r\n        config.update(\r\n            {\r\n                \"output_hidden_states\": True,\r\n                \"hidden_dropout_prob\": hidden_dropout_prob,\r\n                \"layer_norm_eps\": layer_norm_eps,\r\n                \"add_pooling_layer\": False,\r\n                \"num_labels\": self.num_labels,\r\n            }\r\n        )\r\n\r\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.dropout1 = nn.Dropout(0.1)\r\n        self.dropout2 = nn.Dropout(0.2)\r\n        self.dropout3 = nn.Dropout(0.3)\r\n        self.dropout4 = nn.Dropout(0.4)\r\n        self.dropout5 = nn.Dropout(0.5)\r\n\r\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\r\n\r\n    def forward(self, ids, mask, token_type_ids=None):\r\n        transformer_out = self.transformer(ids, mask)\r\n        sequence_output = transformer_out.last_hidden_state\r\n        sequence_output = self.dropout(sequence_output)\r\n\r\n        if self.multi_sample_dropout:\r\n            logits1 = self.output(self.dropout1(sequence_output))\r\n            logits2 = self.output(self.dropout2(sequence_output))\r\n            logits3 = self.output(self.dropout3(sequence_output))\r\n            logits4 = self.output(self.dropout4(sequence_output))\r\n            logits5 = self.output(self.dropout5(sequence_output))\r\n\r\n            logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\r\n            logits = torch.softmax(logits, dim=-1)\r\n            return logits\r\n        else:\r\n            return sequence_output\r\n\r\n    def configure_optimizers(self):\r\n        param_optimizer = list(self.named_parameters())\r\n        no_decay = [\"bias\", \"LayerNorm.bias\"]\r\n        optimizer_parameters = [\r\n            {\r\n                \"params\": [\r\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\r\n                ],\r\n                \"weight_decay\": 0.01,\r\n            },\r\n            {\r\n                \"params\": [\r\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\r\n                ],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n        ]\r\n        optimizer = AdamW(optimizer_parameters, lr=self.learning_rate)\r\n\r\n        scheduler = get_cosine_schedule_with_warmup(\r\n            optimizer,\r\n            num_warmup_steps=int(0.1 * self.num_train_steps),\r\n            num_training_steps=self.num_train_steps,\r\n            num_cycles=1,\r\n            last_epoch=-1,\r\n        )\r\n        scheduler = {\r\n            \"scheduler\": scheduler,\r\n            \"interval\": self.step_scheduler_after,\r\n            \"frequency\": 1,\r\n        }\r\n\r\n        return [optimizer], [scheduler]\r\n\r\n    def _calculate_loss(self, outputs, targets, attention_mask):\r\n        loss_fct = nn.CrossEntropyLoss()\r\n\r\n        active_loss = attention_mask.view(-1) == 1\r\n        active_logits = outputs.view(-1, self.num_labels)\r\n        true_labels = targets.view(-1)\r\n        outputs = active_logits.argmax(dim=-1)\r\n        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\r\n        active_logits = active_logits[idxs]\r\n        true_labels = true_labels[idxs].to(torch.long)\r\n\r\n        loss = loss_fct(active_logits, true_labels)\r\n\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\r\n        outputs = self(ids, mask)\r\n        loss = self._calculate_loss(outputs, targets, mask)\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\r\n        outputs = self(ids, mask)\r\n        loss = self._calculate_loss(outputs, targets, mask)\r\n\r\n        return {\r\n            \"loss\": loss,\r\n            \"preds\": outputs,\r\n            \"targets\": targets\r\n        }\r\n    \r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in validation_step_outputs:\r\n            preds += output['preds']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets) #torch.Size([2, 1536])\r\n        preds = torch.stack(preds) # torch.Size([2, 1536, 15])\r\n\r\n        return {\r\n            \"targets\": targets,\r\n            \"preds\": preds\r\n        }\r\n```\r\n\r\n### Custom callback\r\n```python\r\nclass CompMetricEvaluator(Callback):\r\n    def __init__(self):\r\n        pass\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n\r\n        print(\"After validation epoch [custom metric evaluation]\")\r\n        # calculate custom metric here....\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11659",
    "createdAt": "2022-01-29T13:06:22Z",
    "updatedAt": "2022-09-06T19:20:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Gladiator07"
    },
    "answer": {
      "body": "hey @Gladiator07!\r\n\r\nyou can either override `on_validation_batch_end` hook and cache the outputs in some variable of the callback use that.\r\n\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(self):\r\n        self.val_outs = []\r\n    def on_validation_batch_end(self, trainer, pl_module, outputs, ...):\r\n        self.val_outs.append(outputs)\r\n\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        self.val_outs  # <- access them here\r\n```\r\nor cache the val outputs in pl_module inside `validation_epoch_end`\r\n```py\r\nclass LitModel(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n       new_outputs = ...\r\n       self.val_outs = new_outputs\r\n\r\n\r\nclass CustomCallback(Callback):\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        pl_module.val_outs  # <- access them here\r\n```\r\nnote that the trainer and pl_module passed inside callbacks are passed by reference so that ever changes in the original lightningmodule will reflect in this referred instance here too.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-30T19:31:05Z"
    }
  },
  {
    "title": "Link arguments from Datamodule into init_args of lr_scheduler",
    "body": "Hey!\r\n\r\nI'm trying to use `LightningArgumentParser.link_arguments` to link an argument from the Datamodule to the init_args of the LR scheduler with:\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\nand I get the following error:\r\n```python\r\nValueError: No action for key \"lr_scheduler.init_args.num_training_steps\".\r\n```\r\n\r\nI was wondering if such thing is possible, or is linking to `init_args` is exclusive to the model and data classes?\r\n\r\n<details>\r\n  <summary><b>Code to reproduce</b></summary>\r\n\r\n  `trainer.py`\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch.nn\r\nfrom pytorch_lightning.utilities.cli import LR_SCHEDULER_REGISTRY\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom torch.optim import Optimizer\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n\r\n\r\n@LR_SCHEDULER_REGISTRY\r\nclass WarmupLR(LambdaLR):\r\n    def __init__(\r\n        self,\r\n        optimizer: Optimizer,\r\n        warmup_proportion: float,\r\n        num_training_steps: int,\r\n        last_epoch=-1,\r\n    ) -> None:\r\n        self.num_training_steps = num_training_steps\r\n        self.num_warmup_steps = round(num_training_steps * warmup_proportion)\r\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=last_epoch)\r\n\r\n    def lr_lambda(self, current_step: int) -> float:\r\n        if current_step < self.num_warmup_steps:\r\n            return float(current_step) / float(max(1, self.num_warmup_steps))\r\n        return max(\r\n            0.0,\r\n            float(self.num_training_steps - current_step)\r\n            / float(max(1, self.num_training_steps - self.num_warmup_steps)),\r\n        )\r\n\r\n\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, name):\r\n        super().__init__()\r\n        self.length = len(name)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(Dataset())\r\n\r\n    def get_num_training_steps(self) -> int:\r\n        return self.length\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n    def __init__(self, num_labels):\r\n        super().__init__()\r\n        self.num_labels = num_labels\r\n        self.nn = torch.nn.Linear(num_labels, num_labels)\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli = MyLightningCLI(\r\n        model_class=LitModel,\r\n        datamodule_class=DataModule,\r\n    )\r\n```\r\n\r\n`config.yaml`\r\n```yaml\r\ndata:\r\n  name: blablabla\r\n\r\nmodel:\r\n  num_labels: 5\r\n\r\noptimizer:\r\n  class_path: torch.optim.Adam\r\n  init_args:\r\n    lr: 0.01\r\n\r\nlr_scheduler:\r\n  warmup_proportion: 0.1\r\n\r\ntrainer:\r\n  max_epochs: 2\r\n```\r\n</details>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11628",
    "createdAt": "2022-01-26T12:05:23Z",
    "updatedAt": "2023-11-20T16:27:00Z",
    "closedAt": "2023-11-20T16:27:00Z",
    "isAnswered": true,
    "author": {
      "login": "LourencoVazPato"
    },
    "answer": {
      "body": "You need to add an empty `configure_optimizers` method to your model as there's a bug that disallows leaving it unimplemented. It will be fixed with #11672 \r\n\r\nThe error appears because the `lr_scheduler` arguments have not been added yet. You can see the order here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/utilities/cli.py#L603-L609\r\n\r\nSo you have two options:\r\n\r\n1. Delay the linking until we've automatically added the lr scheduler classes\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def link_optimizers_and_lr_schedulers(parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n        LightningCLI.link_optimizers_and_lr_schedulers(parser)\r\n```\r\n\r\n2. Manually add the classes yourself at this hook:\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Manually add the lr scheduler classes\r\n        parser.add_lr_scheduler_args(LR_SCHEDULER_REGISTRY.classes)\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\n\r\nAlso, the config for the scheduler should be:\r\n\r\n```yaml\r\nlr_scheduler:\r\n  class_path: __main__.WarmupLR\r\n  init_args:\r\n    warmup_proportion: 0.01\r\n    num_training_steps: 1\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-01-31T16:30:14Z"
    }
  },
  {
    "title": "\"resume from checkpoint\" lead to CUDA out of memory",
    "body": "When I use \u201cresume from checkpoint\u201d, \r\nthere is a \u201cCUDA out of memory\u201d problem, \r\nwhen using torch.load(), set \"map location\" to \"cpu\" can solve this problem, \r\nin \"resume from checkpoint\" scenario, what should I do?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11563",
    "createdAt": "2022-01-21T04:26:35Z",
    "updatedAt": "2022-08-10T06:06:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Defiler24"
    },
    "answer": {
      "body": "I solved the problem after setting the strategy to 'ddp'.",
      "author": {
        "login": "Defiler24"
      },
      "createdAt": "2022-01-24T08:04:32Z"
    }
  },
  {
    "title": "How to change optimizer and lr scheduler in the middle of training",
    "body": "I need to train a model multi-phases with a pre-trained backbone.\r\n\r\nFor the first 10 epoch, I want to have the backbone frozen and train the classifier only. After epoch 10, I want to start training the whole network. After a certain point (e.g. 100 epochs), I need to enable certain blocks in the network. In regular PyTorch, I would instantiate a new optimizer adding the backbone params, additional required blocks params that I want to train. Then I\u2019d swap both optimizer and lr_scheduler.\r\n\r\nI know I can make a multi `trainer.fit()`. But, what\u2019s the recommended way to do something like this in PL in a callback like BaseFinetuninig?\r\n\r\nHere\u2019s my sample code in a callback\r\n\r\n```python\r\n    def freeze_before_training(self, pl_module):\r\n        # Here, we are freezing `backbone`\r\n        self.freeze(pl_module.net.encoder)\r\n        if not pl_module.shared_weights:\r\n            self.freeze(pl_module.net.left_encoder)\r\n            self.freeze(pl_module.net.right_encoder)\r\n\r\n    def on_train_start(self, trainer, pl_module) -> None:\r\n\r\n        if trainer.current_epoch == self._unfreeze_at_epoch:\r\n            print(\"unfreeze and add param group...\")\r\n            pl_module.net.freeze_backbone(False)\r\n            new_optimizer = optim.Adam(\r\n                filter(\r\n                    lambda p: p.requires_grad,\r\n                    pl_module.net.parameters()),\r\n                lr=pl_module.lr,\r\n                weight_decay=pl_module.weight_decay)\r\n            new_schedulers = optim.lr_scheduler.ReduceLROnPlateau(\r\n                new_optimizer,\r\n                mode=\"min\",\r\n                factor=0.1,\r\n                patience=pl_module.scheduler_patience,\r\n                cooldown=3,\r\n            )\r\n            # not sure if its correct or safe to do this\r\n            trainer.optimizers = [new_optimizer]\r\n            trainer.lr_schedulers = [new_schedulers]\r\n    if not pl_module.shared_weights and current_epoch == self._enable_left_view_at_epoch:\r\n        # do the same process\r\n        # unfreeze, and change opt and scheduler\r\n    if not pl_module.shared_weights and current_epoch == self._enable_right_view_at_epoch:\r\n       # do the same process\r\n       # unfreeze, and change opt and scheduler\r\n   # and more conditions and blocks\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11508",
    "createdAt": "2022-01-17T12:26:23Z",
    "updatedAt": "2022-08-14T13:35:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "icedpanda"
    },
    "answer": {
      "body": "@icedpanda Lightning supports multiple optimizers. You can define multiple optimizers and LR_schedules in LightningModule.configure_optimizers(), for example:\r\n```\r\nclass yourModule(LightningModule):\r\n    def __init_(..):\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = torch.optim.Adam(params, lr)\r\n        optimizer2 = torch.optim.Adam(params, lr)\r\n        lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1)\r\n        lr_scheduler2 = torch.optim.lr_scheduler. ExponentialLR(optimizer2)\r\n        return [optimizer1, optimizer2], [lr_scheduler1, lr_scheduler2]\r\n\r\n   def training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            # do training_step when freeze\r\n            ...\r\n        if optimizer_idx == 1:\r\n            # do training_step when unfreeze\r\n            ...\r\n```",
      "author": {
        "login": "four4fish"
      },
      "createdAt": "2022-01-21T19:27:38Z"
    }
  },
  {
    "title": "WGAN - discriminator and generator updates inconsistency",
    "body": "Hi,\r\n\r\nWhen training a WGAN we update the discriminator several times for each update of the generator (a typical choice is 5 to 1).\r\n\r\nIn PL we control this by setting the \"frequency\" parameter within the _configure_optimizers_ function:\r\n`return (\r\n        {'optimizer': dis_opt, 'frequency': 5},\r\n        {'optimizer': gen_opt, 'frequency': 1}\r\n    )\r\n`\r\n\r\nNow, if the number of batches for each epoch is not divisible by the sum of frequencies (6 in this case), the generator will end up being trained less than the discriminator.\r\n\r\nIf, for example, there are 11 batches in our dataset, it will result in the discriminator being updated 10 times and the generator only 1 for each epoch because the optimizers' order is reset at the beginning of each epoch.\r\n\r\nIs there a workaround for this? The most useful solution would be to be able to save the number of updates across epochs.\r\n\r\nThanks for any suggestion.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11500",
    "createdAt": "2022-01-15T20:20:20Z",
    "updatedAt": "2023-03-02T15:16:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mcomunita"
    },
    "answer": {
      "body": "Solved using `limit_train_batches` parameter in Trainer.",
      "author": {
        "login": "mcomunita"
      },
      "createdAt": "2022-01-15T20:43:18Z"
    }
  },
  {
    "title": "Access a registered buffer is very slow",
    "body": "Hello,\r\n\r\nI implemented MoCo in Pytorch lightning. I was surprised to see that my lightning version was slower than Pytorch's and I ran the profiler to check which function is slow. I can't share all my code but here are the relevant parts:\r\n\r\n``` python\r\nclass MoCoModel(LightningModule):\r\n    def __init__(\r\n        ...\r\n    ) -> None:\r\n        ...\r\n\r\n        self.register_buffer('queue', torch.randn(queue.feature_dim, queue.size))\r\n        self.queue = nn.functional.normalize(self.queue, dim=0)\r\n        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\r\n    \r\n    @torch.no_grad()\r\n    def _update_queue(self, x: Tensor) -> None:\r\n        x = self.concat_all_gather_without_backprop(x)\r\n\r\n        #batch_size = x.shape[0]\r\n        batch_size = self._get_batch_size(x)\r\n\r\n        # for simplicity\r\n        ptr = self._get_ptr()\r\n        #ptr = int(self.queue_ptr)\r\n\r\n        self._assert(batch_size)\r\n        #assert self.queue_size % batch_size == 0\r\n\r\n        # replace the keys at ptr (dequeue and enqueue)\r\n        x = self._transpose(x)\r\n        self._assign_in_queue(x, ptr, batch_size)\r\n        #self.queue[:, ptr: ptr + batch_size] = x.T\r\n\r\n        # move pointer\r\n        ptr = self._compute_ptr(ptr, batch_size)\r\n        self._assign_ptr(ptr)\r\n        #ptr =  (ptr + batch_size) % self.queue_size\r\n    \r\n    def _get_batch_size(self, x):\r\n        return x.shape[0]\r\n\r\n    def _get_ptr(self):\r\n        return int(self.queue_ptr)\r\n\r\n    def _assert(self, batch_size):\r\n        assert self.queue_size % batch_size == 0\r\n    \r\n    def _assign_ptr(self, ptr):\r\n        self.queue_ptr[0] = ptr\r\n    \r\n    def _compute_ptr(self, batch_size, ptr):\r\n        return (ptr + batch_size) % self.queue_size\r\n\r\n    def _transpose(self, x):\r\n        return x.T\r\n    \r\n    def _assign_in_queue(self, x, ptr, batch_size):\r\n        self.queue[:, ptr: ptr + batch_size] = x\r\n\r\n    def training_step(self, batch):\r\n        ...\r\n        self._update_queue(k)\r\n```\r\n\r\nHere is the output of running simple profiler:\r\n\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  53.595         \t|  100 %          \t|\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                 \t|  45.224         \t|1              \t|  45.224         \t|  84.381         \t|\r\nrun_training_batch                 \t|  0.21673        \t|195            \t|  42.262         \t|  78.854         \t|\r\noptimizer_step_with_closure_0      \t|  0.20378        \t|195            \t|  39.738         \t|  74.145         \t|\r\ntraining_step_and_backward         \t|  0.19978        \t|195            \t|  38.957         \t|  72.688         \t|\r\nmodel_forward                      \t|  0.1909         \t|195            \t|  37.225         \t|  69.457         \t|\r\ntraining_step                      \t|  0.19077        \t|195            \t|  37.201         \t|  69.411         \t|\r\nbackward                           \t|  0.0083673      \t|195            \t|  1.6316         \t|  3.0443         \t|\r\non_train_batch_end                 \t|  0.0077772      \t|195            \t|  1.5166         \t|  2.8296         \t|\r\nget_train_batch                    \t|  0.0034326      \t|196            \t|  0.6728         \t|  1.2553         \t|\r\nfetch_next_train_batch             \t|  0.0034203      \t|196            \t|  0.67037        \t|  1.2508         \t|\r\nzero_grad                          \t|  0.00049274     \t|195            \t|  0.096084       \t|  0.17928        \t|\r\nconfigure_optimizers               \t|  0.093719       \t|1              \t|  0.093719       \t|  0.17486        \t|\r\ntraining_batch_to_device           \t|  0.00028381     \t|195            \t|  0.055342       \t|  0.10326        \t|\r\non_train_batch_start               \t|  0.00018134     \t|195            \t|  0.03536        \t|  0.065977       \t|\r\non_train_start                     \t|  0.033906       \t|1              \t|  0.033906       \t|  0.063264       \t|\r\non_pretrain_routine_start          \t|  0.006531       \t|1              \t|  0.006531       \t|  0.012186       \t|\r\non_batch_start                     \t|  3.062e-05      \t|195            \t|  0.0059708      \t|  0.011141       \t|\r\non_after_backward                  \t|  3.0163e-05     \t|195            \t|  0.0058817      \t|  0.010974       \t|\r\non_before_optimizer_step           \t|  2.989e-05      \t|195            \t|  0.0058285      \t|  0.010875       \t|\r\non_batch_end                       \t|  2.9087e-05     \t|195            \t|  0.005672       \t|  0.010583       \t|\r\non_before_zero_grad                \t|  2.8804e-05     \t|195            \t|  0.0056167      \t|  0.01048        \t|\r\non_before_backward                 \t|  2.6982e-05     \t|195            \t|  0.0052616      \t|  0.0098172      \t|\r\non_train_epoch_end                 \t|  0.0014064      \t|1              \t|  0.0014064      \t|  0.0026241      \t|\r\ntraining_step_end                  \t|  4.9198e-06     \t|195            \t|  0.00095937     \t|  0.00179        \t|\r\non_train_epoch_start               \t|  0.00025167     \t|1              \t|  0.00025167     \t|  0.00046957     \t|\r\non_train_end                       \t|  0.00017067     \t|1              \t|  0.00017067     \t|  0.00031844     \t|\r\non_before_accelerator_backend_setup\t|  6.968e-05      \t|1              \t|  6.968e-05      \t|  0.00013001     \t|\r\nsetup                              \t|  5.0209e-05     \t|1              \t|  5.0209e-05     \t|  9.3682e-05     \t|\r\nprepare_data                       \t|  4.4779e-05     \t|1              \t|  4.4779e-05     \t|  8.355e-05      \t|\r\non_fit_end                         \t|  3.892e-05      \t|1              \t|  3.892e-05      \t|  7.2618e-05     \t|\r\non_epoch_start                     \t|  3.332e-05      \t|1              \t|  3.332e-05      \t|  6.2169e-05     \t|\r\non_pretrain_routine_end            \t|  3.009e-05      \t|1              \t|  3.009e-05      \t|  5.6143e-05     \t|\r\non_epoch_end                       \t|  2.741e-05      \t|1              \t|  2.741e-05      \t|  5.1142e-05     \t|\r\non_configure_sharded_model         \t|  2.556e-05      \t|1              \t|  2.556e-05      \t|  4.7691e-05     \t|\r\non_fit_start                       \t|  2.0869e-05     \t|1              \t|  2.0869e-05     \t|  3.8938e-05     \t|\r\nteardown                           \t|  1.9379e-05     \t|1              \t|  1.9379e-05     \t|  3.6158e-05     \t|\r\nconfigure_sharded_model            \t|  6.5197e-06     \t|1              \t|  6.5197e-06     \t|  1.2165e-05     \t|\r\nconfigure_callbacks                \t|  5.16e-06       \t|1              \t|  5.16e-06       \t|  9.6277e-06     \t|\r\non_train_dataloader                \t|  4.2003e-06     \t|1              \t|  4.2003e-06     \t|  7.837e-06      \t|\r\n```\r\n\r\nAs we can see a large time is spent in `training_step` and here is the output of advanced profiler for this function:\r\n```\r\nProfile stats for: training_step rank: 0\r\n         1065072 function calls (862519 primitive calls) in 37.086 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n      195    0.001    0.000   37.082    0.190 accelerator.py:210(training_step)\r\n      195    0.000    0.000   37.079    0.190 ddp.py:438(training_step)\r\n31980/195    0.053    0.000   37.079    0.190 module.py:1096(_call_impl)\r\n      195    0.015    0.000   37.078    0.190 distributed.py:852(forward)\r\n      195    0.002    0.000   36.629    0.188 base.py:76(forward)\r\n      195    0.009    0.000   36.624    0.188 moco.py:201(training_step)\r\n 1170/390    0.006    0.000   34.429    0.088 grad_mode.py:25(decorate_context)\r\n      195    0.002    0.000   32.216    0.165 moco.py:83(_update_queue)\r\n      195   32.171    0.165   32.171    0.165 moco.py:109(_get_ptr)\r\n      390    0.000    0.000    3.942    0.010 resnet.py:268(forward)\r\n     ...\r\n      195    0.008    0.000    0.008    0.000 moco.py:124(_assign_in_queue)\r\n      195    0.005    0.000    0.006    0.000 moco.py:115(_assign_ptr)\r\n      195    0.002    0.000    0.002    0.000 moco.py:121(_transpose)\r\n      195    0.001    0.000    0.001    0.000 gather.py:44(concat_all_gather_without_backprop)\r\n      195    0.000    0.000    0.000    0.000 moco.py:106(_get_batch_size)\r\n     ...\r\n```\r\n\r\nThe function `_update_queue` is very long and the function taking the most time is `_get_ptr` which should be really fast in comparison with forwards or computation of MoCo loss. I watched [lightning bolts implementation](https://github.com/PyTorchLightning/Lightning-Bolts/blob/master/pl_bolts/models/self_supervised/moco/moco2_module.py#L38-L328) that uses the same kind of operations so I don't really understand why it is this slow.\r\n\r\nI tested with DDP and SingleDevice strategy that resulted in the same kind of slow down on a SLURM cluster environment.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11493",
    "createdAt": "2022-01-15T11:01:11Z",
    "updatedAt": "2022-12-02T13:14:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "juliendenize"
    },
    "answer": {
      "body": "Fixed it, lightning is now as fast as my previous implementation, the problem was elsewhere but I didn't detect it using the profiler because of the asynchronous computation from GPUs which were not synchronized during profiling.",
      "author": {
        "login": "juliendenize"
      },
      "createdAt": "2022-01-20T08:30:45Z"
    }
  },
  {
    "title": "LightningCLI: how to configure logger using cmd-line args?",
    "body": "I would like to change the names of the logging directories from the default \"version_{n}\" to something of my own choosing. How can I do this using command-line arguments to LightningCLI?\r\n\r\nI know I can set the logger using `trainer.logger` but setting logger args e.g. `trainer.logger.version` does not work (unrecognized argument). So how can I pass args to the logger?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11491",
    "createdAt": "2022-01-15T10:43:35Z",
    "updatedAt": "2022-08-17T14:26:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "slinnarsson"
    },
    "answer": {
      "body": "See my reply here: https://github.com/PyTorchLightning/pytorch-lightning/issues/10574#issuecomment-1015864152\r\n\r\nWe'll be adding support for shorthand notation shortly too: #11533 ",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-01-18T23:13:44Z"
    }
  },
  {
    "title": "trainer.test( ) not working",
    "body": "I have used the below code\r\n1. Dataclass below\r\n![carbon (2)](https://user-images.githubusercontent.com/56019599/149141526-b9944c92-fa36-43c7-a93b-2dc78f0ec643.png)\r\n\r\n2. Model Class below\r\n![carbon (3)](https://user-images.githubusercontent.com/56019599/149141568-dce360de-68bf-4f27-b2eb-5f2fc4195bba.png)\r\n\r\n3. Trainnig below\r\n![carbon (1)](https://user-images.githubusercontent.com/56019599/149140420-1cc753ac-f9df-4f80-b8c2-2b944f43d1a2.png)\r\n\r\nI have trained the model for 20 epochs, After that trainer. test( ) didn't run. I Got the Following error lines :\r\n\r\n![Screenshot from 2022-01-12 17-47-57](https://user-images.githubusercontent.com/56019599/149139274-35e32fb8-72c6-41f6-a7f8-0ff2a0bfb53a.png)\r\n![Screenshot from 2022-01-12 17-48-21](https://user-images.githubusercontent.com/56019599/149139306-848c9b49-3aa1-49a3-b937-f3433fa785a5.png)\r\n\r\nI have also tried the way suggested in Doc [here](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#test) But still, the error exists. This happening mostly in Google collab and in AwsInstance. In Local, It works sometimes and sometimes not. \r\nPlease let me know what am I missing here.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11437",
    "createdAt": "2022-01-12T12:38:41Z",
    "updatedAt": "2023-08-25T13:52:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "purnasai-soulpageit"
    },
    "answer": {
      "body": "hey @purnasai-soulpageit !\r\n\r\nyou need to pass in the datamodule to `trainer.test`.\r\n```\r\ntrainer.test(datamodule=data_module)\r\n```\r\nsince lightning no longer patches the data_modules passed during `.fit` you need to pass it there too.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-12T14:29:18Z"
    }
  },
  {
    "title": "Unable to load pretrained weight into custom model in Pytorch Lightning",
    "body": "I have created an issue on this. Moderators, please delete this discussion",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11419",
    "createdAt": "2022-01-11T12:30:22Z",
    "updatedAt": "2022-01-11T13:00:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JohnDuke259"
    },
    "answer": {
      "body": "Will be discussed in #11420.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-01-11T13:00:40Z"
    }
  },
  {
    "title": "Gradient Clipping with mix precision in case of NaN loss",
    "body": "Greetings. I am getting NaN val loss `Cannot log infinite or NaN value to attribute training/val_loss`/ with cnnlstm network.I am thinking to use gradient clipping.\r\nBut the doc say gradient clipping should not be used with mixed precision.\r\n```\r\nIf using mixed precision, the gradient_clip_val does not need to be changed as the gradients are unscaled before applying the clipping function.\r\n```\r\nFurther, i am doing regression, i  dont know what value of gradient clipping should i use?\r\n\r\nFurther i checked trainer doc and find that\r\n```\r\ntrack_grad_norm\r\n(Union[int, float, str]) \u2013 -1 no tracking. Otherwise tracks that p-norm. May be set to \u2018inf\u2019 infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\r\n```\r\nCan you explain what this mean 'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11413",
    "createdAt": "2022-01-11T08:33:40Z",
    "updatedAt": "2023-07-11T09:30:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "> But the doc say gradient clipping should not be used with mixed precision.\r\n\r\nYou totally can, that's saying that any scaling applied by 16bit precision training will be undone before clipping the gradients.\r\n\r\nWhich means you do not need to worry about changing the gradient clipping value with vs without `precision=16`\r\n\r\n>  i dont know what value of gradient clipping should i use?\r\n\r\nNobody does :P\r\nTry some experiments and find out!\r\n\r\n>  'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'\r\n\r\nSame thing as I explained above. It's just a technical detail, you do not need to worry about it\r\n",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-01-12T03:24:17Z"
    }
  },
  {
    "title": "How to show the validation loss in progress bar?",
    "body": "Hi.\r\n\r\nI'm trying to come up with ways to get my validation loss shown in the progress bar. My model is defined like this:\r\n\r\n```\r\nclass DummyNet(pl.LightningModule):\r\n    def __init__(self, batch_size):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n        self.fc = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Linear(512, 2)\r\n        )\r\n        # loss\r\n        self.loss_fn = nn.CrossEntropyLoss()\r\n        # metrics\r\n        metrics = torchmetrics.MetricCollection(\r\n            {\r\n                \"accuracy\": torchmetrics.Accuracy(),\r\n                \"precision\": torchmetrics.Precision(),\r\n                \"recall\": torchmetrics.Recall(),\r\n                \"auc\": torchmetrics.AUC(reorder=True),\r\n            }, \r\n        )\r\n        self.f1 = nn.ModuleDict({\r\n            \"train_f1\": torchmetrics.F1(),\r\n            \"val_f1\": torchmetrics.F1(),\r\n            \"test_f1\": torchmetrics.F1(),\r\n        })\r\n        self.metrics = nn.ModuleDict({\r\n            f\"{k}_metrics\": metrics.clone(prefix=k) for k in \"train val test\".split()\r\n        })\r\n    def forward(self, x):\r\n        x = self.fc(x)\r\n        return x\r\n    \r\n    def loop_step(self, batch, stage):\r\n        x, targets = batch[\"windows\"], batch[\"diagnosis\"]\r\n        logits = self(x)\r\n        loss = self.loss_fn(logits, targets)\r\n        preds = logits.argmax(-1)\r\n        # computing metrics\r\n        f1_str = f\"{stage}_f1\"\r\n        metric_str = f\"{stage}_metrics\"\r\n        self.f1[f1_str](preds, targets)\r\n        self.metrics[metric_str](preds, targets)\r\n        # logging metrics\r\n        on_step = False if stage != \"train\" else True\r\n        self.log(f1_str, self.f1[f1_str], on_step=on_step, on_epoch=True)\r\n        self.log_dict(self.metrics[metric_str], on_step=False, on_epoch=True)   \r\n        self.log(f\"{stage}_loss\", loss, on_step=on_step, on_epoch=True)\r\n        return loss\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"train\")\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"val\")\r\n\r\n    def testing_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"test\")\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)\r\n```\r\n\r\nBut as of now none of my metrics nor my validation loss comes up in the progress bar. Is it because I'm returning `loss` in the dictionary and not \"{stage}_loss\"?\r\n\r\nThank you.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11412",
    "createdAt": "2022-01-11T06:19:07Z",
    "updatedAt": "2022-06-01T15:38:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "FeryET"
    },
    "answer": {
      "body": "Hi @FeryET, I believe the below should work as documented in https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging.\r\n```python\r\nself.log(..., prog_bar=True)\r\n```\r\n",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-01-11T07:31:44Z"
    }
  },
  {
    "title": "val_dataloader` has `shuffle=True` though its false",
    "body": "I am working with [pytorchvideo](https://github.com/facebookresearch/pytorchvideo) with pytorch lightning but it say ```UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.```\r\n```\r\nfrom pytorchvideo.models.resnet import create_resnet\r\nclass OurModel(LightningModule):\r\n    def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        self.model =  create_resnet(\r\n                      input_channel=3, # RGB input from Kinetics\r\n                      model_depth=50, # For the tutorial let's just use a 50 layer network\r\n                      model_num_class=1, # Kinetics has 400 classes so we need out final head to align\r\n                      norm=nn.BatchNorm3d,\r\n                      activation=nn.ReLU)\r\n\r\n    def forward(self,x):\r\n        return self.model(x)\r\n\r\n  \r\n    def val_dataloader(self):\r\n        val_dataset=LabeledVideoDataset(labeled_video_paths=\\\r\n                    list(zip(val_df.vidpath,val_df.pulse)),\r\n                   clip_sampler=make_clip_sampler(\"uniform\", 2),\\\r\n                    transform=train_transform,  decode_audio=False)\r\n        \r\n        val_loader=DataLoader(val_dataset,shuffle=False,\r\n                   batch_size=self.batch_size,\r\n                   num_workers=self.numworker,\r\n                   pin_memory=True)\r\n        return val_loader\r\n    \r\n    def validation_step(self,batch,batch_idx):\r\n        out = self(batch[\"video\"]).flatten()\r\n        loss=self.criterion(out,batch[\"label\"].float())\r\n        mae=self.metric(out,batch[\"label\"].float())\r\n        return {'loss':loss,'mae':mae.detach()}\r\n\r\n```\r\nA you can see, shuffle is False. but it keep me giving warning that\r\n```\r\nUserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\r\n```\r\nSorry, i am not sure whether i had to ask it at pytorchvideo or here",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11392",
    "createdAt": "2022-01-10T10:38:24Z",
    "updatedAt": "2022-06-24T18:17:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "Hi @talhaanwarch, you're asking it in the right place!\r\n\r\nIt's a warning from Lightning, and as I looked at the definition of `make_clip_sampler` of pytorchvideo, I believe it's the same reason as https://github.com/PyTorchLightning/pytorch-lightning/discussions/10771. You can simply ignore it with some filter like below if you need.\r\n```python\r\nimport warnings\r\nwarnings.simplefilter('ignore', category=UserWarning, message=\"Your `val_dataloader` has `shuffle=True`.*\")\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-01-10T14:32:47Z"
    }
  },
  {
    "title": "what's the difference between `load_from_checkpoint ` and `resume_from_checkpoint`",
    "body": "I'm confused about two API:\r\n- `Module.load_from_checkpoint`\r\n- `trainer.resume_from_checkpoint`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11378",
    "createdAt": "2022-01-09T14:21:00Z",
    "updatedAt": "2022-06-13T12:52:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "marsggbo"
    },
    "answer": {
      "body": "[resume_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#resume-from-checkpoint) is used to resume the training using the checkpointed state_dicts. It will reload model's state_dict, optmizer's and schedulers's state_dicts, training state as well in a general case.\r\nuse-case: to restart the training\r\n\r\n[load_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html#checkpoint-loading) just reloads the model's state_dict and return the model with the loaded weights.\r\nuse-case: for quick evaluation/prediction.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-09T18:05:42Z"
    }
  },
  {
    "title": "PyTorch Lightning Optimizer_Step() prevents training_step() from running",
    "body": "Hello,\r\n\r\nI had an error where the training_step() was not run properly. I just found out the cause was because of the optimizer_step(). My training step immediately runs after I commented out optimizer_step().\r\n\r\nSome other users also experienced the same error as described here: https://stackoverflow.com/questions/66756245/training-step-not-executing-in-pytorch-lightning\r\n\r\nMy question is: Now that training_step() is running, but my train_loss is explosive because of the lack of a learning rate scheduler, henceforth, what can I implement to re-enable back my learning rate scheduler?\r\n\r\nHere's my chunk of code:\r\n```\r\n    def configure_optimizers(self):\r\n        \r\n        \"\"\"\r\n        AdamW Optimizer lr=0.0006\r\n\r\n        \"\"\"        \r\n        optimizer = optim.AdamW(self.parameters(),\r\n                               lr=self.lr,\r\n                               weight_decay=0.01 # Default\r\n                               )\r\n        \r\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n            optimizer,\r\n            mode='min',\r\n            factor=0.1,\r\n            patience=2,\r\n            min_lr=1e-6,\r\n            verbose=True\r\n        )\r\n\r\n        return optimizer\r\n    \r\n\r\n        \r\n    #def optimizer_step(self, epoch=None,\r\n    #                   batch_idx=None,\r\n    #                   optimizer=None,\r\n    #                   optimizer_idx=None,\r\n    #                   optimizer_closure=None,\r\n    #                   on_tpu=None,\r\n    #                   using_native_amp=None,\r\n    #                   using_lbfgs=None,\r\n    #                   second_order_closure=None):              \r\n    #    \r\n    #    if batch_idx == 0: # to call the scheduler after each validation\r\n    #        \r\n    #        self.scheduler.step(self.epoch_val_loss)\r\n    #        \r\n    #        print(f'metric: {self.epoch_val_loss}, \\\r\n    #              best: {self.scheduler.best}, \\\r\n    #                  num_bad_epochs: {self.scheduler.num_bad_epochs}') # for debugging\r\n    #    \r\n    #    optimizer.step()\r\n    #    \r\n    #    optimizer.zero_grad()\r\n```\r\n\r\nThank you!\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11358",
    "createdAt": "2022-01-07T01:07:53Z",
    "updatedAt": "2022-06-29T07:38:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tsuijenk"
    },
    "answer": {
      "body": "hey @tsuijenk \r\n\r\n`optimizer _closure` must be passed in `optimizer.step()` since it runs training_step and backward call. You can check the docstrings and examples here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#optimizer-step",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-07T10:31:53Z"
    }
  },
  {
    "title": "Why is on_before_optimizer_step incompatible with accumulate_grad_batches ?",
    "body": "The documentation about the `on_before_optimizer_step` hook mentions that:\r\n\r\n> The hook is only called if gradients do not need to be accumulated.\r\n\r\nHowever, in theory, combining both seems possible to me. \r\nI took a look at PR #8048 which is the implementation of this hook but it didn't seem clear there.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11331",
    "createdAt": "2022-01-05T15:01:17Z",
    "updatedAt": "2022-06-21T20:03:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "NathanGodey"
    },
    "answer": {
      "body": "hey @NathanGodey !\r\n\r\nit doesn't mean that it won't be called if `accumulate_grad_batches > 1`, but only be called when accumulation is done and it's time to update the gradients with `optimizer.step`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-05T18:35:43Z"
    }
  },
  {
    "title": "self.manual_backward() vs. loss.backward() when optimizing manually",
    "body": "According to the manual_backward() documentation, it takes care of scaling when using mixed precision. In that case, is it correct to assume one can simply and safely use loss.backward() during manual optimization if not using mixed precision?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11318",
    "createdAt": "2022-01-04T20:58:02Z",
    "updatedAt": "2022-06-13T03:32:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MGheini"
    },
    "answer": {
      "body": "hey @MGheini \r\n\r\nIt's not just precision but a common hook to support all other strategies like deepspeed/ddp and certain hooks like `on_after_backward` are called too. So `manual_backward` is suggested to make sure no-code change is required for eg in case any of the strategies is updated by the user.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-04T22:40:30Z"
    }
  },
  {
    "title": "Where to transform and inverse-transform",
    "body": "Hi!\r\n\r\nI\u2019m working on a LSTM to predict price changes. The data has to be transformed (standardized) when training/validering and later inverse-transformed when predicting in production.\r\n\r\nI\u2019m using the LightningModule as well as the LightingDataModule, but I\u2019m not sure where to apply the StandardScaler\u2019s transform and more specifically; where to save the scaler-parameters and where to apply the inverse-transform on the predictions. And ideeas?\r\n\r\n// R",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11297",
    "createdAt": "2022-01-03T13:25:52Z",
    "updatedAt": "2023-09-18T12:16:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "aurany"
    },
    "answer": {
      "body": "Assuming that you are using pytorch TensorDataset, you can apply transform inside `setup` and inverse transform inside `predict_step` itself.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2022-01-03T18:43:06Z"
    }
  },
  {
    "title": "model inference but self.training is save true",
    "body": "i use `self.training` param to judge what data return.\r\n\r\n`return x if self.training else (torch.cat(z, 1), x)`\r\n\r\nbut when i load my model, i use debug mode find that the self.training is save True.\r\n```Python\r\nself.model = CustomModel.load_from_checkpoint(model_path)\r\nself.model.training = False\r\n```\r\n\r\ni use above code change model.training status, but its not work\r\n\r\nthis is my inference full code:\r\n\r\n```Python\r\nclass CustomModelInference:\r\n    def __init__(\r\n            self,\r\n            model_path: str,\r\n            conf_thres: float = 0.25,\r\n            iou_thres: float = 0.45,\r\n            max_det: int = 1000,\r\n            device: str = 'cuda:0',\r\n            need_classes: list | None = None\r\n    ):\r\n        self.conf_thres = conf_thres\r\n        self.iou_thres = iou_thres\r\n        self.max_det = max_det\r\n        self.device = device\r\n        self.need_classes = need_classes\r\n\r\n        self.model = CustomModel.load_from_checkpoint(model_path)\r\n        self.model.training = False\r\n        self.model.to(device)\r\n        self.stride = int(self.model.stride.max())\r\n        self.names = self.model.names\r\n        self.imgsz = self.model.imgsz\r\n\r\n    @torch.no_grad()\r\n    def infer(self, img: np.ndarray):\r\n        imgsz = check_img_size(self.imgsz, s=self.stride)\r\n        cudnn.benchmark = True\r\n        img = letterbox(img, imgsz, stride=self.stride, auto=True)[0]\r\n        # img = np.stack(img, 0)\r\n        if len(img.shape) == 3:\r\n            img = img[None]\r\n        img = img[..., ::-1].transpose((0, 3, 1, 2))\r\n        img = np.ascontiguousarray(img)\r\n        img = torch.from_numpy(img).to(self.device)\r\n        img = img.float()\r\n        img = img / 255.0\r\n\r\n        out, train_out = self.model(img)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11285",
    "createdAt": "2021-12-31T02:28:59Z",
    "updatedAt": "2022-05-31T10:09:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "fine, i know the answer, i have to set model.eval()......",
      "author": {
        "login": "morestart"
      },
      "createdAt": "2021-12-31T02:32:41Z"
    }
  },
  {
    "title": "`save_hyperparameters()` is very slow when there are many hyperparameters, any speed up?",
    "body": "Hi,\r\nI've currently refactored a part of code to use Pytorch Lightning instead of a regular pytorch script. I find a 35% speed up on a toy dataset, where the model had 20.9 K parameters.\r\n\r\nHowever, when trying a bigger version of the model (9.1 M params), the initialization of the model takes a lot more time (even before lightning prints the number of params). I've managed to nail it down to the `self.save_hyperparameters()` function in init, since the initialization is instantaneous without calling `self.save_hyperparameters()`, but takes more than a full minute when calling it.\r\n\r\nRemoving `self.save_hyperparameters()` causes an issue in my code since I am calling  `model.load_from_checkpoint()` afterward. Would you have any thoughts on how I can speed up the code? The \"regular\" pytorch model manages to initialize, train and save the model faster than the Lightning one due to this slow down ...\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11281",
    "createdAt": "2021-12-30T18:09:30Z",
    "updatedAt": "2022-10-03T14:28:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "VictorJouault"
    },
    "answer": {
      "body": "You have to set` self.save_hyperparameters(ignore=\"model\")`, since you are passing in a full model as a \"hyperparameter\", you don't want to save that!",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2022-01-01T11:16:31Z"
    }
  },
  {
    "title": "Accuracy doesn't show up in progress bar",
    "body": "```python\r\nclass Mynet(pl.LightningModule)\r\n    ... ...\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        accu = ((torch.argmax(logits, dim=1) == y).sum()/x.shape[0]).item()   \r\n        return {'loss':loss, 'accuracy':accu}\r\n```\r\n\r\ni define training_step like this, the progress bar show loss,  but not show accu. \r\ni want to know why?\r\n\r\npytorch_lightning.__version__ = 1.5.7",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11277",
    "createdAt": "2021-12-30T03:53:05Z",
    "updatedAt": "2022-06-01T23:01:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lastadreamer"
    },
    "answer": {
      "body": "You need to log it: \r\n\r\n\r\n```py\r\nself.log(\"accuracy\", accu, prog_bar=True)\r\nreturn {'loss':loss}\r\n```\r\n\r\nDocs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-from-a-lightningmodule\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-31T02:45:22Z"
    }
  },
  {
    "title": "using EMA with model checkpoints",
    "body": "I'm trying to incorporate the [pytorch_ema](https://github.com/fadel/pytorch_ema) library into the PL training loop. I found one topic relating to using pytorch_ema in lightning in [this discussion thread](https://forums.pytorchlightning.ai/t/adopting-exponential-moving-average-ema-for-pl-pipeline/488), but how would this work if i want to save a model checkpoint based on the EMA weights? for example if i want to save the model weights using just pytorch, i could do something like\r\n\r\n```\r\n# using accuracy as an example\r\nif current_val_acc >= best_val_acc:\r\n    with ema.average_parameters():\r\n        torch.save(model.state_dict(), saved_model_pth)\r\n```\r\n\r\nso that i save the smoothed weights, but restore the original weights to the model so it doesn't affect training\r\n\r\none workaround i can think of is to create my own model saving logic in the `validation_epoch_end` instead of relying on the `ModelCheckpoint` callback, but that seems to be a bit hacky. are there any potentially better solutions?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11276",
    "createdAt": "2021-12-29T21:59:22Z",
    "updatedAt": "2022-06-11T21:46:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "maxmatical"
    },
    "answer": {
      "body": "you can replace the model state_dict inside the checkpoint\r\n\r\n```py\r\nclass LitModel(LightningModule):\r\n    ...\r\n    \r\n    def on_save_checkpoint(self, checkpoint):\r\n        with ema.average_parameters():\r\n            checkpoint['state_dict'] = self.state_dict()\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-31T20:01:41Z"
    }
  },
  {
    "title": "How to access the strategy of the trainer",
    "body": "Hi, I am trying to make my code invariant to the choice of strategies by being able to compute the global batch size which depends on the strategy. For example, for DDP it is `N * batch_size` with N being the number of processes.\r\n\r\nThe use case I can think of is using the global batch size to initialize the optimizer.\r\n\r\n```python\r\ntrainer(num_nodes=1, gpus=2, strategy='ddp') # pass the strategy ddp for example\r\n\r\nclass MyLightningModule(pl.LightningModule):\r\n    @property\r\n    def global_batch_size(self) -> int:\r\n        if self.trainer.strategy is None:\r\n            return self.trainer.datamodule.train.loader.batch_size\r\n        elif self.trainer.strategy is DDPStrategy:\r\n            return self.trainer.num_nodes * self.trainer.gpus *\\       # There might be a better way to compute the\r\n                      self.trainer.datamodule.train.loader.batch_size  # number of processes using the strategy\r\n        ...\r\n\r\n    def configure_optimizers(self) -> Dict[Any, Any]:\r\n        optimizer, scheduler = hydra.utils.instantiate(\r\n            self.hparams.optimizer, model=self, batch_size=self.global_batch_size, _recursive_=False)\r\n\r\n        return {\r\n            'optimizer': optimizer,\r\n            'lr_scheduler': scheduler\r\n        }\r\n```\r\n\r\nTo do so, I would like to retrieve inside my Lightning module the strategy used by my trainer. I tried to find in the trainer code how to access the strategy and I found the property:\r\n\r\n```python\r\n# in pytorch_lightning.trainer.trainer.py\r\nclass Trainer(...):\r\n    ...\r\n    @property\r\n    def strategy(self) -> Strategy:\r\n        return self._accelerator_connector.strategy\r\n```\r\n\r\nHowever `self.trainer.strategy` in `configure_optimizers` raises `AttributeError: 'Trainer' object has no attribute 'strategy'`.\r\n\r\nWeirdly, `self.trainer._accelerator_connector.strategy` works and returns the passed strategy in the trainer. Yet, if I understood correctly the `_accelerator_connector` should resolve the strategy `'ddp'` to `DDPStrategy` in its initialization but it returns `'ddp'`:\r\n \r\n```python\r\n# in pytorch_lightning.trainer.connectors.accelerator_connector.py\r\n\r\nclass AcceleratorConnector(...):\r\n    def __init__(...):\r\n        ...\r\n        self.strategy = self.final_strategy()\r\n     ...\r\n```\r\n\r\nIs it possible to access the strategy used for training?\r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11272",
    "createdAt": "2021-12-28T16:05:51Z",
    "updatedAt": "2022-05-31T14:34:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "juliendenize"
    },
    "answer": {
      "body": "just out of curiosity, what sort of scheduler/optimizer are you initializing using the global_batch_size?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-28T21:54:52Z"
    }
  },
  {
    "title": "ddp: how to combine multi-gpus outputs like \"training_step_end\" which is only used in dp/ddp2?",
    "body": "My question is like title. Thank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11265",
    "createdAt": "2021-12-27T02:02:09Z",
    "updatedAt": "2022-06-06T18:23:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jiyt17"
    },
    "answer": {
      "body": "checkout the example here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-with-dataparallel",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-27T18:20:20Z"
    }
  },
  {
    "title": "Does LightningLite still support various callbacks?",
    "body": "`LightningModule` can be coupled with various callbacks? I wonder if it is possible `LightningLite` can also reuse those callbacks, such as `wandb`, `checkpoint`?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11260",
    "createdAt": "2021-12-26T12:11:35Z",
    "updatedAt": "2021-12-31T02:48:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "marsggbo"
    },
    "answer": {
      "body": "I don't think they are because the hooks in callbacks are just some code injections that are supposed to run at a certain point where lightning defines the training loops/validation loops but in `LightningLite` user defines the training loop, so it won't be possible. Although in the case of wandb for eg you can use their native API to log the stuff.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-26T19:14:47Z"
    }
  },
  {
    "title": "Why trainer.logged_metrics is mixed of tensor and non tensor value.",
    "body": "When I print `trainer.logged_metrics`, I got\r\n```\r\n{\r\n    'train_f1': tensor(1.),\r\n    'train_prc': tensor(1.),\r\n    'train_rec': tensor(1.),\r\n    'train_loss': tensor(0.0152),\r\n    'valid_f1': 0.861257791519165,\r\n    'valid_prc': 0.9134419560432434,\r\n    'valid_rec': 0.8147138953208923,\r\n    'valid_loss': 0.16973154246807098\r\n}\r\n```\r\nWhy trainer.logged_metrics is mixed of tensor and non tensor value.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11251",
    "createdAt": "2021-12-24T08:11:14Z",
    "updatedAt": "2022-09-07T08:39:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tshu-w"
    },
    "answer": {
      "body": "would you mind sharing the log calls in your training_step and validation_step hooks?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-24T18:40:10Z"
    }
  },
  {
    "title": "training on gpu becomes non-deterministic",
    "body": "I usually set seed_everything() at the beginning of my script but this does not always solves the problem.\r\n\r\nWhen I train models on cpu, it is determinstic; but when I switch to gpu, it becomes non-deterministic.\r\n\r\nWhen I am training simple model, like one-layer LSTM, it is deterministic both on cpu and gpu.\r\n\r\nBut when I train a more completed model like LSTM-FCN, it is deterministic on cpu but not on gpu\r\n\r\nCan I get any help on debugging?\r\n\r\nI got my LSTM-FCN model from here (https://github.com/timeseriesAI/tsai/blob/main/tsai/models/RNN_FCN.py) and the LSTM model I tested was simply a nn.LSTM",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11250",
    "createdAt": "2021-12-24T07:03:43Z",
    "updatedAt": "2022-06-09T01:34:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "2533245542"
    },
    "answer": {
      "body": "maybe setting `Trainer(deterministic=True)` might help?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-24T18:41:02Z"
    }
  },
  {
    "title": "How can i know the current iter?",
    "body": "i want use iter to control multi_scale",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11215",
    "createdAt": "2021-12-22T02:16:46Z",
    "updatedAt": "2022-07-23T11:58:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "Just for anyone looking at this Discussion, here's the list of relevant properties:\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#global-step\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#current-epoch",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-06-03T04:59:30Z"
    }
  },
  {
    "title": "How should the number of steps be set against to processed data when using ddp and multi GPU",
    "body": "Hi I'm newbie for pytorch-lightning.\r\nPlease teach me about this topic.\r\n\r\nI want to process 100,000 records. So I set `max_step` = 100,000.\r\nAnd to speed up learning, I also set `strategy = ddp` and use 4 GPUs with single node.\r\nBut when I observe behavior of pytorch-lightning, it seems process 400,000 records. \r\nbut step number is still 100,000 steps.\r\n\r\nIs there any recognition that the number of steps specified when using multiple GPUs needs to be divided by the number of GPUs with the expected number of steps?\r\n(on above example, should I set `max_step` to 25,000? )\r\n\r\n(my pytorch-lightinng version is 1.5.4)\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11192",
    "createdAt": "2021-12-21T03:20:19Z",
    "updatedAt": "2022-06-23T04:27:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kash203"
    },
    "answer": {
      "body": "hey @kash203 \r\n\r\nwith DDP if batch_size is 32 with 4 GPUs then the effective batch size actually 32*4. See the [docs here](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size).\r\n\r\nnow in your case, there are 100_000 samples. Considering batch_size as 1, a single training step with 4 GPUs means 4 calls to the dataloader at 4 samples are covered in a single step. Thus your max_steps here should be 25_000 as you stated.\r\n\r\nIn case you want to process all the samples only once, you can just set max_epoch=1. `max_steps` is generally used for sequential learning tasks where data is iteratively created.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-22T13:25:32Z"
    }
  },
  {
    "title": "val_check_interval every N global steps?",
    "body": "Hello, is there a way to call validation for every N global steps? For example, we could have vall_check_interval greater than the number of batches in the training dataset.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11152",
    "createdAt": "2021-12-18T02:23:21Z",
    "updatedAt": "2022-06-29T08:44:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "senzeyu"
    },
    "answer": {
      "body": "Unfortunately, not at the moment.\r\n\r\nThere's an issue for it here https://github.com/PyTorchLightning/pytorch-lightning/issues/8135",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-01-12T03:35:02Z"
    }
  },
  {
    "title": "Multiple Validation Sets",
    "body": "Hello, I'm trying to validate my model on multiple subsets of the initial validation set to compare performance. Reading [this page](https://pytorch-lightning.readthedocs.io/en/stable/guides/data.html) I got the idea that returning a list contaning the multiple Dataloaders would be enough. My val_dataloader method became the following:\r\n\r\n![image](https://user-images.githubusercontent.com/73995923/146572624-438bc306-f049-4cba-972a-64162b98294e.png)\r\n\r\nBut this isn't working properly. I get the following error: _\"TypeError: validation_step() takes 3 positional arguments but 4 were given\"_\r\n(it worked properly when I only used 1 validation Dataloader).\r\nWhat am I doing wrong? Can someone help me with this? Or just point me to some more documentation on this.\r\n\r\nThanks in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11135",
    "createdAt": "2021-12-17T16:02:33Z",
    "updatedAt": "2022-06-06T08:18:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Jose-Bastos"
    },
    "answer": {
      "body": "you must be missing the additional `dataloader_idx` required in the `validation_step` for multiple dataloaders\r\ndocs: https://pytorch-lightning.readthedocs.io/en/latest/guides/data.html#multiple-validation-test-predict-dataloaders",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-17T17:44:30Z"
    }
  },
  {
    "title": "Save checkpoint with specific monitor criteria",
    "body": "Hello everyone, I'm currently implementing a Wasserstain type of GAN using Gradient Penalty. I want to save the checkpoints monitoring the negative critic loss, which starts from low values, increases to higher values in the first epochs and then decreases reaching almost 0. A plot of this loss can be seen in the paper: https://arxiv.org/pdf/1704.00028.pdf\r\n\r\nThe problem is that if I use **ModelCheckpoint** and set the monitor parameter to negative critic_loss and mode = 'min', it basically saves the first epoch only. However I don't want to consider the training start epochs, when the negative loss increase, but only the epochs when the loss decrease.\r\n\r\nI'm currently using multi-gpu training\r\n\r\nHow can I implement this? Should I override the function on_train_epoch_end and save there the checkpoints, after checking the above criteria? Or should I use a lightning Callback? If so how can I acces to the monitored values?\r\nThanks in advance\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11129",
    "createdAt": "2021-12-17T11:11:31Z",
    "updatedAt": "2022-06-10T21:33:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SalvatoreCognetta"
    },
    "answer": {
      "body": "Thanks to @tchaton on the slack community I solved the issue overriding the `ModelCheckpoint` class.\r\nIn the `on_train_epoch_end` I've added a new check that follow the above conditions, as such:\r\n\r\n```python\r\nclass WGANModelCheckpoint(ModelCheckpoint):\r\n    def __init__(self,\r\n                 dirpath: Optional[Union[str, Path]] = None,\r\n                 filename: Optional[str] = None,\r\n                 monitor: Optional[str] = None,\r\n                 verbose: bool = False,\r\n                 save_last: Optional[bool] = None,\r\n                 save_top_k: int = 1,\r\n                 save_weights_only: bool = False,\r\n                 mode: str = \"min\",\r\n                 auto_insert_metric_name: bool = True,\r\n                 every_n_train_steps: Optional[int] = None,\r\n                 train_time_interval: Optional[timedelta] = None,\r\n                 every_n_epochs: Optional[int] = None,\r\n                 save_on_train_epoch_end: Optional[bool] = None,\r\n                 period: Optional[int] = None,\r\n                 every_n_val_epochs: Optional[int] = None):\r\n        super().__init__(\r\n            dirpath=dirpath,\r\n            filename=filename,\r\n            monitor=monitor,\r\n            verbose=verbose,\r\n            save_last=save_last,\r\n            save_top_k=save_top_k,\r\n            save_weights_only=save_weights_only,\r\n            mode=mode,\r\n            auto_insert_metric_name=auto_insert_metric_name,\r\n            every_n_train_steps=every_n_train_steps,\r\n            train_time_interval=train_time_interval,\r\n            every_n_epochs=every_n_epochs,\r\n            save_on_train_epoch_end=save_on_train_epoch_end,\r\n            period=period,\r\n            every_n_val_epochs=every_n_val_epochs)\r\n        self.is_monitoring_on = False\r\n\r\n    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", unused: Optional = None) -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        # as we advance one step at end of training, we use `global_step - 1` to avoid saving duplicates\r\n        trainer.fit_loop.global_step -= 1\r\n        if (\r\n            not self._should_skip_saving_checkpoint(trainer)\r\n            and self._save_on_train_epoch_end\r\n            and self._every_n_epochs > 0\r\n            and (trainer.current_epoch + 1) % self._every_n_epochs == 0\r\n            and (self.is_monitoring_on or self.monitor_can_start(trainer, pl_module))\r\n        ):\r\n            self.save_checkpoint(trainer)\r\n        trainer.fit_loop.global_step += 1\r\n\r\n    def monitor_can_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> bool:\r\n        \"\"\"Let start monitoring only after the loss curve start increasing\"\"\"\r\n       monitor_candidates = self._monitor_candidates(trainer, trainer.current_epoch, trainer.global_step - 1)\r\n        current = monitor_candidates.get(self.monitor)\r\n\r\n        # Check if the critic loss is increasing (the network is starting to\r\n        # train)\r\n        if trainer.current_epoch > 0 and pl_module.previous_metric < current:\r\n            self.is_monitoring_on = True\r\n\r\n        pl_module.previous_metric = current.detach().clone()\r\n\r\n        return self.is_monitoring_on\r\n```\r\n\r\nThe function `monitor_can_start()` does the trick.",
      "author": {
        "login": "SalvatoreCognetta"
      },
      "createdAt": "2021-12-17T15:59:08Z"
    }
  },
  {
    "title": "Save checkpoints without overwrite",
    "body": "Hi there,\r\n\r\nI am using the ModelCheckpoint callback to save my model every n epochs but I cannot find a way to prevent PL from overwriting/deleting the previous checkpoint.\r\n\r\nIdeally, I would like to keep the default naming convention {epoch}-{step} but without losing previous checkpoints.\r\n\r\nThanks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11087",
    "createdAt": "2021-12-15T17:03:36Z",
    "updatedAt": "2022-06-01T15:27:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mcomunita"
    },
    "answer": {
      "body": "you can create a custom ModelCheckpoint instance with `save_top_k=-1` and pass it in inside Trainer callbacks.\r\n\r\n```py\r\nckpt_callback = ModelCheckpoint(save_top_k=-1, ...)\r\ntrainer = Trainer(callbacks=[ckpt_callback], ...)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-16T14:01:49Z"
    }
  },
  {
    "title": "Combine outputs in test epochs when using DDP",
    "body": "I'm training a model across two GPUs on patient data (id). In my test steps, I output dictionaries, which contain the id, as well as all the metrics. I store these (a list with a dict per id) at the end of the test epoch, so I can later on statistically evaluate model performances.\r\n\r\nI'm experiencing a problem with the test step, however. \r\n\r\n    # Test step\r\n    def test_step(self, batch, batch_idx):\r\n\r\n        # Get new input and predict, then calculate loss\r\n        x, y, id = batch[\"input\"], batch[\"target\"], batch[\"id\"]\r\n\r\n        # Infer and time inference\r\n        start = time()\r\n        y_hat = self.test_inference(x, self, **self.test_inference_params)\r\n        end = time()\r\n\r\n        # Calculate metrics\r\n        id = id[0] if len(id) == 1 else tuple(id)\r\n\r\n        # Output dict with duration of inference\r\n        output = {\"id\": id, \"time\": end - start}\r\n\r\n        # Add other metrics to output dict\r\n        for m, pars in zip(self.metrics, self.metrics_params):\r\n\r\n            metric_value = m(y_hat, y, **pars)\r\n\r\n            if hasattr(metric_value, \"item\"):\r\n                metric_value = metric_value.item()\r\n\r\n            output[f\"test_{m.__name__}\"] = metric_value\r\n\r\n        return output\r\n\r\n    # Test epoch end (= test end)\r\n    def test_epoch_end(self, outputs):\r\n\r\n        # Go over outputs and gather\r\n        self.test_results = outputs     #self.all_gather(outputs)\r\n\r\nI hadn't considered this before (as I'm used to training on a single GPU), but the test_results attribute now only contains half of the outputs (one half per process). So when my main script reaches this section, only half the output is effectively stored:\r\n\r\n    log(\"Evaluating model.\")\r\n    trainer.test(model=model,\r\n                 dataloaders=brats.val_dataloader())\r\n    results = model.test_results\r\n\r\n    # Save test results\r\n    log(\"Saving results.\")\r\n    np.save(file=join(result_dir, f'{model_name}_v{version}_fold{fold_index}.npy'), arr=results)\r\n\r\nI have read about the `self.all_gather` method, but I'm not sure it suits my needs. I want to merge the lists, not reduce anything. Also, they're not Tensors, but dicts. How can I store all dicts across both DDP processes?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11086",
    "createdAt": "2021-12-15T16:52:45Z",
    "updatedAt": "2022-07-08T18:38:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "WouterDurnez"
    },
    "answer": {
      "body": "all_gather is different from all_reduce. It doesn't do any math operation here.\r\nsort of like:\r\n```\r\nall_gather -> collect outputs from all devices\r\nall_reduce -> in general, collect outputs from all devices and reduce (apply a math op)\r\n```\r\nall_gather isn't working for you?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-16T14:06:51Z"
    }
  },
  {
    "title": "Can I turn off Validation step when overfit_batches=X?",
    "body": "I call `trainer.fit(model=model_lit, datamodule=datamodule)`\r\n\r\nI have set parameters:\r\n\r\n```\r\nlimit_val_batches: 0\r\nlimit_test_batches: 0\r\n```\r\n\r\n`datamodule.val_dataloader()` returns **None**\r\n\r\nBut it is still trying to perform validation... \r\nIs it a problem of lightning, or am I doing something wrong? :)\r\n\r\n![image](https://user-images.githubusercontent.com/27057946/145991475-291a9199-22f8-43ce-a195-127d992ab09e.png)\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11057",
    "createdAt": "2021-12-14T11:41:10Z",
    "updatedAt": "2024-02-11T19:22:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "IgorHoholko"
    },
    "answer": {
      "body": "it's now turned off by default on master:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/2faaf35b91a00aff397609a875a66c87f8ed6390/pytorch_lightning/trainer/trainer.py#L636-L640",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-14T14:58:12Z"
    }
  },
  {
    "title": "Accessing available values to monitor when saving checkpoints",
    "body": "I would like to save the top-10 checkpionts along training. By checking documentations, setting `save_top_k`, `monitor` and `mode` options in [`ModelCheckpoint`](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html?highlight=ModelCheckpoint) jointly seem to do the job.\r\n\r\nBut I am not sure what are the parameters available for the this callback to monitor. Are they logged values saved during `training_step()` or `validation_step()` through `self.log(\"loss\", XYZ)`?\r\n\r\nThank you in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11032",
    "createdAt": "2021-12-10T17:06:48Z",
    "updatedAt": "2021-12-11T02:55:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "guanqun-yang"
    },
    "answer": {
      "body": "yes, that's correct.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-10T22:04:08Z"
    }
  },
  {
    "title": "Do LightningDataModule support multi train dataloader like LightningModule?",
    "body": "Can I define multi training dataloaders in LightningDataModule? \r\n```\r\n    def train_dataloader(self):\r\n        lab_loader = torch.utils.data.DataLoader(\r\n                         self.train_subset_lab_train,\r\n                         batch_size=self.batch_size,\r\n                         shuffle=True,\r\n                         num_workers=self.num_workers,\r\n                         pin_memory=True,\r\n                         drop_last=True,\r\n                    )\r\n        unlab_loader = torch.utils.data.DataLoader(\r\n                         self.train_subset_unlab_train,\r\n                         batch_size=self.batch_size*self.uratio,\r\n                         shuffle=True,\r\n                         num_workers=self.num_workers,\r\n                         pin_memory=True,\r\n                         drop_last=True,\r\n                    )\r\n\r\n        return [lab_loader, unlab_loader]\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11024",
    "createdAt": "2021-12-10T03:20:03Z",
    "updatedAt": "2022-08-22T09:32:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kleinzcy"
    },
    "answer": {
      "body": "@kleinzcy @tshu-w,\r\n\r\nPyTorch Lightning supports multiple data loaders but they will be sampled at the same time and return a batch composed of a batch sample for each dataloaders return for the train_dataloader method.\r\n\r\nAlternatively, if you want to make this sequential, you can implement a wrapper which will sample from one and the other dataloader.\r\n\r\n```py\r\nclass SequentialLoader(Iterator):\r\n\r\n    def __init__(self, *dataloaders):\r\n        self.dataloaders = dataloaders\r\n\r\n    def __len__(self):\r\n        return sum([len(dl) for dl in self.dataloaders])\r\n\r\n    def __iter__(self):\r\n        for dl in self.dataloaders:\r\n            dataloader_iter = iter(dl)\r\n            for batch in dataloader_iter:\r\n                yield batch\r\n```",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-12-10T09:11:05Z"
    }
  },
  {
    "title": "How can one use an external optimizer with LightningCLI?",
    "body": "I would like to use [Adafactor](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor) as my optimizer with LightningCLI. I've tried the method described in the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#optimizers-and-learning-rate-schedulers) for custom optimizers but it didn't work. Can anybody tell me how they would train a model with this optimizer using LightningCLI?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11016",
    "createdAt": "2021-12-09T20:56:17Z",
    "updatedAt": "2022-06-10T19:01:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "goncalomcorreia"
    },
    "answer": {
      "body": "Hi! I got it to work in the meantime. I added this to the main file where I call CLI:\r\n\r\n```\r\nimport transformers\r\nfrom pytorch_lightning.utilities.cli import OPTIMIZER_REGISTRY\r\n\r\n@OPTIMIZER_REGISTRY\r\nclass Adafactor(transformers.Adafactor):\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n```\r\n\r\nThe main issue was in the config file---apparently one needs to write:\r\n\r\n```\r\noptimizer:\r\n  class_path: __main__.Adafactor\r\n```\r\n\r\ninstead of:\r\n\r\n\r\n```\r\noptimizer:\r\n  class_path: Adafactor\r\n```\r\n\r\nDoing the former, I got it to work.\r\n\r\nBy the way, is there a way to have the optimizer register in a separate file than the one that calls CLI?",
      "author": {
        "login": "goncalomcorreia"
      },
      "createdAt": "2021-12-10T10:29:35Z"
    }
  },
  {
    "title": "AttributeError: module 'pytorch_lightning' has no attribute 'metrics'",
    "body": "AttributeError: module 'pytorch_lightning' has no attribute 'metrics'\r\n\r\nwhat is this...? T.T",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11005",
    "createdAt": "2021-12-09T02:44:02Z",
    "updatedAt": "2022-07-02T00:55:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "NeighborhoodCoding"
    },
    "answer": {
      "body": "Pytorch Lightning Metrics were moved to a separate package/library/repo, torchmetrics, starting from Lightning 1.5",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-12-09T02:50:48Z"
    }
  },
  {
    "title": "Why accumulate_grad_batches cannot be used with manual optimization?",
    "body": "I've stumbled upon the problem of not being able to use `accumulate_grad_batches` argument in the Trainer as I was doing manual optimization in my LightningModule to use adversarial loss functions.\r\n\r\nHowever, I think it would be possible to implement something that would \"store\" calls to the `step` method for the module's optimizers and actually apply them once every `accumulate_grad_batches` iterations. I've seen several related issues about similar behavours when overriding `optimizer_step` or close to my use case (#5054, #5108). The proposed fixes always leave some manual get-arounds in the final code.\r\n\r\n**My question: is there a reason for such incompatibility of `accumulate_grad_batches` with manual optimization ?**\r\n\r\nOne reason might be the need to `step` different optimizers at different paces (one every batch, another every n batches ...) but this seems to be an extreme use case.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10998",
    "createdAt": "2021-12-08T16:45:28Z",
    "updatedAt": "2022-07-09T08:05:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "NathanGodey"
    },
    "answer": {
      "body": "Hey @NathanGodey,\r\n\r\nmanual optimization was built to provide full control optimization control to the user while abstracting distributed training and precision.\r\n\r\nThere is no way Lightning can automate properly accumulate grad batches for all the possible use cases and therefore isn't supported.\r\n\r\nHowever, you can easily implement it by not calling zero_grad, step every n batches.",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-12-10T09:04:20Z"
    }
  },
  {
    "title": "How to compute a squad metric?",
    "body": "Hallo, \r\nI am trying to train a BERT model with the squad dataset. \r\nSince the model has a maximum input length, examples with longer contexts must be split into several samples. \r\nIn the validation_epoch_end() and test_epoch_end() methods I want to compute the squad metric (https://github.com/PyTorchLightning/metrics/blob/master/torchmetrics/text/squad.py) that needs the predicted answers in text format. \r\nBut for each sample the model predicts only the start and end token. \r\nTo generate the answers in text format I need the dataset from the val/test_dataloader as well as the original val/test_dataset (with text instead of input_ids, offset_mappings etc.)\r\nSo I was wondering how to access the original dataset in those methods. \r\nOf course I can use self.trainer.datamodule.original_val_dataset but then I need to ensure that the trainer uses a datamodule and not only dataloaders. What is the best practice for this case? \r\nAnd how do I use this model in production? Can I compute the text answers in the LightningModule and return them when predict() is called ?\r\nOr should predict() only return the start and end tokens and the text answer is computed outside of the LightningModule? \r\nThank you in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10997",
    "createdAt": "2021-12-08T16:07:14Z",
    "updatedAt": "2023-02-28T12:36:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "saneim"
    },
    "answer": {
      "body": "I'd suggest accessing the dataloader/dataset isn't a reliable solution, but rather you can return the original text and offests in Dataset.getitem or you can store the offests and text in the state variables for easy access.\r\n\r\nHere are some examples I worked with on kaggle:\r\nFor the first one where you return the offests/text: https://www.kaggle.com/rohitgr/roberta-with-pytorch-lightning-train-test-lb-0-710\r\nThis one uses an old version of lightning, but should still be relevant I guess.\r\n\r\nHere I store them in my datamodule: https://www.kaggle.com/rohitgr/chaii-q-a-with-pytorch-lightining\r\nthis one is recent so should be working.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-08T18:46:03Z"
    }
  },
  {
    "title": "CUDA OOM during validation of first epoch",
    "body": "hi all,\r\n\r\nMy model validation code (see below) appears to leak memory which leads to a rapid increase in GPU memory usage and, eventually, to an OOM error right before the validation loop is about to complete (about 90% done or so). CUDA memory usage hovers around 8-9GB during training, then increases rapidly to ca. 15+GB during validation, hitting the memory limit of my GPU card. \r\n\r\nWhat am I doing wrong here?\r\n\r\n```\r\nclass Lightning_WGAN_GP(pl.LightningModule):\r\n    \"\"\"Conditional Wasserstein GAN with gradient penalty.\"\"\"\r\n \r\n    # (...)\r\n\r\n    def _get_noise(self, X: torch.Tensor) -> torch.Tensor:\r\n        bs, _, h, w = X.shape\r\n        return torch.randn(bs, 1, h, w).type_as(X)\r\n\r\n    def validation_step(self, batch: Tuple[Dict, ...], batch_idx: int) -> Dict:\r\n        del batch_idx  # not used\r\n        X, X_hr, real = batch[0][\"X_lr\"], batch[0][\"X_hr\"], batch[1][\"y\"]\r\n\r\n        with torch.no_grad():\r\n            noise = self._get_noise(X)\r\n            fake = self.gen(noise, X, X_hr)  # calling the generator\r\n            loss_gen_val = F.l1_loss(fake, real)   # generator loss\r\n            disc_real = self.disc(X, real, X_hr).reshape(-1)  # calling the discriminator\r\n            disc_fake = self.disc(X, fake, X_hr).reshape(-1)\r\n            loss_disc_val = -torch.mean(disc_real) + torch.mean(disc_fake)  # discriminator loss\r\n            \r\n        self.log(\"gen_val_loss\", loss_gen_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\r\n        self.log(\"disc_val_loss\", loss_disc_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\r\n\r\n        return {\"gen_val\": loss_gen_val, \"disc_val\": loss_disc_val, \"batch\": batch}\r\n```\r\n\r\n\r\n![Screenshot 2021-12-06 at 19 57 18](https://user-images.githubusercontent.com/47196359/144905777-aa2799bb-fd7a-402f-a239-2c30c38a1fda.png)\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 16.00 GiB total capacity; 12.84 GiB already allocated; 96.55 MiB free; 13.52 GiB reserved in total by PyTorch)\r\n```\r\n\r\nDecreasing (or increasing) the validation batch size doesn't make the problem go away. Any thoughts?\r\n\r\n```\r\n$ conda list | grep pytorch\r\npytorch                   1.9.1           cuda102py38ha031fbe_3    conda-forge\r\npytorch-gpu               1.9.1           cuda102py38hf05f184_3    conda-forge\r\npytorch-lightning         1.5.3              pyhd8ed1ab_0    conda-forge\r\n```\r\n\r\n**Later edit**: Skipping the validation loop, i.e.,\r\n```\r\ngan_trainer.fit(gan_model, train_dataloaders=dl_train)\r\n``` \r\ngets rid of the OOM error (the trainer makes it past the 1st epoch). \r\n\r\nAlso, I am running in mixed precision (although i suspect precision doesn't have much to do with this issue?)\r\n\r\nThank you!\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10959",
    "createdAt": "2021-12-06T19:03:23Z",
    "updatedAt": "2022-06-14T09:16:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mishooax"
    },
    "answer": {
      "body": "Dear @mishooax,\r\n\r\nYou are returning the batch from the validation_step, which would be stored. As it is currently on the GPU, after X batches, you would get a OOM.\r\n\r\nUnless you need the batch on epoch end, I would recommend to not return anything from the validation_step.",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-12-07T11:56:10Z"
    }
  },
  {
    "title": "Best practices: CLI and Loading DataModule from config.yaml",
    "body": "Hey,\r\n\r\nI've been using both, PyTorch Lightning/CLI and jsonargparse for quite a while. Yet, I haven't found a simple method to instantiate a specific DataModule whose parameters are set in a config.yaml that was used during training. I have 2 workarounds which are both unsatisfying:\r\n\r\n## Workaround 1 - Reuse CLI\r\n\r\nDefine an instantiate-only CLI and use `.datamodule`\r\n```\r\nclass InstantiateOnlyLightningCLI(LightningCLI): # probably unnecessary in current RC: run=False\r\n    def fit(self) -> None:\r\n        return None\r\n\r\ncli = InstantiateOnlyLightningCLI(\r\n        wave.WaveNet,\r\n        pl.LightningDataModule,\r\n        subclass_mode_model=False,\r\n        subclass_mode_data=True,\r\n    )\r\ncli.datamodule\r\n```\r\nDownsides\r\n - trainer and model are instantiated although not used\r\n - `--help` is populated with many unneeded parameters\r\n\r\n## Workaround 2 - Load Yaml directly\r\nWhen the actual datamodule is known then\r\n```\r\nwith open(config, \"r\") as f:\r\n  plcfg = yaml.safe_load(f.read())\r\n  datamodule = SpecificDataModule(**plcfg[\"data\"][\"init_args\"])\r\n```\r\nDownsides\r\n - No parameter validation\r\n - No class arguments (i.e transformation classes) supported\r\n\r\nDoes anyone have a better solution?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10956",
    "createdAt": "2021-12-06T13:38:14Z",
    "updatedAt": "2024-05-04T12:51:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cheind"
    },
    "answer": {
      "body": "This is similar to https://github.com/PyTorchLightning/pytorch-lightning/discussions/10363. You can use jsonargparse directly to create a parser and instantiate. You can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=dict) # to ignore model\r\nparser.add_argument('--data', type=pl.LightningDataModule)\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated data module will be in `config_init.data`. In the pytorch-lightning source code the add of arguments is done slightly different but this argparse style should be more familiar to more people. Just for reference in lightning for subclass mode it is https://github.com/PyTorchLightning/pytorch-lightning/blob/a7aed2af7a0de344c4a8eac32f9a86a36a3eaeec/pytorch_lightning/utilities/cli.py#L164",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2021-12-07T05:44:51Z"
    }
  },
  {
    "title": "Trainer(amp_level='O2')",
    "body": "When I try to set `Trainer` like this:\r\n\r\n```python\r\ntrainer = Trainer(\r\n    num_sanity_val_steps=0, \r\n    logger=cfg.load_loggers,\r\n    callbacks=cfg.callbacks,\r\n    max_epochs= cfg.General.epochs,\r\n    gpus=cfg.General.gpus,\r\n    amp_level=cfg.General.amp_level, # O2\r\n    precision=cfg.General.precision,  \r\n    accumulate_grad_batches=cfg.General.grad_acc,\r\n    deterministic=True,\r\n    check_val_every_n_epoch=1,\r\n)\r\n```\r\n\r\nIt will throw an error:\r\n\r\n`pytorch_lightning.utilities.exceptions.MisconfigurationException: You have asked for amp_level='O2' but it's only supported with amp_backend='apex'`\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10950",
    "createdAt": "2021-12-06T04:12:05Z",
    "updatedAt": "2022-06-23T00:01:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "RuixiangZhao"
    },
    "answer": {
      "body": "Hey @RuixiangZhao,\r\n\r\nThere are currently 2 precision backends. AMP and APEX. level are supported only with apex and you need to provide Trainer(amp_backend='apex') to activate it as native is the default.\r\n\r\nI am curious, what was missing from the `MisconfigurationException` to make this clearer? ",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-12-06T09:48:51Z"
    }
  },
  {
    "title": "Uninstalling pytorch-lightning",
    "body": "Hi\r\n\r\nI am running `pyTorch 1.10` and `pytorch-lightning 1.5.4`. I want to downgrade pytorch-lightning to `0.7.1` because the code I am testing uses this version and It looks to me a lot of breaking changes have happened since then. \r\n\r\nHow can I do that please? Would a `pip uninstall pytorch-lightning` and then `pip install pytorch-lightning==0.7.1` suffice or there is something else I need to take care of?\r\n\r\nDoes anyone know also if `0.7.1` is going to be compatible with  `pyTorch 1.10` ?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10942",
    "createdAt": "2021-12-04T21:08:14Z",
    "updatedAt": "2022-07-02T13:06:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "acycliq"
    },
    "answer": {
      "body": "Hello\r\nUnfortunately I must predict that your chances will be very low that 0.7.1 will run with pytorch 1.10. I would not expect it. \r\n\r\nBut in any case, if the code was written in that version, the best is to use the pytorch version that was used in that project. Otherwise you may struggle to reproduce the results entirely. If you use conda or virtualenv, you can create different environments isolated from each other, for example, one for the old PL+pytorch project and one with the latest packages. \r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-05T00:59:44Z"
    }
  },
  {
    "title": "Logging tensorboard not showing loss",
    "body": "I am trying to get my losses in Tensorboard, but I am quite confused.\r\nI simply return a dict after `training_step` and `validation_step`, containing `loss` and `log`. However, the only thing that is showing up in Tensorboard of that run is the 'hp_metric' thing... Nothing on the scalars of the losses... Both during and after training.\r\nI got it working with manual logging (self.log(...)), however, that should not be necessary right? And is more complicated when I want for example training and validation loss in one plot. I am working on a Super Resolution GAN. This is my trainer:\r\n\r\n```\r\nclass LitTrainer(pl.LightningModule):\r\n    def __init__(self,\r\n                 netG,\r\n                 netD,\r\n                 lr: float = 0.0002,\r\n                 b1: float = 0.5,\r\n                 b2: float = 0.999,\r\n                 **kwargs\r\n                 ):\r\n        super().__init__()\r\n        self.save_hyperparameters(ignore=[\"netG\", \"netD\"])\r\n\r\n        self.netG = netG\r\n        self.netD = netD\r\n\r\n        self.criterion_GAN = GANLoss(\"vanilla\")\r\n        self.criterion_edge = edge_loss1\r\n        self.criterion_pixel = torch.nn.L1Loss()\r\n\r\n    def forward(self, inputs):\r\n        return self.netG(inputs)\r\n\r\n    def prepare_batch(self, batch):\r\n        return batch[\"LR\"][tio.DATA].squeeze(4), batch[\"HR\"][tio.DATA].squeeze(4)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        imgs_lr, imgs_hr = self.prepare_batch(batch)\r\n\r\n        # train generator\r\n        if optimizer_idx == 0:\r\n            self.gen_hr = self(imgs_lr)\r\n\r\n            loss_adv = self.criterion_GAN(self.netD(self.gen_hr), True)\r\n            loss_edge = self.criterion_edge(self.gen_hr, imgs_hr)\r\n            loss_pixel = self.criterion_pixel(self.gen_hr, imgs_hr)\r\n            g_loss = loss_adv + loss_edge + loss_pixel\r\n\r\n            # self.log(\"loss/G train\", g_loss, on_step=True, on_epoch=True)\r\n            tensorboard_logs = {\"loss_g\": {\"train\": g_loss}}\r\n            return {\"loss\": g_loss, \"log\": tensorboard_logs}\r\n\r\n        # train discriminator\r\n        if optimizer_idx == 1:\r\n\r\n            # for real image\r\n            pred_real = self.netD(imgs_hr)\r\n            real_loss = self.criterion_GAN(pred_real, True)\r\n            # for fake image\r\n            pred_fake = self.netD(self.gen_hr.detach())\r\n            fake_loss = self.criterion_GAN(pred_fake, False)\r\n\r\n            d_loss = (real_loss + fake_loss) / 2\r\n            tensorboard_logs = {\"loss_d\": {\"train\": d_loss}}\r\n\r\n            # self.log(\"loss/D train\", d_loss, on_step=True)\r\n            return {\"loss\": d_loss, \"log\": tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        with torch.no_grad():\r\n            imgs_lr, imgs_hr = self.prepare_batch(batch)\r\n            gen_hr = self(imgs_lr)\r\n            loss_adv = self.criterion_GAN(self.netD(gen_hr), True)\r\n            loss_edge = self.criterion_edge(gen_hr, imgs_hr)\r\n            loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\r\n            g_loss = loss_adv + loss_edge + loss_pixel\r\n\r\n            # for real image\r\n            pred_real = self.netD(imgs_hr)\r\n            real_loss = self.criterion_GAN(pred_real, True)\r\n            # for fake image\r\n            pred_fake = self.netD(self.gen_hr.detach())\r\n            fake_loss = self.criterion_GAN(pred_fake, False)\r\n\r\n            d_loss = (real_loss + fake_loss) / 2\r\n            tensorboard_logs = {\"loss_g\": {\"val\": g_loss},\r\n                                \"loss_d\": {\"val\": d_loss},\r\n                                }\r\n\r\n            # self.log(\"loss/G val\", g_loss, on_step=True)\r\n            # self.log(\"loss/D val\", d_loss, on_step=True)\r\n\r\n            return {\"log\": tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        lr = self.hparams.lr\r\n        b1 = self.hparams.b1\r\n        b2 = self.hparams.b2\r\n        opt_g = torch.optim.Adam(self.netG.parameters(), lr=lr, betas=(b1, b2))\r\n        opt_d = torch.optim.Adam(self.netD.parameters(), lr=lr, betas=(b1, b2))\r\n        return [opt_g, opt_d], []\r\n```\r\n\r\nAnd this is how I start training:\r\n\r\n```\r\nlogger = TensorBoardLogger(\"log\", name=\"test\")\r\n\r\nmodel = LitTrainer(netG=generator, netD=discriminator)\r\ntrainer = pl.Trainer(gpus=1, max_epochs=1, logger=logger, log_every_n_steps=10)\r\ntrainer.fit(model, train_dataloaders=training_loader, val_dataloaders=val_loader)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10925",
    "createdAt": "2021-12-03T16:31:13Z",
    "updatedAt": "2022-06-07T19:15:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rienboonstoppel"
    },
    "answer": {
      "body": "Are you using the latest Lightning version? If yes, this won't work. The new way for logging is through `self.log` instead of returning it from the step methods. See the docs [here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging).",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-05T00:52:15Z"
    }
  },
  {
    "title": "how to use Apex DistributedDataParallel with Lightining?",
    "body": "I was wondering if there's a way to use apex.parallel.DistributedDataParallel instead of pytorch native DistributedDataParallel. (I am trying to reproduce a paper that used Apex DDP and apex mixed precision and i am getting lower results using pytorch native one)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10922",
    "createdAt": "2021-12-03T15:51:09Z",
    "updatedAt": "2022-06-17T03:53:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mostafaelaraby"
    },
    "answer": {
      "body": "Here is a quick draft of what you could try:\r\n\r\n```py\r\nfrom pytorch_lightning.plugins.training_type import DDPPlugin\r\nfrom apex.parallel import DistributedDataParallel\r\nclass ApexDDPPlugin(DDPPlugin):\r\n\r\n    def _setup_model(self, model: Module):\r\n        return  DistributedDataParallel(module=model, device_ids=self.determine_ddp_device_ids(), **self._ddp_kwargs)\r\n\r\n    @property\r\n    def lightning_module(self):\r\n        return self.module.module\r\n```\r\nI'm not sure if apex DistributedDataParallel supports device ids (it seems not??), you may need to remove it.\r\n\r\nUse it in the trainer:\r\n\r\n```py\r\ntrainer = Trainer(gpus=2, strategy=ApexDDPPlugin(), precision=...)\r\ntrainer.fit(model)\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-05T01:11:00Z"
    }
  },
  {
    "title": "Dataloader pickle torchio",
    "body": "I am trying to get multi-gpu training working, on single gpu it is al working fine. However, when I increase the number of GPUs I get a pickling error, and I don't know what to do about it. For the dataloader I am using the patch-based approach from TorchIO, which creates a Queue, maybe that is the cause? Does anyone has experience with TorchIO Queue and Lightning multi-gpu maybe? Or is something else going on?\r\n\r\nThe error i am getting is as follows:\r\n\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3,4]\r\n\r\nTraceback (most recent call last):\r\n  File \"/filepath/SRGAN-patch_tio.py\", line 94, in <module>\r\n    main()\r\n  File \"/filepath/SRGAN-patch_tio.py\", line 91, in main\r\n    trainer.fit(model, train_dataloaders=training_loader)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\r\n    self._dispatch()\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 547, in __getstate__\r\n    raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__)\r\nNotImplementedError: ('{} cannot be pickled', '_SingleProcessDataLoaderIter')\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10898",
    "createdAt": "2021-12-02T14:39:26Z",
    "updatedAt": "2022-09-01T08:59:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rienboonstoppel"
    },
    "answer": {
      "body": "How are launching your training? Can you try to set `strategy='ddp'` instead of the default 'ddp_spawn' for multiple GPUs. For me that works and in opposite to ddp_spawn it does not pickle your dataloader.",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-12-03T09:52:15Z"
    }
  },
  {
    "title": "Not able to Generate Predictions with Trainer.predict()",
    "body": "Hi, I'm new to PyTorch Lightning, used it for the first time and kind of liked it. However, I am facing this one problem, Implemented a classification task for which I trained the model with Huggingface pretrained model as base and classification head on top. The model is training successfully and giving decent validation losses. The problem is, I'm not quite able to figure out the inferencing part. \r\n\r\ncan anyone please point out what is it that I'm doing wrong? It's probably something very basic. \r\n\r\nI'll add the classes of the lightning modules and the Data Modules below.\r\n```\r\n# |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n# | Define the Pytorch Lightning Module Classifier Class     |\r\n# |__________________________________________________________|\r\n\r\nclass ABSASentimentClassifier(pl.LightningModule):\r\n\r\n  def __init__(self, learning_rate = setup['lr'], weights=None, **kwargs):\r\n    super().__init__()\r\n\r\n    self.save_hyperparameters('learning_rate', 'max_epochs')\r\n    self.model = ABSAModel_Bert()\r\n    self.weights = weights\r\n    self.preds = []\r\n  \r\n  def training_step(self, batch, batch_nb):\r\n\r\n    # Forward\r\n    y_hat = self.model(batch)\r\n\r\n    # if self.weights:\r\n    #   self.weights = torch.tensor(class_weights,dtype=torch.float) \r\n    \r\n    # Loss\r\n    loss_fct = torch.nn.CrossEntropyLoss()\r\n    \r\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n\r\n    # Logs\r\n    self.log_dict({'training_loss':loss}, prog_bar=True)\r\n\r\n    return loss\r\n\r\n  \r\n  def validation_step(self, batch, batch_nb):\r\n    \r\n    # Forward\r\n    y_hat = self.model(batch)\r\n        \r\n    # Loss\r\n    loss_fct = torch.nn.CrossEntropyLoss()\r\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n\r\n    # Acc\r\n    a, y_hat = torch.max(y_hat, dim=1)\r\n    val_acc = accuracy_score(y_hat.cpu(), batch['label'].cpu())\r\n    val_acc = torch.tensor(val_acc)\r\n    \r\n    # Logs\r\n    self.log_dict({'val_loss':loss,'val_acc':val_acc}, prog_bar=True)\r\n    \r\n    return loss\r\n\r\n  \r\n  def test_step(self, batch, batch_nb):\r\n    self.model.eval()\r\n    \r\n    # Forward\r\n    yhat = self.model(batch)\r\n      \r\n    # Loss\r\n    # loss_fct = torch.nn.CrossEntropyLoss()\r\n    # loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n    \r\n    # a, y_hat = torch.max(y_hat, dim=1)\r\n    # test_acc = accuracy_score(y_hat.cpu(), batch['label'].cpu())\r\n    \r\n    # Logs\r\n    # self.log_dict({'test_loss':loss,'test_acc':test_acc}, prog_bar=True)\r\n    self.preds = self.preds.extend(yhat.cpu().detach().numpy().tolist())\r\n    return \r\n\r\n  \r\n  def predict_dataloader(self, batch, batch_idx: int , dataloader_idx: int = None):\r\n\r\n    return self.model(batch)\r\n\r\n\r\n  '''\r\n  |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n  | Training Setup  |\r\n  |_________________|\r\n  '''\r\n  def configure_optimizers(self):\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    |   REQUIRED                                                            |\r\n    |   can return multiple optimizers and learning_rate schedulers         |\r\n    |   (LBFGS it is automatically supported, no need for closure function) |\r\n    |_______________________________________________________________________|\r\n    '''\r\n    optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate, eps=1e-08)\r\n    scheduler = {   \r\n      'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, \r\n                                                       steps_per_epoch=len(self.trainer.datamodule.train_dataloader()),\r\n                                                       epochs=self.hparams.max_epochs),\r\n                 \r\n      'interval': 'step'  # called after each training step\r\n    } \r\n    \r\n    return [optimizer], [scheduler]\r\n\r\n  @staticmethod\r\n  def add_model_specific_args(parent_parser, root_dir):  # pragma: no-cover\r\n    \"\"\"\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | Define parameters that only apply to this model     |\r\n    |_____________________________________________________|\r\n    \"\"\"\r\n    parser = ArgumentParser(parents=[parent_parser])\r\n\r\n    # data\r\n    parser.add_argument('--data_root', default=os.path.join(root_dir, 'train_val_data'), type=str)\r\n\r\n    # training params (opt)\r\n    parser.add_argument('--learning_rate', default=setup['lr'], type=float, help = \"type (default: %(default)f)\")\r\n    return parser\r\n```\r\n\r\nalso the dataset class is :\r\n```\r\nclass ABSADataset(Dataset):\r\n  def __init__(self, df, tokenizer, max_len=setup['max_sen_length']):\r\n    self.texts = df['text']\r\n    self.aspects = df['aspect']\r\n    if 'label' in df.columns:\r\n      # print('****Labels Present****')\r\n      self.targets = df['label']\r\n\r\n    else:\r\n      self.targets = None\r\n\r\n    self.tokenizer = tokenizer\r\n    self.max_len = max_len\r\n\r\n  def __len__(self):\r\n    return len(self.aspects)\r\n\r\n  def __getitem__(self, idx):\r\n\r\n    # convert indexes, tensor->list\r\n    if torch.is_tensor(idx):\r\n      idx = idx.tolist()\r\n    \r\n    # define the aspect and text item\r\n    text = (str(self.texts[idx]))\r\n    aspect = str(self.aspects[idx])\r\n\r\n    # define the label\r\n    target = self.targets[idx]\r\n\r\n    # pair the aspect and text for pair-encoding\r\n    pairs = [text, aspect]\r\n    \r\n    '''\r\n    # |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    # | For Debugging            |\r\n    # |__________________________|\r\n    # print(f' text: {text}')\r\n    # print(f' aspect: {aspect}')\r\n    # print(type(text))\r\n    # print(type(aspect))\r\n    '''\r\n    \r\n    # encode the feature pair\r\n    encoded = self.tokenizer.encode_plus(pairs,\r\n                                    add_special_tokens=True,\r\n                                    padding='max_length', \r\n                                    max_length=setup['max_sen_length'], \r\n                                    return_attention_mask=True,\r\n                                    return_tensors='pt',\r\n                                    truncation=True)\r\n    '''\r\n    # |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    # | For Debugging            |\r\n    # |__________________________|\r\n    # for ids in encoded['input_ids']:\r\n    #   print('*'*20)\r\n    #   print(f'{self.tokenizer.decode(ids)} of length = {len(self.tokenizer.decode(ids).split(\" \"))}')\r\n    #   print(f'is encoded as : \\n{ids} \\nwith length = {len(ids)}')\r\n    #   print('*'*20)\r\n    '''\r\n    \r\n    return {\r\n        'label' : target,\r\n        'input_ids' : encoded['input_ids'],\r\n        'attention_mask' : encoded['attention_mask'] \r\n    }\r\n```\r\n\r\nMy goal is to be able to generate predictions for data without any labels present, using the trained model (saved as checkpoint (.ckpt))\r\n\r\nThis is what I did:\r\n```\r\ntestset = ABSATest_Dataset(test, tokenizer=transformer_tokenizer)\r\n\r\ntestLoader = DataLoader(testset, batch_size=setup['test_batch_size'])\r\n\r\ntrainer.predict(model_infer, testLoader)\r\n```\r\nWhere model_infer is :\r\n```\r\nmodel_infer = ABSASentimentClassifier.load_from_checkpoint(PATH_TO_CKPT_FILE)\r\n```\r\n\r\nand got :\r\n```\r\n---------------------------------------------------------------------------\r\nMisconfigurationException                 Traceback (most recent call last)\r\n<ipython-input-44-c724efd019b7> in <module>()\r\n----> 1 trainer.predict(model_infer, testLoader)\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n    987         \"\"\"\r\n    988         return self._call_and_handle_interrupt(\r\n--> 989             self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n    990         )\r\n    991 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    680         \"\"\"\r\n    681         try:\r\n--> 682             return trainer_fn(*args, **kwargs)\r\n    683         # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    684         except KeyboardInterrupt as exception:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1030         )\r\n   1031 \r\n-> 1032         results = self._run(model, ckpt_path=self.predicted_ckpt_path)\r\n   1033 \r\n   1034         assert self.state.stopped\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _run(self, model, ckpt_path)\r\n   1115             parsing.clean_namespace(model.hparams)\r\n   1116 \r\n-> 1117         verify_loop_configurations(self, model)\r\n   1118 \r\n   1119         # attach model log function to callback\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in verify_loop_configurations(trainer, model)\r\n     38         __verify_eval_loop_configuration(trainer, model, \"test\")\r\n     39     elif trainer.state.fn == TrainerFn.PREDICTING:\r\n---> 40         __verify_eval_loop_configuration(trainer, model, \"predict\")\r\n     41 \r\n     42     __verify_dp_batch_transfer_support(trainer, model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in __verify_eval_loop_configuration(trainer, model, stage)\r\n    187             raise MisconfigurationException(\"`predict_step` cannot be None to run `Trainer.predict`\")\r\n    188         elif not has_step and not is_overridden(\"forward\", model):\r\n--> 189             raise MisconfigurationException(\"`Trainer.predict` requires `forward` method to run.\")\r\n    190     else:\r\n    191         # -----------------------------------\r\n\r\nMisconfigurationException: `Trainer.predict` requires `forward` method to run.\r\n```\r\n\r\nALso, I haven't defined a forward function in the lightning module because it is present in the model class:\r\n```\r\nclass ABSAModel_Bert(torch.nn.Module):\r\n\r\n  def __init__(self, num_labels=setup['num_labels'], config = setup, **kwargs):\r\n    super(ABSAModel_Bert, self).__init__()\r\n    \r\n    self.num_labels = num_labels\r\n    self.bert = transformers.AutoModel.from_pretrained(config['model_name'])\r\n    self.bert_config = transformers.AutoConfig.from_pretrained(config['model_name'])\r\n\r\n    self.pre_classifier = torch.nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\r\n\r\n    self.classifier = torch.nn.Linear(self.bert_config.hidden_size, self.num_labels)\r\n\r\n    self.dropout = torch.nn.Dropout(self.bert_config.hidden_dropout_prob)\r\n    # print(f'Using Dropout = {self.bert.config.seq_classif_dropout}')\r\n\r\n    self.relu = torch.nn.ReLU()\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | freeze the layers of Bert for training if needed so that |   \r\n    | the embeddings of all layers of Bert are not changed     |\r\n    |__________________________________________________________|\r\n    '''\r\n    # for param in self.bert.parameters():\r\n    #   param.requires_grad = False\r\n\r\n  \r\n  def forward(self, batch):\r\n\r\n  #   print((batch['input_ids'].squeeze(1)).shape)\r\n  #   print(\"*\"*10)\r\n  #   print(batch['input_ids'])\r\n  #   print(\"*\"*10)\r\n    \r\n    outputs = self.bert(input_ids=batch['input_ids'].squeeze(1), \r\n                        attention_mask=batch['attention_mask'])\r\n    \r\n    # output from last hidden layer\r\n    hidden_state = outputs[0]  # (batch_size, seq_len, dim)\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | *output of [CLS] token                                   |\r\n    |                                                          |\r\n    | [CLS] token contains the pooled embeddings of the entire | \r\n    | Sequence, these are used for the classification.         |\r\n    |__________________________________________________________|\r\n    '''\r\n    pooled_output = hidden_state[:, 0] # (batch_size, dim)\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | sending the [CLS] token embeddings through Linear, ReLU  |\r\n    | and Dropout layers                                       |\r\n    |__________________________________________________________|\r\n    '''\r\n    pooled_output = self.pre_classifier(pooled_output)  # (batch_size, dim)\r\n    pooled_output = self.relu(pooled_output)  # (batch_size, dim)\r\n    pooled_output = self.dropout(pooled_output)  # (batch_size, dim)\r\n    logits = self.classifier(pooled_output)  # (batch_size, num_labels)\r\n\r\n    return logits\r\n\r\n  def get_outputs(self, input_ids, attention_mask):\r\n    outputs = self.bert(input_ids=input_ids, \\\r\n                        attention_mask=attention_mask)\r\n    \r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10897",
    "createdAt": "2021-12-02T12:37:46Z",
    "updatedAt": "2022-06-21T07:39:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "spranjal25"
    },
    "answer": {
      "body": "since your `model` is an instance of your `LightningModule` it cannot rely on `model.forward` to generate the predictions because `predict_step` by default calls `LightningModule.predict`.\r\nyou need to either override predict_step\r\n```py\r\ndef predict_step(...):\r\n    return self.model(...)\r\n```\r\nor define forward method in your lightning module\r\n```py\r\ndef forward(...):\r\n    return self.model(...)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-02T12:47:36Z"
    }
  },
  {
    "title": "Found mistake in pytorch-lightning DQN example. How do I upload a fix?",
    "body": "I am learning deep RL, and as an exercise I looked through and tried to implement this example of DQN in pytorch lightning:\r\n\r\n[DQN example](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/reinforce-learning-DQN.html)\r\n\r\nI believe that I have found a bug and I am not sure how to flag it or how to upload a fix myself. This is my first time trying to contribute to an open source project and any advice will be greatly appreciated.\r\n\r\nOne line 106 of the section for the DQN Lightning Module, the expression for epsilon (which decays over the first eps_last_frame steps) is incorrect. The code is currently:\r\n`\r\nepsilon = max(\r\n    self.hparams.eps_end,\r\n    self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\r\n)`\r\n\r\nI believe this is incorrect for the following reason: 0 <= epsilon <= 1, so self.global_step immediately overcomes the other two terms. There is no decay, only a single timestep at `eps_start + 1/eps_last_frame`, and then timesteps at `eps_end`. This also seems like an error because with the default values,  `eps_start + 1/eps_last_frame > 1`. \r\n\r\nThe intended behavior is for the epsilon to decay linearly from `eps_start` to `eps_end` over the first `eps_last_frame` frames. I believe the second argument to max() should be:\r\n\r\n`self.hparams.eps_start - (self.global_step / self.hparams.eps_last_frame) * (self.hparams.eps_start - self.hparams.eps_end)`\r\n\r\nWho do I contact, or is there a way I can upload this fix myself?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10883",
    "createdAt": "2021-12-01T19:30:33Z",
    "updatedAt": "2022-08-02T06:08:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "carsondenison"
    },
    "answer": {
      "body": "Hello! This fix would be done in the Lightning Tutorials repo here: https://github.com/PyTorchLightning/lightning-tutorials\r\n\r\n1. Fork the repo (button on top right)\r\n2. Clone the repo from your fork\r\n2. Go to the DQN example source code/notebook\r\n3. Edit with your proposed fix\r\n4. commit the changes and push them to your fork\r\n5. Then go to the https://github.com/PyTorchLightning/lightning-tutorials/pulls and open a Pull Request. \r\n\r\nLet me know if that helps and thanks for checking out the tutorials and the help!",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-03T12:50:43Z"
    }
  },
  {
    "title": "Any guide on how the callbacks and hooks workflow works?",
    "body": "Hi!\r\nI have been working with PyTorch lightning for a year or so, but I am still confused (probably because I am not a software developer and my development skills are not really sharp). \r\nDo any of you know of any guide or reference on how callbacks and hooks interact with each other? Some of my particular questions are:\r\n\r\n- While training, where (or in which order) each callback (train_step, train_epoch_start...) is called?\r\n- How exactly do the hooks work?\r\nI am aware that maybe this is a really basic question, but any advice on how to better understand the workflow would be great!\r\nThanks in advance, you rock guys!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10869",
    "createdAt": "2021-12-01T10:46:22Z",
    "updatedAt": "2022-06-27T07:14:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "hi!\r\n\r\n- There is a gif here representing the call order for some of the hooks. We are planning to add more info regarding this inside the docs. https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html\r\n- You can consider hooks as just normal methods of objects that are called at certain points within the code. For eg when training starts it calls `on_train_start`, when training epoch starts it calls `on_train_epoch_start` etc... The hook name can also help you determine where it's called.\r\n\r\nFor a complete cycle, there can be just one lightning module, so hooks related to the lightning module will be called only once whenever they are required, but in the case of callbacks, it can be many so it sequentially makes the calls to the same hook at that time for each callback. You can read about callbacks and hooks here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-01T12:00:58Z"
    }
  },
  {
    "title": "How many effective workers does my code use?",
    "body": "If I select X workers in my dataloader, and I train my model using ddp with Y GPUs, is the effective amount of workers running in the machine X*Y or X?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10853",
    "createdAt": "2021-11-30T19:41:20Z",
    "updatedAt": "2022-06-11T17:06:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "miraodasilva"
    },
    "answer": {
      "body": "Yes! The script will launch Y times in a separate process. Each process will then create their own dataloader with as many workers as given. So X*Y workers in total accessing your filesystem.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-12-03T12:46:29Z"
    }
  },
  {
    "title": "checkpoint every module in a different ckpt file",
    "body": "Hi!\r\nI am currently working on a project where I would like to checkpoint my model in separated pieces. \r\nMy model has a backbone composed by:\r\n\r\n- a backbone, which is also composed by 3 modules\r\n- several heads, each one being a module\r\nI would like to save one ckpt with the backbone and one ckpt per head. I understand that I should create a custom callback inheriting from ModelCheckpoint and then modifying on_save_checkpoint, I am not really aware of how to do it.\r\non_save_checkpoint is defined as: \r\n\r\n![image](https://user-images.githubusercontent.com/95293295/144058369-9930d133-734a-46cc-9413-9c663a4d6ac9.png)\r\n \r\nAnother solution would be to modify my lightning module to load the ckpt when the training ends as a dict and then save each subpart as a ckpt using torch.save(), but I understand that this solution is much less elegant.\r\n\r\nAny suggestions? Thanks in advance!\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10840",
    "createdAt": "2021-11-30T13:46:47Z",
    "updatedAt": "2023-01-17T23:31:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "malfonsoarquimea"
    },
    "answer": {
      "body": "I'd suggest using [checkpoint_io](https://pytorch-lightning.readthedocs.io/en/latest/advanced/checkpoint_io.html) plugin for your use-case.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-30T14:16:02Z"
    }
  },
  {
    "title": "RichProgressBar is hard to read in light theme",
    "body": "As shown in the screenshot,RichProgressBar is hard to read in light theme.  I don't have much time to report a bug, so I open a discussion temporary.\r\n<img width=\"1917\" alt=\"Screen Shot 2021-11-30 at 10 47 40\" src=\"https://user-images.githubusercontent.com/13161779/143977084-5182f702-6f41-4d39-85b3-1b202f5db309.png\">\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10833",
    "createdAt": "2021-11-30T02:51:17Z",
    "updatedAt": "2022-07-25T11:07:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tshu-w"
    },
    "answer": {
      "body": "Thanks @tshu-w fr reporting the issue! I will look into if it's possible to detect theme in interactive environments.\r\n\r\nTill then you could customize the `description` color by customizing `RichProgressTheme`\r\n\r\n```python\r\ntrainer = Trainer(..., callbacks=RichProgressBar(theme=RichProgressBarTheme(description=\"black\")))\r\n```\r\n\r\nDo check [RichProgressTheme](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/progress/rich_progress.py#L166) out to customize styles for various components.",
      "author": {
        "login": "kaushikb11"
      },
      "createdAt": "2021-11-30T15:03:29Z"
    }
  },
  {
    "title": "Is there a way to save only part of the Lightning sub-modules to the checkpoint file?",
    "body": "I'll explain: Let's say that I have two nn.modules inside my main LightningModule, but one of them is frozen, i.e. doesn't learn during the training but is only used for inferencing during training (requires_grad is False in this module) and I would like to avoid saving the state_dictionray of this static (frozen) module to the checkpoint file.\r\n\r\nIn plain PyTorch I'd probably filter manually the state_dictionray fields of the frozen module before the saving.  \r\nIs there a simple way to do that with pytorch-lightning? Or to raise some flag inside the modules which say to the LightningModule not to save all the parameters inside this frozen module?\r\n#\r\nA simple toy example for clarification.\r\nIn this example, I'd like to avoid saving the parameters of self.frozen_nn_module. \r\nAll parameters in self.frozen_nn_module don't require grads. \r\n\r\n```\r\nclass LightMod(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        #some non frozen module  \r\n        self.non_frozen_nn_module = non_frozen_nn_module\r\n        #some frozen(static) nn.Module\r\n        self.frozen_nn_module= frozen_nn_module\r\n\r\n    def forward(self, x):\r\n    Some code....\r\n```\r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10808",
    "createdAt": "2021-11-29T10:40:45Z",
    "updatedAt": "2022-09-14T07:23:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ahikaml"
    },
    "answer": {
      "body": "you have to do that here too.\r\nwithin lightning you can override `on_save_checkpoint` hook of LightningModule.\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n    checkpoint['state_dict'] <- remove/pop keys from here\r\n``` ",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-29T18:38:43Z"
    }
  },
  {
    "title": "Does .predict() also use the best weights?",
    "body": "On https://pytorch-lightning.readthedocs.io/en/latest/starter/converting.html, it says that \".test() loads the best checkpoint automatically\". Is that also the case for .predict()? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10795",
    "createdAt": "2021-11-28T15:40:17Z",
    "updatedAt": "2021-12-12T12:15:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kaare-mikkelsen"
    },
    "answer": {
      "body": "yes, by default it does load the best checkpoint if you don't provide the model, you can set it too if you want!\r\n```py\r\ntrainer.predict(ckpt_path='best')\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-28T17:56:27Z"
    }
  },
  {
    "title": "What if I load data in __init__ function of LightningDataModule",
    "body": "Hi,\r\n\r\nI see the doc describing the functions of LightningDataModule.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/datamodules.html#Defining-The-MNISTDataModule\r\n\r\nHere is my thinking. If some variable, e.g., a transform, can be defined in __init__ function, and later shared across different GPUs. Theoretically, if we load data in __init__, the data should also be able to transfer to different GPUs similarly. In the case of a single machine with multiple GPUs, the data will be copied multiple times in the memory. In the case of multiple machines, the data will broadcast through the network from the main node to other nodes. Broadcasting large data through networks may have efficiency issue, which is why we had better load data in the setup function.\r\n\r\nPlease let me know whether my analysis is correct or not. Basically, I am not clear about how the variables, e.g., i.e. self.something, defined in __init__ are shared across multiple GPUs/machines. Thanks!\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10772",
    "createdAt": "2021-11-26T06:42:00Z",
    "updatedAt": "2022-09-01T17:34:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhiqiangdon"
    },
    "answer": {
      "body": "@zhiqiangdon \r\n1. lightning just runs `prepare_data` on the main process before the distributed process actually starts so there is no blocking happening behind the scenes.\r\n2. To tackle this issue we have [prepare_data_per_node](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#prepare-data-per-node). A node is just a machine. If they share the disk then `prepare_data_per_node` should be set to False.\r\n3. User runs the `__init__` function when they initialize the DataModule, lightning just send to across devices.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-26T21:16:50Z"
    }
  },
  {
    "title": "UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.",
    "body": "It is strange that my `val_dataloader`'s `shuffle` is set to `False`, but I still get this warning. Any ideas on how to solve this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10771",
    "createdAt": "2021-11-26T05:21:08Z",
    "updatedAt": "2022-06-15T12:04:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hiyyg"
    },
    "answer": {
      "body": "I know what the bug is, the sampler used for val_dataloader is not `SequentialSampler` but it does not do any shuffling.",
      "author": {
        "login": "hiyyg"
      },
      "createdAt": "2021-11-26T06:31:37Z"
    }
  },
  {
    "title": "How to do model comparison with pytorch lightning",
    "body": "I want to test performance of my model on a few different feature sets and visualize losses of different features on the same plot in tensorboard. \r\nMy current work flow is, \r\n\r\n```\r\nclass LightningWrapper(pl.LightningModule):\r\n    def__init__(self, features):\r\n        self.features = features \r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ....\r\n        .....\r\n        self.logger.experiment.add_scalars(\"version_0\", { f\"{self.features}\" : loss})\r\n\r\ntrainer = pl.Trainer()\r\nfor feature_set in potential_feature_sets:\r\n    model = LightningWrapper(feature_set)\r\n    trainer.fit(model)\r\n```\r\n\r\nBut this is creating different versions inside tensorboard logging. Does each call to Lightning module create a new version ? Is this because the loggers are not shared across feature sets in my current design ? How do I share logger across different feature sets or different models ?\r\n\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10764",
    "createdAt": "2021-11-25T16:59:34Z",
    "updatedAt": "2023-07-07T07:29:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ajayrfhp"
    },
    "answer": {
      "body": "Found a rather simple solution that worked for me. Put different features/different models under different directories and fire up tensorboard in root directory. \r\n\r\n```\r\nclass LightningWrapper(pl.LightningModule):\r\n    def__init__(self, features):\r\n        self.features = features \r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ....\r\n        .....\r\n        self.logger.experiment.add_scalar(\"loss\", loss)\r\n\r\n\r\nfor feature_set in potential_feature_sets:\r\n    logger = pl.loggers.TensorBoardLogger(f\"lightning_logs/{num_features}\", version=\"0\")\r\n    trainer = pl.Trainer(logger=logger)\r\n    model = LightningWrapper(feature_set)\r\n    trainer.fit(model)\r\n```\r\n",
      "author": {
        "login": "ajayrfhp"
      },
      "createdAt": "2021-11-25T21:54:31Z"
    }
  },
  {
    "title": "After changing to pytorch-lightning 1.5.2, omitting the argument of trainer.test() does not work.",
    "body": "In case of pytorch-lightning 1.5.2, omission of argument of `trainer.test()` does not work.\r\n\r\n```python\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n```\r\n\r\nIf it is below, it worked fine. Is this the expected behavior?\r\n\r\n```python\r\n        trainer.fit(model, datamodule)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n```\r\n\r\nI'm currently refactoring the following:\r\n\r\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10747",
    "createdAt": "2021-11-25T04:28:54Z",
    "updatedAt": "2024-04-04T14:20:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "yes, you need to pass the datamodule. I think this was the case after v1.5 and not just v1.5.2.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-25T12:14:47Z"
    }
  },
  {
    "title": "Doing the sampling of batch indexes inside lightning",
    "body": "I am trying to port my very old pytorch code to lightning and in my training loop, I have something as follows:\r\n\r\n```\r\nbatch_order = np.arange(data.x_train.shape[0])\r\nbatch_index = np.random.choice(batch_order, batch_size, p=seq_sample_probs).tolist()\r\nbatch = torch.tensor(data.x_train[batch_index], dtype=torch_dtype, device=torch_device, requires_grad=False)\r\n\r\n# then call the forward on this batch\r\nmodel.encoder.forward(batch)\r\n```\r\n\r\nI was wondering how I can incorporate this batch index selection in the lightning code. In my code, I have the usual:\r\n\r\n```\r\ndef train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\r\n```\r\n\r\nBut I do not know where I can inject my sampling code inside all this.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10741",
    "createdAt": "2021-11-24T20:14:56Z",
    "updatedAt": "2023-08-03T09:15:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pamparana34"
    },
    "answer": {
      "body": "I think that's the job of the PyTorch sampler.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-25T12:10:07Z"
    }
  },
  {
    "title": "Can I use \"torch.autograd.grad\" in \"training_step\" funtion?",
    "body": "Hello! \r\n\r\nI hope to **calculate the  gradient of the loss on the model input** every batch when training. The calculated gradients are then processed by some other functions and added into the loss function. The way I do this  in pytorch is like this, with the `torch.autograd.grad` function:\r\n\r\n```python\r\nmodel_input.requires_grad = True\r\nmodel.zero_grad()\r\nmodel.eval()\r\ngrads = torch.autograd.grad(loss, model_input, grad_outputs=None, only_inputs=True, retain_graph=False)[0]\r\nmodel.train()\r\n```\r\n\r\nCan I add these codes directly into the `training_step` function in pytorch-lightning? \r\n\r\nMy concern is: \r\n1. Will this gradient calculation (`torch.autograd.grad` function)  influence the accuracy of model training since I add it in the `training_step` function? \r\n2. Do I need to set `model.zero_grad()`,   `model.eval()`  and `model.train()`  when calculating the gradients on the input?\r\n\r\nThank you very much!\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10731",
    "createdAt": "2021-11-24T13:43:32Z",
    "updatedAt": "2022-08-03T02:55:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "chenshuang-zhang"
    },
    "answer": {
      "body": "> 1. Will this gradient calculation (`torch.autograd.grad` function)  influence the accuracy of model training since I add it in the `training_step` function?\r\n\r\nYes, I believe. I don't think it'll work with `retain_graph=False` there because backward pass uses the graph to compute gradients wrt weights using the `loss` returned from `training_step`.\r\n\r\n> 2. Do I need to set `model.zero_grad()`,   `model.eval()`  and `model.train()`  when calculating the gradients on the input?\r\n\r\nYes, partially. I think you need `zero_grad()` after `grads = torch.autograd.grad(...)` to avoid accumulating gradients wrt weights, but I'm not sure why you need `eval()` and `train()`.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-11-26T07:53:54Z"
    }
  },
  {
    "title": "How does LightningLite handle the grad scaler state dict of torch.amp?",
    "body": "Is the grad scaler included in the `model.state_dict()` after steup by `model, optimizer = self.setup(model, optimizer)`?\r\n\r\nIf not, how can I save and load the state of the grad scaler for resume?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10727",
    "createdAt": "2021-11-24T09:43:36Z",
    "updatedAt": "2022-07-09T07:58:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hiyyg"
    },
    "answer": {
      "body": "Hey @hiyyg.\r\n\r\nYou can access the scale through the precision plugin as follows:\r\n\r\n```py\r\nself._precision_plugin.scaler\r\n```\r\n\r\nI believe you could get the state and reload it manually.\r\n\r\nBest,\r\nT.C\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-11-24T12:27:35Z"
    }
  },
  {
    "title": "[RFC] Thoughts on `on_init_start` and `on_init_end` hooks",
    "body": "These hooks are called when trainer initialization begins and ends, before the model has been set, essentially allowing the user to modify the Trainer constructor.  Should we be giving the user this much control over Trainer constructor? Are there scenarios where this is needed? Or can we deprecate these hooks?\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/338f3cf63686935355c749920b2f298f3d18a26f/pytorch_lightning/trainer/callback_hook.py#L55-L63\r\n\r\ncc @ananthsub ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10677",
    "createdAt": "2021-11-22T17:41:07Z",
    "updatedAt": "2022-10-04T11:20:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "daniellepintz"
    },
    "answer": {
      "body": "@carmocca @tchaton @awaelchli do you know how these hooks are used? Have you seen any examples of these being used by the community? These hooks go way way back, but I can't think of when they'd be needed given the user \"owns\" the Trainer initialization. It's also unclear when `on_init_start` actually happens: does that mean callbacks should be the first thing initialized? \r\n\r\nit seems a lot more straightforward to write this:\r\n```python\r\ntrainer = Trainer(...)\r\nrun_all_my_fancy_logic_now(trainer)\r\n\r\n# use the trainer here\r\n```\r\n\r\nLet's discuss in https://github.com/PyTorchLightning/pytorch-lightning/issues/10894",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-12-02T03:23:29Z"
    }
  },
  {
    "title": "After updating to 1.5.2, NotImplementedError: `train_dataloader`",
    "body": "When I update the following repository to 1.5.2, I get the following error. Is there an implementation guide somewhere?\r\n\r\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10\r\n\r\n```\r\n(PyTorch-Lightning-CIFAR10) \u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\npipenv run python train.py +experiments=train_exp01 hydra.run.dir=outputs/train_exp01\r\n\r\nGlobal seed set to 0\r\nGPU available: True, used: False\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\r\n  rank_zero_warn(\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 68, in main\r\n    trainer.fit(model,\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1141, in _run\r\n    self.accelerator.setup(self)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu.py\", line 35, in setup\r\n    return super().setup(trainer)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 93, in setup\r\n    self.setup_optimizers(trainer)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 351, in setup_optimizers\r\n    optimizers, lr_schedulers, optimizer_frequencies = self.training_type_plugin.init_optimizers(\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 245, in init_optimizers\r\n    return trainer.init_optimizers(model)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/optimizers.py\", line 35, in init_optimizers\r\n    optim_conf = self.call_hook(\"configure_optimizers\", pl_module=pl_module)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1496, in call_hook\r\n    output = model_fx(*args, **kwargs)\r\n  File \"/mnt/nfs/kuroyanagi/clones/PyTorch-Lightning-CIFAR10/model.py\", line 73, in configure_optimizers\r\n    total_steps = cfg.train.num_epochs * len(self.train_dataloader())\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/hooks.py\", line 477, in train_dataloader\r\n    raise NotImplementedError(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")\r\nNotImplementedError: `train_dataloader` must be implemented to be used with the Lightning Trainer\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n(PyTorch-Lightning-CIFAR10) \u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f \r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10652",
    "createdAt": "2021-11-20T02:41:25Z",
    "updatedAt": "2022-06-02T08:43:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "for this problem, tracking issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/10430",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-21T18:20:04Z"
    }
  },
  {
    "title": "What is the relationship beween accumulate_grad_batches and lr_scheduler?",
    "body": "I wrote following code:\r\n```python\r\n    def configure_optimizers(self):\r\n        ......\r\n        return [\r\n            {\r\n                'optimizer': optimizer,\r\n                'lr_scheduler': {\r\n                    'scheduler': scheduler,\r\n                    'interval': 'step',\r\n                    'frequency': 1\r\n                }\r\n            }\r\n```\r\nI choose `step` as the `interval`. Actually, **I don't understand what `step` means**!!!\r\n\r\nIn my opinion, `step` may mean a batch? But when I set Trainer parameter: ` accumulate_grad_batches=5`, will `lr_scheduler` still execute after one batch or it only execute after every ` accumulate_grad_batches` batches? If the answer is the later, so the `step` means the call of `optimizer.step()`?\r\n\r\n(I know `accumulate_grad_batches` can affect `optimizer`, but I don't know whether it can affect `lr_scheduler`)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10651",
    "createdAt": "2021-11-20T02:09:08Z",
    "updatedAt": "2024-04-15T02:45:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yc1999"
    },
    "answer": {
      "body": "yes, `step` means optimization step and `accumulate_grad_batches` will be taken under consideration while calling the lr_scheduler.\r\nRef code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/8ea39d2c8f68cc33273c3431a310a262e2240cf9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L434-L437",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-21T18:23:55Z"
    }
  },
  {
    "title": "Trainer.fit validating before finishing current training epoch",
    "body": "When the train.fit() starts to train my model, it starts validating at 95% of training of current epoch instead of waiting until 100%.\r\n\r\nWhy does it happen? It makes Val-loss/acc/miou not accurate anymore\u2026\r\n![79A18250-AF4C-4F48-BB41-EB930C6ACA7F](https://user-images.githubusercontent.com/75472853/142167568-d02d1ae6-5886-4d9e-bd06-7bc37e3a3e28.jpeg)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10586",
    "createdAt": "2021-11-17T08:49:31Z",
    "updatedAt": "2021-11-18T09:40:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "minwang-ai"
    },
    "answer": {
      "body": "the master progress bar that shows you the progress of the corresponding epoch consists of both train and val steps. So it completes the training within the first 95% of the total steps and the last 5% is completed for validation.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-17T20:16:22Z"
    }
  },
  {
    "title": "Test results different between logging in test_step and logging in test_epoch_end",
    "body": "Why do these two test codes result in different test results(both average acc and average loss)?\r\n\r\n1. \r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    input_ids, labels = batch\r\n    outs = self(input_ids)\r\n    loss = self.loss_fn(outs, labels)\r\n    acc = self.acc_fn(outs, labels)\r\n    self.log_dict({'test_loss': loss, 'test_acc': acc}, on_step=False, on_epoch=True, logger=False)\r\n    return loss, acc\r\n```\r\n\r\n2. \r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    input_ids, labels = batch\r\n    outs = self(input_ids)\r\n    loss = self.loss_fn(outs, labels)\r\n    acc = self.acc_fn(outs, labels)\r\n    return loss, acc\r\n\r\ndef test_epoch_end(self, step_outputs):\r\n    avg_loss = torch.stack([x[0] for x in step_outputs]).mean()\r\n    avg_acc = torch.stack([x[1] for x in step_outputs]).mean()\r\n    self.log_dict({'test_loss': avg_loss, 'test_acc': avg_acc}, logger=False)\r\n```\r\n\r\nI only use one GPU for testing.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10517",
    "createdAt": "2021-11-13T06:44:12Z",
    "updatedAt": "2022-06-02T14:14:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Bowen-n"
    },
    "answer": {
      "body": "just in case anyone else sees this discussion, adding more context to @Bowen-n answer. Within lightning we use a weighted average to accumulate the results at the epoch end where weights are the batch_size for each batch inside `test_step`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-14T19:19:40Z"
    }
  },
  {
    "title": "How to access `LightningDataModule` in `LightningModule`",
    "body": "In [TorchGeo](https://github.com/microsoft/torchgeo), we use PyTorch Lightning to organize reproducible benchmarks for geospatial datasets. Currently, we have a set of LightningDataModules for each dataset and a much smaller number of LightningModules for each task (semantic segmentation, classification, regression, etc.). Each Dataset defines its own `plot()` method that describes how to plot images and masks.\r\n\r\nDuring training/validation steps, we would like to plot a few examples to see how training is progressing. However, the LightningModule doesn't seem to know anything about the LightningDataModule/DataLoader/Dataset. Because of this, if we want to perform dataset-specific plotting during training or validation steps, we're forced to create a separate LightningModule for each dataset, increasing code duplication and defeating the whole purpose of PyTorch Lightning ([example](https://github.com/microsoft/torchgeo/blob/main/torchgeo/trainers/chesapeake.py)).\r\n\r\nIs there an easy way for a LightningModule to tell which DataModule/DataLoader/Dataset is being used and call its `dataset.plot()` method?\r\n\r\n@calebrob6 @isaaccorley\r\n\r\n@tchaton this is slightly related to #10469 but different enough that I wanted to start a separate discussion about it.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10492",
    "createdAt": "2021-11-11T21:36:39Z",
    "updatedAt": "2022-06-14T14:35:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "adamjstewart"
    },
    "answer": {
      "body": "@adamjstewart There is a reference to datamodule via trainer from LightningModule, but would that solve your issue?\r\n```python\r\nself.trainer.datamodule\r\n```\r\n",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-11-11T23:35:20Z"
    }
  },
  {
    "title": "Early stop saving best checkpoints",
    "body": "Hi all, do you know how to save the best model? Since pytorchlighting 's earlystop callback will monitor val_loss and if val_loss stop decreasing,\u00a0it will stop training automaticlly. In this case, the checkpoint of the final model would be the final epoch (the val_loss starts to increase). Can I save epoch 5 or 6 (before val_loss increasing) as the best model?\r\n![CC51157F-0EBE-4D35-B343-5C85F17A4EC2](https://user-images.githubusercontent.com/75472853/140654686-02726372-16fd-40b4-9d7d-ee323ae500ba.jpeg)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10399",
    "createdAt": "2021-11-07T17:11:46Z",
    "updatedAt": "2022-06-01T10:43:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "minwang-ai"
    },
    "answer": {
      "body": "you can specify\r\n```py\r\nModelCheckpoint(monitor='val_loss', save_top_k=1, ...)\r\n```\r\nthis will save the best model.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-11-08T18:28:12Z"
    }
  },
  {
    "title": "Lightning is very slow - Performance divided by ~4 compared to Pytorch. 10s wait between epochs.",
    "body": "I converted some Pytorch code to Lightning. The dataset is loaded lazily by the train & eval dataloaders.\r\n\r\nHowever, when moving the code to Lightning, I noticed a huge slowdown. After digging around, I noticed that there was a ~10 seconds delay between each epoch. For comparison, on my vanilla Pytorch, an epoch takes ~4s.\r\n\r\nI first thought it was a data loading problem, but during the 10s delay, no data is loaded (at least that's what my `print` tell me).\r\n\r\nI think the issue is related to the number of workers, because setting `n_workers=0` solves the problem (but is slower in the end, since only one worker is not enough). I know starting workers is slow, however I have `persistent_workers=True` and this does not happen in normal Pytorch. My data loaders also have `pin_memory=True` (removing pin_memory does not solve the problem).\r\n\r\nSince this is company code, I cannot disclose the before/after, but I'll try to \"anonymize\" some code if necessary. Here is the lightning module:\r\n```py\r\nclass RawModule(pl.LightningModule):\r\n    def __init__(self):\r\n        super(RawModule, self).__init__()\r\n\r\n        self.encoder1 = nn.Sequential(...)\r\n        self.encoder2 = nn.Sequential(...)\r\n\r\n    def forward(self, data1, data2):\r\n        result1 = self.encoder1(data1)\r\n        result2 = self.encoder2(data2)\r\n\r\n        result1 = result1 .view(result1 .size(0), -1)\r\n        result2 = result2 .view(result2 .size(0), -1)\r\n\r\n        result1 = F.normalize(result1 , p=2, dim=1)\r\n        result2 = F.normalize(result2 , p=2, dim=1)\r\n\r\n\r\n        return result1, result2\r\n\r\n    \r\n    def calculate_loss(self, batch):\r\n        x, r, y = batch\r\n        a, v = self.forward(r, x)\r\n\r\n        d = nn.functional.cosine_similarity(a, v)\r\n        loss = logloss(d.unsqueeze(1), y)\r\n\r\n        return loss\r\n\r\n\r\nclass Module(RawModule):\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.calculate_loss(batch)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.calculate_loss(batch)\r\n        self.log(\"validation_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\r\n        return optimizer\r\n\r\n\r\nif __name__ == '__main__':\r\n    # stuff...\r\n\r\n    train_loader = data_utils.DataLoader(\r\n        train_dataset, batch_size=256, shuffle=True,\r\n        num_workers=5, persistent_workers=True,\r\n        pin_memory=True,\r\n    )\r\n\r\n    val_loader = data_utils.DataLoader(\r\n        test_dataset, batch_size=256,\r\n        num_workers=2, persistent_workers=True,\r\n        pin_memory=True,\r\n    )\r\n\r\n    # Model\r\n    load_from_pytorch = True\r\n\r\n    if checkpoint_path is None:\r\n        model = Module()\r\n\r\n        if load_from_pytorch:\r\n            if not checkpoint_path:\r\n                raise ValueError(\"Please provide a checkpoint path\")\r\n            model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\r\n    else:\r\n        model = Module.load_from_checkpoint(checkpoint_path)\r\n\r\n\r\n    trainer = pl.Trainer(\r\n        gpus=1,\r\n        max_epochs=5,\r\n        check_val_every_n_epoch=10,\r\n        log_every_n_steps=5,\r\n    )\r\n    trainer.fit(model, train_loader, val_loader)\r\n```\r\n\r\nHere is the result of `profiler=\"simple\"`: \r\n```\r\nAction                                  |  Mean duration (s)    |Num calls              |  Total time (s)       |  Percentage %         |\r\n----------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                                   |  -                    |_                      |  48.813               |  100 %                |\r\n----------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                      |  27.922               |1                      |  27.922               |  57.202               |\r\nfetch_next_sanity_check_batch           |  4.4013               |3                      |  13.204               |  27.05                |\r\nget_sanity_check_batch                  |  4.4013               |3                      |  13.204               |  27.05                |\r\nfetch_next_train_batch                  |  1.2734               |10                     |  12.734               |  26.087               |\r\nget_train_batch                         |  1.2734               |10                     |  12.734               |  26.087               |\r\nrun_training_batch                      |  0.47733              |9                      |  4.296                |  8.8009               |\r\noptimizer_step_with_closure_0           |  0.40089              |9                      |  3.608                |  7.3915               |\r\nvalidation_step                         |  0.664                |2                      |  1.328                |  2.7206               |\r\nevaluation_step_and_end                 |  0.664                |2                      |  1.328                |  2.7206               |\r\ntraining_step_and_backward              |  0.12644              |9                      |  1.138                |  2.3313               |\r\nbackward                                |  0.096889             |9                      |  0.872                |  1.7864               |\r\ntraining_step                           |  0.029556             |9                      |  0.266                |  0.54494              |\r\nmodel_forward                           |  0.029556             |9                      |  0.266                |  0.54494              |\r\non_train_start                          |  0.016                |1                      |  0.016                |  0.032778             |\r\n```\r\n\r\nHere is the result of `profiler=\"advanced\"`: [https://pastebin.com/q3C5P826](https://pastebin.com/q3C5P826).\r\n\r\nFinally, here is a video demonstrating the problem. I'm printing each piece of data loading, to prove it's not the issue.\r\nhttps://user-images.githubusercontent.com/30944236/140587623-ae184fa3-370a-42be-8593-200026d11ba4.mp4\r\n\r\nRandom informations:\r\n* I'm on Windows 10\r\n* CPU: AMD Ryzen 5 5600X 6 Core\r\n* GPU: Nvidia RTX 3070\r\n* Pytorch version: 1.10.0\r\n* Pytorch Lightning version: 1.5.0\r\n\r\nAny idea on how to find the source of the problem?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10382",
    "createdAt": "2021-11-05T22:36:51Z",
    "updatedAt": "2022-06-08T15:59:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "TheMrZZ"
    },
    "answer": {
      "body": "Fixed in the 1.5.1 release. See the #10389 issue, or the release itself.",
      "author": {
        "login": "TheMrZZ"
      },
      "createdAt": "2021-11-10T13:21:50Z"
    }
  },
  {
    "title": "Questions about Fault-tolerant Training",
    "body": "Hi! I'm working on a SLURM cluster with preemption, so I'm really excited to see the support of Fault-tolerant Training in 1.5.0. However, when I upgrade package and try `PL_FAULT_TOLERANT_TRAINING=1 python train.py xxx` in the cluster, it doesn't seem to work.\r\n\r\nI look into the code of `Trainer`, it seems that the code responsible for fault-tolerant is [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L668). I assume preemption is a `BaseException` so the code will go to [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L693) and finally [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L1550) so that we save a checkpoint?\r\n\r\nHowever, when set some print in the code, when I use ctrl+C to interrupt code, it indeed goes to [this](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L681) `KeyBoardInterrupt`. But if I use `scontrol requeue` to simulate a preemption, the code didn't got to `BaseException`. And that's why it didn't save a checkpoint for Fault-tolerant Training.\r\n\r\nIs there anything wrong with my code? I assume interruptions like `scancel` `requeue` are considered in this case. Can anyone help me? Thank you in advance!\r\n\r\nEDIT: I've looked in the code a little bit more, it seems that when I do `scancel` or `scontrol requeue`, the code directly exit, without throwing an exception, and that's why it didn't go to the `except  _on_exception` section. Is this expected behavior? Or is there anyway to solve it?\r\n\r\nI think that's related to the signal that SLURM sent to my program, and I already see a `SignalConnector` dealing with SLURM in pytorch-lightning [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/connectors/signal_connector.py#L40). I also see [this answer](https://stackoverflow.com/questions/65326039/how-to-handle-job-cancelation-in-slurm) about the signal of SLURM. Maybe I should set it in the sbatch script? Any suggestions?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10380",
    "createdAt": "2021-11-05T20:43:27Z",
    "updatedAt": "2022-06-13T15:38:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Wuziyi616"
    },
    "answer": {
      "body": "Solved. That's indeed because in my SLURM cluster, there is no time interval between signal sending and program killing, so PyTorch-Lightning just don't have time to do checkpointing",
      "author": {
        "login": "Wuziyi616"
      },
      "createdAt": "2021-11-06T05:24:03Z"
    }
  },
  {
    "title": "LightningCLI  - instantiate model from config",
    "body": "So lets say i have a model and i'm using the newest CLI API to train it: The config uses sub modules and can look something like:\r\n\r\n```\r\n# config.yaml\r\nmodel:\r\n  class_path: pl_models_2.ModelPL\r\n  init_args:\r\n    margin: 0.3\r\n    basemodel:\r\n      class_path: pl_models_2.ModelBackbone\r\n      init_args:\r\n        base_model: resnet50\r\n        pooling: both\r\n\r\ndata:\r\n  batch_size: 32\r\n  image_size: 224\r\n  augmentation_strategy: medium2\r\n```\r\n\r\nNow i want  to run some scripts  or notebooks using the model i trained with this config and i would like to  instantiate the model using this config file.\r\nf/e. model = load_model(config.yaml)\r\n\r\nI was digging how this happens in LightningCLI and jsonargsparse but gave up after a while.. and started trying to hack this around with importlib, which works but feels like reinventing the wheel - i mean this logic has to be somewhere already :) i just cant find it.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10363",
    "createdAt": "2021-11-04T21:11:44Z",
    "updatedAt": "2024-06-09T18:53:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "i008"
    },
    "answer": {
      "body": "If you want to use that exact config (not stripping out everything except the model) you can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=ModelClass)\r\nparser.add_argument('--data', type=dict) # to ignore data\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated model will be in `config_init.model`.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2021-11-05T15:17:35Z"
    }
  },
  {
    "title": "Issue in fitting model and finding optimal learning rate parameter",
    "body": "\r\n```py\r\nfollowing is the error: NotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\r\n\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-11-263e8be26564> in <module>()\r\n      3     tft,\r\n      4     train_dataloader=train_dataloader,\r\n----> 5     val_dataloaders=val_dataloader,\r\n      6 )\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/hooks.py in val_dataloader(self)\r\n    590             will have an argument ``dataloader_idx`` which matches the order here.\r\n    591         \"\"\"\r\n--> 592         raise NotImplementedError(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")\r\n    593 \r\n    594     def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n\r\nNotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\r\n```\r\n\r\n```py\r\ntrainer.fit(\r\n    tft,\r\n    train_dataloader=train_dataloader,\r\n    val_dataloaders=val_dataloader,\r\n)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10341",
    "createdAt": "2021-11-03T17:15:51Z",
    "updatedAt": "2022-10-12T08:22:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "geeksouvik"
    },
    "answer": {
      "body": "I think it's a problem with pytorch-lightning==1.5.0.\r\nHad this problem too with code that worked before recreating my venv and the difference was version 1.5.0 release on 2. Nov.\r\nSwitched to 1.4.9 and it worked again.\r\nChecked 1.5.0rc1 and it did not work either. ",
      "author": {
        "login": "furbyhaxx"
      },
      "createdAt": "2021-11-04T09:09:12Z"
    }
  },
  {
    "title": "ModelCheckpoint save nothing",
    "body": "I want to use ModelCheckpoint to save mode while training, however, nothing has been saved. The following is my code. I don't know what leads to this problem, any suggestions?\r\n \r\n![image](https://user-images.githubusercontent.com/22525811/140004044-fbba057f-6932-450e-97ac-1889c59d1d29.png)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10327",
    "createdAt": "2021-11-03T02:56:04Z",
    "updatedAt": "2024-01-24T16:00:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Ryuk17"
    },
    "answer": {
      "body": "I solve the problem by add `self.log('val_loss', val_loss)` in function `validation_epoch_end`.",
      "author": {
        "login": "Ryuk17"
      },
      "createdAt": "2021-11-03T06:53:37Z"
    }
  },
  {
    "title": "How to pass gradients to `backward()`",
    "body": "In my experiment, I have a loss function, which is not defined by an expression. But I have a formulation of the gradient (formally a subgradient) so I have to pass the gradient manually. In pytorch, I implement it in the following way and it is working fine.\r\n\r\n```python\r\nself.optimizer.zero_grad()\r\ny_hat = self.model(x_train)\r\ngrad =  compute_grad(y_hat, y)\r\ny_hat.backward(gradient=grad)\r\nself.optimizer.step()\r\n```\r\nWould the following be a correct implementation in lightning?\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n        opt = self.optimizers()\r\n        x,y = batch\r\n        y_hat =  self(x)\r\n        grad =  compute_grad(y_hat, y)\r\n        opt.zero_grad()\r\n        y_hat.backward(gradient= grad)\r\n        opt.step()\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10257",
    "createdAt": "2021-10-29T22:01:16Z",
    "updatedAt": "2022-05-31T08:20:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JayMan91"
    },
    "answer": {
      "body": "@JayMan91 Haven't tried, but you could probably try manual optimization.\r\n\r\n```python\r\ndef __init__(...):\r\n    ...\r\n    self.automatic_optimization = False  # use manual optimization\r\n    ...\r\n\r\ndef training_step(...):\r\n    ...\r\n    self.manual_backward(y_hat, gradient=grad)\r\n    ...\r\n```\r\n\r\nmanual optimization: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n`LightningModule.manual_backward`: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#manual-backward\r\n",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-11-06T21:07:25Z"
    }
  },
  {
    "title": "Distributed training with multiple optimizers",
    "body": "Hi,\r\n\r\nI have a model with multiple models inside the object (similar to a GAN) except that i want to use a single loss function with multiple optimizers.  I am disabling automatic_optimization using the **automatic_optimization** flag. The code has been implemented and it works for a single GPU configuration.\r\n\r\nIn order to accelerate the training process, i need to use DDP and across 3 GPU devices (1 node, multiple devices). The distributed training succeeds but i am not sure if it is working the way it is supposed to. Is there a way to figure out things?\r\n\r\nMy psuedo code is something like below, cant share the full code due to NDAs:\r\n[sample_model.py.txt](https://github.com/PyTorchLightning/pytorch-lightning/files/7441363/sample_model.py.txt)\r\n\r\nMy query is, for distributed training with multiple optimizers, will the above code work in the INTENDED way? What should **training_step_end** function contain then? and how does multiple optimizers update across different devices?\r\n\r\nThank you\r\n\r\n---\r\nEDIT by @akihironitta\r\n\r\n<details><summary>provided script</summary>\r\n\r\n```python\r\nimport <standard imports>\r\n\r\nclass some_model1(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat1\r\n        \r\n\r\nclass some_model2(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat2\r\n\r\n\r\n\r\nclass some_model3(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat3\r\n\r\n\r\nclass some_model4(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return loss\r\n\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n        self.model1 = some_model1()\r\n        self.model2 = some_model2()\r\n        self.model3 = some_model3()\r\n        self.sub_model = some_model4()\r\n\r\n        self.common = nn.Sequential(some_layers)\r\n\r\n        self.sofmax_layer = nn.Softmax(dim=-1)   \r\n        self.automatic_optimization=False\r\n\r\n    def forward(self, input_dict, output_dict, *args, **kwargs):\r\n\r\n        data1, data2, data3 = input_dict['data1'], input_dict['data2'],input_dict['data3']\r\n\r\n        feat1 = self.model1(data1)\r\n        pred1 = self.common(feat1)\r\n\r\n        feat2 = self.model2(data2)\r\n        pred3 = self.common(feat2)\r\n\r\n        feat3 = self.model3(data3)\r\n        pred3 = self.common(feat3)\r\n\r\n        combined_metrics2 = some metric_calculation based on output_dict\r\n\r\n        return {'combined_metrics1':combined_metrics1, 'combined_metrics2':combined_metrics2}\r\n\r\n    def training_step(self, batch, batch_idx, *args, **kwargs): \r\n        input_dict, output_dict = batch\r\n\r\n        self.optimizers()[0].zero_grad()\r\n        self.optimizers()[1].zero_grad()\r\n\r\n        combined_metrics1, combined_metrics2 = self.forward(input_dict, output_dict)\r\n\r\n        final_loss = some_loss_fn(combined_metrics1, combined_metrics2, output_dict)\r\n\r\n        self.optimizers()[0].zero_grad()\r\n        self.optimizers()[1].zero_grad()\r\n\r\n        self.manual_backward(final_loss)\r\n\r\n\r\n        self.optimizers()[0].step()\r\n        self.optimizers()[1].step()\r\n\r\n        return combined_metrics1, combined_metrics2\r\n\r\n    def training_step_end(self, outputs):\r\n\r\n        return ??\r\n\r\n    def configure_optimizers(self):\r\n        normal_params = list(self.model1.parameters()) + list(self.model2.parameters()) + list(self.model3.parameters())\r\n        self.normal_opt = optim.Adam(\r\n            normal_params,\r\n            lr = 0.001\r\n        )\r\n        self.opt2 = optim.Adam(\r\n            self.sub_model.parameters(),\r\n            lr= 0.001 \r\n        )\r\n        return self.normal_opt, self.opt2 \r\n\r\nif __name__ == '__main__':\r\n    from xyz import TrainDataloader\r\n    from xyz import TestDataloader\r\n\r\n    train_set = TrainDataloader(params, partition='train')\r\n    test_set = TestDataloader(params, partition='test')\r\n\r\n    data_loader_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True,num_workers=1)\r\n    data_loader_test = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True,num_workers=1)\r\n\r\n    class DataModule(pl.LightningDataModule):\r\n        def __init__(self, batch_size: int = 32):\r\n            super().__init__()\r\n            self.batch_size = batch_size\r\n\r\n        def setup(self, stage=None):\r\n            print('Setting up the data loader')\r\n\r\n        def train_dataloader(self):\r\n            return data_loader_loader \r\n\r\n        def val_dataloader(self):\r\n            return data_loader_test \r\n\r\n        def test_dataloader(self):\r\n            return data_loader_test \r\n\r\n        def teardown(self):\r\n            # Used to clean-up when the run is finished\r\n            ...\r\n    dm = DataModule()\r\n\r\n    from pytorch_lightning.loggers import TensorBoardLogger\r\n\r\n    logger = TensorBoardLogger(\"logs\", name=\"Custom\")\r\n\r\n    model = Model(params)\r\n    trainer = pl.Trainer(max_epochs=50, log_every_n_steps=2, gpus=2, accelerator='ddp', logger=logger)\r\n    trainer.fit(model, datamodule=dm) \r\n```\r\n\r\n</detaills>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10241",
    "createdAt": "2021-10-29T11:01:44Z",
    "updatedAt": "2022-08-03T03:16:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "piseabhijeet"
    },
    "answer": {
      "body": "> My query is, for distributed training with multiple optimizers, will the above code work in the INTENDED way?\r\n\r\nYour code looks good to me.\r\n\r\n> What should training_step_end function contain then?\r\n\r\nNo need to do anything if you don't need to run anything at the end of `training_step`.\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#training-step-end\r\n\r\n> how does multiple optimizers update across different devices?\r\n\r\nDDP syncs gradients across different devices overlapping backprop, and each device updates the weights with gradients synced across devices. See the PyTorch documentation for details: https://pytorch.org/docs/1.12/notes/ddp.html",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-08-03T03:16:34Z"
    }
  },
  {
    "title": "Saving one single inference example per validation stage",
    "body": "I would like to save one single inference example per validation stage.  To this end I came up with:\r\n```\r\nclass Model(pl.LightningModule):\r\n\r\n    ...\r\n\r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        # self.trainer.log_dir is not set during fast_dev_test\r\n        if self.trainer.log_dir is not None:\r\n            x, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\r\n            y_hat = self.model(x)\r\n            x, y, y_hat = (d.squeeze().cpu().numpy() for d in (x, y, y_hat))\r\n\r\n            save_dir = Path(self.trainer.log_dir) / 'wavs'\r\n            save_dir.mkdir(exist_ok=True)\r\n\r\n            soundfile.write(save_dir/f'{self.global_step:06}_input.wav',\r\n                    x, 44100, subtype='PCM_24')\r\n            soundfile.write(save_dir/f'{self.global_step:06}_output.wav',\r\n                    y_hat, 44100, subtype='PCM_24')\r\n            soundfile.write(save_dir/f'{self.global_step:06}_target.wav',\r\n                    y, 44100, subtype='PCM_24')\r\n```\r\n\r\nThis works OK when running on a single device (CPU) but when running on GPUs on a slurm cluster I get:\r\n\r\n```\r\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\r\n```\r\nI guess because the data has not been sent to the same device as the model.\r\n\r\nThis seems like a problem that must have been solved before.  Can anyone suggest a typical pytorch-lightning way of doing this?\r\n\r\nThanks for your input\r\nLeo",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10223",
    "createdAt": "2021-10-28T15:45:06Z",
    "updatedAt": "2022-06-21T11:16:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "leoauri"
    },
    "answer": {
      "body": "here:\r\n```py\r\nx, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\r\n```\r\nI think this is something not part of the dataloader, so PL won't move to the device automatically.\r\nyou can do:\r\n```py\r\nx = x.to(self.device)\r\ny_hat = self.model(x)\r\n...\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-28T17:53:44Z"
    }
  },
  {
    "title": "MisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`.",
    "body": "### Throws this Exception. IDK what to do. Can anyone help me? \r\n\r\n**Code:**\r\n\r\n```\r\nimport enum\r\nimport os\r\nimport torch\r\nimport torchmetrics\r\n\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.callbacks import EarlyStopping\r\n\r\n\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nfrom dataclasses import dataclass, asdict\r\nimport pytorch_lightning_spells as pls\r\nimport typer\r\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\r\n\r\nfrom Cord19Dataset import Cord19Dataset, DATASET_DIR, Parts\r\nfrom t2t import BaseConfig, T5BaseModel, masked_cross_entropy_loss\r\nfrom pathlib import Path\r\nimport pandas as pd\r\n\r\n\r\nclass Corpus(enum.Enum):\r\n    CORD19 = \"cord19\"\r\n\r\nCACHE_DIR = \r\n\r\n@dataclass\r\nclass Config(BaseConfig):\r\n    dataset: Corpus = Corpus.CORD19\r\n\r\nfrom pathlib import Path\r\nimport pandas as pd\r\n\r\n\r\n\r\n\r\nclass T5Model(T5BaseModel):\r\n    def __init__(self, config: Config, **kwargs):\r\n        model = T5ForConditionalGeneration.from_pretrained(config.base_t5_model)\r\n        tokenizer = T5Tokenizer.from_pretrained(config.base_t5_model)\r\n        super().__init__(config, model, tokenizer)\r\n        self.config = config\r\n        # log the config values\r\n        self.save_hyperparameters(asdict(config))\r\n        self.train_dataset = Cord19Dataset(Parts.TRAIN)\r\n        print(\"Train dataset: \", len(self.train_dataset))\r\n        self.valid_dataset = Cord19Dataset(Parts.VALID)\r\n        print(\"Valid dataset: \", len(self.valid_dataset))\r\n        \r\nprint(Cord19Dataset(Parts.TRAIN))\r\n\r\ndef main(\r\n        t5_model: str = \"t5-small\", lr: float = 1e-4,  # 3^4\r\n        epochs: int = 5, fp16: bool = False,\r\n        #dataset: Corpus = Corpus.CORD19, batch_size: int = 16,\r\n        dataset: Corpus = Corpus.CORD19, batch_size: int = 8,\r\n        max_len: int = 64, grad_accu: int = 1,\r\n        num_gpus: int = 1\r\n      \r\n):\r\n    pl.seed_everything(int(os.environ.get(\"SEED\", 738)))\r\n    config = Config(\r\n        base_t5_model=t5_model,\r\n        learning_rate=lr,\r\n        epochs=epochs,\r\n        dataset=dataset,\r\n        max_len=max_len,\r\n        grad_accu=grad_accu,\r\n        batch_size=batch_size,\r\n        fp16=fp16,\r\n        weight_decay=0,\r\n        num_gpus=num_gpus,\r\n        loss_fn=masked_cross_entropy_loss\r\n    )\r\n\r\n    pl_module = T5Model(config)\r\n\r\n    callbacks = [\r\n        pl.callbacks.ModelCheckpoint(\r\n            dirpath=str(DATASET_DIR / \"model_checkpoints\"),\r\n            monitor='val_loss',\r\n            mode=\"min\",\r\n            filename='{step:06d}-{val_loss:.4f}',\r\n            save_top_k=1,\r\n            save_last=False\r\n        ),\r\n        pl.callbacks.LearningRateMonitor(logging_interval='step'),\r\n    ]\r\n    trainer = pl.Trainer(\r\n        accelerator='dp' if num_gpus > 1 else None,\r\n        # amp_backend=\"apex\", amp_level='O2',\r\n        precision=16 if config.fp16 else 32,\r\n        gpus=config.num_gpus,\r\n        val_check_interval=0.25,\r\n        gradient_clip_val=10,\r\n        max_epochs=epochs,\r\n        # max_steps=steps,\r\n        callbacks=callbacks,\r\n        accumulate_grad_batches=grad_accu,\r\n        # auto_scale_batch_size='power' if batch_size is None else None,\r\n        logger=[\r\n            pl.loggers.TensorBoardLogger(str(DATASET_DIR / \"tb_logs\"), name=\"\"),\r\n            pls.loggers.ScreenLogger(),\r\n            # pl.loggers.WandbLogger(project=\"t5-paraphrase\")\r\n        ],\r\n        log_every_n_steps=100\r\n    )\r\n\r\n    trainer.fit(pl_module)\r\n\r\n#     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n#     pl_module.tokenizer.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n#     print(\"Last model saved\")\r\n\r\n    assert isinstance(callbacks[0], pl.callbacks.ModelCheckpoint)\r\n    print(callbacks[0].best_model_path)\r\n    pl_module = T5Model.load_from_checkpoint(\r\n        callbacks[0].best_model_path,\r\n        config=config\r\n    )\r\n    pl_module.model.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\r\n    pl_module.tokenizer.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\r\n    print(\"Best model saved\")\r\n    pl_module.model.save_pretrained(path+t5+\"model/\")\r\n    pl_module.tokenizer.save_pretrained(path+t5+\"tokenizer/\")\r\n\r\n```\r\n\r\nThis gives me: \r\n\r\n```\r\nGlobal seed set to 738\r\nSome weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\r\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\r\n  stream(template_mgs % msg_args)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 175.62it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 24.88it/s]\r\nLoading train dataset into memory...\r\nUsed File: ['dataset_1.jbl', 'dataset_2.jbl', 'dataset_3.jbl', 'dataset_4.jbl', 'dataset_5.jbl', 'dataset_6.jbl']\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/[...]Pretraining-T5-PyTorch-Lightning/model_checkpoints exists and is not empty.\r\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\r\n\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\r\nValid dataset:  124\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\r\n\r\n  | Name  | Type                       | Params\r\n-----------------------------------------------------\r\n0 | model | T5ForConditionalGeneration | 60.5 M\r\n-----------------------------------------------------\r\n60.5 M    Trainable params\r\n0         Non-trainable params\r\n60.5 M    Total params\r\n242.026   Total estimated model params size (MB)\r\nSteps per epochs: 46\r\nValidation sanity check: 0%\r\n0/2 [00:00<?, ?it/s]\r\n---------------------------------------------------------------------------\r\nMisconfigurationException                 Traceback (most recent call last)\r\n<ipython-input-10-b1e689bd5891> in <module>\r\n      2 # if __name__ == \"__main__\":\r\n      3 #     typer.run(main)\r\n----> 4 main()\r\n\r\n<ipython-input-9-576858e8663d> in main(t5_model, lr, epochs, fp16, dataset, batch_size, max_len, grad_accu, num_gpus)\r\n     56     )\r\n     57 \r\n---> 58     trainer.fit(pl_module)\r\n     59 \r\n     60 #     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\r\n    551         self.checkpoint_connector.resume_start()\r\n    552 \r\n--> 553         self._run(model)\r\n    554 \r\n    555         assert self.state.stopped\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    916 \r\n    917         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 918         self._dispatch()\r\n    919 \r\n    920         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _dispatch(self)\r\n    984             self.accelerator.start_predicting(self)\r\n    985         else:\r\n--> 986             self.accelerator.start_training(self)\r\n    987 \r\n    988     def run_stage(self):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     90 \r\n     91     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n---> 92         self.training_type_plugin.start_training(trainer)\r\n     93 \r\n     94     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    159     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n    160         # double dispatch to initiate the training loop\r\n--> 161         self._results = trainer.run_stage()\r\n    162 \r\n    163     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    994         if self.predicting:\r\n    995             return self._run_predict()\r\n--> 996         return self._run_train()\r\n    997 \r\n    998     def _pre_training_routine(self):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_train(self)\r\n   1029             self.progress_bar_callback.disable()\r\n   1030 \r\n-> 1031         self._run_sanity_check(self.lightning_module)\r\n   1032 \r\n   1033         # enable train mode\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model)\r\n   1113             # run eval step\r\n   1114             with torch.no_grad():\r\n-> 1115                 self._evaluation_loop.run()\r\n   1116 \r\n   1117             self.on_sanity_check_end()\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\r\n    109             try:\r\n    110                 self.on_advance_start(*args, **kwargs)\r\n--> 111                 self.advance(*args, **kwargs)\r\n    112                 self.on_advance_end()\r\n    113                 self.iteration_count += 1\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py in advance(self, *args, **kwargs)\r\n    109 \r\n    110         dl_outputs = self.epoch_loop.run(\r\n--> 111             dataloader_iter, self.current_dataloader_idx, dl_max_batches, self.num_dataloaders\r\n    112         )\r\n    113 \r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\r\n    109             try:\r\n    110                 self.on_advance_start(*args, **kwargs)\r\n--> 111                 self.advance(*args, **kwargs)\r\n    112                 self.on_advance_end()\r\n    113                 self.iteration_count += 1\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in advance(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\r\n    109         with self.trainer.profiler.profile(\"evaluation_step_and_end\"):\r\n    110             output = self.evaluation_step(batch, batch_idx, dataloader_idx)\r\n--> 111             output = self.evaluation_step_end(output)\r\n    112 \r\n    113         self.batch_progress.increment_processed()\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in evaluation_step_end(self, *args, **kwargs)\r\n    159         \"\"\"Calls the `{validation/test}_step_end` hook\"\"\"\r\n    160         hook_name = \"test_step_end\" if self.trainer.testing else \"validation_step_end\"\r\n--> 161         output = self.trainer.call_hook(hook_name, *args, **kwargs)\r\n    162         return output\r\n    163 \r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n   1222             if is_overridden(hook_name, model_ref):\r\n   1223                 hook_fx = getattr(model_ref, hook_name)\r\n-> 1224                 output = hook_fx(*args, **kwargs)\r\n   1225 \r\n   1226             # call the accelerator hook\r\n\r\n/home/[...]Pretraining-T5-PyTorch-Lightning/t2t/__init__.py in validation_step_end(self, outputs)\r\n    227                 outputs['target']['ids'].view(-1).cpu()\r\n    228             )\r\n--> 229             self.log(\"val_\" + name, metric)\r\n    230 \r\n    231     def _should_log(self, flag):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py in log(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, tbptt_reduce_fx, tbptt_pad_token, enable_graph, sync_dist, sync_dist_op, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\r\n    432                 if not self._metric_attributes:\r\n    433                     raise MisconfigurationException(\r\n--> 434                         \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n    435                         \" You can fix this by setting an attribute for the metric in your `LightningModule`.\"\r\n    436                     )\r\n\r\nMisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`.\r\n```\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10178",
    "createdAt": "2021-10-27T08:45:23Z",
    "updatedAt": "2022-06-15T11:56:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "adrian-jaques-b"
    },
    "answer": {
      "body": "ok well i got it. Seems like I accidentally updated the module...",
      "author": {
        "login": "adrian-jaques-b"
      },
      "createdAt": "2021-10-28T13:22:04Z"
    }
  },
  {
    "title": "multiple on_train_epoch_start callbacks but only one on_train_epoch_end?",
    "body": "I thought the number of on_train_epoch_start and on_train_epoch_end should be equal to the number of epochs. But when I passed the following callback function:\r\n```\r\nclass MyPrintingCallback(Callback):\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        print('Train epoch start for epoch: ', pl_module.current_epoch)\r\n        \r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        # will run only once in the beginning\r\n        print('Train step end for epoch: ', pl_module.current_epoch)\r\n```\r\n\r\non_train_epoch_end is only called in the 0th epoch:\r\n\r\n> Training: -1it [00:00, ?it/s]Train epoch start for epoch:  0                                                                                                                                                                                                                        \r\nEpoch 0: : 4875it [00:33, 143.57it/s, Train step end for epoch:  0\r\nTrain epoch start for epoch:  1       \r\nEpoch 1: : 0it [00:00, 7096.96it/s, loss=2.13e+09, v_num=37] Train epoch start for epoch:  2\r\nEpoch 2: : 0it [00:00, 11335.96it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  3\r\nEpoch 3: : 0it [00:00, 12052.60it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  4\r\nEpoch 4: : 0it [00:00, 4301.85it/s, loss=2.13e+09, v_num=37] \r\n\r\n\r\nAny idea why this is happening? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10140",
    "createdAt": "2021-10-25T20:42:16Z",
    "updatedAt": "2024-03-05T17:01:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhh210"
    },
    "answer": {
      "body": "It turns out that the issue is because I was chaining many data loaders with itertools.chain() which was called only once by lightning. ",
      "author": {
        "login": "zhh210"
      },
      "createdAt": "2021-10-26T04:08:23Z"
    }
  },
  {
    "title": "Getting the test score after restoring a pretrained model",
    "body": "When I load the saved Lightning model and test on test data, I saw different epochs are running starting from 0, suggesting the model is **being trained once again**. Please suggest how to get the test score on the test data. \r\nLet me illustrate in detail. I have a  Lightning model like the following\r\n```\r\nclass  MyLightningModule(pl.LightningModule):\r\n    def __init__(self,loss_fn, lr=1e-1):\r\n        super().__init__()\r\n        self.loss_fn = loss_fn\r\n        self.lr = lr\r\n        self.save_hyperparameters(\"lr\")\r\n    \r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"SemanticPO\")\r\n        parser.add_argument(\"--lr\", type=float, default=0.1)\r\n        return parent_parser\r\n    def forward(self,x):\r\n        return self.layer1(x)   \r\n    def training_step(self, batch, batch_idx):\r\n        x,y,sol = batch\r\n        y_hat =  self(x)\r\n        loss = self.loss_fn(y,y_hat)\r\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True )\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        x,y,sol = batch\r\n        y_hat =  self(x)\r\n        val_loss=  self.loss_fn(y,y_hatl)\r\n        self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=True, )\r\n        return val_loss\r\n    def test_step(self, batch, batch_idx):\r\n        # Here we just reuse the validation_step for testing\r\n        return self.validation_step(batch, batch_idx)\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\r\n        return optimizer \r\n```\r\nI am using three separate data for training. testing and validation.\r\n```\r\ntrain_dl = DataLoader(train_df, batch_size= 64)\r\nvalid_dl = data_utils.DataLoader(valid_df, batch_size= 64)\r\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\r\n```\r\nFirst, I train and validate in the following manner\r\n```\r\nmodel = MyLightningModule(loss_fn= Myloss)\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    dirpath=\"ckpt_dir/\",\r\n    filename=\"mymodel-{epoch:02d}-{val_loss:.2f}\",\r\n    save_top_k=2, mode=\"min\",\r\n)\r\ntrainer = pl.Trainer(max_epochs= 25, callbacks=[checkpoint_callback])\r\ntrainer.fit(model, train_dl, valid_dl)\r\n```\r\nThe training is complete and the checkpoints are saved as expected.\r\nThen I want to restore the saved model and test it on test data. This is done on a separate file like this\r\n```\r\nmodel = MyLightningModule.load_from_checkpoint('ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt')\r\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\r\ntrainer = pl.Trainer()\r\nresult = trainer.test(model,\r\nckpt_path='ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt',dataloaders=test_dl)\r\nprint(result)\r\n```\r\nI expect it to just test on test data and display the result. But I see the model is running once again for a number of epochs. \r\nMay be I am doing something wrong. Please suggest the best way to obtain the test result after restoring a saved model.\r\n\r\n<details>\r\npytorch-lightning==1.4.9\r\n</details>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10136",
    "createdAt": "2021-10-25T14:52:57Z",
    "updatedAt": "2023-01-17T16:41:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JayMan91"
    },
    "answer": {
      "body": "your code looks correct. Just tested on 1.4.9 it's not running from scratch when you call `trainer.test()`. Can you share a minimal example to reproduce your issue? ",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-25T19:50:29Z"
    }
  },
  {
    "title": "Model with best validation accuracy",
    "body": "Is there a way to save the model with the best validation accuracy when using early stopping? I believe right now, the model weights are the weights from the latest snapshot; but i am looking for a way to access the model with the best performance on validation.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10126",
    "createdAt": "2021-10-25T10:25:26Z",
    "updatedAt": "2022-07-18T04:20:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kanodiaayush"
    },
    "answer": {
      "body": "by on validation you mean while calling `trainer.validate` or validation happening within `trainer.fit` call?",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-25T11:24:08Z"
    }
  },
  {
    "title": "'NeuralNetwork' object has no attribute 'log'",
    "body": "Hello I am trying to train a neural network using pytorch lightning. I have run into an issue with the trainer when I try to run the program. I am getting the following issue:\r\n```\r\nGPU available: False, used: False\r\nTPU available: False, using: 0 TPU cores\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/PytorchLightningGRUtraining.py\", line 179, in <module>\r\n    main(args)\r\n\r\n  File \"/home/PytorchLightningGRUtraining.py\", line 171, in main\r\n    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 449, in fit\r\n    self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 123, in setup_fit\r\n    self.trainer.callback_connector.attach_model_logging_functions(model)\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py\", line 123, in attach_model_logging_functions\r\n    callback.log = model.log\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n\r\nAttributeError: 'NeuralNetwork' object has no attribute 'log'\r\n```\r\n\r\nI defined the logger for the trainer after I created an instance of the model class. What am I doing wrong?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10096",
    "createdAt": "2021-10-23T19:46:17Z",
    "updatedAt": "2022-05-31T02:52:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cwoolfo1"
    },
    "answer": {
      "body": "did you pass in a lightningmodule instance?\r\n```py\r\nclass YourModel(pl.LightningModule): <- here?\r\n    ...\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-24T11:51:12Z"
    }
  },
  {
    "title": "The call of training_step and validation_step .etc.",
    "body": "Anyone help me where did these funcs been called in the core parts? I expect it to be called in the loop instance of trainer however not. Quite confused about this.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10070",
    "createdAt": "2021-10-21T14:27:19Z",
    "updatedAt": "2023-05-20T02:55:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MrChenFeng"
    },
    "answer": {
      "body": "depends upon the configuration but in a general case here: https://github.com/PyTorchLightning/pytorch-lightning/blob/454e93bacecb4f1d6ebc9ebd3bc11b26723ddbec/pytorch_lightning/plugins/training_type/training_type_plugin.py#L217-L230\r\n\r\ncall sequence is `loop -> accelerators -> training_type_plugin -> actual call`\r\n",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-21T17:56:18Z"
    }
  },
  {
    "title": "How to disable logging temporarily?",
    "body": "My `LightningModule.training_step` includes calls to `self.log` and finally returns the loss value. What is the best way to run `training_step` outside of a `Trainer` context, for debugging purposes (such as manual gradient inspection, etc)? Without the instrumentation by `Trainer`, logger is not defined and `self.log` calls cause an exception. I was trying to mock the logger to turn them to no-ops, but I wasn't successful.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10049",
    "createdAt": "2021-10-20T12:07:11Z",
    "updatedAt": "2023-01-16T11:36:21Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jatentaki"
    },
    "answer": {
      "body": "it was updated to warning recently: https://github.com/PyTorchLightning/pytorch-lightning/pull/9733\r\nmight work with recent release or master",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-20T12:52:07Z"
    }
  },
  {
    "title": "Exporting PyTorch Lightning model to ONNX format not working",
    "body": "I am using Jupyter Lab to run. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.\r\n\r\n```\r\nPyTorch Lightning Version (e.g., 1.3.0): '1.4.6'\r\nPyTorch Version (e.g., 1.8): '1.6.0+cu101'\r\nPython version: 3.6\r\nOS (e.g., Linux): system='Linux'\r\nCUDA/cuDNN version: 11.2\r\nHow you installed PyTorch (conda, pip, source): pip\r\n```\r\n\r\nHere is the screenshot of my model and it got interrupted due to connection issue.\r\n\r\nI am saving the best model in checkpoint.\r\n\r\n`I am doing multi-label classification using Hugging face model. After training the model I want to export the model using ONNX format.`\r\n\r\n**Here is the DataModule Class**\r\n\r\n```\r\n\r\nN_EPOCHS = 30\r\nBATCH_SIZE = 10\r\n\r\nclass  SRDataModule(pl.LightningDataModule):\r\n    \r\n    def __init__(self, X_train,y_train, X_test,y_test, tokenizer, batch_size=8, max_token_len=512):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n        self.train_df = X_train\r\n        self.test_df = X_test\r\n        self.train_lab = y_train\r\n        self.test_lab = y_test\r\n        self.tokenizer = tokenizer\r\n        self.max_token_len = max_token_len\r\n\r\n    def setup(self, stage=None):\r\n        self.train_dataset = SRDataset(\r\n          self.train_df,\r\n          self.train_lab,\r\n          self.tokenizer,\r\n          self.max_token_len\r\n        )\r\n\r\n        self.test_dataset = SRDataset(\r\n          self.test_df,\r\n          self.test_lab,\r\n          self.tokenizer,\r\n          self.max_token_len\r\n    )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n          self.train_dataset,\r\n          batch_size=self.batch_size,\r\n          shuffle=True,\r\n          num_workers=10\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(\r\n          self.test_dataset,\r\n          batch_size=self.batch_size,\r\n          num_workers=10\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n          self.test_dataset,\r\n          batch_size=self.batch_size,\r\n          num_workers=10\r\n        )\r\n\r\n```\r\n\r\n\r\n**Here is the model class:**\r\n\r\n\r\n```\r\nclass SRTagger(pl.LightningModule):\r\n\r\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\r\n    super().__init__()\r\n    self.save_hyperparameters()\r\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n    self.n_training_steps = n_training_steps\r\n    self.n_warmup_steps = n_warmup_steps\r\n    self.criterion = nn.BCELoss()\r\n\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    \r\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def test_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def training_epoch_end(self, outputs):\r\n    \r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    for i, name in enumerate(LABEL_COLUMNS):\r\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\r\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\r\n\r\n\r\n  def configure_optimizers(self):\r\n\r\n    optimizer = optim.RAdam(self.parameters(), lr=2e-4)\r\n\r\n    scheduler = get_linear_schedule_with_warmup(\r\n      optimizer,\r\n      num_warmup_steps=self.n_warmup_steps,\r\n      num_training_steps=self.n_training_steps\r\n    )\r\n\r\n    return dict(\r\n      optimizer=optimizer,\r\n      lr_scheduler=dict(\r\n        scheduler=scheduler,\r\n        interval='step'\r\n      )\r\n    )\r\n```\r\n\r\n\r\n**Sample Data**\r\n\r\n\r\n```\r\nsample_batch = next(iter(DataLoader(train_dataset, batch_size=10, num_workers=2)))\r\nsample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape\r\n\r\n(torch.Size([10, 512]), torch.Size([10, 512]))\r\n```\r\n\r\n```\r\nsample_batch.keys()\r\ndict_keys(['text_data', 'input_ids', 'attention_mask', 'labels'])\r\n```\r\n\r\n**Model**\r\n\r\n```\r\nmodel = SRTagger(\r\n  n_classes=100,\r\n  n_warmup_steps=warmup_steps,\r\n  n_training_steps=total_training_steps \r\n)\r\n\r\n```\r\n**ONNX code**\r\n\r\n```\r\n# # Export the model\r\ntorch.onnx.export(model,                     # model being run\r\n                  ##since model is in the cuda mode, input also need to be\r\n                  (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"]),              # model input (or a tuple for multiple inputs)\r\n                  \"model_torch_export.onnx\", # where to save the model (can be a file or file-like object)\r\n                  export_params=True,        # store the trained parameter weights inside the model file\r\n                  opset_version=10,          # the ONNX version to export the model to\r\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\r\n                  input_names = ['input'],   # the model's input names\r\n                  output_names = ['output'], # the model's output names\r\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\r\n                                'output' : {0 : 'batch_size'}})\r\n```\r\n\r\n**Error**\r\n\r\n```\r\nRuntimeError: output 1 (0\r\n[ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10063",
    "createdAt": "2021-10-20T05:23:36Z",
    "updatedAt": "2022-06-23T19:50:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pratikchhapolika"
    },
    "answer": {
      "body": "Hi\r\n\r\nA google search reveals some help on this issue here: \r\n\r\nhttps://github.com/pytorch/pytorch/issues/31591\r\n\r\nCiting the thread there\r\n\r\n> As the error message indicates, the tracer detected that the output of your model didn't have any relationship to the input. \r\n\r\nIf we look closer at your code, we see that loss=0 and labels=None.\r\n\r\n```py\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n```\r\n\r\nthe if condition does not hold, so the part of your output (the loss) cannot be traced back to any inputs by onnx. \r\n\r\nChange your code to something like this and try again please:\r\n```py\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n        return loss, output\r\n    return output\r\n```\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-10-21T00:38:08Z"
    }
  },
  {
    "title": "How to save and load LightningModule whose input containing the pretrained moduel?",
    "body": "Hi,\r\n\r\nI'm applying Pytorch Lightning module to VAE and our model We first train VAE and give the best checkpoint of pretrained VAE as the initial weight of our model.\r\n\r\n```python\r\n# STEP 1. Train VAE\r\nvae = VAE(...)\r\n\r\ntrainer = Trainer(...)\r\ntrainer.fit(vae)\r\n\r\n# STEP 2.\r\nvae = VAE.load_from_checkpoint(...)\r\n\r\nclass Model(LightningModule):\r\n    def __init__(self, encoder, decoder, learning_rate):\r\n       super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n        \r\n        self.save_hyperparameters(\"learning_rate\")\r\n        ...\r\n\r\nencoder = copy.deepcopy(vae.encoder)\r\ndecoder = copy.deepcopy(vae.decoder)\r\n\r\nmodel = Model(\r\n    encoder=encoder,\r\n    decoder=decoder,\r\n    ...\r\n)\r\ntrainer.fit(model)\r\n```\r\n\r\nThe problem is when I load the **model** after train ends. Since the torch modules are contained in input arguments of **Model**, the common approach \r\n\r\n```python\r\nmodel = Model.load_from_checkpoint(...) \r\n```\r\nyields following error messages. `TypeError: __init__() missing 2 required positional arguments: 'encoder' and 'decoder'\r\n`\r\nSo, what is the best practice for saving and loading the model which uses the pre-trained model?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/10037",
    "createdAt": "2021-10-20T04:45:16Z",
    "updatedAt": "2022-07-15T01:33:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vrvrv"
    },
    "answer": {
      "body": "since part of your model is inside arguments, you can randomly initialize your VAE and let the new checkpoint configure it's weights\r\n```py\r\nvae = VAE()\r\nencoder = copy.deepcopy(vae.encoder)\r\ndecoder = copy.deepcopy(vae.decoder)\r\n\r\nmodel = Model.load_from_checkpoint(..., encoder=encoder, decoder=decoder) \r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-20T11:59:38Z"
    }
  },
  {
    "title": "Example on training with TPU does not run at all",
    "body": "I am currently try this colab notebook https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/mnist-tpu-training.ipynb#scrollTo=2772a2e1 provided by PL teams to get some experience with TPU training. But when I try to execute the third cell, there is some Import error with the _XLAC module. \r\n\r\n<img width=\"937\" alt=\"Screen Shot 2021-10-17 at 01 44 00\" src=\"https://user-images.githubusercontent.com/37470762/137598792-4478d28d-485a-45da-9186-947387a5394a.png\">\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9966",
    "createdAt": "2021-10-16T18:44:04Z",
    "updatedAt": "2022-08-23T09:45:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tungts1101"
    },
    "answer": {
      "body": "Hi @tungts1101! this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!",
      "author": {
        "login": "kaushikb11"
      },
      "createdAt": "2021-11-19T12:59:44Z"
    }
  },
  {
    "title": "How to save/load only part of the weights in the model?",
    "body": "For example, part of my model's parameters are frozen, no need to train, no need to save",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9961",
    "createdAt": "2021-10-16T09:18:21Z",
    "updatedAt": "2025-08-19T11:40:19Z",
    "closedAt": "2025-08-19T11:40:19Z",
    "isAnswered": true,
    "author": {
      "login": "Achazwl"
    },
    "answer": {
      "body": "This might work:\r\n```py\r\ndef on_save_checkpoint(checkpoint):\r\n    # pop the backbone here using custom logic\r\n    del checkpoint['state_dict'][backbone_keys]\r\n\r\nLitModel.load_from_checkpoint(ckpt_path, strict=False)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-16T21:16:18Z"
    }
  },
  {
    "title": "What are ones options for manually defining the parallelization?",
    "body": "(Q1) Does PyTorch Lightning enable parallelization across multiple dimension or does it only allow data parallelism?\r\nThe [FlexFlow](https://github.com/flexflow/FlexFlow) implements the parallelism across 4 different dimensions (\"SOAP\": the sample, operator, attribute and parameter dimensions). (Q2) Over which of these does PyTorch-Lightning do parallelization? (Q3) Does PyTorch-Lightning's API give an option to manually control the parallelization for each and every layer individually?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9881",
    "createdAt": "2021-10-10T13:14:44Z",
    "updatedAt": "2023-03-22T07:20:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "roman955b"
    },
    "answer": {
      "body": "Dear @roman955b,\r\n\r\n1 ) Currently, Lightning automatically implement distributed data parallelism. However, we are currently working on making manual parallelization for users who want deeper control of the parallelisation schema. \r\n\r\n2 ) Lightning supports only (S, P) with DeepSpeed, FSDP integrations.\r\n\r\n3 ) Yes, we are currently working on this. Here is an issue to track the conversation https://github.com/PyTorchLightning/pytorch-lightning/issues/9375\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-10-11T08:57:48Z"
    }
  },
  {
    "title": "pytorch-lightning output embeddings completely differnt than pytorch vanilla",
    "body": "Hey pytorch-lightning team,\r\n\r\nI am trying to build an autoencoder architecture via **pytorch-lightning** (newbie), which I observe its advantages while running complex models in comparison to **pytorch implementation**. \r\n\r\nHowever in this **AE vanilla model,** I observe **significant difference in the embedding output of pytorch-lightning** vs **pytorch** where I expect that pytorch-lightning should give the exact UMAP embedding as in pytorch \r\n\r\n### and here is an example:\r\n\r\n### Pytorch implementation\r\n\r\n```\r\n## AE Architecture\r\nclass Autoencoder(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        self.encoder = nn.Sequential(\r\n            #nn.Flatten(),\r\n            nn.Linear(nfeatures_rna, 200),\r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\r\n            nn.Dropout(0.2)\r\n        )\r\n        \r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(10, 200), \r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\r\n        )\r\n    def forward(self, x):\r\n        x = torch.flatten(x, start_dim=1)\r\n        encoded = self.encoder(x)\r\n        decoded = self.decoder(encoded)\r\n        return decoded\r\nmodelV = Autoencoder()\r\nlr = 1e-2\r\noptimizer = torch.optim.Adam(modelV.parameters(), lr=lr)\r\nnum_epochs = 50\r\noutputs = []\r\nfor epoch in range(num_epochs):\r\n    for x, y in data_loader:\r\n        #x = x.reshape(-1, nfeatures_rna) # -> use for Autoencoder_Linear\r\n        recon = modelV(x)\r\n        \r\n        loss = criterion(recon, x)\r\n        \r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n    print(f'Epoch:{epoch+0}, Loss:{loss.item():.4f}')\r\n    outputs.append((epoch, x, recon))\r\ndef get_encodings(model, dl):\r\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    model.eval()\r\n    with torch.no_grad():\r\n        encodings = [model.encoder(x) for x, _ in dl]\r\n    return torch.cat(encodings, dim=0)\r\nencodings = get_encodings(modelV, data_loader)\r\nencodings = encodings.cpu().numpy()\r\nencodings.shape\r\n## UMAP\r\nimport umap\r\nembedding = umap.UMAP(random_state=0).fit_transform(encodings)\r\nplot_df = pd.DataFrame({'A' : []})\r\n#plot_df = metadata.copy()\r\nplot_df[\"UMAP1\"] = embedding[:, 0]\r\nplot_df[\"UMAP2\"] = embedding[:, 1]\r\n```\r\n\r\n### Here is the UMAP plot for pytorch embedding:\r\n\r\n\r\n<img width=\"492\" alt=\"Screenshot 2021-10-08 at 4 08 45 PM\" src=\"https://user-images.githubusercontent.com/29317258/136575302-ef4c94b5-eebf-420b-83db-fdb551c43730.png\">\r\n\r\n### Pytorch-lightning implementation for the same Pytorch AE model above\r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch import nn\r\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\r\nlr = 1e-2\r\nclass Autoencoder(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        \r\n        ## Encoder Arch\r\n        \r\n        self.encoder = nn.Sequential(\r\n            # nn.Flatten(),\r\n            nn.Linear(nfeatures_rna, 200),\r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\r\n            nn.Dropout(0.2)\r\n        )\r\n        \r\n        ## Decoder Arch\r\n        \r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(10, 200), \r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\r\n        )\r\n    \r\n    ## Traning step\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        # encode\r\n        #x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n    \r\n    \r\n        # decode\r\n        recons = self.decoder(z)\r\n        \r\n        # reconstruction #1#\r\n        reconstruction_loss_1 = nn.functional.mse_loss(x, recons)\r\n        return reconstruction_loss_1\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=lr)\r\n    \r\n## Fotward step\r\n    \r\n    def forward(self, x):\r\n        z = self.encoder(x)\r\n        recons = self.decoder(z)\r\n        embedding = z\r\n        return embedding \r\nautoencoder = Autoencoder()\r\ntrainer = pl.Trainer(gpus=1, max_epochs=50)\r\n# trainer = pl.Trainer(gpus=1)\r\ntrainer.fit(autoencoder, data_loader)\r\ndef get_encodings(model, dl):\r\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    model.eval()\r\n    with torch.no_grad():\r\n        encodings = [model.encoder(x) for x, _ in dl]\r\n    return torch.cat(encodings, dim=0)\r\nmodel = Autoencoder()\r\nencodings = get_encodings(model, data_loader)\r\nencodings = encodings.cpu().numpy()\r\nencodings.shape\r\nembedding = umap.UMAP(random_state=0).fit_transform(encodings) \r\nplot_df = pd.DataFrame({'A' : []})\r\nplot_df[\"UMAP1\"] = embedding[:, 0]\r\nplot_df[\"UMAP2\"] = embedding[:, 1]\r\n##\r\ncustom = sns.scatterplot(data=plot_df, x='UMAP1', \r\n                      y='UMAP2', \r\n                      #hue='Gen_and_RealCells',\r\n                      palette=\"deep\", \r\n                      #style=\"Gen_and_RealCells\"\r\n                     )\r\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\r\n```\r\n\r\n\r\n### Here is the UMAP plot for pytorch-lightning embedding :\r\n<img width=\"399\" alt=\"Screenshot 2021-10-08 at 4 18 59 PM\" src=\"https://user-images.githubusercontent.com/29317258/136575417-f96b8655-cd45-49ab-a5f7-414db3b23d5d.png\">\r\n\r\n\r\nSince I have experience with this data type and the data generative process, which give me confidence to believe in the  pytorch output\r\n\r\nAny hints or explanations would be highly appreciated! Thank you in advance for your help.\r\n\r\nBest wishes,\r\nAbdelrahman \r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9873",
    "createdAt": "2021-10-08T14:47:12Z",
    "updatedAt": "2022-06-18T08:29:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "AMA111"
    },
    "answer": {
      "body": "Hi\r\nhere, I think you are reinitializing the model and using the one with random weights.\r\n```py\r\nmodel = Autoencoder()\r\nencodings = get_encodings(model, data_loader)\r\n```\r\n\r\nyou need to load a checkpoint here\r\n```py\r\nmodel = AutoEncoder.load_from_checkpoint(ckpt_path)\r\nencodings = get_encodings(model, data_loader)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-09T20:46:16Z"
    }
  },
  {
    "title": "Increase grad_batches during training",
    "body": "Hi!\r\nI work with cyclical learning rates, which I don't want to further reduce. So for annealing the training, I would like to increase the number of grad batches in \"accumulate_grad_batches\" every N epochs. Is there a way to do that?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9870",
    "createdAt": "2021-10-08T09:27:55Z",
    "updatedAt": "2025-07-12T06:27:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "OlfwayAdbayIgbay"
    },
    "answer": {
      "body": "you can try:\r\n```py\r\nfactor = # factor by which you want to increase the accumulate_grad_batches\r\nN = \r\nmax_epochs = \r\ninit_acc_grad_batches = \r\nacc_grad_batches = {ep: init_acc_grad_batches + (i)*factor for i, ep in enumerate(range(0, max_epochs, N))}\r\ntrainer = Trainer(accumulate_grad_batches=acc_grad_batches, max_epochs=max_epochs, ...)\r\n```\r\n`accumulate_grad_batches` can take input as a Dict where the key represents the epoch where this value will be changed and the value represents the `accumulate_grad_batches` to use.\r\n\r\nSo if it's {0: 1, 3: 2, 5: 4}, then\r\nepoch [1-4) -> 1\r\nepoch [4-6) -> 3\r\nepoch [6-max_epochs] -> 4\r\n\r\nlooks like the epoch(key) here should be zero-indexed, which might be a small bug here.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-09T21:03:20Z"
    }
  },
  {
    "title": "Update Adam learning rate after 10 epochs",
    "body": "hello,\r\n\r\nhow can i update the learning rate of adam optimizer after 10 epochs ?\r\nmy code is like this\r\n```\r\nself.lr_decay_epoch = [15,]\r\nif epoch in self.lr_decay_epoch:\r\n     self.lr = self.lr * 0.1\r\n     self.optimizer = Adam(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.lr, weight_decay=self.wd)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9853",
    "createdAt": "2021-10-07T10:54:59Z",
    "updatedAt": "2022-06-30T08:35:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cmdrootaccess"
    },
    "answer": {
      "body": "you can use [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#) where lambda function can be something like:\r\n```py\r\nlambda epoch: return lr * (0.1 if epoch in self.lr_decay_epoch else 1)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-07T14:34:30Z"
    }
  },
  {
    "title": "How to calculate AUC over an entire test set?",
    "body": "So I have code like this:\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x = batch['src']\r\n        y = batch['label']\r\n        mask = batch['mask']\r\n\r\n        x = self.base_model(x, mask)\r\n        x = self.linear(x).mean(axis=1).squeeze(1)\r\n        \r\n        loss = F.binary_cross_entropy_with_logits(input=x,\r\n                               target=y)\r\n        try:\r\n            auc = roc_auc_score(y_true=y.cpu().numpy(), y_score=torch.sigmoid(x).cpu().numpy())\r\n        except ValueError:\r\n            auc = 0\r\n        metrics = {'test_loss': loss, 'test_auc': auc}\r\n        self.log_dict(metrics)\r\n        return metrics\r\n\r\n\r\nI'm trying to calculate the AUC for the model over a test set,  but I assume this function is only calculating the AUC for a single batch. So I guess my question is whether the test set is treated as a single batch and if not, how would I go about doing this? Would I have to average the AUC over all the batches? If so, I'm not sure how to do that. But more importantly, that doesn't seem like the ideal way to calculate this metric to me, given how much variance there would be over each batch. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9845",
    "createdAt": "2021-10-06T17:48:53Z",
    "updatedAt": "2022-07-19T14:11:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EvanZ"
    },
    "answer": {
      "body": "averaging auc won't give the correct metric over the entire dataset plus you might get some error in case you have all targets of the same label which is possible in case of a single batch. To avoid this you can checkout https://torchmetrics.readthedocs.io/en/stable/references/modules.html#auroc. It will calculate the metrics the right way for you and you don't have to take care of averaging/accumulation or anything. There are different subpackage for each metric: module and functional. For your use-case I'd suggest using module one.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-07T10:17:34Z"
    }
  },
  {
    "title": "String \u201cbest\u201d at argument \u201cckpt_path\u201d for test method of Trainer class",
    "body": "Hi, \r\n\r\nThe `test` method of the `Trainer` class, has the input argument `ckpt_path`. According to the docs: \r\n\r\n> ckpt_path (Optional[str]) \u2013 Either best or path to the checkpoint you wish to test. If None and the model instance was passed, use the current weights. Otherwise, the best model from the previous trainer.fit call will be loaded.\r\n\r\nAlso, in the [documentation of PyTorch Lightning for the test set](https://pytorch-lightning.readthedocs.io/en/latest/common/test_set.html), using Trainer, there is the following: \r\n\r\n```python\r\n# run full training\r\ntrainer.fit(model)\r\n\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test(ckpt_path=\"best\")\r\n```\r\n\r\nMy question is, according to what the \"best\" checkpoint is decided? That is, is the \"best\" decided on maximising or minimising some value? What would be that value? Can someone configure the policy (i.e. minimising or maximising) and the value? How one should use this \"best\" string? \r\n\r\nLinks for reference: \r\n\r\n1. [Test set documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/test_set.html)\r\n2. [Test method of Trainer class documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#test)\r\n\r\nP.S. Please note that I'm not referring to using the `ModelChekpoint` callback, but explicitly to the above, which seems that the `ModelCheckpoint` callback is not used. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9836",
    "createdAt": "2021-10-06T12:13:26Z",
    "updatedAt": "2022-07-13T14:41:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dr-costas"
    },
    "answer": {
      "body": "it actually relies on `ModelCheckpoint` callback only https://github.com/PyTorchLightning/pytorch-lightning/blob/b3e9dff32d842431b067b0ab83e508ffe3262968/pytorch_lightning/trainer/trainer.py#L1268-L1280\r\nso if there is no `ModelCheckpoint` callback it will raise an error",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-06T12:33:55Z"
    }
  },
  {
    "title": "Custom scheduler",
    "body": "I would like to provide my own learning rate scheduler. What I would love is do is doing this in the lightning style, e.g. implementing some hooks:\r\n```python\r\nclass MyScheduler(pl.LightningScheduler):\r\n    ...\r\n    def on_step(self...):\r\n        ...\r\n    def on_epoch(self...):\r\n        ...\r\n```\r\nIs something like this possible? How do other people handle custom schedulers?\r\n\r\nPS: I asked this question before on the [deprecated forum.]\r\n(https://forums.pytorchlightning.ai/t/custom-scheduler-class/1238)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9817",
    "createdAt": "2021-10-04T14:39:27Z",
    "updatedAt": "2022-07-18T05:42:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jw3126"
    },
    "answer": {
      "body": "can you please elaborate on the usage more? like what do you want to do inside `on_step` and `on_epoch` methods? ",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-05T08:05:07Z"
    }
  },
  {
    "title": "Optimization in a dual encoder LitModel",
    "body": "Hello,\r\n\r\nCurrently, I am working in a Lit Model, which has two encoders. Each of them has its optimizer, scheduler, and loss, as shown below:\r\n\r\n```python\r\nimport importlib\r\n\r\nimport torch\r\nfrom pytorch_lightning.core.lightning import LightningModule\r\nfrom hydra.utils import instantiate\r\n\r\nfrom source.metric.ULMRRMetric import ULMRRMetric\r\n\r\n\r\nclass LitModel(LightningModule):\r\n\r\n    def __init__(self, hparams):\r\n\r\n        super(LitModel, self).__init__()\r\n        self.save_hyperparameters(hparams)\r\n\r\n        # encoders\r\n        self.x1_encoder = instantiate(hparams.x1_encoder)\r\n        self.x2_encoder = instantiate(hparams.x2_encoder)\r\n\r\n        # loss function\r\n        self.x1_loss = instantiate(hparams.x1_loss)\r\n        self.x2_loss = instantiate(hparams.x2_loss)\r\n\r\n\r\n    def forward(self, x1, x2):\r\n        x1_repr = self.x1_encoder(x1)\r\n        x2_repr = self.x2_encoder(x2)\r\n        return x1_repr, x2_repr\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        x1_repr, x2_repr = self(x1, x2)\r\n        x1_loss=self.x1_loss(x1_repr, x2_repr)\r\n        x2_loss = self.x2_loss(x1_repr, x2_repr)\r\n\r\n        # what to return here?\r\n        return\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        x1_repr, x2_repr = self(x1, x2)\r\n        self.log(\"val_x1_LOSS\", self.x1_loss(x1_repr, x2_repr), prog_bar=True)\r\n        self.log(\"val_x2_LOSS\", self.x2_loss(x1_repr, x2_repr), prog_bar=True)\r\n\r\n\r\n\r\n\r\n    # Alternating schedule for optimizer steps\r\n    def optimizer_step(\r\n            self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure,\r\n            on_tpu=False, using_native_amp=False, using_lbfgs=False,\r\n    ):\r\n        # update x1 encoder every even step\r\n        if optimizer_idx == 0:\r\n            if batch_idx % 2 == 0:\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n        # update x2 encoder every odd step\r\n        if optimizer_idx == 1:\r\n            if batch_idx % 2 != 0:\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n\r\n\r\n    def configure_optimizers(self):\r\n        # optimizers\r\n        optimizers = [\r\n            torch.optim.AdamW(self.x1_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\r\n                              weight_decay=self.hparams.weight_decay, amsgrad=True),\r\n\r\n            torch.optim.AdamW(self.x2_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\r\n                              weight_decay=self.hparams.weight_decay, amsgrad=True)\r\n        ]\r\n\r\n        # schedulers\r\n        step_size_up = round(0.03 * self.num_training_steps)\r\n        schedulers = [\r\n            torch.optim.lr_scheduler.CyclicLR(optimizers[0], mode='triangular2', base_lr=self.hparams.base_lr,\r\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\r\n                                              cycle_momentum=False),\r\n            torch.optim.lr_scheduler.CyclicLR(optimizers[1], mode='triangular2', base_lr=self.hparams.base_lr,\r\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\r\n                                              cycle_momentum=False)\r\n        ]\r\n\r\n        return optimizers, schedulers\r\n\r\n\r\n    @property\r\n    def num_training_steps(self) -> int:\r\n        \"\"\"Total training steps inferred from datamodule and number of epochs.\"\"\"\r\n        steps_per_epochs = len(self.train_dataloader()) / self.trainer.accumulate_grad_batches\r\n        max_epochs = self.trainer.max_epochs\r\n        return steps_per_epochs * max_epochs\r\n\r\n```\r\nMy intention is to update each encoder in alternate steps (even steps: `x1_encoder`; odd steps: `x2_encoder`).\r\nAfter reading the documentation, it was not clear to me how it would be possible to update the parameters of each encoder from the loss of each one of them. For instance, I would like to update `x1_encoder`'s parameters based on the `x1_loss` value and leveraging `optimizer_1`. Respectively, I would like to update `x2_encoder`'s parameters based on the `x2_loss` value and employing `optimizer_2`.\r\n\r\nI appreciate any help you can provide.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9807",
    "createdAt": "2021-10-03T15:21:53Z",
    "updatedAt": "2022-10-04T18:26:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "celsofranssa"
    },
    "answer": {
      "body": "I see two ways. I think your example is quite simple so it does not matter which way you choose in the end:\r\n\r\n## 1) Automatic Optimization:\r\n```py\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        if optimizer_idx == 0;\r\n            x1_repr = self.x1_encoder(x1)\r\n            x1_loss=self.x1_loss(x1_repr, x2_repr)\r\n            return x1_loss\r\n       if optimizer_idx == 1:\r\n           x2_repr = ...\r\n           return x2_loss\r\n\r\n\r\n    def configure_optimizers(self):\r\n        return [\r\n        {\"optimizer\": torch.optim.AdamW(self.x1_encoder.parameters(), ...), \"frequency\": 1}, \r\n        {\"optimizer\": torch.optim.AdamW(self.x2_encoder.parameters(), ...), \"frequency\": 1},\r\n    ]\r\n```\r\n(and delete your overridden optimizer step method)\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers (see frequency description)\r\n\r\n## 2) Manual Optimization:\r\n```py\r\ndef __init__(self, hparams):\r\n\r\n    super().__init__()\r\n    self.automatic_optimization = False\r\n    ...\r\n\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        \r\n        opt0, opt1 = self.optimizers()\r\n        if batch_idx % 2 == 0:\r\n            loss = ...\r\n            opt0.zero_grad()\r\n            loss.backward()\r\n            opt0.step()\r\n        else:\r\n            loss = \r\n            opt1.zero_grad()\r\n            loss.backward()\r\n            opt1.step()\r\n```\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nNote: I converted this issue to a GitHub discussion article as this is the primary forum for implementation help questions :) ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-10-03T17:09:54Z"
    }
  },
  {
    "title": "How to load a released PL model in my directory?",
    "body": "Hello! I\u2019m new to PyTorch Lightning. Recently, I am developing a PyTorch project. I want to run an open-source DeepSpeech code in [Github ](https://github.com/SeanNaren/deepspeech.pytorch/)and load its checkpoint. I clone their repository and run successfully. Then, I want to add their model to my project.\r\nIn PyTorch, this is easy. I only need to copy their model definition files and checkpoint files to my project directory. However, when I did these, I received an error when I ran load_from_checkpoint.\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nThen, I try to directly call `torch.load('librispeech_pretrained_v3.ckpt')`. The error is still raised.\r\nThus, I was wondering if I can load their model in my own directory without copying their whole repository?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9797",
    "createdAt": "2021-10-02T02:59:36Z",
    "updatedAt": "2022-09-06T16:04:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ydc123"
    },
    "answer": {
      "body": "Ok, I got it. It's my fault.",
      "author": {
        "login": "ydc123"
      },
      "createdAt": "2021-10-02T11:30:00Z"
    }
  },
  {
    "title": "How to load a released PL model in my directory?",
    "body": "Hello! I\u2019m new to PyTorch Lightning. Recently, I am developing a PyTorch project. I want to run an open-source DeepSpeech code in Github and load its checkpoint. I clone their repository and run successfully. Then, I want to add their model to my project.\r\nIn PyTorch, this is easy. I only need to copy their model definition files and checkpoint files to my project directory. However, when I did these, I received an error when I ran load_from_checkpoint.\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nThen, I try to directly call `torch.load('librispeech_pretrained_v3.ckpt')`. The error is still raised.\r\nThus, I was wondering if I can load their model in my own directory without copying their whole repository?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9795",
    "createdAt": "2021-10-02T02:58:29Z",
    "updatedAt": "2022-12-01T08:20:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ydc123"
    },
    "answer": {
      "body": "I assume you are talking about this one, right? \r\nhttps://github.com/SeanNaren/deepspeech.pytorch\r\n\r\nMaybe you are getting\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nbecause you didn't install that module into your environment. Follow the instructions on the README page of the repository to install the package, i.e., `pip install -e .` in the cloned repo folder. \r\n\r\nThen in your other projects you can import the deepspeech_pytorch module given that you active the same virtual environment. An so loading a model checkpoint should be able to import that too under the same conditions.\r\n\r\nHope that helps\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-10-03T17:26:46Z"
    }
  },
  {
    "title": "DDP failing when using multiple nodes",
    "body": "Hi, I'm trying to use multi-node DDP, but my training job never gets past the following errors:\r\n\r\n```\r\nstore_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n```\r\n\r\nMy code works fine using DDP with multiple GPUs on a single node.\r\n\r\nI'm defining `NODE_RANK`, `WORLD_SIZE`, `MASTER_PORT`, `MASTER_ADDR` as environment variables, but I'm not defining `LOCAL_RANK`, `GROUP_RANK`, `GLOBAL_RANK` or anything else. I was assuming lightning would handle creating the processes and would then set those variables accordingly. Am I mistaken?\r\n\r\n### Edit:\r\nTo add some more details, it looks like what is happening (in some cases) is that the rank 1 node is setup before the rank 0 node and this causes issues:\r\n```\r\n1: [10/02/21 13:14:57] INFO: Node index: 1\r\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\r\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\r\n...\r\n0: [10/02/21 13:15:27] INFO: Node index: 0\r\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\r\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\r\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 2\r\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 3\r\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n```\r\n\r\nHowever, I also get this error if I ensure the rank 0 node is setup first:\r\n```\r\n0: [10/02/21 15:42:10] INFO: Node index: 0\r\n0: GPU available: True, used: True\r\n0: TPU available: False, using: 0 TPU cores\r\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\r\n...\r\n0: [10/02/21 15:42:12] INFO: num_nodes_to_use: 2\r\n0: [10/02/21 15:42:12] INFO: num_gpus_to_use: 2\r\n0: [10/02/21 15:42:12] INFO: Node index: 0\r\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\r\n...\r\n1: [10/02/21 15:43:29] INFO: num_nodes_to_use: 2\r\n1: [10/02/21 15:43:29] INFO: num_gpus_to_use: 2\r\n1: [10/02/21 15:43:29] INFO: Node index: 1\r\n1: GPU available: True, used: True\r\n1: TPU available: False, using: 0 TPU cores\r\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\r\n...\r\n1: [10/02/21 15:44:22] INFO: num_nodes_to_use: 2\r\n1: [10/02/21 15:44:22] INFO: num_gpus_to_use: 2\r\n1: [10/02/21 15:44:22] INFO: Node index: 1\r\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\r\n0: [10/02/21 16:12:13] INFO: Added key: store_based_barrier_key:1 to store for rank: 0\r\n...\r\nline 199, in _env_rendezvous_handler\r\n0:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\n0: RuntimeError: connect() timed out.\r\n...\r\n0: [10/02/21 16:12:23] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n0: [10/02/21 16:12:33] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n0: [10/02/21 16:12:43] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n...\r\nline 199, in _env_rendezvous_handler\r\n1:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\n1: RuntimeError: connect() timed out.\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9793",
    "createdAt": "2021-10-01T17:36:40Z",
    "updatedAt": "2022-06-15T09:01:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EricWiener"
    },
    "answer": {
      "body": "This ended up being an issue with the port a node communicated through was changing everytime the script ran (since DDP re-runs the entire script for each GPU). Note that the `MASTER_PORT` environment variable was only set once and remained the same, but since the node kept getting new ports allocated, the original value of `MASTER_PORT` was invalidated.\r\n\r\n### Extra info:\r\nThe SLURM cluster I use has a utility that will allocate you a port. My issue was that `ddp` will re-run the entire script, so it meant if I launched a 2 node x 4 GPU job, each of the two nodes would allocate 4 different ports for communication.\r\n\r\nI originally had:\r\n```python3\r\ndef setup_multinode():\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nHowever, I needed to change it so the port is only allocated once per node (not once for every GPU):\r\n```python3\r\ndef setup_multinode():\r\n    if os.environ.get(\"LOCAL_RANK\", None):\r\n        # if this node has already been setup\r\n        return\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nThis way each node only allocates a single port to communicate through once.\r\n",
      "author": {
        "login": "EricWiener"
      },
      "createdAt": "2021-10-05T00:29:18Z"
    }
  },
  {
    "title": "KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'",
    "body": "this is my code:\r\n\r\n```Python\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor='hmean',\r\n    mode='max',\r\n    dirpath='../weights',\r\n    filename='DB-{epoch:02d}-{hmean:.2f}',\r\n    save_last=True,\r\n    save_weights_only=True,\r\n)\r\n```\r\n\r\n```\r\ntrainer = pl.Trainer(\r\n    # open this, must drop last\r\n    benchmark=True,\r\n    checkpoint_callback=True,\r\n    gpus=[0],\r\n    max_epochs=1200,\r\n    min_epochs=300,\r\n    logger=[logger],\r\n    callbacks=[early_stop, checkpoint_callback],\r\n    resume_from_checkpoint='../weights/DB-epoch=130-hmean=0.70.ckpt'\r\n)\r\n```\r\n\r\nwhen i try resume train from checkpoint, i got this error: KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9745",
    "createdAt": "2021-09-29T00:49:20Z",
    "updatedAt": "2022-11-01T16:30:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "if you set `save_weights_only=True` in `ModelCheckpoint` then it won't save optimizer/scheduler states in an ideal case. So assigning this checkpoint to resume training won't work because it needs to restore optimizer/scheduler state as well to actually resume it.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-09-29T07:58:16Z"
    }
  },
  {
    "title": "How to apply uniform length batching(smart batching)?",
    "body": "## How to apply uniform length batching(smart batching)?  \r\n  \r\n<img src=\"https://user-images.githubusercontent.com/42150335/130908773-73f38a84-041c-4c13-b102-3dba09493785.png\" width=600>. \r\n  \r\nHi all! I have a question about applying a smart batching system like the above picture.  \r\nTo implement smart batching system, I write the code like below:\r\n  \r\n- Dataset Class\r\n\r\n```python\r\nclass ExampleDataset(Dataset):\r\n    def __init__(self, datas, tokenizer):\r\n        super(ExampleDataset, self).__init__()\r\n        self.tokenizer = tokenizer\r\n\r\n        tokenized = [self.tokenize(data) for data in tqdm(datas, desc='Tokenizing..')]\r\n        self.input_ids, self.attention_masks, self.labels = list(zip(*tokenized))\r\n\r\n    def tokenize(self, data):\r\n        encodings_dict = self.tokenizer(data)\r\n        return [\r\n            encodings_dict['input_ids'],\r\n            encodings_dict['attention_mask'],\r\n            encodings_dict['input_ids']\r\n        ]\r\n\r\n    def __len__(self):\r\n        return len(self.input_ids)\r\n\r\n    def __getitem__(self, idx):\r\n        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\r\n```\r\n\r\n- Sampler Class\r\n\r\n```python\r\nclass SmartBatchingSampler(Sampler):\r\n    def __init__(self, data_source: torch.utils.data.Dataset, batch_size=1):\r\n        super(SmartBatchingSampler, self).__init__(data_source)\r\n        self.batch_size = batch_size\r\n        self.data_source = data_source\r\n\r\n        sentence_lengths = [len(sentence[0]) for sentence in data_source]\r\n        sentence_indices = [idx for idx in range(len(data_source))]\r\n\r\n        pack_by_length = list(zip(sentence_lengths, sentence_indices))\r\n        sort_by_length = sorted(pack_by_length)\r\n        sentence_lengths, sentence_indices = zip(*sort_by_length)\r\n\r\n        self.bins = [\r\n            sentence_indices[i: i + batch_size]\r\n            for i in range(0, len(sentence_indices), batch_size)\r\n        ]\r\n        self.bins = list(chain.from_iterable(self.bins))\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        for ids in self.bins:\r\n            yield ids\r\n\r\n    def __len__(self):\r\n        return len(self.bins)\r\n\r\n    def shuffle(self, epoch):\r\n        np.random.shuffle(self.bins)\r\n```\r\n\r\n- collate_fn function\r\n\r\n```python\r\ndef collate_fn(batch):\r\n    def seq_length_(p):\r\n        return len(p[0])\r\n\r\n    max_seq_sample = max(batch, key=seq_length_)[0]\r\n    max_seq_size = len(max_seq_sample)\r\n\r\n    batch_size = len(batch)\r\n\r\n    input_ids = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n    attention_masks = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n    labels = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n\r\n    for idx in range(batch_size):\r\n        sample = batch[idx]\r\n        sample_input_ids = sample[0]\r\n        sample_attention_masks = sample[1]\r\n        sample_labels = sample[2]\r\n\r\n        input_ids[idx].narrow(0, 0, len(sample_input_ids)).copy_(torch.LongTensor(sample_input_ids))\r\n        attention_masks[idx].narrow(0, 0, len(sample_attention_masks)).copy_(torch.LongTensor(sample_attention_masks))\r\n        labels[idx].narrow(0, 0, len(sample_labels)).copy_(torch.LongTensor(sample_labels))\r\n\r\n    return input_ids, attention_masks, labels\r\n```\r\n\r\n- LightningDataModule\r\n\r\n```python\r\nclass ExampleDataModule(pl.LightningDataModule):\r\n    ...\r\n    ...\r\n\r\n    def train_dataloader(self):\r\n        sampler = SmartBatchingSampler(self.dataset['train'], batch_size=self.batch_size)\r\n        return DataLoader(\r\n            dataset=self.dataset['train'],  # ExampleDataset class\r\n            sampler=sampler,\r\n            collate_fn=collate_fn,\r\n        )\r\n```\r\n\r\nI have three questions.  \r\n  \r\n1. Can I apply it like this?  If not, let me know how to apply it.\r\n2. If it is done in the same way as above, the batch size must be determined in advance. If 'auto_scale_batch_size' is performed, how can I know the determined batch size?\r\n3. If I designate SmartBatchingSampler that the batch size is 32, and 'auto_scale_batch_size' has set the batch size to 128, how does this work?\r\n  \r\nThank you.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9740",
    "createdAt": "2021-09-28T19:14:18Z",
    "updatedAt": "2022-10-13T17:26:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sooftware"
    },
    "answer": {
      "body": "you can sort the data by `len` initially while creating the dataset itself. now just use a sequential sampler to avoid shuffle by just setting `shuffle=False` inside dataloader. collate_fn looks good, although can be optimized a little bit. apart from that even if you use `auto_scale_batch_size`, it will work just fine since your dataset will already be sorted by length.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-09-28T20:24:51Z"
    }
  },
  {
    "title": "How to pass arrays to callbacks?",
    "body": "In a previous version of pytorch lightning I could return a dictionary in the method `validation_epoch_end` and then the content of the dictionary would automatically populate `trainer.callback_metrics`. I can then use this in my callbacks. \r\n\r\nHowever, if I try this in 1.4.8, `trainer.callback_metrics` is an empty dictionary.\r\n\r\nDo you suggest any alternative? Calling `self.log` is not an option, because it cannot log np.ndarrays:\r\n\r\n```\r\nself.log(val_output, [[array]])` was called, but `ndarray` values cannot be logged\r\n``` ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9736",
    "createdAt": "2021-09-28T16:51:10Z",
    "updatedAt": "2023-07-07T09:56:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "fjhheras"
    },
    "answer": {
      "body": "At the end, what I do is to place the dictionary as a new member (e.g. `self.extra_data`). Then, from the callback I can access it `pl_module.extra_data`. I guess it is not clean, but it works. ",
      "author": {
        "login": "fjhheras"
      },
      "createdAt": "2021-10-07T12:30:01Z"
    }
  },
  {
    "title": "WandB login during multi-node distributed training",
    "body": "Hello,\r\nThis is my first post here, sorry if posting a silly question or in incorrect place.\r\n\r\nI am doing multi-node distibuted learning on kubernetes cluster with a RayPlugin. I want to use `WandbLogger`, but get an error, because cluster nodes are not logged in to WandB. \r\nIt looks as follows:\r\n\r\n```\r\nwandb: ERROR ray::RayExecutor.execute() (pid=98717, ip=10.42.2.22, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x7fe5182cf820>)\r\nwandb: ERROR   File \"/home/sportv10/Desktop/ML/lightning/venv/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\r\nwandb: ERROR     super(RayPlugin, self).new_process(\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\r\nwandb: ERROR     results = trainer.run_stage()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 995, in run_stage\r\nwandb: ERROR     return self._run_train()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1044, in _run_train\r\nwandb: ERROR     self.fit_loop.run()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\nwandb: ERROR     self.advance(*args, **kwargs)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\nwandb: ERROR     epoch_output = self.epoch_loop.run(train_dataloader)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\nwandb: ERROR     self.advance(*args, **kwargs)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 163, in advance\r\nwandb: ERROR     self.trainer.logger_connector.update_train_step_metrics()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 221, in update_train_step_metrics\r\nwandb: ERROR     self.log_metrics(self.metrics[MetricSource.LOG])\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 108, in log_metrics\r\nwandb: ERROR     self.trainer.logger.save()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\", line 313, in save\r\nwandb: ERROR     self._finalize_agg_metrics()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\", line 155, in _finalize_agg_metrics\r\nwandb: ERROR     self.log_metrics(metrics=metrics_to_log, step=agg_step)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\r\nwandb: ERROR     return fn(*args, **kwargs)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py\", line 218, in log_metrics\r\nwandb: ERROR     self.experiment.log({**metrics, \"trainer/global_step\": step})\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\", line 42, in experiment\r\nwandb: ERROR     return get_experiment() or DummyExperiment()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\r\nwandb: ERROR     return fn(*args, **kwargs)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\", line 40, in get_experiment\r\nwandb: ERROR     return fn(self)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py\", line 193, in experiment\r\nwandb: ERROR     self._experiment = wandb.init(**self._wandb_init) if wandb.run is None else wandb.run\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 741, in init\r\nwandb: ERROR     wi.setup(kwargs)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 155, in setup\r\nwandb: ERROR     wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 216, in _login\r\nwandb: ERROR     wlogin.prompt_api_key()\r\nwandb: ERROR   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 152, in prompt_api_key\r\nwandb: ERROR     raise UsageError(\"api_key not configured (no-tty). call \" + directive)\r\nwandb: ERROR wandb.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])\r\n```\r\n\r\nIs there a way to login to WandB via Pytorch Lightning API on multiple nodes (by for example providing WandB login key or file containing said key. Something similar seems to be implemented [here](https://docs.ray.io/en/latest/tune/examples/wandb_example.html) in Ray Tune) or must this be done manually on each node of the cluster?\r\n\r\nThank for your help",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9732",
    "createdAt": "2021-09-28T13:04:51Z",
    "updatedAt": "2023-02-17T07:37:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "janpawlowskiof"
    },
    "answer": {
      "body": "This should be fixed if you update your `wandb` version.\r\n `pip install wandb --upgrade`",
      "author": {
        "login": "scottire"
      },
      "createdAt": "2021-11-11T16:40:41Z"
    }
  },
  {
    "title": "How to have a silent lr_find()",
    "body": "Is there a way to make lr_find not print anything while searching?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9635",
    "createdAt": "2021-09-22T04:56:34Z",
    "updatedAt": "2022-10-11T16:14:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "grudloff"
    },
    "answer": {
      "body": "Hey @grudloff,\r\n\r\nI don't believe this is supported. You could either capture the logs or contribute the feature :)\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-09-22T12:28:01Z"
    }
  },
  {
    "title": "Does PL support customizing indicators in checkpoint callback\uff1f",
    "body": "this is my val func:\r\n```Python\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        data = batch\r\n\r\n        output = self.forward(data['img'])\r\n        boxes, scores = self.postprocess(output.cpu().numpy(), batch['shape'])\r\n        raw_metric = self.metric(batch, (boxes, scores))\r\n\r\n        return raw_metric\r\n\r\n    def validation_epoch_end(self, outputs):\r\n\r\n        metric = self.metric.gather_measure(outputs)\r\n        self.log('recall', value=metric['recall'].avg)\r\n        self.log('precision', value=metric['precision'].avg)\r\n        self.log('hmean', value=metric['fmeasure'].avg)\r\n        return {'hmean': metric['fmeasure'].avg}\r\n```\r\n\r\nthis is my checkpoint callback:\r\n\r\n```Python\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor='hmean',\r\n    mode='max',\r\n    dirpath='../det_weights',\r\n    filename='{epoch:02d}-{hmean:.2f}',\r\n    save_last=True,\r\n)\r\ncheckpoint_callback.FILE_EXTENSION = '.pt'\r\n```\r\n\r\nuse this callback, i can only get a `epoch=00-hmean=0.00.pt` file, It does not record the value of heman",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9633",
    "createdAt": "2021-09-22T02:26:01Z",
    "updatedAt": "2023-05-23T09:47:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "i solvd this problem, thanks...",
      "author": {
        "login": "morestart"
      },
      "createdAt": "2021-09-22T02:34:25Z"
    }
  },
  {
    "title": "How to ignore certain \"parameters\" with model checkpointing?",
    "body": "I have some data that I store on my LightningModule during validation.  I want to prevent this from being saved by the model checkpoint.  They are not actually parameters and do not affect the state at all.  I want to maintain other parts of the state, I don't want to use weights only.\r\n\r\nIs it possible to do this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9627",
    "createdAt": "2021-09-21T20:17:06Z",
    "updatedAt": "2022-06-12T09:44:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jmerkow"
    },
    "answer": {
      "body": "Hey @jmerkow,\r\n\r\nThe checkpoint is generated from the `dump_checkpoint` function of the CheckpointConnector. One of the latest hooks called is lightning_module.on_save_checkpoint(checkpoint) here: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/checkpoint_connector.py#L386\r\n\r\nThis is actually done in-place.\r\n\r\nTherefore, you could do the following.\r\n\r\n```py\r\nclass MyModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self, checkpoint):\r\n        # pop the keys you are not interested by\r\n        checkpoint\r\n```",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-09-22T12:02:11Z"
    }
  },
  {
    "title": "Confusion in training_step_end() API",
    "body": "Hi! I am playing around with pytorch-lightning. \r\n\r\n**Problem**\r\nI tried to use 2 gpus and manually merge training loss described in [Lightning in 2 steps](https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html#data-flow).\r\nBut when I call `training_step_end()`, it just gives me only one gpu's loss, not all gpus loss. \r\n![image](https://user-images.githubusercontent.com/31476895/134098524-698ec172-0aa6-46aa-a3a2-2e916850f60c.png)\r\n\r\n**Question**\r\nDo I have to reduce loss myself in `training_step_end()`?\r\n\r\n**My code**\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import optim\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms, datasets\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass BaseImageClassificationSystem(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.backbone = nn.Sequential(nn.Conv2d(1, 64, 3), nn.AdaptiveAvgPool2d((1, 1)))\r\n        self.fc = nn.Linear(64, 10)\r\n\r\n    def forward(self, x):\r\n        return self.fc(torch.flatten(self.backbone(x), 1))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.fc(torch.flatten(self.backbone(x), 1))\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('train/loss', loss)\r\n        return loss\r\n\r\n    def training_step_end(self, losses):\r\n        print(losses)\r\n        return (losses[0] + losses[1]) / 2\r\n\r\n    def configure_optimizers(self):\r\n        return optim.SGD(self.parameters(), lr=0.01)\r\n\r\n\r\ntrain_dl = DataLoader(datasets.MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True),\r\n                      batch_size=128)\r\nmodel = BaseImageClassificationSystem()\r\ntrainer = pl.Trainer(num_processes=8, gpus='1, 2', accelerator='ddp', max_epochs=100)\r\ntrainer.fit(model, train_dl)\r\n```\r\n\r\n**Output**\r\n```Bash\r\ntensor(2.3002, device='cuda:2', grad_fn=<NllLossBackward>)\r\ntensor(2.2930, device='cuda:1', grad_fn=<NllLossBackward>)\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9617",
    "createdAt": "2021-09-21T01:28:51Z",
    "updatedAt": "2023-03-07T06:42:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hankyul2"
    },
    "answer": {
      "body": "It's mentioned in the doc that this configuration works only for DP or DDP2, but in your code, you are using DDP so there will only be 1 loss item since gradient sync happens within DDP so each device has its own loss and backward call and won't require manual reduction of loss across devices.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-12-29T10:10:38Z"
    }
  },
  {
    "title": "ValueError('signal only works in main thread')",
    "body": "Has anyone else run into this error:\r\n`ValueError('signal only works in main thread')`\r\n\r\nI'm running a hyper parameter sweep using Weights and Biases's framework.\r\n\r\nRunning on a GPU on Google Colab which causes all launched runs to fail. Running it locally (Mac OS) prompts 'signal only works in main thread' to be printed to stdout (which also happens on Colab) but it doesn't crash.\r\n\r\nAny ideas? It seems people using [Ray with PL](https://github.com/PyTorchLightning/pytorch-lightning/issues/3651) have come across this. The hacky solution presented there (`os.environ['SLURM_JOB_NAME'] = 'bash'`) doesn't work in my case (neither on Mac OS or Colab).",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9589",
    "createdAt": "2021-09-18T00:55:13Z",
    "updatedAt": "2022-06-04T13:49:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "maxwass"
    },
    "answer": {
      "body": "@max0x7ba @borisdayma The issue has been with #10610 ",
      "author": {
        "login": "kaushikb11"
      },
      "createdAt": "2021-11-19T09:03:01Z"
    }
  },
  {
    "title": "How can I save and restore the trained model when I call fit() at pytorch_lightning every time?",
    "body": "Hi, everyone!\r\nI want to load model from checkpoint when start a training, and save it to disk when finished every epoch automatically,\r\nIs there any nice way to do that correctly?\r\nShall we modify the Trianer code, or just use a special hook?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9551",
    "createdAt": "2021-09-16T00:56:17Z",
    "updatedAt": "2022-09-14T01:40:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "densechen"
    },
    "answer": {
      "body": "- For loading the model, are you looking to load just the weights? If so, take a look at LightningModule.load_from_checkpoint: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#checkpoint-loading\r\n- Otherwise, if you want to load the whole training state (for example, including the optimizer states), take a look here: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#restoring-training-state\r\n\r\n- For saving, take a look at the ModelCheckpoint callback: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#automatic-saving",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-09-16T05:40:52Z"
    }
  },
  {
    "title": "How to define an interval validate callbacks in lightning",
    "body": "I want to train my model in 20000 steps and run evaluation on each 1000 steps\r\nI try to define the callback like this:\r\n```\r\nclass IntervalStepValidate(Callback):\r\n    def __init__(self, config):\r\n        self.config = config\r\n        self.total_steps = 20000\r\n        self.validation_interval = 1000\r\n\r\n    def on_batch_end(self, trainer, pl_module):\r\n        if self.total_steps % self.validation_interval == 0:\r\n            trainer.run_evaluation()\r\n```\r\nBut I find out that there is no `run_evaluation()` in the latest version of Pytorch-Lightning :(\r\nHow can I update this code to get the function I want?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9534",
    "createdAt": "2021-09-15T06:02:19Z",
    "updatedAt": "2023-08-26T23:39:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rentainhe"
    },
    "answer": {
      "body": "there is a `val_check_interval` argument for it inside `Trainer`. You can set `Trainer(val_check_interval=1000)`.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-09-15T06:17:21Z"
    }
  },
  {
    "title": "Inheritance and `save_hyperparameters`",
    "body": "Hello Lightning folks!\r\n\r\nSuppose I have a base model class that I'd like to inherit from as follows:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass ParentModel(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.001,\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n        self.loss = loss\r\n```\r\n\r\nI would like to be able to access the hyperparameters of `ChildModel` and one way to do that is by including `save_hyperparameters()` in the `__init__` as follows:\r\n\r\n```python\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n        self.loss = loss\r\n        self.save_hyperparameters()\r\n```\r\n\r\nHowever, I would like to avoid the need to call `save_hyperparameters()` in every class that inherits from `ParentModel` and I was wondering whether it is possible to do this in PyTorch Lightning somehow? \r\n\r\nOne idea I have in mind is something like a `__post_init__`  that calls `save_hyperparameters()` after the `__init__` is called, but this doesn't seem to be supported.\r\n\r\nThanks!\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9509",
    "createdAt": "2021-09-14T10:33:11Z",
    "updatedAt": "2022-10-12T18:55:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lewtun"
    },
    "answer": {
      "body": "You could do something like this:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass ParentModel(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.001,\r\n        **kwargs\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.save_hyperparameters()\r\n        self.lr = lr\r\n\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__(lr=lr, loss=loss)\r\n        self.loss = loss\r\n```\r\n\r\nThat would save all hparams passed to the parent model (including the ones passed through the kwargs). If you want to go one step further, you could also include the following there:\r\n```python\r\n\r\nfor k, v in kwargs.items():\r\n    setattr(self, k, v)\r\n\r\n```\r\n\r\nwhich sets all attributes that are passed through kwargs automatically as model attributes.\r\nThat means you could also spare the `self.loss=loss` line in the child model :) ",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-09-17T11:53:30Z"
    }
  },
  {
    "title": "ValueError: `Dataloader` returned 0 length. Please make sure that it returns at least 1 batch",
    "body": "use this code, i can get test data. But when i use pl data module to fit train model, i got dataloader returned 0 length error\r\n```Python\r\nimport os\r\nfrom typing import Optional\r\nimport PIL\r\nimport cv2\r\nimport json\r\nimport copy\r\nimport numpy as np\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import Dataset, random_split, DataLoader\r\n\r\nfrom det.det_modules import ResizeShortSize, IaaAugment, EastRandomCropData, MakeBorderMap, MakeShrinkMap\r\n\r\n\r\ndef load_json(file_path: str):\r\n    with open(file_path, 'r', encoding='utf8') as f:\r\n        content = json.load(f)\r\n    return content\r\n\r\n\r\nclass ICDARDataset(Dataset):\r\n    def __init__(self, json_path, img_path, is_train=True):\r\n        self.ignore_tags = ['*', '###']\r\n        self.load_char_annotation = False\r\n        self.data_list = self.load_data(json_path, img_path)\r\n        self.transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n            ]\r\n        )\r\n        self.iaa_augment = IaaAugment()\r\n        self.east_random_crop_data = EastRandomCropData()\r\n        self.make_border_map = MakeBorderMap()\r\n        self.make_shrink_map = MakeShrinkMap()\r\n        self.resize = ResizeShortSize(short_size=736, resize_text_polys=False)\r\n        self.is_train = is_train\r\n\r\n    def load_data(self, json_path: str, img_path) -> list:\r\n        data_list = []\r\n        content = load_json(json_path)\r\n        for item in content:\r\n            p = os.path.join(img_path, item + '.jpg')\r\n            polygons = []\r\n            texts = []\r\n            illegibility_list = []\r\n            for annotation in content[item]:\r\n                if len(annotation['points']) == 0 or len(annotation['transcription']) == 0:\r\n                    continue\r\n                polygons.append(annotation['points'])\r\n                texts.append(annotation['transcription'])\r\n                illegibility_list.append(annotation['illegibility'])\r\n            data_list.append(\r\n                {\r\n                    'img_path': p,\r\n                    'text_polys': np.array(polygons, dtype=object),\r\n                    'texts': texts,\r\n                    'ignore_tags': illegibility_list\r\n                }\r\n            )\r\n        return data_list\r\n\r\n    def __getitem__(self, index):\r\n        data = self.data_list[index]\r\n        im = cv2.imread(data['img_path'])\r\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\r\n\r\n        data['img'] = im\r\n        data['shape'] = [im.shape[0], im.shape[1]]\r\n        if self.is_train:\r\n            data = self.iaa_augment(data)\r\n            data = self.east_random_crop_data(data)\r\n            data = self.make_border_map(data)\r\n            data = self.make_shrink_map(data)\r\n        else:\r\n            data = self.resize(data)\r\n        # resize = ResizeShortSize(short_size=736, resize_text_polys=False)\r\n        # data = resize(data)\r\n        data['img'] = self.transform(data['img'])\r\n        data['text_polys'] = data['text_polys']\r\n        return copy.deepcopy(data)\r\n\r\n    def __len__(self):\r\n        return len(self.data_list)\r\n\r\n\r\nclass DetCollectFN:\r\n    def __init__(self, *args, **kwargs):\r\n        pass\r\n\r\n    def __call__(self, batch):\r\n        data_dict = {}\r\n        to_tensor_keys = []\r\n        for sample in batch:\r\n            for k, v in sample.items():\r\n                if k not in data_dict:\r\n                    data_dict[k] = []\r\n                if isinstance(v, (np.ndarray, torch.Tensor, PIL.Image.Image)):\r\n                    if k not in to_tensor_keys:\r\n                        to_tensor_keys.append(k)\r\n                    if isinstance(v, np.ndarray):\r\n                        v = torch.tensor(v)\r\n                    if isinstance(v, PIL.Image.Image):\r\n                        v = transforms.ToTensor()(v)\r\n                data_dict[k].append(v)\r\n        for k in to_tensor_keys:\r\n            data_dict[k] = torch.stack(data_dict[k], 0)\r\n        return data_dict\r\n\r\n\r\nclass DBDataModule(pl.LightningDataModule):\r\n    def __init__(self, train_json_path, train_img_path, val_json_path, val_img_path):\r\n        super(DBDataModule, self).__init__()\r\n        self.train = ICDARDataset(train_json_path, train_img_path, is_train=True)\r\n        self.val = ICDARDataset(val_json_path, val_img_path, is_train=False)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train, batch_size=32, num_workers=0, shuffle=True, collate_fn=DetCollectFN)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val, batch_size=32, num_workers=0, collate_fn=DetCollectFN)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    import torch\r\n    from torch.utils.data import DataLoader\r\n\r\n    from matplotlib import pyplot as plt\r\n\r\n\r\n    def show_img(imgs: np.ndarray, title='img'):\r\n        from matplotlib import pyplot as plt\r\n        color = (len(imgs.shape) == 3 and imgs.shape[-1] == 3)\r\n        imgs = np.expand_dims(imgs, axis=0)\r\n        for i, img in enumerate(imgs):\r\n            plt.figure()\r\n            plt.title('{}_{}'.format(title, i))\r\n            plt.imshow(img, cmap=None if color else 'gray')\r\n\r\n\r\n    def draw_bbox(img_path, result, color=(255, 0, 0), thickness=2):\r\n        import cv2\r\n        if isinstance(img_path, str):\r\n            img_path = cv2.imread(img_path)\r\n            # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\r\n        img_path = img_path.copy()\r\n        for point in result:\r\n            # point = point.astype(int)\r\n            cv2.polylines(img_path, [point], True, color, thickness)\r\n        return img_path\r\n\r\n\r\n    dataset = ICDARDataset('/home/data/OCRData/icdar2019/train/train.json', '/home/data/OCRData/icdar2019/train/images')\r\n    print(len(dataset))\r\n    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=0)\r\n    for i, data in enumerate(train_loader):\r\n        img = data['img']\r\n        shrink_label = data['shrink_map']\r\n        threshold_label = data['threshold_map']\r\n\r\n        print(threshold_label.shape, threshold_label.shape, img.shape)\r\n        show_img(img[0].numpy().transpose(1, 2, 0), title='img')\r\n        show_img((shrink_label[0].to(torch.float)).numpy(), title='shrink_label')\r\n        show_img((threshold_label[0].to(torch.float)).numpy(), title='threshold_label')\r\n        # img = draw_bbox(img[0].numpy().transpose(1, 2, 0), np.array(data['text_polys']))\r\n        # show_img(img, title='draw_bbox')\r\n        plt.show()\r\n\r\n        break\r\n\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9478",
    "createdAt": "2021-09-13T08:11:54Z",
    "updatedAt": "2022-09-18T17:57:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "Dear @morestart,\r\n\r\nWould you mind unit-testing your code ? Can you check your `DBDataModule` train and `val` ICDARDataset length aren't 0 ?\r\nLightning doesn't manipulate your dataset / dataloaders, so maybe your dataset are empty.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-09-13T08:42:20Z"
    }
  },
  {
    "title": "Lightning CLI is incompatible with models defined by data",
    "body": "Is there an easy way to make a [Lightning CLI](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html) work with a [Lightning Module defined by data](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html)? This seems like a very common design pattern.\r\n\r\nFor example (from the docs) it doesn't appear possible to easily convert the following to a Lightning CLI:\r\n```\r\n# init dm AND call the processing manually\r\ndm = ImagenetDataModule()\r\ndm.prepare_data()\r\ndm.setup()\r\n\r\nmodel = LitModel(out_features=dm.num_classes, img_width=dm.img_width, img_height=dm.img_height)\r\ntrainer.fit(model, dm)\r\n```\r\n\r\nThe current CLI implementation works for model re-loading. However, it requires data-dependent attributes to be specified to the config files prior to fitting, which does not seem ideal.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9452",
    "createdAt": "2021-09-11T18:17:09Z",
    "updatedAt": "2022-06-10T19:01:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ml-mountainman"
    },
    "answer": {
      "body": "Resolved: see https://github.com/PyTorchLightning/pytorch-lightning/issues/9473",
      "author": {
        "login": "ml-mountainman"
      },
      "createdAt": "2021-09-13T19:00:33Z"
    }
  },
  {
    "title": "How to collect batched predictions?",
    "body": "Hello :)\r\nCurrently I use  `trainer.predict(model=..., dataloaders=...)` which returns the results of `predict_step(...)` in a list where each element in the list corresponds to one batch input to the `predict_step` function which I already implemented. I am looking for an `predict_epoch_end` kind of function to collect to batched predictions into one data structure but only found the possibility to define a callback `on_prediction_epoch_end()` but this has no return possibility. How should one best proceed to collect the batched predictions?\r\n\r\ni.e. each predict_step() returns a tensor of shape batch_size x 10\r\nand lets assume the dataloader gives 5 batches to predict. Then the `trainer.predict()` function will return a list of lenght 5 with each element beeing a tensor of shape batch_size x 10. However I would rather like to receive one tensor of shape 5*batch_size x 10. \r\n\r\nThis is just a simple example to illustrate my problem. My actual return per prediction step is a little bit more involved and I would like to clean up the structure before returning it and also make this cleanup logic part of the module so that I dont have to remember the specifics.\r\n\r\nIs there a way to do that in the current framework and if so how?\r\nThanks in advance for any help!\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9379",
    "createdAt": "2021-09-08T20:46:33Z",
    "updatedAt": "2022-06-01T16:08:29Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "VietTralala"
    },
    "answer": {
      "body": "Issue to track https://github.com/PyTorchLightning/pytorch-lightning/issues/9380",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-09-08T22:52:08Z"
    }
  },
  {
    "title": "How to redefine optimizer for pl.LightningModule?",
    "body": "The great advantage and convenient feature of PyTorch-Lightning over vanilla Pytorch is the more convenient interface for optimization loops.  Instead of reinventing the wheel every time with manual `train` and `test` function, one can just define\r\n`training_step` and `validation_step`, e.t.c. \r\n\r\nThe setup of the optimizer is done in the `configure_optimizers` method https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers, where one can create optimizers and schedulers.\r\n\r\nHowever, it seems, that this choice fixes the optimizer properties in the definition of the class, and often you would like to try several optimizers and change their params. One way to do it - is to pass additional arguments to the constructor of \r\n`pl.LightningModule` to be processed in `configure_optimizers`, where an appropriate optimizer will be chosen. \r\n\r\nAlso one may try to access directly the optimizers outside the class, but probably it is an unsafe approach.\r\n\r\nIs there some alternative way to reconfigure optimizer, like in `Tensorflow`, where models have `complie` method https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9354",
    "createdAt": "2021-09-07T04:35:56Z",
    "updatedAt": "2024-06-19T10:28:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Godofnothing"
    },
    "answer": {
      "body": "Hi @Godofnothing ,\r\n\r\nI am afraid there is no clean way to do so. What I personally do to hack around this (if I need it) is that I have arguments to the model that define what is to be returned by `configure_optimizers` and then call `self.trainer.accelerator.setup_optimizers(self.trainer)` in any of the other hooks. That makes sure `configure_optimizers` is called again and converts everything to the correct types. Inside your `configure_optimizers` function you would have to handle the creation of the optimizers for the different stages in training then.",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-09-07T08:14:20Z"
    }
  },
  {
    "title": "Keep only the best and the latest artifacts in wandb logger",
    "body": "Hey,\r\nI am not sure if I can currently keep only the best and latest models as a wandb artifact using the `WandbLogger`? That is, I am looking for a behavior similar to `log_model='all'`, but which keeps only the latest and best models and deletes previous checkpoints from the wandb artifacts of the experiment.\r\nMy checkpoints weigh about 1GB and I don't want to keep the entire history of checkpoints with the `log_model='all'` flag, but rather only the best and the latest models. I thought about inheriting from the `WandbLogger` and following the guideline here:\r\nhttps://gitbook-docs.wandb.ai/guides/artifacts/api#cleaning-up-unused-versions\r\n\r\nAny thoughts?\r\nMaybe this should be a feature request?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9342",
    "createdAt": "2021-09-06T06:38:47Z",
    "updatedAt": "2022-06-23T15:20:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ohayonguy"
    },
    "answer": {
      "body": "Hey @ohayonguy \r\n\r\nGiven the docs of WandbLogger I think you can just set `log_model=True`. This will then upload the checkpoints at the end of training. And if you have set `save_top_k=n` it will only upload the best n ones.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-09-06T14:10:32Z"
    }
  },
  {
    "title": "return_result role in training_batch_loop.py",
    "body": "Hello,\r\n\r\nI have been trying to debug OOM after a few iterations on my model.\r\nAfter investigation I saw that in loops/batch/training_batch_loop.py there is this piece of code (L235):\r\n`result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)`\r\n`  if result is not None:`\r\n`     return_result.update(result)`\r\n`     return return_result.loss`\r\n     \r\nWhat I see is that return_result will keep the computation graph as it contains the loss. So I wonder what is the role of this variable? Also, where is the graph released, I could no go any further than \"_training_step_and_backward_closure\"? I don't understand why my model runs fine for a few iterations then there is some increase in memory.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9332",
    "createdAt": "2021-09-05T09:33:26Z",
    "updatedAt": "2024-12-12T09:17:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Jovp"
    },
    "answer": {
      "body": "Hello Justus!\r\n\r\nIndeed I have seen during my investigations that master changed quite a lot for that part of the code.\r\n\r\nFor the specific concern that I described see:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/9343 and\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/pull/9336\r\n\r\npull request 9336 seems to address exactly the issue that I was referring to.\r\n\r\nMany thanks to the team for the great responsiveness and great work!\r\nJulien",
      "author": {
        "login": "Jovp"
      },
      "createdAt": "2021-09-07T08:27:39Z"
    }
  },
  {
    "title": "Pytorch-lightning CPU-only installation",
    "body": "Hello all - just wanted to discuss a use-case with CPU vs GPU PL install. We do normal training on GPUs, but when deploying for prediction we use CPUs and would like to keep the Docker container size as small as possible. It't not clear if it's possible to install pytorch-lightning with CPU-only torch distribution, which is much smaller. Is there any possible equivalent to `pip install pytorch-lightning[cpu]` . Thanks for suggestions! ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9325",
    "createdAt": "2021-09-04T17:47:56Z",
    "updatedAt": "2024-03-11T13:42:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sivakhno"
    },
    "answer": {
      "body": "Hey, if you install pytorch first (cpu only) and then Lightning it will just use that version. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-09-04T20:42:09Z"
    }
  },
  {
    "title": "Setting random seed before training",
    "body": "Hi,\r\n\r\nDo I need to manually set the random seed to ensure the synchronization of state (e.g. initialized parameters) across processes when using distributed training?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9268",
    "createdAt": "2021-09-02T03:13:26Z",
    "updatedAt": "2022-06-19T06:27:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "w2kun"
    },
    "answer": {
      "body": "No, parameters get broadcast on the first time the model performs forward. This is implemented in the PyTorch [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel) wrapper and so applies to Lightning as well. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-09-04T20:59:20Z"
    }
  },
  {
    "title": "Loading from checkpoints re-downloads pre-trained BERT model",
    "body": "I am defining a simple multi-class BERT classification model and then training it using pytorch-lightning. The code is in https://colab.research.google.com/drive/1os9mz7w7gmLBL_ZDvZ9K1saz9UA3rmD7?usp=sharing under class BertForMulticlassSequenceClassification(BertPreTrainedModel). The issue is that after training when I am loading the classifier model model = ClassTaggerModel.load_from_checkpoint(checkpoint_file) I get\r\n\r\n```\r\nSome weights of BertForMulticlassSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifiers.0.weight', 'classifiers.1.bias', 'classifiers.0.bias', 'classifiers.1.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nThe reason is probably because `pl.LightningModule` module has transformer's `from_pretrained` function that would normally downloads weights from huggingface. This is undesirable behaviour when loading from the trained checkpoint. Is there any feature in Pytroch-lightning that can help having different logic for these two cases (training vs loading). Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9236",
    "createdAt": "2021-08-31T16:53:45Z",
    "updatedAt": "2022-06-11T15:19:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sivakhno"
    },
    "answer": {
      "body": "It's because lightning instantiates the LightningModel and then loads the weights using load_from_checkpoint and since you have HFModel.from_pretrained in the init it will load the pretrained weights every time. There is a way around for this.\r\n\r\n```python\r\nclass HFLightningModule(LightningModule):\r\n    def __init__(self, ..., model_name=None)\r\n        if model_name is not None:\r\n            self.model = HFModel.from_pretrained(model_name, ...)\r\n        else:\r\n            self.model = HFModel(config, num_classes)\r\n\r\n\r\nmodel = HFLightningModule(..., model_name='bert-base-cased')\r\ntrainer.fit(model, ...)\r\n\r\nmodel = HFLightningModule.load_from_checkpoint(...)\r\n```\r\nAlthough there might be a better solution.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-08-31T20:40:50Z"
    }
  },
  {
    "title": "How to set experiment name such that it can be some unique name instead of version_0, ... etc.",
    "body": "I'm currently running a lot of experiments and in order to track all of them in tensorboard I have to rename each experiment folder by hand (e.g. lightning_logs/version_0 -> lightning_logs/{unique_informative_exp_name}).\r\nHowever, it would be better if I could pass the as an argument to Trainer.\r\nI searched documentation thoroughly, but couldn't find anything related to names of experiments (I found only default_root_dir argument, however, it's not that interesting)\r\n\r\nThe question is follows :\r\nIs there a way to set experiment name on program level? I'm using default logger. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9185",
    "createdAt": "2021-08-29T09:44:41Z",
    "updatedAt": "2022-08-20T12:44:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dazzle-me"
    },
    "answer": {
      "body": "You can pass Trainer a custom logger with the version specified.\r\n\r\n```python\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\nlogger = TensorBoardLogger(\"default_root_dir\", version=\"your_version\", name=\"my_model\")\r\ntrainer = Trainer(logger=logger)\r\n```\r\nHere is the api of [TensorBoardLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.TensorBoardLogger.html#pytorch_lightning.loggers.TensorBoardLogger)",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-08-29T12:12:47Z"
    }
  },
  {
    "title": "__about__.py \"version\" field automatically updated (unwanted behavior)",
    "body": "When editing code in VSCode, the `pytorch_lightning/__about__.py` file keeps getting automatically updated\r\n```\r\n__version__ = \"1.5.0dev\" -> __version__ = \"20210827\"\r\n```\r\n\r\nlike in https://github.com/PyTorchLightning/pytorch-lightning/pull/8985/commits/5f4f3c5447a30dde3d78b8727a3ca432ec81131b#r697656347\r\n\r\nDoes anyone know how to stop this from happening? Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9181",
    "createdAt": "2021-08-28T23:41:09Z",
    "updatedAt": "2021-10-19T07:08:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "daniellepintz"
    },
    "answer": {
      "body": "After a few days of my investigation, it turned out that it's not from any extensions but from our script in `.github/`:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/83ce1bf5158c4c03f7811e385624bbd903cd9c5f/.github/prepare-nightly_version.py#L13\r\n\r\nI locally confirmed that `$ pytest` in the project root directory will run all the scripts matching `*.py` except for those in the excluded directories, and thus it also runs the above file. Not sure why everyone doesn't experience this issue, but I'll submit a fix anyway.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-10-19T06:37:37Z"
    }
  },
  {
    "title": "Can we get the value of `self.log()`?",
    "body": " `self.log(on_epoch=True)` computes the metrics across GPUS and epoch batches automatically. How can we get the value of it when training in distributed mode?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9174",
    "createdAt": "2021-08-28T09:28:45Z",
    "updatedAt": "2023-07-18T07:50:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "marsggbo"
    },
    "answer": {
      "body": "It should be available in the `trainer.callback_metrics` dictionary",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-08-31T01:00:15Z"
    }
  },
  {
    "title": "Cross Entropy Loss and loss of PyTorch Lightning does not matches",
    "body": "Hi, I am working on building a question and answering model using T5(Huggingface) with PyTorch Lightning Module and I am checking my loss and PyTorch Lightning Loss is not being matched.\r\n\r\n```\r\nclass UQAFineTuneModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = T5ForConditionalGeneration.from_pretrained(\r\n            \"allenai/unifiedqa-t5-small\", return_dict=True\r\n        )\r\n        self.model.train()\r\n    def forward(\r\n        self,\r\n        source_text_input_ids,\r\n        source_text_attention_mask,\r\n        target_text_input_ids=None,\r\n    ):\r\n        output = self.model(\r\n            input_ids=source_text_input_ids,\r\n            attention_mask=source_text_attention_mask,\r\n            labels=target_text_input_ids,\r\n        )\r\n        return output.loss, output.logits\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        source_text_input_ids = batch[\"source_text_input_ids\"]\r\n        source_text_attention_mask = batch[\"source_text_attention_mask\"]\r\n        target_text_input_ids = batch[\"target_text_input_ids\"]\r\n        # labels_attention_mask = batch[\"target_text_attention_mask\"]\r\n        loss, outputs = self(\r\n            source_text_input_ids, source_text_attention_mask, target_text_input_ids\r\n        ) \r\n        loss_mine = None  \r\n        output = self.model(\r\n            input_ids=source_text_input_ids,\r\n            attention_mask=source_text_attention_mask,\r\n            labels=target_text_input_ids,\r\n        ) \r\n        labels = batch[\"target_text_input_ids\"].clone() \r\n        labels[labels == 0] = -100 \r\n        if target_text_input_ids is not None:  \r\n            loss_fct = CrossEntropyLoss(ignore_index=-100) \r\n            loss_mine = loss_fct(output.logits.view(-1, outputs.size(-1)), labels.view(-1)) \r\n            print(f\"loss_Hugginface: {loss.item()}, loss_mine : {loss_mine.item()}\") \r\n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n        return {\"loss\": loss, \"predictions\": outputs}\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/81796368/131071640-14d4dade-c9ec-4c6a-8ec3-d04f0ed5411c.png)\r\nYou can see the above image, why loss is not same, help is very much needed, I asked the same question on HuggingFace but they told me to ask here, you can view that discussion here.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9159",
    "createdAt": "2021-08-27T04:27:06Z",
    "updatedAt": "2023-09-18T09:37:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ayush714"
    },
    "answer": {
      "body": "Hey @ayush714,\r\n\r\nI believe this is related to HF and you might get your answers by opening an issue on their repo directly.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-27T08:07:39Z"
    }
  },
  {
    "title": "NeptunePossibleLegacyUsageException  when logging accuracy",
    "body": "I want to log accuracy to neptune.ai logger. I tried this way\r\n```\r\ndef test_epoch_end(self, outputs):\r\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy()\r\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy()\r\n        pred=np.argmax(pred,1)\r\n        acc=accuracy_score(label,pred)\r\n        print('acc',acc)\r\n        self.logger.experiment.log_metric(\"acc\", acc)\r\n```\r\nand i am getting this error\r\n```\r\n---------------------------------------------------------------------------\r\nNeptunePossibleLegacyUsageException       Traceback (most recent call last)\r\n<ipython-input-15-a549be9bb6fe> in <module>\r\n----> 1 trainer.test(model)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in test(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\r\n    577 \r\n    578         # run test\r\n--> 579         results = self._run(model)\r\n    580 \r\n    581         assert self.state.stopped\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run(self, model)\r\n    754 \r\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 756         self.dispatch()\r\n    757 \r\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in dispatch(self)\r\n    791     def dispatch(self):\r\n    792         if self.evaluating:\r\n--> 793             self.accelerator.start_evaluating(self)\r\n    794         elif self.predicting:\r\n    795             self.accelerator.start_predicting(self)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py in start_evaluating(self, trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n---> 99         self.training_type_plugin.start_evaluating(trainer)\r\n    100 \r\n    101     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py in start_evaluating(self, trainer)\r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n    147         # double dispatch to initiate the test loop\r\n--> 148         self._results = trainer.run_stage()\r\n    149 \r\n    150     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_stage(self)\r\n    802 \r\n    803         if self.evaluating:\r\n--> 804             return self.run_evaluate()\r\n    805         if self.predicting:\r\n    806             return self.run_predict()\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_evaluate(self)\r\n   1042 \r\n   1043         with self.profiler.profile(f\"run_{self.state.stage}_evaluation\"):\r\n-> 1044             eval_loop_results = self.run_evaluation()\r\n   1045 \r\n   1046         # remove the tensors from the eval results\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_evaluation(self, on_epoch)\r\n    986 \r\n    987         # lightning module method\r\n--> 988         self.evaluation_loop.evaluation_epoch_end(outputs)\r\n    989 \r\n    990         # hook\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py in evaluation_epoch_end(self, outputs)\r\n    206             if is_overridden('test_epoch_end', model=model):\r\n    207                 model._current_fx_name = 'test_epoch_end'\r\n--> 208                 model.test_epoch_end(outputs)\r\n    209 \r\n    210         else:\r\n\r\n<ipython-input-9-b08c6aa77d3f> in test_epoch_end(self, outputs)\r\n    104         acc=accuracy_score(label,pred)\r\n    105         print('acc',acc)\r\n--> 106         self.logger.experiment.log_metric(\"acc\", acc)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\neptune\\new\\run.py in __getattr__(self, item)\r\n    161     def __getattr__(self, item):\r\n    162         if item in LEGACY_METHODS:\r\n--> 163             raise NeptunePossibleLegacyUsageException()\r\n    164         raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{item}'\")\r\n    165 \r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9157",
    "createdAt": "2021-08-27T01:53:04Z",
    "updatedAt": "2022-08-22T07:00:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "Dear @9157,\r\n\r\nNetpune is currently in the process of switching their client and I believe you are using their latest version while the currently Logger within Lightning users the old client.\r\nPlease, downgrade your netpune version and it should work !\r\n\r\nNote: Netpune is currently in the process of updating the logger within Lightning.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-27T08:09:58Z"
    }
  },
  {
    "title": "DeepSpeedPlugin with activation checkpoint fails",
    "body": "Hi there, \r\nI'm trying to run pytorch-lightning training with deepspeed plugin and activation checkpoints to support bigger batch sizes, based on https://pytorch-lightning.readthedocs.io/en/stable/advanced/advanced_gpu.html#deepspeed-activation-checkpointing. \r\nAs specified in the docs, running the model should be done using the checkpoint function. However, this function seems to return a tensor without gradients. When computing loss based on this value and returning from `training_step`, I'm getting an error\r\n`RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`\r\n\r\nMinimal code to reproduce\r\n```python\r\nimport os\r\n\r\nimport deepspeed\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom deepspeed.ops.adam import FusedAdam\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch import nn\r\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\r\nfrom torch.utils.data import DataLoader, RandomSampler\r\n\r\n\r\nclass PlModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = nn.Linear(1, 1)\r\n\r\n    def forward(self, batch):\r\n        return self.model(batch)\r\n\r\n    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\r\n        res = deepspeed.checkpointing.checkpoint(self.model, batch)\r\n        return nn.MSELoss()(res, torch.zeros_like(res, device=res.device))\r\n\r\n    def configure_optimizers(self):\r\n        return FusedAdam(self.parameters(), lr=0.1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    trainer = pl.Trainer(gpus=-1, precision=16, plugins=DeepSpeedPlugin(stage=3, partition_activations=True))\r\n    model = PlModel()\r\n    dataset = torch.rand(100, 1)\r\n    dl = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=os.cpu_count(),\r\n                                     sampler=RandomSampler(dataset))\r\n    trainer.fit(model, dl)\r\n```\r\n\r\npytorch-lightning version: 1.3.3\r\ndeepspeed version: 0.5.0\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9144",
    "createdAt": "2021-08-26T18:47:56Z",
    "updatedAt": "2024-09-04T20:15:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nachshonc"
    },
    "answer": {
      "body": "Thanks @nachshonc!\r\n\r\nI've managed to reproduce the same case without Deepspeed using `torch.utils.checkpoint` and our bug report model:\r\n\r\n```python\r\nimport deepspeed\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nI think the issue arises from the fact that the entire model's activations have been removed, with the input tensors not requiring any gradients, thus the autograd engine not being able to infer any gradients.\r\n\r\nFor activation checkpointing, it only makes sense to include it if you have intermediate layers which can create expensive activations. For example, swap the model out to look like this:\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = torch.utils.checkpoint.checkpoint(self.layer_h, x)\r\n        return self.layer(x)\r\n```\r\n\r\nActivation checkpointing just means on the backwards, we'll need to re-compute the activations (unless you do CPU checkpointing with Deepspeed or something, where activations are just transferred to the CPU memory). In this case, there is no point checkpointing the final layer, as the final layer will instantly need to be re-computed.\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.layer_h(x)\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x) # no point doing this!\r\n```\r\n\r\nWe should definitely make the docs clearer for this, I'll make this an issue :)\r\n\r\n",
      "author": {
        "login": "SeanNaren"
      },
      "createdAt": "2021-08-26T22:56:29Z"
    }
  },
  {
    "title": "Run specific code only once (which generates randomized values) before starting DDP",
    "body": "Hi. I have a function which generates a set of random values (hyperparameters) which are then used to create my model. I want to run this function only once, then use it to create my model and then start ddp training on this model.\r\n\r\nHowever, with the current setup, when I start ddp, the randomize function gets called again, so now I have 2 GPU processes, each having initialized the model with different set of hyperparameters. (random values from both calls aren't same)\r\n\r\nIf I add `if os.getenv(\"LOCAL_RANK\",0):` before my randomize function, then there is no way for the second GPU process to access the hyperparameters generated by the first GPU process. How do I go about this ? Thanks.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9134",
    "createdAt": "2021-08-26T11:30:04Z",
    "updatedAt": "2022-06-14T05:31:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Gateway2745"
    },
    "answer": {
      "body": "Hey @Gateway2745,\r\n\r\nYou could do this\r\n\r\n```py\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom unittest import mock\r\nimport optuna\r\n\r\nconfig_path = ...\r\n\r\nclass MyModel(LightningModule):\r\n\r\n    def __init__(self, num_layers):\r\n        ...\r\n\r\ndef objective(trial):\r\n    num_layers = trial.suggest_uniform('num_layers', 10, 100)\r\n\r\n    with mock.patch(\"sys.argv\", [\"any.py\", \"--config\", str(config_path), \"--trainer.accelerator\", \"ddp_spawn\", \"--trainer.gpu\", \"2\", \"--model.num_layers\", str(num_layers)]):\r\n        cli = LightningCLI(MyModel, MyDataModule)\r\n\r\n    return cli.trainer.model_checkpoint.best_score\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=100)\r\nstudy.best_params\r\n```",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-26T18:51:49Z"
    }
  },
  {
    "title": "Hyperparameter Tuning in Lightning CLI",
    "body": "I wonder how people do Hyperparameter Tuning with Lightning CLI?\r\nAny suggestion of good practices?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9108",
    "createdAt": "2021-08-25T14:24:11Z",
    "updatedAt": "2022-06-10T19:01:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ohayonguy"
    },
    "answer": {
      "body": "Personally when I tune hyperparameters (e.g. with optuna or nevergrad), I don't use the Lightning CLI much but use the programmatic way to inject the arguments there (since it's easier for communication across different python libs directly in python and not leaving it to os calls).",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-08-25T15:44:42Z"
    }
  },
  {
    "title": "Calculation of inference time",
    "body": "Hi . I want to calculate the inference time of my model. I am not sure where to the code for measuring the time. I thought its better to do it inside `predict_step` of the `LightningModule`. Something like this \r\n\r\n```\r\nclass LitModule(pl.LightningModule):\r\n        def __init__(self, hparams):\r\n               ....\r\n               self.starter, self.ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\r\n               .....\r\n        ......\r\n        ......\r\n         def predict_step(self, batch, batch_idx, dataloader_idx=None):\r\n                 torch.cuda.empty_cache()\r\n                 minibatch_model_in, _ = batch\r\n                 self.starter.record()\r\n                 _ = self(minibatch_model_in)\r\n                 self.ender.record()\r\n                 # wait for gpu sync\r\n                 torch.cuda.synchronize()\r\n                 inference_time = self.starter.elapsed_time(self.ender)\r\n                 return inference_time*1e-3\r\n```\r\nAnd in the main funtion,\r\n```\r\ninference_metrics = trainer.predict(model=pl_model, datamodule=pl_data)\r\n```\r\nAfter removing the initial measurements (considering GPU warm-up) and taking mean of 200 samples, I get `0.0196` seconds.\r\n\r\nIf I do the measurement outside the LightningModule then I get a different value. This is how I measured \r\n```\r\n# Inferencing\r\npl_data.setup()\r\n# Initializing cuda event loggers\r\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\r\nmetrics = []\r\npl_model.eval()\r\nwith torch.no_grad():\r\nfor batch in pl_data.train_dataloader():\r\n      torch.cuda.empty_cache()\r\n      minibatch_model_in, _, _, _, _, _ = batch\r\n      starter.record()\r\n        _, _, _ = pl_model(minibatch_model_in)\r\n      # wait for gpu sync\r\n       torch.cuda.synchronize()\r\n       curr_time = starter.elapsed_time(ender)\r\n       metrics.append(curr_time)\r\n```\r\nUsing the above code, I get `0.027` seconds. Please someone tell me if I am doing something wrong. Which method is correct? Why is there a decrease in the inference time when measured within `predict_step`?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9068",
    "createdAt": "2021-08-24T02:29:45Z",
    "updatedAt": "2022-06-04T15:52:41Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "karthi0804"
    },
    "answer": {
      "body": "Hi, the first snippet measures raw prediction time, whereas the second one doesn't seem to be on gpu at all (e.g. your data likely isn't on gpu) or (if it is) can already include the data preparation and host to device transfers.",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-08-25T11:43:13Z"
    }
  },
  {
    "title": "Validation crashes when setting seed or with val_num_workers > 0 with CUDA initialization error",
    "body": "I get the below error immediately when I set a CUDA seed or after a few epochs without seed and with val_workers>0. I also find that I get the error when I try reading from a pickled file containing PyTorch tensors (on cpu).\r\n\r\nI have a small dataset, so I load all the data in the `__init__` of my `Dataset` class. I then save it on my disk using pickle so I can save on dataloading time when I run my code again. Now, since I have 2 GPUs, DDP in pytorch-lightning starts 2 processes and each of these processes start reading from the pickle file. Both the training data and validation data are being read from pickle files. \r\n\r\nMy error is quite similar to the the comment mentioned here - https://github.com/pytorch/pytorch/issues/28950#issuecomment-694938684. Note that both the person who commented and I, have `pin_memory=False` although the title says otherwise. \r\n\r\nAny ideas as to why this is happening will surely help me! Thank you.\r\n\r\nPL version = 1.4.2 , torch version = '1.9.0+cu102', CUDA version = 10.2\r\n\r\n```\r\nValidation sanity check: 0it [00:00, ?it/s]/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val \r\ndataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this m\r\nachine) in the `DataLoader` init to improve performance.                                                                                                                      \r\n  rank_zero_warn(                                                                                                                                                             \r\nValidation sanity check:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\r\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\r\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \r\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \r\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\r\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \r\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \r\nGlobal seed set to 42                                                                                                                                                         \r\nGlobal seed set to 42                                                                                                                                                         \r\nEpoch 0:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                     | 4/5 [00:14<00:02,  2.80s/it, loss=4.33, v_num=d09et\r\nerminate called after throwing an instance of 'c10::CUDAError'                                                                                          | 0/1 [00:00<?, ?it/s]\r\n  what():  CUDA error: initialization error                                                                                                                                   \r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \r\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b5f7135ca22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \r\nframe #1: <unknown function> + 0x10d7e (0x2b5f710ecd7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \r\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b5f710ee027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \r\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b5f713465a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \r\nframe #4: <unknown function> + 0xa27e1a (0x2b5f1a569e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \r\n<omitting python frames>                                                                                                                                                      \r\n                                                                                                                                                                              \r\nterminate called after throwing an instance of 'c10::CUDAError'                                                                                                               \r\n  what():  CUDA error: initialization error                                                                                                                                   \r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \r\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b4b41756a22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \r\nframe #1: <unknown function> + 0x10d7e (0x2b4b414e6d7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \r\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b4b414e8027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \r\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b4b417405a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \r\nframe #4: <unknown function> + 0xa27e1a (0x2b4aea963e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \r\n<omitting python frames>                                                                                                                                                      \r\n                                                                                                                                                                              \r\nTraceback (most recent call last):                                                     \r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\nTraceback (most recent call last):                                             \r\nFile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\r\n    if not self._poll(timeout):\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\r\n    if not self._poll(timeout):\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\r\n    return self._poll(timeout)\r\n    return self._poll(timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\r\n    r = wait([self], timeout)\r\n    r = wait([self], timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\r\n    ready = selector.select(timeout)\r\n    ready = selector.select(timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\n    fd_event_list = self._selector.poll(timeout)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3404) is killed by signal: Aborted. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3407) is killed by signal: Aborted. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\r\n    main()\r\n  File \"/home/usr/mymodel/run.py\", line 18, in main\r\n   main()\r\n  File \"/home/usr/mymodel/run.py\", line 18, in main\r\n    return train(CFG)\r\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\r\n    return train(CFG)\r\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\r\n    trainer.fit(model,dm)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    trainer.fit(model,dm)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    self._run(model)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._run(model)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self._dispatch()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.accelerator.start_training(self)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\r\n    return self._run_train()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self.fit_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._results = trainer.run_stage()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    return self._run_train()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\r\n    self.on_advance_end()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\r\n\r\n   self.fit_loop.run()                                                                                                                                                       \r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._run_validation()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n    self.val_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    self.on_advance_end()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\r\n    dl_outputs = self.epoch_loop.run(\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._run_validation()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\r\n    self.val_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    batch_idx, batch = next(dataloader_iter)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    data = self._next_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\r\n    batch_idx, batch = next(dataloader_iter)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    idx, data = self._get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n    data = self._next_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n    success, data = self._try_get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\r\n    idx, data = self._get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\nRuntimeError: DataLoader worker (pid(s) 3404) exited unexpectedly\r\n    success, data = self._try_get_data()\r\nile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\nRuntimeError: DataLoader worker (pid(s) 3407) exited unexpectedly                          \r\n```                                                                      ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9060",
    "createdAt": "2021-08-23T17:35:40Z",
    "updatedAt": "2022-06-14T05:31:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Gateway2745"
    },
    "answer": {
      "body": "Same as https://github.com/PyTorchLightning/pytorch-lightning/issues/8821.",
      "author": {
        "login": "Gateway2745"
      },
      "createdAt": "2021-08-25T17:20:44Z"
    }
  },
  {
    "title": "Why would GPU memory always surge after training and cause CUDA memory error?",
    "body": "I use pytorch lightning to train a model but it always strangely fail at end: After validations completed, the trainer will start an epoch that bigger that *max_epoch* and causing GPU memory allocation failure (CUDA out of memory) right after this epoch (which should not run) started. For my example, I set max_epoch=5 so there should only be epoch 0-4. But there will always be an additional epoch-5 after 5 validations are done and a few seconds later the CUDA memory error will occur.\r\n\r\nNotebook log:\r\n![log](https://user-images.githubusercontent.com/53288091/130439543-2de5205d-d11f-43b1-9f52-65837e493690.JPG)\r\n\r\nWandb system info:\r\n![image](https://user-images.githubusercontent.com/53288091/130439389-4d93caf7-e467-486e-8ef9-da24e80f569d.png)\r\n\r\nMy dataset should be fine as CUDA memory and system memory are stable during the training period, except the GPU memory surge at the very end. And here are my code for lightning module and training loop which I think may cause this trouble:\r\n\r\n```\r\nclass BaseModel(pl.LightningModule):\r\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'], pretrained=True):\r\n        super().__init__()\r\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\r\n        \r\n        # Change output features. Input features keep the same.\r\n        if model_name == 'resnet18d':\r\n            n_features = self.model.fc.in_features\r\n            self.model.fc = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        if model_name == 'nfnet_f1':\r\n            n_features = self.model.head.fc.in_features\r\n            self.model.head.fc = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        elif model_name == 'efficientnet_b1':\r\n            n_features = self.model.classifier.in_features\r\n            self.model.classifier = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        \r\n    def forward(self, x):\r\n        output = self.model(x)\r\n        return output\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self.model(x)\r\n        labels = y.unsqueeze(1)\r\n        loss = self.criterion(output, labels)\r\n\r\n        try:\r\n            auc = roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu())\r\n            self.log('auc', auc, on_step=True, prog_bar=True, logger=True)\r\n            self.log('Train Loss', loss, on_step=True, prog_bar=True, logger=True)\r\n        except:\r\n            pass\r\n\r\n        return {'loss': loss, 'predictions': output, \"labels\": labels}\r\n\r\n    def training_epoch_end(self, outputs):\r\n        preds = []\r\n        labels = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions'].detach()\r\n            labels += output['labels'].detach()\r\n\r\n        preds = torch.stack(preds)\r\n        labels = torch.stack(labels)\r\n\r\n        train_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log('mean_train_auc', train_auc, prog_bar=True, logger=True)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self.model(x)\r\n        labels = y.unsqueeze(1)\r\n        loss = self.criterion(output, labels)\r\n\r\n        self.log('val_loss', loss, on_step=True, prog_bar=True, logger=True)\r\n        return {'predictions': output, 'labels': labels}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        preds = []\r\n        labels = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions'].detach()\r\n            labels += output['labels'].detach()\r\n\r\n        preds = torch.stack(preds)\r\n        labels = torch.stack(labels)\r\n\r\n        val_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log('val_auc', val_auc, prog_bar=True, logger=True)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self(x).sigmoid()\r\n        return output\r\n\r\n    def configure_optimizers(self):\r\n        param_optimizer = list(self.model.named_parameters())\r\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # no decay\r\n        optimizer_parameters = [\r\n            {\r\n                'params': [\r\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\r\n                ], # Do not optimize no decay parameters.\r\n                'weight_decay': params['weight_decay'],\r\n            },\r\n            {\r\n                'params': [\r\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\r\n                ],\r\n                'weight_decay': 0.0,\r\n            }\r\n        ]\r\n\r\n        optimizer = FusedAdam(optimizer_parameters, lr=params['lr'])\r\n\r\n        scheduler = CosineAnnealingLR(optimizer,\r\n                                      T_max=params['T_max'],\r\n                                      eta_min=params['min_lr'],\r\n                                      last_epoch=-1)\r\n\r\n        # Give out optimizer & scheduler for pytorch lightning in python dict format.\r\n        return dict(optimizer=optimizer,\r\n                    lr_scheduler=scheduler) # lr_scheduler for scheduler.  \r\n```\r\n\r\n```\r\nkfolds = StratifiedKFold(n_splits=params['nfolds'], shuffle=True, random_state=params['seed'])\r\n\r\nmodel = BaseModel()\r\n\r\nfor fold, (trn_idx, val_idx) in enumerate(kfolds.split(train_df[\"id\"], train_df['target'])):\r\n    # Run first round.\r\n    if fold != 0:\r\n        continue\r\n    \r\n    # PL + wandb\r\n    wandb_logger = WandbLogger(project='G2Net-steady-exp',\r\n                               config=params,\r\n                               group='Effnet-CQT',\r\n                               job_type='train',\r\n                               name=f'Fold{fold}')\r\n    print(f\"{'='*20} Fold: {fold} {'='*20}\")\r\n    \r\n    # Set up data module.\r\n    train_data = train_df.loc[trn_idx]\r\n    train_sample_data = data_sample(train_data)\r\n    valid_data = train_df.loc[val_idx] # About 65k samples.\r\n    data_module = DataModule(train_sample_data,\r\n                             valid_data,\r\n                             valid_data) # No test data yet.\r\n    data_module.setup()\r\n    \r\n    # Add callbacks.\r\n    early_stopping_callback = EarlyStopping(monitor='val_auc',\r\n                                            mode='max',\r\n                                            patience=5)\r\n    checkpoint_callback = ModelCheckpoint(dirpath='./checkpoints/',\r\n                                          filename= f'fold-{fold}-best' + '-val_auc{val_auc:.3f}',\r\n                                          save_top_k=2,\r\n                                          verbose=True,\r\n                                          monitor='val_auc',\r\n                                          mode='max')\r\n    \r\n    trainer = pl.Trainer(gpus=1,\r\n                         callbacks=[early_stopping_callback,\r\n                                    checkpoint_callback],\r\n                         max_epochs=params['epochs'],\r\n                         precision=params['precision'],\r\n                         progress_bar_refresh_rate=1,\r\n                         stochastic_weight_avg=True,\r\n                         logger=wandb_logger)\r\n    \r\n    trainer.fit(model, data_module)\r\n```\r\n\r\nCan I get any clue about why this would happen and how to avoid it ? I'm new to pytorch lightning so there might be problems I'm not aware of. Thanks a lot!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9048",
    "createdAt": "2021-08-23T11:25:40Z",
    "updatedAt": "2022-05-31T18:44:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EMUNES"
    },
    "answer": {
      "body": "Let's continue discussing in https://github.com/PyTorchLightning/pytorch-lightning/issues/9441\r\n\r\nLocking this thread to avoid discussing in two places.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-09-11T14:21:38Z"
    }
  },
  {
    "title": "Get best metrics after call to trainer.fit without evaluating best model on val set again",
    "body": "Hi. I have a `ModelCheckpoint` callback where I am monitoring the `val_acc` (validation set accuracy) with `mode=max`. After training my model using `trainer.fit`, I want to get the maximum value of `val_acc`. How can I get this value without re-evaluating my best model on the validation dataset? Thanks.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9016",
    "createdAt": "2021-08-20T14:27:13Z",
    "updatedAt": "2022-06-14T05:31:54Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Gateway2745"
    },
    "answer": {
      "body": "Can be accessed with checkpoint_callback.best_model_score.",
      "author": {
        "login": "Gateway2745"
      },
      "createdAt": "2021-08-21T06:55:43Z"
    }
  },
  {
    "title": "Installation with conda?",
    "body": "I got this question from someone around installing Lightning with conda\r\n\r\nRunning both `conda install -c conda-forge pytorch-lightning`  and `conda update pytorch-lightning` resulted in the version 0.8.5 being used, instead of the latest (as of now, 1.4.2). https://anaconda.org/conda-forge/pytorch-lightning\r\nI'm curious if anyone else has seen this issue. @Borda ? \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8990",
    "createdAt": "2021-08-19T04:57:48Z",
    "updatedAt": "2022-07-09T14:13:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ananthsub"
    },
    "answer": {
      "body": "Conda-forge does not have any latest pytorch (the last is 1.1) which in leads installing pytorch-lightning 0.8 as it was last supporting this PT version... You need to add channel pytorch\r\n`conda install -c pytorch -c conda-forge pytorch-lightning`",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-08-19T07:03:37Z"
    }
  },
  {
    "title": "Problem with dictionary of DataLoaders in training_step.",
    "body": "I followed the steps from the documentation when it comes to returning a dictionary of DataLoaders. The problems occurs in the training_step method. I expect the batch parameter to be a dictionary, but in reality it is a string, the first key of the expected dictionary.\r\n\r\n```\r\ndef train_dataloader(self):\r\n    dl1 =  DataLoader(IconDataset(self.train_icons_path),batch_size=128,\r\n              collate_fn=self.triplet_mining.batch_hard_negatives,num_workers=5)\r\n    dl2 = DataLoader(IconDataset(self.train_icons_path),batch_size=128,\r\n              collate_fn=self.triplet_mining.batch_semi_hard_negative,num_workers=5)\r\n    return {\r\n            \"hard\": dl1,\r\n            \"semi-hard\": dl2 }\r\n```\r\n\r\nIn training_step\r\n```\r\ndef training_step(self,batch,batch_idx):\r\n    print(type(batch))   # str\r\n    print(batch)         # hard\r\n```\r\n\r\nI don't know if there is a problem with my collate_fn method. The batching was working all right when one single DataLoader was used.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8978",
    "createdAt": "2021-08-18T12:16:16Z",
    "updatedAt": "2022-06-29T04:46:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "SergiuTalmacel"
    },
    "answer": {
      "body": "It's so funny how fast one can find the answer to his own question. The problem was related to the validation DataLoaders. I was trying to return a dictionary in a similar manner to the training_dataloader, but the validation loop works with a sequence of DataLoaders. There is a similar question answered here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8623",
      "author": {
        "login": "SergiuTalmacel"
      },
      "createdAt": "2021-08-18T12:32:33Z"
    }
  },
  {
    "title": "Trainer flags: amp_level vs. precision",
    "body": "Hi! \r\n\r\nI'm a bit confused regarding the trainer's flags. According to Lightning's [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#amp-level), the default settings are:\r\n- `amp_level='O2'`  (i.e., \"Almost FP16\" Mixed Precision, cf. Nvidia [documentation](https://nvidia.github.io/apex/amp.html#opt-levels))\r\n- `precision=32`\r\n\r\nIsn't it a bit contradictory ? Is the default training mode full precision or mixed precision? \r\n\r\nThanks in advance for your clarification :)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8923",
    "createdAt": "2021-08-15T17:23:16Z",
    "updatedAt": "2022-10-14T08:56:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dianemarquette"
    },
    "answer": {
      "body": "Dear @dianemarquette,\r\n\r\n`precision=32` is the default one. However, if you turn on `precision=16` and set `amp_backend=\"apex\"`, then `amp_level=02` is considered as the default one.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-16T09:01:53Z"
    }
  },
  {
    "title": "TypeError: 'Subset' object is not callable",
    "body": "I am getting this weird error `TypeError: 'Subset' object is not callable`\r\n```\r\nclass OurModel(LightningModule):\r\n    def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        #architecute\r\n        self.model =  timm.create_model('efficientnetv2_rw_s', pretrained=True)\r\n        self.dataset=torchvision.datasets.ImageFolder('./PSL DATA SET')\r\n\r\n        self.train, self.val, self.test = torch.utils.data.random_split(self.dataset, [1009, 250, 250])\r\n\r\n    def forward(self,x):\r\n        x= self.model(x)\r\n        return x\r\n\r\n    def val_dataloader(self):\r\n        print('val_dataloader')\r\n        ds=DataLoader(MyLazyDataset(self.val,aug), batch_size = self.batch_size,num_workers=self.numworker, shuffle=False)\r\n        print(len(ds))\r\n        return ds\r\n\r\n    def validation_step(self,batch,batch_idx):\r\n        print('validation step')\r\n        image,label=batch\r\n        out=self(image).squeeze(1)\r\n        loss=self.criterion(out,label.float())\r\n        self.log('val_loss', loss)#use by early stopping\r\n        return loss\r\n```\r\nprint statement shows `len(ds)` but it does not print 'validation step', so data from `val_dataloader` is not moving to `validation_step`\r\n\r\nComplete logs\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-8edf32f6d94d> in <module>\r\n      8 #trainer.tune(model)\r\n      9 \r\n---> 10 trainer.fit(model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    458         )\r\n    459 \r\n--> 460         self._run(model)\r\n    461 \r\n    462         assert self.state.stopped\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    756 \r\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 758         self.dispatch()\r\n    759 \r\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    797             self.accelerator.start_predicting(self)\r\n    798         else:\r\n--> 799             self.accelerator.start_training(self)\r\n    800 \r\n    801     def run_stage(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     94 \r\n     95     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n---> 96         self.training_type_plugin.start_training(trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    142     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n    143         # double dispatch to initiate the training loop\r\n--> 144         self._results = trainer.run_stage()\r\n    145 \r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    807         if self.predicting:\r\n    808             return self.run_predict()\r\n--> 809         return self.run_train()\r\n    810 \r\n    811     def _pre_training_routine(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    842             self.progress_bar_callback.disable()\r\n    843 \r\n--> 844         self.run_sanity_check(self.lightning_module)\r\n    845 \r\n    846         self.checkpoint_connector.has_trained = False\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)\r\n   1110 \r\n   1111             # run eval step\r\n-> 1112             self.run_evaluation()\r\n   1113 \r\n   1114             self.on_sanity_check_end()\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, on_epoch)\r\n    930 \r\n    931         # enable eval mode + no grads\r\n--> 932         self.evaluation_loop.on_evaluation_model_eval()\r\n    933         # ref model\r\n    934         model = self.lightning_module\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_model_eval(self)\r\n     87             model_ref.on_test_model_eval()\r\n     88         else:\r\n---> 89             model_ref.on_validation_model_eval()\r\n     90 \r\n     91     def on_evaluation_model_train(self) -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py in on_validation_model_eval(self)\r\n    195         Sets the model to eval during the val loop\r\n    196         \"\"\"\r\n--> 197         self.trainer.model.eval()\r\n    198 \r\n    199     def on_validation_model_train(self) -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in eval(self)\r\n   1291             Module: self\r\n   1292         \"\"\"\r\n-> 1293         return self.train(False)\r\n   1294 \r\n   1295     def requires_grad_(self: T, requires_grad: bool = True) -> T:\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8915",
    "createdAt": "2021-08-14T17:26:50Z",
    "updatedAt": "2022-06-21T16:34:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "Reason `train` is a method thats why its throwing this error. Renaming `self.train` to `self.trainx` resolved the error. but I think logs should print this message",
      "author": {
        "login": "talhaanwarch"
      },
      "createdAt": "2021-08-14T17:30:02Z"
    }
  },
  {
    "title": "How save deepspeed stage 3 model with pickle or torch",
    "body": "Hi, I'm trying to save a model trained using deepspeed stage 2 using this code:\r\n```\r\ntrainer = pl.Trainer(\r\n    gpus=4,\r\n    plugins=DeepSpeedPlugin(\r\n              stage=3,\r\n              cpu_offload=True,\r\n              partition_activations=True,),\r\n    precision=16,\r\n    accelerator=\"ddp\",\r\n    )\r\ntrainer.fit(model, train_dataloader)\r\n```\r\n\r\nWith stage 2 it worked if I added this code:\r\n\r\n```\r\ntrainer = pl.Trainer(gpus=0,max_epochs=0,)\r\ntrainer.fit(model, train_dataloader)\r\npickle.dump(model,open(\"model.p\",\"wb\")\r\n```\r\n\r\nBut using stage=3 I get this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"t5-11b-regression.py\", line 227, in <module>\r\n    torch.save(model,fileName)\r\n  File \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 379, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 484, in _save\r\n    pickler.dump(obj)\r\nAttributeError: Can't pickle local object 'FP16_DeepSpeedZeroOptimizer_Stage3._register_hooks_recursively.<locals>._post_forward_module_hook'\r\n\r\nI also tried saving using torch.save, but got same error. I also tried both pytorch-lightning version 1.3.8 and 1.4.1\r\n\r\ncc: @SeanNaren ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8910",
    "createdAt": "2021-08-14T12:40:26Z",
    "updatedAt": "2022-06-10T13:04:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ViktorThink"
    },
    "answer": {
      "body": "After some debugging with a user, I've come up with a final script to show how you can use the `convert_zero_checkpoint_to_fp32_state_dict` to generate a single file that can be loaded using pickle, or lightning.\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom pytorch_lightning.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        strategy=DeepSpeedPlugin(stage=2),\r\n        precision=16,\r\n        gpus=2,\r\n        callbacks=ModelCheckpoint(dirpath='checkpoints', save_last=True)\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n    # once saved via the model checkpoint callback,\r\n    # it saves a folder containing the deepspeed checkpoint rather than a single file\r\n    checkpoint_path = \"checkpoints/last.ckpt/\"\r\n\r\n    if trainer.is_global_zero:\r\n        single_ckpt_path = \"single_model.pt\"\r\n\r\n        # magically converts the folder into a single lightning loadable pytorch file (for ZeRO 1,2 and 3)\r\n        convert_zero_checkpoint_to_fp32_state_dict(checkpoint_path, single_ckpt_path)\r\n        loaded_parameters = BoringModel.load_from_checkpoint(single_ckpt_path).parameters()\r\n\r\n        model = model.cpu()\r\n        # Assert model parameters are identical after loading\r\n        for orig_param, saved_model_param in zip(model.parameters(), loaded_parameters):\r\n            if model.dtype == torch.half:\r\n                # moved model to float32 for comparison with single fp32 saved weights\r\n                saved_model_param = saved_model_param.half()\r\n            assert torch.equal(orig_param, saved_model_param)\r\n```\r\n\r\nThe above where we use the Trainer as an engine still works, but now you'd need to pass the checkpoint path like so `trainer.predict(ckpt_path=..., ...)`",
      "author": {
        "login": "SeanNaren"
      },
      "createdAt": "2021-12-17T11:00:19Z"
    }
  },
  {
    "title": "AttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>",
    "body": "Here is DataReader\r\n```\r\nclass DataReader(torch.utils.data.Dataset):\r\n  def __init__(self, df):\r\n        super(DataReader,self).__init__()\r\n        self.df = df\r\n  def __len__(self):\r\n        'Denotes the total number of samples'\r\n        return len(self.df)\r\n\r\n  def __getitem__(self, index):\r\n        'Generates one sample of data'\r\n        # Select sample\r\n        file = self.df.iloc[index,0]\r\n        label=self.df.iloc[index,1]\r\n        return data,label\r\n```\r\nwhen i do \r\n```\r\nx=DataLoader(DataReader(train), batch_size = 2,collate_fn=my_collate)\r\nb=next(iter(x))\r\n```  \r\nit worked. but when I call it inside LightningModule, it throws an error\r\n\r\n```\r\nclass OurModel(LightningModule):\r\n  def __init__(self):\r\n    super(OurModel,self).__init__()\r\n    self.model =cnnmodel()\r\n  def forward(self,x):\r\n    x= self.model(x)\r\n    return x\r\n\r\n  def configure_optimizers(self):\r\n    return torch.optim.AdamW(params=self.parameters(),lr=self.lr )\r\n\r\n  def train_dataloader(self):\r\n    return DataLoader(DataReader(train))\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    return loss\r\n\r\n  def val_dataloader(self):\r\n    ds= DataLoader(DataReader(val))\r\n    print('ds',len(ds))\r\n    return\r\n    \r\n  def validation_step(self,batch,batch_idx):\r\n    print('val step')\r\n    return \r\n```\r\nI am unable to figure out what is the error and how to resolve it.I tried to debug it, val_dataloader function is working, it print `ds 7` but validation_step is not working, it should print val step, but its not printing it\r\n\r\n**EDIT**\r\nThis issue is because of `numworker`, setting numworker to 0, resolve the issue, but I am getting this warning \r\n` Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine)`\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 872, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\queues.py\", line 108, in get\r\n    raise Empty\r\n\r\nEmpty\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-9-5ebd04eb49d1>\", line 101, in <module>\r\n    trainer.fit(model)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 458, in fit\r\n    self._run(model)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 842, in run_train\r\n    self.run_sanity_check(self.lightning_module)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1107, in run_sanity_check\r\n    self.run_evaluation()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 949, in run_evaluation\r\n    for batch_idx, batch in enumerate(dataloader):\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1068, in _next_data\r\n    idx, data = self._get_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1034, in _get_data\r\n    success, data = self._try_get_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 885, in _try_get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\n\r\nRuntimeError: DataLoader worker (pid(s) 4984, 20904) exited unexpectedly\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8898",
    "createdAt": "2021-08-14T05:21:54Z",
    "updatedAt": "2022-06-07T21:13:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "do i need to write whole code under `if __name__ ==  \"__main__\"` or just\n`trainer.fit(model)`\n\nOn Mon, Aug 16, 2021 at 2:00 PM thomas chaton ***@***.***>\nwrote:\n\n> Did you try to add\n>\n> if *name* == \"*main*\"\n>\n> to your script ?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/8898#discussioncomment-1188890>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5FYO3X4RDTD7LMWYTWCBTT5DHTXANCNFSM5CEXIGTA>\n> .\n>\n",
      "author": {
        "login": "talhaanwarch"
      },
      "createdAt": "2021-08-17T04:18:14Z"
    }
  },
  {
    "title": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment",
    "body": "how can i solve this problem, my net is DB,  data is coco text det\r\n\r\nthis is error log\r\n\r\n```\r\nFile \"/home/cattree/PycharmProjects/torch-ocr/BoatNumber/ocr_det/train/train.py\", line 38, in <module>\r\n    trainer.fit(model, data)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 869, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 535, in run_training_epoch\r\n    monitor_metrics = deepcopy(self.trainer.logger_connector.callback_metrics)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 153, in deepcopy\r\n    y = copier(memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/torch/tensor.py\", line 55, in __deepcopy__\r\n    raise RuntimeError(\"Only Tensors created explicitly by the user \"\r\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8897",
    "createdAt": "2021-08-14T00:58:27Z",
    "updatedAt": "2022-06-01T03:54:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "What Lightning version are you using?\r\n\r\nThe fastest way for us to be able to help you is if you can adapt [this script](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py) to reproduce the behaviour you are seeing.\r\n\r\nJudging by the stacktrace, it's an error on our side.\r\n\r\nYou can try fixing it by calling `.detach()` on your logged tensors, but it'd be better if you can reproduce it and share the code\r\n\r\n",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-08-14T12:49:00Z"
    }
  },
  {
    "title": "Saving and loading HF transformer model fine tuned with PL?",
    "body": "I am fine-tuning hugging face transformer models, essentially exactly as shown in the following example found in the pytorch lightning docs:\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html\r\n\r\nWhere we instantiate the LightningModule doing something like this:\r\n\r\n```python\r\nclass GLUETransformer(LightningModule):\r\n\r\n    def __init__(self, ... ):\r\n        super().__init__()\r\n        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\r\n        self.model = AutoModelForSequenceClassification.from_pretrained(\r\n            model_name_or_path, config=self.config\r\n        )\r\n```\r\n\r\nBut I have been confused about how I should be saving and loading checkpoints. \r\n\r\nWhen saving checkpoints, should I be using \r\n`mymodel.model.save_pretrained(\"model_save_dir\")`, \r\nand reloading from this checkpoint using \r\n`AutoModelForSequenceClassification.from_pretrained(\"model_save_dir\")`,\r\n\r\nor saving with \r\n`trainer.save_checkpoint(\"model_save_dir/checkpoint.ckpt\")`,\r\nand reloading with \r\n`GLUETransformer.load_from_checkpoint(\"model_save_dir/checkpoint.ckpt\")`?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8893",
    "createdAt": "2021-08-13T22:03:05Z",
    "updatedAt": "2025-09-10T11:51:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "brijow"
    },
    "answer": {
      "body": "Dear @brijow,\r\n\r\nYou should be using the second approach. An even better one would be to rely on `ModelCheckpoint` to save the checkpoints and provide `Trainer(resume_from_checkpoint=...)` for reloading all the states.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-16T07:26:02Z"
    }
  },
  {
    "title": "Create 2 learning rate schedulers for 1 optimizer",
    "body": "In my custom `pl.LightningModule`, I try to specify two learning rate schedulers (`warming up` in the first epoch, and `multi-step` scheduler in the following epochs) for 1 `optimizer` as follows:\r\n\r\n```\r\n    def configure_optimizers(self):\r\n        args = self.args\r\n\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\r\n\r\n        warmup_iters = min(1000, self.num_batches - 1)\r\n        warmup_scheduler = {\r\n                    'scheduler': utils.warmup_lr_scheduler(optimizer, warmup_iters, 1./1000),\r\n                    'name': 'learning_rate',\r\n                    'interval':'step',\r\n                    'frequency': 1\r\n                }\r\n\r\n        lr_scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma),\r\n            'name': 'learning_rate',\r\n            'interval':'epoch',\r\n            'frequency': 1\r\n        }\r\n        return [optimizer], [warmup_scheduler, lr_scheduler]\r\n```\r\nHowever, it seems like the learning rate in the `optimizer` only controlled by the first `warmup_scheduler`. \r\n\r\nSo how can I create 2 learning rate schedulers for 1 optimizer?\r\nMany thanks.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8882",
    "createdAt": "2021-08-13T09:37:34Z",
    "updatedAt": "2022-06-16T06:25:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "davidnvq"
    },
    "answer": {
      "body": "The answer of my question would be involved and solve the question in https://github.com/PyTorchLightning/pytorch-lightning/discussions/6234.\r\nThus, I am still looking for the answer :)",
      "author": {
        "login": "davidnvq"
      },
      "createdAt": "2021-08-13T09:40:22Z"
    }
  },
  {
    "title": "Changing the computed gradients before backprop",
    "body": "I want to add noise to the gradients in pytorch lightning . Specifically, something similar to this paper: https://arxiv.org/pdf/1511.06807.pdf . Basically, I would compute the gradients and before the call to backward, i want to add noise.\r\n\r\nWhat is the best way to achieve this in pytorch lightning?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8866",
    "createdAt": "2021-08-12T05:07:23Z",
    "updatedAt": "2022-06-14T21:56:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sebastiangonsal"
    },
    "answer": {
      "body": "Dear @sebastiangonsal,\r\n\r\nFrom your paper, it seems you might want to call backward which computes the gradients and then add some noise right ?\r\n\r\nTherefore, you could override the `before_optimizer_step` and add noise to all the params.grad.\r\n\r\n```\r\nclass TestModel(LightningModule):\r\n\r\n    def on_before_optimizer_step(self):\r\n        for param in self.parameters():\r\n            param.grad += torch.rand_like(param.grad)\r\n```\r\n\r\nIf you want this re-usable, you could just move this to a callback instead.",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-19T10:14:14Z"
    }
  },
  {
    "title": "Train on cpu but gives error \"You have asked for native AMP on CPU, but AMP is only available on GPU\"",
    "body": "hi  dear friends,\r\nI wanted to train on cpu for debug purpose which can help me understand more about codes. \r\nso in my trainer class, i didn't pass gpus parameter,  the parameter looks like below:\r\n\r\n args of trainer  Namespace(accelerator=None, accumulate_grad_batches=1, adam_epsilon=1e-08, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=10, benchmark=False, bert_config_dir='/Users/i052090/Downloads/segmentation/data/bertmany/bert-base-uncased', bert_dropout=0.2, bert_max_length=128, best_dev_f1=0.0, check_val_every_n_epoch=1, checkpoint_callback=True, data_dir='data/conll03', dataname='conll03', default_root_dir='./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488', deterministic=False, distributed_backend=None, fast_dev_run=False, final_div_factor=10000.0, flush_logs_every_n_steps=100, fp_epoch_result='./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488/epoch_results.txt', gpus=None, gradient_clip_algorithm='norm', gradient_clip_val=1.0, label2idx_list=[('O', 0), ('ORG', 1), ('PER', 2), ('LOC', 3), ('MISC', 4)], limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=1e-05, max_epochs=1, max_spanLen=4, max_steps=None, max_time=None, min_epochs=None, min_steps=None, modelName='spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5', model_dropout=0.2, morph2idx_list=[('isupper', 1), ('islower', 2), ('istitle', 3), ('isdigit', 4), ('other', 5)], morph_emb_dim=100, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', n_class=5, neg_span_weight=0.5, num_nodes=1, num_processes=1, num_sanity_val_steps=2, optimizer='adamw', overfit_batches=0.0, param_name='epoch1_batchsize10_lr1e-5_maxlen128', plugins=None, precision=16, prepare_data_per_node=True, pretrained_checkpoint='', process_position=0, profiler=None, progress_bar_refresh_rate=1, random_int='38274488', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, spanLen_emb_dim=100, span_combination_mode='x,y', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tokenLen_emb_dim=50, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, use_morph=True, use_prune=True, use_spanLen=True, use_span_weight=True, use_tokenLen=False, val_check_interval=0.25, warmup_steps=0, weight_decay=0.01, weights_save_path=None, weights_summary='top', workers=0)\r\n\r\nBut it would give errors:\r\n File \"trainer.py\", line 572, in main\r\n    trainer = Trainer.from_argparse_args(\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 207, in from_argparse_args\r\n    return from_argparse_args(cls, args, **kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py\", line 52, in from_argparse_args\r\n    return cls(**trainer_kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 40, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 319, in __init__\r\n    self.accelerator_connector = AcceleratorConnector(\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 136, in __init__\r\n    self.accelerator = self.select_accelerator()\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 483, in select_accelerator\r\n    precision_plugin=self.precision_plugin,\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 220, in precision_plugin\r\n    self._precision_plugin = self.select_precision_plugin()\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 350, in select_precision_plugin\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: You have asked for native AMP on CPU, but AMP is only available on GPU.\r\n\r\n\r\n\r\n\r\nthanks,",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8855",
    "createdAt": "2021-08-11T13:41:58Z",
    "updatedAt": "2022-07-12T16:56:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "patricwan"
    },
    "answer": {
      "body": "Setting `Trainer(precision=16)` is only supported on GPU!\r\n\r\nIf you have them available, you can do: `Trainer(gpus=N, precision=16)`",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-08-11T14:33:24Z"
    }
  },
  {
    "title": "How to ensure objects saved as model attributes are saved in the checkpoint file?",
    "body": "Say I have a lightning model `model = MyLightningModel()` that contains an object that gets created and updated throughout training `model.my_object`. Upon loading the model from the checkpoint `MyLightningModel.load_from_checkpoint(ckpt_path)` I'm noticing the `model.my_object` attribute is not being saved and restored upon loading from checkpoint.\r\n\r\nIs it possible to somehow ensure that these model attributes get saved in the checkpoint file and properly restored when loading the model from checkpoint? \r\n\r\nThanks in advance for your help! ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8841",
    "createdAt": "2021-08-10T14:41:17Z",
    "updatedAt": "2022-10-24T21:09:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "KirillShmilovich"
    },
    "answer": {
      "body": "Dear @KirillShmilovich.\r\n\r\nYou could use the LightningModule `on_save_checkpoint` and `on_load_checkpoint` hooks.\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object\": self.my_object}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = state_dict[\"my_object\"]\r\n```\r\n\r\nHowever, pickling objets isn't always the best approach. A slightly better approach is\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object_state_dict\": self.my_object.state_dict()}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = my_object_cls.from_state_dict(state_dict[\"my_object_state_dict\"])\r\n```",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-19T09:32:54Z"
    }
  },
  {
    "title": "What's the difference between `on_step` and `on_epoch` of `pl_module.log`",
    "body": "> https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=on_epoch%20#pytorch_lightning.core.lightning.LightningModule.log.params.on_epoch\r\n\r\nI'm using horovod to train the model. I wonder if `on_step` and `on_epoch` average the metrics across all GPUs automatically. In other words, do we need to explicitly average the metrics in functions like `training_epoch_end` and `validation_epoch_end`?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8806",
    "createdAt": "2021-08-09T08:03:36Z",
    "updatedAt": "2023-02-23T14:00:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "marsggbo"
    },
    "answer": {
      "body": "Dear @marsggbo,\r\n\r\nWhen using self.log(..., on_step=True), this will compute the metric per step locally as synchronisation adds performance hit. \r\nWhen using self.log(..., on_step=True, sync_dist=True), this will compute the metric per step across GPUS.\r\nWhen using self.log(..., on_epoch=True), this will compute the metrics across GPUS and epoch batches automatically.\r\n  \r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-09T13:17:06Z"
    }
  },
  {
    "title": "Trainer test with ckpt_path not work as expected",
    "body": "I wander If I misunderstanding doc. Trainer test with ckpt_path doesn't load ckpt. model.load_from_checkpoint works.\r\n```\r\nIn [20]: model = MMTSMatcher(model_name=\"bert-base-uncased\")\r\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n\r\nIn [21]: trainer.test(model, data, ckpt_path=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\")\r\nThe following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\r\n  rank_zero_deprecation(\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.04s/it]--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_f1': 0.0,\r\n 'test_loss': 0.6186538934707642,\r\n 'test_prc': 0.0,\r\n 'test_rec': 0.0}\r\n--------------------------------------------------------------------------------\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.27s/it]\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\r\n  rank_zero_deprecation(\r\nOut[21]:\r\n[{'test_f1': 0.0,\r\n  'test_prc': 0.0,\r\n  'test_rec': 0.0,\r\n  'test_loss': 0.6186538934707642}]\r\n\r\nIn [22]: model = MMTSMatcher.load_from_checkpoint(checkpoint_path=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\", hparams_file=\"tests/mmtsmatcher_wdcd\r\n    ...: atamodule/shoes_small_false_32_0808-182020/hparams.yaml\")\r\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n\r\nIn [23]: trainer.test(model, data)\r\nThe following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping, ModelCheckpoint\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\r\n  rank_zero_deprecation(\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.21it/s]--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_f1': 0.670258641242981,\r\n 'test_loss': 0.284279465675354,\r\n 'test_prc': 0.5298126339912415,\r\n 'test_rec': 0.9120234847068787}\r\n--------------------------------------------------------------------------------\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.27s/it]\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\r\n  rank_zero_deprecation(\r\nOut[23]:\r\n[{'test_f1': 0.670258641242981,\r\n  'test_prc': 0.5298126339912415,\r\n  'test_rec': 0.9120234847068787,\r\n  'test_loss': 0.284279465675354}]\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8797",
    "createdAt": "2021-08-08T12:32:14Z",
    "updatedAt": "2022-09-01T07:54:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tshu-w"
    },
    "answer": {
      "body": "Dear @tshu-w,\r\n\r\nMind opening an issue with a reproducible script using the BoringModel.\r\n\r\nBest,\r\nT.C ",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-19T10:21:59Z"
    }
  },
  {
    "title": "Why Trainer resume_from_checkpoint only resume when fit.",
    "body": "I want to test again with ckpt. However I found Trainer resume_from_checkpoint only work for fit. Is there any particular reason for this?\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/f3442db3f0155a32c8bc0ca3bc943d4b0039897f/pytorch_lightning/trainer/trainer.py#L840-L841\r\n```\r\nIn [1]: from pytorch_lightning import seed_everything, Trainer\r\n\r\nIn [2]: from src import MMTSMatcher, WDCDataModule\r\n\r\nIn [3]: trainer = Trainer(resume_from_checkpoint=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\", gpus=[4])\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n\r\nIn [4]: trainer\r\nOut[4]: <pytorch_lightning.trainer.trainer.Trainer at 0x7f2345178fa0>\r\n\r\nIn [5]: model = MMTSMatcher(model_name=\"bert-base-uncased\")\r\n\r\nIn [6]: data = WDCDataModule(training_size=\"small\", cate=\"shoes\")\r\n\r\nIn [7]: data\r\nOut[7]: <src.datamodules.wdcdatamodule.WDCDataModule at 0x7f22ccd258e0>\r\n\r\nIn [10]: trainer.test(model, data)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8796",
    "createdAt": "2021-08-08T12:05:25Z",
    "updatedAt": "2022-07-18T04:19:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tshu-w"
    },
    "answer": {
      "body": "Dear @tshu-w,\r\n\r\nThe resume_start will reload all states which might not be required for testing. We are currently refactoring this part of the code.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-19T10:23:13Z"
    }
  },
  {
    "title": "How to handle pretrained models without training them",
    "body": "Hey gang!\r\n\r\nI have written an Encoder model and a decoder model and I want to train them **separately**. \r\n\r\n```\r\nclass Decoder(pl.LightningModule):\r\n    def __init__(self, encoder_model):#visualize_latent):\r\n        super().__init__()\r\n        self.encoder_model = encoder_model\r\n```\r\n\r\nHowever, when I give my Decoder an Encoder hyperparameter, how do I make sure it will not be trained?\r\nI actually asked that question before, but it wasn't answered for a while and I do not find sufficient support in the docs.\r\n\r\nThanks in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8778",
    "createdAt": "2021-08-06T15:49:09Z",
    "updatedAt": "2022-07-21T00:06:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "staniPetrox"
    },
    "answer": {
      "body": "Ok, I found out from other forums that one should use `.freeze()`, in this case: `self.encoder_model.freeze()`",
      "author": {
        "login": "staniPetrox"
      },
      "createdAt": "2021-08-06T19:48:50Z"
    }
  },
  {
    "title": "training_epoch_end only returning the last train batch",
    "body": "I have a pytorch lightning module that includes the following section:\r\n\r\n`def training_step(self, batch, batch_idx):\r\n        losses, tensors = self.shared_step(batch)\r\n        return losses\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        losses, tensors = self.shared_step(batch)\r\n        return losses, tensors\r\n\r\n    def training_epoch_end(self, losses):\r\n        my_function(losses)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        my_function2(outputs)`\r\n\r\nThe `losses` variable is a dictionary that includes the key \"loss\". The input to `validation_epoch_end()` is a list of `(losses, tensors)` tuples from each of the batches as expected. The input to `training_epoch_end()` is a list of the correct length (the number of batches), but every element is the same losses dictionary from the final batch. \r\n\r\nHow can I input the losses from _every_ train batch into the `training_epoch_end()` method (like the inputs to `validation_epoch_end()`)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8757",
    "createdAt": "2021-08-05T19:14:36Z",
    "updatedAt": "2022-08-12T14:53:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "armoses"
    },
    "answer": {
      "body": "This issue was reported in https://github.com/PyTorchLightning/pytorch-lightning/issues/8603 - are you able to try Lightning 1.4.1, which contains the fix? \r\n\r\nAnd for broader discussion on these hooks, and alternatives you have to access the per-step outputs, see https://github.com/PyTorchLightning/pytorch-lightning/issues/8731",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-08-05T19:21:32Z"
    }
  },
  {
    "title": "on_post_move_to_device is leaky",
    "body": "The decorator here is leaky: https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/decorators.py#L73-L108\r\n\r\nIt assumes it is called with the `model_to_device` method, and that `self` has access to `model` which implements `on_post_move_to_device` and has `parameters` defined.  \r\n\r\nThe hook here: https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/hooks.py#L341-L355\r\n\r\nis only called by the TPU backend. Is it intended to be called by other plugins? \r\n\r\nSince this is under the `core/decorators.py`, one might assume that this is more general. Should this be an implementation detail of the TPU plugins instead? @kaushikb11 ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8736",
    "createdAt": "2021-08-05T04:59:26Z",
    "updatedAt": "2022-07-06T00:42:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ananthsub"
    },
    "answer": {
      "body": "This hook is intended only for TPU as TPU don't support tying parameters on cpu.\r\n\r\nHowever, I believe we could actually depreciate it once https://github.com/PyTorchLightning/pytorch-lightning/issues/8555 is implemented.\r\n\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-20T16:33:08Z"
    }
  },
  {
    "title": "Why is the default implementation for train_dataloader in DataHooks a warning?",
    "body": "The default implementation for these is logging a warning that nothing is implemented. https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/hooks.py#L529\r\n\r\nWhy isn\u2019t the default implementation to raise a NotImplementedError? This would make errors much clearer in case users forget to override these hooks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8734",
    "createdAt": "2021-08-05T04:22:33Z",
    "updatedAt": "2021-08-05T17:15:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ananthsub"
    },
    "answer": {
      "body": "I believe this is just legacy code. No real reason. It's in the original implementation (to bolts!)\r\n\r\nhttps://github.com/PyTorchLightning/lightning-bolts/commit/797464c421652c4ceb4e8527b9f56ab059382200\r\n\r\nFeel free to try changing it :)",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-08-05T17:08:36Z"
    }
  },
  {
    "title": "Using callbacks for datamodule setup preprocessing logic?",
    "body": "I have some preprocessing logic to run during datamodule setup process, but only in certain situations (mostly to try out different preprocessing steps while experimenting, but at other times, its due to the model I am using with the datamodule).\r\n\r\nIs there a way to specify a set of data preprocessing steps to perform using callbacks? Reading the documentation, I could not find the correct hook to use.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8650",
    "createdAt": "2021-07-30T15:53:05Z",
    "updatedAt": "2022-06-14T02:46:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "brijow"
    },
    "answer": {
      "body": "Dear @brijow,\r\n\r\nI wondered if something like that could fit your use-case ?\r\n\r\n```py\r\nclass MyDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n\r\n        self._processed_train_dataset = None\r\n\r\n    def setup(self):\r\n        self.train_dataset = ...\r\n\r\n    @property\r\n    def processed_train_dataset(self):\r\n        return self._processed_train_dataset or self.train_dataset\r\n\r\n    @processed_train_dataset.setter\r\n    def processed_train_dataset(self, processed_train_dataset):\r\n        self._processed_train_dataset = processed_train_dataset\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.processed_train_dataset)\r\n\r\n\r\nclass Preprocessing1(Callback):\r\n\r\n    def preprocess_function(self, dataset):\r\n        # your preprocessing logic\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n       #\u00a0apply processing\r\n        trainer.datamodule.processed_train_dataset = self.preprocess_function(trainer.datamodule.train_dataset)\r\n        \r\n        # force dataloader reload\r\n        # trainer.reset_train_dataloader(pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=Preprocessing1())\r\ntrainer.fit(model, dm)\r\n```\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-03T10:07:25Z"
    }
  },
  {
    "title": "Optimizers for nested modules",
    "body": "```python\r\nclass MyNet(pl.LightningModule):\r\n    def __init__(self):\r\n        self.m1 = MyMod1()\r\n        self.m2 = MyMod2()\r\n```\r\nIf I implement different `configure_optimizers` for different submodules `MyMod`(also `pl.LightningModule`), is it correct that parameters in each `MyMod` will be updated by their own optimizers returned by `configure_optimizers`? If I only implement `configure_optimizers` for the top module, is it correct that parameters in submodules will be optimized by the same optimizer returned by `configure_optimizers` of the top module?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8637",
    "createdAt": "2021-07-30T03:21:38Z",
    "updatedAt": "2022-07-20T14:49:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Co1lin"
    },
    "answer": {
      "body": "When you have nested LightningModules, their `configure_optimizers` will never be called unless you explicitly call it in the top-level `configure_optimizers`.\r\n\r\nThat being said, if you call, merge and return the optimizers created there, these optimizers should only contain parameters from the respective submodule (if implemented correctly)",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-07-30T06:32:42Z"
    }
  },
  {
    "title": "Emulating multiple devices with a single GPU",
    "body": "Hello,\r\n\r\nI have a single GPU, but I would like to spawn multiple replicas on that single GPU and train a model with DDP. Of course, each replica would have to use a smaller batch size in order to fit in memory. (For my use case, I am not interested in having a single replica with a large batch size).\r\n\r\nI tried to pass `--gpus \"0,0\"` to the Lightning Trainer, and it managed to spawn two processes on the same GPU:\r\n```\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\r\n```\r\nBut in the end it crashed with `RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:911, invalid usage`.\r\n\r\nPlease, is there any way to split a single GPU into multiple replicas with Lightning?\r\nThanks!\r\n\r\nP.S.: Ray has a really nice support for fractional GPUs: https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus. I've never used them with Lightning, but maybe it could be a workaround?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8630",
    "createdAt": "2021-07-29T21:08:27Z",
    "updatedAt": "2025-04-17T06:15:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tholop"
    },
    "answer": {
      "body": "For reference: it seems to be possible when the backend is `gloo` instead of `nccl`. See discussion here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8630#discussioncomment-1127286.",
      "author": {
        "login": "tholop"
      },
      "createdAt": "2021-08-03T20:01:02Z"
    }
  },
  {
    "title": "What is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?",
    "body": "Hi everyone, I wonder what is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?\r\n\r\nI think [torch.Storage.from_file](https://pytorch.org/docs/stable/storage.html?highlight=from_file#torch.DoubleStorage.from_file) with `share=True` may suit my needs, but I can\u2019t find a way to save storage and read it as a tensor. (see [here](https://discuss.pytorch.org/t/how-to-save-a-storage-file-that-can-be-loaded-using-torch-storage-from-file/127737) for details)\r\n\r\nI also tried to copy training data to /dev/shm ([reference](https://pytorch-lightning.readthedocs.io/en/stable/benchmarking/performance.html#preload-data-into-ram)) and run DDP with 8 GPUs, but nothing is different. The memory usage when running with 8 GPUs is the same as before, but I tested with a single process, loading the dataset may occupy more than 1 GB of memory. Am I missing something here?\r\n\r\nFor `torch.shared_memory`, how should I pass the same reference to all processes in pytorch-lightning pure DDP mode?\r\n\r\nThank you.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8611",
    "createdAt": "2021-07-29T02:56:38Z",
    "updatedAt": "2022-09-01T06:12:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "siahuat0727"
    },
    "answer": {
      "body": "I found that `torch.Storage.from_file` suits my needs and it can reduce the memory usage in my Lightning DDP program.\r\nFor the way to create a storage file, see [here](https://discuss.pytorch.org/t/how-to-save-a-storage-file-that-can-be-loaded-using-torch-storage-from-file/127737).",
      "author": {
        "login": "siahuat0727"
      },
      "createdAt": "2021-08-04T08:51:10Z"
    }
  },
  {
    "title": "How are callback calls handled in multi-gpu mode?",
    "body": "Say I have a callback that changes a hyper-parameter of the underlying model before every epoch.\r\n\r\n```python\r\nclass ChangeHyperParam(pl.Callback):\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n         pl_module.hyper_param1 = func(trainer.current_epoch)\r\n```\r\n\r\nWhat happens when I use this callback in DDP mode. Will this call be called on every GPU automatically? Or do I have to do something else?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8607",
    "createdAt": "2021-07-28T22:55:59Z",
    "updatedAt": "2022-07-17T16:06:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ktrapeznikov"
    },
    "answer": {
      "body": "Callbacks run on all ranks, so this would be called on each GPU in distributed training. As long as `func` returns the same value given the inputs, this should be fine",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-07-29T00:10:59Z"
    }
  },
  {
    "title": "Is it possible to access the global rank and world size outside of `LightningModule`?",
    "body": "Hi,\r\n\r\nI'm using an `IterableDataset` and need to manually split up the data depending on the GPU (I'm using DDP). The lightning docs say that `replace_sampler_ddp` doesn't automatically handle `IterableDatasets`, so I need to do this myself.\r\n\r\nIs it possible to access the global rank or world size in a dataset (aka are there any utility functions available for this purpose)? I saw `_get_rank` [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/24db914093978a0b3fbd61d2d2ca331f0d6a4628/pytorch_lightning/utilities/distributed.py#L55), but ideally I wouldn't use a private helper function.\r\n\r\nI tried using:\r\n```\r\nfile_paths_filtered = file_paths[\r\n    torch.distributed.get_rank() :: torch.distributed.get_world_size()\r\n]\r\n```\r\n\r\ninside the IterableDataset, but it fails because the default process group has not been setup yet (which makes sense):\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\n\r\nIs there any way to access the global rank/world size? If not, how would you recommend I check which GPU the dataset is on without causing an issue with `init_process_group` not being called yet (should I maybe call this manually)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8590",
    "createdAt": "2021-07-28T00:29:38Z",
    "updatedAt": "2022-07-07T00:03:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EricWiener"
    },
    "answer": {
      "body": "Issue ended up being `torch.distributed.get_rank()` and `torch.distributed.get_world_size()` aren't available when on a single GPU, which I was testing on locally before sending to multi-GPU server.\r\n\r\nThe following works:\r\n```\r\nif torch.distributed.is_available() and torch.distributed.is_initialized():\r\n    file_paths = file_paths[\r\n        torch.distributed.get_rank() :: torch.distributed.get_world_size()\r\n    ]\r\n```",
      "author": {
        "login": "EricWiener"
      },
      "createdAt": "2021-07-28T18:25:13Z"
    }
  },
  {
    "title": "How to reinit wanbd in a for loop with PL Trainer",
    "body": "I am training 5-fold CV with PyTorch Lightning in a for loop. I am also logging all the results to wandb. I want wanbd to reinitalize the run after each fold, but it seems to continue with the same run and it logs all the results to the same run. I also tried passing kwargs in the WandbLogger as mentioned in the docs [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger.params.**kwargs), with no luck.\r\nHere's a pseudo code of it:\r\n\r\n```python\r\ndef run(fold):\r\n    kwargs = {\r\n        \"reinit\": True,\r\n        \"group\": f\"{CFG['exp_name']}\"\r\n    }\r\n    wandb_logger = WandbLogger(project='<name>', \r\n                        entity='<entity>', \r\n                        config = CFG,\r\n                        name=f\"fold_{fold}\",\r\n                        **kwargs\r\n            )\r\n    trainer = Trainer(\r\n        precision=16,\r\n        gpus=1,\r\n        fast_dev_run=False,\r\n        callbacks = [checkpoint_callback],\r\n        logger=wandb_logger,\r\n        progress_bar_refresh_rate=1,\r\n        max_epochs=2,\r\n        log_every_n_steps=1\r\n\r\n    )\r\n\r\n    trainer.fit(\r\n        lit_model,\r\n        data_module\r\n    )\r\n\r\nif __name__ == \"__main__\":\r\n    for fold in range(5):\r\n        run(fold)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8572",
    "createdAt": "2021-07-27T06:33:50Z",
    "updatedAt": "2022-06-19T12:45:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Gladiator07"
    },
    "answer": {
      "body": "@Gladiator07 you could try call `wandb.finish()` at the end of every run. This should close the wandb process. A new one will be started when you call the next run",
      "author": {
        "login": "morganmcg1"
      },
      "createdAt": "2021-11-11T17:09:11Z"
    }
  },
  {
    "title": "Multiple models, one dataloader?",
    "body": "I have a training regime that is disk-speed bound, because instances are loaded from disk.\r\n\r\nI would like to train multiple models with one dataloader. That way, I can do model selection over many models, but reduce the number of disk reads. Is this possible?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8565",
    "createdAt": "2021-07-26T22:56:04Z",
    "updatedAt": "2024-08-28T15:20:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "turian"
    },
    "answer": {
      "body": "Dear @turian,\r\n\r\nYes, it is possible.\r\n\r\nYou could do something like this.\r\n\r\n```\r\nclass MultiModels(LightningModule):\r\n\r\n    def __init__(self, models: List[nn.Module]):\r\n        self.models = models\r\n\r\n    def compute_loss(self, model, batch):\r\n        loss = ...\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = sum(compute_loss(model, batch) for model in self.models)\r\n        return loss\r\n\r\n\r\nmodel = MultiModels([resnet50_model, alexnet_model, ...])\r\ndm = ...\r\ntrainer.fit(model, dm)\r\n```\r\n\r\nDoes this answer your questions ?\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-29T09:42:02Z"
    }
  },
  {
    "title": "How to instantiate plugins with LightningCLI?",
    "body": "I'm looking at the following example: \r\n\r\n```py\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n\r\ndef run():\r\n    LightningCLI(model_class=BoringModel)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nwith config file: \r\n\r\n```\r\ntrainer:\r\n    max_steps: 1\r\n    limit_val_batches: 1\r\n    accelerator: ddp_cpu\r\n    plugins:\r\n      - class_path: \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\"\r\n        init_args:\r\n          find_unused_parameters: false\r\n\r\n```\r\n\r\nIn Lightning 1.4 I am getting the following error.\r\n\r\n```python\r\nbug_report_model.py: error: Parser key \"trainer.plugins\": Value \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\" does not validate against any of the types in typing.Union[typing.List[typing.Union[pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str]], pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str, NoneType]:\r\n  - Value \"{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}\" does not validate against any of the types in typing.Union[pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str]:\r\n    - Problem with given class_path \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\":\r\n      - 'Configuration check failed :: No action for key \"find_unused_parameters\" to check its value.'\r\n    - \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\" is not a subclass of ClusterEnvironment\r\n    - Expected a <class 'str'> but got \"{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}\"\r\n  - Type <class 'pytorch_lightning.plugins.base_plugin.Plugin'> expects an str or a Dict with a class_path entry but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Type <class 'pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment'> expects an str or a Dict with a class_path entry but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Expected a <class 'str'> but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Expected a <class 'NoneType'> but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n```\r\n\r\nThe relevant error is:\r\n\r\n```python\r\n    - Problem with given class_path \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\":\r\n      - 'Configuration check failed :: No action for key \"find_unused_parameters\" to check its value.'\r\n```\r\n\r\nThe [DDPPlugin signature has kwargs defined](https://github.com/PyTorchLightning/pytorch-lightning/blob/75e18a52988f760a8b32a3c4fc6f0f3217868e57/pytorch_lightning/plugins/training_type/ddp.py#L74) and I am not sure if jsonargparse is supposed to be able to handle them, I couldn't find any explanation in their [docs](https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes). \r\n\r\nI would like to pass in `find_unused_parameters=False` as part of these kwargs. \r\n\r\nCurrent workaround: \r\n\r\n```\r\ntrainer:\r\n    ...\r\n    plugins: ddp_find_unused_parameters_false \r\n\r\n```\r\n\r\ncc @mauvilsa @carmocca who might be pleased to hear that I started to really like the CLI <3  \r\n\r\nEDIT\r\n- A similar issue exists here #8262, but it is not the same because what OP posted works with the latest jsonargparse version and is not my exact use case.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8561",
    "createdAt": "2021-07-26T15:03:06Z",
    "updatedAt": "2022-06-22T07:31:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "awaelchli"
    },
    "answer": {
      "body": "The issues related to plugins instantiation have been resolved with #13283.",
      "author": {
        "login": "mauvilsa"
      },
      "createdAt": "2022-06-22T04:57:21Z"
    }
  },
  {
    "title": "RuntimeError: Trying to backward through the graph a second time",
    "body": "I'm migrating my repository to pytorch-lightning and I get the following error: \r\n\r\n```bash\r\nRuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\r\n```\r\n\r\nThe CNNLSTM model seems to be the problem, what should I do?\r\n\r\n[My repository]\r\nKeiku/Action-Recognition-CNN-LSTM: Action recognition tutorial using UCF-101 dataset. https://github.com/Keiku/Action-Recognition-CNN-LSTM",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8549",
    "createdAt": "2021-07-26T02:38:09Z",
    "updatedAt": "2022-06-09T10:30:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "Hi @Keiku This error happens if you try to call backward on something twice in a row without calling optimizer.step. Are you able to share your LightningModule code? It looks like the code in your repo just uses vanilla pytorch.\r\n\r\nThanks :smiley:",
      "author": {
        "login": "ethanwharris"
      },
      "createdAt": "2021-07-26T08:47:19Z"
    }
  },
  {
    "title": "How to sync buffers in multi-gpu training",
    "body": "I know that parameters are indirectly synced in multi-gpu via grad-syncing.  But how to sync buffers that are not updated via gradient? \r\n\r\nI find that I can use all_reduce() or all_gather() method manually in ddp doc, but what pytorch-lightning does under the hood? Does it sync buffers automatically or offer an interface to do that? \r\n\r\nConsidering pl handles single and multi-gpu without any need to change Modules, do it manually will result in ugly judgement code whether the training is in ddp mode inside an independent module.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8545",
    "createdAt": "2021-07-26T00:39:50Z",
    "updatedAt": "2022-06-03T06:15:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MeteorsHub"
    },
    "answer": {
      "body": "Dear @MeteorsHub,\r\n\r\nGreat question. Lightning doesn't do any magic there. It relies on PyTorch DistributedDataParallel to handle multi-gpu via grad-syncing.\r\n\r\nThe buffers are being synced on start using `_sync_params_and_buffers`: https://github.com/pytorch/pytorch/blob/3e3acf8a9ac005db3094f23bd41a5fbc0c3c154b/torch/nn/parallel/distributed.py#L570\r\n\r\nIf you need to access this private function for any reason, you can do the following:\r\n\r\n```\r\n\r\ntrainer.training_type_plugin.model._sync_params_and_buffers(authoritative_rank=0)\r\n```\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-26T09:15:34Z"
    }
  },
  {
    "title": "RuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode",
    "body": "I'm getting the following error after setting up an EC2 instance p3.8xlarge (so 4 GPUs) and setting gpus=4:\r\n\r\n```python\r\n/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:524: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  'You requested multiple GPUs but did not specify a backend, e.g.'\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 79, in <module>\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 197, in lr_find\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 688, in tune\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 54, in _tune\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/lr_finder.py\", line 250, in lr_find\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 64, in _run\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 122, in start_training\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/context.py\", line 284, in _Popen\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/reduction.py\", line 60, in dump\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 328, in reduce_storage\r\nRuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode\r\n```\r\n\r\nMy code runs fine on a single gpu instance. Any idea what I need to look at here?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8524",
    "createdAt": "2021-07-22T16:18:09Z",
    "updatedAt": "2022-06-15T13:32:50Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EvanZ"
    },
    "answer": {
      "body": "Some quick googling \ud83d\udd0d \r\nhttps://github.com/facebookresearch/maskrcnn-benchmark/issues/103\r\n\r\nThis issue is not Lightning related, so if the fixes mentioned there do not help, then you should try asking on PyTorch discussions.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-23T02:04:07Z"
    }
  },
  {
    "title": "`init_process_group` not called when training on multiple-GPUs",
    "body": "Hi,\r\n\r\nI\u2019m trying to train a model on 2 GPUs. I do this by specifying Trainer(..., gpus=2). ddp_spawn should automatically be selected for the method, but I instead get the following message + error:\r\n\r\n```\r\nUserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(\r\naccelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  'You requested multiple GPUs but did not specify a backend, e.g.'\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nTraceback (most recent call last):\r\n File \"train.py\", line 186, in <module>\r\n    main(sys.argv[1:])\r\n  File \"train.py\", line 173, in main\r\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\r\n    dirpath = self.accelerator.broadcast(dirpath)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/accelerators/accelerator.py\", line 436, in broadcast\r\n    return self.training_type_plugin.broadcast(obj, src)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 275, in broadcast\r\n    return self.dist.broadcast(obj)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/distributed/dist.py\", line 33, in broadcast\r\n    broadcast_object_list(obj, 0, group=group or _group.WORLD)\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 1700, in broadcast_object_list\r\n    my_rank = get_rank()\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 725, in get_rank\r\n    default_pg = _get_default_group()\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 358, in _get_default_group\r\n    raise RuntimeError(\"Default process group has not been initialized, \"\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\n\r\nI looked at the [source code](https://github.com/PyTorchLightning/pytorch-lightning/blob/51ea84222ba07b45d01d94348f8e671e50fccc7e/pytorch_lightning/plugins/training_type/ddp_spawn.py#L259-L261) of ddp_spawn and it looks like it should print out a message when initializing ddp, but it didn\u2019t.\r\n\r\nCould I please have advice on how to correct this error.\r\n\r\nThank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8517",
    "createdAt": "2021-07-21T21:46:48Z",
    "updatedAt": "2022-06-13T07:17:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "EricWiener"
    },
    "answer": {
      "body": "The issue comes from the line\r\n\r\n```python\r\n  File \"train.py\", line 173, in main\r\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\r\n```\r\n\r\nwhich tries to access `trainer.log_dir` outside of the trainer scope.\r\n\r\n`trainer.log_dir` tries to `broadcast` the directory but fails as DDP hasn\u2019t been initialized yet.\r\n\r\n```python\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\r\n    dirpath = self.accelerator.broadcast(dirpath)\r\n```\r\n\r\nThis is fixed in the 1.4 release as `broadcast` becomes a no-op in that case",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-21T22:41:06Z"
    }
  },
  {
    "title": "How to implement a deep ensemble",
    "body": "I am looking to implement `n` parallel independent ensembles. My idea is the following:\r\n\r\n``` python\r\nclass DeepEnsemble(LightningModule):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n        self.net = nn.ModuleList([configure_network(self.cfg) for _ in range(self.cfg.METHOD.ENSEMBLE)])\r\n\r\n    def configure_optimizers(self):\r\n        return [torch.optim.Adam(net.parameters(), lr=self.cfg.SOLVER.LR) for net in self.net]\r\n\r\n    def forward(self, x):\r\n        x = [net.forward(x) for net in self.net]\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image)\r\n        loss = [self.criterion(logit, label) for logit in logits]\r\n\r\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\r\n\r\n        metrics = self.log_metrics(mean_logit, label, 'train')\r\n\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image)\r\n\r\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\r\n\r\n        metrics = self.log_metrics(mean_logit, label, 'val')\r\n\r\n        return metrics[self.cfg.CKPT.MONITOR]\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        pass\r\n```\r\n\r\nI have `n` networks and `n` optimisers. My solution works (I think), but the `training_step` gets called with a new `optimizer_idx` every time, which indicates that Pytorch Lightning expects to only train 1 network per training_step. Therefore, my solution is very inefficient, because n^2 forward passes are executed instead of n. If I only do the forward pass for the *i*th network, then I can't compute metrics based on all ensembles (e.g. disagreement) unless I write some very inelegant if statements.\r\n\r\nIn addition It would be nice to have all forward passes done in parallel instead of sequential like in this list comprehension.\r\n\r\nSo what is the most elegant way to train an ensemble and still access all predictions for metric logging together?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8505",
    "createdAt": "2021-07-21T11:21:04Z",
    "updatedAt": "2022-06-20T11:48:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "cemde"
    },
    "answer": {
      "body": "I see two potential options.\r\n\r\n1. cache the forward output for a specific batch idx. Check the automatic optimization flow: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\r\n\r\n2. Use manual optimization. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-22T00:36:28Z"
    }
  },
  {
    "title": "Data augmentation and reload_dataloaders_every_epoch",
    "body": "Hi! \r\n\r\nI'm training my neural network with Pytorch Lightning and [MONAI](https://monai.io/) (a PyTorch-based framework for deep learning in healthcare imaging). Because my training dataset is small, I need to perform data augmentation using random transforms. \r\n\r\n**Context** \r\n\r\nI use MONAI's `CacheDataset` (basically, a PyTorch `Dataset` with cache mechanism). [From what I understood](https://github.com/Project-MONAI/MONAI/discussions/2608), , `CacheDataset` will cache the consistent result of the transforms until the first random transform, then reuse the cache content and apply the remaining random transforms (such as Gaussian noise, random intensity shift, etc.) for every epoch. As a result, the dataloader trains the network with a different dataset at each epoch. \r\n\r\nIn the [video](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#reload-dataloaders-every-n-epochs) presenting the `reload_dataloaders_every_epoch` flag, William Falcon mentions that:\r\n\r\n> By default, Lighthning only loads your dataset once (so that you don't occur the cost of downloading that data and process it every single time). On every epoch, Lightning shuffles the data and feeds it into the training loop.\r\n\r\n**My questions** \r\n\r\nDid Lightning add a cache mechanism to load the data once? Must I use the `reload_dataloader_every_epoch` flag to do data augmentation or else my random transforms will only be applied once (therefore defeating my data augmentation goal)? \r\n\r\nThanks in advance for your explanation :)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8504",
    "createdAt": "2021-07-21T10:12:15Z",
    "updatedAt": "2022-06-21T14:53:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dianemarquette"
    },
    "answer": {
      "body": "> Did Lightning add a cache mechanism to load the data once?\r\n\r\nNo, the flag just means that we call `LightningModule.train_dataloader()` every epoch if enabled, thus creating a new `DataLoader` instance.\r\n\r\n>  Must I use the reload_dataloader_every_epoch flag to do data augmentation or else my random transforms will only be applied once \r\n\r\nIf I understand correctly, no.\r\nThe transformations are applied directly in the `Dataset`, so every time an item is consumed from it, the random transforms should be applied regardless of whether the `DataLoader` has or hasn't been recreated.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-22T00:44:06Z"
    }
  },
  {
    "title": "DDP with shared file system",
    "body": "Is it possible to use shared filesystem for DDP init_group in pytorch lighting? If so how what should I do to the Trainer?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8496",
    "createdAt": "2021-07-20T19:48:11Z",
    "updatedAt": "2023-04-11T18:32:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "YuShen1116"
    },
    "answer": {
      "body": "I'm not quite sure about what you want to do, but if it's about customizing `DDP`, you can do the following:\r\n\r\n```python\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\nclass MyCustomDDP(DDPPlugin):\r\n    ...\r\n\r\ntrainer = Trainer(plugins=[MyCustomDDP()])\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-22T00:51:44Z"
    }
  },
  {
    "title": "When does loss.backward() get called when accumulating gradients?",
    "body": "Just wondering when backward gets called when you are accumulating over (say 8) batches. I had put a breakpoint in `on_after_backward()` and that seemed to be only getting called on the 8th iteration of training. According to [this answer](https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/20?u=alband), in order to save on GPU memory, it seems its best to call `loss.backward()` on each iteration.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8460",
    "createdAt": "2021-07-19T08:55:23Z",
    "updatedAt": "2022-06-22T02:43:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sachinruk"
    },
    "answer": {
      "body": "Dear @sachinruk,\r\n\r\nThis behaviour was a bug and should have been resolved on master and on_after_backward should be called after each backward call. In the case of accumulating over (say 8) batches, you should see `on_after_backward ` called 8 times on master, and only 1 time previously.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-19T09:01:00Z"
    }
  },
  {
    "title": "Loss Module with inner Network",
    "body": "I have a loss module which is part of my lightning module with its own inner pretrained vgg network. \r\nThe problem comes when I am trying to use the checkpoint (that is saved automatically) to resume training or to test my model. \r\nThen I get an error ` unexpected key(s) in state_dict pytorch lightning` which points to the keys of the network which is part of the loss function.\r\n\r\nIs there a way to load my model properly?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8445",
    "createdAt": "2021-07-16T13:12:43Z",
    "updatedAt": "2023-04-03T16:56:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vasl12"
    },
    "answer": {
      "body": "Dear @vasl12,\r\n\r\nYou have multiple options:\r\n* pass `strict=False`\r\n* drop the key before saving the weights. Use `on_save_checkpoint` hook to access the checkpoint and drop the associated key.\r\n* re-create the missing module in your model\r\n\r\nBest,\r\nT.C\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-19T09:29:20Z"
    }
  },
  {
    "title": "Save model into separate files",
    "body": "Hei everyone\r\n\r\nI am working on a small framework based on PyTorch lightning to perform some experiments. \r\nOur models always are a composition out of two networks (backbone and some header) as we want to be able to replace them independently.\r\n\r\n```\r\nclass EncoderHeaderModel(pl.LightningModule):\r\n\r\n        def __init__(self, backbone: Union[pl.LightningModule, torch.nn.Module],\r\n                 header: Union[pl.LightningModule, torch.nn.Module]):\r\n        super().__init__()\r\n\r\n        # sanity check if the last layer of the backbone is compatible with the first layer of the header\r\n\r\n        self.backbone = backbone\r\n        self.header = header\r\n\r\n    def forward(self, x):\r\n        x = self.backbone(x)\r\n        x = self.header(x)\r\n        return x\r\n```\r\n\r\nNow we have the problem with saving the state_dict of these two models separately. To store the whole model we are using the `model_checkpoint` callback which works fine. \r\n\r\nIs there an easy way to save the models each time `model_checkpoint` would save the whole model (I am already experimenting with a subclass of `model_checkpoint`)? Or should we after the training just extract the state_dicts out of the checkpoint? Any idea how to make this in a \"clean\" way?\r\n\r\nThanks",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8415",
    "createdAt": "2021-07-14T08:00:41Z",
    "updatedAt": "2023-02-04T17:52:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "lvoegtlin"
    },
    "answer": {
      "body": "I was able to solve my problem by overwriting `_del_model` and `_save_model` to perform the desired actions.",
      "author": {
        "login": "lvoegtlin"
      },
      "createdAt": "2021-07-14T14:09:25Z"
    }
  },
  {
    "title": "Multiple dataloaders in training",
    "body": "Hi, I'm trying to use integrate pytorch lightning into my current pipeline. But I'm having some difficulties in using multiple dataloaders. In my current use case, let's say I have 10 dataloaders in my pipeline, but for each training step, I'm only sampling data from 5 of them. Is it doable in pytorch lightning? I can sample from 10 dataloaders each step but that would be a waste to system IO and GPU memory. Thanks for any help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8410",
    "createdAt": "2021-07-14T04:32:05Z",
    "updatedAt": "2022-06-13T15:42:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "YuShen1116"
    },
    "answer": {
      "body": "Hey @YuShen1116,\r\n\r\nHere is the pseudo code to get it working\r\n\r\n```\r\nfrom typing import List\r\nimport itertools\r\nfrom pytorch_lightning.trainer.supporters import CombinedLoader\r\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\r\n\r\n\r\nclass CyclingLoader(object):\r\n\r\n    def __init__(self, combined_loaders: List[CombinedLoader]):\r\n        self.combined_loaders = combined_loaders\r\n        self._dataloader_idx_cycle = itertools.cycle(range(len(combined_loaders)))\r\n\r\n    def __iter__(self):\r\n        self._iterators = apply_to_collection(self.combined_loaders, CombinedLoader, iter)\r\n        self._dataloader_idx_cycle_iter = iter(self._dataloader_idx_cycle)\r\n        return self\r\n\r\n    def __next__(self):\r\n        iterator_idx = next(self._dataloader_idx_cycle_iter)\r\n        return next(self._iterators[iterator_idx])\r\n\r\n\r\nclass MyDataModule(DataModule):\r\n\r\n\r\n    def train_dataloader(self):\r\n        ds_1, .... ds_10 = create_dataloaders()\r\n\r\n        ds_1_5 = CombinedLoader([ds_1, ... ds_5])\r\n        ds_6_10 = CombinedLoader([ds_6, ... ds_10])\r\n\r\n        return CyclingLoader([ds_1_5, ds_6_10])\r\n\r\n\r\nclass Model(LightningModule):\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        if batch_idx % 2 == 0:\r\n            # batches from dataloaders from 1 - 5\r\n        elif batch_idx % 2 == 1:\r\n            # batches from dataloaders from 6 - 10\r\n```",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-19T09:59:44Z"
    }
  },
  {
    "title": "Interpret the output (logs) during training",
    "body": "```\r\nThis is my training and validation steps  \r\n def training_step(self,batch,batch_idx):\r\n    self.log('train/loss', loss, on_epoch=True,prog_bar=True)\r\n    self.log('train/iou', iou, on_epoch=True,prog_bar=True)\r\n    return loss  \r\n\r\ndef validation_step(self,batch,batch_idx):\r\n    self.log('val/loss', loss, on_epoch=True,prog_bar=True)\r\n    self.log('val/iou', iou, on_epoch=True,prog_bar=True)\r\n    return loss\r\n\r\n```\r\nThis is what logs displayed\r\n```\r\n360/361 [16:07<00:02, 2.69s/it, loss=0.148, v_num=0, val/loss=0.664, val/iou=0.234, train/loss_step=0.129, train/iou_step=0.379, train/loss_epoch=0.210, train/iou_epoch=0.371]\r\n```\r\nI am confused what is the difference between `loss` and `val/loss`? If there is `val/loss` which makes sense as I output it, then what is `loss`. There is `val/iou`, but there is not any `train/iou`, and `train/loss` as in val case. why? Is `train/iou_epoch` is for epoch or for all previous epochs?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8407",
    "createdAt": "2021-07-13T23:02:42Z",
    "updatedAt": "2022-06-21T09:47:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "Hey @talhaanwarch,\r\n\r\nLightning automatically takes your `training_step` output and applies a running mean on it and add it to the progress bar using the key loss.\r\n\r\nAs you may see, there is `train/iou_step=0.379` and `train/loss_epoch=0.210`. \r\n\r\nHere is why:\r\nFor training, Lightning logs automatically set on_step=True and you provided on_epoch=True. Therefore, to prevent values to be logged under the same key, Lightning adds a _step and _epoch suffix to differentiate them.\r\n\r\n`_epoch` is used to describe the value compute on the previous epoch, until the new reduction is performed. If you use a logger, you will clearly see the `_epoch` value logged at the right epoch.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-07-19T10:10:08Z"
    }
  },
  {
    "title": "how to put ```trainer.fit()``` in for loop?",
    "body": "I am trying to create multiple model using loop as below.\r\n\r\n```\r\nfor client in clients:\r\n    t.manual_seed(10)\r\n    client['model'] = LinearNN(learning_rate = args.lr, i_s = args.input_size,  h1_s = args.hidden1, h2_s = args.hidden2, n_c = args.output, client=client)\r\n    client['optim'] = optim.Adam(client['model'].parameters(), lr= args.lr)\r\n```\r\n\r\nHowever, ```trainer.fit()``` is an async method. To train multiple models, I need to put ```trainer.fit()``` in a loop as follows \r\n\r\n```\r\nfor client in clients:\r\n     trainer = pl.Trainer(\r\n     max_epochs=args.epochs+1,\r\n     progress_bar_refresh_rate=20,\r\n     )\r\n     trainer.fit(client['model'])\r\n```\r\n\r\nAs this is an async method, it gives an error \r\n\r\n> AttributeError: can't set attribute\r\n\r\nas it doesn't wait for finishing ```trainer.fit()```.\r\n\r\nIs there any way to do that? \r\n\r\nThanks in advance.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8380",
    "createdAt": "2021-07-12T12:17:28Z",
    "updatedAt": "2022-10-18T06:55:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "anik123"
    },
    "answer": {
      "body": "Hi!\r\n\r\nI'm not entirely sure, but I don't see why the code snippets you shared would not work.\r\n\r\n>  is an async method\r\n\r\nWhat do you mean exactly by \"async\" method?\r\n\r\n>  it gives an error\r\n\r\nCan you share the full error stacktrace and your Lightning version?",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-23T02:08:21Z"
    }
  },
  {
    "title": "I cannot assign the GPU index by ddp or dp backends..",
    "body": "It's weired. I have a single machine with 8 GPUSs and now 0~4 are full load.\r\nI would like to use parallel training on 5 6 7 GPUs but cannot assign the task to them.\r\n\r\nSome of my codes:\r\n\r\n`    parser = argparse.ArgumentParser(description='Solver')\r\n    parser.add_argument('--config', required=True, type=str)\r\n    fargs = parser.parse_args()\r\n    args = parse_config(fargs.config)\r\n\r\n    data = DDPMData(args.Data)\r\n    data.train_dataloader()\r\n    data.test_dataloader()\r\n\r\n    num_cls = data.n_classes\r\n    shape = data.data_shapes\r\n    args.Model.Unet.kwargs.num_classes = num_cls\r\n    args.Model.DiscreteDiffusion.kwargs.num_class = num_cls\r\n    args.Model.DiscreteDiffusion.kwargs.shape = shape\r\n    model = DDDPM(args)\r\n\r\n    wandb_logger = WandbLogger()\r\n\r\n    callbacks = []\r\n    callbacks.append(ModelCheckpoint(save_last=True,every_n_train_steps=600))\r\n    callbacks.append(LearningRateMonitor(logging_interval='step'))\r\n\r\n    trainer = pl.Trainer(callbacks=callbacks,max_steps=args.max_steps, \r\n                    accelerator='dp', gpus=[5,6,7],\r\n                    logger = wandb_logger, check_val_every_n_epoch=120,\r\n                    num_sanity_val_steps=0)\r\n    trainer.fit(model, data)\r\n`\r\nAnd now nvidia-smi shows:\r\n\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\r\n| 63%   72C    P2   151W / 200W |   7997MiB /  8119MiB |     85%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA GeForce ...  On   | 00000000:06:00.0 Off |                  N/A |\r\n| 71%   77C    P2   134W / 200W |   8109MiB /  8119MiB |     98%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA GeForce ...  On   | 00000000:07:00.0 Off |                  N/A |\r\n| 82%   83C    P2   143W / 200W |   7405MiB /  8119MiB |     89%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\r\n| 80%   83C    P2   158W / 200W |   7263MiB /  8119MiB |     59%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  NVIDIA GeForce ...  On   | 00000000:0C:00.0 Off |                  N/A |\r\n| 78%   82C    P2   148W / 200W |   5719MiB /  8119MiB |    100%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  NVIDIA GeForce ...  On   | 00000000:0D:00.0 Off |                  N/A |\r\n|  0%   24C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  NVIDIA GeForce ...  On   | 00000000:0E:00.0 Off |                  N/A |\r\n|  0%   34C    P8     8W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  NVIDIA GeForce ...  On   | 00000000:0F:00.0 Off |                  N/A |\r\n|  0%   25C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      4082      C   python                           7993MiB |\r\n|    1   N/A  N/A     13619      C   python                           8105MiB |\r\n|    2   N/A  N/A      4082      C   python                           7401MiB |\r\n|    3   N/A  N/A      4082      C   python                           7259MiB |\r\n|    4   N/A  N/A     24206      C   python                           5715MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nand when I run the codes, it will assign task to GPU:2 , I don't know why..\r\n`\r\nRuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 2; 7.93 GiB total capacity; 117.19 MiB already allocated; 45.50 MiB free; 120.00 MiB reserved in total by PyTorch)`\r\n\r\nAny idea..?\r\n\r\n\r\n##############################\r\ntorch = 1.7.1\r\npl = 1.3.8",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8368",
    "createdAt": "2021-07-10T19:35:38Z",
    "updatedAt": "2022-06-15T14:33:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mhh0318"
    },
    "answer": {
      "body": "Problemed solved.\r\nI forgot to map device when I load a ckpt file as data...",
      "author": {
        "login": "mhh0318"
      },
      "createdAt": "2021-07-14T08:44:13Z"
    }
  },
  {
    "title": "Getting error after completion of 1st epoch",
    "body": "I am training an image classifier on tpu and am getting errors after execution of the first epoch.\r\n\r\nhttps://colab.research.google.com/drive/1Lgz0mF6UiLirsDltPQH5HtctmVvb7gHm?usp=sharing",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8339",
    "createdAt": "2021-07-08T18:35:08Z",
    "updatedAt": "2021-07-23T10:58:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Abhishek-Prajapat"
    },
    "answer": {
      "body": "I don't see an error in the provided link.\r\n\r\nIs this still relevant?",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-23T03:45:02Z"
    }
  },
  {
    "title": "ImportError: _XLAC.cpython: undefined symbol",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nImport error after installing pytorch-lightning on google colab TPU instance.\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\nColab link: https://colab.research.google.com/drive/1ssH3PwBh6skcIze440LwHn5R5dUnih7Q?usp=sharing\r\n\r\n\r\nCode to reproduce:\r\n\r\n```\r\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\r\n!pip install pytorch-lightning\r\n\r\nimport pytorch_lightning as pl\r\n```\r\n\r\nConsole output:\r\n\r\n```\r\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\r\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\r\nWARNING:root:TPU has started up successfully with version pytorch-1.8\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-efd8697dde72> in <module>()\r\n----> 1 import pytorch_lightning as pl\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/__init__.py in <module>()\r\n    126 import torch\r\n    127 from ._patched_functions import _apply_patches\r\n--> 128 import _XLAC\r\n    129 \r\n    130 \r\n\r\nImportError: /usr/local/lib/python3.7/dist-packages/_XLAC.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at11result_typeERKNS_6TensorEN3c106ScalarE\r\n```\r\n\r\nI followed the instructions from the docs: https://pytorch-lightning.readthedocs.io/en/stable/advanced/tpu.html. I also tried xla version 1.7, 1.6, and pl version 1.1.8.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8320",
    "createdAt": "2021-07-06T22:40:23Z",
    "updatedAt": "2022-08-23T09:45:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "BryanWBear"
    },
    "answer": {
      "body": "Hi @BryanWBear, this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!",
      "author": {
        "login": "kaushikb11"
      },
      "createdAt": "2021-07-07T08:52:21Z"
    }
  },
  {
    "title": "ModelCheckpoint creating unexpected subfolders",
    "body": "Hi! I have found a weird behavior when using ModelCheckpoint, if I have a metric that I want to save in my filename and it has a \"/\" on it it will create nested directories. For example\r\n\r\n```py\r\ncheckpoint_callback = ModelCheckpoint(\r\n        monitor='val/acc',\r\n        dirpath=checkpoints_dir,\r\n        filename='checkpoint_{epoch:02d}-{val/acc}',\r\n        save_top_k=-1,\r\n     )\r\n```\r\nThis one will create one extra folder per checkpoint:\r\n```bash\r\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.04-v1.ckp\r\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.05-v1.ckp\r\n```\r\nIs there any way to make the modelcheckpoint callback store the \"val/acc\" value while not using the string \"val/acc\" to reference it? Something like:\r\ncheckpoints/base_lstm/checkpoint_epoch=00-valAcc=0.04-v1.ckp\r\nI think is quite standard to use \"/\" on tensorboard to be able to use the inbuilt tabs to better group metrics.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8300",
    "createdAt": "2021-07-06T00:08:24Z",
    "updatedAt": "2022-06-11T10:13:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jiwidi"
    },
    "answer": {
      "body": "hi, can you provide also your model sample, in particular, the metrics section\r\nI guess that the problem is with `/` as it is interpreted as a normal folder path, as you can see that `'val/acc'` is not replaced by a number either... :rabbit: ",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-07-07T09:09:40Z"
    }
  },
  {
    "title": "Help understanding data module error",
    "body": "Hi there,\r\n\r\nI am trying to implement a data module however I keep getting an error that I cannot understand. I normally setup my data module as:\r\n```python\r\nclass dataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size, csv_file, data_dir):\r\n        super().__init__()\r\n        self.csv_file = csv_file\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n        self.preprocess = None\r\n        self.transform = None\r\n        self.train_set = None\r\n        self.val_set = None\r\n        self.test_set = None\r\n    \r\n    def get_augmentation_transform(self):\r\n        augment = tio.Compose([\r\n            tio.RandomAffine(),\r\n            tio.RandomFlip(p = 0.25),\r\n            tio.RandomGamma(p=0.25),\r\n            tio.RandomNoise(p=0.25),\r\n            tio.RandomMotion(p=0.1),\r\n            tio.RandomBiasField(p=0.25),\r\n        ])\r\n        return augment\r\n\r\n    def setup(self, stage=None):\r\n        \r\n        subjList = fmriDataset(csv_file = self.csv_file,\r\n                              root_dir = self.data_dir)\r\n    \r\n        train_size, val_size = int(0.7 * len(subjList)), int(0.2 * len(subjList))\r\n        test_size = len(subjList) - train_size - val_size\r\n        \r\n        if stage == 'fit' or stage is None:\r\n            self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n        if stage == 'test' or stage is None:\r\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n        augment = self.get_augmentation_transform()\r\n\r\n        self.train_set = tio.SubjectsDataset(self.train_dataset, transform=augment)\r\n        self.val_set = tio.SubjectsDataset(self.val_dataset, transform=None)\r\n        self.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=27)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val_set, self.batch_size, num_workers=27)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_set, self.batch_size, num_workers=27)\r\n```\r\n\r\nHowever, when I call `trainer.tune(model = model, datamodule = data)` I get the error:\r\n```python\r\nAttributeError: 'dataModule' object has no attribute 'test_dataset'\r\n```\r\n<br><br><br><br>\r\nHowever, if I change these lines\r\n```python\r\nif stage == 'fit' or stage is None:\r\n    self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\nif stage == 'test' or stage is None:\r\n    _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n```\r\nto a single line:\r\n```python\r\nself.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n```\r\neverything works. Is there something basic that I have missed?\r\n<br><br><br><br>\r\nFor completeness:\r\n- Pytorch version: 1.9.0\r\n- Pytorch-lightning version: 1.3.7\r\n\r\nAnd I initialise the data module/model/trainer with:\r\n```python\r\ndata = dataModule(data_dir = '/home/data/', csv_dir = '/home/scanList.csv', batch_size = 24)\r\n\r\nmodel = cnnRnnClassifier()\r\n\r\nearly_stop_callback = Earlystopping(\r\n    monitor = 'val_loss',\r\n    min_delta = 1e-4,\r\n    patience = 10,\r\n    Verbose = True,\r\n    mode = 'min')\r\n\r\ntrainer = Trainer(\r\n    gpus =  1,\r\n    fast_dev_run = False,\r\n    max_epochs = 100,\r\n    weights_summary = 'full',\r\n    callbacks = [early_stop_callback],\r\n    auto_lr_find = True,\r\n    precision = 16)\r\n\r\ntrainer.tune(model = model, datamodule = data)\r\ntrainer.fit(model = model, datamodule = data)\r\ntrainer.test(model = model, datamodule = data)\r\n```\r\n\r\nThanks in advance for your help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8298",
    "createdAt": "2021-07-05T16:23:44Z",
    "updatedAt": "2022-10-12T12:01:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ik362"
    },
    "answer": {
      "body": "The problem is the combination of this line:\r\n\r\n```python\r\n if stage == 'test' or stage is None:\r\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n```\r\n\r\nand this line:\r\n```python\r\nself.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\r\n```\r\n\r\nas you can see, `self.test_dataset` is only defined if the condition above applies. With this hint you should be able to figure it out now. Let me know :) ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-07-05T16:33:13Z"
    }
  },
  {
    "title": "In load_from_checkpoint, \"TypeError: __init__ () missing 1 required positional argument:'cfg'\"",
    "body": "I am experimenting with the following repository. [Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning](https://github.com/Keiku/PyTorch-Lightning-CIFAR10)\r\n\r\nIt is implemented as follows. I was able to train/validation. But test has not been implemented yet.\r\n\r\n```python\r\nimport os\r\n\r\nimport hydra\r\nimport torch\r\nfrom hydra.core.hydra_config import HydraConfig\r\nfrom omegaconf import DictConfig, OmegaConf\r\nfrom pytorch_lightning import Trainer, seed_everything\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\r\n\r\nfrom datamodule import LitCIFAR10DataModule\r\nfrom model import LitCIFAR10Model\r\n\r\n\r\n@hydra.main(config_path=\"./configs\", config_name=\"default.yaml\")\r\ndef main(cfg: DictConfig) -> None:\r\n\r\n    if \"experiments\" in cfg.keys():\r\n        cfg = OmegaConf.merge(cfg, cfg.experiments)\r\n\r\n    seed_everything(0)\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.runs.gpu_id\r\n\r\n    if cfg.runs.logger == \"wandb\":\r\n        logger = WandbLogger(name=cfg.model.classifier, project=\"cifar10\")\r\n    elif cfg.runs.logger == \"tensorboard\":\r\n        logger = TensorBoardLogger(cfg.train.tensorboard_dir, name=cfg.model.classifier)\r\n\r\n    checkpoint = ModelCheckpoint(monitor=\"acc/val\", mode=\"max\", save_last=True)\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=cfg.runs.dev,\r\n        logger=logger if not (cfg.runs.dev or cfg.runs.evaluate) else None,\r\n        gpus=-1,\r\n        deterministic=True,\r\n        weights_summary=None,\r\n        log_every_n_steps=1,\r\n        max_epochs=cfg.train.num_epochs,\r\n        checkpoint_callback=checkpoint,\r\n        precision=cfg.runs.precision,\r\n        resume_from_checkpoint=cfg.train.checkpoint,\r\n    )\r\n\r\n    datamodule = LitCIFAR10DataModule(cfg)\r\n    model = LitCIFAR10Model(cfg)\r\n\r\n    if cfg.runs.evaluate:\r\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n    else:\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nI tried using `load_from_checkpoint` for test, but I get the following error. How can I solve it?\r\n\r\n```bash\r\n\u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\npipenv run python train.py +experiments=test_exp01 hydra.run.dir=outputs/test_exp01\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 48, in main\r\n    model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 159, in load_from_checkpoint\r\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\r\n    model = cls(**_cls_kwargs)\r\nTypeError: __init__() missing 1 required positional argument: 'cfg'\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n\u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8289",
    "createdAt": "2021-07-05T12:16:47Z",
    "updatedAt": "2022-06-20T14:14:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "I was able to infer test by setting as follows. I think the documentation is missing, so please refer to my implementation.\r\n\r\n```python\r\n    if cfg.runs.evaluate:\r\n        hparams = OmegaConf.load(cfg.test.hparams)\r\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint, **hparams)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n    else:\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n```",
      "author": {
        "login": "Keiku"
      },
      "createdAt": "2021-07-06T02:19:31Z"
    }
  },
  {
    "title": "How to accumulate grad batches for GANs in `pytorch_lightning >= 1.3.3`?",
    "body": "Hi,\r\n\r\nSince the `1.3.3`, I cannot use the flag `accumulate_grad_batches` for the training of GANs.\r\nIt is immediately reproducible on the GAN example given in `pl_examples.domain_templates.generative_adversarial_net`.\r\n\r\nFor any version higher than `1.3.2`, in the `Trainer`, if `accumulate_grad_batches` is set to any value higher than `1`, after one batch (`batch_idx == 1`) the following error arises:\r\n```\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\nThis is due to the fact that the `g_loss` does not require gradients, because the generator's output does not requires any gradients.\r\n\r\nIs it intended?\r\nIf yes, what is the workaround to train GANs while still accumulating gradients?\r\n\r\nThanks in advance,\r\n\r\nGuillaume",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8280",
    "createdAt": "2021-07-04T20:33:07Z",
    "updatedAt": "2022-09-05T15:28:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "GuillaumeRochette"
    },
    "answer": {
      "body": "Hey, I believe this is the same bug I fixed here: https://github.com/PyTorchLightning/pytorch-lightning/pull/8284 \r\nWill try to put it into the next bugfix release. \r\n\r\nAs a workaround, you can try to manually call `self.untoggle_optmizer(optimizer)` in your LM.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-07-05T15:05:22Z"
    }
  },
  {
    "title": "GPU usage does not remain high for lightweight models when loaded CIFAR-10 as a custom dataset",
    "body": "I am experimenting with the following repository. [Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning](https://github.com/Keiku/PyTorch-Lightning-CIFAR10)\r\n\r\nI have implemented two methods, one is to load CIFAR-10 from torchvision and the other is to load CIFAR-10 as a custom dataset. Also, I have implemented two models: a lightweight model (eg scratch resnet18, timm MobileNet V3, etc.) and a relatively heavy model (eg scratch resnet50, timm resnet152).\r\n\r\nAfter some experiments, I found the following.\r\n\r\n* GPU usage remains high (nearly 100%) on any model when loading CIFAR-10 with torchvision\r\n* When loading CIFAR-10 as a custom dataset, GPU usage remains relatively high (still temporarily zero) for heavy models\r\n* When loading CIFAR-10 as a custom dataset, GPU usage remains low (going back and forth between 0% and 100%) for lightweight models (resnet18, MobileNetV3)\r\n\r\nIn this situation, is there a problem with the implementation code of the custom dataset? Also, please let me know if there is a way to increase GPU usage even for lightweight models.\r\n\r\nI am experimenting in the following EC2 g4dn.xlarge environment.\r\n\r\n```\r\n\u22ca> ~ lsb_release -a                                                    (base) 21:45:51\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 18.04.5 LTS\r\nRelease:        18.04\r\nCodename:       bionic\r\n\u22ca> ~ nvidia-container-cli info                                         (base) 21:48:20\r\nNVRM version:   450.80.02\r\nCUDA version:   11.0\r\n\r\nDevice Index:   0\r\nDevice Minor:   0\r\nModel:          Tesla T4\r\nBrand:          Tesla\r\nGPU UUID:       GPU-ba54be15-066e-e7e5-87d0-84b8ac2672c6\r\nBus Location:   00000000:00:1e.0\r\nArchitecture:   7.5\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8274",
    "createdAt": "2021-07-04T12:31:54Z",
    "updatedAt": "2022-08-16T14:18:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "I got a replay from ptrblck.\r\n\r\nhttps://discuss.pytorch.org/t/gpu-usage-does-not-remain-high-for-lightweight-models-when-loaded-cifar-10-as-a-custom-dataset/125738",
      "author": {
        "login": "Keiku"
      },
      "createdAt": "2021-07-05T04:12:06Z"
    }
  },
  {
    "title": "Earlystopping callback metrics in different devices with single gpu",
    "body": "The device of the metric return by validation_step is GPU, related code is\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    if y.device != self.device:\r\n        y = y.to(self.device)\r\n    y_hat = self(x)\r\n    loss = self.loss(y_hat, y)    # loss.device is cuda.\r\n    self.log('valid loss', loss.item())\r\n    return loss\r\n```\r\n\r\nAfter an epoch of validation compeleted when using earlystopping, follow error occured: \r\n```\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 871, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 584, in run_training_epoch\r\n    self.trainer.run_evaluation(on_epoch=True)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1011, in run_evaluation\r\n    self.evaluation_loop.on_evaluation_end()\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\", line 102, in on_evaluation_end\r\n    self.trainer.call_hook('on_validation_end', *args, **kwargs)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1228, in call_hook\r\n    trainer_hook(*args, **kwargs)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\callback_hook.py\", line 227, in on_validation_end\r\n    callback.on_validation_end(self, self.lightning_module)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 173, in on_validation_end\r\n    self._run_early_stopping_check(trainer)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 193, in _run_early_stopping_check\r\n    should_stop, reason = self._evalute_stopping_criteria(current, trainer)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 226, in _evalute_stopping_criteria\r\n    elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n```\r\n\r\nThis error didn't appear until I updated the version of pytorch_lightning.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8267",
    "createdAt": "2021-07-03T08:58:18Z",
    "updatedAt": "2022-06-12T09:44:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Les1ie"
    },
    "answer": {
      "body": "Looking into it in #8295",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-07-05T15:55:40Z"
    }
  },
  {
    "title": "how to plot confusion_matrix with lightning tensorboard?",
    "body": "I need to plot confusion_matrix in tensotboard ,how to do it ?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8254",
    "createdAt": "2021-07-02T04:56:56Z",
    "updatedAt": "2022-06-13T02:35:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jaffe-fly"
    },
    "answer": {
      "body": "https://stackoverflow.com/questions/65498782/how-to-dump-confusion-matrix-using-tensorboard-logger-in-pytorch-lightning",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-07-02T05:39:36Z"
    }
  },
  {
    "title": "Getting Validation Accuracy with validation_epoch_end",
    "body": "Hello i managed to implement training accuracy per epoch with training_epoch_end but i want to do the same with validation accuracy with validation_epoch_end but i get an error of \"too many indices for tensor of dimension 0\" when train.fit() . I am using pytorch-lightning==1.2.8  . Thanks in advance.\r\n\r\n\r\nError is: \r\n\r\n```\r\n<ipython-input-31-41c968828bca> in validation_epoch_end(self, outputs)\r\n     39     predictions = []\r\n     40     for output in outputs:\r\n---> 41       for out_labels in output[\"labels\"].detach().cpu():\r\n     42         labels.append(out_labels)\r\n     43       for out_predictions in output[\"predictions\"].detach().cpu():\r\n\r\nIndexError: too many indices for tensor of dimension 0\r\n```\r\n\r\n\r\n ```\r\n def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def validation_epoch_end(self, outputs):\r\n\r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    validation_acc = accuracy(predictions, labels)\r\n    self.logger.experiment.add_scalar(\"Validation Accuracy\", validation_acc, self.current_epoch)  \r\n\r\n\r\n\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8240",
    "createdAt": "2021-07-01T15:21:38Z",
    "updatedAt": "2022-08-10T00:51:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "TehJimmmyy"
    },
    "answer": {
      "body": "I am a fool , i forgot to add return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels} in the validation step.",
      "author": {
        "login": "TehJimmmyy"
      },
      "createdAt": "2021-07-01T16:07:38Z"
    }
  },
  {
    "title": "CNN dimension error",
    "body": "Hi there!\r\n\r\nI am trying to build a very basic CNN for a binary classification task however I am getting an odd dimensionality issue.\r\n\r\nMy CNN is:\r\n\r\n```python\r\nclass convNet(nn.Module):\r\n    def __init__(self):\r\n        \r\n        super().__init__()\r\n        self.conv2d_1 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.conv2d_2 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.conv2d_3 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.maxpool = nn.MaxPool2d(2)\r\n    \r\n    def forward(self, x):\r\n        x = self.conv2d_1(x)\r\n        x = F.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.conv2d_2(x)\r\n        x = F.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.conv2d_3(x)\r\n        return F.relu(x)\r\n```\r\n\r\nAnd my lightning module is:\r\n```python\r\n\r\nclass classifier(pl.LightningModule):\r\n    \r\n    def __init__(self, learning_rate = float):\r\n        super().__init__()\r\n        \r\n        self.learning_rate = learning_rate\r\n        self.cnn =covNet()\r\n        self.flat = nn.Flatten()\r\n        \r\n        self.fc1 = nn.Linear(450076, 100)\r\n        self.fc2 = nn.Linear(100, 10)\r\n        self.fc3 = nn.Linear(10, 1)\r\n        \r\n        self.dropout = nn.Dropout(p = 0.2)\r\n        \r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        \r\n        self.accuracy = tm.Accuracy()\r\n\r\n    def prepare_batch(self, batch):\r\n        img = batch['image'][tio.DATA]\r\n        img = torch.squeeze(img)\r\n        diagnosis = batch['diagnosis']\r\n        return img, diagnosis\r\n\r\n    def forward(self, x):\r\n        cnn_out = self.cnn(x)\r\n        flat = self.flat(cnn_out)\r\n        fc1_out = self.dropout(F.relu(self.fc1(flat)))\r\n        fc2_out = self.dropout(F.relu(self.fc2(fc1_out)))\r\n        fc3_out = F.relu(self.fc3(fc2_out))\r\n        return fc3_out\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        train_loss = self.criterion(y_hat, y)\r\n        self.log('train_loss', train_loss, prog_bar = True)\r\n        return train_loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        val_loss = self.criterion(y_hat, y)\r\n        self.log('val_loss', val_loss, prog_bar = True)\r\n        return val_loss\r\n        \r\n    def test_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        testAcc = self.accuracy(y_hat, y)\r\n        self.log_dict({'test_acc': testAcc})\r\n        return testAcc\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n```\r\n\r\nThe odd part is that the validation sanity completes and I get 75% through the first epoch when the validation loop starts before I get the error:\r\n\r\n<img width=\"613\" alt=\"Screen Shot 2021-07-01 at 3 33 38 pm\" src=\"https://user-images.githubusercontent.com/57671726/124141934-b824fc00-da81-11eb-9dba-4197b7220117.png\">\r\n\r\nAnd I have checked that all my inputs have the same dimensions of `[193, 229, 193]`. Have I missed something obvious? (sorry if it is obvious)\r\n\r\nAny help would be greatly appreciated!\r\n\r\nFor completeness:\r\n- pytorch version: 1.9.0\r\n- pytorch lightning version: 1.3.7\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8238",
    "createdAt": "2021-07-01T14:40:51Z",
    "updatedAt": "2024-02-27T23:05:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ik362"
    },
    "answer": {
      "body": "To me it seems like you have forgotten the batch dimension. 2D convolutions expect input to have shape `[N, C, H, W]` where `C=193`, `H=229` and `W=193` (is it correct that you have the same amount of channels as the width?). If you only want to feed in a single image you can do `sample.unsqueeze(0)` to add the extra batch dimension in front.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-07-02T13:21:09Z"
    }
  },
  {
    "title": "how to use pl to process tfrecords data?",
    "body": "how to use ```LightningDataModule```  to process tfrecords data, anyone can give a Tutorial ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8161",
    "createdAt": "2021-06-28T03:23:58Z",
    "updatedAt": "2022-06-04T03:04:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jaffe-fly"
    },
    "answer": {
      "body": "Depends on what data you have inside TFRecord, but you can see usage:\r\n https://github.com/vahidk/tfrecord#reading-tfexample-records-in-pytorch\r\n\r\n```py\r\ndef train_dataloader():\r\n    # index_path = None\r\n    # tfrecord_path = \"/tmp/data.tfrecord\"\r\n    # description = {\"image\": \"byte\", \"label\": \"float\"}\r\n    dataset = TFRecordDataset(tfrecord_path, index_path, description)\r\n    loader = torch.utils.data.DataLoader(dataset, batch_size=32)\r\n    return loader\r\n```\r\n\r\nalso check how to work with dataset - https://discuss.pytorch.org/t/read-dataset-from-tfrecord-format/16409/15",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-07-07T13:30:38Z"
    }
  },
  {
    "title": "Can training step return dictionary?",
    "body": "My training_step returns {\u2018loss\u2019: loss, \u2018num\u2019: len(batch)}, so I could calculate mean loss at the end of epoch. But now I get this warning, that my training_step returns None and loss doesn't display in the progress bar. How can I return dict from training_step and display loss in the progress bar at the same tim?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8153",
    "createdAt": "2021-06-27T14:07:29Z",
    "updatedAt": "2022-07-09T03:28:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "igoldov"
    },
    "answer": {
      "body": "> Can training step return dictionary?\r\n\r\nYes, check the [doc](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step) of `LightningModule.training_step`.\r\n\r\n> How can I return dict from training_step and display loss in the progress bar at the same time?\r\n\r\njust return dict like what you do now and use `self.log(loss, prog_bar=True)` to display loss in prog_bar.\r\n\r\n> so I could calculate mean loss at the end of epoch\r\n\r\nBTW, If you just want mean loss, I think pytorch_lightning have already done for you and you don't need to modified training_step_end(not sure)\r\n\r\n",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-06-27T14:22:33Z"
    }
  },
  {
    "title": "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
    "body": "Hi, everyone.\r\n\r\nI have a repo whose link is [https://github.com/zhoufengfan/pytorch-lightning-cifar10](https://github.com/zhoufengfan/pytorch-lightning-cifar10).\r\n\r\nWhen I run `python train.py`, it returns this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 842, in run_train\r\n    self.run_sanity_check(self.lightning_module)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1107, in run_sanity_check\r\n    self.run_evaluation()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 962, in run_evaluation\r\n    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 174, in evaluation_step\r\n    output = self.trainer.accelerator.validation_step(args)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 226, in validation_step\r\n    return self.training_type_plugin.validation_step(*args)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 322, in validation_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 619, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 57, in forward\r\n    output = self.module.validation_step(*inputs, **kwargs)\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 42, in validation_step\r\n    self.evaluate(batch, 'val')\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 32, in evaluate\r\n    logits = self(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 20, in forward\r\n    out = self.model(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 220, in forward\r\n    return self._forward_impl(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 215, in _forward_impl\r\n    x = self.fc(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 93, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\", line 1690, in linear\r\n    ret = torch.addmm(bias, input, weight.t())\r\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\r\n```\r\nBut after I change the `AVAIL_GPUS ` in the `hyper_var.py` file to `min(1, torch.cuda.device_count())`, the bug disappear()(see the comment in the `hyper_var.py` file).\r\n\r\nWhen I change the  `AVAIL_GPUS ` in the `hyper_var.py` file to `\"1,2\"`, it returns this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\nTraceback (most recent call last):\r\n  File \"/root/code/test-of-pytorch-lightning/train.py\", line 21, in <module>\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    work = _default_pg.barrier()\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    work = _default_pg.barrier()\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n```\r\n\r\nThe hardware parameters of my server are:\r\n```\r\nNVIDIA GeForce 3090\r\nCUDA Version: 11.2\r\nDriver Version: 460.39\r\n```\r\nCan anyone help me to fix the bug?\r\n\r\nThanks.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8143",
    "createdAt": "2021-06-26T08:23:13Z",
    "updatedAt": "2022-06-20T10:50:21Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhoufengfan"
    },
    "answer": {
      "body": "OK, I know the reason. The reason is that I haven't installed NCCL on my machine.\r\n\r\nAfter I installed NCCL on my machine, the errors disappeared.\r\n\r\nThanks to all the people who have replied to me. @awaelchli @SkafteNicki ",
      "author": {
        "login": "zhoufengfan"
      },
      "createdAt": "2021-07-16T13:52:00Z"
    }
  },
  {
    "title": "What's the difference between on_fit_start and on_train_start in LightningModule?",
    "body": "What's the difference between `on_fit_start` and `on_train_start` hooks in LightningModule?\r\n![image](https://user-images.githubusercontent.com/13477956/123500499-1a8c8f80-d671-11eb-9598-ebfc6f25c809.png)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8142",
    "createdAt": "2021-06-26T03:25:05Z",
    "updatedAt": "2022-06-14T14:37:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "marsggbo"
    },
    "answer": {
      "body": "I think the document [here](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks) has answered your question very well.",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-06-28T02:22:01Z"
    }
  },
  {
    "title": "Accessing DataModule or DataLoaders within model hooks",
    "body": "Hey, as the title says, I want to access the DataModule or the DataLoader inside the on fit start hook. Is this possible and how can I do it? To be more specifc I want to access my model, when I have access to my DataModule, to get a batch of data, then use it to apply some pruning algorithm on my model. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8114",
    "createdAt": "2021-06-24T09:06:57Z",
    "updatedAt": "2022-06-21T11:14:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MohammedAljahdali"
    },
    "answer": {
      "body": "`self.datamodule` or `self.trainer.train_dataloader` in the LightningModule",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-24T09:43:51Z"
    }
  },
  {
    "title": "how to load dataset only once on the same machine?",
    "body": "My dataset is large, with total CPU memory usage of 20 GB. I train on 2 nodes with 8 GPU. And I use slurm to train it. But I found that each process will consume 20 GB memory, which is equivelence to 80 GB each node. That's not what I want. I want a node to consume only 20GB in total. Is there a way to do that?\r\n```python\r\nclass DataModule(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.batch_size = 1\r\n        self.CT_dataset=np.load(\"./CT_dataset.npy\")#shape:(7000,1,512,512)\r\n        self.MR_dataset=np.load(\"./MR_dataset.npy\")#shape:(7000,1,512,512)\r\n        self.batch_size=1\r\n        self.CT_dataset = torch.from_numpy(self.CT_dataset)\r\n        self.CT_dataset = self.CT_dataset.float()\r\n        self.MR_dataset = torch.from_numpy(self.MR_dataset)\r\n        self.MR_dataset = self.MR_dataset.float()\r\n        self.train_dataset, self.test_dataset = random_split(TensorDataset(self.MR_dataset,self.CT_dataset), [len(self.CT_dataset)-100, 100])\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_dataset, batch_size=self.batch_size)\r\nmodel = CycleGAN()\r\nds = DataModule()\r\nlogger = TensorBoardLogger(save_dir=\"./run\")\r\ntrainer = pl.Trainer(max_epochs=1,fast_dev_run=False,profiler=\"pytorch\",overfit_batches=8,gpus=4,logger=logger,accelerator='ddp',num_nodes=2,auto_scale_batch_size='power',weights_summary='full')\r\ntrainer.fit(model, ds)\r\ntrainer.test(model,datamodule=ds)\r\n```\r\nMy code will raise `MemoryError: Unable to allocate 7.00 GiB for an array with shape (1879572480,) and data type int32`, I can't think of a way to solve it.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8112",
    "createdAt": "2021-06-24T08:09:33Z",
    "updatedAt": "2022-06-29T09:34:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yllgl"
    },
    "answer": {
      "body": "Since your data is in one single binary file, it won't be possible to reduce the memory footprint. Each ddp process is independent from the others, there is no shared memory. You will have to save each dataset sample individually, so each process can access a subset of these samples through the dataloader and sampler. \r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-24T09:55:11Z"
    }
  },
  {
    "title": "How to test in low version",
    "body": "There is no train.test () in the lower version. How to test the test data set?\r\nmy version: 0.4.6\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8103",
    "createdAt": "2021-06-23T14:11:14Z",
    "updatedAt": "2022-08-10T15:10:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "AIhang427"
    },
    "answer": {
      "body": "Hi\r\nPyTorch Lightning 0.4.6 is extremely old. You should consider upgrading. \r\nThat being said, you can always test your model as you would in plain pytorch, because the LightningModule is also just a nn.Module:\r\n\r\n```python\r\nfor inp, target in test_dataloader:\r\n    pred = model(inp)\r\n    test_loss = loss(pred, target)\r\n    ...\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-24T10:00:34Z"
    }
  },
  {
    "title": "NN output within a numba jitted function",
    "body": "Hello,\r\n\r\nI have a jitted function within which I need to use the output of a neural network (trained using PyTorch Lightning). The pseudo code will make this clearer:\r\n```py\r\nwhile True:\r\n    x = sample_from_model() # \u2190 numpy type, hence compatible with numba\r\n    out = NN(torch.Tensor(x)) # \u2190 incompatible with numba\r\n```\r\nIs there a way to circumvent this problem? First thing that comes to mind is to manually extract the weights and compute the forward pass.\r\n\r\nThanks in advance,\r\nPetar",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8099",
    "createdAt": "2021-06-23T12:57:07Z",
    "updatedAt": "2024-07-10T04:50:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pjovanovski1"
    },
    "answer": {
      "body": "Hi Petar,\r\n\r\nI'm not that familiar with numba, but if it runs with numpy types, you should be able to do this via ONNX export. \r\n\r\nyou should be able to simply get this with `my_lightning_model.to_onnx()`.\r\n\r\nNote: This dumps it to disk and you can use the onnx runtime for prediction then.",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-06-23T13:16:59Z"
    }
  },
  {
    "title": "forward() takes 1 positional argument but 2 were given",
    "body": "when i try to write a model, i got `forward() takes 1 positional argument but 2 were given` error, this is my code, i want to know the wrong plcace, thanks!!\r\n\r\ni guess the error is in UpSample place, but i don't know why...\r\n\r\n```Python\r\nclass DownSample(nn.Module):\r\n\r\n    def __init__(self, in_planes: int, out_planes: int, kernel_size: int):\r\n        super(DownSample, self).__init__()\r\n\r\n        self.down = nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=1),\r\n            nn.BatchNorm2d(out_planes),\r\n            nn.LeakyReLU()\r\n        )\r\n\r\n        init_weight.initialize(self)\r\n\r\n    def forward(self, x):\r\n        return self.down(x)\r\n\r\n\r\nclass UpSample(nn.Module):\r\n\r\n    def __init__(self, in_planes: int, out_planes: int,\r\n                 kernel_size: int, padding: int, output_padding: int,\r\n                 apply_dropout: bool = False):\r\n        super(UpSample, self).__init__()\r\n\r\n        self.up = nn.ModuleList()\r\n\r\n        self.up.append(\r\n            nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride=2,\r\n                               padding=padding, output_padding=output_padding),\r\n        )\r\n        self.up.append(nn.BatchNorm2d(out_planes))\r\n        if apply_dropout:\r\n            self.up.append(nn.Dropout())\r\n        self.up.append(nn.LeakyReLU())\r\n\r\n        init_weight.initialize(self)\r\n\r\n    def forward(self, inputs):\r\n        return self.up(inputs)\r\n```\r\n\r\n```Python\r\nclass MyEncoder(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MyEncoder, self).__init__()\r\n\r\n        down_stack = [\r\n            pix2pix.DownSample(3, 64, 4),\r\n            pix2pix.DownSample(64, 128, 4),\r\n            pix2pix.DownSample(128, 256, 4),\r\n            pix2pix.DownSample(256, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n        ]\r\n\r\n        self.encoder = nn.ModuleList()\r\n\r\n        for item in down_stack:\r\n            self.encoder.append(item)\r\n\r\n    def forward(self, inputs):\r\n        feat = inputs\r\n        for i in range(len(self.encoder)):\r\n            feat = self.encoder[i](feat)\r\n\r\n        return feat\r\n\r\n\r\nclass MyDecoder(nn.Module):\r\n    def __init__(self):\r\n        super(MyDecoder, self).__init__()\r\n\r\n        up_stack = [\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 256, 4, 1, 1, True),\r\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\r\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\r\n            pix2pix.UpSample(128, 64, 4, 1, 1, True),\r\n        ]\r\n\r\n        self.up = nn.ModuleList()\r\n\r\n        for item in up_stack:\r\n            self.up.append(item)\r\n\r\n    def forward(self, inputs):\r\n\r\n        return self.up(inputs)\r\n\r\n\r\nclass MyNet(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super(MyNet, self).__init__()\r\n\r\n        self.encoder = MyEncoder()\r\n        self.decoder = MyDecoder()\r\n\r\n    def forward(self, inputs):\r\n        feat = self.encoder(inputs)\r\n        feat = self.decoder(feat)\r\n        return feat\r\n\r\n\r\nif __name__ == '__main__':\r\n    from torchsummaryX import summary\r\n\r\n    import torch\r\n\r\n    x = torch.ones((1, 3, 512, 512))\r\n    u = UNet()\r\n\r\n    summary(model=u, x=x)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8091",
    "createdAt": "2021-06-23T02:42:27Z",
    "updatedAt": "2022-10-07T08:44:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "Hi, have a look at the full stack trace so you know which of the forward methods of these different `nn.Modules` is meant. \r\n\r\nhave you verified that\r\n`u(x)` \r\nworks?",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-25T00:48:45Z"
    }
  },
  {
    "title": "Best way to bypass requirement of a DataLoader when inputs are generated stochastically on-demand",
    "body": "Hello. Please forgive the very basic question!\r\n\r\nI really like the perks and freebies that come with letting the Lightning Trainer handle training. However, for some applications I **do not need or want a dataloader**, and so far I've not been able to figure out how to use the Trainer without fooling it by defining a dummy DataLoader that does nothing. :roll_eyes: \r\n\r\nI would like to know if there is a 'nice' way to use the 'standard' Lightning setup of a LightningModule + Trainer that bypasses any dependence on a data loader.\r\n\r\nThe context is that model (normalizing flow) inputs are generated on-demand by sampling from some known distribution, and the loss function is the KL divergence with respect to some other distribution whose un-normalised density function is known.\r\n\r\nGrateful for any suggestions!\r\n\r\nCheers,\r\nJoe.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8081",
    "createdAt": "2021-06-22T18:35:51Z",
    "updatedAt": "2022-12-28T16:28:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jmarshrossney"
    },
    "answer": {
      "body": "Hi Joe,\r\n\r\nYou don't need a DataLoader. You just need something we can iterate on, that produces batches. \r\nThat being said, some features (like automatically adapting the sampler for distributed training) won't work in that case, but it sounds like you don't need them anyways.\r\n\r\nAnother possibility would be to use the `IterableDataset` from PyTorch and wrap that one into a loader for convencience.\r\n\r\nBest,\r\nJustus",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-06-23T07:34:03Z"
    }
  },
  {
    "title": "Selecting one gpu from cli",
    "body": "Hi, I have 4 gpus on my machine. I want to select the third one, that has index 2. How do I pass an argument from CLI? \r\nI call \r\n`python 4_pretrain_encoder.py --gpus 2 --max_epochs 5`\r\nbut it runs one script on two gpus instead of the third.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8063",
    "createdAt": "2021-06-21T15:20:39Z",
    "updatedAt": "2023-06-15T15:01:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "alexyalunin"
    },
    "answer": {
      "body": "`python 4_pretrain_encoder.py --gpus 2, --max_epochs 5`\r\n\r\nMaybe you should search first\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/discussions/7461",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-06-21T15:34:48Z"
    }
  },
  {
    "title": "Can I set the epoch/step initial value to 1?",
    "body": "Can I set the epoch/step initial value to 1? Now the initial default is 0, it feels awkward when watching Tensorboard.\r\nIn addition, can I bring plots in the training and valid in one tersorboard plot?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8054",
    "createdAt": "2021-06-21T07:26:04Z",
    "updatedAt": "2022-06-11T21:10:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yuys0602"
    },
    "answer": {
      "body": "Dear @yuys0602,\r\n\r\nWe are starting at 0 as we are counting the number of epoch and before starting, you actually haven't completed any epoch.\r\nIf this is really an issue for you, I believe there might be a hacky solution.\r\n\r\nI believe this should be working if you are using master. @carmocca Could you confirm ?\r\n\r\n```\r\ndef training_step()\r\n\r\n    self.log(\"epoch\", self.trainer.current_epoch + 1)\r\n```\r\n\r\nConcerning, `can I bring plots in the training and valid in one tersorboard plot?` \r\nUnfortunately, I believe this is a feature provided or not by Logger and Lightning can't do much about it.\r\n\r\nBest,\r\nT.C\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-06-21T08:32:20Z"
    }
  },
  {
    "title": "load checkpoint model error",
    "body": "this is my model `__init__` func\r\n```Python\r\ndef __init__(self, num_classes: int, image_channels: int = 3, drop_rate: int = 0.5,\r\n                 filter_config: tuple = (64, 128, 256, 512, 512), attention=False):\r\n```\r\n\r\nthis is my load code:\r\n```Python\r\nm = SegNet(num_classes=1)\r\nmodel = m.load_from_checkpoint('checkpoints/epoch=99-step=312499.ckpt')\r\n```\r\nwhen i try to load a checkpoint model, i got this error:\r\n\r\n```\r\nTypeError: __init__() missing 1 required positional argument: 'num_classes'\r\n```\r\n\r\nwho can help me?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8050",
    "createdAt": "2021-06-21T00:51:13Z",
    "updatedAt": "2022-06-14T17:20:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "morestart"
    },
    "answer": {
      "body": "Dear @morestart,\r\n\r\nGreat question ! You should use save_hyperparameters function, so Lightning can save your init arguments inside the checkpoint for future reload.\r\n\r\nHere is the associated doc: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=save_hyperparameters#save-hyperparameters\r\n\r\n```\r\nclass SegNet(LightningModule)\r\n\r\n    def __init__(\r\n        self,\r\n        num_classes: int,\r\n        image_channels: int = 3,\r\n        drop_rate: int = 0.5,\r\n        filter_config: tuple = (64, 128, 256, 512, 512),\r\n        attention=False\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        ....\r\n```\r\n\r\nBest,\r\nT.C\r\n        \r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-06-21T08:43:53Z"
    }
  },
  {
    "title": "share steps among test and validation steps",
    "body": "I'm implementing the same functionality for _validation_step_ and _test_step_.\r\nCurrently, I have implemented it by calling to a shared function (_val_and_test_step_)\r\n```\r\n    def val_and_test_step(self, data_batch, batch_nb):\r\n        output = shared_functionality(data_batch, batch_nb)\r\n        return output\r\n\r\n    def validation_step(self, data_batch, batch_nb):\r\n        return self.val_and_test_step(data_batch, batch_nb)\r\n\r\n    def test_step(self, data_batch, batch_nb):\r\n        return self.val_and_test_step(data_batch, batch_nb) \r\n```\r\nIs there a more _pythonic_ way to implement the above?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8046",
    "createdAt": "2021-06-20T08:53:37Z",
    "updatedAt": "2023-11-01T07:47:41Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ItamarKanter"
    },
    "answer": {
      "body": "Dear @ItamarKanter,\r\n\r\nI believe this great and pretty pythonic ! \r\n\r\nYou could do this to make it slightly cleaner.\r\n```\r\nclass Model(LightningModule)\r\n\r\n    def common_step(self, batch, batch_idx, stage):\r\n        logits = self(batch[0])\r\n        loss = self.compute_loss(logits, batch[1])\r\n        self.log(f\"{state}_loss\", loss)\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.common_step(batch, batch_idx, \"train\")\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"val\")\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"test\")\r\n```\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-06-21T08:48:32Z"
    }
  },
  {
    "title": "I want to apply custom learning rate scheduler.",
    "body": "I want to apply custom learning rate scheduler like below.  \r\n\r\n```python\r\nclass WarmupLRScheduler(torch.optim.lr_scheduler._LRScheduler):\r\n    \"\"\"\r\n    Warmup learning rate until `total_steps`\r\n\r\n    Args:\r\n        optimizer (Optimizer): wrapped optimizer.\r\n        configs (DictConfig): configuration set.\r\n    \"\"\"\r\n    def __init__(\r\n            self,\r\n            optimizer: Optimizer,\r\n            configs: DictConfig,\r\n    ) -> None:\r\n        super(WarmupLRScheduler, self).__init__(optimizer, configs.lr_scheduler.init_lr)\r\n        if configs.lr_scheduler.warmup_steps != 0:\r\n            warmup_rate = configs.lr_scheduler.peak_lr - configs.lr_scheduler.init_lr\r\n            self.warmup_rate = warmup_rate / configs.lr_scheduler.warmup_steps\r\n        else:\r\n            self.warmup_rate = 0\r\n        self.update_steps = 1\r\n        self.lr = configs.lr_scheduler.init_lr\r\n        self.warmup_steps = configs.lr_scheduler.warmup_steps\r\n\r\n    def step(self, val_loss: Optional[torch.FloatTensor] = None):\r\n        if self.update_steps < self.warmup_steps:\r\n            lr = self.init_lr + self.warmup_rate * self.update_steps\r\n            self.set_lr(self.optimizer, lr)\r\n            self.lr = lr\r\n        self.update_steps += 1\r\n        return self.lr\r\n```\r\n\r\nBut I find that my custom lr schedulers doesn't work in pytorch lightning.  \r\nI set lightning module's `configure_optimizers` like below:\r\n\r\n```python\r\ndef configure_optimizers(self):\r\n    r\"\"\"\r\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\r\n\r\n\r\n    Returns:\r\n        - **Dictionary** - The first item has multiple optimizers, and the second has multiple LR schedulers\r\n            (or multiple ``lr_dict``).\r\n    \"\"\"\r\n    SUPPORTED_OPTIMIZERS = {\r\n        \"adam\": Adam,\r\n        \"adamp\": AdamP,\r\n        \"radam\": RAdam,\r\n        \"adagrad\": Adagrad,\r\n        \"adadelta\": Adadelta,\r\n        \"adamax\": Adamax,\r\n        \"adamw\": AdamW,\r\n        \"sgd\": SGD,\r\n        \"asgd\": ASGD,\r\n        \"novograd\": Novograd,\r\n    }\r\n\r\n    assert self.configs.model.optimizer in SUPPORTED_OPTIMIZERS.keys(), \\\r\n        f\"Unsupported Optimizer: {self.configs.model.optimizer}\\n\" \\\r\n        f\"Supported Optimizers: {SUPPORTED_OPTIMIZERS.keys()}\"\r\n\r\n    self.optimizer = SUPPORTED_OPTIMIZERS[self.configs.model.optimizer](\r\n        self.parameters(),\r\n        lr=self.configs.lr_scheduler.lr,\r\n    )\r\n    scheduler = SCHEDULER_REGISTRY[self.configs.lr_scheduler.scheduler_name](self.optimizer, self.configs)\r\n\r\n    if self.configs.lr_scheduler.scheduler_name == \"reduce_lr_on_plateau\":\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'monitor': 'val_loss',\r\n            'interval': 'epoch',\r\n        }\r\n    elif self.configs.lr_scheduler.scheduler_name == \"warmup_reduce_lr_on_plateau\":\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'monitor': 'val_loss',\r\n            'interval': 'step',\r\n        }\r\n    else:\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'interval': 'step',\r\n        }\r\n\r\n    return {\r\n        'optimizer': self.optimizer,\r\n        'lr_scheduler': lr_scheduler\r\n    }\r\n``` \r\n\r\nIf you fine some weird, please let me know. Thank you.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8042",
    "createdAt": "2021-06-19T16:51:31Z",
    "updatedAt": "2024-10-10T13:34:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sooftware"
    },
    "answer": {
      "body": "Dear @sooftware,\r\n\r\nAny updates ? We solved some bugs related monitoring.\r\n\r\nBest,\r\nT.C",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-08-26T15:21:41Z"
    }
  },
  {
    "title": "How to use predict function to return predictions",
    "body": "How to get predictions \r\n```py\r\n  class OurModel(LightningModule):\r\n      def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        self.layer = MyModelV3()\r\n      def forward(self,x):\r\n        return self.layer(x)\r\n\r\n  def train_dataloader(self):\r\n    return DataLoader(DataReader(train_df))\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    return loss\r\n\r\n  def test_dataloader(self):\r\n    return DataLoader(DataReader(test_df))\r\n    \r\n  def test_step(self,batch,batch_idx):\r\n    image,label=batch\r\n    out=self(image)\r\n    loss=self.criterion(out,label)\r\n    return loss\r\n\r\n  def predict(self, batch):\r\n        return self(batch) \r\n ```\r\n\r\nI am not sure, how to use predict function. How to define data loader for predict function. I want to get predictions for `test_df`. But now idea how to do this.\r\n ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8038",
    "createdAt": "2021-06-19T11:21:34Z",
    "updatedAt": "2022-07-19T08:19:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "talhaanwarch"
    },
    "answer": {
      "body": "Hi @talhaanwarch, in order to get predictions from a data loader you need to implement predict_step in your `LightningModule` (docs here: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#predict-step). You would then be able to call `Trainer.predict` with the dataloader you want use following the API here: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.predict\r\n\r\nHope that helps :smiley:",
      "author": {
        "login": "ethanwharris"
      },
      "createdAt": "2021-06-19T18:44:40Z"
    }
  },
  {
    "title": "The validating log does not remain in the console.",
    "body": "The validating log does not remain in the console. How can I print and display it?\r\n\r\nThe current `validation_step` is as follows.\r\n\r\n```python\r\n    def validation_step(self, batch, batch_nb):\r\n        loss, accuracy = self.forward(batch)\r\n        self.log(\"loss/val\", loss)\r\n        self.log(\"acc/val\", accuracy)\r\n        return loss\r\n```\r\n\r\nIt will be displayed for a moment as shown below, but it will not remain as a log.\r\n\r\n```bash\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nEpoch 26:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 114/116 [00:25<00:00,  4.50it/s, loss=0.623, v_num=0]\r\nValidating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19/19 [00:01<00:00, 18.68it/s]\r\n```\r\n\r\n```\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nEpoch 28:   3%|\u258c                 | 4/116 [00:01<00:42,  2.62it/s, loss=0.588, v_num=0]\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8012",
    "createdAt": "2021-06-17T02:42:19Z",
    "updatedAt": "2023-02-17T16:45:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Keiku"
    },
    "answer": {
      "body": "Yes that's correct, it's a UI design choice. \r\nIf you wish to see your values in the progress bar, you can log with `self.log(\"loss/val\", loss, prog_bar=True)` and it will show up in the main bar.\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-25T00:55:33Z"
    }
  },
  {
    "title": "code hangs when creating a dir",
    "body": "Hello,\r\n\r\nI have written a small class derived from `Callback` in order to log some outputs during training. Initially, a directory is created, for the corresponding files/outputs to be stored:\r\n\r\n```\r\nclass epochCallback(pl.callbacks.Callback):\r\n\r\n    @pl.utilities.rank_zero_only\r\n    def on_train_start(self, trainer, pl_module):\r\n        if not restart:\r\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\r\n                if not isdir(aiOutputDir +\"outputs_train\"):\r\n                    os.mkdir(aiOutputDir +\"outputs_train\")\r\n```\r\nMy intention is to create the directory only once, for the main rank/GPU, that's why I use the `rank_zero_only` decorator.\r\nWhen I try it on multiple GPUs (ddp), the code hangs. If I create an empty directory in advance, then it runs! It seems to me like a barrier is needed before creating the dir... If this is the case, how do I add this in lightning? I have also noticed that the print statement is printed for all GPUs, so I am not sure if the decorator works as it should... Should I use os.environ() instead of the decorator, and specify explicitly the global ID rank which will create the directory?\r\n\r\nAny ideas? What is the standard practice here?\r\n\r\nThanks in advance,\r\nNikos\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7956",
    "createdAt": "2021-06-12T14:42:02Z",
    "updatedAt": "2022-11-03T01:27:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nickKyr"
    },
    "answer": {
      "body": "Can you try to remove the decorator and instead do this?\r\n\r\n```python\r\ndef on_train_start(self, trainer, pl_module):\r\n        print(\"global_rank is\", trainer.global_rank)\r\n        if trainer.is_global_zero:\r\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\r\n                if not isdir(aiOutputDir +\"outputs_train\"):\r\n                    os.mkdir(aiOutputDir +\"outputs_train\")\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-12T23:26:56Z"
    }
  },
  {
    "title": "How to remove version number when saving model checkpoint?",
    "body": "Hi. I trained a model several times,\r\nand I found that Pytorch-lightning sometimes appends version, e.g. model-v1.ckpt, and sometimes doesn't append.\r\n(I don't know the reason because the codes of the two situations are the same)\r\n\r\nHere, I want to let PyTorch-lightning do not append any version. Is there any solution?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7953",
    "createdAt": "2021-06-12T09:34:33Z",
    "updatedAt": "2023-10-26T13:33:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Kyeongpil"
    },
    "answer": {
      "body": "Hi, it happens when the checkpoint tries to save the file but a file with that name already exists. It is to prevent accidentally overwriting/deleting a checkpoint. \r\n\r\nYou can prevent that for example by setting the filename to include epoch and step, e.g., \r\n\r\n```python\r\nModelCheckpoint(monitor=..., filename=\"your-filename-{epoch}-{step}\")\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-12T23:30:50Z"
    }
  },
  {
    "title": "ddp replicates whole script",
    "body": "Hi all, I am running a script that performs some calculations and then does a testing loop using multi GPU using a single node with DDP.\r\nThe code looks like this:\r\n\r\n```py\r\nsome prepocesing code\r\n...\r\nmodel = LitModel(path_weights)\r\n\r\ndataloader = DataLoader(dataset, batch_size=512,\r\n                        shuffle=False, num_workers=32)\r\ntrainer = pl.Trainer(accelerator='ddp', gpus=8)\r\ntrainer.test(model, dataloader)\r\n```\r\nThe issue that I have is that it seems the the whole script is being computed several times, I can see that the preprocessing code is being called 8 times. It seems to be the same issue as posted in SO (https://stackoverflow.com/questions/66261729/pytorch-lightning-duplicates-main-script-in-ddp-mode)\r\nAm I missing something in my code?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7950",
    "createdAt": "2021-06-11T22:05:10Z",
    "updatedAt": "2022-08-16T16:07:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rogertrullo"
    },
    "answer": {
      "body": "after reading more carefully I realized that it is what it is supposed to do, I would like to know however, if is there a way to avoid replicating some heavy calculations several times, say I want to do the postprocessing only ones and share the results with all the other processes? i",
      "author": {
        "login": "rogertrullo"
      },
      "createdAt": "2021-06-12T08:13:02Z"
    }
  },
  {
    "title": "TypeError: __init__() got an unexpected keyword argument 'row_log_interval'",
    "body": "**Question:** \r\nI am trying to run the EpicKitchen codes from https://github.com/epic-kitchens/C1-Action-Recognition-TSN-TRN-TSM\r\nI am getting this Typeerror may be related to older and new versions of lightining module and i was not able to resolve it. \r\n**Error:**\r\nTraceback (most recent call last):\r\n  File \"src/test.py\", line 145, in <module>\r\n    main(parser.parse_args())\r\n  File \"src/test.py\", line 139, in main\r\n    trainer = Trainer(**cfg.trainer, callbacks=[saver])\r\n  File \"/home/code-base/.intdoc/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 41, in overwrite_by_env_vars\r\n    return fn(self, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'row_log_interval'\r\n\r\n**Code:**   test.py file \r\n\r\n\r\n```\r\nfrom collections import defaultdict\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport pickle\r\nfrom pathlib import Path\r\nimport colorlog\r\nimport torch\r\nimport numpy as np\r\nfrom omegaconf import OmegaConf\r\nfrom pytorch_lightning import Callback, Trainer\r\nfrom typing import Any, Dict, List, Sequence, Union\r\nfrom systems import EpicActionRecogintionDataModule\r\nfrom systems import EpicActionRecognitionSystem\r\n\r\nparser = argparse.ArgumentParser(\r\n    description=\"Test model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n)\r\nparser.add_argument(\"checkpoint\", type=Path)\r\nparser.add_argument(\"results\", type=Path)\r\nparser.add_argument(\"--split\", choices=[\"val\", \"test\"], default=\"test\")\r\nparser.add_argument(\r\n    \"--n-frames\",\r\n    type=int,\r\n    help=\"Overwrite number of frames to feed model, defaults to the \"\r\n    \"data.test_frame_count or data.frame_count if the former is not present\",\r\n)\r\nparser.add_argument(\r\n    \"--batch-size\",\r\n    type=int,\r\n    help=\"Overwrite the batch size for loading data, defaults to learning.batch_size\",\r\n)\r\nparser.add_argument(\r\n    \"--datadir\",\r\n    default=None,\r\n    help=\"Overwrite data directory in checkpoint. Useful when testing a checkpoint \"\r\n    \"trained on a different machine.\",\r\n)\r\n\r\nLOG = logging.getLogger(\"test\")\r\nclass ResultsSaver(Callback):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.results: Dict[str, Dict[str, List[Any]]] = dict()\r\n    def on_test_batch_end(\r\n        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\r\n    ):\r\n        self._store_batch_results(\"test\", outputs)\r\n\r\n    def _store_batch_results(\r\n        self, dataset_name: str, batch_outputs: Dict[str, Sequence[Any]]\r\n    ):\r\n        if dataset_name not in self.results:\r\n            self.results[dataset_name] = {k: [] for k in batch_outputs.keys()}\r\n\r\n        for k, vs in batch_outputs.items():\r\n            if isinstance(vs, torch.Tensor):\r\n                vs = vs.detach().cpu().numpy()\r\n            self.results[dataset_name][k].extend(vs)\r\n\r\n    def save_results(self, dataset_name: str, filepath: Union[str, Path]):\r\n        filepath = Path(filepath)\r\n        filepath.parent.mkdir(parents=True, exist_ok=True)\r\n        results_dict = self.results[dataset_name]\r\n        new_results_dict = {\r\n            k: np.stack(vs)\r\n            for k, vs in results_dict.items()\r\n        }\r\n\r\n        with open(filepath, \"wb\") as f:\r\n            pickle.dump(new_results_dict, f)\r\n\r\n\r\ndef main(args):\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    handler = colorlog.StreamHandler()\r\n    handler.setFormatter(\r\n        colorlog.ColoredFormatter(\"%(log_color)s%(levelname)s:%(name)s:%(message)s\")\r\n    )\r\n\r\n    logger = colorlog.getLogger(\"example\")\r\n    logger.addHandler(handler)\r\n\r\n    ckpt = torch.load(args.checkpoint, map_location=lambda storage, loc: storage)\r\n    # Publicly released checkpoints use dicts for longevity, so we need to wrap them\r\n    # up in an OmegaConf object as this is what EpicActionRecognitionSystem expects.\r\n    cfg = OmegaConf.create(ckpt[\"hyper_parameters\"])\r\n    OmegaConf.set_struct(cfg, False)  # allow writing arbitrary keys without raising\r\n    # exceptions\r\n    cfg.data._root_gulp_dir = os.getcwd()  # set default root gulp dir to prevent\r\n    # exceptions on instantiating the EpicActionRecognitionSystem\r\n\r\n    system = EpicActionRecognitionSystem(cfg)\r\n    system.load_state_dict(ckpt[\"state_dict\"])\r\n    if not cfg.get(\"log_graph\", True):\r\n        # MTRN can't be traced due to the model stochasticity so causes a JIT tracer\r\n        # error, we allow you to prevent the tracer from running to log the graph when\r\n        # the summary writer is created\r\n        try:\r\n            delattr(system, \"example_input_array\")\r\n        except AttributeError:\r\n            pass\r\n\r\n    if args.n_frames is not None:\r\n        cfg.data.test_frame_count = args.n_frames\r\n    if args.batch_size is not None:\r\n        cfg.learning.batch_size = args.batch_size\r\n    if args.datadir is not None:\r\n        data_dir_key = f\"{args.split}_gulp_dir\"\r\n        cfg.data[data_dir_key] = args.datadir\r\n\r\n    # Since we don't support writing results when using DP or DDP\r\n    LOG.info(\"Disabling DP/DDP\")\r\n    cfg.trainer.accelerator = None\r\n\r\n    n_gpus = 1\r\n    LOG.info(f\"Overwriting number of GPUs to {n_gpus}\")\r\n    cfg.trainer.gpus = n_gpus\r\n    cfg[\"test.results_path\"] = str(args.results)\r\n\r\n    data_module = EpicActionRecogintionDataModule(cfg)\r\n    if args.split == \"val\":\r\n        dataloader = data_module.val_dataloader()\r\n    elif args.split == \"test\":\r\n        dataloader = data_module.test_dataloader()\r\n    else:\r\n        raise ValueError(\r\n            f\"Split {args.split!r} is not a recognised dataset split to \" f\"test on.\"\r\n        )\r\n\r\n    saver = ResultsSaver()\r\n    trainer = Trainer(**cfg.trainer, callbacks=[saver])\r\n    trainer.test(system, test_dataloaders=dataloader)\r\n    saver.save_results(\"test\", args.results)\r\nif __name__ == \"__main__\":\r\n    main(parser.parse_args())\r\n\r\n```\r\n**Versions:** \r\nPython3.6, lightining-1.1.8, pytorch-1.7.1, Running on an linux machine \r\n\r\nI am new to this library, any help would be appreciated\r\nThanks in advance",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7943",
    "createdAt": "2021-06-11T12:55:03Z",
    "updatedAt": "2022-06-01T00:54:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jeevana28"
    },
    "answer": {
      "body": "> TypeError: init() got an unexpected keyword argument 'row_log_interval'\r\n\r\n`row_log_interval` got deprecated and removed. You must have taken this from an old version of the docs. Use [`log_every_n_steps`](https://pytorch-lightning.readthedocs.io/en/1.3.5/common/trainer.html#log-every-n-steps) instead. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-12T23:56:03Z"
    }
  },
  {
    "title": "Global parameters yaml file",
    "body": "I am using CLI to train my model. Instead of specifying parameters directly, I provide a yaml file with variables defined.\r\n\r\nSince I'm using several loggers, they have a common name parameter. So in order to start a new experiment I have to change this parameter in each logger. This raises a question is there a way to create global variable in yaml file while using CLI?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7940",
    "createdAt": "2021-06-11T10:41:19Z",
    "updatedAt": "2022-07-16T18:49:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Serega6678"
    },
    "answer": {
      "body": "@Serega6678 You can try jsonnet format config file which supports global variables. https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.core.ArgumentParser\r\nTo use jsonnet format, you can init CLI by passing `{\"parser_mode\": \"jsonnet\"}` to `parser_kwargs`.",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-06-11T11:04:53Z"
    }
  },
  {
    "title": "Is there an all_gather before training_step_end when using DDP?",
    "body": "From the [training_step_end()](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step-end) docs it says:\r\n\r\n> If you later switch to ddp or some other mode, this will still be called so that you don\u2019t have to change your code\r\n\r\nWhen using dp or ddp2 the first dimension is equal to the number of GPUs, and it has the per-GPU results like `gpu_n_pred = training_step_outputs[n]['pred']` as shown in the example in the docs. This makes sense since there is a gather in the forward pass. For DDP though there does not need to be a gather / sync / barrier in the forward pass, only for the gradients in the backward pass.\r\n\r\nSo does this just pass through the single-GPU output in the DDP case? E.g. in the same example, is `training_step_outputs` just the dictionary from that single GPU, like `gpu_pred = training_step_outputs['pred']`? \r\n\r\nOr if I define this method does it add a barrier / gather as if doing `outputs = self.all_gather(outputs)`, such that all of the GPU results are actually available like `gpu_n_pred = training_step_outputs[n]['pred']` as in the DP/DDP2 case? \r\n\r\nI just want to make sure I'm not slowing down my code if I define this method for the dp / ddp2 case but then almost always use standard ddp. Sorry if this was already asked or is in the docs, I tried my best to find the answer. Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7934",
    "createdAt": "2021-06-11T02:09:26Z",
    "updatedAt": "2022-08-15T14:30:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "collinmccarthy"
    },
    "answer": {
      "body": "I'll answer my own question. Short answer is **no**, there is no barrier / gather before `training_step_end()` when using DDP.\r\n\r\nI could be wrong, but it appears these methods just get called using the normal callback mechanism, e.g. PyTorch-Lightning doesn't post-process the output beyond what DP / DDP will do. So in the DP case the outputs are automatically aggregated by concatenating the first dimension, and in the DDP case the outputs are just passed through.\r\n\r\nI tried returning a dictionary from `training_step_end()` containing (1) a scalar, e.g. `loss`, and (2) a tensor of output predictions e.g. `pred_outs`, shape (N, K) for batch size N and number of classes K.\r\n\r\nThe results were as follows, using 2 GPUs for DP / DDP with a batch size of 128 for DP, and 64 for DDP (maintaining the effective batch size of 128):\r\n\r\nRough setup:\r\n\r\n```python\r\ndef training_step(\r\n    self,\r\n    batch: Tuple[torch.Tensor, torch.Tensor],\r\n    batch_idx: int\r\n) -> Dict[str, torch.Tensor]:\r\n    inputs, targets = batch\r\n    pred_odds = self.forward(inputs)\r\n    log_probs = F.log_softmax(pred_odds, dim=1)\r\n    loss = F.nll_loss(log_probs, targets)\r\n    return {'pred_odds': pred_odds, 'loss': loss}\r\n\r\ndef training_step_end(\r\n    self,\r\n    step_outputs: Dict[str, torch.Tensor]\r\n) -> Dict[str, torch.Tensor]:\r\n    print(step_outputs['loss'])\r\n    print(step_outputs['loss'].shape)\r\n\r\n    print(step_outputs['pred_odds'].shape)\r\n    print(step_outputs['pred_odds'].device)\r\n```\r\n\r\nGave the following results:\r\n\r\n```text\r\n---------------------------\r\nSINGLE GPU\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8134], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([1])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n---------------------------\r\nDP\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8472, 4.8262], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([2])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n-----------------------------\r\nDDP\r\n-----------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor(4.8477, device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([])\r\n\r\nTENSOR_OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([64, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n```\r\n\r\nSo if you're using a tensor (the only case that I actually needed, since the loss will be computed in `training_step_end()` anyway), you can just use the tensor as you normally would in `training_step_end()` as if it were coming from a single GPU in `training_step()`. \r\n\r\nIf you're using a scalar, it will get converted into a 1D tensor equal to the number of GPUs in the no-backend and DP case, but it will be still be a scalar tensor in the DDP case (see the size above). That could throw you off, but again, you're probably not passing scalars to `training_step_end()` anyway. \r\n\r\nHope this helps someone. Cheers.",
      "author": {
        "login": "collinmccarthy"
      },
      "createdAt": "2021-06-12T23:35:45Z"
    }
  },
  {
    "title": "Training slows down significantly for small dataset sizes",
    "body": "## \ud83d\udc1b Bug\r\n\r\nWhen the dataset size is small (i.e. comparable to the minibatch size), it slows training down significantly.\r\n\r\nNo GPU, batch size 64, dataset size 1024: 185 iterations/second\r\nNo GPU, batch size 64, dataset size 100: 47 iterations/second\r\n\r\n1 GPU, batch size 64, dataset size 1024: 110 iterations/second\r\n1 GPU, batch size 64, dataset size 100: 23 iterations/second\r\n\r\n1 GPU, batch size 800, dataset size 1024: 19 iterations/second\r\n1 GPU, batch size 800, dataset size 10000: 90 iterations/second\r\n1 GPU, batch size 64, dataset size 10000: 235 iterations/second\r\n\r\n\r\n\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n```python\r\nimport os, sys\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset, DistributedSampler, DataLoader\r\n\r\nfrom pl_examples import cli_lightning_logo\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.utilities.seed import seed_everything\r\nfrom pytorch_lightning.callbacks.progress import ProgressBar, ProgressBarBase, tqdm, reset, convert_inf\r\n\r\nclass CustomProgressBar(ProgressBar):\r\n    def init_train_tqdm(self) -> tqdm:\r\n        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\r\n        bar = tqdm(\r\n            desc='Training',\r\n            initial=self.trainer.global_step,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n        )\r\n        return bar\r\n    def on_train_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_start(trainer, pl_module)\r\n        self.main_progress_bar = self.init_train_tqdm()\r\n        self.prev_train_gs = -1\r\n        reset(self.main_progress_bar, self.trainer.max_steps)\r\n\r\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\r\n        super(ProgressBar, self).on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\r\n        if self.prev_train_gs != self.trainer.global_step and self._should_update(self.trainer.global_step, self.trainer.max_steps):\r\n            self._update_bar(self.main_progress_bar)\r\n            self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\r\n            self.prev_train_gs = self.trainer.global_step\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_epoch_start(trainer, pl_module)\r\n\r\n    def on_train_end(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_end(trainer, pl_module)\r\n\r\nclass RandomDataset(Dataset):\r\n    \"\"\"\r\n    >>> RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS\r\n    <...bug_report_model.RandomDataset object at ...>\r\n    \"\"\"\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    \"\"\"\r\n    >>> BoringModel()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    BoringModel(\r\n      (layer): Linear(...)\r\n    )\r\n    \"\"\"\r\n\r\n    def __init__(self, train_data, test_data, bs):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer1 = torch.nn.Linear(32, 32)\r\n        self.layer2 = torch.nn.Linear(32, 32)\r\n        self.layer3 = torch.nn.Linear(32, 2)\r\n\r\n        self.train_data = train_data\r\n        self.test_data = test_data\r\n        self.bs = bs\r\n\r\n    def forward(self, x):\r\n        return self.layer3(torch.relu(self.layer2(torch.relu(self.layer1(x)))))\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.forward(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(list(self.layer1.parameters()) + list(self.layer2.parameters()) + list(self.layer3.parameters()), lr=0.001)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(self.train_data, shuffle=True, num_workers=1, batch_size=self.bs)\r\n        return train_loader\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument(\"--gpus\", type=int, default=0)\r\nparser.add_argument(\"--num_processes\", type=int, default=1)\r\nparser.add_argument(\"--dataset_size\", type=int, default=1024)\r\nparser.add_argument(\"--mb_size\", type=int, default=64)\r\nargs = parser.parse_args()\r\n\r\n\r\ndef test_run():\r\n    # data\r\n    train_data = torch.randn(args.dataset_size, 32)\r\n    test_data = torch.randn(256, 32)\r\n\r\n    # model\r\n    model = BoringModel(train_data, test_data, bs=args.mb_size)\r\n    trainer = Trainer(\r\n        gpus=args.gpus,\r\n        logger=False,\r\n        max_steps=5000,\r\n        limit_val_batches=0,\r\n        num_processes=args.num_processes,\r\n        weights_summary=None,\r\n        reload_dataloaders_every_epoch=False,\r\n        callbacks=[CustomProgressBar()]\r\n    )\r\n\r\n    # fit\r\n    trainer.fit(model)\r\n\r\n    print(f\"{trainer.accelerator_backend=}\")\r\n    print(f\"{trainer.gpus=}\")\r\n    print(f\"{trainer.num_processes=}\")\r\n    print(f\"{trainer.global_step=}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_run()\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun the following command: `python bug_report.py --gpus 1 --dataset_size 10000 --mb_size 64`\r\nfor varying values of gpus, dataset_size, and mb_size.\r\n\r\n### Expected behavior\r\n\r\nIterations/second is unaffected by dataset size.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.9\r\n\r\n### Additional context\r\n\r\nMy guess is that this is caused by inter-epoch reloading of the dataset. The code should be restructured to pre-load a fixed number of minibatches ahead, rather than caring about the location of epoch boundaries.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/8113",
    "createdAt": "2021-06-10T22:14:52Z",
    "updatedAt": "2022-06-10T15:24:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jbuckman"
    },
    "answer": {
      "body": "I would say that the case here is that you would need to assume always some overhead and for a small dataset the initial phase is dominant compare to the full run, you can see a parallel with a car riding 100 or 1000 meters, in both cases, you need to start from zero and as long you go you benefit from no need starting again...",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-06-24T08:21:02Z"
    }
  },
  {
    "title": "How to tell Lightning which data to load onto the GPU (for 3rd party compatibility)",
    "body": "Hi ! \r\n\r\nI'm currently working on a segmentation network using PyTorch Lightning and MONAI. \r\n\r\n_Context_\r\n- In my LightningDataModule, I preprocess my images by applying transforms (e.g., to resample them) before feeding them to my DataLoaders. For the transforms, I use a 3rd party framework called MONAI. It stores the context information of all the applied transforms in a structured nested dictionary. \r\n- After predicting the labels of my predict dataset, I would like to invert these transforms (e.g., to recover the original pixel dimensions) in my `predict_step`. \r\n\r\n_Problem_\r\n- MONAI's inverting logic requires the context information to be numpy data or **CPU** tensors (**GPU** tensors are not supported). However, by default, Lightning moves all the data of my batch (including its context info) to GPU. It causes the bug I reproduced in this test example [notebook](https://github.com/dianemarquette/Invertd_demo/blob/master/test_invertd.ipynb) (cf. last cell). \r\n\r\n_My question_\r\n**How can I tell Lightning to only move the images and their labels to GPU and keep the context info on the CPU?**\r\n\r\nFor further details, please refer to the following discussion Project-MONAI/MONAI#2348.\r\nThanks for your help !",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7919",
    "createdAt": "2021-06-10T13:51:43Z",
    "updatedAt": "2022-07-03T02:26:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dianemarquette"
    },
    "answer": {
      "body": "Hi,\r\n\r\nyour `LightningModule` has a hook `def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:` that you can override and which should be a perfect fit for that. Just make sure to only use the provided device.",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-06-10T17:44:23Z"
    }
  },
  {
    "title": "How to customize the version or name of log file",
    "body": "`v_num` in progress bar means the version number of this running, and the log file's save directory is `version_{v_num}`.\r\n\r\nHow can i customize the directory's name, such as `version_GAT`, `version_GCN`.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7897",
    "createdAt": "2021-06-09T08:12:24Z",
    "updatedAt": "2023-02-23T09:38:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Les1ie"
    },
    "answer": {
      "body": "you change it by passing your logger:\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\nlogger = TensorBoardLogger(\"tb_logs\", name=\"my_model\", version=\"GAT\")\r\ntrainer = Trainer(logger=logger)\r\n```",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-06-09T08:31:40Z"
    }
  },
  {
    "title": "Processing in predict_step() requires access to a DataModule attribute",
    "body": "Hi! \r\n\r\nIn my LightningDataModule, I apply preprocessing transforms to my input data before feeding it to my dataloader. In the same datamodule, I also defined the postprocessing transforms to apply after the inference. \r\n\r\n```\r\nself.post_transforms = Compose([\r\n            AsDiscreted(keys=\"pred\", argmax=True, to_onehot=False, n_classes=3),\r\n            Invertd(\r\n                keys=\"pred\",  # invert the `pred` data field, also support multiple fields\r\n                transform=val_transforms,\r\n                loader=val_dataloader,\r\n                orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\r\n                                  # then invert `pred` based on this information. we can use same info\r\n                                  # for multiple fields, also support different orig_keys for different fields\r\n                meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\r\n                orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\r\n                                                 # for example, may need the `affine` to invert `Spacingd` transform,\r\n                                                 # multiple fields can use the same meta data to invert\r\n                meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\r\n                                               # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\r\n                                               # otherwise, no need this arg during inverting\r\n                nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\r\n                to_tensor=True,  # convert to PyTorch Tensor after inverting\r\n            ),\r\n            SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\r\n        ])\r\n```\r\nI want to apply these post_transforms to my inference outputs in `predict_step()`. What would be the best \"Lightning way\" to give access to my datamodule.post_transforms attribute to `predict_step`?\r\n\r\n```\r\ndef predict_step(self, batch: Any, batch_idx: int):\r\n        batch[\"pred\"] = self.forward(batch)\r\n        post_transforms(batch)\r\n```\r\n\r\nThanks in advance for your suggestions :)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7884",
    "createdAt": "2021-06-08T12:58:59Z",
    "updatedAt": "2023-09-02T23:36:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dianemarquette"
    },
    "answer": {
      "body": "Hi, you should be able to access the datamodule inside the LightningModule. \r\nTry \r\n\r\n```python\r\ndef predict_step(self, batch: Any, batch_idx: int):\r\n        batch[\"pred\"] = self(batch)\r\n        self.datamodule.post_transforms(batch)\r\n```\r\nAlso, another tipp: Better use `self()` instead of `self.forward()` (generally in PyTorch).  ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-09T15:07:16Z"
    }
  },
  {
    "title": "predict with multiple GPUs doesn't aggregate the predictions even with on_predict_end or on_predict_batch_end",
    "body": "I train a model with 2 GPUs\uff0c when running ```predictions = trainer.predict(model, datamodule)``` in a script, what I get is not a single variable ```predictions``` as desired, instead, I get two independent variables on two GPUs. I tried to use ```on_predict_end``` and ```on_predict_batch_end``` to aggregate the predictions on different GPUs, but it seems they change the behavior of the method ```trainer.predict```\uff0c i.e. I can't get an aggregated ```predicitons```.  What should I do to aggregate predictions defined in ```predict_step``` when using multiple GPUs?\r\n\r\n---\r\nTo see the issue, simple create a ```LightningModule``` with the ```predict_step``` method \r\n```py\r\nclass Model(pl.LightningModule):\r\n    ...\r\n    def predict_step(self, batch, batch_idx, dataloader_idx):\r\n        y = self(x)\r\n        return {\"predict\":y}\r\n \r\nm = Modle(...)\r\n```\r\nand use any ```pl.DataModule``` with a ```predict_dataloader``` method \r\n```py\r\ntrainer = pl.Trainer(gpus=2, accelerator=ddp)\r\npredictions = trainer.predict(model=m, datamodule=dm)\r\n```\r\nbut we'll get two ```predictions``` on two GPUs(they are actually the outputs of two scripts that ```pytorch_lightning``` creates for us anyway...), and we can't use it as a single object for later processing in the script. \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7852",
    "createdAt": "2021-06-07T03:43:15Z",
    "updatedAt": "2022-06-14T15:27:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Waerden001"
    },
    "answer": {
      "body": "You can either sync them by writing them both to disk (we have a `PredictionWriter` for that) or you can use `self.all_gather` inside your `predict_step` to sync across GPUs (note that this can lead to GPU OOM!)",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-06-10T12:02:46Z"
    }
  },
  {
    "title": "Patience reset in EarlyStopping once loss has improved",
    "body": "Hey, \r\nim wondering whether the patience parameter is or can be reset once we have started improving again. Reading the docs it sounds like the patience parameter is the absolute number of steps the loss is allowed to not decrease. Is it possible to have it such that the patience counter is reset to its original value whenever we have improved? \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7849",
    "createdAt": "2021-06-06T19:15:43Z",
    "updatedAt": "2023-08-04T21:02:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "danielkorth"
    },
    "answer": {
      "body": "This is exactly how patience is supposed to work. As you can see [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/7b531ac7ac86141237f758a7e7446930964b91a9/pytorch_lightning/callbacks/early_stopping.py#L230), the counter resets to 0 upon improvement. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-06T22:17:27Z"
    }
  },
  {
    "title": "Backward twice in one training_step",
    "body": "I have 2 losses for my model.\r\nAnd I need the grads of the first loss to compute the second one.\r\nThe pseudocode in pytorch is like:\r\n```python\r\noptimizer.zero_grad()\r\nhidden_value = model.part1(input)\r\noutput = model.part2(hidden_value)\r\nloss1 = criterion(output, label)\r\nloss1.backward(retain_graph=True)\r\nloss2 = criterion2(hidden_value.grad, label2)\r\nloss2.backward()\r\noptimizer.step()\r\n```\r\nI found an API named manual_backward() which may fit my problem.\r\nHowever, I build this model on a project based on pytorch_lighting 0.6.0, and it doesn\u2019t have this API.\r\nSo, my questions are:\r\n1.How can I implement my operation using pytorch_lightning 0.6.0?\r\n2.If I can\u2019t implement it in pytorch_lightning 0.6.0, which lighting version should I chose? (Please recommend a close version which may cause less error after I update the lightning.)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7845",
    "createdAt": "2021-06-06T08:12:25Z",
    "updatedAt": "2022-08-08T13:04:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "1994cxy"
    },
    "answer": {
      "body": "I don't think this is really possible in 0.6. This version is too old and manual optimization was introduced to cover your exact use case. I can only recommend the latest version because manual backward underwent many changes and bugfixes so believe it is worth it to invest the time to get that code updated. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-06T22:22:34Z"
    }
  },
  {
    "title": "move_metrics_to_cpu does not work",
    "body": "I used torchmetrics.AveragePrecision as the metric in PL module. This metric takes a lot of gpu memory as it save all data in buffer. \r\nSo I wanted to move the metric to cpu and set move_metrics_to_cpu True in PL Trainer. But the metric's buffer was still on gpu. \r\nAny ideas of this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7828",
    "createdAt": "2021-06-04T08:07:54Z",
    "updatedAt": "2022-11-24T17:02:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MeteorsHub"
    },
    "answer": {
      "body": "I think `move_metrics_to_cpu` does not move the state of the metric modules but just the logged values.\r\n\r\ncc @tchaton for confirmation.\r\n\r\n@SkafteNicki @tchaton should we maybe extend this? I see that this can be misleading.\r\n\r\n@MeteorsHub The issue we do it that way is mainly to avoid having the same thing in memory twice. But I think especially for metrics like AP the only option would be to manually move it to cpu (since this will be moved to gpu together with the model). This will introduce some synchronization points though and might slow down your training!\r\n\r\n",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-06-04T09:11:40Z"
    }
  },
  {
    "title": "UserWarning: cleaning up ddp environment...",
    "body": "When I execute some program in PyTorch-Lightning, it implements very fast. But, at the last part, I got a message as below\r\n\r\n```\r\n/home/mydirectory/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: cleaning up ddp environment...\r\n\r\nwarnings.warn(*args, **kwargs)\r\n```\r\n\r\nAfter this message, all the system stops. I can't see the result that I want stopped by this message.\r\n\r\nCan anyone tell me how to solve it? I'm in Hurry \u3160.\u3160\r\n\r\n(I am using it with `wandb` & `HuggingFace`.)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7820",
    "createdAt": "2021-06-03T15:54:42Z",
    "updatedAt": "2022-06-27T16:47:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tryumanshow"
    },
    "answer": {
      "body": "@data-weirdo mind share some sample code to reproduce? I have been using DDP in some of our examples and all is fine :rabbit: ",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-07-07T22:25:28Z"
    }
  },
  {
    "title": "Loading checkpoint for LightningModule that defines a system",
    "body": "#### Systems\r\nThe [style guide](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html) encourages to use systems, like this one\r\n\r\n```python\r\nclass LitModel(LightningModule):\r\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n```\r\n\r\nI have problems loading the checkpoint for such modules. \r\nBelow example fails. How can I make it work?\r\n\r\n#### Minimal example\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass SystemModel(LightningModule):\r\n\r\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None, multiplier=10):\r\n        super().__init__()\r\n        self.save_hyperparameters('multiplier')\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n        self.multiplier = multiplier\r\n        print(\"type of hparams\", type(self.hparams))\r\n        print(\"class of hparams type\", self.hparams.__class__.__name__)\r\n\r\n    def forward(self, x):\r\n        return self.multiplier * self.decoder(self.encoder(x))\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.forward(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ntrain_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\nval_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\ntest_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n# model\r\nmodel = SystemModel(torch.nn.Linear(32, 16), torch.nn.Linear(16, 2), multiplier=15)\r\ntrainer = Trainer(\r\n    default_root_dir=os.getcwd(),\r\n    limit_train_batches=1,\r\n    limit_val_batches=1,\r\n    max_epochs=1,\r\n    weights_summary=None,\r\n)\r\ntrainer.fit(model, train_data, val_data)\r\n\r\nckpt_path = trainer.checkpoint_callback.best_model_path\r\n\r\n# Try to load\r\n# Loading fails....\r\nmodel = SystemModel.load_from_checkpoint(ckpt_path) # Fails\r\n\r\n# Edit: answer by Adrian\r\n# the correct way to load is \r\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=torch.nn.Linear(32, 16), decoder=torch.nn.Linear(16, 2))  \r\n\r\nprint(\"multiplier:\", model.multiplier)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7818",
    "createdAt": "2021-06-03T14:21:27Z",
    "updatedAt": "2022-06-22T15:00:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sjdh"
    },
    "answer": {
      "body": "When you reload you need to specify the missing arguments for instantiation: \r\n```python\r\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=..., decoder=...) \r\n``` ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-05T21:16:01Z"
    }
  },
  {
    "title": "TypeError: setup() got an unexpected keyword argument 'stage'",
    "body": "Hi,\r\n\r\nWhen calling `train.fit(model)` I get the following TypeError. I got a couple of more similar errors but could find solutions to them (mainly due to the newer version of pl), but I could not find any fix for following error:\r\n\r\n```\r\n  File \"train.py\", line 540, in <module>\r\n    main(args)\r\n  File \"train.py\", line 506, in main\r\n    logger=logger,\r\n  File \"/Structure-Aware-BART/src/lightning_base.py\", line 700, in generic_train\r\n    trainer.fit(model)\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 713, in _run\r\n    self.call_setup_hook(model)  # allow user to setup lightning_module in accelerator environment\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1161, in call_setup_hook\r\n    model.setup(stage=fn)\r\nTypeError: setup() got an unexpected keyword argument 'stage'\r\n```\r\n\r\nAny pointers would be much appreciated. Thx",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7800",
    "createdAt": "2021-06-01T23:00:48Z",
    "updatedAt": "2022-06-04T16:01:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "fabrahman"
    },
    "answer": {
      "body": "Can you share your LightningModule code? Are you overriding the `setup` function? If so, are you overriding it with this signature? https://github.com/PyTorchLightning/pytorch-lightning/blob/03bb389b218084f365b922f2a50133a5aaaadaf0/pytorch_lightning/core/hooks.py#L395",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-06-02T04:32:35Z"
    }
  },
  {
    "title": "find_unused_parameters in the lightning trainer (1.3.2)",
    "body": "## \ud83d\udc1b Bug\r\n\r\nIf there are unsued parameters in the model, should we still explicitly mentioned the trainer as follows?\r\n\r\n**plugins=[DDPPlugin(find_unused_parameters=True)]**\r\n\r\n` trainer = pl.Trainer.from_argparse_args(\r\nargs, weights_summary=None, \r\ncallbacks=[logging_callback],  logger=logger,        \r\nplugins=[DDPPlugin(find_unused_parameters=True)],\r\n**train_params,)`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7796",
    "createdAt": "2021-05-31T23:38:57Z",
    "updatedAt": "2022-06-11T07:48:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "shamanez"
    },
    "answer": {
      "body": "No, it is set to True by default and you want to turn it off unless you need it :) \r\n\r\nReference:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.3.3/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-false",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-01T15:05:17Z"
    }
  },
  {
    "title": "Custom gather method",
    "body": "How do I correctly override `gather` behaviour?\r\nIdeally, I would have a single place where I should specify new method and it would work with `dp`, `ddp` and `ddp_spawn`.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7782",
    "createdAt": "2021-05-31T20:20:23Z",
    "updatedAt": "2023-07-11T18:32:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Rizhiy"
    },
    "answer": {
      "body": "Managed to get this working for `dp` by subclassing `DataParallelPlugin` and passing it to trainer. Although this approach requires individual plugins for each accelerator, it would be nice to have a way to set this for all accelerators at the same time.",
      "author": {
        "login": "Rizhiy"
      },
      "createdAt": "2021-06-02T08:46:09Z"
    }
  },
  {
    "title": "Does LearningRateMonitor work with deepspeed?",
    "body": "I'm trying to run code with PL+Deepspeed. I set my scheduler to be `WarmupDecayLR`, but get a warning `RuntimeWarning: You are using LearningRateMonitor callback with models that have no learning rate schedulers. Please see documentation for configure_optimizers method.`\r\nIn the CometML logging I can't see learning rate logs. Is this the expected behaviour?\r\nThank you! :) ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7777",
    "createdAt": "2021-05-31T07:11:55Z",
    "updatedAt": "2022-11-21T13:35:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yuvalkirstain"
    },
    "answer": {
      "body": "Thanks for reporting! Closing in favor of the linked issue",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-07-23T01:36:35Z"
    }
  },
  {
    "title": "WGANGP not working as expected",
    "body": "Hi, can you please tell me if I am doing something wrong here, especially the manual update for the generator and the critic:\r\n\r\n```python\r\nclass Generator(nn.Module):\r\n    def __init__(self, latent_dim=64, img_shape=None):\r\n        super().__init__()\r\n        self.img_shape = img_shape\r\n        self.init_size = 8 #self.img_shape[1] // 4\r\n\r\n        self.l1 = nn.Sequential(\r\n            nn.Linear(latent_dim, 64*self.init_size**2), nn.LeakyReLU(0.2, inplace=True))\r\n        self.conv_blocks = nn.Sequential(\r\n            nn.BatchNorm2d(64),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(32),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(16),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(8),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=8, out_channels=img_shape[0], kernel_size=3, padding=1),\r\n            nn.Tanh()\r\n        )\r\n    \r\n    def forward(self, z):\r\n        out = self.l1(z)\r\n        out = out.view(out.shape[0], 64, self.init_size, self.init_size)\r\n        img = self.conv_blocks(out)\r\n        return img\r\n\r\nclass Critic(nn.Module):\r\n    def __init__(self, img_shape):\r\n        super().__init__()\r\n        self.disc = nn.Sequential(\r\n            nn.Conv2d(in_channels=img_shape[0], out_channels=16, kernel_size=4, stride=2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(16, 32, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(32),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(64, 128, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        # The height and width of downsampled image\r\n        #\r\n        ds_size = 2 ** 4\r\n\r\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size, 1))\r\n    def forward(self, img):\r\n        out = self.disc(img)\r\n        # import pdb; pdb.set_trace()\r\n        out = out.view(out.shape[0], -1)\r\n        validity = self.adv_layer(out)\r\n        return validity       \r\n\r\nclass WGANGP(pl.LightningModule):\r\n    def __init__(self, latent_dim=128, lr=0.0002, lambda_pen=10, crit_repeats=5):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.latent_dim = latent_dim\r\n        self.lr = lr\r\n        self.lambda_pen = lambda_pen\r\n        self.crit_repeats = crit_repeats\r\n        self.b1 = 0.0\r\n        self.b2 = 0.9\r\n\r\n        ### initializing networks\r\n        img_shape = (1, 100, 100)\r\n        self.generator = Generator(self.latent_dim, img_shape)\r\n        self.critic = Critic(img_shape)\r\n\r\n        # application of weight\r\n        self.generator.apply(self.weights_init)\r\n        self.critic.apply(self.weights_init)\r\n        #\r\n        self.validation_z = torch.randn(10, self.latent_dim)\r\n        self.example_input_array = torch.zeros(10, self.latent_dim)\r\n\r\n        # Important: This property activates manual optimization.\r\n        self.automatic_optimization = False # True - Auto // # False - Manual update\r\n\r\n    def forward(self, z):\r\n        return self.generator(z)\r\n\r\n    ### weight initialization\r\n    def weights_init(self, m):\r\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n        if isinstance(m, nn.BatchNorm2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        if isinstance(m, nn.Linear):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        \r\n    def training_step(self, batch, batch_idx):\r\n        imgs = batch\r\n\r\n        # # sample noise\r\n        # z = torch.randn(imgs.shape[0], self.latent_dim)\r\n        # z = z.type_as(imgs)\r\n\r\n        # optimizers, manual access\r\n        g_opt, c_opt = self.optimizers()\r\n\r\n        # update critic\r\n        mean_iteration_critic_loss = 0\r\n        for _ in range(self.crit_repeats):\r\n            c_opt.zero_grad()\r\n            # sample noise\r\n            z = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\r\n            # fake image\r\n            fake = self(z)\r\n            crit_fake_pred = self.critic(fake.detach())\r\n            crit_real_pred = self.critic(imgs)\r\n            # eps\r\n            epsilon = torch.rand(len(imgs), 1, 1, 1, device=self.device, requires_grad=True)\r\n            # gradient penalty\r\n            gp = self.gradient_penalty(self.critic, imgs, fake, epsilon)\r\n\r\n            # critic loss\r\n            critic_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + self.lambda_pen * gp\r\n\r\n            # Keep track of the average critic loss in this batch\r\n            mean_iteration_critic_loss += critic_loss.item() / crit_repeats\r\n\r\n            # Update gradients\r\n            self.manual_backward(critic_loss)\r\n            # Update optimizer\r\n            c_opt.step()\r\n\r\n        # log critic average loss\r\n        self.log('c_loss_mean', mean_iteration_critic_loss, prog_bar=True)\r\n        \r\n        # update generator\r\n        g_opt.zero_grad()\r\n        # sample new noise\r\n        z_new = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\r\n        # new fake image\r\n        fake_new = self(z_new)\r\n        crit_fake_pred = self.critic(fake_new)\r\n        # generator loss\r\n        gen_loss = -torch.mean(crit_fake_pred)\r\n\r\n        # Update gradients\r\n        self.manual_backward(gen_loss)\r\n        # Update optimizer\r\n        g_opt.step()\r\n\r\n        # log generator average loss\r\n        self.log('g_loss', gen_loss, prog_bar=True)\r\n\r\n    def gradient_penalty(self, crit, real, fake, epsilon):\r\n        # mix/interpolate images\r\n        mixed_images = real * epsilon + fake * (1 - epsilon)\r\n\r\n        # Calculate the critic's scores on the mixed images\r\n        mixed_scores = crit(mixed_images)\r\n\r\n        # Take the gradient of the scores with respect to the images\r\n        gradient = torch.autograd.grad(\r\n            inputs=mixed_images,\r\n            outputs=mixed_scores,\r\n            grad_outputs=torch.ones_like(mixed_scores),\r\n            create_graph=True,\r\n            retain_graph=True,\r\n        )[0]\r\n\r\n        # Flatten the gradients so that each row captures one image\r\n        gradient = gradient.view(len(gradient), -1)\r\n\r\n        # Calculate the magnitude of every row\r\n        gradient_norm = gradient.norm(2, dim=1)\r\n\r\n        # Penalize the mean squared distance of the gradient norms from 1\r\n        gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\r\n\r\n        return gradient_penalty\r\n    \r\n    def configure_optimizers(self):\r\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\r\n        opt_c = torch.optim.Adam(self.critic.parameters(), lr=self.lr, betas=(self.b1, self.b2))\r\n        return opt_g, opt_c\r\n\r\n    def on_epoch_end(self):\r\n        z = self.validation_z.to(self.device)\r\n        # log sampled images\r\n        sample_imgs = self(z)\r\n        grid = torchvision.utils.make_grid(sample_imgs)\r\n        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\r\n\r\n# defining the hyperparameters\r\nn_epochs = 1000\r\nz_dim = 50\r\nbatch_size = 64\r\nlr = 0.0002\r\nc_lambda = 10\r\ncrit_repeats = 5\r\n```\r\n\r\ndesired output\r\ncurrent output\r\n![image](https://user-images.githubusercontent.com/54369128/120057409-9efadc80-bff7-11eb-90f6-5cd421891557.png)\r\n\r\n*SOLVED* \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7754",
    "createdAt": "2021-05-29T03:59:47Z",
    "updatedAt": "2024-01-20T09:04:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ncuxomun"
    },
    "answer": {
      "body": "Solved.",
      "author": {
        "login": "ncuxomun"
      },
      "createdAt": "2021-05-30T00:35:45Z"
    }
  },
  {
    "title": "Error with predict()",
    "body": "I am trying to predict on my test data but it throws an error:\r\nTypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list\r\n\r\nTo help you get context:\r\nI have called a pre-trained model and am trying to predict on the model parameters. The model is resnet18().\r\n\r\nThe `test_loader` is a dataloader that loads images from a test image folder. The folder has labelled folders of the test files.\r\n   \r\n```py\r\n# test dataloader\r\ntest_dataset = torchvision.datasets.ImageFolder(\r\n    'path/to/test_data_labelled_folders',\r\n    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\r\n)\r\n\r\ntestloader = torch.utils.data.DataLoader(\r\n    test_dataset, num_workers=4, batch_size=4)\r\n\r\n# loading previously trained model \r\nmodel = LitClassifier.load_from_checkpoint('path/to/checkpoint.ckpt') #LitClassifier is a Lightningmodule\r\n\r\n# calling predict function\r\ntrainer = pl.Trainer()\r\ntrainer.predict(model=model, dataloaders=[testloader], return_predictions=True)\r\n```\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7747",
    "createdAt": "2021-05-27T22:54:40Z",
    "updatedAt": "2023-04-14T04:19:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Prinjesh"
    },
    "answer": {
      "body": "Did you overwrite the `predict_step`? By default it just feeds the whole batch through `forward` (which with the image folder also includes the label and therefore is a list)\r\n\r\nSo you have two choices: Remove the labels from you predict data or overwrite the predict step to ignore them :)",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-05-28T07:25:41Z"
    }
  },
  {
    "title": "Checkpoints not getting saved",
    "body": "I'm using the below callback \r\n`checkpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        dirpath=other_arguments.output_dir,\r\n        monitor=\"val_loss\",\r\n        save_top_k=other_arguments.save_top_k,\r\n        save_last=other_arguments.save_last,\r\n        mode='min'\r\n    )\r\n\r\n    train_params = dict(\r\n        accumulate_grad_batches=other_arguments.gradient_accumulation_steps,\r\n        gpus=training_arguments.n_gpu,\r\n        deterministic=True,\r\n        max_epochs=other_arguments.num_train_epochs,\r\n        precision=16 if training_arguments.fp_16 else 32,\r\n        amp_level=training_arguments.opt_level,\r\n        gradient_clip_val=training_arguments.max_grad_norm,\r\n        checkpoint_callback=checkpoint_callback,\r\n        fast_dev_run=other_arguments.do_fast_dev_run,\r\n    )`\r\n\r\n\r\nThe output dir is empty which means the checkpoints are not getting saved. There is no error as well",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7745",
    "createdAt": "2021-05-27T22:26:34Z",
    "updatedAt": "2022-06-15T10:04:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nrjvarshney"
    },
    "answer": {
      "body": "Pass `checkpoint_callback` to the `callbacks` argument. `checkpoint_callback` is a boolean only flag to indicate whether checkpointing should be enabled or not.",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-05-27T22:39:48Z"
    }
  },
  {
    "title": "What are all of the differences between `.validate()` and `.test()`?",
    "body": "After `.fit()` concludes, I run first `.validate(ckpt_path=None)` and then `.test(ckpt_path=None)`. I use the exact same code for both of them: `validation_step` is identical to `test_step`, `validation_epoch_end` matches `test_epoch_end`, and `val_dataloader` is identical to `test_dataloader`. My trainer also has `limit_val_batches=1.0`, running the whole dataset. Yet, the two values I get are dramatically different: test error is far lower, typically up to an order of magnitude. What is going on in the test phase that could cause this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7732",
    "createdAt": "2021-05-27T03:14:31Z",
    "updatedAt": "2022-06-14T14:37:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jbuckman"
    },
    "answer": {
      "body": "In the end it appears to have been some sort of seed issue -- I still don't understand quite what was happening, but seed_everything(1) fixed it.",
      "author": {
        "login": "jbuckman"
      },
      "createdAt": "2021-05-28T14:00:59Z"
    }
  },
  {
    "title": "How to design a multi-task architecture?",
    "body": "Hello,\r\n\r\nI'm designing a multi-task architecture for depth estimation and semantic segmentation. Because these are very similar tasks, I can use my existing `nn.Module` to learn both of them. I wrapped the `nn.Module` in a `pl.LightningModule` and got a working segmentation. Now I want to extend the `LightningModule` to be able to do the depth estimation. \r\nI want to reuse as much code as possible to reduce errors. But the validation for both tasks is very different, because I'm using many metrics and extensive W&B logging. I first thought of subclassing the `pl.LightningModule` to a segmentation and a depth estimation module, but I want to be able to use both segmentation and depth in a single `LightningModule` later on, when the network should learn both tasks.\r\nThus I created methods for the specific validation steps and set them to variables in the classes init. These get called from the overwritten methods of the class. \r\nHeres some pseudo-code from my structure:\r\n\r\n```python\r\nclass LitModel(pl.LightningModel):\r\n def __init__(self, model: nn.Module,  hparams):\r\n        super().__init__()\r\n        # default for segmentation and depth\r\n        self.hparams.update(hparams)\r\n        self.model = model\r\n\r\n        # SEGMENTATION STUFF\r\n        if segmentation:\r\n            \r\n            self._batch_target_name = 'label'\r\n\r\n            # VAL STUFF\r\n            self._loss_func = CrossEntropyLoss\r\n            self._val_loss_func = CrossEntropyLoss\r\n            self._confusion_matrix = ConfusionMatrix(num_classes=num_classes, compute_on_step=False)\r\n            self._best_metric = 0\r\n            self._best_metric_name = 'mIoU'\r\n\r\n            self._val_reset_metric_list = [self._val_loss_func, self._confusion_matrix]\r\n\r\n            self._validation_step = self.seg_validation_step\r\n            self._validation_epoch_end = self.seg_validation_epoch_end\r\n\r\n        # DEPTH STUFF\r\n        elif depth:\r\n            \r\n            self._batch_target_name = 'depth'\r\n\r\n            # VAL STUFF\r\n            self._loss_func = L1\r\n            self._val_loss_func = MSE\r\n           \r\n            self._best_metric = 0\r\n            self._best_metric_name = 'RMSE'\r\n\r\n            self._val_reset_metric_list = [self._val_loss_func]\r\n\r\n            self._validation_step = self.dpt_validation_step\r\n            self._validation_epoch_end = self.dpt_validation_epoch_end\r\n\r\n    def forward(self, sample):\r\n        return self.model(sample)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # disassemble batch/samples\r\n        image = batch['image']\r\n        target_scales = [batch[self._batch_target_name]]\r\n\r\n        # predict input images\r\n        pred_scales = self.model(image)\r\n\r\n        # calculate losses\r\n        losses = self._loss_func(pred_scales, target_scales)\r\n        summed_loss = sum(losses)\r\n        self.log('train/loss', summed_loss, on_step=False, on_epoch=True)  # logs mean of all losses during that epoch\r\n        return {'loss': summed_loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # disassemble batch/samples\r\n        image = batch['image']\r\n        gt = batch[self._batch_target_name]\r\n\r\n        # predict input images\r\n        prediction = self.model(image)\r\n\r\n        return self._validation_step(batch, batch_idx, gt, prediction)\r\n\r\n    def seg_validation_step(self, batch, batch_idx, gt, prediction):\r\n        # calculate segmentation validation losses\r\n\r\n    def dpt_validation_step(self, batch, batch_idx, gt, prediction):...\r\n        # calculate depth validation losses\r\n\r\n    def on_validation_epoch_start(self) -> None:\r\n        for f in self._val_reset_metric_list:\r\n            f.reset()\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        metric = self._validation_epoch_end(outputs)\r\n        if self._best_metric < metric:\r\n            self.log(f'eval/best_{self._best_metric_name}', metric)\r\n            self.log('eval/best_epoch', self.current_epoch)\r\n            self._best_metric = metric\r\n\r\n    def seg_validation_epoch_end(self, outputs) -> metric:\r\n        # calculate confusion matrix and mIoU\r\n        return mIoU\r\n\r\n    def dpt_validation_epoch_end(self, outputs) -> metric:\r\n        # calculate RMSE\r\n        return RMSE\r\n\r\n    def configure_optimizers(self):...\r\n```\r\nTo me, this seems as really bad practice and I hope it doesn't hurt you to much seeing this.\r\nWhat would be a better/the best way to achieve this? I'm looking for the best practice.\r\n\r\nI'm coming from TensorFlow and I really really like how clean PyTorch Lightning is. It feels so good to work with it. But what I wrote up there just feels wrong, but it works \ud83d\ude28 ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7729",
    "createdAt": "2021-05-26T22:27:17Z",
    "updatedAt": "2022-07-28T02:13:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "teufelweich"
    },
    "answer": {
      "body": "First of all: I'm glad you are enjoying lightning :)\r\n\r\nComing to your code: It actually doesn't look so bad to me. You're separating functionality for different use cases in different functions (which is perfectly fine). What you could do ( if you really want to) is something like this:\r\n\r\n```python\r\nclass BaseModel(LightningModule):\r\n   ... # implements all the logic to be shared between the models such as the module logic or something like this\r\n\r\nclass SegmentationModel(BaseModel):\r\n    ... # adds all the segmentation-only logic\r\n\r\nclass DepthModel(BaseModel):\r\n    ... # adds all the depth-only logic\r\n\r\nclass CombinedModel(LightningModel):\r\n    def __init__(self, model, hparams):\r\n        if depth:\r\n            model = DepthModel(model, hparams)\r\n        else:\r\n            model = SegmentationModel(model, hparams)\r\n        self.model = model\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return self.model.training_step(*args, **kwargs)\r\n\r\n    # do the same for other methods and hooks\r\n```\r\n\r\nThat way your classes would be a bit more separated and self-contained. That being said, I still think your current approach is perfectly fine\r\n",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-05-28T07:48:11Z"
    }
  },
  {
    "title": "How to put all but some vars to GPU",
    "body": "By default, in Lightning everything that is returned by a dataset is collated by the data loader and shipped to the same device.\r\nHowever, I am frequently in the situation where I have let's say `x, y` which are tensors and something like `y_semantic` which is in principle related to y but of higher data type, say a dictionary with some meta information about augmentations or so.\r\nI don't need 'y_semantic' to be on the GPU. Is there some flag or some method that I can overwrite so that some variables stay on CPU?\r\n\r\nSomething like\r\n```python\r\ndef ship_batch(self, batch):\r\n    batch[0] = batch[0].to(self.device)\r\n    # ...\r\n    batch[2] = batch[2].cpu() # just for illustration here\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7725",
    "createdAt": "2021-05-26T13:35:46Z",
    "updatedAt": "2023-03-03T15:22:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Haydnspass"
    },
    "answer": {
      "body": "Dear @Haydnspass,\r\n\r\nYou have several ways to do this:\r\n\r\n1. Create a custom Data / Batch Object and implement the .to function to move only what is required.\r\n\r\n2. Simpler: Override LightningModule.transfer_batch_to_device hook and add your own logic to move only x, y to the right device.\r\n\r\nBest,\r\nT.C\r\n",
      "author": {
        "login": "tchaton"
      },
      "createdAt": "2021-05-26T14:53:59Z"
    }
  },
  {
    "title": "Is it possible to call dist.all_reduce manually in train_step?",
    "body": "In my code, I would like to synchronize a tensor across all the gpus in `train_step`, which is a temporary variable. Is it allowed to call `torch.distributed.all_reduce` in this case? Or there is a specific function in `pytorch_lightning` that does the job?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7693",
    "createdAt": "2021-05-24T23:57:32Z",
    "updatedAt": "2022-11-23T08:57:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sandylaker"
    },
    "answer": {
      "body": "Hey @sandylaker! You can use `torch.distributed.all_reduce`. There is also within the `LightningModule` this function, however it may be better to expose this within lightning to make it easier to access:\r\n\r\n```python\r\nx = self.trainer.accelerator.training_type_plugin.reduce(x)\r\n```",
      "author": {
        "login": "SeanNaren"
      },
      "createdAt": "2021-05-26T09:57:38Z"
    }
  },
  {
    "title": "save checkpoint issue in nested trainer",
    "body": "Hi all, I use multi-GPU to train the model, this error happens when saving the checkpoint. But using a single GPU to train the model, everything works fine. \r\n\r\n```\r\nTypeError: cannot pickle '_thread.lock' object\r\n```\r\n\r\nI wonder if it is because I nest the trainer inside another model, the codes for a nested trainer is as followed:\r\n\r\n```\r\nself.model.trainer = self.trainer\r\n```\r\nSo many thanks in advance.\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7669",
    "createdAt": "2021-05-24T05:56:01Z",
    "updatedAt": "2022-08-16T16:08:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "eddiecong"
    },
    "answer": {
      "body": "Hi @eddiecong, the trainer is not pickleable and so you're probably correct that it's to do with your nested trainer code. You could override the `on_save_checkpoint` hook and then delete the trainer from the checkpoint as suggested by this answer: https://github.com/PyTorchLightning/pytorch-lightning/issues/3444#issuecomment-797652313\r\n\r\nHope that helps :smiley:",
      "author": {
        "login": "ethanwharris"
      },
      "createdAt": "2021-05-24T07:01:19Z"
    }
  },
  {
    "title": "accumulate_grad_batches and DDP",
    "body": "Hi!\r\nWhen I use no multi-gpu settings and just a single GPU and the following parameters:\r\n```python\r\nbatch_size = 16\r\naccumulate_grad_batches=2\r\n```\r\nmy effective batch size is 32. My question is how `accumulate_grad_batches` and DDP interact.\r\nIf I am using 2 GPUS that are on the same machine and i use the parameters\r\n```\r\nbatch_size = 16\r\naccumulate_grad_batches=2\r\naccelerator='ddp'\r\ngpus=[0,1]\r\n```\r\nis my effective batch size now 64?\r\nThanks for the help!\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7632",
    "createdAt": "2021-05-20T18:42:40Z",
    "updatedAt": "2022-06-28T23:03:19Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "RaivoKoot"
    },
    "answer": {
      "body": "Yes: https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-20T20:25:15Z"
    }
  },
  {
    "title": "Not passing optimizer to manual backward causes downstream error",
    "body": "![image](https://user-images.githubusercontent.com/17240858/118919797-117a0700-b8ea-11eb-90e4-5a9f057697c6.png)\r\nIt seems that if I don't pass in an optimizer to my manual backward, my gradient clipping passes through a None object. I thought passing an optimizer to manual backward was not required. It turns out, passing an optimizer doesn't solve the bug.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7620",
    "createdAt": "2021-05-20T04:36:32Z",
    "updatedAt": "2021-05-20T20:28:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "surya-narayanan"
    },
    "answer": {
      "body": "Can you reproduce it and open an issue about it?\r\n\r\nDiscussions is more for help/questions",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-20T20:27:54Z"
    }
  },
  {
    "title": "Compute Loss After Sharing Tensor Across GPUs",
    "body": "I\u2019m currently attempting to make a Multi-GPU-supported CLIP training script, but am hitting a wall. I need to compute two matrices that are composed of whole batch statistics before I can compute loss. Namely, I need to compute the image and text embeddings of an entire batch. Only then can I compute the sub batch losses.\r\n\r\nHow can I first calculate and share the whole batch matrices across GPUs before computing losses?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7602",
    "createdAt": "2021-05-19T01:19:20Z",
    "updatedAt": "2022-08-15T14:31:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Zasder3"
    },
    "answer": {
      "body": "The LightningModule method `all_gather(Tensor)` solved it all!",
      "author": {
        "login": "Zasder3"
      },
      "createdAt": "2021-05-20T01:45:36Z"
    }
  },
  {
    "title": "what will be the closest replacement for @auto_move_data?",
    "body": "I really liked @auto_move_data as it gave me a nice and easy way to pass (a single) input to a model, without caring about moving data structures to devices, but now it will be removed \ud83d\ude22 \r\nI realize that you want people to use trainer.predict() but this requires a dataloader, right?, which at least for me oftentimes is overly complicated while developing.\r\nAs far as I see the dataloader/collate/transforms also have to be different from the normal ones, since these typically provide batches with inputs and targets, whereas the forward function only takes inputs.\r\nLikely I'm missing something, can you maybe give some hints about this?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7588",
    "createdAt": "2021-05-18T12:07:32Z",
    "updatedAt": "2022-08-05T20:02:49Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "immanuelweber"
    },
    "answer": {
      "body": "After writing this I realized that I can use something like this: \r\n```\r\nmodel = model.eval()\r\ni, _ = next(iter(data.test_dataloader()))\r\nwith th.no_grad():\r\n    outputs = model.forward(model.transfer_batch_to_device(i))\r\n```\r\nBut I'm still curious about the intended future ways of doing this....",
      "author": {
        "login": "immanuelweber"
      },
      "createdAt": "2021-05-18T12:16:53Z"
    }
  },
  {
    "title": "Intel deep learning boost compatibility",
    "body": "Hi,\r\nI was wondering if algorithms implemented with pytorch lightning can take advantage of deep learning boost  hardware: \r\n\r\nhttps://www.intel.com/content/www/us/en/artificial-intelligence/deep-learning-boost.html\r\nhttps://github.com/oneapi-src/oneDNN\r\n\r\nAs far as I know it should be compatible with vanilla pytorch\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7586",
    "createdAt": "2021-05-18T09:45:03Z",
    "updatedAt": "2023-11-14T11:25:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "DonBeo"
    },
    "answer": {
      "body": "If there is a speed-up on vanilla PyTorch ops, there should be one when using Lightning",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-18T12:39:36Z"
    }
  },
  {
    "title": "Error while using custom DistributedSampler",
    "body": "I wanted to set shuffle to False. So, i tried\r\n```\r\nsampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)\r\ndataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\r\n```\r\nI am getting an error \r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\nPlease anyone tell me how to use custom sampler.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7573",
    "createdAt": "2021-05-17T09:48:06Z",
    "updatedAt": "2022-05-31T10:38:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "karthi0804"
    },
    "answer": {
      "body": "Adding on to the existing answer:\r\n\r\n`DataLoader(shuffle, sampler)` are mutually exclusive, i.e., if you set `shuffle=True` you will get a `RandomSampler` and if it is set to False you get a `SequentialSampler`.\r\n\r\nWhen using DDP, Lightning takes your dataloader and replaces it with the following\r\n`DataLoader(sampler=DistributedDampler(shuffle=True), ...)`, however ONLY if the sampler is not already a distributed sampler. If it is, no changes are done. \r\n\r\nSo OP can do the following:\r\n\r\n```python\r\ndef train_dataloader(self):\r\n    return DataLoader(sampler=DistributedSampler(shuffle=False), ...)`\r\n```\r\n    \r\nreturning their own sampler. Note this needs to be done in a place where the distributed group is already initialized, so basically in any hook after (including) `setup()`. OP got this error\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\nBecause they probably did the following:\r\n\r\n```python\r\ndataloader = DataLoader(sampler=DistributedSampler(shuffle=False), ...) # fails here, because distributed not init yet\r\ntrainer = Trainer()\r\ntrainer.fit(dataloader)\r\n```\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-06-17T14:19:42Z"
    }
  },
  {
    "title": "Patience Parameter in Trainer",
    "body": "Hey, \r\nIm wondering why the patience parameter has been removed from the Trainer class.\r\nIm pretty sure that there once was this parameter and I used it a lot. \r\nHas it just been moved to another class, or is there a workaround such that I can still use patience when training my Model?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7557",
    "createdAt": "2021-05-15T05:09:43Z",
    "updatedAt": "2021-05-15T06:19:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "danielkorth"
    },
    "answer": {
      "body": "You can use [EarlyStopping Callback](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html?highlight=early_stopping).",
      "author": {
        "login": "tshu-w"
      },
      "createdAt": "2021-05-15T06:01:45Z"
    }
  },
  {
    "title": "How to customize training loop?",
    "body": "I want to marry lightning and https://pytorch-geometric.readthedocs.io/en/latest/ or in particular https://pytorch-geometric-temporal.readthedocs.io/en/latest/\r\n\r\nWhen following the basic examples on their website such as for the ChickenpoxDatasetLoader() a RecurrentGCN is constructed. For me being a total newbie for lightning it is already pretty clear how t convert that to a regular lightning module - kudos to the easy API so far.\r\n\r\nHowever, it is rather unclear for me how to put the training loop into a lightning compatible trainer:\r\n\r\n```py\r\nfrom tqdm import tqdm\r\nmodel = RecurrentGCN(node_features = 4) # chickenpox\r\nmodel\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\nmodel.train()\r\nfor epoch in tqdm(range(200)):\r\n    cost = 0\r\n    for time, snapshot in enumerate(train_dataset):\r\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\r\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\r\n    cost = cost / (time+1)\r\n    cost.backward()\r\n    optimizer.step()\r\n    optimizer.zero_grad() \r\n```\r\n\r\nWould I need a custom trainer in lightning?\r\nIn particular, I need to be able to handle temporal graphs with snapshots over time i.e. think of a social network of people who can perform a hobby at a certain location (lat, long ) and timestamp. I guess it would be fine to discretize the time to i.e. weekly or monthly slices, but the graph is dynamic.\r\n\r\n```py\r\nimport pandas as pd\r\ndf = pd.DataFrame({'person_1':[], 'person_2':[], 'time':[], 'lat':[], 'long':[], 'hobby':[]})\r\ndisplay(df)\r\n```\r\n\r\nI want to perform link prediction - i.e. recommend new friends based on similar hobbies in similar locations & time ranges.\r\n\r\nWith that being said: in the pytorch-geometric-temporal framework they denote snapshots over time (this is not meant as batches, currently they assume that a snapshot contains all the data in a single batch for that particular span of time).\r\n\r\nHowever, the default trainer does not offer this functionality to iterate over snapshots and to me it is unclear how to include it.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7549",
    "createdAt": "2021-05-14T15:35:25Z",
    "updatedAt": "2022-08-01T22:50:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "geoHeil"
    },
    "answer": {
      "body": "just found https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/lightning_example.py  - seems to be the answer to my question - almost, except it is not covering how to handle the iteration over the temporal snapshots.",
      "author": {
        "login": "geoHeil"
      },
      "createdAt": "2021-05-16T10:17:34Z"
    }
  },
  {
    "title": "How to hold 'validation_step' until training one epoch finished",
    "body": "Hi.\r\nIt seems like `validation_step` runs simultaneously at the end of training.\r\nAs soon as `validation_step` start, the percentage of gpu memory allocated is shooting up and `RuntimeError: CUDA out of memory` occur.\r\nHow can i fix it?\r\nThanks in advance!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7541",
    "createdAt": "2021-05-14T07:16:10Z",
    "updatedAt": "2023-03-29T03:41:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JongbinWoo"
    },
    "answer": {
      "body": "> validation_step runs simultaneously at the end of training.\r\n\r\nValidation is considered part of the fitting procedure but it never runs concurrent to training.\r\n\r\n> As soon as validation_step start, the percentage of gpu memory allocated is shooting up and RuntimeError: CUDA out of memory occur. How can i fix it?\r\n\r\nIt's definitely caused by a bug either on your end or ours. Can you try and reproduce it?\r\n\r\nYou can adapt https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py to do it\r\n\r\n",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-17T20:58:30Z"
    }
  },
  {
    "title": "How to display progress on IterableDataset?",
    "body": "When I use an `IterableDataset`, the `Trainer.fit()` gives something like:\r\n\r\n    Epoch 0: : 21it [02:41,  7.69s/it, loss=0.663, v_num=18]  \r\n\r\nWhich does not display a progress. Even if I specify a validation interval, there is no progress shown. Is there a way I can specify an \"epoch length\", or something like that for `IterableDataset`?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7528",
    "createdAt": "2021-05-13T13:50:44Z",
    "updatedAt": "2022-06-13T23:19:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rongcuid"
    },
    "answer": {
      "body": "Hi\r\nIterableDatasets don't have a lengh defined. They can either be infinite in size or raise StopIteration at an arbitrary point. Hence, there is no notion of epochs and we cannot anticipate when it finishes iterations. \r\n\r\nIf you really need to have epochs, I suggest you try to rewrite your dataset logic into a regular Dataset.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-14T11:44:10Z"
    }
  },
  {
    "title": "Do you need to have more than one accuracy metric for different splits?",
    "body": "Howdy! :cowboy_hat_face: I came across something in the PytorchVideo repository that I wasn't so sure about. They're using 2 accuracy metric classes in their `LightningModule` for train and val. I've been using 1 for everything and its making me wonder if I was wrong for some reason?\r\n\r\nHaven't dove into that code in a while...are the metrics being accumulated separately across the different loops, or are they accumulated together?\r\n\r\n[Here's ](https://github.com/facebookresearch/pytorchvideo/blob/6539ed63a1e48289636c00fcb6aa326749028819/tutorials/video_classification_example/train.py#L61-L71)the snippet in question...\r\n\r\n```python\r\n class VideoClassificationLightningModule(pytorch_lightning.LightningModule): \r\n     def __init__(self, args): \r\n         \"\"\" \r\n         This LightningModule implementation constructs a PyTorchVideo ResNet, \r\n         defines the train and val loss to be trained with (cross_entropy), and \r\n         configures the optimizer. \r\n         \"\"\" \r\n         self.args = args \r\n         super().__init__() \r\n         self.train_accuracy = pytorch_lightning.metrics.Accuracy() \r\n         self.val_accuracy = pytorch_lightning.metrics.Accuracy() \r\n```\r\n\r\nI would normally just have `self.accuracy` and use that in both the train and val steps. Is that incorrect?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7520",
    "createdAt": "2021-05-13T03:15:02Z",
    "updatedAt": "2023-10-13T22:32:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nateraw"
    },
    "answer": {
      "body": "> I would normally just have self.accuracy and use that in both the train and val steps. Is that incorrect?\r\n\r\nI would say this is incorrect. You don't want to share the same metric state across training and validation. Otherwise you're mixing data, which will give misleading results compared to having dedicated train and val metric instances. Additionally, depending on how you log the metric value, the same instance could get reset at the end of the training and validation epoch. if you're running validation more frequently (e.g. using `val_check_interval`) this will be another problem. \r\n\r\ncc @SkafteNicki @Borda @edenafek should we add this to the documentation for metrics in lightning? are there ways we can make this clearer in the API itself? ",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-05-13T04:27:52Z"
    }
  },
  {
    "title": "Shouldn't predictions accumulate on CPU, not GPU?",
    "body": "In https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/predict_loop.py#L111:\r\n\r\n```py\r\n        predictions = self.trainer.accelerator.predict_step(args)\r\n\r\n        if predictions is None:\r\n            self.warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.trainer.call_hook(\"on_predict_batch_end\", predictions, batch, batch_idx, dataloader_idx)\r\n\r\n        if self.should_store_predictions:\r\n            self.predictions[dataloader_idx].append(predictions)\r\n```\r\n\r\nShouldn't `.cpu()` be called on predictions before appending them to `self.predictions` so that predictions accumulate con CPU memory, not GPU memory?\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7485",
    "createdAt": "2021-05-11T15:47:12Z",
    "updatedAt": "2022-06-12T00:18:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jamarju"
    },
    "answer": {
      "body": "This was fixed in https://github.com/PyTorchLightning/pytorch-lightning/pull/9085",
      "author": {
        "login": "EricWiener"
      },
      "createdAt": "2021-10-06T16:19:03Z"
    }
  },
  {
    "title": "trying to train  Efficient net",
    "body": "\r\nBut I keep getting this error. I have no what it's about. A hint will be really helpful. Thanks in advance.\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-147-594f245450d7> in <module>\r\n----> 1 trainer.fit(LitPlant, train_dataloader=train_loader, val_dataloaders=valid_loader)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    447         # ----------------------------\r\n    448         # setup data, etc...\r\n--> 449         self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n    450 \r\n    451         # hook\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in setup_fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    112         # clean hparams\r\n    113         if hasattr(model, \"hparams\"):\r\n--> 114             parsing.clean_namespace(model.hparams)\r\n    115 \r\n    116         # links data to the trainer\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py in clean_namespace(hparams)\r\n     73         hparams_dict = hparams.__dict__\r\n     74 \r\n---> 75     del_attrs = [k for k, v in hparams_dict.items() if not is_picklable(v)]\r\n     76 \r\n     77     for k in del_attrs:\r\n\r\nAttributeError: 'property' object has no attribute 'items'",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7465",
    "createdAt": "2021-05-10T16:19:42Z",
    "updatedAt": "2022-06-21T14:36:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "acharjee07"
    },
    "answer": {
      "body": "What's the type of LitPlant? Are you passing an instance of the class or the class type to trainer.fit() ? ",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-05-10T16:48:24Z"
    }
  },
  {
    "title": "MLFlow logger step vs epoch",
    "body": "I am using [MLFlowLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.MLFlowLogger.html#pytorch_lightning.loggers.MLFlowLogger) to log my experiment into Azure ML. Everything works fine but I noticed that when I ask the logger to store a metric every step (instead of every epoch), the logger does not increase the step number but instead keeps overwriting the current step in the batch. That is, if I have 5 minibatches for each epoch, and I have 10 epochs, the logger will overwrite step 1-5 continuously, instead of logging step 1-50. Something I noticed is that MLFlow/AzureML log metrics according to steps, regardless of whether it is a epoch or not, perhaps this is causing some issues.\r\n\r\nWould you be able to help?\r\n\r\nAn example here (7 steps only for `train_loss` but I ran more than 70 epochs)\r\n![image](https://user-images.githubusercontent.com/18400141/117634100-e6bbe000-b17e-11eb-991f-5985e58278b7.png)\r\n![image](https://user-images.githubusercontent.com/18400141/117635314-fee02f00-b17f-11eb-9ce5-703b36debeb0.png)\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7463",
    "createdAt": "2021-05-10T09:08:50Z",
    "updatedAt": "2022-12-01T17:50:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "JackCaster"
    },
    "answer": {
      "body": "I got it. By default the logger logs every 50 steps (https://pytorch-lightning.readthedocs.io/en/1.3.0/extensions/logging.html#control-logging-frequency). It so happened that my dataset was very small so there were way more epochs than training steps. If I increase the frequency `trainer = Trainer(log_every_n_steps=1)` then I do get a log for every steps in the training process. \r\n![image](https://user-images.githubusercontent.com/18400141/117647064-cbf06800-b18c-11eb-9ac8-354eb5d285db.png)\r\n\r\n",
      "author": {
        "login": "JackCaster"
      },
      "createdAt": "2021-05-10T10:40:09Z"
    }
  },
  {
    "title": "Select GPU from cli",
    "body": "The CLI has a flag --gpus\r\n\r\nIn a system with more than 1 GPU, is there a way to select the GPU you want to run on from CLI?\r\n\r\nI tried --gpus [1] to select cuda:1 but it doesn't work.\r\n\r\nAlso the auto gpu selection didn't work for me.  It tried to put the job on cuda:0, but cuda:0 didn't have enough memory to run it.\r\n\r\nin the end I resorted to CUDA_VISIBLE_DEVICES, but that seems silly.\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7461",
    "createdAt": "2021-05-10T02:54:58Z",
    "updatedAt": "2022-07-18T15:46:59Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "DuaneNielsen"
    },
    "answer": {
      "body": "I'm assuming you are referring to the `LightningCLI`\r\n\r\nIn that case, just do `python yourscript.py --trainer.gpus=[1]`",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-11T10:39:06Z"
    }
  },
  {
    "title": "\"Error: 'MyDataModule' object is not iterable\" when calling trainer.predict(model, data)",
    "body": "Hi! \r\n\r\nI've trained a model successfully. I now want to have a look at the model predictions. I've overridden the `predict_dataloader` method in my DataModule:\r\n```\r\ndef predict_dataloader(self):\r\n        pred_loader = torch.utils.data.DataLoader(\r\n            self.val_ds, batch_size=1, num_workers=4)\r\n        return pred_loader\r\n```\r\n\r\nThen, I initialize the model using my checkpoint and call the `predict` method:\r\n```\r\ncheckpoint_dir = os.path.join(root_dir, \"logs2/epoch=199-val_loss=0.26-val_dice=1.67.ckpt\")\r\n\r\n# initialize the data module \r\ndata = KeriDataModule(data_dir=data_dir, pix_dim=(0.6, 0.6, 0.937))\r\n\r\n# initialize the LightningModule (from checkpoint)\r\nnet = Net.load_from_checkpoint(checkpoint_path=checkpoint_dir)\r\n\r\n# initialize Lightning's trainer\r\ntrainer = pl.Trainer(gpus=[0])\r\n\r\nresults = trainer.predict(net, data)\r\n```\r\nUnfortunately, I get the following error\r\n```\r\nPredicting: 0it [43:22, ?it/s]\r\n\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n\r\n\r\nPredicting: 0it [00:00, ?it/s]\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-4bd0b31315bd> in <module>\r\n     10 trainer = pl.Trainer(gpus=[0])\r\n     11 \r\n---> 12 results = trainer.predict(net, data)\r\n     13 print(results)\r\n     14 \r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions)\r\n    629         self.data_connector.attach_data(model, predict_dataloaders=dataloaders, datamodule=datamodule)\r\n    630 \r\n--> 631         results = self._run(model)\r\n    632 \r\n    633         assert self.state.stopped\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    754 \r\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 756         self.dispatch()\r\n    757 \r\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    793             self.accelerator.start_evaluating(self)\r\n    794         elif self.predicting:\r\n--> 795             self.accelerator.start_predicting(self)\r\n    796         else:\r\n    797             self.accelerator.start_training(self)\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_predicting(self, trainer)\r\n    100 \r\n    101     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n--> 102         self.training_type_plugin.start_predicting(trainer)\r\n    103 \r\n    104     def pre_dispatch(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_predicting(self, trainer)\r\n    150     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n    151         # double dispatch to initiate the predicting loop\r\n--> 152         self._results = trainer.run_stage()\r\n    153 \r\n    154     def training_step(self, *args, **kwargs):\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    804             return self.run_evaluate()\r\n    805         if self.predicting:\r\n--> 806             return self.run_predict()\r\n    807         return self.run_train()\r\n    808 \r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_predict(self)\r\n   1071             dataloader = self.accelerator.process_dataloader(dataloader)\r\n   1072             dl_max_batches = self.predict_loop.max_batches[dataloader_idx]\r\n-> 1073             for batch_idx, batch in enumerate(dataloader):\r\n   1074                 if batch is None:\r\n   1075                     continue\r\n\r\nTypeError: 'KeriDataModule' object is not iterable\r\n```\r\n\r\nI don't understand what I messed up ^^'. Any help / tip would be greatly appreciated :)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7425",
    "createdAt": "2021-05-07T09:58:18Z",
    "updatedAt": "2022-09-09T21:07:28Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dianemarquette"
    },
    "answer": {
      "body": "Sorry! We did not add support for `trainer.predict(model, datamodule)`. We'll do it asap!\r\n\r\nYou need to do `trainer.predict(model, datamodule=datamodule)`",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-07T13:06:49Z"
    }
  },
  {
    "title": "How to get the perfect reproducibility",
    "body": "Hi, I'm currently trying to finetune a pretrained BERT model for intent classification using Huggingface's Transformers library and Pytorch Lightning.\r\nThe structure is simple where a linear classifier is simply put on the BERT encoder.\r\nI want to get the same result at the same seed setting, but although the whole setting including the seed is identical, the result changes.\r\nI thought if I pre-fix the seed using `seed_everything` and set the flag `workers=True`, I can get the exact same result, but I don't know what the problem is.\r\nThe fun fact is that all executions save the exact same best checkpoint with identical valid accuracy and actually the flow of the training seems also the same.\r\nBut after executing the test, the results are not same.\r\n\r\nThe main codes are as follows.\r\n```python\r\nfrom transformers import BertConfig, BertTokenizer, BertModel\r\nfrom pytorch_lightning import Trainer, seed_everything\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\ndef run(args):\r\n    # For directory setting\r\n    ...\r\n\r\n    # Tokenizer & Model => This model is a pre-trained encoder, so I think there is no need to fix a random seed.\r\n    config = BertConfig.from_pretrained('bert-base-uncased')\r\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n    model = BertModel.from_pretrained('bert-base-uncased')\r\n\r\n    ...\r\n    \r\n    print(\"Loading datasets...\")\r\n    # For data loading\r\n    train_set = Dataset(...)\r\n    valid_set = Dataset(...)\r\n    test_set = Dataset(...)\r\n\r\n    total_train_steps = int(len(train_set) / batch_size * num_epochs)\r\n    warmup_steps = int(total_train_steps * warmup_prop)\r\n    \r\n    # Random seed fixing for intent classification layer\r\n    seed_everything(0, workers=True)\r\n    \r\n    # Lightning Module setting  \r\n    module = TrainModule(model)\r\n\r\n    # Dataloaders\r\n    ppd = PadCollate(...)\r\n    \r\n    # Reset random seed for data shuffle\r\n    seed_everything(0, workers=True)\r\n\r\n    train_loader = DataLoader(train_set, collate_fn=ppd.pad_collate, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\r\n    valid_loader = DataLoader(valid_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\r\n    test_loader = DataLoader(test_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\r\n\r\n    print(\"Setting pytorch lightning callback & trainer...\")\r\n    # Model checkpoint callback\r\n    filename = \"best_ckpt_{epoch}_{train_all_acc:.4f}_{valid_all_acc:.4f}\"\r\n    monitor = \"valid_all_acc\"\r\n    \r\n    checkpoint_callback = ModelCheckpoint(\r\n        dirpath=save_dir,\r\n        filename=filename,\r\n        verbose=True,\r\n        monitor=monitor,\r\n        mode='max',\r\n        every_n_val_epochs=1,\r\n    )\r\n    \r\n    # Trainer setting\r\n    trainer = Trainer(\r\n        check_val_every_n_epoch=1,\r\n        gpus=gpu,\r\n        auto_select_gpus=True,\r\n        num_nodes=num_nodes,\r\n        max_epochs=num_epochs,\r\n        gradient_clip_val=max_grad_norm,\r\n        num_sanity_val_steps=0,\r\n        deterministic=True,\r\n        callbacks=[checkpoint_callback]\r\n    )\r\n    \r\n    print(\"Train starts.\")\r\n    trainer.fit(model=module, train_dataloader=train_loader, val_dataloaders=valid_loader)\r\n    print(\"Training done.\")\r\n    \r\n    print(\"Test starts.\")\r\n    trainer.test(model=module, test_dataloaders=test_loader, ckpt_path='best')\r\n    \r\n    print(\"GOOD BYE.\")\r\n```\r\n\r\nAlso, `TrainModule` is designed like below.\r\n```python\r\nfrom torch import nn as nn\r\nimport pytorch_lightning as pl\r\n\r\nclass TrainModule(pl.LightningModule):\r\n    def __init__(self, args, encoder):\r\n        super().__init__()\r\n        \r\n        self.args = args\r\n        self.save_hyperparameters(args)\r\n        \r\n        self.encoder = encoder\r\n        self.output_layer = IntentDetection(args)\r\n        self.output_layer.init_params()\r\n        \r\n        self.loss_func = nn.CrossEntropyLoss()\r\n        \r\n    def forward(self, input_ids, padding_masks=None):  # input_ids: (B, L), padding_masks: (B, L)\r\n        hidden_states = self.encoder(input_ids=input_ids, attention_mask=padding_masks)[0]  # (B, L, d_h)\r\n            \r\n        return self.output_layer(hidden_states[:, 0])  # (B, L, C) or  (B, C)\r\n    \r\n    def training_step(self, batch, batch_idx): \r\n        ...\r\n\r\nclass IntentDetection(nn.Module):\r\n    def __init__(self, args):\r\n        super(IntentDetection, self).__init__()\r\n        \r\n        self.hidden_size = args.hidden_size\r\n        self.num_classes = args.num_classes\r\n        \r\n        self.linear = nn.Linear(self.hidden_size, self.num_classes)\r\n        \r\n    def forward(self, hiddens):\r\n        # hiddens: (B, d_h)\r\n        \r\n        return self.linear(hiddens)  # (B, C)\r\n    \r\n    def init_params(self):\r\n        nn.init.xavier_uniform_(self.linear.weight)  \r\n```\r\n\r\nI also post the current training environment.\r\n- OS: Ubuntu 18.04.5 LTS\r\n- Python version: 3.8.5\r\n- Pytorch version: 1.7.1+cu110\r\n- Pytorch Lightning version: 1.3.0\r\n- GPU: A100-SXM4-40GB (DGX)\r\n- CUDA version: 11.0\r\n\r\nAnd I got 3 different results when I run the same codes 3 times.\r\n![image](https://user-images.githubusercontent.com/16731987/117425613-e4c70680-af5d-11eb-90c8-cb3dd990d6bc.png)\r\n\r\nThis is odd, since when I run the exact same code above in different environment, I could get the identical results from a same seed, even the number of workers in a dataloader is different.\r\nI also attach the environment which I was able to get the perfect reproducibility.\r\n- OS: CentOs Linux release 7.9.2009 (Core)\r\n- Python version: 3.7.4\r\n- Pytorch version: 1.7.1+cu110\r\n- Pytorch Lightning version: 1.3.0\r\n- GPU: RTX 3090\r\n- CUDA version: 11.2\r\n\r\nIt will be really great if anyone can help me to solve this issue...\r\nThank you very much.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7423",
    "createdAt": "2021-05-07T08:40:11Z",
    "updatedAt": "2022-06-01T11:25:00Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "devjwsong"
    },
    "answer": {
      "body": "Can you try seeding again right before the `trainer.test` call?\r\n\r\nNot saying you should need to but to know if that makes any difference\r\n\r\nI think your best bet is to try to create a reproducible snippet. You can get started with the https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-07T13:00:30Z"
    }
  },
  {
    "title": "Lightning CLI, PyTorch Profiler, Improved Early Stopping",
    "body": "## [1.3.0] - 2021-05-06\r\n\r\n### Added\r\n\r\n- Added support for the `EarlyStopping` callback to run at the end of the training epoch (#6944)\r\n- Added synchronization points before and after `setup` hooks are run (#7202)\r\n- Added a `teardown` hook to `ClusterEnvironment` (#6942)\r\n- Added utils for metrics to scalar conversions (#7180)\r\n- Added utils for NaN/Inf detection for gradients and parameters (#6834)\r\n- Added more explicit exception message when trying to execute `trainer.test()` or `trainer.validate()` with `fast_dev_run=True` (#6667)\r\n- Added `LightningCLI` class to provide simple reproducibility with minimum boilerplate training CLI (#4492, #6862, #7156, #7299)\r\n- Added `gradient_clip_algorithm` argument to Trainer for gradient clipping by value (#6123).\r\n- Added a way to print to terminal without breaking up the progress bar (#5470)\r\n- Added support to checkpoint after training steps in `ModelCheckpoint` callback (#6146)\r\n- Added `TrainerStatus.{INITIALIZING,RUNNING,FINISHED,INTERRUPTED}` (#7173)\r\n- Added `Trainer.validate()` method to perform one evaluation epoch over the validation set (#4948)\r\n- Added `LightningEnvironment` for Lightning-specific DDP (#5915)\r\n- Added `teardown()` hook to LightningDataModule (#4673)\r\n- Added `auto_insert_metric_name` parameter to `ModelCheckpoint` (#6277)\r\n- Added arg to `self.log` that enables users to give custom names when dealing with multiple dataloaders (#6274)\r\n- Added `teardown` method to `BaseProfiler` to enable subclasses defining post-profiling steps outside of `__del__` (#6370)\r\n- Added `setup` method to `BaseProfiler` to enable subclasses defining pre-profiling steps for every process (#6633)\r\n- Added no return warning to predict (#6139)\r\n- Added `Trainer.predict` config validation (#6543)\r\n- Added `AbstractProfiler` interface (#6621)\r\n- Added support for including module names for forward in the autograd trace of `PyTorchProfiler` (#6349)\r\n- Added support for the PyTorch 1.8.1 autograd profiler (#6618)\r\n- Added `outputs` parameter to callback's `on_validation_epoch_end` & `on_test_epoch_end` hooks (#6120)\r\n- Added `configure_sharded_model` hook (#6679)\r\n- Added support for `precision=64`, enabling training with double precision (#6595)\r\n- Added support for DDP communication hooks (#6736)\r\n- Added `artifact_location` argument to `MLFlowLogger` which will be passed to the `MlflowClient.create_experiment` call (#6677)\r\n- Added `model` parameter to precision plugins' `clip_gradients` signature (#6764, #7231)\r\n- Added `is_last_batch` attribute to `Trainer` (#6825)\r\n- Added `LightningModule.lr_schedulers()` for manual optimization  (#6567)\r\n- Added `MpModelWrapper` in TPU Spawn (#7045)\r\n- Added `max_time` Trainer argument to limit training time (#6823)\r\n- Added `on_predict_{batch,epoch}_{start,end}` hooks (#7141)\r\n- Added new `EarlyStopping` parameters `stopping_threshold` and `divergence_threshold` (#6868)\r\n- Added `debug` flag to TPU Training Plugins (PT_XLA_DEBUG) (#7219)\r\n- Added new `UnrepeatedDistributedSampler` and `IndexBatchSamplerWrapper` for tracking distributed predictions (#7215)\r\n- Added `trainer.predict(return_predictions=None|False|True)` (#7215)\r\n- Added `BasePredictionWriter` callback to implement prediction saving (#7127)\r\n- Added `trainer.tune(scale_batch_size_kwargs, lr_find_kwargs)` arguments to configure the tuning algorithms (#7258)\r\n- Added `tpu_distributed` check for TPU Spawn barrier (#7241)\r\n- Added device updates to TPU Spawn for Pod training (#7243)\r\n- Added warning when missing `Callback` and using `resume_from_checkpoint` (#7254)\r\n- DeepSpeed single file saving (#6900)\r\n- Added Training type Plugins Registry (#6982, #7063, #7214, #7224)\r\n- Add `ignore` param to `save_hyperparameters` (#6056)\r\n\r\n### Changed\r\n\r\n- Changed `LightningModule.truncated_bptt_steps` to be property (#7323)\r\n- Changed `EarlyStopping` callback from by default running `EarlyStopping.on_validation_end` if only training is run. Set `check_on_train_epoch_end` to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069)\r\n- Renamed `pytorch_lightning.callbacks.swa` to `pytorch_lightning.callbacks.stochastic_weight_avg` (#6259)\r\n- Refactor `RunningStage` and `TrainerState` usage (#4945, #7173)\r\n    * Added `RunningStage.SANITY_CHECKING`\r\n    * Added `TrainerFn.{FITTING,VALIDATING,TESTING,PREDICTING,TUNING}`\r\n    * Changed `trainer.evaluating` to return `True` if validating or testing\r\n- Changed `setup()` and `teardown()` stage argument to take any of `{fit,validate,test,predict}` (#6386)\r\n- Changed profilers to save separate report files per state and rank (#6621)\r\n- The trainer no longer tries to save a checkpoint on exception or run callback's `on_train_end` functions (#6864)\r\n- Changed `PyTorchProfiler` to use `torch.autograd.profiler.record_function` to record functions (#6349)\r\n- Disabled `lr_scheduler.step()` in manual optimization  (#6825)\r\n- Changed warnings and recommendations for dataloaders in `ddp_spawn` (#6762)\r\n- `pl.seed_everything` will now also set the seed on the `DistributedSampler` (#7024)\r\n- Changed default setting for communication of multi-node training using `DDPShardedPlugin` (#6937)\r\n- `trainer.tune()` now returns the tuning result (#7258)\r\n- `LightningModule.from_datasets()` now accepts `IterableDataset` instances as training datasets. (#7503)\r\n- Changed `resume_from_checkpoint` warning to an error when the checkpoint file does not exist (#7075)\r\n- Automatically set `sync_batchnorm` for `training_type_plugin` (#6536)\r\n- Allowed training type plugin to delay optimizer creation (#6331)\r\n- Removed ModelSummary validation from train loop on_trainer_init (#6610)\r\n- Moved `save_function` to accelerator (#6689)\r\n- Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)\r\n- Improved verbose logging for `EarlyStopping` callback (#6811)\r\n- Run ddp_spawn dataloader checks on Windows (#6930)\r\n- Updated mlflow with using `resolve_tags` (#6746)\r\n- Moved `save_hyperparameters` to its own function (#7119)\r\n- Replaced `_DataModuleWrapper` with `__new__` (#7289)\r\n- Reset `current_fx` properties on lightning module in teardown (#7247)\r\n- Auto-set `DataLoader.worker_init_fn` with `seed_everything` (#6960)\r\n- Remove `model.trainer` call inside of dataloading mixin (#7317)\r\n- Split profilers module (#6261)\r\n- Ensure accelerator is valid if running interactively (#5970)\r\n- Disabled batch transfer in DP mode (#6098)\r\n\r\n### Deprecated\r\n\r\n- Deprecated `outputs` in both `LightningModule.on_train_epoch_end` and `Callback.on_train_epoch_end` hooks (#7339)\r\n- Deprecated `Trainer.truncated_bptt_steps` in favor of `LightningModule.truncated_bptt_steps` (#7323)\r\n- Deprecated `outputs` in both `LightningModule.on_train_epoch_end` and `Callback.on_train_epoch_end` hooks (#7339)\r\n- Deprecated `LightningModule.grad_norm` in favor of `pytorch_lightning.utilities.grads.grad_norm` (#7292)\r\n- Deprecated the `save_function` property from the `ModelCheckpoint` callback (#7201)\r\n- Deprecated `LightningModule.write_predictions` and `LightningModule.write_predictions_dict` (#7066)\r\n- Deprecated `TrainerLoggingMixin` in favor of a separate utilities module for metric handling (#7180)\r\n- Deprecated `TrainerTrainingTricksMixin` in favor of a separate utilities module for NaN/Inf detection for gradients and parameters (#6834)\r\n- `period` has been deprecated in favor of `every_n_val_epochs` in the `ModelCheckpoint` callback (#6146)\r\n- Deprecated `trainer.running_sanity_check` in favor of `trainer.sanity_checking` (#4945)\r\n- Deprecated `Profiler(output_filename)` in favor of `dirpath` and `filename` (#6621)\r\n- Deprecated `PytorchProfiler(profiled_functions)` in favor of `record_functions` (#6349)\r\n- Deprecated `@auto_move_data` in favor of `trainer.predict` (#6993)\r\n- Deprecated `Callback.on_load_checkpoint(checkpoint)` in favor of `Callback.on_load_checkpoint(trainer, pl_module, checkpoint)` (#7253)\r\n- Deprecated metrics in favor of `torchmetrics` (#6505, #6530, #6540, #6547, #6515, #6572, #6573, #6584, #6636, #6637, #6649, #6659, #7131)\r\n- Deprecated the `LightningModule.datamodule` getter and setter methods; access them through `Trainer.datamodule` instead (#7168)\r\n- Deprecated the use of `Trainer(gpus=\"i\")` (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)\r\n\r\n### Removed\r\n\r\n- Removed the `exp_save_path` property from the `LightningModule` (#7266)\r\n- Removed training loop explicitly calling `EarlyStopping.on_validation_end` if no validation is run (#7069)\r\n- Removed `automatic_optimization` as a property from the training loop in favor of `LightningModule.automatic_optimization` (#7130)\r\n- Removed evaluation loop legacy returns for `*_epoch_end` hooks (#6973)\r\n- Removed support for passing a bool value to `profiler` argument of Trainer (#6164)\r\n- Removed no return warning from val/test step (#6139)\r\n- Removed passing a `ModelCheckpoint` instance to `Trainer(checkpoint_callback)` (#6166)\r\n- Removed deprecated Trainer argument `enable_pl_optimizer` and `automatic_optimization` (#6163)\r\n- Removed deprecated metrics (#6161)\r\n    * from `pytorch_lightning.metrics.functional.classification` removed `to_onehot`, `to_categorical`, `get_num_classes`, `roc`, `multiclass_roc`, `average_precision`, `precision_recall_curve`, `multiclass_precision_recall_curve`\r\n    * from `pytorch_lightning.metrics.functional.reduction` removed `reduce`, `class_reduce`\r\n- Removed deprecated `ModelCheckpoint` arguments `prefix`, `mode=\"auto\"` (#6162)\r\n- Removed `mode='auto'` from `EarlyStopping` (#6167)\r\n- Removed `epoch` and `step` arguments from `ModelCheckpoint.format_checkpoint_name()`, these are now included in the `metrics` argument (#7344)\r\n- Removed legacy references for magic keys in the `Result` object (#6016)\r\n- Removed deprecated `LightningModule` `hparams` setter (#6207)\r\n- Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the `\"log\"/\"progress_bar\"` magic keys. Use `self.log` instead (#6734)\r\n- Removed `trainer.fit()` return value of `1`. It has no return now (#7237)\r\n- Removed `logger_connector` legacy code (#6733)\r\n- Removed unused mixin attributes (#6487)\r\n\r\n### Fixed\r\n\r\n- Fixed NaN errors in progress bars when training with iterable datasets with no length defined (#7306)\r\n- Fixed attaching train and validation dataloaders when `reload_dataloaders_every_epoch=True` and `num_sanity_val_steps=0` (#7207)\r\n- Added a barrier in the accelerator `teardown` to synchronize processes before execution finishes (#6814)\r\n- Fixed multi-node DDP sub-process launch by using `local_rank` instead of `global_rank` for main process assertion (#7061)\r\n- Fixed incorrect removal of `WORLD_SIZE` environment variable in DDP training when launching with torch distributed/torchelastic (#6942)\r\n- Made the `Plugin.reduce` method more consistent across all Plugins to reflect a mean-reduction by default (#6011)\r\n- Move lightning module to correct device type when using LightningDistributedWrapper (#6070)\r\n- Do not print top-k verbose log with `ModelCheckpoint(monitor=None)` (#6109)\r\n- Fixed `ModelCheckpoint(save_top_k=0, save_last=True)` not saving the `last` checkpoint (#6136)\r\n- Fixed `.teardown(stage='fit')` and `.on_fit_{start,end}()` getting called during `trainer.test` (#6386)\r\n- Fixed LightningModule `all_gather` on cpu tensors (#6416)\r\n- Fixed torch distributed not available in setup hook for DDP (#6506)\r\n- Fixed `trainer.tuner.{lr_find,scale_batch_size}` not setting the `Trainer` state properly (#7258)\r\n- Fixed bug where the learning rate schedulers did not follow the optimizer frequencies (#4868)\r\n- Fixed pickle error checker to now check for `pickle.PickleError` to catch all pickle errors (#6917)\r\n- Fixed a bug where the outputs object passed to `LightningModule.training_epoch_end` was different from the object passed to the `on_train_end_epoch` hook (#6969)\r\n- Fixed a bug where the outputs passed to `train_batch_end` would be listed even when using a single optimizer and no truncated backprop through time steps (#6969)\r\n- Fixed bug for trainer error handling which would cause hang for distributed training (#6864)\r\n- Fixed `self.device` not returning the correct device in replicas of data-parallel (#6414)\r\n- Fixed `lr_find` trying beyond `num_training` steps and suggesting a too high learning rate (#7076)\r\n- Fixed logger creating incorrect version folder in DDP with repeated `Trainer.fit` calls (#7077)\r\n- Fixed metric objects passed directly to `self.log` not being reset correctly (#7055)\r\n- Fixed `CombinedLoader` in distributed settings for validation / testing (#7102)\r\n- Fixed the save_dir in `WandbLogger` when the run was initiated externally (#7106)\r\n- Fixed `num_sanity_val_steps` affecting reproducibility of training data shuffling (#7014)\r\n- Fixed resetting device after `fitting/evaluating/predicting` (#7188)\r\n- Fixed bug where `trainer.tuner.scale_batch_size(max_trials=0)` would not return the correct batch size result (#7262)\r\n- Fixed metrics not being properly logged with `precision=16` and `manual_optimization` (#7228)\r\n- Fixed `BaseFinetuning` properly reloading `optimizer_states` when using `resume_from_checkpoint` (#6891)\r\n- Fixed `parameters_to_ignore` not properly set to DDPWrapper (#7239)\r\n- Fixed parsing of `fast_dev_run=True` with the built-in `ArgumentParser` (#7240)\r\n- Fixed handling an `IterableDataset` that fails to produce a batch at the beginning of an epoch (#7294)\r\n- Fixed `LightningModule.save_hyperparameters()` when attempting to save an empty container (#7268)\r\n- Fixed `apex` not properly instantiated when running with `ddp` (#7274)\r\n- Fixed optimizer `state` not moved to `GPU` (#7277)\r\n- Fixed custom init args for `WandbLogger` (#6989)\r\n- Fixed a bug where an error would be raised if the train dataloader sometimes produced None for a batch (#7342)\r\n- Fixed examples (#6600, #6638, #7096, #7246, #6357, #6476, #6294, #6373, #6088, #7398)\r\n- Resolved schedule step bug for PyTorch Profiler (#6674, #6681)\r\n- Updated logic for checking TPUs availability (#6767)\r\n- Resolve TPU miss rendezvous (#6781)\r\n- Fixed auto-scaling mode when calling tune method on trainer (#7321)\r\n- Fixed finetuning complex models correctly unfreezes (#6880)\r\n- Ensure we set the eval/train flag correctly on accelerator model (#6877)\r\n- Set better defaults for `rank_zero_only.rank` when training is launched with SLURM and torchelastic (#6802)\r\n- Fixed matching the number of outputs of backward with forward for AllGatherGrad (#6625)\r\n- Fixed the `gradient_clip_algorithm` has no effect (#6928)\r\n- Fixed CUDA OOM detection and handling (#6934)\r\n- Fixed `unfreeze_and_add_param_group` expects `modules` rather than `module` (#6822)\r\n- Fixed DPP + SyncBN when move on device (#6838)\r\n- Fixed missing arguments in `lr_find` call (#6784)\r\n- Fixed `set_default_tensor_type` to `torch.DoubleTensor` with precision=64 (#7108)\r\n- Fixed `NeptuneLogger.log_text(step=None)` (#7194)\r\n- Fixed importing torchtext batch (#6365, #6323, #6211)\r\n\r\n\r\n### Contributors\r\n\r\n@akihironitta, @alessiobonfiglio, @amisev, @amogkam, @ananthsub, @ArvinZhuang, @ashleve, @asnorkin, @awaelchli, @BloodAxe, @bmahlbrand, @Borda, @borisdayma, @camruta, @carmocca, @ceshine, @dbonner, @dhkim0225, @EdwardJB, @EliaCereda, @EricCousineau-TRI, @ethanwharris, @FlorianMF, @hemildesai, @ifsheldon, @kaushikb11, @mauvilsa, @maxfrei750, @mesejo, @ramonemiliani93, @rohitgr7, @s-rog, @sadiqj, @scart97, @SeanNaren, @shuyingsunshine21, @SkafteNicki, @SpontaneousDuck, @stllfe, @tchaton, @THasthika, @vballoli\r\n\r\n_If we forgot someone due to not matching commit email with GitHub account, let us know :]_\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/PyTorchLightning/pytorch-lightning/releases/tag/1.3.0'>Lightning CLI, PyTorch Profiler, Improved Early Stopping</a>.</em>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7412",
    "createdAt": "2021-05-06T21:23:35Z",
    "updatedAt": "2021-05-07T13:07:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Borda"
    },
    "answer": {
      "body": "Epic!",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-07T13:07:26Z"
    }
  },
  {
    "title": "on_train_epoch_end() runs int he middle of epochs",
    "body": "Hi\r\nI'm trying to calculate some metrics and generate some images to save at the end of each **epoch**.\r\nI put this code in the on_train_epoch_end() (I also tried using a custom callback) but the function seems to be called in the middle of the epochs, approximately 3-4 times per epoch. \r\n\r\nSurely this isn't intended behaviour? Could it be to do with me using a combined loader?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7375",
    "createdAt": "2021-05-05T10:07:45Z",
    "updatedAt": "2022-10-13T16:02:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "inigoval"
    },
    "answer": {
      "body": "@inigoval thanks for reporting this. Could you provide a snippet code so that we can reproduce ?",
      "author": {
        "login": "edgarriba"
      },
      "createdAt": "2021-05-05T10:54:49Z"
    }
  },
  {
    "title": "Generating new data while training.",
    "body": "Hello!\r\n\r\nI'm working on an inference engine where I don't have any data before I start training. The idea is that at each epoch, a new data point is generated by a simulator and that data is fed to the NN only at that epoch. \r\n\r\nI'm wondering if there is a way to do this with pytorch-lightning. I looked at the IterableDataset class but the only thing I came up with is to generate the data beforehand and then iterate over it.\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7374",
    "createdAt": "2021-05-05T09:48:51Z",
    "updatedAt": "2022-12-26T02:23:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pjovanovski1"
    },
    "answer": {
      "body": "hi @pjovanovski1 in this case you could follow a similar strategy proposed in this example of [lightning-bolts](https://lightning-bolts.readthedocs.io/en/latest/) for RL\r\nhttps://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/rl/reinforce_model.py#L254",
      "author": {
        "login": "edgarriba"
      },
      "createdAt": "2021-05-05T10:49:07Z"
    }
  },
  {
    "title": "What is the purpose of reload_dataloaders_every_epoch?",
    "body": "I'm looking to train on chunks of my entire dataset at a time per epoch (preferably over every n epochs, but this is not yet implemented officially), since the size of my dataset exceeds my total RAM. I'd therefore like to update the data in the DataLoaders every epoch. From all the examples I've seen, the actual data content is saved in memory a class attribute (in the following code snippet, it's saved in `self.mnist_train` and `self.mnist_val`). It seems that `train_dataloader()` and `val_dataloader()` only read `self.mnist_train` and `self.mnist_val` into DataLoaders. \r\n\r\nFrom my understanding, `reload_dataloaders_every_epoch=True` calls `train_dataloader()` and `val_dataloader()` at every epoch, but I don't see the point of doing this if `self.mnist_test` and `self.mnist_train` aren't actually being changed. How do I make `train_dataloader()` and `test_dataloader()` return new data every epoch? \r\n\r\n```\r\nclass MyDataModule(pl.LightningDataModule):\r\n\r\n    def __init__(\r\n        self,\r\n        batch_size: int = 32,\r\n    ):\r\n        super().__init__()\r\n        dataset = MNIST(_DATASETS_PATH, train=True, download=True, transform=transforms.ToTensor())\r\n        self.mnist_test = MNIST(_DATASETS_PATH, train=False, download=True, transform=transforms.ToTensor())\r\n        self.mnist_train, self.mnist_val = random_split(dataset, [55000, 5000])\r\n        self.batch_size = batch_size\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7372",
    "createdAt": "2021-05-05T09:30:32Z",
    "updatedAt": "2022-11-02T07:00:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "isabellahuang"
    },
    "answer": {
      "body": "> since the size of my dataset exceeds my total RAM\r\n\r\nThat's not unusual. Often datasets don't fit into the RAM and that's fine. DataLoaders are designed to asynchronously load the data from your hard disk into ram and then onto the GPU.\r\n\r\n> From my understanding, reload_dataloaders_every_epoch=True calls train_dataloader() and val_dataloader() at every epoch, but I don't see the point of doing this if self.mnist_test and self.mnist_train aren't actually being changed.\r\n\r\nYes but first of all this is a toy example and doesn't really do anything interesting. Second even though the dataset does not change, the dataloader will be constructed newly every epoch. You could return  dataloader with a new or growing dataset every epoch or do something crazy like increase the batch size every epoch or turn shuffle on and off haha. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-05T15:51:04Z"
    }
  },
  {
    "title": "Can not log metric which is a tensor",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI get an error `The metric `val_acc_0_step/epoch_0` does not contain a single element thus it cannot be converted to float.` when running training loop.\r\n\r\n###  Full stack trace \r\n```\r\n File \"main.py\", line 76, in <module>\r\n    run_training()\r\n  File \"main.py\", line 69, in run_training\r\n    trainer.fit(model, dm)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 484, in safe_patch_function\r\n    patch_function(call_original, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 241, in patch_with_managed_run\r\n    result = patch_function(original, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 296, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 288, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 440, in call_original\r\n    original_result = original(*og_args, **og_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\r\n    self.dispatch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 577, in run_training_epoch\r\n    self.trainer.run_evaluation(on_epoch=True)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 732, in run_evaluation\r\n    self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 336, in log_evaluation_step_metrics\r\n    self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 351, in __log_result_step_metrics\r\n    self.trainer.logger_connector.log_metrics(metrics_by_epoch, {}, step=batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 222, in log_metrics\r\n    scalar_metrics = self.trainer.metrics_to_scalars(metrics)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py\", line 51, in metrics_to_scalars\r\n    f\"The metric `{k}` does not contain a single element\"\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: The metric `val_acc_0_step/epoch_0` does not contain a single element thus it cannot be converted to float. Found `tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0.])`\r\n```\r\n\r\n### To Reproduce\r\n\r\nI train the model below, which is multi-class / multi-dimensional classifier (have multiple class categories trained in one hierarchical model). The error seem to come from the following piece (running in loop is so that I can specify number of classes, which is different for different dimensions):\r\n```\r\nfor i in range(len(y_true)):\r\n            self.valid_acc[i](preds[i], y_true[i])\r\n            self.log(f'val_acc_{i}', self.valid_acc[i], on_step=True, on_epoch=True)    \r\n```\r\nI was getting the same error when trying to save confusion matrix.\r\n\r\n#### Code sample\r\n\r\n```\r\n\r\nclass OntologyTaggerModel(pl.LightningModule):\r\n    def __init__(self,\r\n                 num_classes,\r\n                 model_name='bert-base-cased',\r\n                 learning_rate=3e-6,\r\n                 **kwargs):\r\n\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.learning_rate = learning_rate\r\n        self.model = BertForMulticlassSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\r\n        self.valid_acc = nn.ModuleList([torchmetrics.Accuracy(average=None, num_classes=num_class) for num_class in torch.tensor(num_classes)])\r\n        self.valid_f1 = torchmetrics.F1(multiclass=True, mdmc_average='samplewise')\r\n        self.cm = nn.ModuleList([torchmetrics.ConfusionMatrix(num_classes=num_class) for num_class in torch.tensor(num_classes)])\r\n        \r\n\r\n    def forward(self, *input, **kwargs):\r\n        return self.model(*input, **kwargs)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y_true = batch\r\n        loss, _ = self(x, labels=y_true)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y_true = batch\r\n        _, y_preds = self(x, labels=y_true)\r\n        preds = [torch.argmax(y_pred, axis=1) for y_pred in y_preds]\r\n        for i in range(len(y_true)):\r\n            self.valid_acc[i](preds[i], y_true[i])\r\n            self.log(f'val_acc_{i}', self.valid_acc[i], on_step=True, on_epoch=True)        \r\n        self.valid_f1(torch.stack(preds), torch.stack(y_true))\r\n        self.log('f1', self.valid_f1, on_step=True, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        'Prepare optimizer and schedule (linear warmup and decay)'\r\n        return torch.optim.Adam(params=self.parameters(), lr=self.learning_rate)\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        avg_loss = torch.tensor([x['loss']\r\n                                 for x in training_step_outputs]).mean()\r\n        self.log('train_loss', avg_loss)\r\n\r\n        print(f'###score: train_loss### {avg_loss}')\r\n\r\n    def validation_epoch_end(self, val_step_outputs):\r\n        for i in range(len(self.valid_acc)):\r\n            acc = self.valid_acc[i].compute()\r\n            self.log(f'val_score_{i}', acc)\r\n        f1 = self.valid_f1.compute()\r\n        self.log('f1', f1)\r\n        print(f'###score: val_score### {acc}')\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nMetrics should be rendered irrespective of dimension\r\n\r\n### Environment\r\n\r\npytorch-lightning==1.2.7\r\ndatasets==1.4.1\r\nmlflow==1.16.0\r\ntorchmetrics=0.3.1\r\ntorch=1.7.1\r\npython=3.6\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7373",
    "createdAt": "2021-05-05T07:21:43Z",
    "updatedAt": "2022-06-13T05:44:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sivakhno"
    },
    "answer": {
      "body": "since you set `average=None` when you initialize the `Accuracy` metric, the output will be the accuracy score calculated per class. As this is a non-scalar tensor and `self.log` is only intended to be used with scalar tensors, you get this error.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-05-05T09:45:45Z"
    }
  },
  {
    "title": "Computing expensive metrics less frequently than using validation_step()",
    "body": "Hi\r\nI need to compute some metrics that are not quick to compute (eg. Frechet Inception Distance).\r\n\r\nIt is too expensive to compute them every validation epoch. Instead I would like to compute the metric once after every training epoch (or after some arbitrary number of steps). To do so, **I need to be able to access the training dataset at the end of every training epoch**, compute the metric and log it.\r\n\r\nIt is not obvious how to do this, as I cannot access the training data-set during \u201con_epoch_end\u201d or one of the other end of epoch hooks.\r\n\r\nIs there a good solution for this?\r\nThanks\r\nInigo.\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7356",
    "createdAt": "2021-05-04T20:59:47Z",
    "updatedAt": "2022-10-25T14:29:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "inigoval"
    },
    "answer": {
      "body": "Solved by using `self.trainer.datamodule`",
      "author": {
        "login": "inigoval"
      },
      "createdAt": "2021-05-05T09:43:28Z"
    }
  },
  {
    "title": "How to implement channels last memory format callback",
    "body": "Hi there\r\n\r\nPytorch docs recommends using channels last when training vision models in mixed precision. To enable, you need to do [two changes](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#converting-existing-models):\r\n1. Move you model to channel last format: `model = model.to(memory_format=torch.channels_last) # Replace with your model`. This can be done in `on_fit_start` callback hook as `model.to` performs an in place modification: \r\n```\r\nclass ChannelsLast(pl.Callback):\r\n    def on_fit_start(self, trainer, pl_module: pl.LightningModule) -> None:\r\n        # Inplace model modification\r\n        pl_module.to(memory_format=torch.channels_last)\r\n```\r\n2. Move input data to channel last format before feeding to the model: `input = input.to(memory_format=torch.channels_last)`. \r\n\r\nMy problem is in step 2. I don't find any PyTorch lightning hook that allows me to make this modification to the batch :/. The only options left are to add it as data transforms (that must be used in conjunction with the callback) or doing all channel last related logic inside the LightningModule. I would prefer to avoid this last solution as it could clutter the LightningModule with unnecessary code.\r\n\r\nDo you know a to do step 2 in a callback?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7349",
    "createdAt": "2021-05-04T15:08:58Z",
    "updatedAt": "2023-03-14T10:46:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "hal-314"
    },
    "answer": {
      "body": "You can use any of the following if done inside the LightningModule:\r\n\r\n```python\r\nbatch = self.on_before_batch_transfer(batch, dataloader_idx)\r\nbatch = self.transfer_batch_to_device(batch, device)\r\nbatch = self.on_after_batch_transfer(batch, dataloader_idx)\r\n```\r\n\r\nIf you really need to do it in the Callback, I guess you could use `on_train_batch_start` since the modification is in-place. But I wouldn't recommend it.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-05-07T13:15:05Z"
    }
  },
  {
    "title": "How to shuffle training data in every epoch?",
    "body": "How could someone shuffle the training dataloader (using [Datamodule](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.datamodule.html#datamodule)) on each epoch?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7332",
    "createdAt": "2021-05-03T18:41:41Z",
    "updatedAt": "2022-06-02T12:42:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "celsofranssa"
    },
    "answer": {
      "body": "You can set `Trainer(reload_dataloaders_every_epoch=True)` and if you have also `shuffle=True` in your dataloader, it will do that by creating a new dataloader every epoch.\r\nThat's my understanding.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-05T15:45:25Z"
    }
  },
  {
    "title": "what is the default scheduler?",
    "body": "I see that there is an option to return just an optimizer in the `configure_optimizers` function. What will be the default scheduler in that case?\r\nBTW I tried very hard to find the answer in the docs and could not find it..",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7314",
    "createdAt": "2021-05-02T13:01:16Z",
    "updatedAt": "2023-10-19T10:39:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yuvalkirstain"
    },
    "answer": {
      "body": "There's no LR scheduler applied in this case: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\r\n\r\n",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-05-02T18:31:52Z"
    }
  },
  {
    "title": "To delete",
    "body": "",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7312",
    "createdAt": "2021-05-02T10:54:31Z",
    "updatedAt": "2023-12-11T16:23:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yuvalkirstain"
    },
    "answer": {
      "body": "to be continued ...",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-03T00:16:57Z"
    }
  },
  {
    "title": "How to save a copy of the running script into the log folder?",
    "body": "Hi,\r\nI want to save a copy of the running script of each experiment.\r\nI have tried to apply the following callback:\r\n\r\n```\r\nimport os\r\nfrom pathlib import Path\r\n\r\nclass MyCopyingCallback(pl.Callback):\r\n\r\n    def on_init_end(self, trainer):\r\n        log_dir = trainer.logger.log_dir\r\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\r\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\r\n``` \r\nThe problem is that I ended up with two copies of my running script under two different folders (I'm using the _ddp_ backend)\r\n\r\nAny ideas on how to solve this issue?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7310",
    "createdAt": "2021-05-02T10:29:49Z",
    "updatedAt": "2023-02-21T05:31:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ItamarKanter"
    },
    "answer": {
      "body": "Hey!\r\nYou can use the rank id to determine in which process you are, like so:\r\n\r\n```python\r\nif trainer.global_rank == 0:\r\n    # do something only on first process\r\n```\r\n\r\nIn your example, this could make sense:\r\n\r\n```python\r\nimport os\r\nfrom pathlib import Path\r\n\r\nclass MyCopyingCallback(pl.Callback):\r\n\r\n    def on_init_end(self, trainer):\r\n        if trainer.global_rank != 0:\r\n            return \r\n        log_dir = trainer.logger.log_dir\r\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\r\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-03T00:25:58Z"
    }
  },
  {
    "title": "where to add preprocessing initialization",
    "body": "I would like to have a step called before the first training step, and that yet necessitates the dataloader\r\n\r\ne.g. (mock code)\r\n\r\n```python\r\nclass Scaler(nn.Module):\r\n    '''center target data'''\r\n     def __init__(self, dims):\r\n         self.mean = nn.Parameter(torch.tensor(dims))\r\n         self.n = nn.Parameters(torch.zeros(1))\r\n\r\n     def forward(self, batch):\r\n          input, target = batch\r\n          if self.training:\r\n              self.mean += target.mean(0)\r\n              self.n += 1\r\n          else:\r\n              return input, (target - self.mean)/self.n\r\n\r\nclass MySystem(pl.LightningModule):\r\n    def __init__(self, scaler_dims, model_dims):\r\n        self.model = nn.Linear(**model_dims)\r\n        self.scaler = Scaler(self.dims).train()\r\n\r\n    def on_first_epoch(self, dataloader):  # <---- not sure where this should live\r\n         # learn to scale the dataset\r\n         for batch in dataloader:\r\n               self.scaler(batch)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n         self.scaler.eval()\r\n         input, target = self.scaler(batch)\r\n         pred = self.model(input)\r\n         loss = F.l1_loss(pred, target)\r\n         return loss\r\n      \r\n\r\ndm = MyDataModule()\r\nsystem = MySystem()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(system, dm) \r\n```\r\n\r\nI'm not clear on how to do this with PL's API: `nn.LithningModule.setup()` does not have access to the dataloader. \r\n\r\nAny advice?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7307",
    "createdAt": "2021-05-01T20:11:32Z",
    "updatedAt": "2022-06-29T13:04:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kingjr"
    },
    "answer": {
      "body": "thanks @ananthsub , I think the point of lightning is to try to keep everything in the same system.\r\n\r\ngoing through the doc, I think the best is either \r\n- move the pl.DataLightningModule to the pl.LightningModule  and setup such scaler with `self._prepare_data` (which is called once in distributed, as opposed to `self.setup`)\r\n- using a callback `on_init_start': https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html",
      "author": {
        "login": "kingjr"
      },
      "createdAt": "2021-05-01T22:00:16Z"
    }
  },
  {
    "title": "Stop training if high enough accuracy isn\u2019t reached",
    "body": "Hi, I know that there is EarlyStopping if validation metrics are deteriorating. But I was wondering if it was possible to stop training if after say epoch 10, the accuracy hasn\u2019t reached say 20%. If such a callback doesn\u2019t exist, any thoughts on how I can get started on the implementation of it?\r\n\r\nFor context I am running a distributed hyper-parameter optimizer and I know that the \u201cgood\u201d hyper-parameter set will get me to 50% accuracy by epoch 5.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7305",
    "createdAt": "2021-05-01T10:02:51Z",
    "updatedAt": "2022-10-26T10:01:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sachinruk"
    },
    "answer": {
      "body": "You could write a callback similar to early stopping which checks your metric for the target value by whatever epoch. if the metric isn't good enough, you can signal to the trainer to stop, like this: https://github.com/PyTorchLightning/pytorch-lightning/blob/490cc57809ebeba19003b4101393a8a058217c31/pytorch_lightning/callbacks/early_stopping.py#L194-L196",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-05-01T11:34:37Z"
    }
  },
  {
    "title": "Forward run: best practices",
    "body": "Once the model is trained, what would be the best way to use it to do a forward pass in a large dataset (that must be divided in batches not to fill GPU memory) and obtain the output?\r\n\r\nWhat I normally do is to use the LightningModule as a torch Module. However, that forces me to: write a `for`, explicitly transfer to the device each batch, make sure to cal `.eval()`, concatenate the output of each batch... all the things that pytorch-lightning saves me from doing during training. But I guess I am missing an obvious simpler option?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7298",
    "createdAt": "2021-04-30T16:51:35Z",
    "updatedAt": "2021-05-07T13:11:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "fjhheras"
    },
    "answer": {
      "body": "In 1.3 there will be a [predict](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#predict) function that does all that for you!\r\nYou can already use it by installing 1.3.0rc1. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-05-03T02:36:52Z"
    }
  },
  {
    "title": "Problem in multi-node training",
    "body": "Hello pytorch-lightning community,\r\n\r\nmy training hangs when training on multi-nodes; on single node with multiple GPUs runs fine :/\r\nIt baffles me that although the global rank ID seems right, the member output has 4 instead of 8 in the denominator.\r\nSince I run in a slurm environment, do I have to add the SLURMEnvironment plugin in the Trainer? I tried to add it alongside the DDPPlugin but it was not accepted (Found invalid type for plugin <class 'pytorch_lightning.plugins.environments.slurm_environment.SLURMEnvironment'>. Expected a precision or training type plugin)\r\n\r\nThe job submission file has the corresponding lines:\r\n#SBATCH --gres=gpu:4\r\n#SBATCH --nodes=2\r\n#SBATCH --exclusive\r\n\r\nsrun --ntasks=8 python3 coolModel.py 2>&1 | tee log.train\r\n\r\nI attach the output and the code below...\r\npytorch version:  1.8.1\r\npytorch-lightning version:  1.2.4\r\n\r\nCheers,\r\nNikos\r\n\r\n[multNodeTraining.txt](https://github.com/PyTorchLightning/pytorch-lightning/files/6398914/multNodeTraining.txt)\r\n\r\n###########python code\r\nimport os\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nimport torchvision.transforms as transforms\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n#from test_tube import Experiment\r\n\r\n\r\nclass database(pl.LightningDataModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    #def setup(self, stage=None):\r\n    def train_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n\r\n\r\nclass CoolModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super(CoolModel, self).__init__()\r\n        # not the best model...\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def my_loss(self, y_hat, y):\r\n        return F.cross_entropy(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'loss': self.my_loss(y_hat, y)}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'val_loss': self.my_loss(y_hat, y)}\r\n\r\n    def validation_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        return {'avg_val_loss': avg_loss}\r\n\r\n    def configure_optimizers(self):\r\n        return [torch.optim.Adam(self.parameters(), lr=0.02)]\r\n\r\n\r\n\r\nif __name__=='__main__':\r\n\r\n    dm = database()\r\n    model = CoolModel()\r\n    #exp = Experiment(save_dir=os.getcwd())\r\n    #checkp1 = pl.callbacks.ModelCheckpoint(\r\n    #    monitor='loss', save_top_k=2, dirpath='./', mode='min', save_last=True\r\n    #    ) \r\n    trainer = Trainer(\r\n            max_epochs=10,\r\n            gpus=4, num_nodes=2, accelerator='ddp',\r\n            plugins= pl.plugins.DDPPlugin(find_unused_parameters=False), #pl.plugins.SLURMEnvironment],\r\n            #progress_bar_refresh_rate=0\r\n            )\r\n    # train on 32 gpus across 4 nodes (make sure to submit appropriate SLURM job)\r\n    # trainer = Trainer(experiment=exp, max_nb_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], nb_gpu_nodes=4)\r\n    trainer.fit(model, dm)\r\n    # view tensorflow logs\r\n    print(f'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}')\r\n    print('and going to http://localhost:6006 on your browser')\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7275",
    "createdAt": "2021-04-29T13:51:27Z",
    "updatedAt": "2024-08-05T19:58:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nickKyr"
    },
    "answer": {
      "body": "Hello Nikos\r\n\r\nDo you have 8 gpus in the node? I think it must match gres. \r\nDon't you also need to specify how many tasks per node in the SBATCH directive? [1]\r\nAlso, I notice some unsupported Trainer arguments in your script. It should be:\r\n`trainer = Trainer(max_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], num_nodes=4)\r\n`\r\nMake sure this script actually runs on CPU first before going to the cluster \ud83d\ude05 \r\n\r\nTotally no slurm expert here, just looking at your script with one eye closed. \r\n\r\n\r\n[1] https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html#slurm-managed-cluster",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-30T02:22:24Z"
    }
  },
  {
    "title": "How to combine multiple lightning module and save hyperparameters",
    "body": "Good day, I'm currently working on two models which train on the same data. I'd like to integrate the two pre-trained models into one and use it for transfer learning. The combination is written as such (you can copy paste to run it). \r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\nclass MyModelA(pl.LightningModule):\r\n    def __init__(self, hidden_dim = 10):\r\n        super(MyModelA, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\r\n        self.save_hyperparameters()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x,y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n    \r\nclass MyModelB(pl.LightningModule):\r\n    def __init__(self, hidden_dim = 10):\r\n        super(MyModelB, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\r\n        self.save_hyperparameters()\r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x,y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\nclass MyEnsemble(pl.LightningModule):\r\n    def __init__(self, modelA, modelB):\r\n        super(MyEnsemble, self).__init__()\r\n        self.modelA = modelA\r\n        self.modelB = modelB\r\n        self.modelA.freeze()\r\n        self.modelB.freeze()\r\n        self.classifier = torch.nn.Linear(4, 2)\r\n\r\n        #self.save_hyperparameters() # Uncomment to show error\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x1 = self.modelA(x)\r\n        x2 = self.modelB(x)\r\n        x = torch.cat((x1, x2), dim=1)\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\ndl = DataLoader(TensorDataset(torch.randn(1000, 10), \r\n                              torch.randn(1000, 2)), \r\n                batch_size = 10)\r\n\r\nmodelA = MyModelA()\r\nmodelB = MyModelB()\r\n\r\n# pretrained modelA and modelB\r\ntrainerA = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainerA.fit(modelA, dl)\r\ntrainerB = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainerB.fit(modelB, dl)\r\n\r\n# modelA and modelB contains pretrained weights\r\nmodel = MyEnsemble(modelA, modelB)\r\n\r\ntrainer = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainer.fit(model, dl)\r\n```\r\nAt first the code worked fine. However, I would like to save the hyperparameters of the ensemble module, but adding `self.save_hyperparameters()` at the end of the ensemble module `__init__` return this error.\r\n\r\n    ValueError: dictionary update sequence element #0 has length 1; 2 is required\r\n\r\nHence my question is how can I combine two or more lightning modules in a single module and save its hyperparameters? Or is there any alternative way to do so?\r\n\r\nThanks in advance!\r\n\r\nEDIT: Code updated to show both modelA and modelB are pretrained.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7249",
    "createdAt": "2021-04-28T07:37:18Z",
    "updatedAt": "2023-11-19T15:31:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kyleong"
    },
    "answer": {
      "body": "I have finally came out with the final solution which can be obtained [here](https://github.com/PyTorchLightning/pytorch-lightning/issues/7447#issuecomment-835695726).\r\n\r\nThank you for anyone who read and participate in this discussion.",
      "author": {
        "login": "kyleong"
      },
      "createdAt": "2021-05-09T03:51:40Z"
    }
  },
  {
    "title": "How to use scheduler correctly ?",
    "body": "I have a huge dataset, so I have set val_check_interval parameter to be 0.25. I want the scheduler to be called after each validation. So, during one epoch, there should be four validation checks. I want the scheduler to be updated based on these 4 intermediate values. For example, if the accuracy doesn't increase for two consecutive checks, I want the learning rate to be halved.\r\n\r\nI was hoping that the following code should be enough to accomplish this behavior, but it doesn't seem to be working. Any help is appreciated. Thanks.\r\n\r\n  ```\r\n      scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\r\n                                                                    mode='max', factor=0.75,\r\n                                                                    patience=2, verbose=True),\r\n            'interval': 'step',\r\n            'frequency': len(self.val_dataloader()) + 1,\r\n            'strict': True,\r\n            'monitor': 'val_acc_epoch',\r\n        }\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7245",
    "createdAt": "2021-04-28T01:10:16Z",
    "updatedAt": "2025-05-29T20:32:36Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sidml"
    },
    "answer": {
      "body": "To me it looks like you just need to set the `frequency` argument to 1/4 of the size of your training data:\r\n```python\r\n    scheduler = {\r\n          'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\r\n                                                                  mode='max', factor=0.75,\r\n                                                                  patience=2, verbose=True),\r\n          'interval': 'step',\r\n          'frequency': int(len(self.train_dataloader()) * 0.25),\r\n          'strict': True,\r\n          'monitor': 'val_acc_epoch',\r\n      }\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-04-28T14:28:11Z"
    }
  },
  {
    "title": "Is the use of data methods on LightningModule standard?",
    "body": "It took me quite some time playing with PyTorch Lightning and reading through the docs before I realized I can attach the data-related methods (`[train|val]_dataloader`, `prepare_data`) directly to `LightningModule` and then pass only the module to the trainer, rather than having to create a separate `LightningDataModule` class or passing the data loaders explicitly. When thinking of `LightningModule` as a task, rather than a model, this makes actually perfect sense. After all, things like batch size and learning rate are inherently related. But since I haven't seen it really explicitly documented, I wonder whether the use of data methods directly on the `LightningModule` is considered standard?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7227",
    "createdAt": "2021-04-27T08:04:07Z",
    "updatedAt": "2022-10-20T20:36:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jhrmnn"
    },
    "answer": {
      "body": "Hi, could you explain what you mean by \r\n\r\n> I realized I can attach the data-related methods ([train|val]_dataloader, prepare_data) directly to LightningModule\r\n\r\nDo you mean dynamically bind the dataloader method of datamodule to lightning module? \r\nI would not personally recommend that because it can be harder to read and debug, especially if your LightningModule also defines dataloader methods that you may expect to run in a different context.\r\n\r\nI recommend passing the datamodule/dataloader into the fit function like so:\r\n```python\r\ntrainer.fit(model, datamodule=datamodule)\r\n# or\r\ntrainer.fit(model, train_dataloader=...)\r\n```\r\nThen it will be clear to any reader that the datamodule loaders are used for training. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-27T17:57:30Z"
    }
  },
  {
    "title": "Control log_dict's on_epoch log name",
    "body": "Hey!\r\n\r\nI tried to change to using the new way of logging through `MetricCollection` and `self.log_dict` instead of logging every metric through `self.log` on `training step` and `test_epoch_end`. However, each metric is then logged as `[metric_name]_epoch_[epoch_number]` which creates a new graph for every epoch instead of allowing me to use epoch on the x-axis of my graphs (on Comet, if that is relevant). Is there a way to control this behaviour of` log_dic`t, or do I just have to keep logging \"manually\" to control log name?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7209",
    "createdAt": "2021-04-26T08:31:08Z",
    "updatedAt": "2023-12-19T12:35:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "johanneskvamme"
    },
    "answer": {
      "body": "Hi @FluidSense,\r\n\r\nCould you try just updating the `MetricCollection` during the `_step` method and then log in `_epoch_end` method. Something like:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    logits = self(x)\r\n    self.train_metrics.update(logits, y)\r\n\r\ndef train_epoch_end(self, outputs):\r\n    self.log_dict(self.train_metrics.compute())\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-04-26T15:48:41Z"
    }
  },
  {
    "title": "Better undestanding how data is loded in datamodule setup method for multi GPU setting in NLP",
    "body": "I want to better understand the setup and prepare_data methods in multi gpu scenariu in context of NLP and text processing.\r\n\r\nI have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\r\nIn `prepare_data()` I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\r\nNext in the `setup()` method I read filtered_data.json and split it to train and valid. \r\nI can perform split deterministically so train and valid splits will always have the same elements or I can split randomly then each GPU process will have a different train and valid sets.\r\n \r\nWhen using it in multi-gpu (2 GPUs) each process will have its own copy of the train and valid set (am I right?). Which approach is better in context data utilization, random or deterministically?\r\nI do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\r\n\r\nIf data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\r\nOr maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7186",
    "createdAt": "2021-04-23T09:20:18Z",
    "updatedAt": "2022-06-06T10:04:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ksopyla"
    },
    "answer": {
      "body": "> I have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\r\n> In prepare_data I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\r\n\r\nDo all of that either offline in a different script, or do it in the `prepare_data` hook. \r\n\r\n> Next in the setup method I read filtered_data.json and split it to train and valid.\r\n\r\nSounds good. Each GPU/node will run the same, so you will have the same train and val split in all of them (initially). Don't split the data differently for each GPU, that part will be done by the DistributedSampler [1].\r\n\r\n> I do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\r\n\r\nLightning takes your DataLoader and adds a DistributedSampler. The DS knows on which GPU it is and will sample only a portion of your data on one GPU and another portion on the other GPU. Each GPU sees a different split of train and a different split of val.\r\n\r\n> If data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\r\n\r\nAs explained above, the dataloader on each GPU will return different samples on each GPU automatically. Each GPU will have the same network weights, uses different data to compute gradients, then gradients are averaged so each GPU gets the same update and starts with the same weights for the next forward/backward [2].\r\n\r\n> Or maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.\r\n\r\nYes, again this is automatically done for you.\r\n\r\nReferences:\r\n[1] [DistributeSampler](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler)\r\n[2] [Distributed Training in PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\r\n[3] [Multi-GPU training in PyTorch](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html)\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-24T22:16:44Z"
    }
  },
  {
    "title": "How to monitor tensorboard logged scalar in modelcheckpoint?",
    "body": "If I save scalar log like this,\r\n```\r\nself.logger.experiment.add_scalars(\u2018loss/nll\u2019, {\u2018train\u2019: trainloss, \u2018valid\u2019: validloss})\r\n```\r\nHow to monitor valid loss in Modelcheckpoint?\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7184",
    "createdAt": "2021-04-23T09:17:45Z",
    "updatedAt": "2022-07-23T22:30:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": null,
    "answer": {
      "body": "you need to call it with self.log instead (we automate the rest for you), so that we are aware of you logging it :)",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-04-23T09:37:07Z"
    }
  },
  {
    "title": "How to stop wandblogger uploading the checkpoint?",
    "body": "I want the checkpoint and the logs stay in the same place while only the logs are uploaded to wandb server.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7177",
    "createdAt": "2021-04-23T03:27:30Z",
    "updatedAt": "2022-07-23T10:01:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stdoo"
    },
    "answer": {
      "body": "Yes, it will be fixed with #6231",
      "author": {
        "login": "borisdayma"
      },
      "createdAt": "2021-04-23T12:43:17Z"
    }
  },
  {
    "title": "Should the total epoch size be less when using multi-gpu DDP?",
    "body": "If I have 100 training examples and 100 validation examples, and I run on a single gpu with a batch size of 10, the tqdm bar will show 20 epoch iterations. If I run on 2 gpus with ddp and the same batch size, the tqdm bar will still show 20 epoch iterations, but isnt the effective batch size now 20 instead of 10 because theres 2 gpus? Shouldnt the total number of iterations be half?\r\n\r\nThanks for any clarification.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7175",
    "createdAt": "2021-04-23T00:34:56Z",
    "updatedAt": "2022-08-12T09:15:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jipson7"
    },
    "answer": {
      "body": "Hi @jipson7 ,\r\nFirst of all: You're right, that's how it should be.\r\n\r\nWe tried to reproduce this, but for us this produced the following (correct) output. Do you have a minimal reproduction example?\r\n\r\n```\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 17.23it/s, loss=-43.6, v_num=272]\r\nseen train: 5\r\nseen train: 5\r\n```",
      "author": {
        "login": "justusschock"
      },
      "createdAt": "2021-04-23T07:50:55Z"
    }
  },
  {
    "title": "Pytorch Lightning doesn't have CUDA?",
    "body": "## \ud83d\udc1b Bug\r\n\r\nHello, I'm trying to use Pytorch Lightning in order to speed up my ESR GAN renders on Windows 10. \r\n\r\nHowever, when I ran the installation code and attempt to run Cupscale (which I use as a GUI for ESR GAN), I get an error saying \"Pytorch compiled without CUDA\".\r\n\r\nIs there a way to choose to install specifically the CUDA version of Pytorch with Lightning install, or are the two incompatible? If the latter is the case, that's not good to hear. If the former, can I have such a code?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7158",
    "createdAt": "2021-04-22T09:38:32Z",
    "updatedAt": "2022-09-28T11:14:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Troceleng"
    },
    "answer": {
      "body": "@TrocelengStudios ho this seems like you do not have CUDA installed, can you run `nvidia-smi`?\r\ncc: @awaelchli ",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2021-04-22T10:54:34Z"
    }
  },
  {
    "title": "how to save the last epoch only?",
    "body": "\"monitor (Optional[str]) \u2013 quantity to monitor. By default it is None which saves a checkpoint only for the last epoch.\"\r\nwhen i trainning a model, i set the 'monitor' to None, it should save the last epoch as the doc says. but it still save depend on the val_loss, it always save the model with lowest val_loss.\r\n\r\ni also try another way, set the 'save_last' to True. while this needs to set a monitor. And if i set save_top_k to 0, it will save nothing; if set to 1, it will save 2 models, the best one and the last one. But i just want to save the last one.\r\n\r\nis this a bug or i made sth wrong?  is there a way to save model with epoch asigned myself? such as the last 3 epochs?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7153",
    "createdAt": "2021-04-21T14:06:16Z",
    "updatedAt": "2023-06-19T11:30:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "machin-x"
    },
    "answer": {
      "body": "Hey! Have a look at this example: \r\n\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss, logger=False)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=5,  # this will save a checkpoint at epoch index 4 (last epoch)\r\n        weights_summary=None,\r\n        logger=False,\r\n        callbacks=[ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)]\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nI'm choosing:\r\n`ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)`\r\n\r\nThat's all, it saves only one checkpoint, named **epoch=4-step=4.ckpt**, it corresponds to the last epoch being run. \r\nNote: for backward compatibility, when monitor=None, we choose \"val_loss\" as the monitor when it is available. You should be able to avoid that by just renaming your validation loss to \"valid_loss\" or something else :) \r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-21T23:20:27Z"
    }
  },
  {
    "title": "Re-enabling gradients in Validation loop?",
    "body": "I'm trying to use [Layer-wise relevance propagation](https://github.com/fhvilshoj/TorchLRP) as part of the training and validation loop of training my model, but it requires gradients to be present in order to calculate the relevance. I'm trying to figure out the best way of approaching this in a Lightning-friendly way. I can already do it for the training loop, but the validation loop is what's giving me problems. The only way I can think to do it at the moment is once the validation loop finishes, re-run the validation dataset with gradients enabled without calling the optimiser's `.step()` function. I'd really appreciate some guidance here before I dive in. Thanks! ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7116",
    "createdAt": "2021-04-20T09:28:59Z",
    "updatedAt": "2023-12-03T18:54:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "harritaylor"
    },
    "answer": {
      "body": "You can call `torch.set_grad_enabled(True)` in your validation loop or in any hook you want. Doesn't that work?\r\nAnd perhaps `model.train()` if you have normalization layers dropout or that kind of layers.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-20T10:47:33Z"
    }
  },
  {
    "title": "trainer.tune causes \"No `train_dataloader()` method defined.\" error",
    "body": "Am I right in calling the tune method as follows?\r\n\r\n`trainer.tune(model, train_dataloader, val_dataloader)\r\n`\r\n\r\nHere is the stacktrace of the error I am getting. \r\n\r\n```\r\ntrainer.tune(model, train_dataloader, val_dataloader)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in tune\r\n    self.tuner.tune(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 63, in tune\r\n    self.lr_find(model, update_attr=True)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 136, in lr_find\r\n    self.setup_trainer(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 44, in setup_trainer\r\n    self.trainer.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 120, in setup_fit\r\n    self.trainer.config_validator.verify_loop_configurations(model)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 36, in verify_loop_configurations\r\n    self.__verify_train_loop_configuration(model)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 58, in __verify_train_loop_configuration\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()`\r\n to be defined.\r\n```\r\n\r\n`trainer.fit(model, train_dataloader, val_dataloader)` works fine. I don't define the data loader as a method in my model, rather I pass one in to my `.fit` methods. This seems to work well for the `.fit` method, but not for `.tune`.\r\n\r\n\r\nFWIW, here is my model definition\r\n\r\n```python\r\nclass PretrainedResnet50FT(pl.LightningModule):\r\n    \r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--num_classes', type=int, default=2)\r\n        parser.add_argument('--lr', type=float, default=1e-3)\r\n        return parser\r\n\r\n    def __init__(self, hparams):\r\n        super().__init__()\r\n        self.hparams = hparams\r\n\r\n        image_modules = list(models.resnet50(pretrained=True, progress=False).children())[:-1]\r\n        self.resnet = nn.Sequential(*image_modules)\r\n        self.classifier = nn.Linear(2048, self.hparams.num_classes)\r\n\r\n    def forward(self, x):\r\n        out = self.resnet(x)\r\n        out = torch.flatten(out, 1)\r\n        out = self.classifier(out)\r\n        return out\r\n\r\n    def step(self, who, batch, batch_nb):    \r\n        x, task_labels, slide_id = batch\r\n        \r\n        #Define logits over the task and source embeddings\r\n        task_logits = self(x)\r\n\r\n        #Define loss values over the logits\r\n        loss = task_loss = F.cross_entropy(task_logits, task_labels, reduction = \"mean\")                \r\n                \r\n        #Train acc\r\n        task_preds = task_logits.argmax(-1)\r\n        task_acc = pl.metrics.functional.accuracy(task_preds, task_labels)\r\n        \r\n        #F1\r\n        task_f1 = pl.metrics.functional.f1(task_preds, task_labels, num_classes = self.hparams.num_classes)\r\n\r\n        self.log(who + '_loss', loss)\r\n        self.log(who + '_acc', task_acc)\r\n        self.log(who + '_f1', task_f1)\r\n\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # REQUIRED\r\n        loss = self.step('train', batch, batch_nb)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        loss = self.step('val', batch, batch_nb)\r\n        return loss\r\n        \r\n    def test_step(self, batch, batch_nb):\r\n        loss = self.step('test', batch, batch_nb)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7092",
    "createdAt": "2021-04-19T00:53:48Z",
    "updatedAt": "2023-02-10T21:13:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "surya-narayanan"
    },
    "answer": {
      "body": "Arguments was not correctly passed from `tune` to `lr_find`. Was solved by this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/6784\r\nPlease upgrade to latest version of lightning :]",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-04-19T09:54:20Z"
    }
  },
  {
    "title": "Example in domain_templates: computer_vision_fine_tuning",
    "body": "I found that in the code below sigmoid activation applied before binary_cross_entropy_with_logits loss:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/71b4611c64059d7589e4d80115209fd2c89e8bdb/pl_examples/domain_templates/computer_vision_fine_tuning.py#L196-L231\r\nFrom the documentation https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss:\r\n```This loss combines a Sigmoid layer and the BCELoss in one single class. ```\r\nIs that performed intentionally? Or it's just a bug?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7081",
    "createdAt": "2021-04-18T11:29:24Z",
    "updatedAt": "2022-06-12T03:10:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "amisev"
    },
    "answer": {
      "body": "Yes, the sigmoid should be removed from the forward, because BCEWithLogits already contains the sigmoid. \r\nDo you want to send a PR to update the example?",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-18T17:17:59Z"
    }
  },
  {
    "title": "Error with predict()",
    "body": "Hi @awaelchli and thanks for your time, as you asked in pull requests, i am pinging you here\r\n\r\nFor other who see this, it's a discussion about Trainer.predict method where it is running BatchNorm Layers, code is below:\r\n\r\n\r\nhttps://colab.research.google.com/drive/1jujP4F_prSmbRz-F_wGfWPTKGOmY5DPE?usp=sharing\r\n\r\nWhat is the problem with my approach?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7068",
    "createdAt": "2021-04-17T02:35:13Z",
    "updatedAt": "2022-06-10T20:20:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "dsantiago"
    },
    "answer": {
      "body": "Predict takes a dataloader, not a tensor. It still \"works\" because the trainer just iterates through the batch dimension, but then you get an error later because the input lost the batch dimension, and batch norm doesn't work with batch size 1. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-17T10:09:58Z"
    }
  },
  {
    "title": "How to disable the automatic reduce/mean while using dp?",
    "body": "Hello everyone,\r\n\r\nI have upgraded pytorch-lightning to 1.2.6 recently, the behavior of dp seems different from 1.2.0. To be specific, the returned values of validation_step() are automatically reduced before sent to validation_epoch_end(). However, the metrics I use need the original predictions of each sample instead of the reduced values. Is there a way to disable the automatic reduce and pass the whole predictions to validation_epoch_end()? Note that the validation_step_end() is not implemented in my model.\r\n\r\ncc: @tchaton ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7009",
    "createdAt": "2021-04-14T08:47:46Z",
    "updatedAt": "2022-08-04T16:38:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stdoo"
    },
    "answer": {
      "body": "I notice that the validation_step_end() and test_step_end() functions in dp.py script are:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return self.reduce(output)\r\n\r\ndef test_step_end(output):\r\n       return self.reduce(output)\r\n```\r\n\r\nThus, overwrite these two methods as follows will disable the automatic reduce in evaluation and test:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return output\r\n\r\ndef test_step_end(output):\r\n       return output\r\n```",
      "author": {
        "login": "stdoo"
      },
      "createdAt": "2021-04-17T09:27:21Z"
    }
  },
  {
    "title": "Why is my gpu-util low?",
    "body": "I use one node and 4 gpus for training. And I use dali dataloader, I don't know why my gpu util is low, and training is also slow. About 1:30 per epoch, I train for 200 epoches, which will cost 5 hours. It's slower than the project mmclassification, which only cost 3.5 hours. Compared to mmclassification project which can only support torch.utils.data.dataloader, I think if I use dali_dataloader, it will accelerate my training. But as you can see, it's the opposite. I don't know why. Could anyone give me some advice? I use cifar10 dataset. And I train on slurm.\r\n![image](https://user-images.githubusercontent.com/23625282/114308721-666e7480-9b17-11eb-8842-7e1eb7334ff9.png)\r\nHere is my code.\r\n\r\nmain.py\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom net import ResNet18\r\nif __name__ == '__main__':\r\n    model = ResNet18()\r\n    trainer = pl.Trainer( max_epochs=200,log_every_n_steps=1,\r\n        log_gpu_memory='min_max',gpus=4,num_nodes=1,accelerator='ddp',\r\n        fast_dev_run=False,callbacks=[ModelCheckpoint(monitor='val_accuracy',mode='max')],\r\n        progress_bar_refresh_rate=1,replace_sampler_ddp=False)\r\n    trainer.fit(model)\r\n```\r\nnet.py\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\nfrom dataloader import dali_DataLoader,HybridPipe,dali_CIFAR10\r\nclass BasicBlock(nn.Module):\r\n    expansion = 1\r\n\r\n    def __init__(self, in_planes, planes, stride=1):\r\n        super(BasicBlock, self).__init__()\r\n        self.conv1 = nn.Conv2d(\r\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                               stride=1, padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*planes:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*planes,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*planes)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = self.bn2(self.conv2(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, in_planes, planes, stride=1):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                               stride=stride, padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, self.expansion *\r\n                               planes, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*planes:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*planes,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*planes)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = F.relu(self.bn2(self.conv2(out)))\r\n        out = self.bn3(self.conv3(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n\r\n\r\nclass ResNet(pl.LightningModule):\r\n    def __init__(self, block, num_blocks, num_classes=10):\r\n        super(ResNet, self).__init__()\r\n        self.in_planes = 64\r\n\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\r\n                               stride=1, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\r\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\r\n        self.linear = nn.Linear(512*block.expansion, num_classes)\r\n        self.correct = 0\r\n        self.total_size = 0\r\n    def _make_layer(self, block, planes, num_blocks, stride):\r\n        strides = [stride] + [1]*(num_blocks-1)\r\n        layers = []\r\n        for stride in strides:\r\n            layers.append(block(self.in_planes, planes, stride))\r\n            self.in_planes = planes * block.expansion\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = self.layer1(out)\r\n        out = self.layer2(out)\r\n        out = self.layer3(out)\r\n        out = self.layer4(out)\r\n        out = F.avg_pool2d(out, 4)\r\n        out = out.view(out.size(0), -1)\r\n        out = self.linear(out)\r\n        return out\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = self(x)\r\n        loss_fn = nn.CrossEntropyLoss()\r\n        loss = loss_fn(x,y)\r\n        predicted = torch.argmax(x, dim=1, keepdim=False)\r\n        self.correct += (predicted == y).sum().item()\r\n        self.total_size += y.size(0)\r\n        self.log('train_loss', loss,prog_bar=True, logger=True)\r\n        self.log('train_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = self(x)\r\n        loss_fn = nn.CrossEntropyLoss()\r\n        loss = loss_fn(x,y)\r\n        predicted = torch.argmax(x, dim=1, keepdim=False)\r\n        self.correct += (predicted == y).sum().item()\r\n        self.total_size += y.size(0)\r\n        self.log('val_loss', loss,on_step=False, on_epoch=True,prog_bar=True, logger=True)\r\n        self.log('val_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        return loss\r\n    def validation_epoch_end(self,out):\r\n        self.log('val_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        self.correct=0\r\n        self.total_size=0\r\n    def train_epoch_end(self,out):\r\n        self.log('train_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        self.correct=0\r\n        self.total_size=0\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\r\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,150], gamma=0.1, last_epoch=-1, verbose=False)\r\n        return [optimizer],[scheduler]\r\n    def train_dataloader(self):\r\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root='./data'), batch_size=32, pad_ratio=1.25,num_threads=4,\r\n             is_distribute=True, crop_size=32,ramdom_flip=True,\r\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\r\n        return loader\r\n\r\n    def val_dataloader(self):\r\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root='./data',test_mode=True), batch_size=100,\r\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\r\n        return loader\r\ndef ResNet18():\r\n    return ResNet(BasicBlock, [2, 2, 2, 2])\r\n```\r\ndataloader.py\r\n```python\r\nimport os,sys,math,random,pickle\r\nimport torch\r\nimport numpy as np\r\nimport torch.distributed as dist\r\ntry:\r\n    from nvidia import dali\r\n    from nvidia.dali.pipeline import Pipeline\r\n    import nvidia.dali.types as types\r\n    import nvidia.dali.fn as fn\r\n    import nvidia.dali.ops as ops\r\n    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\r\nexcept:\r\n    print('Could not import DALI')\r\nclass dali_DataLoader():\r\n    def __init__(self, pipelines, **kwargs):\r\n        pipelines.build()\r\n        try:\r\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, size=len(pipelines.iterator.indices))\r\n            self.sampler = pipelines.iterator\r\n        except:\r\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, reader_name='Reader')\r\n            self.sampler = self\r\n    def set_epoch(self,epoch):\r\n        pass\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __len__(self):\r\n        return int(math.ceil(self._dali_iterator._size / self._dali_iterator.batch_size))\r\n    def __next__(self):\r\n        \r\n        try:\r\n            data = next(self._dali_iterator)\r\n        except StopIteration:\r\n            self._dali_iterator.reset()\r\n            raise StopIteration\r\n        # Decode the data output\r\n        input = data[0]['data']\r\n        target = data[0]['label'].squeeze().long()\r\n\r\n        return input,target\r\nclass identity():\r\n    def __call__(self,x,*tmp,**kargs):\r\n        return x\r\nclass HybridPipe(Pipeline):\r\n    def __init__(self,dataset, batch_size, file_root=None,filelist_path=None,num_threads=1, pad_ratio=1,is_distribute=True, resize=None,crop_size=[0,0],ramdom_flip=False,normalize=None,random_rotate_degree=None):\r\n        device_id = torch.cuda.current_device()\r\n        print(\"device_id\",device_id)\r\n        super(HybridPipe, self).__init__(batch_size, num_threads, device_id, seed=12 + device_id)\r\n        \r\n        if is_distribute:\r\n            if filelist_path is not None:\r\n                if file_root is None:\r\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\r\n                else:\r\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\r\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\r\n                    self.use_file=True\r\n            else:\r\n                self.iterator = iter(Distribute_Input_Iter(dataset, batch_size))\r\n                #self.input = ops.ExternalSource(source=self.iterator, num_outputs=2)\r\n                self.input = ops.ExternalSource()\r\n                self.input_label = ops.ExternalSource()\r\n                self.use_file=False\r\n        else:\r\n            if filelist_path is not None:\r\n                if file_root is None:\r\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\r\n                else:\r\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\r\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\r\n                    self.use_file=True\r\n            else:\r\n                self.iterator = iter(Normal_Input_Iter(dataset, batch_size))\r\n                self.input = ops.ExternalSource()\r\n                self.input_label = ops.ExternalSource()\r\n                self.use_file=False\r\n        dali_device = \"gpu\"\r\n        \r\n        if isinstance(resize,(tuple,list)) and len(resize)==2:\r\n            self.resize = ops.Resize(size=tuple(resize))\r\n        elif isinstance(resize,(int, float)):\r\n            self.resize = ops.Resize(size=tuple(resize,resize))\r\n        else:\r\n            self.resize = identity()\r\n        if normalize is not None and isinstance(normalize,dict):\r\n            self.mean = normalize.get('mean',0)\r\n            self.std = normalize.get('std',1)\r\n        else:\r\n            self.mean = 0\r\n            self.std = 1\r\n        if isinstance(crop_size, (int, float)):\r\n            crop_size = [crop_size,crop_size]\r\n        if (len(crop_size)==2 and (crop_size[0]==0 or crop_size[1]==0)):\r\n            self.crop = identity()\r\n        else:\r\n            self.crop = ops.Crop(device=dali_device, crop_h=crop_size[0], crop_w=crop_size[1])\r\n        if pad_ratio>1:\r\n            self.pad = ops.Paste(device=dali_device, ratio=pad_ratio, fill_value=0)\r\n        else:\r\n            self.pad = identity()\r\n        self.cmnp = ops.CropMirrorNormalize(device=\"gpu\",\r\n                                            dtype=types.FLOAT,\r\n                                            output_layout=types.NCHW,\r\n                                            mean=self.mean,\r\n                                            std=self.std\r\n                                            )\r\n        if ramdom_flip:\r\n            self.coin = ops.random.CoinFlip(probability=0.5)\r\n        else:\r\n            self.coin = lambda :0\r\n        if random_rotate_degree is not None:\r\n            try:\r\n                tmp = math.abs(int(random_rotate_degree))\r\n                self.degree = ops.random.Uniform(range=(-tmp, tmp))\r\n                self.rotate = ops.Rotate()\r\n            except:\r\n                self.degree = lambda :0\r\n                self.rotate = identity()\r\n        else:\r\n            self.degree = lambda :0\r\n            self.rotate = identity()\r\n        \r\n    def iter_setup(self):\r\n        if not self.use_file:\r\n            (images, labels) = self.iterator.__next__()\r\n            self.feed_input(self.jpegs, images, layout=\"HWC\")\r\n            self.feed_input(self.labels, labels)\r\n\r\n    def define_graph(self):\r\n        rng = self.coin()\r\n        print()\r\n        if self.use_file:\r\n            self.jpegs,self.labels = self.input(name=\"Reader\")\r\n            self.jpegs = self.decode(self.jpegs)\r\n        else:\r\n            self.jpegs= self.input()\r\n            self.labels = self.input_label()\r\n        output = self.jpegs\r\n        output = self.resize(output)\r\n        output = self.rotate(output, angle=self.degree())\r\n        output = self.pad(output.gpu())\r\n        output = self.crop(output)\r\n        output = self.cmnp(output, mirror=rng)\r\n        return [output, self.labels]\r\nclass Distribute_Input_Iter():\r\n    def __init__(self,dataset, batch_size, num_replicas=None,rank=None,shuffle=True,seed=0,drop_last=False):\r\n        if num_replicas is None:\r\n            if not dist.is_available():\r\n                raise RuntimeError(\"Requires distributed package to be available\")\r\n            num_replicas = dist.get_world_size()\r\n            #num_replicas = 1\r\n        if rank is None:\r\n            if not dist.is_available():\r\n                raise RuntimeError(\"Requires distributed package to be available\")\r\n            rank = dist.get_rank()\r\n            #rank = 0\r\n        if rank >= num_replicas or rank < 0:\r\n            raise ValueError(\r\n                \"Invalid rank {}, rank should be in the interval\"\r\n                \" [0, {}]\".format(rank, num_replicas - 1))\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.num_replicas = num_replicas\r\n        self.rank = rank\r\n        self.epoch = 0\r\n        self.drop_last = drop_last\r\n        \r\n        # If the dataset length is evenly divisible by # of replicas, then there\r\n        # is no need to drop any data, since the dataset will be split equally.\r\n        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore\r\n            # Split to nearest available length that is evenly divisible.\r\n            # This is to ensure each rank receives the same amount of data when\r\n            # using this Sampler.\r\n            self.num_samples = math.ceil(\r\n                # `type:ignore` is required because Dataset cannot provide a default __len__\r\n                # see NOTE in pytorch/torch/utils/data/sampler.py\r\n                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore\r\n            )\r\n        else:\r\n            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore\r\n        self.total_size = self.num_samples * self.num_replicas\r\n        self.shuffle = shuffle\r\n        self.seed = seed\r\n        self.epoch=0\r\n        indices = list(range(len(self.dataset)))  # type: ignore\r\n\r\n        if not self.drop_last:\r\n            # add extra samples to make it evenly divisible\r\n            padding_size = self.total_size - len(indices)\r\n            if padding_size <= len(indices):\r\n                indices += indices[:padding_size]\r\n            else:\r\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\r\n        else:\r\n            # remove tail of data to make it evenly divisible.\r\n            indices = indices[:self.total_size]\r\n        assert len(indices) == self.total_size,'len(indices) != self.total_size'\r\n\r\n        # subsample\r\n        indices = indices[self.rank:self.total_size:self.num_replicas]\r\n        assert len(indices) == self.num_samples,'len(indices) != self.num_samples'\r\n        self.indices = indices\r\n    def set_epoch(self,epoch):\r\n        self.epoch = epoch\r\n    def __iter__(self):\r\n        self.i = 0\r\n        self.n = len(self.indices)\r\n        return self\r\n    def __next__(self):\r\n        batch = []\r\n        labels = []\r\n        should_shuffle = False\r\n        \r\n        for _ in range(self.batch_size):\r\n            if self.i % self.n == self.n-1:\r\n                should_shuffle = True\r\n            img, label = self.dataset.__getitem__(self.indices[self.i])\r\n            batch.append(img)\r\n            labels.append(label)\r\n            self.i = (self.i + 1) % self.n\r\n        if should_shuffle:\r\n            random.shuffle(self.indices)\r\n        return (batch, labels)\r\nclass Normal_Input_Iter():\r\n    def __init__(self,dataset, batch_size):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.indices = list(range(len(self.dataset)))\r\n    def __iter__(self):\r\n        self.i = 0\r\n        self.n = len(self.dataset)\r\n        return self\r\n    def __next__(self):\r\n        batch = []\r\n        labels = []\r\n        should_shuffle = False\r\n        \r\n        for _ in range(self.batch_size):\r\n            if self.i % self.n == self.n-1:\r\n                should_shuffle = True\r\n            img, label = self.dataset.__getitem__(self.indices[self.i])\r\n            batch.append(img)\r\n            labels.append(label)\r\n            self.i = (self.i + 1) % self.n\r\n        if should_shuffle:\r\n            random.shuffle(self.indices)\r\n        return (batch, labels)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/7082",
    "createdAt": "2021-04-13T08:35:45Z",
    "updatedAt": "2022-06-13T13:37:18Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "yllgl"
    },
    "answer": {
      "body": "When you compare the two implementations, make sure to leave out as many changing variables as possible. For example, since you train with DDP, run it only on 2 GPUs so that you can be sure it's not bottlenecked by CPU. I don't know the Dali data loader very well, but I doubt that they can guarantee a throughput increase for all use cases. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-18T12:14:27Z"
    }
  },
  {
    "title": "Can a pl.LightningModule be used from native pytorch?",
    "body": "Can a pl.LightningModule be used from native pytorch?\r\n\r\nWe are writing a library, and the use of pl.LightningModule for our modules is convenient, particularly because each module knows its device.\r\n\r\nHowever, our clients might be using native pytorch, and want to include our LightningModule as an nn.Module in their code.\r\n\r\nFWIW, our LightningModule currently is used *purely* for forward inference and currently passes no gradients, nor is it trainable.\r\n\r\n1) Are there any interoperability pitfalls in having a LightningModule be an nn.Module in a pure pytorch codebase?\r\n2) Are the benefits gained by using pl.LightningModules in our codebase no longer relevant when called from a pure pytorch codebase, particularly given that we pass back no gradients?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6953",
    "createdAt": "2021-04-11T05:12:31Z",
    "updatedAt": "2025-03-02T18:10:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "turian"
    },
    "answer": {
      "body": "1.  It should work as a native `nn.Module`, it actually subclasses it. If you find any, you can assume it's a bug so feel free to open issues about any pitfalls found.\r\n2. Depends on which features of the `LightningModule` you value the most. The benefits left would be mainly organization and such.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-15T22:32:31Z"
    }
  },
  {
    "title": "How to do this test in a lightning way?",
    "body": "My model has the property that I can prepare the test data in multiple different ways, which results in a set of equally plausible predictions for each data point (one prediction for each way of preparing the test data). By combining these predictions, it is possible to slightly boost overall performance on the test set. Right now, I do this in the following (abstract) way:\r\n\r\n------------------------------------------------------------------\r\n\r\n```\r\nfor iPrep in range(nPrep):\r\n   preppedData=prepare_data(testData,iPrep)\r\n   predictions[iPrep]=trainer.test(model,preppedData)\r\n\r\nfinal_predictions=combinePredictions(predictions)\r\n```\r\n\r\n------------------------------------------------------------------\r\n(obviously it is much longer in reality)\r\n\r\nis there a proper, 'lightning' way of hiding this loop inside the model, so I can still use the trainer for this, but only call it once? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6938",
    "createdAt": "2021-04-09T20:07:10Z",
    "updatedAt": "2021-06-03T21:54:03Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kaare-mikkelsen"
    },
    "answer": {
      "body": "Maybe the prediction api can help you (currently beta, will be released in version 1.3).\r\n\r\nYou can have multiple predict dataloaders (your different test data). If you do\r\n\r\n```python\r\npredictions = trainer.predict(model, predict_dataloaders=[data1, data2, data3, ...])\r\n```\r\nand it returns the predictions grouped by the dataloader index. Then you can combine them with your own function.\r\n\r\n```python\r\nfinal_predictions=combinePredictions(predictions)\r\n```\r\n\r\nNot sure if this is 100% what you are looking for, but it can at least eliminate that one for loop you have.\r\nOptionally, you can also override `predict_step` in the LightningModule. \r\nIf you install the latest version, you can use this predict feature already. The documentation will be included in the 1.3 release.\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-22T22:22:32Z"
    }
  },
  {
    "title": "Problems in Pruning",
    "body": "Hello, I am trying to get pruning to work within my lightning model. I have tried multiple methods, but have not been able to get the ModelPruning module to work. \r\n\r\nWhen I try this:\r\n\r\n````\r\ndef compute_amount(epoch):\r\n    # the sum of all returned values need to be smaller than 1\r\n    if epoch == 10:\r\n        return 0.5\r\n\r\n    elif epoch == 50:\r\n        return 0.25\r\n\r\n    elif 75 < epoch < 99 :\r\n        return 0.01\r\n    else:\r\n      return 0\r\n\r\nprune = ModelPruning(\r\n            pruning_fn='l1_unstructured',\r\n            parameters_to_prune=[(model.model.conv1, 'weight'),\r\n                            (model.model.conv2, 'weight'),\r\n                            (model.model.lin1, 'weight'),\r\n                            (model.model.lin2, 'weight'),\r\n                            (model.model.upconv1, 'weight'),\r\n                            (model.model.upconv2, 'weight')],\r\n            amount=compute_amount,\r\n            use_global_unstructured=True,\r\n        )\r\n\r\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\ntrainer.fit(model, dm)\r\n````\r\n\r\nI get this output:\r\n\r\n> MisconfigurationException                 Traceback (most recent call last)\r\n> <ipython-input-27-327a57ef597c> in <module>()\r\n>      11 \r\n>      12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\n> ---> 13 trainer.fit(model, dm)\r\n> \r\n> 4 frames\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n>     457         # ----------------------------\r\n>     458         self.call_setup_hook(model)\r\n> --> 459         self.call_hook(\"on_before_accelerator_backend_setup\", model)\r\n>     460         self.accelerator.setup(self, model)  # note: this sets up self.lightning_module\r\n>     461 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n>    1089             if hasattr(self, hook_name):\r\n>    1090                 trainer_hook = getattr(self, hook_name)\r\n> -> 1091                 trainer_hook(*args, **kwargs)\r\n>    1092 \r\n>    1093             # next call hook in lightningModule\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_before_accelerator_backend_setup(self, model)\r\n>      33         \"\"\"Called in the beginning of fit and test\"\"\"\r\n>      34         for callback in self.callbacks:\r\n> ---> 35             callback.on_before_accelerator_backend_setup(self, model)\r\n>      36 \r\n>      37     def setup(self, model, stage: str):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_before_accelerator_backend_setup(self, trainer, pl_module)\r\n>     360     def on_before_accelerator_backend_setup(self, trainer, pl_module: LightningModule):\r\n>     361         parameters_to_prune = self.sanitize_parameters_to_prune(\r\n> --> 362             pl_module, self._parameters_to_prune, parameter_names=self._parameter_names\r\n>     363         )\r\n>     364 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in sanitize_parameters_to_prune(pl_module, parameters_to_prune, parameter_names)\r\n>     441             if missing_modules or missing_parameters:\r\n>     442                 raise MisconfigurationException(\r\n> --> 443                     \"Some provided `parameters_to_tune` don't exist in the model.\"\r\n>     444                     f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n>     445                 )\r\n> \r\n> MisconfigurationException: Some provided `parameters_to_tune` don't exist in the model. Found missing modules: [Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Linear(in_features=784, out_features=32, bias=True), Linear(in_features=32, out_features=784, bias=True), ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))] and missing parameters: []\r\n\r\nWhich makes it seem like ModelPruning is not finding my model's weights, though I do not understand why. If I put into ModelPruning weights that do not exist in the model. It catches that problem during ModelPruning's init. Which is not happening here. So that does not seem to be the problem.  \r\n\r\nI have also tried bypassing the parameters_to_prune parameter altogether. \r\n\r\n````\r\ndef compute_amount(epoch):\r\n    # the sum of all returned values need to be smaller than 1\r\n    if epoch == 10:\r\n        return 0.5\r\n\r\n    elif epoch == 50:\r\n        return 0.25\r\n\r\n    elif 75 < epoch < 99 :\r\n        return 0.01\r\n    else:\r\n      return 0\r\n\r\nprune = ModelPruning(\r\n            pruning_fn='l1_unstructured',\r\n            parameter_names=['weight', 'bias'],\r\n            amount=compute_amount,\r\n            use_global_unstructured=True,\r\n        )\r\n\r\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\ntrainer.fit(model, dm)\r\n````\r\n\r\n\r\nThis works right until it does not. One epoch runs and then it snags on a callback within the validation step. \r\n\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n>     636                     # run train epoch\r\n> --> 637                     self.train_loop.run_training_epoch()\r\n>     638 \r\n> \r\n> 56 frames\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n>     576         if should_check_val:\r\n> --> 577             self.trainer.run_evaluation(on_epoch=True)\r\n>     578 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, max_batches, on_epoch)\r\n>     750         # hook\r\n> --> 751         self.evaluation_loop.on_evaluation_end()\r\n>     752 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_end(self, *args, **kwargs)\r\n>      99         else:\r\n> --> 100             self.trainer.call_hook('on_validation_end', *args, **kwargs)\r\n>     101 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n>    1090                 trainer_hook = getattr(self, hook_name)\r\n> -> 1091                 trainer_hook(*args, **kwargs)\r\n>    1092 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_validation_end(self)\r\n>     184         for callback in self.callbacks:\r\n> --> 185             callback.on_validation_end(self, self.lightning_module)\r\n>     186 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\r\n>     211         \"\"\"\r\n> --> 212         self.save_checkpoint(trainer, pl_module)\r\n>     213 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\r\n>     261         # Mode 2: save the last checkpoint\r\n> --> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\r\n>     263 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\r\n>     545         else:\r\n> --> 546             self._save_model(last_filepath, trainer, pl_module)\r\n>     547         if (\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\r\n>     334         if self.save_function is not None:\r\n> --> 335             self.save_function(filepath, self.save_weights_only)\r\n>     336         else:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\r\n>     326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\r\n> --> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n>     328 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\r\n>     396         # dump states as a checkpoint dictionary object\r\n> --> 397         checkpoint = self.dump_checkpoint(weights_only)\r\n>     398         if self.trainer.is_global_zero:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\r\n>     283             # dump callbacks\r\n> --> 284             checkpoint['callbacks'] = self.trainer.on_save_checkpoint(checkpoint)\r\n>     285 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\r\n>     221             else:\r\n> --> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\r\n>     223             if state:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\r\n>     399             # prune a copy so training can continue with the same buffers\r\n> --> 400             copy = deepcopy(pl_module.to(\"cpu\"))\r\n>     401             self.make_pruning_permanent(copy)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     168                     if reductor:\r\n> --> 169                         rv = reductor(4)\r\n>     170                     else:\r\n> \r\n> TypeError: cannot serialize '_io.TextIOWrapper' object\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-29-327a57ef597c> in <module>()\r\n>      11 \r\n>      12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\n> ---> 13 trainer.fit(model, dm)\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n>     497 \r\n>     498         # dispath `start_training` or `start_testing` or `start_predicting`\r\n> --> 499         self.dispatch()\r\n>     500 \r\n>     501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n>     544 \r\n>     545         else:\r\n> --> 546             self.accelerator.start_training(self)\r\n>     547 \r\n>     548     def train_or_test_or_predict(self):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n>      71 \r\n>      72     def start_training(self, trainer):\r\n> ---> 73         self.training_type_plugin.start_training(trainer)\r\n>      74 \r\n>      75     def start_testing(self, trainer):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n>     112     def start_training(self, trainer: 'Trainer') -> None:\r\n>     113         # double dispatch to initiate the training loop\r\n> --> 114         self._results = trainer.run_train()\r\n>     115 \r\n>     116     def start_testing(self, trainer: 'Trainer') -> None:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n>     667         finally:\r\n>     668             # hook\r\n> --> 669             self.train_loop.on_train_end()\r\n>     670 \r\n>     671     def run_evaluation(self, max_batches=None, on_epoch=False):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in on_train_end(self)\r\n>     132         # when a checkpoint was saved at the last step\r\n>     133         self.trainer.global_step -= 1\r\n> --> 134         self.check_checkpoint_callback(should_update=True, is_last=True)\r\n>     135         self.trainer.global_step += 1\r\n>     136 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in check_checkpoint_callback(self, should_update, is_last)\r\n>     162 \r\n>     163             for cb in callbacks:\r\n> --> 164                 cb.on_validation_end(self.trainer, model)\r\n>     165 \r\n>     166     def check_early_stopping_callback(self, should_update):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\r\n>     210         checkpoints can be saved at the end of the val loop\r\n>     211         \"\"\"\r\n> --> 212         self.save_checkpoint(trainer, pl_module)\r\n>     213 \r\n>     214     def on_save_checkpoint(self, trainer, pl_module, checkpoint: Dict[str, Any]) -> Dict[str, Any]:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\r\n>     260 \r\n>     261         # Mode 2: save the last checkpoint\r\n> --> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\r\n>     263 \r\n>     264     def __validate_init_configuration(self):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\r\n>     544             trainer.training_type_plugin.rpc_save_model(self._save_model, last_filepath, trainer, pl_module)\r\n>     545         else:\r\n> --> 546             self._save_model(last_filepath, trainer, pl_module)\r\n>     547         if (\r\n>     548             self.last_model_path and self.last_model_path != last_filepath\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\r\n>     333         # delegate the saving to the trainer\r\n>     334         if self.save_function is not None:\r\n> --> 335             self.save_function(filepath, self.save_weights_only)\r\n>     336         else:\r\n>     337             raise ValueError(\".save_function() not set\")\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\r\n>     325 \r\n>     326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\r\n> --> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n>     328 \r\n>     329     @property\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\r\n>     395         \"\"\"\r\n>     396         # dump states as a checkpoint dictionary object\r\n> --> 397         checkpoint = self.dump_checkpoint(weights_only)\r\n>     398         if self.trainer.is_global_zero:\r\n>     399             # write the checkpoint dictionary on the file\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\r\n>     282         if not weights_only:\r\n>     283             # dump callbacks\r\n> --> 284             checkpoint['callbacks'] = self.trainer.on_save_checkpoint(checkpoint)\r\n>     285 \r\n>     286             optimizer_states = []\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\r\n>     220                 state = callback.on_save_checkpoint(self, self.lightning_module)  # noqa: parameter-unfilled\r\n>     221             else:\r\n> --> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\r\n>     223             if state:\r\n>     224                 callback_states[type(callback)] = state\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\r\n>     398             prev_device = pl_module.device\r\n>     399             # prune a copy so training can continue with the same buffers\r\n> --> 400             copy = deepcopy(pl_module.to(\"cpu\"))\r\n>     401             self.make_pruning_permanent(copy)\r\n>     402             checkpoint[\"state_dict\"] = copy.state_dict()\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     167                     reductor = getattr(x, \"__reduce_ex__\", None)\r\n>     168                     if reductor:\r\n> --> 169                         rv = reductor(4)\r\n>     170                     else:\r\n>     171                         reductor = getattr(x, \"__reduce__\", None)\r\n> \r\n> TypeError: cannot serialize '_io.TextIOWrapper' object\r\n\r\n\r\nI am a bit overwhelmed by that error. Does anyone know what I am doing wrong? Or have I found a bug?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6932",
    "createdAt": "2021-04-09T17:19:45Z",
    "updatedAt": "2022-06-13T19:48:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "joshclancy"
    },
    "answer": {
      "body": "For the first error, what does `pl_module.modules()` return?\r\n\r\nYou get the error because the following code can't find the parameters you passed in your model:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4c07ab5e99dd20c1f309d9e73cdaacc1ebad9499/pytorch_lightning/callbacks/pruning.py#L422-L444\r\n\r\nThe second error is definitely a bug. It happens because you are also using a profiler. Should be fixed in master, the fix will be released in 1.3. Feel free to try it on master in the meantime.",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-15T22:41:09Z"
    }
  },
  {
    "title": "How to Log Metrics (eg. Validation Loss, Accuracy) To TensorBoard Hparams?",
    "body": "I am using Pytorch Lightning 1.2.6 to train my models using DDP and TensorBoard is the default logger used by Lightning.\r\n\r\nMy code is setup to log the training and validation loss on each training and validation step respectively.\r\n\r\n```py\r\nclass MyLightningModel(pl.LightningModule):\r\n\r\n    def training_step(self, batch):\r\n        x, labels = batch\r\n        out = self(x)\r\n        loss = F.mse_loss(out, labels)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch):\r\n        x, labels = batch\r\n        out = self(x)\r\n        loss = F.mse_loss(out, labels)\r\n        self.log(\"val_loss\", loss)\r\n        return loss\r\n```\r\n\r\nTensorBoard correctly plots both the `train_loss` and `val_loss` charts in the `SCALERS` tab. However, in the `HPARAMS` tab, on the left side bar, only `hp_metric` is visible under `Metrics`. \r\n\r\n[![enter image description here][1]][1]\r\n\r\nHowever, in the `HPARAMS` tab, on the left side bar, only `hp_metric` is visible under `Metrics`. \r\n\r\n[![enter image description here][2]][2]\r\n\r\nHow can we add `train_loss` and `val_loss` to the `Metrics` section? This way, we will be able to use `val_loss` in the `PARALLEL COORDINATES VIEW` instead of `hp_metric`.\r\n\r\n**Image showing `hp_metric` and no `val_loss`:**\r\n[![enter image description here][3]][3]\r\n\r\n*Using Pytorch 1.8.1, Pytorch Lightning 1.2.6, TensorBoard 2.4.1*\r\n\r\n  [1]: https://i.stack.imgur.com/x9L17.png\r\n  [2]: https://i.stack.imgur.com/4lECK.png\r\n  [3]: https://i.stack.imgur.com/05U6a.png",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6904",
    "createdAt": "2021-04-08T20:39:54Z",
    "updatedAt": "2022-06-02T21:00:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "athenawisdoms"
    },
    "answer": {
      "body": "I think it is explained very well in this section of the documentation:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-hyperparameters\r\nBasically, you just need to overwrite the `hp_metric` tag with whatever value you want to show up in the `HPARAMS` tab in tensorboard.\r\n\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-04-09T08:52:13Z"
    }
  },
  {
    "title": "How to use Accuracy with ignore class?",
    "body": "Please see same question on[ Stack Overflow](https://stackoverflow.com/questions/67002099/how-to-use-pytorch-lightning-accuracy-with-ignore-class).\r\n\r\nWhen using `Accuracy` with a class that should be ignored, meaning it has labels but can never be predicted, the scoring is wrong, because it is calculated with the never predicted labels that should be ignored.\r\n\r\nHow to use Accuracy while ignoring some class?\r\n\r\n\r\nThanks :)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6890",
    "createdAt": "2021-04-08T10:39:40Z",
    "updatedAt": "2022-06-09T06:15:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "noamzilo"
    },
    "answer": {
      "body": "It is currently not supported in the accuracy metric, but we have an open PR for implementing that exact feature https://github.com/PyTorchLightning/metrics/pull/155\r\n\r\nCurrently what you can is instead calculate the confusion matrix and then ignore some classes based on that (remember that the true positive/correctly classified are found on the diagonal of the confusion matrix):\r\n```python\r\nignore_index = 3\r\nmetric = ConfusionMatrix(num_classes=3)\r\nconfmat = metric(preds, target)\r\nconfmat = confmat[:2,:2] # remove last column and row corresponding to class 3\r\nacc = confmat.trace() / confmat.sum()\r\n```\r\n\r\n    \r\n\r\n\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-04-09T08:30:09Z"
    }
  },
  {
    "title": "why load_from_checkpoint is not defaultly inplace?",
    "body": "Why\r\n```\r\nnew_model = MyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\r\n```\r\ninstead of\r\n```\r\nMyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\r\n```\r\nThe former one is not friendly to native PyTorch users. And cost me an afternoon to find the bug.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6888",
    "createdAt": "2021-04-08T10:15:25Z",
    "updatedAt": "2022-09-07T18:39:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MagicFrogSJTU"
    },
    "answer": {
      "body": "`load_from_checkpoint` is a class function which instantiates the object with the `hyper_parameters` in the checkpoint and then loads the state_dict.\r\nIf done like in your proposal `my_model.load_from_checkpoint(checkpoint_path=\"example.ckpt\")` similar to pytorch's `load_state_dict` you'd have to create the `my_model` object first with the `hyper_parameters` parsed from the checkpoint and then load the weights. `load_from_checkpoint` combines both.\r\nMaybe adding a way to load the `state_dict` from a checkpoint into an already instantiated object without using a classmethod would make sense. Something like `load_state_dict_from_checkpoint`. \r\nThe alternative is to load the checkpoint and then call `model.load_state_dict(checkpoint['state_dict'], strict=strict)`.\r\n",
      "author": {
        "login": "FlorianMF"
      },
      "createdAt": "2021-04-30T09:21:31Z"
    }
  },
  {
    "title": "cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected) Error",
    "body": "I am trying to run trainer.fit with only models passed. However, I am getting the following error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 116, in <module>\r\n    main_func(args)\r\n  File \"train.py\", line 55, in main_func\r\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 443, in fit\r\n    self.model_connector.copy_trainer_model_properties(model)\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/model_connector.py\", line 39, in copy_trainer_model_properties\r\n    m.precision = self.trainer.precision\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 982, in __setattr__\r\n    raise TypeError(\"cannot assign '{}' as child module '{}' \"\r\nTypeError: cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected)\r\n```\r\n\r\n\r\nI am unable to figure this out. Any help would be appreciated\r\n\r\n```py\r\n    trainer = Trainer(\r\n        logger=logger, resume_from_checkpoint=args.resume_from_checkpoint, max_epochs=args.num_epochs,\r\n        accumulate_grad_batches=args.gradient_accumulation_steps, \r\n        default_root_dir=args.experiment_path, checkpoint_callback=ckpt, num_sanity_val_steps=args.num_sanity_val_steps, \r\n        gradient_clip_val=args.gradient_clip_val, gpus=1, auto_select_gpus=True, sync_batchnorm=args.sync_batchnorm\r\n    )\r\n                        \r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    criterion_val = torch.nn.CrossEntropyLoss()\r\n    \r\n    model = Model(args, tr_dl=train_dataloader, val_dl=val_dataloader, test_dl=test_dataloader, criterion=criterion, criterion_val=criterion_val)\r\n    trainer.fit(model)\r\n```\r\nHere precision value is the default.\r\n\r\nThank You",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6883",
    "createdAt": "2021-04-08T00:52:07Z",
    "updatedAt": "2022-06-05T07:41:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "gaps013"
    },
    "answer": {
      "body": "Do you have any properties in your `Model` class named `precision` ? ",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-04-08T01:14:21Z"
    }
  },
  {
    "title": "Questions and problems with different precisions",
    "body": "Hi,\r\n\r\nI try to implement an autoencoder where the latent space is not euclidean but has hyperbolic and spherical geometry. I use geoopt for this. They recommend to use double precision for numerical stability. When I run my LightningModule with `precision=32` I encounter NaNs in the training. When I change to `precision=64` I get this pickling error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 494, in run_training_epoch\r\n    self.check_checkpoint_callback(True)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\r\n    cb.on_validation_end(self.trainer, model)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\r\n    self.save_checkpoint(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\r\n    self._save_model(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\r\n    self._do_save(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\r\n    self.save_function(filepath, self.save_weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\r\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\r\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\r\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\r\n    atomic_save(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\r\n    torch.save(checkpoint, bytesbuffer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\r\n    pickler.dump(obj)\r\n_pickle.PicklingError: Can't pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it's not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 40, in <module>\r\n    trainer.fit(module, datamodule=datamodule)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 480, in fit\r\n    self.dispatch()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 523, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 95, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 142, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 535, in run_stage\r\n    self.run_train()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 634, in run_train\r\n    self.train_loop.on_train_end()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 117, in on_train_end\r\n    self.check_checkpoint_callback(should_update=True, is_last=True)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\r\n    cb.on_validation_end(self.trainer, model)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\r\n    self.save_checkpoint(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\r\n    self._save_model(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\r\n    self._do_save(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\r\n    self.save_function(filepath, self.save_weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\r\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\r\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\r\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\r\n    atomic_save(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\r\n    torch.save(checkpoint, bytesbuffer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\r\n    pickler.dump(obj)\r\n_pickle.PicklingError: Can't pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it's not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\r\nException ignored in: <function tqdm.__del__ at 0x7f086346a430>\r\nTraceback (most recent call last):\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1145, in __del__\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1299, in close\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1492, in display\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1148, in __str__\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1450, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n```\r\n\r\nI think this error comes from inconsistencies in my Module. But I do not know how to find them. Do you have any tips to track down the inconsistencies that disturb pickling?\r\n\r\nHowever, when I run with `precision=16` I do not run into NaNs. As this precision should be more numerically unstable than `precision=32` my only explanation for this is that there is automatic gradient clipping going on or so in 16-bit precision. Is this correct? And where do I find these values? Then I could apply them to 32-bit precision training.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6851",
    "createdAt": "2021-04-06T12:32:32Z",
    "updatedAt": "2022-07-07T11:41:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "gatoniel"
    },
    "answer": {
      "body": "Hi @gatoniel, not sure about the NaNs but the pickle error definitely looks like a bug on our end. The double precision plugin patches the `training_step` which would explain that error. Mind opening an issue for it?",
      "author": {
        "login": "ethanwharris"
      },
      "createdAt": "2021-04-06T13:33:26Z"
    }
  },
  {
    "title": "How to set Checkpoints to be used in the automatically generated `version_N` directories?",
    "body": "If the TensorBoard logger is set up as shown\r\n\r\n```py\r\nlogger = TensorBoardLogger(name=\"MyModel\")\r\ncheckpoint_callback = ModelCheckpoint(\r\n    filename=\"{epoch}-{step}-{val_loss:.2f}\",\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntrainer = pl.Trainer(\r\n    default_root_dir=ROOT_DIR,\r\n    callbacks=[checkpoint_callback],\r\n    logger=[logger],\r\n)\r\n```\r\n\r\nhow do we configure the checkpoints to be written to directories that are automatically named `version_0`, `version_1`, the way it is if you do not pass a logger to `Trainer`?\r\n\r\n```py\r\ntrainer = pl.Trainer(\r\n    default_root_dir=ROOT_DIR,\r\n    callbacks=[checkpoint_callback],\r\n)\r\n```\r\nIf we pass in a logger to `Trainer`, the checkpoints are written to\r\n\r\n    <root_path>/<experiment_name>/<integer>/checkpoints\r\n\r\nwhile the tensorboard logs and `hparams.yaml` are written to\r\n\r\n    <root_path>/<experiment_name>/version_<integer>/\r\n\r\nIf we do not pass in a logger to `Trainer`, then checkpoint files, Tensorboard files and `hparams.yaml` are all written to the same directory\r\n\r\n    <root_path>/<experiment_name>/version_<integer>/\r\n\r\nHow can both checkpoints and tensorboard files we written to the same `version_<integer>` directory?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6821",
    "createdAt": "2021-04-04T17:37:26Z",
    "updatedAt": "2023-07-06T08:02:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nyxynyx"
    },
    "answer": {
      "body": "Hi\r\nFor this you need to set the \"default_root_dir\" in the Trainer, and set the save_dir of the Logger to the same.\r\n\r\nThis works for me (latest PL version):\r\n\r\n```python \r\n\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"LitClassifier\")\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parent_parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args, num_workers=2)\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n\r\n    ROOT_DIR = \"here\"\r\n    mylogger = TensorBoardLogger(name=\"MyModel\", save_dir=ROOT_DIR)\r\n    ckpt_callback = ModelCheckpoint(monitor=\"valid_loss\", filename=\"{epoch}-{step}-{valid_loss:.2f}\")\r\n    trainer = pl.Trainer.from_argparse_args(args, default_root_dir=ROOT_DIR, logger=mylogger, callbacks=[ckpt_callback], limit_train_batches=2, limit_val_batches=2)\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522562-ce660d80-95a1-11eb-8352-2701158a172f.png)\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T21:59:51Z"
    }
  },
  {
    "title": "How to train two optimizer with one loss?",
    "body": "I have read the GAN [Demo](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=ArrPXFM371jR), it is for two losses.\r\n\r\nSuppose I have two modules A and B. The training_step is:\r\n```\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        x, y = batch\r\n        a1, a2 = self.A(x[0])\r\n        preds = self.B(x[1], a1, a2)\r\n        loss = loss_fn(preds, y)\r\n        return loss\r\n```\r\nHow to train A and B with different optimizer or learning rate?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6818",
    "createdAt": "2021-04-04T12:35:10Z",
    "updatedAt": "2022-07-14T12:50:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stdoo"
    },
    "answer": {
      "body": "The best is to use manual optimization: \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nThen you have control over each optimizer step. \r\n\r\nYou can get the optimzers with self.optimizers()\r\nand for backward you simply have to replace loss.backward() with self.manual_backward(loss)\r\nMake sure you use the latest version of Lightning for that for optimal support.\r\n\r\nHope this helps",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T21:42:47Z"
    }
  },
  {
    "title": "Missing Saved Checkpoints when using Multiple Loggers",
    "body": "Hello Lightning gods!\r\n\r\nWhen using a single logger (without specifying a logger when creating the `Trainer`) and also using a `ModelCheckpoint` callback to automatically save good checkpoints, everything works as expected: The directory created by TensorBoard at `<path_to_project>/lightning_logs/<experiment_name>/version_<N>` contains `hparams.yaml` and a directory `checkpoints` containing several `.ckpt` files.\r\n\r\n```py\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback],\r\n)\r\n```\r\n\r\nHowever, when I added a second logger, the `checkpoints` directory is no longer find inside the `lightning_logs` subdirectories. These subdirectories created by TensorBoard only contains `hparams.yaml` and a `events.out.tfevents.*` file.\r\n\r\n```py\r\nfrom aim.pytorch_lightning import AimLogger\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntb_logger = TensorBoardLogger(\"lightning_logs\", name=\"my_experiment\")\r\naim_logger = AimLogger(experiment=\"my_experiment\")\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback],\r\n    logger=[aim_logger, tb_logger],\r\n)\r\n```\r\nHow can we configure Lightning to also include the checkpoint files in the TensorBoard log directories, just like in the original case?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6809",
    "createdAt": "2021-04-03T19:40:06Z",
    "updatedAt": "2023-01-13T11:39:33Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "athenawisdoms"
    },
    "answer": {
      "body": "Hi\r\nWhen using multiple loggers, there is no canonical way to store the checkpoints in subdirs. What Lightning currently does is put the checkpoints one level above in a directory with the names of the loggers concatenated:\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522729-5567b580-95a3-11eb-816b-b73e456bb448.png)\r\n\r\nThis is very reasonable, since both loggers were used to produce the same checkpoints. \r\nThere are alternatives, for example saving the checkpoints in the experiment dir of the first logger in the list, or copy the checkpoints to both subdirs, but this is not implemented. \r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T22:16:41Z"
    }
  },
  {
    "title": "Error when training: \"closure_loss\" is NoneType",
    "body": "Hi all, \r\n\r\nI am trying to train my LightningModule but I seem to keep getting the error `TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'` on the line `closure_loss = closure_loss / self.accumulate_grad_batches`, in the function `training_step()` in the file training_loop.py. \r\n\r\nI think it might be something to do with how I format my LightningModule, so here is what my LightningModule looks like\r\n\r\n```python\r\nclass HPAModelV1(pl.LightningModule):\r\n  def __init__(self):\r\n    super().__init__()\r\n\r\n    #self.lossfunc = F.cross_entropy\r\n    self.lossfunc = F.nll_loss\r\n\r\n    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=3, padding=7)\r\n    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\r\n    self.conv3 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=1)\r\n    self.dense = nn.Linear(16, 19)\r\n\r\n  def forward(self, x): #input size is (256, 3, 256, 256)\r\n\r\n    x = x.float()\r\n    \r\n    out = self.conv1(x)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 30, 30)\r\n    \r\n    out = self.conv2(out)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 10, 10)\r\n\r\n    out = self.conv3(out)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 8) # output is (bs, 16, 1, 1)\r\n\r\n    # dense layer\r\n    out = out.reshape(out.size()[0], 16)\r\n    out = self.dense(out)\r\n\r\n    return out\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n    return optimizer \r\n\r\n\r\n  def training_step(self, batch, batchidx):\r\n    # set labels and data\r\n    x = batch[0]\r\n    y = batch[1]\r\n    \r\n\r\n    # compute loop\r\n    preds = self(x)\r\n  \r\n  \r\n    probs = F.softmax(preds, dim=1)\r\n \r\n\r\n    # compute the loss function\r\n    J = self.lossfunc(probs, y)\r\n \r\n   \r\n\r\n    # compute accuracy \r\n    acc = accuracy(probs, y)\r\n\r\n    \r\n    #log for weights and biases\r\n    self.log('training loss (step)', J)\r\n    self.log('training accuracy (step)', acc)\r\n    self.log('mean training loss (epoch)', J, on_step=False, on_epoch=True)\r\n    self.log('mean training accuracy (epoch)', acc, on_step=False, on_epoch=True)\r\n\r\n\r\n\r\n    # add information to the progress bar\r\n    pbar =  {'train_acc': acc, 'train_loss' : J}\r\n\r\n    return J, acc\r\n\r\n  def validation_step(self, valbatch, valbatchidx):\r\n    # use the same training step on the val set\r\n\r\n    valJ, valAcc = self.training_step(valbatch, valbatchidx)\r\n\r\n    # log for wb\r\n    self.log('validation loss (step)', valJ)\r\n    self.log('validation accuracy (step)', valAcc)\r\n    self.log('mean validation loss (epoch)', valJ, on_step=False, on_epoch=True)\r\n    self.log('mean validation accuracy (epoch)', valAcc, on_step=False, on_epoch=True)\r\n\r\n    return valJ, valAcc\r\n\r\n  def validation_epoch_end(self, valStepOutputs):\r\n    pass\r\n   ```\r\n\r\nAnd if it may help in diagnosing the cause of the issue, here is the stack trace and output of of the Trainer:\r\n\r\n```\r\nGPU available: False, used: False\r\nTPU available: True, using: 1 TPU cores\r\nGlobal seed set to 0\r\n\r\n  | Name  | Type   | Params\r\n---------------------------------\r\n0 | conv1 | Conv2d | 448   \r\n1 | conv2 | Conv2d | 2.3 K \r\n2 | conv3 | Conv2d | 6.4 K \r\n3 | dense | Linear | 323   \r\n---------------------------------\r\n9.5 K     Trainable params\r\n0         Non-trainable params\r\n9.5 K     Total params\r\n0.038     Total estimated model params size (MB)\r\nEpoch 0: 0%\r\n0/7759 [00:02<?, ?it/s]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-caf15077ca9b> in <module>()\r\n      2 os.environ['WANDB_CONSOLE'] = 'on'\r\n      3 trainer = Trainer(logger=wbLogger, tpu_cores=1, deterministic=True, max_epochs=epochNum, replace_sampler_ddp=False, num_sanity_val_steps=0)\r\n----> 4 trainer.fit(HPAModelV1(), trainDL, valDL)\r\n      5 \r\n      6 print(time.time() - t0)\r\n\r\n23 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    497 \r\n    498         # dispath `start_training` or `start_testing` or `start_predicting`\r\n--> 499         self.dispatch()\r\n    500 \r\n    501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    544 \r\n    545         else:\r\n--> 546             self.accelerator.start_training(self)\r\n    547 \r\n    548     def train_or_test_or_predict(self):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     71 \r\n     72     def start_training(self, trainer):\r\n---> 73         self.training_type_plugin.start_training(trainer)\r\n     74 \r\n     75     def start_testing(self, trainer):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in start_training(self, trainer)\r\n    264             del os.environ[\"XLA_USE_BF16\"]\r\n    265         self._close_logger(trainer)\r\n--> 266         xmp.spawn(self.new_process, **self.xmp_spawn_kwargs)\r\n    267 \r\n    268     def start_testing(self, trainer) -> None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)\r\n    384   pf_cfg = _pre_fork_setup(nprocs)\r\n    385   if pf_cfg.num_devices == 1:\r\n--> 386     _start_fn(0, pf_cfg, fn, args)\r\n    387   else:\r\n    388     return torch.multiprocessing.start_processes(\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in _start_fn(index, pf_cfg, fn, args)\r\n    321   # environment must be fully setup before doing so.\r\n    322   _setup_replication()\r\n--> 323   fn(gindex, *args)\r\n    324 \r\n    325 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in new_process(self, process_idx, trainer, mp_queue)\r\n     98         self.barrier(\"pre-run-stage\")\r\n     99 \r\n--> 100         results = trainer.train_or_test_or_predict()\r\n    101 \r\n    102         self.__save_end_of_training_weights(self.lightning_module)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in train_or_test_or_predict(self)\r\n    554 \r\n    555         else:\r\n--> 556             results = self.run_train()\r\n    557 \r\n    558         return results\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    635                 with self.profiler.profile(\"run_training_epoch\"):\r\n    636                     # run train epoch\r\n--> 637                     self.train_loop.run_training_epoch()\r\n    638 \r\n    639                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    495             # ------------------------------------\r\n    496             with self.trainer.profiler.profile(\"run_training_batch\"):\r\n--> 497                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    498 \r\n    499             # when returning -1 from train_step, we end epoch early\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    657 \r\n    658                         # optimizer step\r\n--> 659                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    660 \r\n    661                     else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    436             on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\r\n    437             using_native_amp=using_native_amp,\r\n--> 438             using_lbfgs=is_lbfgs,\r\n    439         )\r\n    440 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\r\n   1388             # wraps into LightingOptimizer only for running step\r\n   1389             optimizer = LightningOptimizer._to_lightning_optimizer(optimizer, self.trainer, optimizer_idx)\r\n-> 1390         optimizer.step(closure=optimizer_closure)\r\n   1391 \r\n   1392     def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in step(self, closure, *args, **kwargs)\r\n    212             profiler_name = f\"optimizer_step_and_closure_{self._optimizer_idx}\"\r\n    213 \r\n--> 214         self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n    215         self._total_optimizer_step_calls += 1\r\n    216 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, **kwargs)\r\n    132 \r\n    133         with trainer.profiler.profile(profiler_name):\r\n--> 134             trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n    135 \r\n    136     def step(self, *args, closure: Optional[Callable] = None, **kwargs):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in optimizer_step(self, optimizer, opt_idx, lambda_closure, **kwargs)\r\n    275         )\r\n    276         if make_optimizer_step:\r\n--> 277             self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n    278         self.precision_plugin.post_optimizer_step(optimizer, opt_idx)\r\n    279         self.training_type_plugin.post_optimizer_step(optimizer, opt_idx, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/tpu.py in run_optimizer_step(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\r\n     32 \r\n     33     def run_optimizer_step(self, optimizer: Optimizer, optimizer_idx: int, lambda_closure: Callable, **kwargs):\r\n---> 34         xm.optimizer_step(optimizer, barrier=False, optimizer_args={'closure': lambda_closure, **kwargs})\r\n     35 \r\n     36     def all_gather(self, tensor: Union[torch.Tensor], group: Optional[Any] = None, sync_grads: bool = False):\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py in optimizer_step(optimizer, barrier, optimizer_args, groups)\r\n    779   \"\"\"\r\n    780   reduce_gradients(optimizer, groups=groups)\r\n--> 781   loss = optimizer.step(**optimizer_args)\r\n    782   if barrier:\r\n    783     mark_step()\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs)\r\n     86                 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\r\n     87                 with torch.autograd.profiler.record_function(profile_name):\r\n---> 88                     return func(*args, **kwargs)\r\n     89             return wrapper\r\n     90 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\r\n     25         def decorate_context(*args, **kwargs):\r\n     26             with self.__class__():\r\n---> 27                 return func(*args, **kwargs)\r\n     28         return cast(F, decorate_context)\r\n     29 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py in step(self, closure)\r\n     64         if closure is not None:\r\n     65             with torch.enable_grad():\r\n---> 66                 loss = closure()\r\n     67 \r\n     68         for group in self.param_groups:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()\r\n    652                         def train_step_and_backward_closure():\r\n    653                             result = self.training_step_and_backward(\r\n--> 654                                 split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\r\n    655                             )\r\n    656                             return None if result is None else result.loss\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    745         with self.trainer.profiler.profile(\"training_step_and_backward\"):\r\n    746             # lightning module hook\r\n--> 747             result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    748             self._curr_step_result = result\r\n    749 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    325 \r\n    326 \r\n--> 327             closure_loss = closure_loss / self.trainer.accumulate_grad_batches\r\n    328 \r\n    329             # the loss will get scaled for amp. avoid any modifications to it\r\n\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n\r\nThank you, and sorry for all the text\r\nA",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6806",
    "createdAt": "2021-04-03T05:59:44Z",
    "updatedAt": "2022-07-05T20:20:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "adamDhalla"
    },
    "answer": {
      "body": "Hi @adamDhalla, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-04-03T09:43:04Z"
    }
  },
  {
    "title": "Can custom logger extending `LightningLoggerBase` be imported from separate library?",
    "body": "I'm responsible for implementing `pytorch_lightning.loggers.neptune.NeptuneLogger`.\r\n\r\nRight now we're going through massive api update for neptune client, which requires updating `NeptuneLogger` in your repository (`PyTorchLightning/pytorch-lightning`).\r\n\r\nTheoretically we could implement our `class NeptuneLogger(LightningLoggerBase)` in our repo, and import this class as module in `PyTorchLightning/pytorch-lightning`.\r\nIn such configuration we could easily update logger in our repo without having to create PR in `PyTorchLightning/pytorch-lightning`, when updating our api again in the future.\r\n\r\nAre there any reasons against such solution?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6787",
    "createdAt": "2021-04-01T13:59:34Z",
    "updatedAt": "2022-06-22T15:11:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "shnela"
    },
    "answer": {
      "body": "Go for it! This is the best way to make sure users get the best experience. ",
      "author": {
        "login": "edenlightning"
      },
      "createdAt": "2021-04-05T14:01:15Z"
    }
  },
  {
    "title": "TPU Training: No TPU devices were found.",
    "body": "Thanks for great framework.  \r\nI tried to train with tpu (Google Cloud Platform Environment). I encounter error like this:\r\n```\r\nkaki_ai@kaki-ins:~/kopite-bot$ python3 train_blender.py\r\n16:14:31 | Overriding opt[\"no_cuda\"] to True (previously: False)\r\n16:14:31 | Loading model with `--beam-block-full-context false`\r\n16:14:31 | loading dictionary from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model.dict\r\n16:14:31 | num words = 54944\r\n16:14:32 | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\r\n16:14:33 | Total parameters: 87,508,992 (87,508,992 trainable)\r\n16:14:33 | Loading existing model params from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model\r\nTraceback (most recent call last):\r\n  File \"train_blender.py\", line 47, in <module>\r\n    val_dataloader=test_loader,\r\n  File \"/home/kaki_ai/kopite-bot/training/lightning_base.py\", line 135, in fit\r\n    accumulate_grad_batches=self.accumulate_grad_batches,\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 39, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 321, in __init__\r\n    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 91, in __init__\r\n    self.tpu_cores = device_parser.parse_tpu_cores(tpu_cores)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\", line 113, in parse_tpu_cores\r\n    raise MisconfigurationException('No TPU devices were found.')\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No TPU devices were found.\r\n```\r\n\r\nIf you have any doubts, please help me. Thank you!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6774",
    "createdAt": "2021-04-01T02:11:32Z",
    "updatedAt": "2022-08-23T09:45:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sooftware"
    },
    "answer": {
      "body": "See #6778. (just for the record)",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-04-01T06:54:59Z"
    }
  },
  {
    "title": "New ModelCheckpoint Code Update Breaking Model Saving Functionality : v1.2.5->v.1.2.6",
    "body": "I am using Latest PT-Lightning and your upgrade to [ModelCheckpoint](https://github.com/PyTorchLightning/pytorch-lightning/blob/f0c5479de99e9a1524c95a9dffe2fa0cc76ab1ea/pytorch_lightning/callbacks/model_checkpoint.py) broke my functionality to my codebase. \r\n\r\nV1.2.6 specifically coz I started facing these issues yesterday.  Downgraded to V1.2.5. Please check once. The dict based metrics returned from the `train_step` or `validation_step` are not captured in the ModelCheckpointCallback \r\n\r\n```python \r\n monitor_candidates = self._monitor_candidates(trainer)\r\n```\r\n`monitor_candidates` doesn't have any values returned from the trainer. And I am not using the `Result` class. I am returning a pure dictionary. \r\n\r\nDidn't have time to create replicate the bug exactly in a notebook because I am on a tight deadline. But please do check. \r\n\r\nAs I have more time, I will update more on the issue. \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6769",
    "createdAt": "2021-03-31T17:28:34Z",
    "updatedAt": "2022-07-13T22:10:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "valayDave"
    },
    "answer": {
      "body": "You need to use `self.log(\"your_metric\", metric)`\r\nAnd then set the monitor argument in `ModelCheckpoint(monitor=\"your_metric\")`\r\n\r\nLet me know if that helps.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T23:05:28Z"
    }
  },
  {
    "title": "Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance.",
    "body": "Hi, I started noticing the following warning message after setting up a new conda environment with Pytorch 1.8.1, which is an update from my previous environment that uses Pytorch 1.7.0.\r\n\r\n> Epoch 0:   0%|     \r\n[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())\r\n\r\nAny idea if this is a real concern? How can we disable `find_unused_parameters`?\r\n\r\n\r\n```py\r\ntrainer = pl.Trainer(\r\n    val_check_interval=0.1,\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback, early_stop_callback],\r\n    precision=16,\r\n)\r\n```\r\n\r\nPackages:\r\n- pytorch 1.8.1\r\n- pytorch-lightning 1.2.6\r\n- cudatoolkit 11.1.1\r\n- cudnn 8.0.5\r\n- python 3.8\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6761",
    "createdAt": "2021-03-31T05:33:35Z",
    "updatedAt": "2025-01-15T03:58:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "athenawisdoms"
    },
    "answer": {
      "body": "Hi @athenawisdoms the docs here cover how you can disable `find_unused_parameters` and speed up your DDP training\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-false",
      "author": {
        "login": "ananthsub"
      },
      "createdAt": "2021-03-31T08:17:54Z"
    }
  },
  {
    "title": "LightningModule.log does not work for validation metrics",
    "body": "I implemented the validation loop like this (using `validation_epoch_end` because the number of labels in every batch is variable):\r\n\r\n```\r\nclass MyModel(pl.LightningModule):\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        return {'target': ... ,'pred': ...}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        targets = torch.cat([o['target'] for o in outputs])\r\n        preds = torch.cat([o['pred'] for o in outputs])\r\n        accuracy = (preds == targets).sum().item() / len(preds)\r\n        self.log(\"val_acc\", accuracy * 100)\r\n```\r\n\r\nI verified that `accuracy` is computed correctly (as a float). However, I can't see it in the console. The console output looks very odd, because the progress bar is repeated during validation (it looks okay during training), and the \"Validating\" prefix changed back to \"Epoch 0\" (the number of epochs also changed from 74 to 79).\r\n\r\n```\r\nEpoch 0:   6%|\u258b         | 5/79 [00:04<01:03,  1.16it/s, loss=6.29]\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\r\nEpoch 0:  10%|\u2588         | 8/79 [00:04<00:39,  1.80it/s, loss=6.29]\r\nEpoch 0:  14%|\u2588\u258d        | 11/79 [00:04<00:28,  2.41it/s, loss=6.29]\r\nEpoch 0:  18%|\u2588\u258a        | 14/79 [00:04<00:21,  2.98it/s, loss=6.29]\r\nEpoch 0:  22%|\u2588\u2588\u258f       | 17/79 [00:04<00:17,  3.51it/s, loss=6.29]\r\nEpoch 0:  25%|\u2588\u2588\u258c       | 20/79 [00:04<00:14,  4.04it/s, loss=6.29]\r\nEpoch 0:  29%|\u2588\u2588\u2589       | 23/79 [00:05<00:12,  4.52it/s, loss=6.29]\r\nEpoch 0:  33%|\u2588\u2588\u2588\u258e      | 26/79 [00:05<00:10,  5.00it/s, loss=6.29]\r\nEpoch 0:  37%|\u2588\u2588\u2588\u258b      | 29/79 [00:05<00:09,  5.45it/s, loss=6.29]\r\nEpoch 0:  41%|\u2588\u2588\u2588\u2588      | 32/79 [00:05<00:07,  5.88it/s, loss=6.29]\r\nEpoch 0:  44%|\u2588\u2588\u2588\u2588\u258d     | 35/79 [00:05<00:07,  6.27it/s, loss=6.29]\r\nEpoch 0:  48%|\u2588\u2588\u2588\u2588\u258a     | 38/79 [00:05<00:06,  6.65it/s, loss=6.29]\r\nEpoch 0:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 41/79 [00:05<00:05,  6.99it/s, loss=6.29]\r\nEpoch 0:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 44/79 [00:05<00:04,  7.34it/s, loss=6.29]\r\nEpoch 0:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 47/79 [00:06<00:04,  7.67it/s, loss=6.29]\r\nEpoch 0:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 50/79 [00:06<00:03,  7.99it/s, loss=6.29]\r\nEpoch 0:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 53/79 [00:06<00:03,  8.29it/s, loss=6.29]\r\nEpoch 0:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 56/79 [00:06<00:02,  8.58it/s, loss=6.29]\r\nEpoch 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 59/79 [00:06<00:02,  8.88it/s, loss=6.29]\r\nEpoch 0:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 62/79 [00:06<00:01,  9.15it/s, loss=6.29]\r\nEpoch 0:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 65/79 [00:06<00:01,  9.40it/s, loss=6.29]\r\nEpoch 0:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 68/79 [00:07<00:01,  9.63it/s, loss=6.29]\r\nEpoch 0:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 71/79 [00:07<00:00,  9.85it/s, loss=6.29]\r\nEpoch 0:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 74/79 [00:07<00:00, 10.10it/s, loss=6.29]\r\nEpoch 0:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 77/79 [00:07<00:00, 10.31it/s, loss=6.29]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [00:07<00:00, 10.41it/s, loss=6.29]\r\nEpoch 1:   8%|\u258a         | 6/79 [00:03<00:46,  1.56it/s, loss=5.08]\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\r\nEpoch 1:  11%|\u2588\u258f        | 9/79 [00:04<00:31,  2.25it/s, loss=5.08]\r\nEpoch 1:  15%|\u2588\u258c        | 12/79 [00:04<00:23,  2.90it/s, loss=5.08]\r\n```\r\n\r\nWhat is the right way to implement validation accuracy? I'm new to lightning, but I looked at the documentation and couldn't find the answer. Thanks for your help.\r\n\r\nA few related posts:\r\n* https://github.com/PyTorchLightning/pytorch-lightning/issues/5774 (proposed solution is too complex)\r\n* https://github.com/PyTorchLightning/pytorch-lightning/issues/1906 (out of date, as validation_epoch_end shouldn't return anything now)\r\n* https://forums.pytorchlightning.ai/t/computing-validation-accuracy-at-the-end-of-each-epoch/188/2 (out of date, as EvalResult has been removed)\r\n\r\nUpdate:\r\nI added `self.log(\"val_loss\", ...)` to `validation_step`, but even this does not log anything in the progress bar (or anywhere in the console). It seems that `self.log` is not working at all.\r\n\r\nUpdate:\r\nI finally made it work by explicitly specifying all these parameters: `self.log(\"val_acc\", raw_accuracy * 100, prog_bar=True, on_step=False, on_epoch=True)`. (However, it still doesn't work for `val_loss`, which has on_step=True, on_epoch=False.)  The repeated progress bar problem is not fixed.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6748",
    "createdAt": "2021-03-30T15:44:20Z",
    "updatedAt": "2022-06-02T13:57:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "qpng"
    },
    "answer": {
      "body": "Set `prog_bar=True` in self.log()\r\nOr you can just open tensorboard to see the graphical logs\r\n\r\ntensorboard --logdir lightning_logs\r\n\r\ncheers",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T22:01:59Z"
    }
  },
  {
    "title": "embedding manual control location CPU vs GPU",
    "body": "I would like to create an embedding that does not fit in the GPU memory\r\nbut can fit in the CPU memory. \r\n\r\nSelect the subset for a batch, send it to the GPU at the start of mini-batch.\r\n\r\nGPU_tensor = embedding(idx)\r\n\r\nThen at the end of training update the CPU embedding from the GPU embedding.\r\n\r\nI am using \r\npl.Trainer( gpus=[0,1], distributed_backend='ddp')\r\n\r\nand probably will need accumulate_grad_batches\r\n\r\nAny idea for how to do this ?\r\n\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6726",
    "createdAt": "2021-03-29T16:38:14Z",
    "updatedAt": "2024-10-11T14:47:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "johngrabner"
    },
    "answer": {
      "body": "Duplicate of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6725",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-20T23:54:08Z"
    }
  },
  {
    "title": "Access trainer parameters from LightningModule",
    "body": "Hi,\r\nI was wondering how could i access trainer parameters from a LightningModule.\r\nI want indeed to do setup that depend on max_epochs. Configure a linear decay lr scheduler and also i want to take some special action on the validation step on the last epoch.\r\n\r\nThanks for the help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6723",
    "createdAt": "2021-03-29T15:49:46Z",
    "updatedAt": "2022-06-05T17:30:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nicolas-dufour"
    },
    "answer": {
      "body": "@nicolas-dufour You can access it with `self.trainer.max_epochs`.",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-04-03T10:01:23Z"
    }
  },
  {
    "title": "Manually call model.eval() and model.train() inside the training loop",
    "body": "How to manually call `model.eval()` or `model.train()` inside the lightning module? I happen to have several models and not all of them need to be updated during each forward pass. Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6718",
    "createdAt": "2021-03-29T10:02:59Z",
    "updatedAt": "2023-04-02T12:13:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "rwbfd"
    },
    "answer": {
      "body": "You can use [`self.freeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1479) and [`self.unfreeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1494).",
      "author": {
        "login": "kkirtac"
      },
      "createdAt": "2021-03-29T10:41:02Z"
    }
  },
  {
    "title": "How does a gpu cluster system like SLRUM use ddp training ?",
    "body": "I need to use `srun` run `python, so how does set Trainer of `pl` correctly?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6715",
    "createdAt": "2021-03-29T06:45:39Z",
    "updatedAt": "2022-07-29T17:30:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "takfate"
    },
    "answer": {
      "body": "Not sure what you are asking. Is the question how to use PL with SLURM?\r\nI can point you to the SLURM tutorial in our docs:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/clouds/slurm.html",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T23:17:21Z"
    }
  },
  {
    "title": "When use multiple optimizer, TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'",
    "body": "I want build a Super-Resolution Network with multiple optimizer. \r\n\r\nThe code is below, \r\n\r\n```python\r\n    def configure_optimizers(self):\r\n        d_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        g_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        id_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        recon_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        # use multi optimizer\r\n        return [g_optimizer, d_optimizer, id_optimizer, recon_optimizer]\r\n\r\n```\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        print('optimizer_idx', optimizer_idx)\r\n        # print('criterionG', next(self.criterionG.parameters()).requires_grad)\r\n        # print('generator', next(self.generator.parameters()).requires_grad)\r\n\r\n        lr_img, id_label, hr_img = batch\r\n\r\n        fake_img = self(lr_img)\r\n        d_fake = self.discriminator(fake_img)\r\n        d_real = self.discriminator(hr_img)\r\n\r\n        # train generator\r\n        if optimizer_idx == 0:\r\n            # log sampled images\r\n            grid = torchvision.utils.make_grid(fake_img)\r\n            self.logger.experiment.add_image('generated_images', grid, 0)\r\n            g_loss = self.g_loss_function(d_fake, fake_img, hr_img)\r\n            g_loss.requires_grad_(True)\r\n\r\n            return {'loss': g_loss}\r\n\r\n        # train discriminator\r\n        elif optimizer_idx == 1:\r\n            d_fake_loss = torch.mean(d_fake)\r\n            d_real_loss = torch.mean(d_real)\r\n            d_loss = (d_fake_loss + d_real_loss)/2\r\n            tqdm_dict ={'d_loss': d_loss}\r\n            self.log('d_loss', d_loss)\r\n\r\n            return {'d_loss': d_loss}\r\n\r\n        # fine-tuning arcface model\r\n        elif optimizer_idx == 2:\r\n            fake_img = self.conv1(fake_img)\r\n            pred = self.recognition(fake_img)\r\n            loss = self.loss_function(pred, id_label)\r\n            self.log('id_loss', loss)\r\n            tqdm_dict = {'id_loss': loss}\r\n            output = OrderedDict({\r\n                'id_loss': loss,\r\n                'progress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n\r\n        # training reconstruction model\r\n        elif optimizer_idx == 3:\r\n            fake_lr = self.reconstruction(fake_img)\r\n            loss = self.recon_loss_function(hr_img, fake_lr)\r\n            self.log('recon_loss', loss)\r\n            tqdm_dict = {'recon_loss': loss}\r\n            output = OrderedDict({\r\n                'recon_loss': loss,\r\n                'pregress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n```\r\n\r\nBut, i got this error in 'if optimizer_idx == 0:'\r\n\r\n```\r\n    closure_loss = closure_loss / self.trainer.accumulate_grad_batches\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n\r\nCan you give me a advice? \r\nThank you.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6708",
    "createdAt": "2021-03-29T02:49:29Z",
    "updatedAt": "2022-07-01T14:25:13Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "choieq"
    },
    "answer": {
      "body": "Hi @choieq, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-04-03T10:11:15Z"
    }
  },
  {
    "title": "set_to_none=True and accumulate_grad_batches",
    "body": "Is it possible to use automatic optimization with accumulate_grad_batches and performance trick zero_grad(set_to_none=True)?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6703",
    "createdAt": "2021-03-27T22:50:23Z",
    "updatedAt": "2023-02-02T16:40:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "denix56"
    },
    "answer": {
      "body": "Yes, you can override the zero_grad hook in LightningModule\r\n\r\n```python\r\n    def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n        optimizer.zero_grad(set_to_None=True)\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T23:16:00Z"
    }
  },
  {
    "title": "Progress Bar Variables from Validation Step",
    "body": "Greetings,\r\n\r\nI can only show metrics of variables calculated on training step but can't show validation step metrics on the progress bar. How can show a metric in the validation step ? `self.log(...., prog_bar=True)` does not work. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6688",
    "createdAt": "2021-03-26T15:11:06Z",
    "updatedAt": "2022-06-03T04:42:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Spawnfile"
    },
    "answer": {
      "body": "Hi\r\n\r\nIt works fine for me. Have a look at this running example (latest PL version 1.2.6:\r\n\r\n```python\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.layer(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        self.log(\"VALIDATION_STEP\", loss, prog_bar=True)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_run():\r\n\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=3,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_run()\r\n\r\n```\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-04T23:47:16Z"
    }
  },
  {
    "title": "How to not create lightning_logs when using a external logger like wandb ?",
    "body": "I would like my wandb logger to just place their data under `wandb` dir, and checkpointcallback to save ckpts under `dir_path` I specified.\r\nAnd I don't want pl to create `lightning_logs` and files under it, but I can't set `logger=False` b/c I use a logger. Is there any suggestion ?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6685",
    "createdAt": "2021-03-26T05:45:47Z",
    "updatedAt": "2025-01-28T10:34:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "richarddwang"
    },
    "answer": {
      "body": "You can set the `save_dir` in WandbLogger, something like\r\n\r\n```python\r\nlogger = WandbLogger(save_dir=\"wandb\", ...)\r\nTrainer(logger=logger, ...)\r\n```\r\nThis should work (haven't tested it).\r\nThen your logs and checkpoints will save to two different locations.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-05T00:11:52Z"
    }
  },
  {
    "title": "load_from_checkpoint giving different validation results",
    "body": "I'm creating a classifier that first trains a VAE then passes it into a convolutional network. The psudo code below kind of describes it:\r\n```python\r\nclass VAE(pl.LightningModule):\r\n# ...\r\n\r\nclass ConvNetwork(pl.LightningModule):\r\n    def __init__(self, vae):\r\n        # Trying both ways: pass in entire model vs loading checkpoint\r\n        # self.vae = vae\r\n        # self.vae = VAE.load_from_checkpoint(vae)\r\n        freeze_training(self.vae) # sets all params to requries_grad=False\r\n\r\n        self.sub_network = nn.Sequential(\r\n            # Mix of convolutional layers, ReLU activations, and Batch Normalization\r\n        )\r\n\r\n    def forward(self, data):\r\n         vae_decoded_results = self.vae(data)\r\n         results_that_differ_wildly = self.sub_network(vae_decoded_results)\r\n        \r\n```\r\nIf I train the VAE and pass in the entire model before training the convolutional network, I get good training/validation results. What I would prefer, however, is to train the VAE in a separate script, save off checkpoints, then pass the path of the checkpoint into the convolutional network. Then in the convolutional network's init I load the vae network, freeze training on it, and proceed to train the convolutional network. When I do this, my training results seem okay, but my validation results are all over the place. Some things I've checked:\r\n\r\n- After loading the VAE from a checkpoint, I verified the model parameters perfectly match the VAE that produced the checkpoint.\r\n- In my forward function for the convolutional network I call the VAE's forward function. The results at this step differ by less than 1% between loading a checkpoint and passing in an entire model.\r\n- After passing the VAE forward() results into the first stage of my Convolution network (consists of some convolution layers, ReLU activations, and batch normalization) I get very different results. \r\n\r\nI can't for the life of me figure out why using the results from a loaded model would so wildly differ from the results of a model I train and pass in all in one script, especially when the parameters and vae output appear to match. I'm sure I'm just missing something stupid. ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6678",
    "createdAt": "2021-03-25T15:20:06Z",
    "updatedAt": "2022-09-04T09:01:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "keitht226"
    },
    "answer": {
      "body": "Just a wild guess, but maybe the model is in `train`-mode after loading from a checkpoint. Have you tried `model.eval()` in addition to setting the `requires_grad`? I'm thinking about BN layers and so on, where this is important (see [here](https://stackoverflow.com/a/55627781/10429267)).",
      "author": {
        "login": "kielnino"
      },
      "createdAt": "2021-03-27T14:19:15Z"
    }
  },
  {
    "title": "ModelCheckpoint with multiple validation dataloaders",
    "body": "Hi all. How can one make ModelCheckpoint work with multiple val dataloaders? Currently, it receives a single monitor parameter. I would like to make the checkpointing happen only when both dataloaders show improvement. Should I subclass ModelCheckpoint to receive a monitor per dataloader?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6664",
    "createdAt": "2021-03-24T15:17:22Z",
    "updatedAt": "2022-08-23T15:28:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kkirtac"
    },
    "answer": {
      "body": "You can compute the min or max (depending if your improvement is a decreasing or increasing metric) of both metrics and use self.log to log that combined value. Then, you only have one monitor for both and save_top_k can work on that.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-04-05T00:16:15Z"
    }
  },
  {
    "title": "Clarification on reload_dataloaders_every_epoch",
    "body": "I have a PyTorch Lightning DataModule instance that defines train_dataloader, val_dataloader, and test_dataloader.\r\n\r\nCurrently using a custom callback to reload the train_dataloader that will resample the data.\r\n\r\nI saw that there is a Trainer flag called reload_dataloaders_every_epoch and soon to be reload_dataloaders_every_n_epochs.\r\n\r\nDo these just reload the train_dataloader, or do the do all 3?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6635",
    "createdAt": "2021-03-22T15:57:49Z",
    "updatedAt": "2024-04-26T15:40:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "thingsofleon"
    },
    "answer": {
      "body": "Only the train and validation dataloaders:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L168-L170\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L203-L207",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-20T23:58:22Z"
    }
  },
  {
    "title": "Accessing the best validation loss so far in validation_epoch_ends",
    "body": "How can I access the best validation loss in `validation_epoch_end`?  I am assuming the loss is stored somewhere as it is used to save the best model based on it, so I was wondering if I can somehow directly access it instead of tracking it my self.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6628",
    "createdAt": "2021-03-22T12:14:43Z",
    "updatedAt": "2024-08-26T13:41:26Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jandonov"
    },
    "answer": {
      "body": "I think if you include a [ModelCheckpoint](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint) in your trainer, you should be able to retrieve the [best model score](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L201). But you should set your validation loss as the monitored variable in your ModelCheckpoint callback. From your `validation_epoch_end`, you should be able to retrieve your best score by reaching the callback with something similar to `self.trainer.callback.best_model_score`.",
      "author": {
        "login": "kkirtac"
      },
      "createdAt": "2021-03-26T10:32:32Z"
    }
  },
  {
    "title": "Is it possible to disable CheckpointConnector?",
    "body": "Even with `checkpoint_callback=False`, Trainer appears to be using CheckpointConnector for some reason. Since very occasionally (once every ~30 runs) the checkpoint is either deleted too early or is never created in the first place (no idea which one), the whole experiment fails, as shown in the log below. Since CheckpointConnector does not appear to be doing anything important when running locally, is it possible to eliminate it without breaking the training process?\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\my_code\\run_automated.py\", line 95, in <module>\r\n    main(experiment, config, dataset)\r\n  File \"D:\\GIT\\my_code\\processing.py\", line 117, in main\r\n    trainer.fit(model, dataloader_train, dataloader_val)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 513, in fit\r\n    self.dispatch()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 553, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 74, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 111, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 609, in run_train\r\n    self._pre_training_routine()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 600, in _pre_training_routine\r\n    self.checkpoint_connector.restore_weights()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py\", line 65, in restore_weights\r\n    max_suffix = self.max_ckpt_in_folder(dir_path_hpc, \"hpc_ckpt_\")\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py\", line 372, in max_ckpt_in_folder\r\n    files = [os.path.basename(f[\"name\"]) for f in fs.listdir(dir_path)]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\spec.py\", line 1122, in listdir\r\n    return self.ls(path, detail=detail, **kwargs)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 51, in ls\r\n    return [self.info(f) for f in paths]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 51, in <listcomp>\r\n    return [self.info(f) for f in paths]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 61, in info\r\n    out = os.stat(path, follow_symlinks=False)\r\nFileNotFoundError: [WinError 2] Nie mo\u017cna odnale\u017a\u0107 okre\u015blonego pliku: 'D:/GIT/my_code/04d2a63662b34828946b5545646c063f.pt'\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6596",
    "createdAt": "2021-03-19T11:24:01Z",
    "updatedAt": "2023-03-30T19:13:20Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "PALuczak"
    },
    "answer": {
      "body": "So are you saying the file returned by `fs.listdir(dir_path)` gets removed?\r\nIf you can reproduce this, please open an Issue about it. Seems like a bug.\r\n\r\nIn the meantime, you can patch the problematic function like this:\r\n\r\n```python\r\ntrainer.checkpoint_connector.max_ckpt_in_folder = lambda *args, **kwargs: None\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-21T00:09:39Z"
    }
  },
  {
    "title": "Add AWS DataParellism",
    "body": "Hi!\r\nI currently use AWS SageMaker to train my PL models. I recently found out this link : \r\nhttps://aws.amazon.com/fr/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/\r\nSageMaker provides its own implementation of DDP and I think that would be nice to be able to use it with PL :)\r\nI looked into PL code and I think I could add  this feature by extending pytorch_lightning.accelerators.accelerator.Accelerator. Does it seems like a good way to implement it? Are there some general advices/guidance you could give me about this?\r\nIf the you are interested in this feature, I can make a PR once I'm done with it.\r\nThank you!\r\n\r\nR\u00e9mi",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6560",
    "createdAt": "2021-03-17T10:08:52Z",
    "updatedAt": "2022-06-13T19:21:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nomDeZeus"
    },
    "answer": {
      "body": "In progress in https://github.com/PyTorchLightning/pytorch-lightning/pull/6271",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-21T00:12:36Z"
    }
  },
  {
    "title": "Train Discriminator less than Generator",
    "body": "I want to train my discriminator ones per 10 iterations but couldn't figure out how to implement it with lightning. Do you have any advice on this?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6526",
    "createdAt": "2021-03-15T11:04:29Z",
    "updatedAt": "2022-05-31T13:41:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "paxhx"
    },
    "answer": {
      "body": "Check out the optimization docs. There are a few examples that may help you. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#step-optimizers-at-arbitrary-intervals",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-04-14T00:53:29Z"
    }
  },
  {
    "title": "Datamodule without Trainer (for inference)",
    "body": "In my usage, LightningDatamodule is currently encapsulating batch collation, moving to device, and batch transformations (via `on_after_batch_transfer`). \r\n\r\nHowever, when I want to do inference on a bunch of inputs, I want the same steps to happen. What is the recommended way to achieve this? The problem is that Trainer drives the device transfers and hooks around it, and I don't have a Trainer during inference.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6502",
    "createdAt": "2021-03-12T21:12:55Z",
    "updatedAt": "2023-12-14T14:14:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "indigoviolet"
    },
    "answer": {
      "body": "Why would you not want to use the Trainer?\r\n\r\nYou can now use `trainer.predict` for inference (will be in beta after the 1.3 release)",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-20T23:52:45Z"
    }
  },
  {
    "title": "Proper way to log things when using DDP",
    "body": "Hi, I was wondering what is the proper way of logging metrics when using DDP. I noticed that if I want to print something inside `validation_epoch_end` it will be printed twice when using 2 GPUs. I was expecting `validation_epoch_end` to be called only on rank 0 and to receive the outputs from all GPUs, but I am not sure this is correct anymore. Therefore I have several questions:\r\n\r\n1. `validation_epoch_end(self, outputs)` - When using DDP does every subprocess receive the data processed from the current GPU or data processed from all GPUs, i.e. does the input parameter `outputs` contains the outputs of the entire validation set, from all GPUs?\r\n2. If `outputs` is GPU/process specific what is the proper way to calculate any metric on the entire validation set in `validation_epoch_end` when using DDP?\r\n\r\nI understand that I can solve the printing by checking `self.global_rank == 0` and printing/logging only in that case, however I am trying to get a deeper understanding of what I am printing/logging in this case. \r\n\r\nHere is a code snippet from my use case. I would like to be able to report f1, precision and recall on the entire validation dataset and I am wondering what is the correct way of doing it when using DDP.\r\n\r\n    \r\n```Python\r\n    def _process_epoch_outputs(self,\r\n                               outputs: List[Dict[str, Any]]\r\n                               ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"Creates and returns tensors containing all labels and predictions\r\n\r\n        Goes over the outputs accumulated from every batch, detaches the\r\n        necessary tensors and stacks them together.\r\n\r\n        Args:\r\n            outputs (List[Dict])\r\n        \"\"\"\r\n        all_labels = []\r\n        all_predictions = []\r\n\r\n        for output in outputs:\r\n            for labels in output['labels'].detach():\r\n                all_labels.append(labels)\r\n\r\n            for predictions in output['predictions'].detach():\r\n                all_predictions.append(predictions)\r\n\r\n        all_labels = torch.stack(all_labels).long().cpu()\r\n        all_predictions = torch.stack(all_predictions).cpu()\r\n\r\n        return all_predictions, all_labels\r\n\r\n    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> None:\r\n        \"\"\"Logs f1, precision and recall on the validation set.\"\"\"\r\n\r\n        if self.global_rank == 0:\r\n            print(f'Validation Epoch: {self.current_epoch}')\r\n\r\n        predictions, labels = self._process_epoch_outputs(outputs)\r\n        for i, name in enumerate(self.label_columns):\r\n\r\n            f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],\r\n                                                             labels[:, i],\r\n                                                             threshold=None)\r\n            self.logger.experiment.add_scalar(f'{name}_f1/Val',\r\n                                              f1,\r\n                                              self.current_epoch)\r\n            self.logger.experiment.add_scalar(f'{name}_Precision/Val',\r\n                                              prec,\r\n                                              self.current_epoch)\r\n            self.logger.experiment.add_scalar(f'{name}_Recall/Val',\r\n                                              recall,\r\n                                              self.current_epoch)\r\n\r\n            if self.global_rank == 0:\r\n                print((f'F1: {f1}, Precision: {prec}, '\r\n                       f'Recall: {recall}, Threshold {t}'))\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6501",
    "createdAt": "2021-03-12T20:52:25Z",
    "updatedAt": "2025-06-19T09:52:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jandonov"
    },
    "answer": {
      "body": "Hi all,\r\nSorry we have not got back to you in time, let me try to answer some of your questions:\r\n1. Is `validation_epoch_end` only called on rank 0?\r\n\r\nNo, it is called by all processes\r\n\r\n2. What does the `sync_dist` flag do:\r\n\r\nHere is the essential code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/core/step_result.py#L108-L115\r\nIf `sync_dist=True` then it will as default call the `sync_ddp` function which will sum the value across all processes using `torch.distributed.all_reduce` \r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/utilities/distributed.py#L120\r\nUse this flag if you want to synchronize the value between different processes. \r\n\r\n3. How to print stuff in distributed lightning:\r\n\r\nRecommended is using either the `rank_zero_info` function. Import as:\r\n```python\r\nfrom pytorch_lightning.utilities import rank_zero_info\r\n```\r\nor use the `rank_zero_only` decorator (imported from the same module) which can be wrapped around any function such that it only gets called on `rank=0`. Each logger experiment is decorated with `rank_zero_experiment` which internally calls `rank_zero_only`\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/loggers/base.py#L31-L43\r\n\r\n4. What about `pytorch_lightning.metrics` (now known as `torchmetrics`)\r\n\r\nOur own metrics have custom synchronization going on. Any metric will automatically synchronize between different processes whenever `metric.compute()` is called. Metrics calculated this way should therefore not be logged using `sync_dist=True`.\r\n\r\n5. Recommended way of logging:\r\n\r\nUsing `self.log` in your lightning module \r\n\r\nNot sure this answers all questions.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-31T15:14:11Z"
    }
  },
  {
    "title": "How to continue training with a different learning rate",
    "body": "I want to resume training from a checkpoint, but I want to use a different learning rate, How to achieve that? I don't  really care about the training states and don't mind start a fresh training as long as the weights are proprely restored.\r\n\r\nRight now I'm using ``resume_from_checkpoint=ckpt_file`` when creating the trainer, this automatically would give the old learning rate. \r\n\r\nI also tried remove ``resume_from_checkpoint=ckpt_file``, and do \r\n```\r\nnet_learner.load_from_checkpoint(cfg.ckpt_path, cfg=cfg)\r\ntrainer.fit(net_learner, train_data_loader, val_data_loader)\r\n```\r\nbut it seems the weights are erased, and the trainer starts from random weights.\r\n\r\nAny help will be most appreciated, thanks so much!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6494",
    "createdAt": "2021-03-12T14:52:26Z",
    "updatedAt": "2024-05-02T16:49:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MightyChaos"
    },
    "answer": {
      "body": "I opened this as an [issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/12118). However (as you'll see in the discussion there), it turns out that in my case there was no problem - the `.load_from_checkpoint()` method works as expected. I probably just made a different mistake which caused my loss to (immediately) blow up after resuming training, which I interpreted as arising from the issue that you described of the weights being overwritten with a new initialization. I shouldn't have jumped to that conclusion so quickly as I didn't actually verify that the weights were different in my case. I just tried it again and it works fine now.\r\n\r\nIn your case, it looks like you're using the wrong syntax, which I hadn't spotted but another user did - please refer to the link to see how it should be used. This should solve the problem for you.",
      "author": {
        "login": "rubvber"
      },
      "createdAt": "2022-02-27T10:25:55Z"
    }
  },
  {
    "title": "Is Trainer.validate not available now in the latest version?",
    "body": "My code is like this:\r\n\r\n```python\r\nmodel = MyLitModel()\r\ntrainer = Trainer(gpus=1)\r\ntrainer.validate(model, dataloader)\r\n```\r\n\r\nHowever, I got an `AttributeError`:\r\n\r\n```\r\nAttributeError: type object 'Trainer' has no attribute 'validate'\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6479",
    "createdAt": "2021-03-11T10:40:41Z",
    "updatedAt": "2024-03-07T17:15:15Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "onpix"
    },
    "answer": {
      "body": "It's available in master!\r\n\r\nNote that your first call is using `Trainer.validate` not `trainer.validate` and that's why you get that error\r\n\r\nfixed:\r\n```python\r\nmodel = MyLitModel()\r\ntrainer = Trainer(gpus=1)\r\ntrainer.validate(model, dataloader)\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-11T14:39:13Z"
    }
  },
  {
    "title": "Pretrain some sections of a model, then initialize those parts when training a full model",
    "body": "I need to pretrain the encode and decode section of an autoencoder first, then later attach a transformer in the middle of the encode and decode section. When I load the weights of the encode and decode section when pretraining it first, while initializing the weights of the transformer section, will I get an error about missing layers?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6473",
    "createdAt": "2021-03-10T23:34:10Z",
    "updatedAt": "2022-12-01T06:14:27Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stanleyshly"
    },
    "answer": {
      "body": "You can use the `strict` flag of `load_from_checkpoint` to avoid the missing layer failure:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/079fe9bc0908fffdd55a08f7321a199bceaef6f8/pytorch_lightning/core/saving.py#L94-L95",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-11T14:43:29Z"
    }
  },
  {
    "title": "How to get global step from checkpoint?",
    "body": "Hello--\r\n\r\nI am saving checkpoints inside my module using `self.trainer.save_checkpoint(path)`. I am able to load these checkpoints into the model using `MyModel.load_from_checkpoint(path)` and trainer using `Trainer(resume_from_checkpoint=path)`. However, both the resulting model and trainer have `global_step=0` regardless of the step when saving. From the documentation I was of the impression that checkpoints saved the global step, which is important for my use case. How can I attain the global step from a checkpoint?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6463",
    "createdAt": "2021-03-10T13:45:46Z",
    "updatedAt": "2022-12-25T12:29:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "timothybrooks"
    },
    "answer": {
      "body": "Discussion continued in https://github.com/PyTorchLightning/pytorch-lightning/issues/6470",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-11T14:44:26Z"
    }
  },
  {
    "title": "Custom training loop for LightningModule",
    "body": "Hello,\r\n\r\nI was wondering if it is possible to control the trainloop behavior of a module (beyond overriding `training_step()`).  I want to manually override the `.grad` value of each parameter by myself.\r\n\r\nFor example, let's say I have this routine:\r\n\r\n```Python\r\nm_0 = MyModel()\r\nloader_1 = getTrainLoader(1)\r\nloader_2 = getTrainLoader(2)\r\nloader_3 = getTrainLoader(3)\r\n\r\n# train the first two models\r\nm_1 = train_model_for_one_epoch(m_0, loader_1)\r\nm_2 = train_model_for_one_epoch(m_1, loader_2)\r\n\r\n# train the third model based on the previous models\r\nm_3 = MyModel()\r\ncriteriton = nn.CrossEntropyLoss()\r\noptimizer = optim.SGD(m_3.parameters(), lr)\r\n\r\n# main trainloop\r\nfor data, target in loader_3:\r\n    loss_1 = criteriton(m_1(data), target)\r\n    loss_1.backward()\r\n    grad_1 = get_gradient_vector(m_1)\r\n    loss_2 = criterion(m_2(data), target)\r\n    loss_2.backward()\r\n    grad_2 = get_gradient_vector(m_2)\r\n    \r\n    # manually calculate & set gradient\r\n    grad_3 = (grad_1 + grad_2) / 2.0\r\n    set_model_gradient(m_3, grad_3)\r\n    optimizer.step()\r\n    \r\n```\r\n\r\nHow can I implement the final loop in the above code in PL?\r\n\r\nThanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6456",
    "createdAt": "2021-03-10T09:11:45Z",
    "updatedAt": "2022-06-06T12:36:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "imirzadeh"
    },
    "answer": {
      "body": "Currently there's no easy way for users to manage the dataloaders themselves, but you can perform the optimization (and manipulate the gradients) by setting `automatic_optimization=False`\r\n\r\nsee: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-11T14:59:02Z"
    }
  },
  {
    "title": "wrong global rank when trying multi-nodes",
    "body": "Hello, \r\nI'm trying to train my model with multi-nodes (2 nodes, 8 gpus per each, using ddp accelator & trying without using slurm)\r\nBut I got problem with GLOBAL_RANK\r\n\r\nin node 1, \r\n```\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/16\r\n...\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\r\n```\r\nsame as in node 2,\r\n```\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/16\r\n...\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\r\n```\r\n\r\nAnd got stuck with repeated message like below\r\n```\r\nWaiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\r\nWaiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\r\n```\r\n\r\nI'm trying to setup like [this document](https://pytorch-lightning.readthedocs.io/en/latest/advanced/cluster.html#cluster-setup) but also got problem, like below\r\n```python\r\n os.environ[\"MASTER_ADDR\"] = master_addr\r\n os.environ[\"MASTER_PORT\"] = master_port\r\n os.environ[\"WORLD_SIZE\"] = \"16\"\r\n os.environ[\"NODE_RANK\"] = rank\r\n```\r\n```\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 474, in fit\r\n    self.accelerator.setup(self, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu.py\", line 19, in setup\r\n    return super().setup(trainer, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 69, in setup\r\n    self.connect_training_type_plugin(self.training_type_plugin, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 328, in connect_training_type_plugin\r\n    plugin.connect(model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/parallel.py\", line 68, in connect\r\n    self.setup(model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 95, in setup\r\n    self.task_idx = self.cluster_environment.local_rank()\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py\", line 48, in local_rank\r\n    return int(os.environ['LOCAL_RANK'])\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/os.py\", line 681, in __getitem__\r\n    raise KeyError(key) from None\r\n```\r\n\r\nI'd appreciate any help. thanks in advance",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6454",
    "createdAt": "2021-03-10T08:16:05Z",
    "updatedAt": "2024-08-20T18:41:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jungwhank"
    },
    "answer": {
      "body": "What have you set for `MASTER_ADDR` and `MASTER_PORT`? These have to reference to one of the two machines you are using. For example if I have two nodes like this:\r\n\r\nIP: `512.124.134.4`\r\nIP: `512.124.136.8`\r\n\r\nAnd I want `512.124.134.4` to be my master node.\r\n\r\nFor both my machines I'd need to run something like `MASTER_ADDR=512.124.134.4 MASTER_PORT=4500 python train.py`.\r\n\r\nLet me know if this helps! On top of this, we should update the doc if this does work :)",
      "author": {
        "login": "SeanNaren"
      },
      "createdAt": "2021-03-14T01:05:34Z"
    }
  },
  {
    "title": "Scheduler.step() only called on the end of validation",
    "body": "**What I did:**\r\n```python\r\ndef configure_optimizers(self):\r\n\r\n    optimizer = torch.optim.AdamW(\r\n        self.parameters(),\r\n        lr=self.hparams.learning_rate,\r\n        eps=1e-5,\r\n    )\r\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\r\n        optimizer,\r\n        max_lr=self.hparams.learning_rate,\r\n        epochs=self.trainer.max_epochs,\r\n        steps_per_epoch=len(self.datamodule.train_dataloader()),\r\n    )\r\n    set_trace()\r\n    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\r\n```\r\nI put an breakpoint on `scheduler.step` to see when it will be called.\r\n\r\n**What I got:**\r\n![image](https://user-images.githubusercontent.com/17963619/110569166-1b1b1c00-818f-11eb-804e-f6c9f645cd06.png)\r\nAnd I found \r\n- I ran into breakpoint (`scheduler.step` is called) only on the end of validation.\r\n- `epochs=self.trainer.max_epochs=1` and `steps_per_epoch=len(self.datamodule.train_dataloader())=144`, which are correct.\r\n\r\n**Code for your reference if needed**\r\n```python\r\nimport random\r\nfrom IPython.core.debugger import set_trace\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom transformers import AutoConfig, ElectraForTokenClassification\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\r\nfrom pl_bolts.callbacks import PrintTableMetricsCallback\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n\r\nclass SortNumberDataset(torch.utils.data.Dataset):\r\n    def __init__(self, dataset_size, vocab_size, sequence_length):\r\n        super().__init__()\r\n        self.dataset_size = dataset_size\r\n        self.vocab_size = vocab_size\r\n        self.sequence_length = sequence_length\r\n\r\n    def __getitem__(self, i):\r\n        x = [\r\n            random.randint(0, self.vocab_size - 1) for _ in range(self.sequence_length)\r\n        ]\r\n        y = sorted(x)\r\n        return {\"x\": torch.tensor(x), \"y\": torch.tensor(y)}\r\n\r\n    def __len__(self):\r\n        return self.dataset_size\r\n\r\n\r\nclass NumberSorting(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        hf_config,\r\n        learning_rate,\r\n        trainset_size,\r\n        valset_size,\r\n        vocab_size,\r\n        sequence_length,\r\n        batch_size=128,\r\n        num_workers=4,\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.datamodule = pl.LightningDataModule.from_datasets(\r\n            SortNumberDataset(trainset_size, vocab_size, sequence_length),\r\n            SortNumberDataset(valset_size, vocab_size, sequence_length),\r\n            batch_size=batch_size,\r\n            num_workers=num_workers,\r\n        )\r\n        # self.model = ElectraForTokenClassification(hf_config)\r\n         # tie input/output embeddings\r\n         delattr(self.model, \"classifier\")\r\n         self.model.classifier = (\r\n             lambda x: x @ self.model.electra.embeddings.word_embeddings.weight.t()\r\n        )\r\n        self.val_acc = pl.metrics.Accuracy()\r\n\r\n    def forward(self, batch):\r\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\r\n        return result.logits.argmax(dim=-1), result.loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"])\r\n        return self(batch)[-1]\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        preds, loss = self(batch)\r\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\r\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        timizer = torch.optim.AdamW(optimizer_grouped_parameters, eps=1e-5)\r\n        optimizer = torch.optim.AdamW(\r\n            self.parameters(),\r\n            lr=self.hparams.learning_rate,\r\n        )\r\n        # return optimizer\r\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\r\n            optimizer,\r\n            max_lr=self.hparams.learning_rate,\r\n            epochs=self.trainer.max_epochs,\r\n            steps_per_epoch=len(self.datamodule.train_dataloader()),\r\n        )\r\n        set_trace()\r\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\r\n\r\n\r\nconfig = AutoConfig.from_pretrained(\r\n    \"google/electra-small-generator\",\r\n    pad_token_id=-1,\r\n    max_position_embeddings=7,\r\n    vocab_size=7,\r\n    num_labels=7,\r\n    embedding_size=10,\r\n    hidden_size=10,\r\n    intermediate_size=8,\r\n    num_hidden_layers=2,\r\n    num_attention_heads=2,\r\n)\r\nplmodule = NumberSorting(\r\n    config,\r\n    learning_rate=0.05,\r\n    trainset_size=18333,\r\n    valset_size=20000,\r\n    vocab_size=7,\r\n    sequence_length=7,\r\n)\r\n\r\n\r\ntrainer = pl.Trainer(\r\n    max_epochs=1,\r\n    gpus=\"0\",\r\n    callbacks=[\r\n        PrintTableMetricsCallback(),\r\n        LearningRateMonitor(logging_interval=\"step\", log_momentum=True),\r\n    ],\r\n    logger=WandbLogger(),\r\n)\r\ntrainer.fit(plmodule)\r\n\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6450",
    "createdAt": "2021-03-10T03:03:08Z",
    "updatedAt": "2022-06-15T16:59:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "richarddwang"
    },
    "answer": {
      "body": "That is the default behaviour of learning rate schedulers, that they step at the end of the training epoch. \r\nCan I ask you what you are trying to achieve?\r\nIf you want the learning rate scheduler to step after each batch, you can read more about what the output of `configure_optimizers` should look like here: https://pytorch-lightning.readthedocs.io/en/0.9.0/optimizers.html#learning-rate-scheduling",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-10T12:54:32Z"
    }
  },
  {
    "title": "changing val_check_interval from 0.01 to 0.005 changes number of steps in an epoch",
    "body": "Changing val_check_interval from 0.01 to 0.005 changed the number of steps in an epoch from 2k to 3k for one of my experiments. Wanted to know if that is expected behavior.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6440",
    "createdAt": "2021-03-09T14:50:20Z",
    "updatedAt": "2022-06-29T08:44:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "AyushP123"
    },
    "answer": {
      "body": "The number of steps in a epoch that is shown in the progressbar is both the number of training steps AND the number of validation steps. Since `val_check_interval` changes the number of validation steps, it makes sense that you are seeing the number changing :]",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-09T15:16:51Z"
    }
  },
  {
    "title": "import pytorch_lightning as pl does not work on colab",
    "body": "I can install pl in colab by\r\n`!pip install pytorch-lightning==1.2.2 --quiet`\r\nbut I cannot import it by\r\n`import pytorch_lightning as pl`\r\nI am thankful if you help me with this issue.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6425",
    "createdAt": "2021-03-09T01:20:22Z",
    "updatedAt": "2022-07-19T10:36:42Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Bajo1994"
    },
    "answer": {
      "body": "Please upgrade to version 1.2.3 (released yesterday) where this issue was solved.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-10T12:57:57Z"
    }
  },
  {
    "title": "Access datamodule in custom callbacks",
    "body": "I want to create a custom `Callback` class where I can access certain attributes from my `DataModule` and log/save them before the start of the train step. \r\n\r\nI am little confused on how to do this. Can anyone help me out with a quick snippet? Thanks!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6412",
    "createdAt": "2021-03-08T17:52:01Z",
    "updatedAt": "2022-06-29T09:35:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "vishal-wiai"
    },
    "answer": {
      "body": "Something like this should work :]\r\n```python\r\nfrom pytorch_lightning.callbacks import Callback\r\nclass MyCallback(Callback):\r\n    def __init__(self, ...):\r\n        ...\r\n\r\n    # hook for doing something with your datamodule before training step\r\n    def on_train_batch_start(self, trainer, *args, **kwargs):\r\n        dm = trainer.datamodule # this is a reference to your datamodule during training\r\n        # do something here with your datamodule\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-08T21:27:56Z"
    }
  },
  {
    "title": "How to print metric value every epoch ?",
    "body": "**What I want it to look like:**\r\n![image](https://user-images.githubusercontent.com/17963619/110324632-3253ef80-8051-11eb-8ec0-abe20b9100e0.png)\r\n\r\n**What I got now:**\r\n![image](https://user-images.githubusercontent.com/17963619/110324679-4566bf80-8051-11eb-9518-cf4b317edc5f.png)\r\n\r\n**I think I do have logged metrics, how to make it printed beautifully ?**\r\n```\r\nclass SortNumberModel(pl.LightningModule):\r\n    def __init__(self, hf_config, lr):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = AutoModelForTokenClassification.from_config(hf_config)\r\n        self.val_acc = pl.metrics.Accuracy()\r\n\r\n    def forward(self, batch):\r\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\r\n        return result.logits.argmax(dim=-1), result.loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.forward(batch)[-1]\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        preds, loss = self.forward(batch)\r\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\r\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, eps=1e-5)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6407",
    "createdAt": "2021-03-08T13:02:54Z",
    "updatedAt": "2022-08-17T09:58:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "richarddwang"
    },
    "answer": {
      "body": "In pytorch-lightning bolts we have a few extra callbacks, with one of them being the `PrintTableMetricsCallback` callback that should output exactly what you want. Simply install bolts as \r\n``` python\r\npip install pytorch-lightning-bolts\r\n```\r\nand initialize your trainer as:\r\n```python\r\ntrainer = Trainer(callbacks=[PrintTableMetricsCallback()])\r\n```\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-08T16:29:36Z"
    }
  },
  {
    "title": "Bug in SLURMConnector?",
    "body": "nvm\r\n\r\ncheers",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6369",
    "createdAt": "2021-03-05T23:19:31Z",
    "updatedAt": "2022-08-26T09:24:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "geecrane"
    },
    "answer": {
      "body": "Cheers! \ud83c\udf7b ",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-24T16:03:17Z"
    }
  },
  {
    "title": "How to correctly apply metrics API in binary use case",
    "body": "How would one correctly apply the Precision metric from v1.2.0 on, with the revised metrics api?\r\n\r\nI am currently doing something like this:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import metrics\r\n\r\n# example data\r\npreds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\r\ntargets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\r\n\r\npreds = torch.tensor(preds)\r\ntargets = torch.tensor(targets)\r\n\r\n# define method for printing metrics\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(\r\n        num_classes=None, is_multiclass=False)\r\n    recall = metrics.classification.Recall(\r\n        num_classes=None, is_multiclass=False)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n    f1beta = metrics.classification.FBeta(\r\n        num_classes=num_classes,\r\n        beta=2\r\n    )\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=None)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n    print(\"F1-Beta:\\n{}\\n\".format(f1beta(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n_print_some_metrics(preds, targets, num_classes=2)\r\n```\r\n\r\nWhich gives me these results:\r\n> Precision:\r\n0.6000000238418579\r\n\r\n> Recall:\r\n0.75\r\n\r\n> F1:\r\n0.8846153616905212\r\n\r\n> F1-Beta:\r\n0.8846153616905212\r\n\r\n> AVG Precision:\r\n0.48846155405044556\r\n\r\n> Accuracy:\r\n0.8846153616905212\r\n\r\n> ConfMat:\r\ntensor([[200.,  20.],\r\n        [ 10.,  30.]])\r\n\r\nHowever, when calculating precision by hand [(TP / TP + FN)](https://en.wikipedia.org/wiki/Precision_and_recall) with the numbers from the contingency table, I get 30 / 50 = 0.6\r\n\r\nWhy does applying the precision class result in this (small) deviation?\r\n\r\nFurther, when logging the metrics on epoch_end steps inside my model, I am not able to reproduce the logged precision, recall or accuraccy numbers on the validation set with the output from the contingency table, logged on the same steps (I haven't validated the other metrics yet by hand).\r\n\r\nIt would be great to get some help on how to correctly apply the new metrics API for a binary use case.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6356",
    "createdAt": "2021-03-05T09:29:27Z",
    "updatedAt": "2022-06-17T07:53:41Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kapsner"
    },
    "answer": {
      "body": "for binary classification, where you are only interested in the positive class you should pass in `num_classes=1`. Here is your corrected code:\r\n``` python\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    recall = metrics.classification.Recall(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n    f1beta = metrics.classification.FBeta(\r\n        num_classes=num_classes,\r\n        beta=2\r\n    )\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=num_classes)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n    print(\"F1-Beta:\\n{}\\n\".format(f1beta(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n_print_some_metrics(preds, targets, num_classes=1)\r\n```\r\nonly `confusion_matrix` need to be set `num_classes=2` because you want the statistics for both classes.\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-05T13:24:52Z"
    }
  },
  {
    "title": "How to sequentially call fit() and test() in DDP",
    "body": "I wrote a Python script that loops over sets of hyperparameters and for each set calls `trainer.fit()`. Subsequently `trainer.test()` is currently called 2 times, each for the best checkpoint that was logged for two metrics.\r\n\r\nThis script is executable via commandline and works well on CPU an 1 GPU for any number of hyperparameter sets.\r\n\r\nI want to run this code on multiple GPUs (2-4) and wonder if I could use 'ddp'?\r\n\r\nThe [latest documentation](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html) says that if `your script needs to invoke both .fit and .test, or one of them multiple times` 'ddp' mode isn't possible.\r\n\r\nHowever, @awaelchli indicated [here](https://forums.pytorchlightning.ai/t/how-to-run-trainer-fit-and-trainer-test-in-ddp-distributed-mode/192/6), that running `trainer.fit()` and `trainer.test()` in 'ddp' mode is now possible but the documentation is outdated.\r\n\r\nQuestions:\r\n* Is it now possible to run both  `trainer.fit()` and subsequently `trainer.test()` (and eventually `trainer.test()` mutiple times) in 'ddp' mode?\r\n* Would I need to reorganize my code so that the loop over hyperparameter sets invokes a `subprocess.call` to actually instantiate every new trainer for 'ddp' in another process?\r\n* If calling `.test()` after `.fit()` in 'ddp' is still not possible, would it be a workaround to instantiate `trainer` again after `.fit()` is finished and just load the model weights from the best checkpoint for testing (to avoid using the same trainer object)?\r\n\r\nI would be really happy for some help on that.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6354",
    "createdAt": "2021-03-05T06:22:48Z",
    "updatedAt": "2024-01-04T17:54:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "kapsner"
    },
    "answer": {
      "body": "No this should be working fine. \r\nWe have tests for this here: https://github.com/PyTorchLightning/pytorch-lightning/blob/a6c98c4e4944cfc4977d27a4ea1e0c7c621dfb4d/tests/accelerators/test_ddp.py#L56\r\nI will update the docs.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-03-08T04:39:24Z"
    }
  },
  {
    "title": "Deterministic DataLoader on DDP reads same data on all subprocesses",
    "body": "I've used `seed_everything(7)` to initially set the seed then passed `deterministic=True, accelerator='ddp'` to Trainer to have it run on 4 GPUs.\r\nThen I load my map-style dataset using a plain DataLoader with `shuffle=True, num_workers=10` .\r\nNow what happens is that each of the forked DDP processes spin up N (here 10) worker processes to read the data. So total 4 x 10 DataLoader processes. I have tried setting up a `worker_init_fn` to see the seeds they each receive, and indeed the worker processes for each GPU get different seeds, but they are the same across worker processes of different GPUs. This causes each data item to be read 4 times (the number of GPU / DDP processes) which I checked in the dataset's `__getitem__`. So the indexes for example would look like [3,3,3,3,7,7,7,7,2,2,2,2,...].\r\n\r\nWhat is the way to fix this? Shouldn't the DistributedSampler for DDP automatically get a seed based on the subprocess that it is forked on? (Similar to DataLoader's `worker_init_fn`)",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6350",
    "createdAt": "2021-03-04T19:24:24Z",
    "updatedAt": "2022-09-27T08:34:43Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mrmotallebi"
    },
    "answer": {
      "body": "Update: This seemed to be the issue if only settings seeds and setting Pytorch and Cuda for deterministic execution, but adding `deterministirc=True` to PL's Trainer object seems to have resolved the issue.",
      "author": {
        "login": "mrmotallebi"
      },
      "createdAt": "2021-03-07T10:46:16Z"
    }
  },
  {
    "title": "How to make PyTorch Lightning quiet?",
    "body": "Because I have a very small network which is quick at training and I want to compare many different runs, I want to disable Lightning's default output to both console and file system. I already tried setting `weights_summary=False, checkpoint_callback=False, logger=False` and `logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)`, but I still get the progress bar and the GPU availability report. How can I disable these?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6341",
    "createdAt": "2021-03-04T09:05:15Z",
    "updatedAt": "2022-06-02T07:08:30Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Turakar"
    },
    "answer": {
      "body": "Set\r\n``` python\r\n# disable progress bar\r\ntrainer = Trainer(progress_bar_refresh_rate=0)\r\n```\r\nwill remove the progressbar. There was a bug related to the user setting the logging level, so if you try to update to master hopefully doing\r\n```  python\r\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\r\n```\r\nshould remove the GPU availability report.\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-04T09:36:58Z"
    }
  },
  {
    "title": "How to save a checkpoint every n steps and overwrite the previous saved checkpoints?",
    "body": "How do I save checkpoints every, as well as deleting and/or overwriting the previously saved checkpoints?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6329",
    "createdAt": "2021-03-03T19:10:41Z",
    "updatedAt": "2022-11-24T10:51:06Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "stanleyshly"
    },
    "answer": {
      "body": "Just to be sure, do you mean every n step or every n epoch.\r\nFor doing it every n epoch, you can initialize model checkpoint with `period` parameter\r\n``` python\r\ncheckpoint = pl.callbacks.ModelCheckpoint(period=n)\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-04T09:51:01Z"
    }
  },
  {
    "title": "TPU on Colab: unexpected keyword argument 'num_tpu_cores'",
    "body": "On a Colab TPU, I get the error:\r\n\r\n```\r\nFile \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\r\n    return fn(self, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'num_tpu_cores'\r\n```\r\n\r\nI'm running a script that is in my Google Drive. I copy the data up onto Colab. When I do the exact same process for the GPU, it works (with Colab set to GPU, and the appropriate trainer parameter adjustment). \r\n\r\nThe trainer instantiation is simply\r\n\r\n```\r\n    trainer = pl.Trainer(logger=logger,\r\n                         num_tpu_cores=8,\r\n                         fast_dev_run=False,\r\n                         max_epochs=20)\r\n```\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6244",
    "createdAt": "2021-02-27T19:32:52Z",
    "updatedAt": "2022-06-22T05:11:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "naraugialusru"
    },
    "answer": {
      "body": "@naraugialusru `num_tpu_cores` should be `tpu_cores`. `num_tpu_cores` was removed in #2760. See also: [the docs](https://pytorch-lightning.readthedocs.io/en/1.2.1/common/trainer.html#tpu-cores)",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-02-28T05:30:35Z"
    }
  },
  {
    "title": "Error importing pytorch lighting",
    "body": "Hi, \r\nI am getting this weird error; I was able to run my code before and today I got this: \r\nimport pytorch_lightning as pl                                              \r\n\"~/dir/miniconda3/envs/pytorchenv/lib/python3.7/site-packages/pytorch_lightning/__init__.py\", line 66, in <module>\r\nfrom pytorch_lightning import metrics                                       \r\nImportError: cannot import name 'metrics' from 'pytorch_lightning' \r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6240",
    "createdAt": "2021-02-27T10:54:49Z",
    "updatedAt": "2022-07-19T10:20:46Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mshooter"
    },
    "answer": {
      "body": "@mshooter Hi, could you try reinstalling it and running it again? I didn't experience the issue with the following command on Google Colab:\r\n```python\r\n!pip install pytorch-lightning --upgrade\r\nfrom pytorch_lightning import metrics\r\n```\r\n If the problem persists, could you run the following commands and share the output?\r\n```console\r\n$ wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\r\n$ python collect_env_details.py\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-02-27T14:27:07Z"
    }
  },
  {
    "title": "How do I incorporate a scheduler with \"step\" and \"batch step\"?",
    "body": "Hi,\r\n\r\nI am trying to incorporate the scheduler provided [here](https://github.com/mpyrozhok/adamwr). \r\n\r\nI went through the documentation of PyTorch lightning and it allows only scheduler interval \"epoch\" or \"step\". But in the scheduler attached, it needs to call \"step\" every epoch and  \"batch step\" after each iteration. Could someone help me figure out a way to do this? Any help will be appreciated.\r\n\r\nThank you,\r\nBest,\r\nShreyas Kamath",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6234",
    "createdAt": "2021-02-26T22:04:05Z",
    "updatedAt": "2023-10-25T15:18:14Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "shreyaskamathkm"
    },
    "answer": {
      "body": "It is currently not supported that a scheduler can step on both batch and epoch. The scheduler you are linking to are also different from a standard scheduler since it both has an `step` method and a `batch_step` method, where lightning only supports the `step` method. \r\nTo get this working in lightning, you would need to split the scheduler into two new schedulers that both operate on the same optimizer: one that steps on batch and one the steps on epoch.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-03T12:52:27Z"
    }
  },
  {
    "title": "How to \u201clightninfy\u201d the official PyTorch sentiment analysis tutorial?",
    "body": "Hi, I'm trying to refactor the official NLP (sentiment analysis) [tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html), using Lightning in order to take advantage of things like early stopping etc.\r\n\r\nI'm moving first steps, and the main hurdle is the creation of a Lightning module, and in particular coding the `training_step`.\r\n\r\nWhat I came up so far is \r\n\r\n```python\r\nclass LitTextClassifier(pl.LightningModule):\r\n    def __init__(self, num_class, criterion = CrossEntropyLoss):\r\n        super().__init__()\r\n        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=False)\r\n        self.fc = nn.Linear(EMBED_DIM, num_class)\r\n        self.init_weights()\r\n        self.criterion = criterion\r\n\r\n    def init_weights(self):\r\n        initrange = 0.5\r\n        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n        self.fc.weight.data.uniform_(-initrange, initrange)\r\n        self.fc.bias.data.zero_()\r\n\r\n    def forward(self, text, offsets):\r\n        embedded = self.embedding(text, offsets)\r\n        return self.fc(embedded)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.SGD(self.parameters(), lr=4.0)\r\n        return optimizer\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # I am messing up things here\r\n        text, offsets, cls = batch\r\n        output = self.forward(text, offsets)\r\n        loss = self.criterion(output, cls)\r\n\r\n        return loss\r\n```\r\n\r\nI think I am getting the `training_step` wrong. Can someone provide guidance here?\r\nA full gist to reproduce code + errors I get is here: https://gist.github.com/davidefiocco/3b6c6b1e09c4f664b3a73e5bf24d1668/5aa4c224f7772db835bbaa92d559837c7a40f4df",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6226",
    "createdAt": "2021-02-26T13:49:45Z",
    "updatedAt": "2022-06-18T08:30:16Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "davidefiocco"
    },
    "answer": {
      "body": "@davidefiocco Hi, I think you're trying to instantiate the criterion class with `output` and `cls`. You need to instantiate it in advance:\r\n```diff\r\n-        self.criterion = criterion\r\n+        self.criterion = criterion()\r\n```",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2021-02-27T16:48:44Z"
    }
  },
  {
    "title": "Logging metrics with compute()",
    "body": "Trying to understand compute() for logging. Could someone please tell if the self.log(train_acc..., on_epoch=True) is same as def training_epoch_end() which uses compute - code below\r\n\r\n    def __init__():\r\n        # ...\r\n        self.train_acc = pl.metrics.Accuracy()\r\n        self.valid_acc = pl.metrics.Accuracy()\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # ...\r\n        self.train_acc(pred, target)\r\n        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\r\n        self.log('train_loss', loss, on_step=True, on_epoch=False)\r\n\r\n        return loss\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        self.log('train_acc_epoch', self.train_acc.compute())\r\n        \r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # ...\r\n        self.valid_acc(pred, target)\r\n        self.log('val_loss', loss, prog_bar=True)\r\n        self.log('val_acc', self.valid_acc, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n        return loss\r\n    \r\n    \r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        self.log('valid_acc_epoch', self.valid_acc.compute())",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6223",
    "createdAt": "2021-02-26T13:16:12Z",
    "updatedAt": "2023-05-27T20:37:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ss007k"
    },
    "answer": {
      "body": "Yes, its the same. Here is the relevant code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4a8422c2dc870d29d26748e6aa01406d2e74fa48/pytorch_lightning/core/step_result.py#L325-L330\r\nSetting `on_epoch=True` will essentially call `metric.compute` at the end of the epoch.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-03-03T13:25:04Z"
    }
  },
  {
    "title": "How to apply a nn.Module (i.e. CNN) across an axis (i.e. Video input) in a parallelizable way",
    "body": "Hi, I\u2019m trying to apply CNN to each image in a video. Currently, my implementation uses a for loop and torch.cat where I take each image and apply the CNN module in the loop. But clearly, this is sequential and I don\u2019t see why it can\u2019t be parallelized in theory since all images are independent from each other.\r\n\r\nHowever, I\u2019m not sure how this can be accomplished. I couldn\u2019t find any built-in function for PyTorch. Is there a way to do this in parallel in PyTorch Lightning?\r\n\r\nMy video input shape looks like this: (batch_size, seq_len, channel, height, width) and CNN takes input shape of (batch_size, channel, height, width).\r\n\r\nThanks in advance for your help!",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6135",
    "createdAt": "2021-02-22T17:13:53Z",
    "updatedAt": "2021-02-25T01:52:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "seunggs"
    },
    "answer": {
      "body": "You can simply convert your `(batch_size, seq_len, channel, height, width)` tensor into an `(batch_size*seq_len, channel, height, width)` tensor, run your model and then reshape your output back:\r\n``` python \r\nbatch_size, seq_len, channel, height, width = 5, 10, 3, 28, 28 # just random picked\r\ninput = torch.randn(batch_size, seq_len, channel, height, width)\r\ninput = input.reshape(batch_size * seq_len, channel, height, width)\r\noutput = model(input) \r\n# split the batch dimension back into the original batch size and sequence length\r\noutput = output.reshape(batch_size, seq_len, *output.shape[1:])\r\n```\r\n\r\n\r\n\r\n\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-02-24T15:21:40Z"
    }
  },
  {
    "title": "Access and change models optimizer after setup",
    "body": "Hey,\r\nwhat is the canonical way to access/change the optimizer of a pl.LightningModule after model.setup('fit') was called?\r\n\r\nE.g. if I follow https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html 4 (the last part where I explicitly plot the found lr and then change it) I don\u2019t see a way how to change the optimizers lr.\r\nIf I call model.configure_optimizers() it\u2019ll just return the optimizer and not set them.\r\nIs there no official way to access and change them after construction?\r\n\r\nCheers",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6131",
    "createdAt": "2021-02-22T15:38:50Z",
    "updatedAt": "2022-06-24T15:49:32Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Haydnspass"
    },
    "answer": {
      "body": "What you probably want to do, is to have an `learning_rate` attribute that you can change after inspecting the plot:\r\nSo your model should look something like this:\r\n``` python\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self, ...)\r\n        self.learning_rate = 1e-2\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n```\r\nthen you should be able to do:\r\n``` python\r\nmodel = MyModel(...)\r\ntrainer = Trainer(...)\r\nlrfinder = trainer.tuner.lr_finder(...)\r\nlffinder.plot() # find the learning rate you want\r\nmodel.learning_rate = 1234 # set what you want\r\n```\r\n\r\n",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-02-24T15:25:45Z"
    }
  },
  {
    "title": "To find r2score of my model",
    "body": "I have a UNet model. I'm trying for a regression model since, in my output, I have different floating values for each pixel. In order to check the r2score, I tried to put the below code in the 'model class', training_step, validation_step, and test_step. \r\n\r\n`from pytorch_lightning.metrics.functional import r2score`\r\n`r2 = r2score(logits, y)`\r\n`self.log('r2:',r2)`\r\n\r\nBut it's giving the following error\r\n\r\n> ValueError: Expected both prediction and target to be 1D or 2D tensors, but recevied tensors with dimension torch.Size([50, 1, 32, 32])\r\n\r\nHow can I check my model fit?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/6125",
    "createdAt": "2021-02-22T14:36:47Z",
    "updatedAt": "2022-11-17T18:10:40Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bibinwils"
    },
    "answer": {
      "body": "I assume that you just want to calculate the total score, and in that case you should simply flatten your input before calculating the score:\r\n``` python\r\nfrom pytorch_lightning.metrics.functional import r2score\r\nr2 = r2score(logits.flatten(), y.flatten())\r\nself.log('r2:',r2)\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-02-24T10:13:46Z"
    }
  },
  {
    "title": "Trainer cannot handle 1d tensor when return results from test_epoch_end",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen trainer run_test() called, the results from test cannot properly handle a 1D tensor in the results dictionary.\r\n\r\nSuch error will happen:\r\n\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_test(self)\r\n    708                 for k, v in result.items():\r\n    709                     if isinstance(v, torch.Tensor):\r\n--> 710                         result[k] = v.cpu().item()\r\n    711 \r\n    712         return eval_loop_results\r\n\r\nValueError: only one element tensors can be converted to Python scalars\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\nTo reproduce with BoringModel, only need to replace the test_epoch_end.\r\n\r\n```python\r\ndef test_epoch_end(self, outputs) -> None:\r\n    torch.stack([x[\"y\"] for x in outputs]).mean()\r\n    f1_score = torch.tensor([1,1,1,1])\r\n    return {'f1_score': f1_score}\r\n```\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n```python\r\ndef run_test(self):\r\n\r\n        # remove the tensors from the eval results\r\n        for i, result in enumerate(eval_loop_results):\r\n            if isinstance(result, dict):\r\n                for k, v in result.items():\r\n                    if isinstance(v, torch.Tensor):\r\n                        # should check if you can call .item()\r\n                        result[k] = v.cpu().item()\r\n```\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.8\r\n\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5979",
    "createdAt": "2021-02-14T23:17:25Z",
    "updatedAt": "2023-03-25T06:03:58Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhiruiluo"
    },
    "answer": {
      "body": "Because for example in multi gpu mode, if we would allow the user to return, then we're missing the information what to do with the data, how the data is collected and synced, or reduced or whatever. \r\nThe logging api offers reduction and sync, by specifying the custom arguments how to do so.\r\nOn the other hand, `self.write` offers a way to collect all results.\r\nThere will also be a prediction api in 1.2. #5752 \r\ncc @tchaton ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2021-02-15T02:30:16Z"
    }
  },
  {
    "title": "How to accumulate metrics for multiple validation dataloaders",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nHow to accumulate metrics for multiple validation dataloaders separately? Currently the metrics are accumulated for all dataloaders simultaneously. \r\n#### Code\r\nThe validation step accepts `dataset_idx` parameter when running validation with multiple dataloaders.\r\n```python\r\ndef validation_step(self, batch, batch_idx, dataset_idx: Optional[int] = None):\r\n```\r\nHowever I'm not sure how to update the metrics separately for each dataloader. Would I have to create separate metrics, one for dataset A and second for B? Or maybe my metric could accept the `dataset_idx` parameter to know for which ds it should log given output.\r\n\r\nThis however wouldn't work with pl factory metrics like average precision, since they are dataset agnostic?\r\n```python\r\ndef update(self, preds: torch.Tensor, target: torch.Tensor):\r\n```\r\nNot sure how to approach this.\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5793",
    "createdAt": "2021-01-29T09:38:39Z",
    "updatedAt": "2022-06-02T12:43:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "potipot"
    },
    "answer": {
      "body": "You would have to create seperate metrics per validation dataloader (similar to how you need seperate metrics for train/val/test). Something like this could maybe work for you\r\n``` python\r\ndef __init__(self, ...)\r\n    ...\r\n    self.val_metrics = nn.ModuleList([pl.metrics.Accuracy() for _ in range(n_val_dataloaders)])\r\n\r\ndef validation_step(self, batch, batch_idx, dataset_idx):\r\n    ...\r\n    self.val_metrics[dataset_idx].update(preds, target)\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2021-01-29T11:25:35Z"
    }
  },
  {
    "title": "Override .ckpt in ModelCheckpoint Callback",
    "body": "Hi,\r\n\r\nI'm using ModelCheckpoint Callback to save my model checkpoint. pytorch lightning automatically attaches `-v0`, `-v1` to the filename I specified if it finds checkpoint models exist in `dirpath`.  Instead of saving all the models from different runs, is there a way to make the ModelCheckpoint Callback only save one model in the checkpoint folder and just override the model from previous runs?\r\n\r\nFor example, my `ModelCheckpoint` is as follow: \r\n```\r\nModelCheckpoint(monitor='valid_score',\r\n                              dirpath=\"./checkpoint/\",\r\n                              filename=\"model\",\r\n                              mode='max', save_top_k=1))\r\n```\r\nIf I run the code for 3 three times, my `checkpoints` folder will have the following: \r\n```\r\n- checkpoint:\r\n    - model.ckpt\r\n    - model-v0.ckpt\r\n    - model-v1.ckpt\r\n```\r\nWould it be possible to just have `model.ckpt` in my `checkpoint` folder no matter how many times I run the code? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5684",
    "createdAt": "2021-01-27T17:52:09Z",
    "updatedAt": "2022-10-06T04:23:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "victkid"
    },
    "answer": {
      "body": "We don't support this, but you could always remove the file manually between runs with `os.remove()`\r\n",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-21T00:17:36Z"
    }
  },
  {
    "title": "Code stuck after running 1 epoch on TPU",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm trying to run the LitAutoEncoder on TPUs, but the code runs for 1 epoch and gets stuck there.\r\n\r\n#### Code\r\n\r\n```\r\nclass LitAutoEncoder(pl.LightningModule):\r\n\r\n    def __init__(self, hparams):\r\n        super().__init__()\r\n        self.hparams = hparams\r\n        self.encoder = nn.Sequential(\r\n            nn.Linear(28*28, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 3)\r\n        )\r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(3, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 28*28)\r\n        )\r\n\r\n    def forward(self, x):\r\n        # in lightning, forward defines the prediction/inference actions\r\n        embedding = self.encoder(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defined the train loop.\r\n        # It is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard by default\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\r\ntrain_loader = DataLoader(dataset, drop_last=True, batch_size=32)\r\n\r\nargs_dict = dict(\r\n    num_train_epochs=1,\r\n    seed=42,\r\n)\r\n\r\nargs = argparse.Namespace(**args_dict)\r\nautoencoder = LitAutoEncoder(args)\r\n\r\ntrain_params = dict(\r\n    tpu_cores=8,\r\n    progress_bar_refresh_rate=30,\r\n)\r\n\r\ntrainer = pl.Trainer(**train_params)\r\ntrainer.fit(autoencoder, train_loader)\r\n\r\n```\r\n#### Reproducible Colab Notebook\r\n\r\n[Notebook](https://colab.research.google.com/drive/1wDIqHsrl5hji4fWlBGznJ3lXqOlxqKfn?usp=sharing)\r\n\r\n#### What's your environment?\r\n\r\n - Colab\r\n - Packaging pip\r\n - pytorch-1.7\r\n - pytorch-lightning-1.1.5",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5797",
    "createdAt": "2021-01-23T10:36:55Z",
    "updatedAt": "2022-08-23T09:44:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sumanthd17"
    },
    "answer": {
      "body": "Thanks Adrian,\n\nSorry I've been editing the notebook. It's running now, but not sure what\nthe issue was. My bad I ended up editing the notebook and error was not\nreproducible anymore. Will update it back to the old version.\n\nOn Sun, Jan 24, 2021 at 12:00 AM Adrian W\u00e4lchli <notifications@github.com>\nwrote:\n\n> Could you check again? I just ran your colab and it finished both epochs.\n> Maybe it's random? So far did not see random behaviour\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/issues/5625#issuecomment-766157230>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGX3GHX6NN62CNNLKPFB6O3S3MIVXANCNFSM4WPSSWOA>\n> .\n>\n",
      "author": {
        "login": "sumanthd17"
      },
      "createdAt": "2021-01-23T18:35:39Z"
    }
  },
  {
    "title": "Is pytorch lightning using MKLDNN when no GPU is available ?",
    "body": "Hi, \r\n\r\nI was wondering if PL can use mkldnn/dnnl/onednn ? I may be mistaken, but it seems pytorch does not default to it and there is no explicit mention of it inside PL's code. If it's not, is there a way to enable it ?\r\n\r\nAlexandre ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5602",
    "createdAt": "2021-01-21T12:59:58Z",
    "updatedAt": "2022-08-19T11:26:08Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "AlexandreDey"
    },
    "answer": {
      "body": "No, unfortunately, MKL-DNN is currently not supported\r\n\r\nDuplicate of https://github.com/PyTorchLightning/pytorch-lightning/issues/6020",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-20T23:46:50Z"
    }
  },
  {
    "title": "how to set find_unused_parameters=True?",
    "body": "## \ud83d\udc1b Bug\r\n\r\n```\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your \r\nmodule has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the \r\nkeyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` \r\nfunction outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel \r\nmodule wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss \r\nfunction and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\r\n```\r\n\r\n\r\nwhen I use pt 1.0.8, my model is ok, but when I switch to 1.1.4, it throws this error.  It seems 1.0.8 enable unused parameters by default, but 1.1.4 not.  How to solve this problem.\r\n\r\nI think switch `find_unused_parameters=True` by default to `False` is a breaking change, but in docs, it doesn't mention, yet no clear instructions to set to `True` .",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5799",
    "createdAt": "2021-01-20T02:32:43Z",
    "updatedAt": "2022-06-01T08:57:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "xiadingZ"
    },
    "answer": {
      "body": "No need to subclass `DDPPlugin`. This is enough:\r\n\r\n```python\r\ntrainer = pl.Trainer(plugins=[DDPPlugin(find_unused_parameters=True)])\r\n```\r\n\r\nWhich is used here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/0a50bb406fa41dfa6a0e2be52f531a9c81c87d00/pytorch_lightning/plugins/ddp_plugin.py#L66-L68\r\n\r\nSorry for the inconvenience!",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-02-10T02:56:07Z"
    }
  },
  {
    "title": "trainer.check",
    "body": "Hi pytorch-lightning friends! :) I'd like to start a discussion about `trainer.check` api. \r\nBasically, this API should check that all the user-define classes (models, data, callbacks, ...) are programmatically sound. I propose to use `inspect` to check for function correctness, here's my PR proposal at https://github.com/PyTorchLightning/pytorch-lightning/issues/3244\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5407",
    "createdAt": "2021-01-07T16:49:59Z",
    "updatedAt": "2021-03-21T21:17:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "gianscarpe"
    },
    "answer": {
      "body": "Locking in favor of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6029",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-03-21T21:17:34Z"
    }
  },
  {
    "title": "Print weights summary",
    "body": "Weights summary gets printed when Trainer calls `fit()`, but the output is not persistent as I have everything wrapped up in ray tune, which overwrites the contents of the output cell in jupyter\r\n\r\nIs there something we can call to manually print the weights summary of a particular model without having to fit the model every time?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5786",
    "createdAt": "2020-12-28T20:49:30Z",
    "updatedAt": "2022-06-15T22:03:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "thesimonho"
    },
    "answer": {
      "body": "just do \r\n```python\r\nmodel = YourLightningModule(...)\r\nmodel.summarize(mode=...)\r\n```\r\n`mode='top', 'full'`",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2020-12-29T18:58:01Z"
    }
  },
  {
    "title": "Trainer.test() on ddp can not get entire dataset.",
    "body": "## Problem\r\nHi, Everyone. I have some question about ddp. Because I want to write `predict result` to file.\r\nAnd I use `trainer.test(model=model, test_dataloaders=test_dataloader)` to process it. But I just get `1/ngpus` dataset.\r\nFor example: \r\n```\r\nGPUS: 2\r\ntotal data: 10000\r\npredict data: 5000(total data / GPUS)\r\n```\r\nHope someone can help me to solve it. Because I have to write predict result to file.\r\n\r\n## Environment\r\n```\r\npytorch: 1.4.0\r\npytorch-lightning: 1.1.1\r\nGPUS: Tesla P100-PCIE-16GB * 2\r\n```\r\n## Sample Code\r\n```python\r\nclass ExampleModel(pl.LightningModule):\r\n   def __init__(self):\r\n       self.original_file = open('/path/to/file', 'a')\r\n   def train_step(...):\r\n       .....\r\n   def test_step(self, batch, batch_idx):\r\n        init_ids = batch['init_ids']\r\n        attention_mask = batch['attention_mask']\r\n        token_type_ids = batch['token_type_ids']\r\n        predictions = self.model(init_ids, attention_mask, token_type_ids)\r\n        ..........\r\n        self.original_file.write(convert_ids_to_str(init_ids.cpu().numpy()) + '\\t' + str(seg.cpu().numpy()[0]) + '\\n')\r\n\r\n\r\n```\r\n* trainer\r\n```python\r\ntrainer = pl.Trainer(max_epochs=EPOCH,\r\n                    gpus=[0,1], \r\n                    num_nodes=1,\r\n                    auto_select_gpus=True,\r\n                    num_sanity_val_steps=0,\r\n                    accelerator='ddp',\r\n                    callbacks=[modelcheckpoint_callback, earlystopping_callback])\r\n```\r\n```python\r\ntrainer.test(model=model, test_dataloaders=test_dataloader)\r\n```\r\n\r\nHope someone can help or answer how to do it.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5263",
    "createdAt": "2020-12-25T07:39:49Z",
    "updatedAt": "2021-04-20T23:44:44Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MarsSu0618"
    },
    "answer": {
      "body": "Please, check out `trainer.predict()` after 1.3 is released!",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2021-04-20T23:44:30Z"
    }
  },
  {
    "title": "Precision and Recall over validation step",
    "body": "When [Precision](https://pytorch-lightning.readthedocs.io/en/stable/metrics.html#precision) and [Recall](https://pytorch-lightning.readthedocs.io/en/stable/metrics.html#recall) are directly computed, I get the following result:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning.metrics import Precision\r\nfrom pytorch_lightning.metrics import Recall\r\n\r\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n\r\nprecision = Precision(num_classes=3)\r\nrecall = Recall(num_classes=3)\r\nprecision(y_hat, y)\r\n#>>>tensor(0.2917)\r\nrecall(y_hat, y)\r\n#>>>tensor(0.4444)\r\n```\r\n\r\nHowever, when the same metrics are computed over `validation_step`, I get the following stranger result:\r\n\r\n```python\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch[\"x\"], batch[\"y\"] # y = tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0], device='cuda:0')\r\n        y_hat = self(x) # y_hat  = tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1], device='cuda:0')\r\n\r\n        precision = self.precision_score(y_hat, y) # precision =  tensor(0.4000, device='cuda:0')\r\n        recall = self.recall_score(y_hat, y) # recall = tensor(0.4000, device='cuda:0')\r\n\r\n```\r\n\r\nwhat am I missing?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5809",
    "createdAt": "2020-12-04T00:37:28Z",
    "updatedAt": "2022-06-07T08:31:05Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "celsofranssa"
    },
    "answer": {
      "body": "@Ceceu after trying myself, I assume you have set the `average` argument in the first case to `macro` and in the second case to `micro` (default):\r\n\r\n``` python\r\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n\r\nprecision = Precision(num_classes=3, average='macro')\r\nrecall = Recall(num_classes=3, average='macro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.2917), tensor(0.4444)\r\n\r\nprecision = Precision(num_classes=3, average='micro')\r\nrecall = Recall(num_classes=3, average='micro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.4000), tensor(0.4000)\r\n```",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2020-12-07T11:46:38Z"
    }
  },
  {
    "title": "Can the learning rate find by dp using one more gpu be used in ddp?",
    "body": "When using pytorch_lightning.tuner.lr_finder.lr_find, ddp have some error. So i change to dp using 4 gpus. Can the learning rate find by dp used by ddp? They have same gpu numbers.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/4878",
    "createdAt": "2020-11-27T12:23:34Z",
    "updatedAt": "2023-03-22T00:55:34Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhhao1"
    },
    "answer": {
      "body": "I think the answer is no. \r\n\r\nDP doesn't change your effective batch size while DDP does (in your case with one node and 4GPUs, the effective batch size is 4 times bigger with DDP). You can find more info about the effective batch size in the \"multi-GPU\" section of Lightning's documentation [here](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html).\r\n\r\nAs a consequence of this, you should probably increase your learning rate. Rule of thumbs is to increase it linearly (so by 4) but there is more than just doing that. Have a look at that paper: [https://arxiv.org/pdf/1706.02677.pdf](https://arxiv.org/pdf/1706.02677.pdf)",
      "author": {
        "login": "floboc"
      },
      "createdAt": "2020-11-27T14:00:26Z"
    }
  },
  {
    "title": "How to remove hp_metric initial -1 point and x=0 points?",
    "body": "Hi, I don't think this is a bug but I'm doing something wrong. I want to use my val_dice as hp_metric tabular AND also see the graph on \"show metric\" (radio button) under the Tensorboard HPARAMS tab:\r\n![image](https://user-images.githubusercontent.com/10415569/100072674-002dd180-2e56-11eb-80e0-2dcb94be0ae5.png)\r\nTo achieve this I'm logging using `self.log('hp_metric', mean_dice)` (for the graph) and `self.logger.log_hyperparams(params=self.hparams, metrics={'hp_metric': mean_val_dice})` (for the hparams value), both in the function `validation_epoch_end`\r\nHow do I get rid of the initial -1 value? How can I fix my graph so it doesn't draw any points at x=0? (zoomed in version)\r\n![image](https://user-images.githubusercontent.com/10415569/100073169-9bbf4200-2e56-11eb-86d8-709ee3288fee.png)\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5890",
    "createdAt": "2020-11-24T09:12:21Z",
    "updatedAt": "2024-03-05T15:37:47Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bartmch"
    },
    "answer": {
      "body": "I already use your first way but setting `default_hp_metric` to `False` makes `hp_metric` be removed from \"hparams\" tab (this tab isn't there at all even if I have set some hyper parameters). Adding the final `log_hyperparams` creates the hparams tab but the graph of `hp_metric` gets a final value at iteration 0 instead of final iteration), and this step will also be skipped if the job is killed.\r\n\r\nHere are what I've tried so far:\r\n- `default_hp_metric=True`: hparams tab visible in Tensorboad with `hp_metric` updated during training, `hp_metric` wrong initial value that makes the corresponding graph unsuitable with log scale and smoothing activated.\r\n- `default_hp_metric=False`: no hparams tab in TensorBoard, `hp_metric` graph OK\r\n- `default_hp_metric=False` and final `log_hyperparams`: no hparams tab in TensorBoard during training, `hp_metric` graph OK until the end of the training where final point is associated to iteration #0. No hparams tab at all if job is killed.\r\n\r\nI will try to calculate metric before training start to appropriately populate it, or to delay `log_hyperparams` at the first validation...",
      "author": {
        "login": "rolanddenis"
      },
      "createdAt": "2021-01-05T13:35:57Z"
    }
  },
  {
    "title": "logging question in DDP",
    "body": "Question regarding logging from lightning module in DDP model\r\n\r\nFor example, here is a validation step function that computes accuracy.\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n       x, y = batch\r\n       logits = self(x)\r\n       acc = acc_fun(logits,y)     \r\n       self.log('val_acc', acc)\r\n```\r\n\r\nSo what happens in DDP, is the logged value averaged across GPUs? At the end of every epoch? \r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/4702",
    "createdAt": "2020-11-16T23:26:06Z",
    "updatedAt": "2022-05-31T04:58:12Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ktrapeznikov"
    },
    "answer": {
      "body": "It is averaged if you set `sync_dist=True` (https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist)\r\n\r\nYou can change the op from average (mean) to something else with `sync_dist_op`.\r\n\r\nGiven that you are logging inside of `validation_step`, it will be done at the end of the epoch by default. You can change this behaviour via `on_step` and `on_epoch`.\r\n\r\nPlease have a look at the docs page I linked. Lots of useful info there :smile: ",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2020-11-17T14:37:02Z"
    }
  },
  {
    "title": "Logging RL results and tracking them with ModelCheckpoint(monitor=...)",
    "body": "I am using Pytorch Lightning in an RL setting and want to save a model when it hits a new max average reward. I am using the Tensorboard logger where I return my neural network loss in the `training_step()` using: \r\n\r\n```\r\nlogs = {\"policy_loss\": pred_loss}\r\nreturn {'loss':pred_loss, 'log':logs}\r\n```\r\n\r\nAnd then I am saving my RL environment rewards using in `on_epoch_end()`: \r\n\r\n```\r\nself.logger.experiment.add_scalar(\"mean_reward\", np.mean(reward_losses), self.global_step)\r\nself.logger.experiment.add_scalars('rollout_stats', {\"std_reward\":np.std(reward_losses),\r\n                \"max_reward\":np.max(reward_losses), \"min_reward\":np.min(reward_losses)}, self.global_step)\r\n```\r\n\r\nAnd every 5 epochs I am also writing out another RL reward loss where I use the best actions rather than sampling from them: \r\n```\r\nif self.current_epoch % self.hparams['eval_every']==0 and self.logger:\r\n            output = self.collect_rollouts(greedy=True, num_episodes=self.hparams['eval_episodes'])\r\n            reward_losses = output[0]\r\n            self.logger.experiment.add_scalar(\"eval_mean\", np.mean(reward_losses), self.global_step)\r\n```\r\n\r\nMy question is, how can I set my ModelCheckpoint to monitor `eval_mean` (which is only written out every 5 epochs, this seems like it would be a problem)? I would also settle for monitoring `mean_reward` (written out every epoch)? Right now I can only successfully monitor `policy_loss` which does not always correspond to higher rewards obtained (setting monitor = to anything else throws an error).\r\n\r\nI know that in the new PL version `self.log()` should be used but after re-writing my code using this it still didn't solve my issue. \r\n\r\nI have spent a lot of time looking through the docs and for examples of this but I have found the logging docs on this to be quite sparse and difficult to even get everything to log in the first place. \r\n\r\nI am using Pytorch Lightning 1.0.5 and Pytorch 1.7.0.\r\n\r\nThank you for any help/guidance. \r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5883",
    "createdAt": "2020-11-09T01:59:49Z",
    "updatedAt": "2022-07-12T07:51:55Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "TrentBrick"
    },
    "answer": {
      "body": "I have multiple comments that I did not verify yet but they might help\r\n\r\n- If I'm not mistaken, `self.log` only works within a selection of hooks currently. I suggest you try to move the relevant code to `training_epoch_end` where `self.log` should work correctly.\r\n- set the monitor key in the `ModelCheckpoint(monitor=)` explicitly. \r\n- You have the problem that you can only update/log every n epochs: I see two solutions: 1) synchronize your ModelCheckpoint with the `period` parameter to only run on the epochs you update the monitor quantity. 2) Cache the last value and log it in the epochs between your regular interval, to make the ModelCheckpoint see it as unchanged. The second option may even be the default behavior by Lightning but need to verify.\r\n\r\nSo in summary, I imagine something like this:\r\n```python\r\n\r\n# Model\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ... compute reward losses\r\n    \r\n    if self.current_epoch % self.hparams['eval_every']==0:\r\n        self.last_eval_mean = # compute the new eval mean\r\n\r\n     self.log(\"eval_mean\", self.last_eval_mean)\r\n\r\n\r\n# Trainer\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\")]\r\n\r\n# or maybe also try\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\", period=self.hparams['eval_every'])]\r\n```",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-11-09T16:45:12Z"
    }
  },
  {
    "title": "DDP NCCL freezes in docker AWS Jupyter",
    "body": "## \u2753 Questions and Help\r\n\r\n#### Problem\r\n\r\nJupyter terminal freezes, and connection to AWS node closes. \r\nThe problem is reproducible with any Lightning example.\r\n\r\n#### What have you tried?\r\n\r\n`python pl_examples/basic_examples/image_classifier.py --gpus 4 --accelerator ddp`\r\n\r\n#### What's your environment?\r\n\r\nLinux using docker image\r\nLightning 1.0.4\r\npytorch 1.6 \r\ncuda 10.2\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/4518",
    "createdAt": "2020-11-04T17:15:25Z",
    "updatedAt": "2023-08-22T09:38:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "awaelchli"
    },
    "answer": {
      "body": "The solution was to use \r\n\r\n`export NCCL_SOCKET_IFNAME=lo`",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-11-04T17:15:47Z"
    }
  },
  {
    "title": "DDP specifics: Single node function execution, test loss sync, num_workers, sync_batchnorm, precision",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nThis is about DDP specifics and about the handling of functions in a script that we only want executed once (not for every GPU).\r\n\r\nI think they are missing from the [doc](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel) and I couldn't find answers elsewhere. I apologize if they have been covered already.\r\n\r\n**Questions:**\r\n1. Does the regular `torch.save(model.state_dict(), path)` call work normally or does DDP complicate things for multiple GPUS?\r\n2. Does DDP run **all** the functions of a script for every GPU? For example, in the following script will the `delete_and_remake_folder` function be executed multiple times (that would result in conflicts)? Is there a way to specify functions to be run only once?\r\n3. Am I correct that `sync_batchnorm=True` and `precision=16` work in DDP?\r\n4. Does the `trainer.test()` function automatically aggregate results accross devices or is it required to set `self.log(loss,  sync_dist=True)` in the model?\r\n5. Am I correct in assuming that, if we set `num_workers=X` in a Dataloader, the actual CPU core usage will be X*N for N GPUS?\r\n\r\n\r\nQuestions 1-4 are summarized in the following script and whether it works/can work.\r\n```\r\ndef main():\r\n   delete_and_remake_folder() # I only want to run once\r\n   model = Model()\r\n   trainer = Trainer(gpus = 8, backend='ddp, sync_batchnorm=True, precision=16)\r\n   trainer.fit()\r\n   trainer.test()\r\n   torch.save(model.pt_model.state_dict(), save_dir) # I probably only want to run once (?)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/4387",
    "createdAt": "2020-10-27T10:42:09Z",
    "updatedAt": "2022-09-09T10:56:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ManiadisG"
    },
    "answer": {
      "body": "1. yes, but you might run into the issue in your 2nd question\r\n2. yes, yes `if self.global_rank == 0: do stuff`\r\n3. yes\r\n4. no, yes (think of it like another val loop)\r\n5. yes\r\n\r\nI'll close this for now, if you have more questions please use our forums! https://forums.pytorchlightning.ai/",
      "author": {
        "login": "s-rog"
      },
      "createdAt": "2020-10-29T01:27:22Z"
    }
  },
  {
    "title": "iterations/gpu don't scale when using custom sampler",
    "body": "Hi! I'm currently using Pytorch's weighted random sampler for my multi-class skewed dataset and I've put \"use_ddp_sampler\" to False.\r\nEverything works well with the custom sampler but it's taking a significantly longer time to train my model on multiple GPUs.\r\nI notice that with the default sampler my 2000 iterations scale well over 4 GPUs, 500 iterations/gpu but with the custom sampler, it doesn't scale and stays at 2000 iterations/GPU over 4 GPUs - highly likely explaining the longer training times.\r\n\r\nHow can I speed up my training procedure using a custom sampler?\r\n\r\nEdit: Trainer settings:\r\n```\r\ntrainer = pl.Trainer(   gpus                = args.gpus, \r\n                            num_nodes           = 1,\r\n                            distributed_backend = 'ddp', \r\n                            max_epochs          = args.epochs,\r\n                            weights_save_path   = args.weights_save_path, \r\n                            logger              = True,\r\n                            replace_sampler_ddp = False,\r\n                            checkpoint_callback = checkpoint_callback)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/3716",
    "createdAt": "2020-09-29T06:45:35Z",
    "updatedAt": "2022-06-07T14:25:45Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "bartmch"
    },
    "answer": {
      "body": "All you need is a custom sampler here + set `replace_sampler_ddp=False` in `Trainer`. `WeightedRandomSampler` just uses the `weights` which you need to define to sample the batch from the dataset something similar to what boosting algorithms do while sampling. In your use case, you need some kind of `DistributedBalancedSampler` that can do either oversampling or undersampling. There are some discussions [here](https://github.com/pytorch/pytorch/issues/23430) which partially might solve your use-case in the future. But for now neither lightning nor PyTorch has this yet. But you check some custom sampler [here](https://catalyst-team.github.io/catalyst/api/data.html#samplers) that might help.",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2020-09-29T19:24:31Z"
    }
  },
  {
    "title": "How to scale learning rate with batch size for DDP training?",
    "body": "When using LARS optimizer, usually the batch size is scale linearly with the learning rate.\r\nSuppose I set the **base_lr** to be **0.1 * batch_size / 256.**\r\nNow for 1 GPU training with batch size 512, the learning rate should be 0.1 * 2 = **0.2**\r\n\r\nHowever when I use **2 GPUs** with **DDP** backend and **batch size** of **512** on each GPU. Should my learning rate be:\r\n\r\n- 0.1 * 2 = **0.2**\r\n- or 0.1 * 2 *  2 (no. GPUs) = **0.4**\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/3706",
    "createdAt": "2020-09-28T17:14:01Z",
    "updatedAt": "2024-12-04T12:29:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "huyvnphan"
    },
    "answer": {
      "body": "As far as I know, learning rate is scaled with the batch size so that the sample variance of the gradients is kept approx. constant. \r\n\r\nSince DDP averages the gradients from all the devices, I think the LR should be scaled in proportion to the effective batch size, namely, batch_size * num_accumulated_batches * num_gpus * num_nodes\r\n\r\nIn this case, assuming batch_size=512, num_accumulated_batches=1, num_gpus=2 and num_noeds=1  the effective batch size is 1024, thus the LR should be scaled by sqrt(2), compared to a single gpus with effective batch size 512.",
      "author": {
        "login": "itsikad"
      },
      "createdAt": "2020-09-29T08:16:31Z"
    }
  },
  {
    "title": "Test step with DDP",
    "body": "Having refactored my code to avoid iterable datasets I've now got DDP training working (I also had to set ulimit to prevent another crash). However now it crashes at the test step.\r\n\r\nThe message implies DDP is not needed for testing - but I don't see any mention in the documentation of how to disable DDP once training has complete (plus I would assume that trainer.test() would do this if it were required).\r\n\r\nIs there something I should be doing different - this is my train / test code - the test dateloader has batchsize=1\r\n\r\n```\r\n    trainer.fit(transformer, datamodule=dm)\r\n    transformer.freeze()\r\n\r\n    # run tests\r\n    result = trainer.test(transformer, datamodule=dm)\r\n```\r\n\r\n> \r\n> -- Process 0 terminated with the following error:\r\n> Traceback (most recent call last):\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n>     fn(i, *args)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 152, in ddp_train\r\n>     model = model.configure_ddp(model, device_ids)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 837, in configure_ddp\r\n>     model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 269, in __init__\r\n>     assert any((p.requires_grad for p in module.parameters())), (\r\n> AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\r\n>  ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/3639",
    "createdAt": "2020-09-24T06:29:33Z",
    "updatedAt": "2022-06-20T12:22:04Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "david-waterworth"
    },
    "answer": {
      "body": "Does not look like a but to me. \r\n\r\nThe error seems to come from the underlying PyTorch DistributedDataParallel, not Lightning. \r\nIt seems we can't really do anything. But freezing the model before test should not be needed, test does not alter the weights and puts model into eval mode anyway. Makes sense?",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-09-24T23:06:04Z"
    }
  },
  {
    "title": "Expectations for custom data parallel implementations",
    "body": "I have a need to use a custom `[DistributedDataParallel](https://pytorch.org/docs/stable/notes/ddp.html)` implementation. I'd like to do this with the existing `DDPBackend` today.\r\n\r\nWith Lightning, the API and docs are unclear as to whether I need to extend `LightningDistributedDataParallel`, or if I can directly extend torch `DistributedDataParallel`.\r\n\r\nThe LightningModule API docs suggest `configure_ddp` should work with torch `DistributedDataParallel`: https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#configure-ddp\r\n\r\nHowever, there are spots in Lightning which rely on checking isinstance of the custom Lightning overrides: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/model_connector.py#L31-L34 1\r\n\r\nThe Lightning DDP also forwards calls to train/val/test step. Is this a requirement for custom DDP implementations when used with Lightning?\r\n\r\nTLDR: should I subclass `LightningDistributedDataParallel` or `DistributedDataParallel` when I implement the model hook for `configure_ddp`? \r\n\r\nAlso asked here: https://forums.pytorchlightning.ai/t/expectations-for-custom-data-parallel-implementations/162",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/3471",
    "createdAt": "2020-09-12T02:28:11Z",
    "updatedAt": "2022-06-06T20:16:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ananthsub"
    },
    "answer": {
      "body": "@ananthsub Not sure if you have seen it already, there is now a DDP Plugin in which you can override the DistributedDataParallel. It can be passed into the trainer via plugins list. \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/plugins.html?highlight=DDPPlugin\r\n",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-10-25T06:27:44Z"
    }
  },
  {
    "title": "Attribute is reset per batch in `dp` mode",
    "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI don't know whether this is a bug...\r\nAs shown in the code below, I think the behavior of `dp` mode is unexpected? (The attribute is reset every batch)\r\nWhen using `ddp` mode, everything is fine. (The property will be initialized only once per GPU)\r\n\r\n#### Code\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n\r\nfrom argparse import Namespace\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        self._dummy_property = None\r\n\r\n    @property\r\n    def dummy_propery(self):\r\n        if self._dummy_property is None:\r\n            self._dummy_property = '*' * 30\r\n            print('print only once per gpu')\r\n        return self._dummy_property\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        print(self._dummy_property)\r\n        # Access every batch\r\n        self.dummy_propery\r\n        print(self._dummy_property)\r\n\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return pl.TrainResult(loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\ntrain_loader = DataLoader(\r\n    MNIST(\r\n        os.getcwd(),\r\n        download=True,\r\n        transform=transforms.ToTensor()\r\n    ),\r\n    batch_size=128\r\n)\r\ntrainer = pl.Trainer(gpus=2,\r\n                     distributed_backend='dp',\r\n                     max_epochs=2)\r\n\r\nmodel = LitModel()\r\ntrainer.fit(model, train_loader)\r\n```\r\n\r\n\r\n#### Output\r\n\r\n```\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\n...\r\n```\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What's your environment?\r\n\r\n - Version 0.9.0\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/3301",
    "createdAt": "2020-09-01T03:40:24Z",
    "updatedAt": "2022-08-09T05:56:24Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "siahuat0727"
    },
    "answer": {
      "body": "oh, this is actually a known problem and comes from DataParallel in PyTorch itself.\r\nSee #565 and #1649 for reference.\r\n@ananyahjha93 has been working on a workaround but it seems to be super non trivial #1895",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-09-20T23:21:20Z"
    }
  },
  {
    "title": "Logging accuracy with batch accumulation",
    "body": "I wanted to ask how pytorch handles accuracy (and maybe even loss) logging when we have something like `pl.Trainer(accumulate_grad_batches=ACCUMULATIONS)`.\r\n\r\nMy training looks like this:\r\n```python\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y, weight=self.weight)\r\n        result = pl.TrainResult(loss)\r\n        result.log(\"train_loss\", loss, prog_bar=True)\r\n        result.log(\"train_accuracy\", self.accuracy(y_hat.argmax(dim=-1), y), prog_bar=True)\r\n\r\n        return result\r\n```\r\nwhere `self.accuracy = pl.metrics.classification.Accuracy()`. Is there a way to make sure that the loss and accuracy is averaged across the accumulated batches?\r\n\r\nIf this is not currently the case, I'm happy to do a PR if someone can show me where to look in the source code to make such a change.\r\n\r\nThanks in advance",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5805",
    "createdAt": "2020-08-27T06:45:47Z",
    "updatedAt": "2022-06-23T10:27:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "sachinruk"
    },
    "answer": {
      "body": "@sachinruk Class based metrics have been revamped! Please checkout the documentation for the new interface.\r\nWhile the metrics package does not directly integrate with the `accumulate_grad_batches` argument (yet), you should be able to do something like this now:\r\n``` python\r\ndef training_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    y_hat = self(x)\r\n\r\n    self.accuracy.update(y_hat.argmax(dim=-1), y)\r\n    if self.trainer.accumulate_grad_batches % self.global_step == 0:\r\n        accumulated_val = self.accuracy.compute()\r\n        self.log('acc_accumulate', accumulated_val)\r\n    ...\r\n```\r\nClosing this for now.",
      "author": {
        "login": "SkafteNicki"
      },
      "createdAt": "2020-10-15T12:56:29Z"
    }
  },
  {
    "title": "Does Pytorch-Lightning have a multiprocessing (or Joblib) module?",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI have been googling around but can't seem to find if there is a `multiprocessing` module available in Pytorch-Lightning, just like how Pytorch has a `torch.multiprocessing` module.\r\n\r\nDoes anyone know if Pytorch-Lightning has this (or a `Joblib` similar) module? I am looking for a Pytorch-Lightning module which allows me to parallelize over multiple GPUs\r\n\r\nMany thanks in advance.\r\n\r\nPs. Sorry if this this the wrong place to post this question. I have posted the same question in Stackoverflow, but haven't received a reply. \r\n\r\n**Edit:** To be more specific, I am looking for a `multiprocessing` module in Pytorch-Lightning which allows me to parallelize over multiple GPUs on non-neural network computations, such as:\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.multiprocessing import Pool\r\n\r\nX = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])\r\nX = torch.DoubleTensor(X)\r\n\r\ndef X_power_func(j):\r\n    X_power = X.cuda()**j\r\n    return X_power\r\n\r\nif __name__ == '__main__':\r\n  with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs\r\n    results = p.map(X_power_func, range(4))\r\n\r\nresults\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/2720",
    "createdAt": "2020-07-27T06:02:27Z",
    "updatedAt": "2022-06-20T01:16:51Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "leockl"
    },
    "answer": {
      "body": "PL is just PyTorch under the hood, you can use `torch` on `joblib` directly...\r\nin case you want train distributed in CPU only you can use `ddp_cpu` backend \ud83d\udc30 ",
      "author": {
        "login": "Borda"
      },
      "createdAt": "2020-07-31T15:08:16Z"
    }
  },
  {
    "title": "Multi-GPU Training GPU Usage",
    "body": "## \u2753 Multi-GPU Training GPU Usage\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### Hi, I'm using lightning and ddp as backend to do multi-gpu training, with Apex amp (amp_level = 'O1'). The gpu number is 8. I noticed that during training, most of time GPU0's utilization is 0%, while others are almost 100%. But their memory usage are the same. Is this normal? I use OpenPAI and have attached their utilization and memeory usage below. Thanks.\r\n\r\n\r\n![amp_O1_gpu_usage](https://user-images.githubusercontent.com/11988890/88447203-9c8b8d00-cdfe-11ea-8a5b-b1c2551fc4e5.png)\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win]\r\n - Packaging [e.g. pip, conda]\r\n - Version [e.g. 0.5.2.1]\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/2701",
    "createdAt": "2020-07-25T02:39:35Z",
    "updatedAt": "2022-05-31T07:03:39Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhenhuahu"
    },
    "answer": {
      "body": "Your cpu usage seems high. It could be the cpu is the bottleneck here. Try fewer gpus and observe then observe the gpu utilization. ",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-08-01T16:50:25Z"
    }
  },
  {
    "title": "What's the Correct Batch Size in Distributed Data Parallel Training?",
    "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### I'm trying to use pl and 'ddp' to do single node multi-GPU training. My code is as below. I used batch_size 2 in Dataloader. After the code, does every gpu (in total 8 gpus) each get batch_size of 2, or only 2 gpus can access the data (each has bacth_size 1)?\r\n\r\n#### train_loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\r\ntrainer = pl.Trainer(gpus=8, num_nodes = 4 distributed_backend='ddp')\r\ntrainer.fit(model, train_dataloader=train_loader)\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win]\r\n - Packaging [e.g. pip, conda]\r\n - Version [e.g. 0.5.2.1]\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/2684",
    "createdAt": "2020-07-24T03:30:23Z",
    "updatedAt": "2022-05-31T15:00:37Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "zhenhuahu"
    },
    "answer": {
      "body": "Is this what you need? https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#batch-size",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2020-07-24T06:23:54Z"
    }
  },
  {
    "title": "How do I set the steps_per_epoch parameter of a lr scheduler in multi-GPU environment?",
    "body": "#### What is your question?\r\nFor some learning rate schedulers, there is a required steps_per_epoch parameter. One example is the OneCycleLR scheduler. On a CPU or single GPU, this parameter should be set to the length of the train dataloader. My question is, how should this parameter be set on a multi-GPU machine using DDP. Does this parameter need to be updated to `len(self.train_dataloader()) / num_gpus`? Or is this done automatically?\r\n\r\n#### What have you tried?\r\nI've tried manually dividing the steps_per_epoch of the OneCycleLR scheduler by the number of GPUs when training on a multi-GPU machine. The LR doesn't seem to be following the expected update pattern and I think the scheduler may be the source of the problem.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging: conda\r\n - Version: 0.7.6\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/2149",
    "createdAt": "2020-06-11T14:45:18Z",
    "updatedAt": "2022-06-09T09:01:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "tayden"
    },
    "answer": {
      "body": "After some more investigation, it seems like dividing the dataloader size by the number of GPUs is the correct way. The documentation could be more clear on this, but I'm closing this now.",
      "author": {
        "login": "tayden"
      },
      "createdAt": "2020-06-11T22:04:10Z"
    }
  },
  {
    "title": "extremely slow training with multiple GPUs",
    "body": "I am training a  model with lightning where I am attempting to use all the GPUs on my system (4 in total).\r\n\r\nMy trainer is run as:\r\n\r\n```\r\nmodel = MyModel(hparams)\r\nif torch.cuda.is_available():\r\n    trainer = Trainer(gpus=-1)\r\nelse:\r\n    trainer = Trainer()\r\ntrainer.fit(model)\r\n```\r\n\r\nMy model is defined as follows:\r\n\r\n```\r\nclass SiameseNet(pl.LightningModule):\r\n    \"\"\"\r\n    Implement a siamese network as a feature extractor withh Lightning module\r\n    \"\"\"\r\n    def __init__(self,\r\n                 hparams):\r\n        \"\"\"\r\n        Build the network\r\n        \"\"\"\r\n        super(SiameseNet, self).__init__()\r\n        self.net = self._build_net()\r\n        self.hparams = hparams\r\n        self.train_data_path = hparams.get('train_data_path', None)\r\n        self.test_data_path = hparams.get('test_data_path', None)\r\n        self.val_data_path = hparams.get('val_data_path', None)\r\n        self.train_dataset = None\r\n        self.val_dataset = None\r\n        self.test_dataset = None\r\n\r\n        self.lossfn = TripletLoss(margin=1.0)\r\n\r\n    def forward_once(self, x):\r\n        output = self.net(x)\r\n        output = torch.squeeze(output)\r\n        return output\r\n\r\n    def forward(self, input1, input2, input3=None):\r\n        output1 = self.forward_once(input1)\r\n        output2 = self.forward_once(input2)\r\n\r\n        if input3 is not None:\r\n            output3 = self.forward_once(input3)\r\n            return output1, output2, output3\r\n\r\n        return output1, output2\r\n\r\n    @staticmethod\r\n    def _build_net():\r\n        net = nn.Sequential(\r\n            nn.Conv2d(3, 32,kernel_size=3,stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(32),\r\n\r\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(64),\r\n\r\n            nn.Conv2d(64, 128, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(128),\r\n\r\n            nn.Conv2d(128, 256, kernel_size=1, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(256),\r\n\r\n            nn.Conv2d(256, 256, kernel_size=1, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(256),\r\n\r\n            nn.Conv2d(256, 512, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(512),\r\n\r\n            nn.Conv2d(512, 1024, kernel_size=1, stride=1),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(1024))\r\n\r\n        return net\r\n\r\n    def prepare_data(self):\r\n        transform = torchvision.transforms.Compose([\r\n            torchvision.transforms.Resize((128, 128)),\r\n            torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\r\n            torchvision.transforms.RandomHorizontalFlip(),\r\n            torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\r\n            torchvision.transforms.ToTensor()\r\n        ])\r\n\r\n        if self.train_data_path:\r\n            train_folder_dataset = dset.ImageFolder(root=self.train_data_path)\r\n            self.train_dataset = SiameseTriplet(image_folder_dataset=train_folder_dataset,\r\n                                                transform=transform)\r\n        if self.val_data_path:\r\n            val_folder_dataset = dset.ImageFolder(root=self.val_data_path)\r\n            self.val_dataset = SiameseTriplet(image_folder_dataset=val_folder_dataset)\r\n\r\n        if self.test_data_path:\r\n            test_folder_dataset = dset.ImageFolder(root=self.test_data_path)\r\n            self.test_dataset = SiameseTriplet(image_folder_dataset=test_folder_dataset)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        anchor, positive, negative = batch\r\n        anchor_out, positive_out, negative_out = self.forward(anchor, positive, negative)\r\n        loss_val = self.lossfn(anchor_out, positive_out, negative_out)\r\n        return {'loss': loss_val}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.get('learning_rate', 0.001))\r\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\r\n        return [optimizer], [scheduler]\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        if self.train_dataset:\r\n            return DataLoader(self.train_dataset,\r\n                              self.hparams.get('batch_size', 64),\r\n                              num_workers=12)\r\n        return None\r\n```\r\n\r\nWhen I try and run it, it seems the beginning of the epoch hangs for like 10 minutes to get data into the model and after that the progress is  very sluggish.\r\n\r\nI also get these messages in  the beginning. Not sure if it is of concern\r\n\r\n```\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 0 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 1 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 2 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 3 world 4\r\n```\r\n\r\nIt basically hangs with this:\r\n```\r\nEpoch 1:   0%|                                                                                          | 0/172 [00:00<?, ?it/s]\r\n```\r\n\r\nDuring this time, looking at GPU  utilisation it seems:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n| 48%   79C    P2    90W / 250W |   4527MiB / 11176MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 45%   76C    P2    85W / 250W |   1636MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 45%   76C    P2    79W / 250W |   1626MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 32%   65C    P2    79W / 250W |   2689MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1584      C   /home/pd/.conda/envs/alchera37/bin/python    801MiB |\r\n|    0     10714      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    0     28957      C   /home/pd/.conda/envs/alchera37/bin/python   1047MiB |\r\n|    0     30880      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    0     32266      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    1     10733      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    1     28972      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    2     10789      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    2     32297      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    3     10807      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    3     29006      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    3     30967      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nSo, it seems that getting the data into the GPU is quite slow even though everything looks maxed out.\r\n\r\nAnd when it does eventually start the epoch after ~30 minutes, it seems to give similar performance as my CPU on MacBook Pro. I am really not sure if I am doing somethingvery  wrong here in how I am using  PL.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/2065",
    "createdAt": "2020-06-03T23:53:57Z",
    "updatedAt": "2022-06-13T05:08:53Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "pamparana34"
    },
    "answer": {
      "body": "> And then it gets stuck after this line (at least no console output)\r\n\r\nThis looks very familiar, and I am sure I fixed this problem in #2997, please try again with the latest version. Regarding the relative import error, you probably just launched the script in the wrong directory, but anyway I recommend to use absolute imports. Please let me know if the upgrade fixes your problem, thanks.",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2020-09-20T00:30:13Z"
    }
  },
  {
    "title": "How to gather results on multiple GPUs while testing? ddp",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI want to test summarization model from [huggingface summarization example](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py) on multiple GPUs . My problem is **how could I collect test results  on different GPUs** , since `test_epoch_end` only processes epoch for a single GPU. \r\nFor more information, the model is trained with ddp backend.\r\n#### Code\r\n```   \r\n def test_epoch_end(self, outputs):\r\n        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\r\n        output_test_targets_file = os.path.join(self.hparams.output_dir, \"test_targets.txt\")\r\n        # write predictions and targets for later rouge evaluation.\r\n        with open(output_test_predictions_file, \"w+\") as p_writer, open(output_test_targets_file, \"w+\") as t_writer:\r\n            for output_batch in outputs:\r\n                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\r\n                t_writer.writelines(s + \"\\n\" for s in output_batch[\"target\"])\r\n            p_writer.close()\r\n            t_writer.close()\r\n\r\n        return self.test_end(outputs)\r\n```\r\n\r\n#### What have you tried?\r\nFor now, I can only use single GPU to get result of whole dataset.\r\n#### What's your environment?\r\n\r\n - OS: Unbuntu 18.04\r\n - Packaging pip\r\n - Version: 0.7.6\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1974",
    "createdAt": "2020-05-27T17:53:23Z",
    "updatedAt": "2022-06-23T15:00:57Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "joe32140"
    },
    "answer": {
      "body": "Use `torch.distributed.all_gather` to gather and merge the outputs from all GPUs.\r\nAnd you should remove the redundant examples due to the ddp_sampler adds extra examples to work with multi GPUS. (https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler)\r\n\r\nHere is the workaround snippet used in my own project.\r\n\r\n```\r\ndef gather_distributed(*tensors):\r\n    output_tensors = []\r\n    for tensor in tensors:\r\n        tensor_list = [torch.ones_like(tensor) for _ in range(dist.get_world_size())]\r\n        dist.all_gather(tensor_list, tensor)\r\n        output_tensors.append(torch.cat(tensor_list))\r\n    return output_tensors\r\n\r\n\r\ndef deduplicate_and_sort(index, *tensors):\r\n    reverse_index = torch.zeros_like(index)\r\n    for ri, i in enumerate(index):\r\n        reverse_index[i] = ri\r\n    reverse_index = reverse_index[:index.max() + 1]\r\n    output_tensors = [tensor.index_select(0, reverse_index) for tensor in tensors]\r\n    return output_tensors\r\n```\r\nIn the above code, you need the **index** of each example to remove redundant examples and sort outputs in order.\r\nNotice that the index should consist of consecutive integers (e.g., 0,1,2,...N).\r\n",
      "author": {
        "login": "haichao592"
      },
      "createdAt": "2020-05-29T16:27:58Z"
    }
  },
  {
    "title": "slow new epoch start with setting ddp, num_workers, gpus",
    "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am training MNIST with below code. 1 GPU training is ok.\r\nBut it shows slow start of new epoch when num_workers is a large number and the number of gpus > 2.\r\nEven dataloading itself is slower than with 1gpu.\r\n#### Code\r\n\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl \r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision.datasets import MNIST \r\nfrom torch.nn import functional as F\r\nimport torch.distributed as dist\r\nimport os, sys\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        super(LightningMNISTClassifier, self).__init__()\r\n        self.layer_1 = nn.Linear(28*28, 128)\r\n        self.layer_2 = nn.Linear(128, 256)\r\n        self.layer_3 = nn.Linear(256, 10)\r\n    \r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        x = x.view(batch_size, -1)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_3(x)\r\n        x = torch.log_softmax(x, dim=-1)\r\n        return x\r\n\r\n    def prepare_data(self):\r\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081,))])\r\n        mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\r\n        self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\r\n        self.mnist_train, self.mnist_val = random_split(mnist_train, [55000,5000])\r\n    \r\n    def train_dataloader(self):\r\n        data_loader2 = DataLoader(self.mnist_train, batch_size=64, num_workers=7, shuffle=True) \r\n        # data_loader2 = DataLoader(self.mnist_train, batch_size=64, shuffle=True) \r\n        return data_loader2\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=64)\r\n\r\n    # def test_dataloader(self):\r\n    #     return DataLoader(self.mnist_test, batch_size=64)\r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n    \r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.nll_loss(logits, labels)\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        logs = {'train_loss': loss}\r\n        return {\"loss\": loss, \"log\": logs}\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, 'log':tensorboard_logs}\r\n\r\nif __name__ == '__main__':\r\n    model = LightningMNISTClassifier()\r\n\r\n    trainer = pl.Trainer(gpus=4, distributed_backend='ddp')\r\n\r\n    trainer.fit(model)\r\n#### What have you tried?\r\nHorovod backend does not show slow start of new epoch.\r\n\r\n#### What's your environment?\r\n\r\n - OS: ubuntu 18.04\r\n - Packaging pip\r\n - Version pytorch 1.5.0, 0.7.6\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1884",
    "createdAt": "2020-05-19T06:46:31Z",
    "updatedAt": "2022-06-01T21:25:31Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jeon30c"
    },
    "answer": {
      "body": "I found that the slow deprecationwarnings shown above  are due to the torchvision library. I changed to a simple dataset and the slow start disappeared until now. ",
      "author": {
        "login": "jeon30c"
      },
      "createdAt": "2020-05-20T09:42:13Z"
    }
  },
  {
    "title": "Is it possible for SLURM auto submit to work on DP?",
    "body": "In my experience, it never works. I looked to the trainer code and saw that the code managing this works only on DDP.\r\n``` python3\r\ndef configure_slurm_ddp(self, num_gpu_nodes):\r\n    self.is_slurm_managing_tasks = False\r\n\r\n    ### !!HERE!!\r\n    if self.use_ddp:\r\n        self.num_requested_gpus = self.num_gpus * num_gpu_nodes\r\n        self.num_slurm_tasks = 0\r\n        try:\r\n            self.num_slurm_tasks = int(os.environ['SLURM_NTASKS'])\r\n            self.is_slurm_managing_tasks = self.num_slurm_tasks == self.num_requested_gpus\r\n\r\n            # in interactive mode we don't manage tasks\r\n            job_name = os.environ['SLURM_JOB_NAME']\r\n            if job_name == 'bash':\r\n                self.is_slurm_managing_tasks = False\r\n\r\n        except Exception:\r\n            # likely not on slurm, so set the slurm managed flag to false\r\n            self.is_slurm_managing_tasks = False\r\n```\r\n\r\n\r\n However, sometimes we are not using the distributed computing on slurm (only DP). It would be nice to have the auto resubmit feature still working in this situation.\r\n\r\n\r\n - OS: Linux\r\n - Packaging conda\r\n - Version 16",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1456",
    "createdAt": "2020-04-11T13:58:03Z",
    "updatedAt": "2022-06-13T20:03:38Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "philip30"
    },
    "answer": {
      "body": "actually, lightning supports slurm no matter what backend you use...\r\n\r\n```\r\n    def register_slurm_signal_handlers(self):\r\n        # see if we're using slurm (not interactive)\r\n        on_slurm = False\r\n        try:\r\n            job_name = os.environ['SLURM_JOB_NAME']\r\n            if job_name != 'bash':\r\n                on_slurm = True\r\n        except Exception as e:\r\n            pass\r\n\r\n        if on_slurm:\r\n            log.info('Set SLURM handle signals.')\r\n            signal.signal(signal.SIGUSR1, self.sig_handler)\r\n            signal.signal(signal.SIGTERM, self.term_handler)\r\n```",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2020-04-13T22:07:22Z"
    }
  },
  {
    "title": "How to use pytorch-lightning distributed training without SLURM?",
    "body": "## \u2753How to use pytorch-lightning distributed training without SLURM?\r\n\r\nCouldn't find anywhere a single note or tutorial on this.\r\n\r\nFor example I have just 2 node with 4 GPUs on each.\r\nOn each node environment variables required for Pytorch distributed communication are configured (see [pytorch documentation](https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods)).\r\n\r\nIs this possible to train pytorch-lightning script in this setup and if so how?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1334",
    "createdAt": "2020-04-01T22:15:04Z",
    "updatedAt": "2022-06-03T03:24:09Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "areshytko"
    },
    "answer": {
      "body": "you can configure your own environment variables and do your own setup.\r\n\r\nJust override ```LightningModule.init_ddp_connection```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#lightningmodule-class\r\n\r\n\r\n(corrected)",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2020-04-04T12:33:27Z"
    }
  },
  {
    "title": "online hard-example mining/examining under Multi-GPU ='dp'",
    "body": "**Background**\r\n\r\nHi, I try to track the prediction of each individual sample during training/validation-step. The main purpose is to do online hard-example mining/examining. \r\n\r\nI found out a way of doing this is to make the input variable of the functions **training/validation_step**  carrying the sample-id information, for example, the file-name. So I made the input to be a dictionary.\r\n\r\n**Example Code**  \r\n\r\n    class LightningModule():\r\n        def validation_step(self, batch, batch_idx):\r\n            y = batch['target'].float()\r\n            y_hat = self.forward(batch) \r\n            loss = self.get_loss(y_hat, y) \r\n\r\n            # append the individual result \r\n            for i in range(len(batch['sample_id'])):\r\n                self.validation_result['prediction_result'].append(y_hat[i])\r\n                self.validation_result['sample_id'].append(batch['sample_id'][i])\r\n                self.validation_result['target'].append(batch['target'][i])\r\n            return {'val_loss': loss}\r\n\r\n        def forward(self, batch):\r\n            x = batch['x']\r\n            y_hat = self.model( x)\r\n            return y_hat\r\n\r\n**Input-Dict works in Single GPU but fail under multi-GPUs-dp** \r\n\r\n    input_batch = {  \r\n        'x' : Tensor (1st dimension as batch), \r\n        'target':  Tensor (1st dimension as batch), \r\n        'sample-id': [a, b, c] (list-object) \r\n    }\r\n\r\n\r\nAND It takes me some time to realize that all value-objects inside the input-dictionary should be torch.Tensor, not list contains strings, otherwise while training under Multi-GPU ='dp' mode, the list-obj won't be separated properly.  \r\n \r\n**Input-Dict works in both Single/multi-GPUs-dp** \r\n\r\n    input_batch = {  \r\n        'x' : Tensor (1st dimension as batch), \r\n        'target':  Tensor (1st dimension as batch), \r\n        'sample-id': 1D-Tensor for sample-id  ex Tensor([1 , 3, 5]) \r\n    }\r\n\r\n\r\n\r\nCurrently, I still have some doubts on this approach... \r\nDoes anyone try to implement similar functions, online hard-example mining, with different approaches?  \r\nTks : ) \r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1170",
    "createdAt": "2020-03-17T09:32:12Z",
    "updatedAt": "2022-06-03T10:02:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "KentChun33333"
    },
    "answer": {
      "body": "have you considered using a library such as [`pytorch-metric-learning`](https://kevinmusgrave.github.io/pytorch-metric-learning/)?\r\n\r\nin general, it would look something like \r\n\r\n```\r\nclass MinerNetwork(pl.LightningModule):\r\n  def __init__(...):\r\n    self.network = # define network here\r\n    self.miner_function = miners.DistanceWeightedMiner()\r\n    self.objective = losses.TripletMarginLoss()\r\n\r\n  def forward(self, data, labels):\r\n    embeddings = self.network(data)\r\n    return embeddings\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    data, labels = batch\r\n    embeddings = self(data)\r\n    pairs = self.miner_function(embeddings, labels)\r\n    loss = self.objective(embeddings, labels, pairs)\r\n    return loss\r\n```\r\n\r\nthis does mining within each batch that you pass in. i'm not sure where you're doing the mining currently but it seems suspicious to be appending data to a class attribute (`self.validation_result`). this will likely break if you try running on ddp because you send a copy of the model to each worker.",
      "author": {
        "login": "jeremyjordan"
      },
      "createdAt": "2020-03-21T01:56:51Z"
    }
  },
  {
    "title": "Why MultiGPU dp seems slower?",
    "body": "## \u2753 Questions and Help\r\n\r\n#### Having 2 gpus with DP seems to be slowers than using just 1. Is it normal?\r\nMy intuition is that if you are using 2 GPUs and the batch is being splitted into 2 batches, this should be faster. But when I tested the same code using 1 vs >1 my epoch time increased\r\n\r\n#### Code\r\n\r\n[Minimalist Implementation of a BERT Sentence Classifier](https://github.com/ricardorei/lightning-text-classification)\r\n\r\n#### What have you tried?\r\n\r\nI also tried to run ddp but my code seems to break with a `TypeError: cannot serialize '_io.TextIOWrapper' object` error. I searched online but I couldn't find the reason...\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/1005",
    "createdAt": "2020-03-02T10:47:23Z",
    "updatedAt": "2022-07-01T06:58:01Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "ricardorei"
    },
    "answer": {
      "body": "you should double your batch size.\r\ndp still has overhead in communication, so it won't be linear scaling.\r\n\r\nalso try ddp",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2020-03-03T04:40:16Z"
    }
  },
  {
    "title": "Where is EarlyStopping searching for metrics?",
    "body": "#### Where is EarlyStopping search for metrics?\r\n\r\n#### Code\r\n\r\n```\r\n    def validation_end(self, outputs):\r\n        ...\r\n        metrics = {\r\n        'val_acc': val_acc,\r\n        'val_loss': val_loss\r\n        }\r\n        ...\r\n        output = OrderedDict({\r\n            'val_acc':  torch.tensor(metrics['val_acc']),\r\n            'val_loss': torch.tensor(metrics['val_loss']),\r\n            'progress_bar': metrics,\r\n            'log': metrics\r\n        })\r\n        return output\r\n```\r\n\r\nif I attempt to early stop according to `val_acc` I get the following error:\r\n```\r\nRuntimeWarning: Early stopping conditioned on metric 'val_acc' which is not available. Available metrics are: loss,train_loss\r\n```\r\nThe metrics mentioned (loss,train_loss) are from `training_step` from what [I could find](https://github.com/williamFalcon/pytorch-lightning/blob/12edc3099cd0d529b4ff0553f5c7cbe9f47dfdfb/pytorch_lightning/trainer/training_loop.py#L461-L467).\r\n\r\nI guess I'm doing something wrong, could anyone point me in the correct direction?\r\n\r\n - OS: Ubuntu\r\n - Packaging: pip\r\n - Version 0.5.3.2\r\n\r\n---\r\n\r\n**Update #1**: the same code works with version `0.5.1`. Bug in `0.5.3`?\r\n\r\n**Update #2**:\r\nI found that [this line](https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L539) in `trainer/training_loop.py`:\r\n```\r\nself.callback_metrics = {k: v for d in all_callback_metrics for k, v in d.items()}\r\n```\r\nFrom what I see, before this line is executed, `self.callback_metrics` contains `val_acc`. After this line values that were put in `callback_metrics` after validation are gone, therefore `EarlyStopping` can't find them. Can anyone confirm this is an issue?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5822",
    "createdAt": "2020-01-08T14:12:11Z",
    "updatedAt": "2022-07-06T04:26:25Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "felixkreuk"
    },
    "answer": {
      "body": "If I understand correctly it is a known issue. Please look at #490. #492 fixes this in master.",
      "author": {
        "login": "kuynzereb"
      },
      "createdAt": "2020-01-09T09:34:47Z"
    }
  },
  {
    "title": "What is hparams exactly?",
    "body": "Hi, thanks for the nice product again.\r\n\r\nFrom #525 and #599, I could guess that `hparams` is required to load a saved model (which I think should be mentioned somewhere in the doc btw). And from the examples, seems like `hparams` may be `argparse.Namespace`. Unfortunately though, it was not so easy to understand the concept. \r\n\r\nWhat is `hparams` exactly? What kind of information it should/can/should not include to work properly? Is it recommended to use [`hyperparameter argument parser`](https://williamfalcon.github.io/test-tube/hyperparameter_optimization/HyperOptArgumentParser/)? Say, if I'm not into hyperparameter search at the moment and just want to be able to load the checkpoint model, what is the requirement on the hparams?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5821",
    "createdAt": "2019-12-26T07:46:27Z",
    "updatedAt": "2023-01-04T01:42:10Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "keunwoochoi"
    },
    "answer": {
      "body": "It should be an [`argparse.Namespace`](https://docs.python.org/3.7/library/argparse.html#argparse.Namespace). You can get this from `argparse`, `testtube`'s `HyperOptArgumentParser`, or create it manually from a dict like so: `argparse.Namespace(**my_dict)`.",
      "author": {
        "login": "neggert"
      },
      "createdAt": "2020-01-02T17:23:53Z"
    }
  },
  {
    "title": "Call or Forward?",
    "body": "Hi, thanks for the nice library. In the `readme`, the example uses  `model.forward(x)` not `model(x)`. But wouldn't it usually recommended to use `model(x)` so that other things (hooks etc) can be, well, hooked as well? What's the best practice? ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5811",
    "createdAt": "2019-12-17T16:23:10Z",
    "updatedAt": "2023-07-19T06:09:02Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "keunwoochoi"
    },
    "answer": {
      "body": "forward should implement what you want to use when calling model(x). \r\n\r\nyou may need to call that in training step (usually do), which means you have to do self.forward(...) because you are in the model when you make that call. ",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-12-17T16:29:11Z"
    }
  },
  {
    "title": "Restore the best model",
    "body": "What would be the most lightning way to restore the best model? Either directly after training (in the same script) or for later use (in another script)? \r\n\r\nThanks in advance !",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5812",
    "createdAt": "2019-12-03T10:17:45Z",
    "updatedAt": "2022-06-21T12:12:07Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "mpariente"
    },
    "answer": {
      "body": "You can use the checkpont callback to save the only the best model as described here: https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/ (note that that doc needs to be updated. Use `save_top_k=1` instead of `save_best_only`)\r\n\r\nYou can then use the `load_from_checkpoint` method to restore your checkpoint: https://williamfalcon.github.io/pytorch-lightning/LightningModule/methods/#load_from_metrics",
      "author": {
        "login": "neggert"
      },
      "createdAt": "2019-12-03T15:49:04Z"
    }
  },
  {
    "title": "Running Average of my accuracy, losses etc.",
    "body": "#### What is your question?    \r\nI want my tqdm logger to show me a history of my training on the terminal. Right now, when a epoch ends, all data for it is scrubbed from the command line and the new epoch data is shown.\r\n\r\nAlso I want to see the running accuracy of my network and a running average of my loss on the tqdm bar. How should I go on about doing that ?\r\n#### What have you tried?    \r\nI have looked at the docs and logging but am unable to figure out how to modify the tqdm logger, more so maintain a running average\r\n#### What's your environment?   \r\n\r\n- conda version: latest\r\n- PyTorch version   : 1.3\r\n- Lightning version : pip install pytorch-lightning at the date of this issue  \r\n- Test-tube version: came boot strapped with lightning.\r\n\r\nI installed everything on the date of this issue.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5819",
    "createdAt": "2019-11-10T05:16:24Z",
    "updatedAt": "2022-08-17T07:53:11Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "karanchahal"
    },
    "answer": {
      "body": "Use tensorboard!\r\n\r\nFor running averages you have to implement the logic in training step",
      "author": {
        "login": "s-rog"
      },
      "createdAt": "2019-11-11T01:36:35Z"
    }
  },
  {
    "title": "How set number of epochs",
    "body": "How do I set the number of epochs to train?\r\n\r\n#### What have you tried?    \r\nLooking for documentation. \r\nLooking for examples.\r\n\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5818",
    "createdAt": "2019-11-08T21:18:18Z",
    "updatedAt": "2022-07-13T08:14:48Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "Vichoko"
    },
    "answer": {
      "body": "Up-to-date link:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-epochs",
      "author": {
        "login": "lyndond"
      },
      "createdAt": "2021-03-10T04:47:53Z"
    }
  },
  {
    "title": "Simple way to get the best scores at the end of a training?",
    "body": "### What is your question?    \r\nAfter a training, is there an easy way to get the best scores returned by the `validation_end` function? In order to use a hyperparameters optimizer like [Tune](https://ray.readthedocs.io/en/latest/tune.html).\r\n\r\n#### Code example\r\n```\r\nmodel = CoolSystem()\r\ntrainer = Trainer()    \r\ntrainer.fit(model)   \r\n\r\nbest_scores = ???\r\nprint(best_scores)\r\n```\r\n\r\n#### What's your environment?   \r\n- conda 4.7.10   \r\n- PyTorch 1.3.0   \r\n- Lightning 0.5.3.1\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5817",
    "createdAt": "2019-11-08T11:05:19Z",
    "updatedAt": "2022-10-17T10:56:56Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "jgsch"
    },
    "answer": {
      "body": "Some options:\r\n- print the scores in training_end\r\n- log them to the logger\r\n- access trainer.tqdm_metrics",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-11-09T02:21:06Z"
    }
  },
  {
    "title": "About the Weight Initialization in PL",
    "body": "Hi,\r\n\r\nI am tring to use BERT for a project. The pretrained BERT model is part of my model. I am wondering how will PL initialize the model weights. Will it overwrite the pretrained BERT weights?\r\n\r\nThanks.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5816",
    "createdAt": "2019-11-08T02:28:25Z",
    "updatedAt": "2024-05-13T11:03:52Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "magic282"
    },
    "answer": {
      "body": "lightning doesn\u2019t do any magic like this under the hood. you control all the weights and what gets initiated ",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-11-08T02:33:44Z"
    }
  },
  {
    "title": "Training/Validation split in minimal example",
    "body": "If you still can't find what you need:     \r\n#### What is your question?    \r\n\r\nI think it's unclear how the training data is split into a training and validation split in the minimal example.\r\n\r\n(https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#minimal-example)\r\n\r\nDoes this example use all training data for both training and validation? As far as I'm aware, this is bad practice. \r\n\r\nIs there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5815",
    "createdAt": "2019-11-04T09:53:27Z",
    "updatedAt": "2022-06-12T21:31:23Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "nikvaessen"
    },
    "answer": {
      "body": "> Is there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\r\n\r\nNo, I don't think this is happening. The dataset in the minimal example is the MNIST dataset, which only has two splits (train and test). In this example, the validation set is the same as the training set and if we wanted to split it, we would have to use something like [torch.utils.random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split).",
      "author": {
        "login": "awaelchli"
      },
      "createdAt": "2019-11-04T11:57:17Z"
    }
  },
  {
    "title": "Unfreezing layers during training?",
    "body": "Freezing layers at the beginning of training works, however unfreezing in ```on_epoch_start()``` during training causes the gradient to explode. Without the unfreezing part (or without freezing at all), the model trains fine with no gradient issues.\r\n\r\nI'm using DDP + Apex O2 and the loss scaling will keep going down to 0 where it would encounter 0 division and crash.\r\n\r\nIs unfreezing during training not possible in pytorch/lightning? or am I missing snippet?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5814",
    "createdAt": "2019-11-02T03:49:48Z",
    "updatedAt": "2022-07-19T21:32:35Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "s-rog"
    },
    "answer": {
      "body": "you can unfreeze whenever. if gradients explode it's for another reason",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2020-01-21T12:45:17Z"
    }
  },
  {
    "title": "Multiple optimizers but only one loss",
    "body": "Hey! I have a question regarding this library. Really, like how it forces me to structure my code better. I encountered one problem I did not know how to solve based on the documentation.\r\n\r\nLet's say I have two optimizers for two parts of the network, e.g. my `configure_optimizers()` looks like this:\r\n```\r\n    def configure_optimizers(self):\r\n        optimizer_encoder = optim.Adam(self.encoder.parameters(), ...)\r\n        optimizer_decoder = optim.Adam(self.decoder.parameters(), ...)\r\n        return [optimizer_encoder, optimizer_decoder]\r\n```\r\n\r\nnow in the training loop I forward pass the encoder, then the decoder and compute my loss based on the output: \r\n```\r\n    def training_step(self, batch, batch_nb, optimizer_idx):\r\n        inp, gt = ...\r\n\r\n        encoding = self.encoder(inp)\r\n        pred = self.decoder(encoding)\r\n\r\n        loss = F.mse_loss(pred, gt)\r\n        return {'loss': loss}\r\n```\r\n Since I have two optimizers I have to respect that this function is called two times with different `optimizer_idx` however I just have one loss to backprop. How would I go about this?  \r\n\r\n#### What have you tried? \r\nI tried something like this\r\n```\r\n    def training_step(self, batch, batch_nb, optimizer_idx):\r\n        if optimizer_idx == 1:\r\n              return {}\r\n        inp, gt = ...\r\n\r\n        encoding = self.encoder(inp)\r\n        pred = self.decoder(encoding)\r\n\r\n        loss = F.mse_loss(pred, gt)\r\n        return {'loss': loss}\r\n```\r\nHowever, this leads to an error since no loss key is present in `trainer.py:1392`.\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/5813",
    "createdAt": "2019-10-21T17:02:11Z",
    "updatedAt": "2022-09-15T17:10:17Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "selflein"
    },
    "answer": {
      "body": "in that case, just pass in both sets of params to a single optimizer",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-10-22T02:11:09Z"
    }
  },
  {
    "title": "How to get gpu id corresponding to each process in 'ddp'?",
    "body": "In ddp mode, if I use 8 gpus, then it will creates 8 processes for each gpu, if I want to create new tensor at runtime, can I use `.cuda(self.trainer.root_gpu)`? will it use correct gpu corresponding to its process?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/264",
    "createdAt": "2019-09-27T13:08:08Z",
    "updatedAt": "2022-06-10T23:57:50Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "xiadingZ"
    },
    "answer": {
      "body": "yes",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-09-27T13:36:31Z"
    }
  },
  {
    "title": "How to analysis the time cost of each part",
    "body": "#### What is your question?    \r\nHi, I'm trying to implement my project with your framework, however, I'd like to count the time each part costs to make full use of GPUs, but it's puzzling that the time count by myself is not the same as tqdm does. So could you give me some advice about what happened? From process bar, the time is 1.4s/it while data time is 0.003s, gpu time is 0.5~0.7s.\r\n#### Code    \r\n```python\r\n            # what I add to the trainer\r\n            # code added by me\r\n            batch_start_tic = time.time()\r\n            for batch_nb, data_batch in enumerate(self.tng_dataloader):\r\n                self.batch_nb = batch_nb\r\n                self.global_step += 1\r\n\r\n                model = self.__get_model()\r\n                model.global_step = self.global_step\r\n\r\n                # stop when the flag is changed or we've gone past the amount\r\n                #  requested in the batches\r\n                self.total_batch_nb += 1\r\n                met_batch_limit = batch_nb > self.nb_tng_batches\r\n                if met_batch_limit:\r\n                    break\r\n\r\n                # ---------------\r\n                # RUN TRAIN STEP\r\n                # ---------------\r\n                batch_fb = time.time()\r\n                batch_result = self.__run_tng_batch(data_batch, batch_nb)\r\n                early_stop_epoch = batch_result == -1\r\n                # code added by me\r\n                batch_fb_end = time.time()\r\n                self.__add_tqdm_metrics({'data time': batch_fb-batch_start_tic,'gpu time': batch_fb_end-batch_fb})\r\n                batch_start_tic = time.time()\r\n```\r\nBy the way, I find the gpu utils is about 80%, is there any tricks can make it up to 100%?\r\n#### What's your environment?   \r\n- PyTorch version   1.1.0\r\n- Lightning version   0.3.6.9\r\n- Test-tube version  0.6.7.6\r\n\r\nThanks a lot.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/112",
    "createdAt": "2019-08-14T07:50:34Z",
    "updatedAt": "2022-06-04T03:07:22Z",
    "closedAt": null,
    "isAnswered": true,
    "author": {
      "login": "MendelXu"
    },
    "answer": {
      "body": "tqdm time is a running average. you have to let it warm up for a bit before it converges to the correct time. ",
      "author": {
        "login": "williamFalcon"
      },
      "createdAt": "2019-08-14T10:46:46Z"
    }
  }
]