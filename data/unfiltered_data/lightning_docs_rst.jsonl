{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/accelerator_prepare.html", "url_rel_html": "accelerators/accelerator_prepare.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/accelerator_prepare.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Hardware agnostic training (preparation)¶", "rst_text": ":orphan:\n\n########################################\nHardware agnostic training (preparation)\n########################################\n\nTo train on CPU/GPU/TPU without changing your code, we need to build a few good habits :)\n\n----\n\n*****************************\nDelete .cuda() or .to() calls\n*****************************\n\nDelete any calls to .cuda() or .to(device).\n\n.. testcode::\n\n    # before lightning\n    def forward(self, x):\n        x = x.cuda(0)\n        layer_1.cuda(0)\n        x_hat = layer_1(x)\n\n\n    # after lightning\n    def forward(self, x):\n        x_hat = layer_1(x)\n\n----\n\n************************************************\nInit tensors using Tensor.to and register_buffer\n************************************************\nWhen you need to create a new tensor, use ``Tensor.to``.\nThis will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.\n\n.. testcode::\n\n    # before lightning\n    def forward(self, x):\n        z = torch.Tensor(2, 3)\n        z = z.cuda(0)\n\n\n    # with lightning\n    def forward(self, x):\n        z = torch.Tensor(2, 3)\n        z = z.to(x)\n\nThe :class:`~lightning.pytorch.core.LightningModule` knows what device it is on. You can access the reference via ``self.device``.\nSometimes it is necessary to store tensors as module attributes. However, if they are not parameters they will\nremain on the CPU even if the module gets moved to a new device. To prevent that and remain device agnostic,\nregister the tensor as a buffer in your modules' ``__init__`` method with :meth:`~torch.nn.Module.register_buffer`.\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def __init__(self):\n            ...\n            self.register_buffer(\"sigma\", torch.eye(3))\n            # you can now access self.sigma anywhere in your module\n\n----\n\n***************\nRemove samplers\n***************\n\n:class:`~torch.utils.data.distributed.DistributedSampler` is automatically handled by Lightning.\n\nSee :ref:`replace-sampler-ddp` for more information.\n\n----\n\n***************************************\nSynchronize validation and test logging\n***************************************\n\nWhen running in distributed mode, we have to ensure that the validation and test step logging calls are synchronized across processes.\nThis is done by adding ``sync_dist=True`` to all ``self.log`` calls in the validation and test step. This will automatically average values across all processes.\nThis ensures that each GPU worker has the same behaviour when tracking model checkpoints, which is important for later downstream tasks such as testing the best checkpoint across all workers.\nThe ``sync_dist`` option can also be used in logging calls during the step methods, but be aware that this can lead to significant communication overhead and slow down your training.\n\nNote if you use any built in metrics or custom metrics that use `TorchMetrics <https://torchmetrics.readthedocs.io/>`_, these do not need to be updated and are automatically handled for you.\n\n.. testcode::\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n        self.log(\"validation_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss(logits, y)\n        # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n\nIt is possible to perform some computation manually and log the reduced result on rank 0 as follows:\n\n.. code-block:: python\n\n    def __init__(self):\n        super().__init__()\n        self.outputs = []\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        tensors = self(x)\n        self.outputs.append(tensors)\n        return tensors\n\n\n    def on_test_epoch_end(self):\n        mean = torch.mean(self.all_gather(self.outputs))\n        self.outputs.clear()  # free memory\n\n        # When you call `self.log` only on rank 0, don't forget to add\n        # `rank_zero_only=True` to avoid deadlocks on synchronization.\n        # Caveat: monitoring this is unimplemented, see https://github.com/Lightning-AI/pytorch-lightning/issues/15852\n        if self.trainer.is_global_zero:\n            self.log(\"my_reduced_metric\", mean, rank_zero_only=True)\n\n\n----\n\n\n**********************\nMake models pickleable\n**********************\nIt's very likely your code is already `pickleable <https://docs.python.org/3/library/pickle.html>`_,\nin that case no change in necessary.\nHowever, if you run a distributed model and get the following error:\n\n.. code-block::\n\n    self._launch(process_obj)\n    File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47,\n    in _launch reduction.dump(process_obj, fp)\n    File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n    _pickle.PicklingError: Can't pickle <function <lambda> at 0x2b599e088ae8>:\n    attribute lookup <lambda> on __main__ failed\n\nThis means something in your model definition, transforms, optimizer, dataloader or callbacks cannot be pickled, and the following code will fail:\n\n.. code-block:: python\n\n    import pickle\n\n    pickle.dump(some_object)\n\nThis is a limitation of using multiple processes for distributed training within PyTorch.\nTo fix this issue, find your piece of code that cannot be pickled. The end of the stacktrace\nis usually helpful.\nie: in the stacktrace example here, there seems to be a lambda function somewhere in the code\nwhich cannot be pickled.\n\n.. code-block::\n\n    self._launch(process_obj)\n    File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47,\n    in _launch reduction.dump(process_obj, fp)\n    File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n    _pickle.PicklingError: Can't pickle [THIS IS THE THING TO FIND AND DELETE]:\n    attribute lookup <lambda> on __main__ failed\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu.html", "url_rel_html": "accelerators/gpu.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Accelerator: GPU training¶", "rst_text": ".. _gpu:\n\nAccelerator: GPU training\n=========================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware\n   :col_css: col-md-4\n   :button_link: accelerator_prepare.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of single and multi-GPU training.\n   :col_css: col-md-4\n   :button_link: gpu_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn about different distributed strategies, torchelastic and how to optimize communication layers.\n   :col_css: col-md-4\n   :button_link: gpu_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Train models with billions of parameters\n   :col_css: col-md-4\n   :button_link: gpu_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Expert\n   :description: Develop new strategies for training and deploying larger and larger models.\n   :col_css: col-md-4\n   :button_link: gpu_expert.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: FAQ\n   :description: Frequently asked questions about GPU training.\n   :col_css: col-md-4\n   :button_link: gpu_faq.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu_advanced.html", "url_rel_html": "accelerators/gpu_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "GPU training (Advanced)¶", "rst_text": ":orphan:\n\n.. _gpu_advanced:\n\nGPU training (Advanced)\n=======================\n**Audience:** Users looking to scale massive models (ie: 1 Trillion parameters).\n\n----\n\nFor experts pushing the state-of-the-art in model development, Lightning offers various techniques to enable Trillion+ parameter-scale models.\n\n----\n\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Train models with billions of parameters\n   :description:\n   :col_css: col-md-4\n   :button_link: ../advanced/model_parallel/index.html\n   :height: 150\n   :tag: advanced\n\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html", "url_rel_html": "accelerators/gpu_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "GPU training (Basic)¶", "rst_text": ":orphan:\n\n.. _gpu_basic:\n\nGPU training (Basic)\n====================\n**Audience:** Users looking to save money and run large models faster using single or multiple\n\n----\n\nWhat is a GPU?\n--------------\nA Graphics Processing Unit (GPU), is a specialized hardware accelerator designed to speed up mathematical computations used in gaming and deep learning.\n\n----\n\n.. _multi_gpu:\n\nTrain on GPUs\n-------------\n\nThe Trainer will run on all available GPUs by default. Make sure you're running on a machine with at least one GPU.\nThere's no need to specify any NVIDIA flags as Lightning will do it for you.\n\n.. code-block:: python\n\n    # run on as many GPUs as available by default\n    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n    # equivalent to\n    trainer = Trainer()\n\n    # run on one GPU\n    trainer = Trainer(accelerator=\"gpu\", devices=1)\n    # run on multiple GPUs\n    trainer = Trainer(accelerator=\"gpu\", devices=8)\n    # choose the number of devices automatically\n    trainer = Trainer(accelerator=\"gpu\", devices=\"auto\")\n\n.. note::\n    Setting ``accelerator=\"gpu\"`` will also automatically choose the \"mps\" device on Apple sillicon GPUs.\n    If you want to avoid this, you can set ``accelerator=\"cuda\"`` instead.\n\nChoosing GPU devices\n^^^^^^^^^^^^^^^^^^^^\n\nYou can select the GPU devices using ranges, a list of indices or a string containing\na comma separated list of GPU ids:\n\n.. testsetup::\n\n    k = 1\n\n.. testcode::\n    :skipif: torch.cuda.device_count() < 2\n\n    # DEFAULT (int) specifies how many GPUs to use per node\n    Trainer(accelerator=\"gpu\", devices=k)\n\n    # Above is equivalent to\n    Trainer(accelerator=\"gpu\", devices=list(range(k)))\n\n    # Specify which GPUs to use (don't use when running on cluster)\n    Trainer(accelerator=\"gpu\", devices=[0, 1])\n\n    # Equivalent using a string\n    Trainer(accelerator=\"gpu\", devices=\"0, 1\")\n\n    # To use all available GPUs put -1 or '-1'\n    # equivalent to `list(range(torch.cuda.device_count())) and `\"auto\"`\n    Trainer(accelerator=\"gpu\", devices=-1)\n\nThe table below lists examples of possible input formats and how they are interpreted by Lightning.\n\n+------------------+-----------+---------------------+---------------------------------+\n| `devices`        | Type      | Parsed              | Meaning                         |\n+==================+===========+=====================+=================================+\n| 3                | int       | [0, 1, 2]           | first 3 GPUs                    |\n+------------------+-----------+---------------------+---------------------------------+\n| -1               | int       | [0, 1, 2, ...]      | all available GPUs              |\n+------------------+-----------+---------------------+---------------------------------+\n| [0]              | list      | [0]                 | GPU 0                           |\n+------------------+-----------+---------------------+---------------------------------+\n| [1, 3]           | list      | [1, 3]              | GPU index 1 and 3 (0-based)     |\n+------------------+-----------+---------------------+---------------------------------+\n| \"3\"              | str       | [0, 1, 2]           | first 3 GPUs                    |\n+------------------+-----------+---------------------+---------------------------------+\n| \"1, 3\"           | str       | [1, 3]              | GPU index 1 and 3 (0-based)     |\n+------------------+-----------+---------------------+---------------------------------+\n| \"-1\"             | str       | [0, 1, 2, ...]      | all available GPUs              |\n+------------------+-----------+---------------------+---------------------------------+\n\n\nFind usable CUDA devices\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you want to run several experiments at the same time on your machine, for example for a hyperparameter sweep, then you can\nuse the following utility function to pick GPU indices that are \"accessible\", without having to change your code every time.\n\n.. code-block:: python\n\n    from lightning.pytorch.accelerators import find_usable_cuda_devices\n\n    # Find two GPUs on the system that are not already occupied\n    trainer = Trainer(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n\n    from lightning.fabric.accelerators import find_usable_cuda_devices\n\n    # Works with Fabric too\n    fabric = Fabric(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n\n\nThis is especially useful when GPUs are configured to be in \"exclusive compute mode\", such that only one process at a time is allowed access to the device.\nThis special mode is often enabled on server GPUs or systems shared among multiple users.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu_expert.html", "url_rel_html": "accelerators/gpu_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "GPU training (Expert)¶", "rst_text": ":orphan:\n\n.. _gpu_expert:\n\nGPU training (Expert)\n=====================\n**Audience:** Experts creating new scaling techniques such as :ref:`FSDP <fully-sharded-training>` or :ref:`DeepSpeed <deepspeed_advanced>`.\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\n----\n\nLightning enables experts focused on researching new ways of optimizing distributed training/inference strategies to create new strategies and plug them into Lightning.\n\nFor example, Lightning worked closely with the Microsoft team to develop a :ref:`DeepSpeed <deepspeed_advanced>` integration and with the Facebook (Meta) team to develop a :ref:`FSDP <fully-sharded-training>` integration.\n\n\n----\n\n.. include:: ../extensions/strategy.rst\n\n\n----\n\n.. include:: ../advanced/strategy_registry.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu_faq.html", "url_rel_html": "accelerators/gpu_faq.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu_faq.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "GPU training (FAQ)¶", "rst_text": ":orphan:\n\n.. _gpu_faq:\n\nGPU training (FAQ)\n==================\n\n***************************************************************\nHow should I adjust the batch size when using multiple devices?\n***************************************************************\n\nLightning automatically shards your data across multiple GPUs, meaning that each device only sees a unique subset of your\ndata, but the `batch_size` in your DataLoader remains the same. This means that the effective batch size e.g. the\ntotal number of samples processed in one forward/backward pass is\n\n.. math::\n\n    \\text{Effective Batch Size} = \\text{DataLoader Batch Size} \\times \\text{Number of Devices} \\times \\text{Number of Nodes}\n\nA couple of examples to illustrate this:\n\n.. code-block:: python\n\n    dataloader = DataLoader(..., batch_size=7)\n\n    # Single GPU: effective batch size = 7\n    Trainer(accelerator=\"gpu\", devices=1)\n\n    # Multi-GPU: effective batch size = 7 * 8 = 56\n    Trainer(accelerator=\"gpu\", devices=8, strategy=...)\n\n    # Multi-node: effective batch size = 7 * 8 * 10 = 560\n    Trainer(accelerator=\"gpu\", devices=8, num_nodes=10, strategy=...)\n\nIn general you should be able to use the same `batch_size` in your DataLoader regardless of the number of devices you are\nusing.\n\n.. note::\n\n    If you want distributed training to work exactly the same as single GPU training, you need to set the `batch_size`\n    in your DataLoader to `original_batch_size / num_devices` to maintain the same effective batch size. However, this\n    can lead to poor GPU utilization.\n\n----\n\n******************************************************************\nHow should I adjust the learning rate when using multiple devices?\n******************************************************************\n\nBecause the effective batch size is larger when using multiple devices, you need to adjust your learning rate\naccordingly. Because the learning rate is a hyperparameter that controls how much to change the model in response to\nthe estimated error each time the model weights are updated, it is important to scale it with the effective batch size.\n\nIn general, there are two common scaling rules:\n\n1. **Linear scaling**: Increase the learning rate linearly with the number of devices.\n\n    .. code-block:: python\n\n        # Example: Linear scaling\n        base_lr = 1e-3\n        num_devices = 8\n        scaled_lr = base_lr * num_devices  # 8e-3\n\n2. **Square root scaling**: Increase the learning rate by the square root of the number of devices.\n\n    .. code-block:: python\n\n        # Example: Square root scaling\n        base_lr = 1e-3\n        num_devices = 8\n        scaled_lr = base_lr * (num_devices ** 0.5)  # 2.83e-3\n\n.. note:: Huge batch sizes are actually really bad for convergence. Check out:\n        `Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour <https://arxiv.org/abs/1706.02677>`_\n\n----\n\n\n*********************************************************\nHow do I use multiple GPUs on Jupyter or Colab notebooks?\n*********************************************************\n\nTo use multiple GPUs on notebooks, use the *DDP_NOTEBOOK* mode.\n\n.. code-block:: python\n\n    Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\")\n\nIf you want to use other strategies, please launch your training via the command-shell.\nSee also: :doc:`../../common/notebooks`\n\n----\n\n*****************************************************\nI'm getting errors related to Pickling. What do I do?\n*****************************************************\n\nPickle is Python's mechanism for serializing and unserializing data. Some distributed modes require that your code is fully pickle compliant. If you run into an issue with pickling, try the following to figure out the issue.\n\n.. code-block:: python\n\n    import pickle\n\n    model = YourModel()\n    pickle.dumps(model)\n\nFor example, the `ddp_spawn` strategy has the pickling requirement. This is a limitation of Python.\n\n.. code-block:: python\n\n    Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_spawn\")\n\nIf you use `ddp`, your code doesn't need to be pickled:\n\n.. code-block:: python\n\n    Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html", "url_rel_html": "accelerators/gpu_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/gpu_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "GPU training (Intermediate)¶", "rst_text": ":orphan:\n\n.. _gpu_intermediate:\n\nGPU training (Intermediate)\n===========================\n**Audience:** Users looking to train across machines or experiment with different scaling techniques.\n\n----\n\n\nDistributed training strategies\n-------------------------------\nLightning supports multiple ways of doing distributed training.\n\n- Regular (``strategy='ddp'``)\n- Spawn (``strategy='ddp_spawn'``)\n- Notebook/Fork (``strategy='ddp_notebook'``)\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/yt/Trainer+flags+4-+multi+node+training_3.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/yt_thumbs/thumb_multi_gpus.png\n    :width: 400\n\n\n.. note::\n    If you request multiple GPUs or nodes without setting a strategy, DDP will be automatically used.\n\n----\n\n\nDistributed Data Parallel\n^^^^^^^^^^^^^^^^^^^^^^^^^\n:class:`~torch.nn.parallel.DistributedDataParallel` (DDP) works as follows:\n\n1. Each GPU across each node gets its own process.\n2. Each GPU gets visibility into a subset of the overall dataset. It will only ever see that subset.\n3. Each process inits the model.\n4. Each process performs a full forward and backward pass in parallel.\n5. The gradients are synced and averaged across all processes.\n6. Each process updates its optimizer.\n\n|\n\n.. code-block:: python\n\n    # train on 8 GPUs (same machine (ie: node))\n    trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n\n    # train on 32 GPUs (4 nodes)\n    trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\", num_nodes=4)\n\nThis Lightning implementation of DDP calls your script under the hood multiple times with the correct environment\nvariables:\n\n.. code-block:: bash\n\n    # example for 3 GPUs DDP\n    MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\n    MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=1 python my_file.py --accelerator 'gpu' --devices 3 --etc\n    MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=2 python my_file.py --accelerator 'gpu' --devices 3 --etc\n\nUsing DDP this way has a few advantages over ``torch.multiprocessing.spawn()``:\n\n1. All processes (including the main process) participate in training and have the updated state of the model and Trainer state.\n2. No multiprocessing pickle errors\n3. Easily scales to multi-node training\n\n|\n\nIt is NOT possible to use DDP in interactive environments like Jupyter Notebook, Google COLAB, Kaggle, etc.\nIn these situations you should use `ddp_notebook`.\n\n\n----\n\n\nDistributed Data Parallel Spawn\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning:: It is STRONGLY recommended to use DDP for speed and performance.\n\nThe `ddp_spawn` strategy is similar to `ddp` except that it uses ``torch.multiprocessing.spawn()`` to start the training processes.\nUse this for debugging only, or if you are converting a code base to Lightning that relies on spawn.\n\n.. code-block:: python\n\n    # train on 8 GPUs (same machine (ie: node))\n    trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_spawn\")\n\nWe STRONGLY discourage this use because it has limitations (due to Python and PyTorch):\n\n1. After ``.fit()``, only the model's weights get restored to the main process, but no other state of the Trainer.\n2. Does not support multi-node training.\n3. It is generally slower than DDP.\n\n\n----\n\n\nDistributed Data Parallel in Notebooks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nDDP Notebook/Fork is an alternative to Spawn that can be used in interactive Python and Jupyter notebooks, Google Colab, Kaggle notebooks, and so on:\nThe Trainer enables it by default when such environments are detected.\n\n.. code-block:: python\n\n    # train on 8 GPUs in a Jupyter notebook\n    trainer = Trainer(accelerator=\"gpu\", devices=8)\n\n    # can be set explicitly\n    trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_notebook\")\n\n    # can also be used in non-interactive environments\n    trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_fork\")\n\nAmong the native distributed strategies, regular DDP (``strategy=\"ddp\"``) is still recommended as the go-to strategy over Spawn and Fork/Notebook for its speed and stability but it can only be used with scripts.\n\n\n----\n\n\nComparison of DDP variants and tradeoffs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. list-table:: DDP variants and their tradeoffs\n   :widths: 40 20 20 20\n   :header-rows: 1\n\n   * -\n     - DDP\n     - DDP Spawn\n     - DDP Notebook/Fork\n   * - Works in Jupyter notebooks / IPython environments\n     - No\n     - No\n     - Yes\n   * - Supports multi-node\n     - Yes\n     - Yes\n     - Yes\n   * - Supported platforms\n     - Linux, Mac, Win\n     - Linux, Mac, Win\n     - Linux, Mac\n   * - Requires all objects to be picklable\n     - No\n     - Yes\n     - No\n   * - Limitations in the main process\n     - None\n     - The state of objects is not up-to-date after returning to the main process (`Trainer.fit()` etc). Only the model parameters get transferred over.\n     - GPU operations such as moving tensors to the GPU or calling ``torch.cuda`` functions before invoking ``Trainer.fit`` is not allowed.\n   * - Process creation time\n     - Slow\n     - Slow\n     - Fast\n\n\n----\n\n\nTorchRun (TorchElastic)\n-----------------------\nLightning supports the use of TorchRun (previously known as TorchElastic) to enable fault-tolerant and elastic distributed job scheduling.\nTo use it, specify the DDP strategy and the number of GPUs you want to use in the Trainer.\n\n.. code-block:: python\n\n    Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n\nThen simply launch your script with the :doc:`torchrun <../clouds/cluster_intermediate_2>` command.\n\n\n----\n\n\nOptimize multi-machine communication\n------------------------------------\n\nBy default, Lightning will select the ``nccl`` backend over ``gloo`` when running on GPUs.\nFind more information about PyTorch's supported backends `here <https://pytorch.org/docs/stable/distributed.html>`__.\n\nLightning allows explicitly specifying the backend via the `process_group_backend` constructor argument on the relevant Strategy classes. By default, Lightning will select the appropriate process group backend based on the hardware used.\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import DDPStrategy\n\n    # Explicitly specify the process group backend if you choose to\n    ddp = DDPStrategy(process_group_backend=\"nccl\")\n\n    # Configure the strategy on the Trainer\n    trainer = Trainer(strategy=ddp, accelerator=\"gpu\", devices=8)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/mps.html", "url_rel_html": "accelerators/mps.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/mps.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Accelerator: Apple Silicon training¶", "rst_text": ".. _mps:\n\nAccelerator: Apple Silicon training\n===================================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware\n   :col_css: col-md-4\n   :button_link: accelerator_prepare.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of Apple silicon gpu training.\n   :col_css: col-md-4\n   :button_link: mps_basic.html\n   :height: 150\n   :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/mps_basic.html", "url_rel_html": "accelerators/mps_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/mps_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "MPS training (basic)¶", "rst_text": ":orphan:\n\n.. _mps_basic:\n\nMPS training (basic)\n====================\n**Audience:** Users looking to train on their Apple silicon GPUs.\n\n.. warning::\n\n   Both the MPS accelerator and the PyTorch backend are still experimental.\n   As such, not all operations are currently supported. However, with ongoing development from the PyTorch team, an increasingly large number of operations are becoming available.\n   You can use ``PYTORCH_ENABLE_MPS_FALLBACK=1 python your_script.py`` to fall back to cpu for unsupported operations.\n\n\n----\n\nWhat is Apple silicon?\n----------------------\nApple silicon chips are a unified system on a chip (SoC) developed by Apple based on the ARM design.\nAmong other things, they feature CPU-cores, GPU-cores, a neural engine and shared memory between all of these features.\n\n----\n\nSo it's a CPU?\n--------------\nApple silicon includes CPU-cores among several other features. However, the full potential for the hardware acceleration of which the M-Socs are capable is unavailable when running on the ``CPUAccelerator``. This is because they also feature a GPU and a neural engine.\n\nTo use them, Lightning supports the ``MPSAccelerator``.\n\n----\n\nRun on Apple silicon gpus\n-------------------------\nEnable the following Trainer arguments to run on Apple silicon gpus (MPS devices).\n\n.. code-block:: python\n\n   trainer = Trainer(accelerator=\"mps\", devices=1)\n\n.. note::\n   The ``MPSAccelerator`` only supports 1 device at a time. Currently there are no machines with multiple MPS-capable GPUs.\n\n----\n\nWhat does MPS stand for?\n------------------------\nMPS is short for `Metal Performance Shaders <https://developer.apple.com/metal/>`_  which is the technology used in the back for gpu communication and computing.\n\n----\n\nTroubleshooting\n---------------\n\n\nIf Lightning can't detect the Apple Silicon hardware, it will raise this exception:\n\n.. code::\n\n   MisconfigurationException: `MPSAccelerator` can not run on your system since the accelerator is not available.\n\nIf you are seeing this despite running on an ARM-enabled Mac, the most likely cause is that your Python is being emulated and thinks it is running on an Intel CPU.\nTo solve this, re-install your python executable (and if using environment managers like conda, you have to reinstall these as well) by downloading the Apple M1/M2 build (not Intel!), for example `here <https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links>`_.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/tpu.html", "url_rel_html": "accelerators/tpu.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/tpu.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Accelerator: TPU training¶", "rst_text": ".. _tpu:\n\nAccelerator: TPU training\n=========================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware\n   :col_css: col-md-4\n   :button_link: accelerator_prepare.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of single and multi-TPU core training.\n   :col_css: col-md-4\n   :button_link: tpu_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Scale massive models using cloud TPUs.\n   :col_css: col-md-4\n   :button_link: tpu_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Dive into XLA and advanced techniques to optimize TPU-powered models.\n   :col_css: col-md-4\n   :button_link: tpu_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: FAQ\n   :description: Frequently asked questions about TPU training.\n   :col_css: col-md-4\n   :button_link: tpu_faq.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/tpu_advanced.html", "url_rel_html": "accelerators/tpu_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/tpu_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "TPU training (Advanced)¶", "rst_text": ":orphan:\n\nTPU training (Advanced)\n=======================\n**Audience:** Users looking to apply advanced performance techniques to TPU training.\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\n----\n\nWeight Sharing/Tying\n--------------------\nWeight Tying/Sharing is a technique where in the module weights are shared among two or more layers.\nThis is a common method to reduce memory consumption and is utilized in many State of the Art\narchitectures today.\n\nPyTorch XLA requires these weights to be tied/shared after moving the model to the XLA device.\nTo support this requirement, Lightning automatically finds these weights and ties them after\nthe modules are moved to the XLA device under the hood. It will ensure that the weights among\nthe modules are shared but not copied independently.\n\nPyTorch Lightning has an inbuilt check which verifies that the model parameter lengths\nmatch once the model is moved to the device. If the lengths do not match Lightning\nthrows a warning message.\n\nExample:\n\n.. code-block:: python\n\n    from lightning.pytorch.core.module import LightningModule\n    from torch import nn\n    from lightning.pytorch.trainer.trainer import Trainer\n\n\n    class WeightSharingModule(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.layer_1 = nn.Linear(32, 10, bias=False)\n            self.layer_2 = nn.Linear(10, 32, bias=False)\n            self.layer_3 = nn.Linear(32, 10, bias=False)\n            # Lightning automatically ties these weights after moving to the XLA device,\n            # so all you need is to write the following just like on other accelerators.\n            self.layer_3.weight = self.layer_1.weight\n\n        def forward(self, x):\n            x = self.layer_1(x)\n            x = self.layer_2(x)\n            x = self.layer_3(x)\n            return x\n\n\n    model = WeightSharingModule()\n    trainer = Trainer(max_epochs=1, accelerator=\"tpu\")\n\nSee `XLA Documentation <https://github.com/pytorch/xla/blob/v2.5.0/TROUBLESHOOTING.md#xla-tensor-quirks>`_\n\n----\n\nXLA\n---\nXLA is the library that interfaces PyTorch with the TPUs.\nFor more information check out `XLA <https://github.com/pytorch/xla>`_.\n\nGuide for `troubleshooting XLA <https://github.com/pytorch/xla/blob/v2.5.0/TROUBLESHOOTING.md>`_\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/tpu_basic.html", "url_rel_html": "accelerators/tpu_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/tpu_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "TPU training (Basic)¶", "rst_text": ":orphan:\n\nTPU training (Basic)\n====================\n**Audience:** Users looking to train on single or multiple TPU cores.\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\n----\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/tpu_cores.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/yt_thumbs/thumb_tpus.png\n    :width: 400\n    :muted:\n\nLightning supports running on TPUs. At this moment, TPUs are available\non Google Cloud (GCP), Google Colab and Kaggle Environments. For more information on TPUs\n`watch this video <https://www.youtube.com/watch?v=kPMpmcl_Pyw>`_.\n\n----------------\n\nWhat is a TPU?\n--------------\nTensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural networks.\n\nA TPU has 8 cores where each core is optimized for 128x128 matrix multiplies. In general, a single TPU is about as fast as 5 V100 GPUs!\n\nA TPU pod hosts many TPUs on it. Currently, TPU v3 Pod has up to 2048 TPU cores and 32 TiB of memory!\nYou can request a full pod from Google cloud or a \"slice\" which gives you\nsome subset of those 2048 cores.\n\n----\n\nRun on TPU cores\n----------------\n\nTo run on different cores, modify the ``devices`` argument.\n\n.. code-block:: python\n\n    # run on as many TPUs as available by default\n    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n    # equivalent to\n    trainer = Trainer()\n\n    # run on one TPU core\n    trainer = Trainer(accelerator=\"tpu\", devices=1)\n    # run on multiple TPU cores\n    trainer = Trainer(accelerator=\"tpu\", devices=8)\n    # run on one specific TPU core: the 2nd core (index 1)\n    trainer = Trainer(accelerator=\"tpu\", devices=[1])\n    # choose the number of cores automatically\n    trainer = Trainer(accelerator=\"tpu\", devices=\"auto\")\n\n----\n\nHow to access TPUs\n------------------\nTo access TPUs, there are three main ways.\n\nGoogle Colab\n^^^^^^^^^^^^\nColab is like a jupyter notebook with a free GPU or TPU\nhosted on GCP.\n\nTo get a TPU on colab, follow these steps:\n\n1. Go to `Google Colab <https://colab.research.google.com/>`_.\n\n2. Click \"new notebook\" (bottom right of pop-up).\n\n3. Click runtime > change runtime settings. Select Python 3, and hardware accelerator \"TPU\".\n   This will give you a TPU with 8 cores.\n\n4. Next, insert this code into the first cell and execute.\n   This will install the xla library that interfaces between PyTorch and the TPU.\n\n   .. code-block::\n\n        !pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp38-cp38m-linux_x86_64.whl\n\n5. Once the above is done, install PyTorch Lightning.\n\n   .. code-block::\n\n        !pip install lightning\n\n6. Then set up your LightningModule as normal.\n\nGoogle Cloud (GCP)\n^^^^^^^^^^^^^^^^^^\nYou could refer to this `page <https://cloud.google.com/tpu/docs/v4-users-guide>`_ for getting started with Cloud TPU resources on GCP.\n\n----\n\nOptimize Performance\n--------------------\n\nThe TPU was designed for specific workloads and operations to carry out large volumes of matrix multiplication,\nconvolution operations and other commonly used ops in applied deep learning.\nThe specialization makes it a strong choice for NLP tasks, sequential convolutional networks, and under low precision operation.\nThere are cases in which training on TPUs is slower when compared with GPUs, for possible reasons listed:\n\n- Too small batch size.\n- Explicit evaluation of tensors during training, e.g. ``tensor.item()``\n- Tensor shapes (e.g. model inputs) change often during training.\n- Limited resources when using TPU's with PyTorch `Link <https://github.com/pytorch/xla/issues/2054#issuecomment-627367729>`_\n- XLA Graph compilation during the initial steps `Reference <https://github.com/pytorch/xla/issues/2383#issuecomment-666519998>`_\n- Some tensor ops are not fully supported on TPU, or not supported at all. These operations will be performed on CPU (context switch).\n\nThe official PyTorch XLA `performance guide <https://github.com/pytorch/xla/blob/v2.5.0/TROUBLESHOOTING.md#known-performance-caveats>`_\nhas more detailed information on how PyTorch code can be optimized for TPU. In particular, the\n`metrics report <https://github.com/pytorch/xla/blob/v2.5.0/TROUBLESHOOTING.md#get-a-metrics-report>`_ allows\none to identify operations that lead to context switching.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/tpu_faq.html", "url_rel_html": "accelerators/tpu_faq.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/tpu_faq.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "TPU training (FAQ)¶", "rst_text": ":orphan:\n\n.. _tpu_faq:\n\nTPU training (FAQ)\n==================\n\n**********************************************************\nHow to clear up the programs using TPUs in the background?\n**********************************************************\n\n.. code-block:: bash\n\n    pgrep python |  awk '{print $2}' | xargs -r kill -9\n\nSometimes, there can still be old programs running on the TPUs, which would make the TPUs unavailable to use. You could use the above command in the terminal to kill the running processes.\n\n----\n\n*************************************\nHow to resolve the replication issue?\n*************************************\n\n.. code-block::\n\n    File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 200, in set_replication\n        replication_devices = xla_replication_devices(devices)\n    File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 187, in xla_replication_devices\n        .format(len(local_devices), len(kind_devices)))\n    RuntimeError: Cannot replicate if number of devices (1) is different from 8\n\nThis error is raised when the XLA device is called outside the spawn process. Internally in the XLA-Strategy for training on multiple tpu cores, we use XLA's `xmp.spawn`.\nDon't use ``xm.xla_device()`` while working on Lightning + TPUs!\n\n----\n\n**************************************\nUnsupported datatype transfer to TPUs?\n**************************************\n\n.. code-block::\n\n    File \"/usr/local/lib/python3.9/dist-packages/torch_xla/utils/utils.py\", line 205, in _for_each_instance_rewrite\n        v = _for_each_instance_rewrite(result.__dict__[k], select_fn, fn, rwmap)\n    File \"/usr/local/lib/python3.9/dist-packages/torch_xla/utils/utils.py\", line 206, in _for_each_instance_rewrite\n        result.__dict__[k] = v\n    TypeError: 'mappingproxy' object does not support item assignment\n\nPyTorch XLA only supports Tensor objects for CPU to TPU data transfer. Might cause issues if the User is trying to send some non-tensor objects through the DataLoader or during saving states.\n\n----\n\n*************************************************\nHow to setup the debug mode for Training on TPUs?\n*************************************************\n\n.. code-block:: python\n\n    import lightning as L\n\n    my_model = MyLightningModule()\n    trainer = L.Trainer(accelerator=\"tpu\", devices=8, strategy=\"xla_debug\")\n    trainer.fit(my_model)\n\nExample Metrics report:\n\n.. code-block::\n\n    Metric: CompileTime\n        TotalSamples: 202\n        Counter: 06m09s401ms746.001us\n        ValueRate: 778ms572.062us / second\n        Rate: 0.425201 / second\n        Percentiles: 1%=001ms32.778us; 5%=001ms61.283us; 10%=001ms79.236us; 20%=001ms110.973us; 50%=001ms228.773us; 80%=001ms339.183us; 90%=001ms434.305us; 95%=002ms921.063us; 99%=21s102ms853.173us\n\n\nA lot of PyTorch operations aren't lowered to XLA, which could lead to significant slowdown of the training process.\nThese operations are moved to the CPU memory and evaluated, and then the results are transferred back to the XLA device(s).\nBy using the `xla_debug` Strategy, users could create a metrics report to diagnose issues.\n\nThe report includes things like (`XLA Reference <https://github.com/pytorch/xla/blob/v2.5.0/TROUBLESHOOTING.md#troubleshooting>`_):\n\n* how many times we issue XLA compilations and time spent on issuing.\n* how many times we execute and time spent on execution\n* how many device data handles we create/destroy etc.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/accelerators/tpu_intermediate.html", "url_rel_html": "accelerators/tpu_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/accelerators/tpu_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "TPU training (Intermediate)¶", "rst_text": ":orphan:\n\nTPU training (Intermediate)\n===========================\n**Audience:** Users looking to use cloud TPUs.\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\n----\n\nDistributedSamplers\n-------------------\nLightning automatically inserts the correct samplers - no need to do this yourself!\n\nUsually, with TPUs (and DDP), you would need to define a DistributedSampler to move the right\nchunk of data to the appropriate TPU. As mentioned, this is not needed in Lightning\n\n.. note:: Don't add distributedSamplers. Lightning does this automatically\n\nIf for some reason you still need to, this is how to construct the sampler\nfor TPU use\n\n.. code-block:: python\n\n    import torch_xla.core.xla_model as xm\n\n\n    def train_dataloader(self):\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n\n        # required for TPU support\n        sampler = None\n        if use_tpu:\n            sampler = torch.utils.data.distributed.DistributedSampler(\n                dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True\n            )\n\n        loader = DataLoader(dataset, sampler=sampler, batch_size=32)\n\n        return loader\n\nConfigure the number of TPU cores in the trainer. You can only choose 1 or 8.\nTo use a full TPU pod skip to the TPU pod section.\n\n.. code-block:: python\n\n    import lightning as L\n\n    my_model = MyLightningModule()\n    trainer = L.Trainer(accelerator=\"tpu\", devices=8)\n    trainer.fit(my_model)\n\nThat's it! Your model will train on all 8 TPU cores.\n\n----------------\n\n16 bit precision\n----------------\nLightning also supports training in 16-bit precision with TPUs.\nBy default, TPU training will use 32-bit precision. To enable it, do\n\n.. code-block:: python\n\n    import lightning as L\n\n    my_model = MyLightningModule()\n    trainer = L.Trainer(accelerator=\"tpu\", precision=\"16-true\")\n    trainer.fit(my_model)\n\nUnder the hood the xla library will use the `bfloat16 type <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/compile.html", "url_rel_html": "advanced/compile.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/compile.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Speed up models by compiling them¶", "rst_text": "#################################\nSpeed up models by compiling them\n#################################\n\nCompiling your LightningModule can result in significant speedups, especially on the latest generations of GPUs.\nThis guide shows you how to apply `torch.compile <https://pytorch.org/docs/2.2/generated/torch.compile.html>`_ correctly in your code.\n\n\n----\n\n\n*******************************************\nApply torch.compile to your LightningModule\n*******************************************\n\nCompiling a LightningModule is as simple as adding one line of code, calling :func:`torch.compile`:\n\n.. code-block:: python\n\n    import torch\n    import lightning as L\n\n    # Define the model\n    model = MyLightningModule()\n\n    # Compile the model\n    model = torch.compile(model)\n\n    # Run with the Trainer\n    trainer = L.Trainer()\n    trainer.fit(model)\n\n\n.. important::\n\n    You should compile the model **before** calling ``trainer.fit()`` as shown above for an optimal integration with features in Trainer.\n\nThe newly added call to ``torch.compile()`` by itself doesn't do much. It just wraps the model in a \"compiled model\".\nThe actual optimization will start when calling the ``forward()`` method for the first time:\n\n.. code-block:: python\n\n    # 1st execution compiles the model (slow)\n    output = model(input)\n\n    # All future executions will be fast (for inputs of the same size)\n    output = model(input)\n    output = model(input)\n    ...\n\n**When you pass the LightningModule to the Trainer, it will automatically also compile the ``*_step()`` methods.**\n\nWhen measuring the speed of a compiled model and comparing it to a regular model, it is important to\nalways exclude the first call to ``forward()``/``*_step()`` from your measurements, since it includes the compilation time.\n\n\n.. collapse:: Full example with benchmark\n\n    Below is an example that measures the speedup you get when compiling the InceptionV3 from TorchVision.\n\n    .. code-block:: python\n\n        import statistics\n        import torch\n        import torchvision.models as models\n        import lightning as L\n        from torch.utils.data import DataLoader\n\n\n        class MyLightningModule(L.LightningModule):\n            def __init__(self):\n                super().__init__()\n                self.model = models.inception_v3()\n\n            def training_step(self, batch):\n                return self.model(batch).logits.sum()\n\n            def train_dataloader(self):\n                return DataLoader([torch.randn(3, 512, 512) for _ in range(256)], batch_size=16)\n\n            def configure_optimizers(self):\n                return torch.optim.SGD(self.parameters(), lr=0.01)\n\n\n        class Benchmark(L.Callback):\n            \"\"\"A callback that measures the median execution time between the start and end of a batch.\"\"\"\n            def __init__(self):\n                self.start = torch.cuda.Event(enable_timing=True)\n                self.end = torch.cuda.Event(enable_timing=True)\n                self.times = []\n\n            def median_time(self):\n                return statistics.median(self.times)\n\n            def on_train_batch_start(self, trainer, *args, **kwargs):\n                self.start.record()\n\n            def on_train_batch_end(self, trainer, *args, **kwargs):\n                # Exclude the first iteration to let the model warm up\n                if trainer.global_step > 1:\n                    self.end.record()\n                    torch.cuda.synchronize()\n                    self.times.append(self.start.elapsed_time(self.end) / 1000)\n\n\n        model = MyLightningModule()\n\n        # Compile!\n        compiled_model = torch.compile(model)\n\n        # Measure the median iteration time with uncompiled model\n        benchmark = Benchmark()\n        trainer = L.Trainer(accelerator=\"cuda\", devices=1, max_steps=10, callbacks=[benchmark])\n        trainer.fit(model)\n        eager_time = benchmark.median_time()\n\n        # Measure the median iteration time with compiled model\n        benchmark = Benchmark()\n        trainer = L.Trainer(accelerator=\"cuda\", devices=1, max_steps=10, callbacks=[benchmark])\n        trainer.fit(compiled_model)\n        compile_time = benchmark.median_time()\n\n        # Compare the speedup for the compiled execution\n        speedup = eager_time / compile_time\n        print(f\"Eager median time: {eager_time:.4f} seconds\")\n        print(f\"Compile median time: {compile_time:.4f} seconds\")\n        print(f\"Speedup: {speedup:.1f}x\")\n\n\n    On an NVIDIA A100 SXM4 40GB with PyTorch 2.2.0, CUDA 12.1, we get the following speedup:\n\n    .. code-block:: text\n\n        Eager median time: 0.0863 seconds\n        Compile median time: 0.0709 seconds\n        Speedup: 1.2x\n\n\n----\n\n**************************************\nApply torch.compile in configure_model\n**************************************\n\n:func:`torch.compile` can also be invoked as part of the :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook.\n\nThis is particularly handy when :func:`torch.compile` is used in combination with :class:`~lightning.pytorch.strategies.model_parallel.ModelParallelStrategy`.\n\nHere is an example:\n\n.. code-block:: python\n\n    import lightning as L\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from lightning.pytorch.demos import Transformer\n    from lightning.pytorch.strategies.model_parallel import ModelParallelStrategy\n    from torch.distributed.device_mesh import DeviceMesh\n    from torch.distributed._composable.fsdp.fully_shard import fully_shard\n\n    class LanguageModel(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.vocab_size = vocab_size\n            self.model = None\n\n        def configure_model(self):\n            if self.model is not None:\n                return\n\n            with torch.device(\"meta\"):\n                model = Transformer(\n                    vocab_size=self.vocab_size,\n                    nlayers=16,\n                    nhid=4096,\n                    ninp=1024,\n                    nhead=32,\n                )\n\n            for module in model.modules():\n                if isinstance(module, (nn.TransformerEncoderLayer, nn.TransformerDecoderLayer)):\n                    fully_shard(module, mesh=self.device_mesh)\n\n            fully_shard(model, mesh=self.device_mesh)\n\n            self.model = torch.compile(model)\n\n        def training_step(self, batch):\n            input, target = batch\n            output = self.model(input, target)\n            loss = F.nll_loss(output, target.view(-1))\n            self.log(\"train_loss\", loss)\n            return loss\n\n        def configure_optimizers(self):\n            return torch.optim.Adam(self.parameters(), lr=1e-4)\n\nThe advantage here is that `configure_model` is called when sharding the model,\nso :func:`torch.compile` is guaranteed to run on model shards and capture distributed operations.\n\nAlso, when using other libraries like `torch ao <https://github.com/pytorch/ao>`_\nthat need to be applied in a similar fashion, it's easy to reason about the sequence of calls\nneeded to achieve the equivalent of `compile(distributed(quantized(model)))`:\n\n.. code-block:: python\n\n    import lightning as L\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from lightning.pytorch.demos import Transformer\n    from lightning.pytorch.strategies.model_parallel import ModelParallelStrategy\n    from torch.distributed._composable.fsdp.fully_shard import fully_shard\n    from torch.distributed.device_mesh import DeviceMesh\n    from torchao.float8 import Float8LinearConfig, convert_to_float8_training\n\n    class LanguageModel(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.vocab_size = vocab_size\n            self.model = None\n\n        def configure_model(self):\n            if self.model is not None:\n                return\n\n            with torch.device(\"meta\"):\n                model = Transformer(\n                    vocab_size=self.vocab_size,\n                    nlayers=16,\n                    nhid=4096,\n                    ninp=1024,\n                    nhead=32,\n                )\n\n            float8_config = Float8LinearConfig(\n                pad_inner_dim=True,\n            )\n\n            def module_filter_fn(mod: torch.nn.Module, fqn: str):\n                return fqn != \"decoder\"\n\n            convert_to_float8_training(model, config=float8_config, module_filter_fn=module_filter_fn)\n\n            for module in model.modules():\n                if isinstance(module, (nn.TransformerEncoderLayer, nn.TransformerDecoderLayer)):\n                    fully_shard(module, mesh=self.device_mesh)\n\n            fully_shard(model, mesh=self.device_mesh)\n\n            self.model = torch.compile(model)\n\nFor a full example, see our `FP8 Distributed Transformer example <https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples/pytorch/fp8_distributed_transformer>`_.\n\n----\n\n******************\nAvoid graph breaks\n******************\n\nWhen ``torch.compile`` looks at the code in your model's ``forward()`` or ``*_step()`` method, it will try to compile as much of the code as possible.\nIf there are regions in the code that it doesn't understand, it will introduce a so-called \"graph break\" that essentially splits the code in optimized and unoptimized parts.\nGraph breaks aren't a deal breaker, since the optimized parts should still run faster.\nBut if you want to get the most out of ``torch.compile``, you might want to invest rewriting the problematic section of the code that produces the breaks.\n\nYou can check whether your model produces graph breaks by calling ``torch.compile`` with ``fullgraph=True``:\n\n.. code-block:: python\n\n    # Force an error if there is a graph break in the model\n    model = torch.compile(model, fullgraph=True)\n\nBe aware that the error messages produced here are often quite cryptic, so you will likely have to do some `troubleshooting <https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html>`_ to fully optimize your model.\n\n\n----\n\n\n*******************\nAvoid recompilation\n*******************\n\nAs mentioned before, the compilation of the model happens the first time you call ``forward()`` or the first time the Trainer calls the ``*_step()`` methods.\nAt this point, PyTorch will inspect the input tensor(s) and optimize the compiled code for the particular shape, data type and other properties the input has.\nIf the shape of the input remains the same across all calls, PyTorch will reuse the compiled code it generated and you will get the best speedup.\nHowever, if these properties change across subsequent calls to ``forward()``/``*_step()``, PyTorch will be forced to recompile the model for the new shapes, and this will significantly slow down your training if it happens on every iteration.\n\n**When your training suddenly becomes slow, it's probably because PyTorch is recompiling the model!**\nHere are some common scenarios when this can happen:\n\n- You are using dataset with different inputs or shapes for validation than for training, causing a recompilation whenever the Trainer switches between training and validation.\n- Your dataset size is not divisible by the batch size, and the dataloader has ``drop_last=False`` (the default).\n  The last batch in your training loop will be smaller and trigger a recompilation.\n\nIdeally, you should try to make the input shape(s) to ``forward()`` static.\nHowever, when this is not possible, you can request PyTorch to compile the code by taking into account possible changes to the input shapes.\n\n.. code-block:: python\n\n    # On PyTorch < 2.2\n    model = torch.compile(model, dynamic=True)\n\nA model compiled with ``dynamic=True`` will typically be slower than a model compiled with static shapes, but it will avoid the extreme cost of recompilation every iteration.\nOn PyTorch 2.2 and later, ``torch.compile`` will detect dynamism automatically and you should no longer need to set this.\n\nIf you still see recompilation issues after dealing with the aforementioned cases, there is a `Compile Profiler in PyTorch <https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html#excessive-recompilation>`_ for further investigation.\n\n\n----\n\n\n***********************************\nExperiment with compilation options\n***********************************\n\nThere are optional settings that, depending on your model, can give additional speedups.\n\n**CUDA Graphs:** By enabling CUDA Graphs, CUDA will record all computations in a graph and replay it every time forward and backward is called.\nThe requirement is that your model must be static, i.e., the input shape must not change and your model must execute the same operations every time.\nEnabling CUDA Graphs often results in a significant speedup, but sometimes also increases the memory usage of your model.\n\n.. code-block:: python\n\n    # Enable CUDA Graphs\n    compiled_model = torch.compile(model, mode=\"reduce-overhead\")\n\n    # This does the same\n    compiled_model = torch.compile(model, options={\"triton.cudagraphs\": True})\n\n|\n\n**Shape padding:** The specific shape/size of the tensors involved in the computation of your model (input, activations, weights, gradients, etc.) can have an impact on the performance.\nWith shape padding enabled, ``torch.compile`` can extend the tensors by padding to a size that gives a better memory alignment.\nNaturally, the tradeoff here is that it will consume a bit more memory.\n\n.. code-block:: python\n\n    # Default is False\n    compiled_model = torch.compile(model, options={\"shape_padding\": True})\n\n\nYou can find a full list of compile options in the `PyTorch documentation <https://pytorch.org/docs/stable/generated/torch.compile.html>`_.\n\n\n----\n\n\n**************************************\nA note about torch.compile in practice\n**************************************\n\nIn practice, you will find that ``torch.compile`` may not work well at first or may be counter-productive to performance.\nCompilation may fail with cryptic error messages that are hard to debug, luckily the PyTorch team is responsive and it's likely that messaging will improve in time.\nIt is not uncommon that ``torch.compile`` will produce a significantly *slower* model or one with higher memory usage. You'll need to invest time in this phase if the model is not among the ones that have a happy path.\nAs a note, the compilation phase itself will take some time, taking up to several minutes.\nFor these reasons, we recommend that you don't invest too much time trying to apply ``torch.compile`` during development, and rather evaluate its effectiveness toward the end when you are about to launch long-running, expensive experiments.\nAlways compare the speed and memory usage of the compiled model against the original model!\n\nFor a thorough troubleshooting guide, see `Torch.compile: the missing manual <https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?usp=sharing>`_.\n\n\n----\n\n\n***********\nLimitations\n***********\n\nThere are a few limitations you should be aware of when using ``torch.compile`` **in conjunction with the Trainer**:\n\n* The Trainer currently does not reapply ``torch.compile`` over :class:`~lightning.pytorch.strategies.DDPStrategy` and :class:`~lightning.pytorch.strategies.FSDPStrategy`, meaning distributed operations can't benefit from speed ups at the moment.\n  This limitation can be avoided by using :class:`~lightning.pytorch.strategies.model_parallel.ModelParallelStrategy`, as described in `Apply torch.compile in configure_model`_ above.\n\n* In some cases, using ``self.log()`` in your LightningModule will cause compilation errors.\n  Until addressed, you can work around these issues by applying ``torch.compile`` to the submodule(s) of your LightningModule rather than to the entire LightningModule at once.\n\n  .. code-block:: python\n\n      import lightning as L\n\n      class MyLightningModule(L.LightningModule):\n          def __init__(self):\n              super().__init__()\n              self.model = MySubModule()\n              self.model = torch.compile(self.model)\n              ...\n\n\n----\n\n\n********************\nAdditional Resources\n********************\n\nHere are a few resources for further reading after you complete this tutorial:\n\n- `PyTorch 2.0 Paper <https://pytorch.org/get-started/pytorch-2-x/>`_\n- `GenAI with PyTorch 2.0 blog post series <https://pytorch.org/blog/accelerating-generative-ai-4/>`_\n- `Training Production AI Models with PyTorch 2.0 <https://pytorch.org/blog/training-production-ai-models/>`_\n- `Empowering Models with Performance: The Art of Generalized Model Transformation Approach <https://pytorch.org/blog/empowering-models-performance/>`_\n- `Torch.compile: the missing manual <https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?usp=sharing>`_\n\n|\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/finetuning.html", "url_rel_html": "advanced/finetuning.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/finetuning.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Transfer Learning¶", "rst_text": ".. include:: transfer_learning.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_init.html", "url_rel_html": "advanced/model_init.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_init.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Efficient initialization¶", "rst_text": ":orphan:\n\n.. _model_init:\n\n########################\nEfficient initialization\n########################\n\nHere are common use cases where you should use Lightning's initialization tricks to avoid major speed and memory bottlenecks when initializing your model.\n\n\n----\n\n\n**************\nHalf-precision\n**************\n\nInstantiating a ``nn.Module`` in PyTorch creates all parameters on CPU in float32 precision by default.\nTo speed up initialization, you can force PyTorch to create the model directly on the target device and with the desired precision without changing your model code.\n\n.. code-block:: python\n\n    trainer = Trainer(accelerator=\"cuda\", precision=\"16-true\")\n\n    with trainer.init_module():\n        # models created here will be on GPU and in float16\n        model = MyLightningModule()\n\nThe larger the model, the more noticeable is the impact on\n\n- **speed:** avoids redundant transfer of model parameters from CPU to device, avoids redundant casting from float32 to half precision\n- **memory:** reduced peak memory usage since model parameters are never stored in float32\n\n\n----\n\n\n***********************************************\nLoading checkpoints for inference or finetuning\n***********************************************\n\nWhen loading a model from a checkpoint, for example when fine-tuning, set ``empty_init=True`` to avoid expensive and redundant memory initialization:\n\n.. code-block:: python\n\n    with trainer.init_module(empty_init=True):\n        # creation of the model is fast\n        # and depending on the strategy allocates no memory, or uninitialized memory\n        model = MyLightningModule.load_from_checkpoint(\"my/checkpoint/path.ckpt\")\n\n    trainer.fit(model)\n\n\n.. warning::\n    This is safe if you are loading a checkpoint that includes all parameters in the model.\n    If you are loading a partial checkpoint (``strict=False``), you may end up with a subset of parameters that have uninitialized weights, unless you handle them accordingly.\n\n\n----\n\n\n********************************************\nModel-parallel training (FSDP and DeepSpeed)\n********************************************\n\nWhen training sharded models with :ref:`FSDP <fully-sharded-training>` or :ref:`DeepSpeed <deepspeed_advanced>`, :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` **should not be used**.\nInstead, override the :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook:\n\n.. code-block:: python\n\n    class MyModel(LightningModule):\n        def __init__(self):\n            super().__init__()\n            # don't instantiate layers here\n            # move the creation of layers to `configure_model`\n\n        def configure_model(self):\n            # create all your layers here\n            self.layers = nn.Sequential(...)\n\n\nDelaying the creation of large layers to the ``configure_model`` hook is necessary in most cases because otherwise initialization gets very slow (minutes) or (and that's more likely) you run out of CPU memory due to the size of the model.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html", "url_rel_html": "advanced/model_parallel.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_parallel.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Train models with billions of parameters¶", "rst_text": ".. _model-parallel:\n\n########################################\nTrain models with billions of parameters\n########################################\n\n**Audience**: Users who want to train massive models of billions of parameters efficiently across multiple GPUs and machines.\n\nLightning provides advanced and optimized model-parallel training strategies to support massive models of billions of parameters.\nCheck out this amazing video for an introduction to model parallelism and its benefits:\n\n.. raw:: html\n\n    <iframe width=\"540\" height=\"300\" src=\"https://www.youtube.com/embed/w_CKzh5C1K4\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n----\n\n\n*****************************************\nWhen NOT to use model-parallel strategies\n*****************************************\n\nModel parallel techniques help when model sizes are fairly large; roughly 500M+ parameters is where we've seen benefits.\nFor small models (for example ResNet50 of around 80M Parameters) where the weights, activations, optimizer states and gradients all fit in GPU memory, you do not need to use a model-parallel strategy.\nInstead, use regular :ref:`distributed data-parallel (DDP) <gpu_intermediate>` training to scale your batch size and speed up training across multiple GPUs and machines.\nThere are several :ref:`DDP optimizations <ddp-optimizations>` you can explore if memory and speed are a concern.\n\n\n----\n\n\n*********************************************\nChoosing the right strategy for your use case\n*********************************************\n\nIf you've determined that your model is large enough that you need to leverage model parallelism, you have two training strategies to choose from: FSDP, the native solution that comes built-in with PyTorch, or the popular third-party `DeepSpeed <https://github.com/microsoft/DeepSpeed>`__ library.\nBoth have a very similar feature set and have been used to train the largest SOTA models in the world.\nOur recommendation is\n\n- **Use FSDP** if you are new to model-parallel training, if you are migrating from PyTorch FSDP to Lightning, or if you are already familiar with DDP.\n- **Use DeepSpeed** if you know you will need cutting edge features not present in FSDP, or you are already familiar with DeepSpeed and are migrating to Lightning.\n\nThe table below points out a few important differences between the two.\n\n.. list-table:: Differences between FSDP and DeepSpeed\n   :header-rows: 1\n\n   * -\n     - :ref:`FSDP <fully-sharded-training>`\n     - :ref:`DeepSpeed <deepspeed_advanced>`\n   * - Dependencies\n     - None\n     - Requires the ``deepspeed`` package\n   * - Configuration options\n     - Simpler and easier to get started\n     - More comprehensive, allows finer control\n   * - Configuration\n     - Via Trainer\n     - Via Trainer or configuration file\n   * - Activation checkpointing\n     - Yes\n     - Yes, but requires changing the model code\n   * - Offload parameters\n     - CPU\n     - CPU or disk\n   * - Distributed checkpoints\n     - Coming soon\n     - Yes\n\n\n----\n\n\n***********\nGet started\n***********\n\nOnce you've chosen the right strategy for your use case, follow the full guide below to get started.\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: FSDP\n   :description: Distribute models with billions of parameters across hundreds GPUs with FSDP\n   :col_css: col-md-4\n   :button_link: model_parallel/fsdp.html\n   :height: 160\n   :tag: advanced\n\n.. displayitem::\n   :header: DeepSpeed\n   :description: Distribute models with billions of parameters across hundreds GPUs with DeepSpeed\n   :col_css: col-md-4\n   :button_link: model_parallel/deepspeed.html\n   :height: 160\n   :tag: advanced\n\n\n.. raw:: html\n\n        </div>\n    </div>\n\n\n----\n\n\n**********************\nThird-party strategies\n**********************\n\nCutting-edge Lightning strategies are being developed by third-parties outside of Lightning.\nIf you want to try some of the latest and greatest features for model-parallel training, check out these :doc:`strategies <../integrations/strategies/index>`.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/deepspeed.html", "url_rel_html": "advanced/model_parallel/deepspeed.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "DeepSpeed¶", "rst_text": ":orphan:\n\n.. _deepspeed_advanced:\n\n*********\nDeepSpeed\n*********\n\n`DeepSpeed <https://github.com/microsoft/DeepSpeed>`__ is a deep learning training optimization library, providing the means to train massive billion parameter models at scale.\nUsing the DeepSpeed strategy, we were able to **train model sizes of 10 Billion parameters and above**, with a lot of useful information in this `benchmark <https://github.com/huggingface/transformers/issues/9996>`_ and the `DeepSpeed docs <https://www.deepspeed.ai/tutorials/megatron/>`__.\nDeepSpeed also offers lower level training optimizations, and efficient optimizers such as `1-bit Adam <https://www.deepspeed.ai/tutorials/onebit-adam/>`_. We recommend using DeepSpeed in environments where speed and memory optimizations are important (such as training large billion parameter models).\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\nBelow is a summary of all the configurations of DeepSpeed.\n\n* :ref:`deepspeed-zero-stage-1` - **Shard optimizer states**, remains at speed parity with DDP whilst providing memory improvement\n\n* :ref:`deepspeed-zero-stage-2` - **Shard optimizer states and gradients**, remains at speed parity with DDP whilst providing even more memory improvement\n\n* :ref:`deepspeed-zero-stage-2-offload` - **Offload optimizer states and gradients to CPU**. Increases distributed communication volume and GPU-CPU device transfer, but provides significant memory improvement\n\n* :ref:`deepspeed-zero-stage-3` - **Shard optimizer states, gradients, parameters and optionally activations**. Increases distributed communication volume, but provides even more memory improvement\n\n* :ref:`deepspeed-zero-stage-3-offload` - **Offload optimizer states, gradients, parameters and optionally activations to CPU**. Increases distributed communication volume and GPU-CPU device transfer, but even more significant memory improvement.\n\n* :ref:`deepspeed-activation-checkpointing` - **Free activations after forward pass**. Increases computation, but provides memory improvement for all stages.\n\nTo use DeepSpeed, you first need to install DeepSpeed using the commands below.\n\n.. code-block:: bash\n\n    pip install deepspeed\n\nIf you run into an issue with the install or later in training, ensure that the CUDA version of the PyTorch you've installed matches your locally installed CUDA (you can see which one has been recognized by running ``nvcc --version``).\n\n.. note::\n\n    DeepSpeed currently only supports single optimizer, single scheduler within the training loop.\n\n    When saving a checkpoint we rely on DeepSpeed which saves a directory containing the model and various components.\n\n\n.. _deepspeed-zero-stage-1:\n\nDeepSpeed ZeRO Stage 1\n======================\n\n`DeepSpeed ZeRO Stage 1 <https://www.deepspeed.ai/tutorials/zero/#zero-overview>`_ partitions your optimizer states (Stage 1) across your GPUs to reduce memory.\n\nIt is recommended to skip Stage 1 and use Stage 2, which comes with larger memory improvements and still remains efficient. Stage 1 is useful to pair with certain optimizations such as `Torch ORT <https://github.com/pytorch/ort>`__.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_1\", precision=16)\n    trainer.fit(model)\n\n\n.. _deepspeed-zero-stage-2:\n\nDeepSpeed ZeRO Stage 2\n======================\n\n`DeepSpeed ZeRO Stage 2 <https://www.deepspeed.ai/tutorials/zero/#zero-overview>`_ partitions your optimizer states (Stage 1) and your gradients (Stage 2) across your GPUs to reduce memory. In most cases, this is more efficient or at parity with DDP, primarily due to the optimized custom communications written by the DeepSpeed team.\nAs a result, benefits can also be seen on a single GPU. Do note that the default bucket sizes allocate around ``3.6GB`` of VRAM to use during distributed communications, which can be tweaked when instantiating the strategy described in a few sections below.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2\", precision=16)\n    trainer.fit(model)\n\n.. code-block:: bash\n\n    python train.py --strategy deepspeed_stage_2 --precision 16 --accelerator 'gpu' --devices 4\n\n\n.. _deepspeed-zero-stage-2-offload:\n\nDeepSpeed ZeRO Stage 2 Offload\n------------------------------\n\nBelow we show an example of running `ZeRO-Offload <https://www.deepspeed.ai/tutorials/zero-offload/>`_. ZeRO-Offload leverages the host CPU to offload optimizer memory/computation, reducing the overall memory consumption.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2_offload\", precision=16)\n    trainer.fit(model)\n\n\nThis can also be done via the command line using a PyTorch Lightning script:\n\n.. code-block:: bash\n\n    python train.py --strategy deepspeed_stage_2_offload --precision 16 --accelerator 'gpu' --devices 4\n\n\nYou can also modify the ZeRO-Offload parameters via the strategy as below.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n\n    model = MyModel()\n    trainer = Trainer(\n        accelerator=\"gpu\",\n        devices=4,\n        strategy=DeepSpeedStrategy(offload_optimizer=True, allgather_bucket_size=5e8, reduce_bucket_size=5e8),\n        precision=16,\n    )\n    trainer.fit(model)\n\n\n.. note::\n    We suggest tuning the ``allgather_bucket_size`` parameter and ``reduce_bucket_size`` parameter to find optimum parameters based on your model size.\n    These control how large a buffer we limit the model to using when reducing gradients/gathering updated parameters. Smaller values will result in less memory, but tradeoff with speed.\n\n    DeepSpeed allocates a reduce buffer size `multiplied by 1.5x <https://github.com/microsoft/DeepSpeed/blob/fead387f7837200fefbaba3a7b14709072d8d2cb/deepspeed/runtime/zero/stage_1_and_2.py#L2188>`_ so take that into consideration when tweaking the parameters.\n\n    The strategy sets a reasonable default of ``2e8``, which should work for most low VRAM GPUs (less than ``7GB``), allocating roughly ``3.6GB`` of VRAM as buffer. Higher VRAM GPUs should aim for values around ``5e8``.\n\nFor even more speed benefit, DeepSpeed offers an optimized CPU version of ADAM called `DeepSpeedCPUAdam <https://deepspeed.readthedocs.io/en/latest/optimizers.html#adam-cpu>`_ to run the offloaded computation, which is faster than the standard PyTorch implementation.\n\n.. code-block:: python\n\n    from lightning.pytorch import LightningModule, Trainer\n    from deepspeed.ops.adam import DeepSpeedCPUAdam\n\n\n    class MyModel(LightningModule):\n        ...\n\n        def configure_optimizers(self):\n            # DeepSpeedCPUAdam provides 5x to 7x speedup over torch.optim.adam(w)\n            return DeepSpeedCPUAdam(self.parameters())\n\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2_offload\", precision=16)\n    trainer.fit(model)\n\n\n.. _deepspeed-zero-stage-3:\n\nDeepSpeed ZeRO Stage 3\n======================\n\nDeepSpeed ZeRO Stage 3 shards the optimizer states, gradients and the model parameters (also optionally activations). Sharding model parameters and activations comes with an increase in distributed communication, however allows you to scale your models massively from one GPU to multiple GPUs.\n**The DeepSpeed team report the ability to fine-tune models with over 40B parameters on a single GPU and over 2 Trillion parameters on 512 GPUs.** For more information we suggest checking the `DeepSpeed ZeRO-3 Offload documentation <https://www.deepspeed.ai/2021/03/07/zero3-offload.html>`__.\n\nWe've ran benchmarks for all these features and given a simple example of how all these features work in Lightning, which you can see at `minGPT <https://github.com/SeanNaren/minGPT/tree/stage3>`_.\n\nTo reach the highest memory efficiency or model size, you must:\n\n1. Use the DeepSpeed strategy with the stage 3 parameter\n2. Use CPU Offloading to offload weights to CPU, plus have a reasonable amount of CPU RAM to offload onto\n3. Use DeepSpeed Activation Checkpointing to shard activations\n\nBelow we describe how to enable all of these to see benefit. **With all these improvements we reached 45 Billion parameters training a GPT model on 8 GPUs with ~1TB of CPU RAM available**.\n\nAlso please have a look at our :ref:`deepspeed-zero-stage-3-tips` which contains a lot of helpful information when configuring your own models.\n\n.. note::\n\n    When saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See :ref:`deepspeed-zero-stage-3-single-file` to obtain a single checkpoint file.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from deepspeed.ops.adam import FusedAdam\n\n\n    class MyModel(LightningModule):\n        ...\n\n        def configure_optimizers(self):\n            return FusedAdam(self.parameters())\n\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\n    trainer.fit(model)\n\n    trainer.test()\n    trainer.predict()\n\n\nYou can also use the Lightning Trainer to run predict or evaluate with DeepSpeed once the model has been trained.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n\n\n    class MyModel(LightningModule):\n        ...\n\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\n    trainer.test(ckpt_path=\"my_saved_deepspeed_checkpoint.ckpt\")\n\n\nShard Model Instantly to Reduce Initialization Time/Memory\n----------------------------------------------------------\n\nWhen instantiating really large models, it is sometimes necessary to shard the model layers instantly.\n\nThis is the case if layers may not fit on one single machines CPU or GPU memory, but would fit once sharded across multiple machines.\nWe expose a hook that layers initialized within the hook will be sharded instantly on a per layer basis, allowing you to instantly shard models.\n\nThis reduces the time taken to initialize very large models, as well as ensure we do not run out of memory when instantiating larger models. For more information you can refer to the DeepSpeed docs for `Constructing Massive Models <https://deepspeed.readthedocs.io/en/latest/zero3.html>`_.\n\n.. code-block:: python\n\n    import torch.nn as nn\n    from lightning.pytorch import Trainer\n    from deepspeed.ops.adam import FusedAdam\n\n\n    class MyModel(LightningModule):\n        ...\n\n        def configure_model(self):\n            # Created within sharded model context, modules are instantly sharded across processes\n            # as soon as they are made.\n            self.block = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n\n        def configure_optimizers(self):\n            return FusedAdam(self.parameters())\n\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\n    trainer.fit(model)\n\n    trainer.test()\n    trainer.predict()\n\n\nSee also: See also: :doc:`../model_init`\n\n\n.. _deepspeed-zero-stage-3-offload:\n\nDeepSpeed ZeRO Stage 3 Offload\n------------------------------\n\nDeepSpeed ZeRO Stage 3 Offloads optimizer state, gradients to the host CPU to reduce memory usage as ZeRO Stage 2 does, however additionally allows you to offload the parameters as well for even more memory saving.\n\n.. note::\n\n    When saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See :ref:`deepspeed-zero-stage-3-single-file` to obtain a single checkpoint file.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n\n    # Enable CPU Offloading\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\n    trainer.fit(model)\n\n    # Enable CPU Offloading, and offload parameters to CPU\n    model = MyModel()\n    trainer = Trainer(\n        accelerator=\"gpu\",\n        devices=4,\n        strategy=DeepSpeedStrategy(\n            stage=3,\n            offload_optimizer=True,\n            offload_parameters=True,\n        ),\n        precision=16,\n    )\n    trainer.fit(model)\n\n\nDeepSpeed Infinity (NVMe Offloading)\n------------------------------------\n\nAdditionally, DeepSpeed supports offloading to NVMe drives for even larger models, utilizing the large memory space found in NVMes. DeepSpeed `reports <https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/>`__ the ability to fine-tune 1 Trillion+ parameters using NVMe Offloading on one 8 GPU machine. Below shows how to enable this, assuming the NVMe drive is mounted in a directory called ``/local_nvme``.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n\n    # Enable CPU Offloading\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\n    trainer.fit(model)\n\n    # Enable CPU Offloading, and offload parameters to CPU\n    model = MyModel()\n    trainer = Trainer(\n        accelerator=\"gpu\",\n        devices=4,\n        strategy=DeepSpeedStrategy(\n            stage=3,\n            offload_optimizer=True,\n            offload_parameters=True,\n            remote_device=\"nvme\",\n            offload_params_device=\"nvme\",\n            offload_optimizer_device=\"nvme\",\n            nvme_path=\"/local_nvme\",\n        ),\n        precision=16,\n    )\n    trainer.fit(model)\n\nWhen offloading to NVMe you may notice that the speed is slow. There are parameters that need to be tuned based on the drives that you are using. Running the `aio_bench_perf_sweep.py <https://github.com/microsoft/DeepSpeed/blob/master/csrc/aio/py_test/aio_bench_perf_sweep.py>`__ script can help you to find optimum parameters. See the `issue <https://github.com/deepspeedai/DeepSpeed/issues/998>`__ for more information on how to parse the information.\n\n.. _deepspeed-activation-checkpointing:\n\nDeepSpeed Activation Checkpointing\n----------------------------------\n\nActivation checkpointing frees activations from memory as soon as they are not needed during the forward pass.\nThey are then re-computed for the backwards pass as needed.\n\nActivation checkpointing is very useful when you have intermediate layers that produce large activations.\n\nThis saves memory when training larger models, however requires using a checkpoint function to run modules as shown below.\n\n.. warning::\n\n    Ensure to not wrap the entire model with activation checkpointing. This is not the intended usage of activation checkpointing, and will lead to failures as seen in `this discussion <https://github.com/Lightning-AI/lightning/discussions/9144>`__.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    import deepspeed\n\n\n    class MyModel(LightningModule):\n        ...\n\n        def __init__(self):\n            super().__init__()\n            self.block_1 = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n            self.block_2 = torch.nn.Linear(32, 2)\n\n        def forward(self, x):\n            # Use the DeepSpeed checkpointing function instead of calling the module directly\n            # checkpointing self.block_1 means the activations are deleted after use,\n            # and re-calculated during the backward passes\n            x = deepspeed.checkpointing.checkpoint(self.block_1, x)\n            return self.block_2(x)\n\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n    import deepspeed\n\n\n    class MyModel(LightningModule):\n        ...\n\n        def configure_model(self):\n            self.block_1 = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n            self.block_2 = torch.nn.Linear(32, 2)\n\n        def forward(self, x):\n            # Use the DeepSpeed checkpointing function instead of calling the module directly\n            x = deepspeed.checkpointing.checkpoint(self.block_1, x)\n            return self.block_2(x)\n\n\n    model = MyModel()\n\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\n\n    # Enable CPU Activation Checkpointing\n    trainer = Trainer(\n        accelerator=\"gpu\",\n        devices=4,\n        strategy=DeepSpeedStrategy(\n            stage=3,\n            offload_optimizer=True,  # Enable CPU Offloading\n            cpu_checkpointing=True,  # (Optional) offload activations to CPU\n        ),\n        precision=16,\n    )\n    trainer.fit(model)\n\n\n.. _deepspeed-zero-stage-3-tips:\n\nDeepSpeed ZeRO Stage 3 Tips\n---------------------------\n\nHere is some helpful information when setting up DeepSpeed ZeRO Stage 3 with Lightning.\n\n* If you're using Adam or AdamW, ensure to use FusedAdam or DeepSpeedCPUAdam (for CPU Offloading) rather than the default torch optimizers as they come with large speed benefits\n* Treat your GPU/CPU memory as one large pool. In some cases, you may not want to offload certain things (like activations) to provide even more space to offload model parameters\n* When offloading to the CPU, make sure to bump up the batch size as GPU memory will be freed\n* We also support sharded checkpointing. By passing ``save_full_weights=False`` to the ``DeepSpeedStrategy``, we'll save shards of the model which allows you to save extremely large models. However to load the model and run test/validation/predict you must use the Trainer object.\n\n.. _deepspeed-zero-stage-3-single-file:\n\nCollating Single File Checkpoint for DeepSpeed ZeRO Stage 3\n-----------------------------------------------------------\n\nAfter training using ZeRO Stage 3, you'll notice that your checkpoints are a directory of sharded model and optimizer states. If you'd like to collate a single file from the checkpoint directory please use the below command, which handles all the Lightning states additionally when collating the file.\n\n.. code-block:: python\n\n    from lightning.pytorch.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\n\n    # lightning deepspeed has saved a directory instead of a file\n    save_path = \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\"\n    output_path = \"lightning_model.pt\"\n    convert_zero_checkpoint_to_fp32_state_dict(save_path, output_path)\n\n\n.. warning::\n\n    This single file checkpoint does not include the optimizer/lr-scheduler states. This means we cannot restore training via the ``trainer.fit(ckpt_path=)`` call. Ensure to keep the sharded checkpoint directory if this is required.\n\nCustom DeepSpeed Config\n=======================\n\nIn some cases you may want to define your own DeepSpeed Config, to access all parameters defined. We've exposed most of the important parameters, however, there may be debugging parameters to enable. Also, DeepSpeed allows the use of custom DeepSpeed optimizers and schedulers defined within a config file that is supported.\n\n.. note::\n    All strategy default parameters will be ignored when a config object is passed.\n    All compatible arguments can be seen in the `DeepSpeed docs <https://www.deepspeed.ai/docs/config-json/>`_.\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n\n    deepspeed_config = {\n        \"zero_allow_untested_optimizer\": True,\n        \"optimizer\": {\n            \"type\": \"OneBitAdam\",\n            \"params\": {\n                \"lr\": 3e-5,\n                \"betas\": [0.998, 0.999],\n                \"eps\": 1e-5,\n                \"weight_decay\": 1e-9,\n                \"cuda_aware\": True,\n            },\n        },\n        \"scheduler\": {\n            \"type\": \"WarmupLR\",\n            \"params\": {\n                \"last_batch_iteration\": -1,\n                \"warmup_min_lr\": 0,\n                \"warmup_max_lr\": 3e-5,\n                \"warmup_num_steps\": 100,\n            },\n        },\n        \"zero_optimization\": {\n            \"stage\": 2,  # Enable Stage 2 ZeRO (Optimizer/Gradient state partitioning)\n            \"offload_optimizer\": {\"device\": \"cpu\"},  # Enable Offloading optimizer state/calculation to the host CPU\n            \"contiguous_gradients\": True,  # Reduce gradient fragmentation.\n            \"overlap_comm\": True,  # Overlap reduce/backward operation of gradients for speed.\n            \"allgather_bucket_size\": 2e8,  # Number of elements to all gather at once.\n            \"reduce_bucket_size\": 2e8,  # Number of elements we reduce/allreduce at once.\n        },\n    }\n\n    model = MyModel()\n    trainer = Trainer(accelerator=\"gpu\", devices=4, strategy=DeepSpeedStrategy(config=deepspeed_config), precision=16)\n    trainer.fit(model)\n\n\nWe support taking the config as a json formatted file:\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.strategies import DeepSpeedStrategy\n\n    model = MyModel()\n    trainer = Trainer(\n        accelerator=\"gpu\", devices=4, strategy=DeepSpeedStrategy(config=\"/path/to/deepspeed_config.json\"), precision=16\n    )\n    trainer.fit(model)\n\n\nYou can use also use an environment variable via your PyTorch Lightning script:\n\n.. code-block:: bash\n\n    PL_DEEPSPEED_CONFIG_PATH=/path/to/deepspeed_config.json python train.py --strategy deepspeed\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html", "url_rel_html": "advanced/model_parallel/fsdp.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Train models with billions of parameters using FSDP¶", "rst_text": ":orphan:\n\n.. _fully-sharded-training:\n\n###################################################\nTrain models with billions of parameters using FSDP\n###################################################\n\nUse Fully Sharded Data Parallel (FSDP) to train large models with billions of parameters efficiently on multiple GPUs and across multiple machines.\n\nToday, large models with billions of parameters are trained with many GPUs across several machines in parallel.\nEven a single H100 GPU with 80 GB of VRAM (one of the biggest today) is not enough to train just a 30B parameter model (even with batch size 1 and 16-bit precision).\nThe memory consumption for training is generally made up of\n\n1. the model parameters,\n2. the layer activations (forward),\n3. the gradients (backward) and\n4. the optimizer states (e.g., Adam has two additional exponential averages per parameter).\n\n|\n\nWhen the sum of these memory components exceed the VRAM of a single GPU, regular data-parallel training (DDP) can no longer be employed.\nOne of the methods that can alleviate this limitation is called **Fully Sharded Data Parallel (FSDP)**, and in this guide, you will learn how to effectively scale large models with it.\n\n\n----\n\n\n***************************\nChecklist: When to use FSDP\n***************************\n\n|\n\n✅   I have multiple GPUs\n\n✅   I have tried regular DDP training with batch size 1 but I run out of memory\n\n✅   I have PyTorch 2.0 or newer installed\n\n\n----\n\n\n**********************\nEnable FSDP in Trainer\n**********************\n\n\nTo enable model-parallel training with FSDP in a single-line change, set ``strategy=\"fsdp\"``:\n\n.. code-block:: python\n\n    trainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=\"fsdp\")\n\nAs we will see in the next sections, there are many settings we can tune to optimize memory usage and throughput, scaling to massively large models.\nThis is equivalent to the above, but will let us configure additional settings later:\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import FSDPStrategy\n\n    trainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=FSDPStrategy())\n\n\nHere is a full code example:\n\n.. code-block:: python\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader\n\n    import lightning as L\n    from lightning.pytorch.strategies import FSDPStrategy\n    from lightning.pytorch.demos import Transformer, WikiText2\n\n\n    class LanguageModel(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(  # 1B parameters\n                vocab_size=vocab_size,\n                nlayers=32,\n                nhid=4096,\n                ninp=1024,\n                nhead=64,\n            )\n\n        def training_step(self, batch):\n            input, target = batch\n            output = self.model(input, target)\n            loss = F.nll_loss(output, target.view(-1))\n            self.log(\"train_loss\", loss, prog_bar=True)\n            return loss\n\n        def configure_optimizers(self):\n            return torch.optim.Adam(self.parameters(), lr=0.1)\n\n\n    L.seed_everything(42)\n\n    # Data\n    dataset = WikiText2()\n    train_dataloader = DataLoader(dataset)\n\n    # Model\n    model = LanguageModel(vocab_size=dataset.vocab_size)\n\n    # Trainer\n    trainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=FSDPStrategy())\n    trainer.fit(model, train_dataloader)\n    trainer.print(torch.cuda.memory_summary())\n\n\n\nWe will reuse this Transformer example throughout the guide, optimize speed and memory usage, and compare it to regular DDP training.\n\n\n----\n\n\n*********************\nIdentify large layers\n*********************\n\nModels that have many large layers like linear layers in LLMs, ViTs, etc. with >100M parameters will benefit the most from FSDP because the memory they consume through parameters, activations and corresponding optimizer states can be evenly split across all GPUs.\nHowever, one should avoid splitting small layers that have a few thousand parameters because communication overhead would dominate and slow the training down.\nWe can specify a list of layer classes in the **wrapping policy** to inform FSDP which parameters it should wrap:\n\n.. code-block:: python\n\n    # 1. Define a set of layers that FSDP should manage\n    #    Here we are choosing the large encoder and decoder layers\n    policy = {nn.TransformerEncoderLayer, nn.TransformerDecoderLayer}\n\n    # 2. Pass the policy to the FSDPStrategy object\n    strategy = FSDPStrategy(auto_wrap_policy=policy)\n\n    trainer = L.Trainer(..., strategy=strategy)\n\n.. collapse:: Alternative ways to define the policy (Lightning < 2.1)\n\n    The ``auto_wrap_policy`` argument also accepts the old-style function-policies. For example:\n\n    .. code-block:: python\n\n        from functools import partial\n\n        # 1. Import a suiting wrapping policy from PyTorch\n        from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n\n        # 2. Configure the policy\n        policy = partial(size_based_auto_wrap_policy, min_num_params=10000)\n\n        # 3. Pass it to the FSDPStrategy object\n        strategy = FSDPStrategy(auto_wrap_policy=policy)\n\n    PyTorch provides several of these functional policies under ``torch.distributed.fsdp.wrap``.\n\n|\n\nVerify that FSDP works with your model by comparing the peak memory usage printed in the CUDA memory summary (see example above) with regular DDP training.\nYou should see a decrease in allocated memory and a slight increase in iteration time:\n\n.. list-table:: Numbers were produced with A100 40GB GPUs, Lightning 2.1 and PyTorch 2.1.\n   :widths: 25 25 25\n   :header-rows: 1\n\n   * -\n     - DDP\n     - FSDP\n   * - Memory (MB)\n     - 23’125\n     - 9’627\n   * - Iterations per second\n     - 4.31\n     - 3.19\n\n----\n\n\n*****************************\nSpeed up model initialization\n*****************************\n\nThe standard practice in PyTorch is to put all model parameters into CPU memory first and then in a second step move them to the GPU device.\nHowever, the larger the model the longer these two steps take.\nIf you create the large model layers inside the :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook, you can initialize very large models quickly and reduce memory peaks.\n\nBefore:\n\n.. code-block:: python\n\n    # Slow: Places the model on CPU first\n    class LanguageModel(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            # 1B parameters\n            self.model = Transformer(vocab_size=vocab_size, nlayers=32, nhid=4096, ninp=1024, nhead=64)\n\nAfter:\n\n.. code-block:: python\n\n    # Fast: Delays the model creation until Trainer can place it on GPU\n    class LanguageModel(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.vocab_size = vocab_size\n            self.model = None\n\n        def configure_model(self):\n            if self.model is not None:\n                return\n            self.model = Transformer(  # 1B parameters\n                vocab_size=self.vocab_size,\n                nlayers=32,\n                nhid=4096,\n                ninp=1024,\n                nhead=64,\n            )\n\n\nIt is best practice to make the code in :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` idempotent as shown here.\nLearn more about :doc:`efficient initialization of models in Lightning <../model_init>`.\n\n\n----\n\n\n******************************\nOptimize the sharding strategy\n******************************\n\nBy default, FSDP will automatically shard 1) the model weights 2) the gradients during backward and 3) the optimizer states across all GPUs of the corresponding layers selected by the auto-wrap-policy.\nYou can configure the following options to trade-off memory for speed:\n\n.. code-block:: python\n\n    strategy = FSDPStrategy(\n        # Default: Shard weights, gradients, optimizer state (1 + 2 + 3)\n        sharding_strategy=\"FULL_SHARD\",\n        # Shard gradients, optimizer state (2 + 3)\n        sharding_strategy=\"SHARD_GRAD_OP\",\n        # Full-shard within a machine, replicate across machines\n        sharding_strategy=\"HYBRID_SHARD\",\n        # Don't shard anything (similar to DDP)\n        sharding_strategy=\"NO_SHARD\",\n    )\n    trainer = L.Trainer(..., strategy=strategy)\n\n\n**Recipe for choosing a sharding strategy:**\n\n1. Try the default settings first (FULL_SHARD). This is the slowest but will save you the most memory.\n2. Try SHARD_GRAD_OP. If you run out of memory, revert back to the default (FULL_SHARD). Otherwise you should expect to see an increase in iteration speed.\n3. If you are training across many machines, try HYBRID_SHARD.\n\n|\n\nHere is the memory and speed impact for each option when configured in our example code:\n\n.. list-table:: Numbers were produced with A100 40GB GPUs, Lightning 2.1 and PyTorch 2.1.\n   :widths: 25 25 25 25 25\n   :header-rows: 1\n\n   * -\n     - DDP\n     - NO_SHARD\n     - SHARD_GRAD_OP\n     - FULL_SHARD\n   * - Memory (MB)\n     - 23’125\n     - 19’296\n     - 11’772\n     - 9’627\n   * - Iterations per second\n     - 4.31\n     - 3.04\n     - 3.61\n     - 3.19\n\n\n----\n\n\n**************************\nTrade-off speed for memory\n**************************\n\nIf you are short on GPU memory because you are training large models with 10+ billion parameters or require extreme batch sizes, consider trading off speed for more memory by enabling activation checkpointing or CPU offload.\n\n\nActivation checkpointing\n========================\n\nActivations, the intermediate outputs of layers, are stored during the forward pass and needed during the backward pass to compute the gradients.\nBy enabling activation checkpointing, we can choose to discard and recompute selected layer activations dynamically during the backward pass when they are required, instead of storing them throughout the forward pass.\nWhile this approach may slightly reduce training speed, it significantly reduces memory consumption.\nThe freed-up memory can then be allocated to increase the model's capacity or accommodate larger batch sizes, resulting in potential performance improvements.\n\nTo enable activation checkpointing, pass in the list of layers to checkpoint.\nThis is typically your transformer block (including attention + feed-forward):\n\n.. code-block:: python\n\n    strategy = FSDPStrategy(\n        # Enable activation checkpointing on these layers\n        activation_checkpointing_policy={\n            nn.TransformerEncoderLayer,\n            nn.TransformerDecoderLayer,\n        },\n    )\n    trainer = L.Trainer(..., strategy=strategy)\n\n\nAs in our example, it is typical to set the ``activation_checkpointing_policy`` the same as ``auto_wrap_policy``.\n\n\nOffload parameters to CPU\n=========================\n\nThe most drastic GPU memory savings can be achieved by offloading parameters to the CPU:\n\n.. code-block:: python\n\n    # Set `cpu_offload=True`\n    strategy = FSDPStrategy(..., cpu_offload=True)\n    trainer = L.Trainer(..., strategy=strategy)\n\nThe drawback is a much slower training speed due to the added communication between CPU and GPU for transferring parameters in every forward pass.\nYou should use this only if you have enough CPU memory and other scaling methods don’t give you enough memory savings.\nIn our example, we see a 3.5x memory saving, but a significant increase in iteration time:\n\n.. list-table:: Numbers were produced with A100 40GB GPUs, Lightning 2.1 and PyTorch 2.1.\n   :widths: 25 25 25 25\n   :header-rows: 1\n\n   * -\n     - DDP\n     - FSDP\n     - FSDP + CPU offload\n   * - Memory (MB)\n     - 23’125\n     - 9’627\n     - 2’790\n   * - Iterations per second\n     - 4.31\n     - 3.19\n     - 0.02\n\n\n----\n\n\n*****************\nSave a checkpoint\n*****************\n\nSince training large models can be very expensive, it is best practice to checkpoint the training state periodically in case it gets interrupted unexpectedly.\nLightning saves a checkpoint every epoch by default, and there are :ref:`several settings to configure the checkpointing behavior in detail <checkpointing>`.\n\n.. code-block:: python\n\n    # Default: Saves a checkpoint every epoch\n    trainer = L.Trainer()\n    trainer.fit(model)\n\n    # You can also manually trigger a checkpoint at any time\n    trainer.save_checkpoint(\"path/to/checkpoint/file\")\n\n    # DON'T do this (inefficient):\n    # torch.save(\"path/to/checkpoint/file\", model.state_dict())\n\nFor single-machine training this typically works fine, but for larger models saving a checkpoint can become slow (minutes not seconds) or overflow CPU memory (OOM) depending on the system.\nTo reduce memory peaks and speed up the saving to disk, set ``state_dict_type=\"sharded\"``:\n\n.. code-block:: python\n\n    # Default: Save a single, consolidated checkpoint file\n    strategy = FSDPStrategy(state_dict_type=\"full\")\n\n    # Save individual files with state from each process\n    strategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n\nWith this, each process/GPU will save its own file into a folder at the given path by default.\nThe resulting checkpoint folder will have this structure:\n\n.. code-block:: text\n\n    path/to/checkpoint/file\n    ├── .metadata\n    ├── __0_0.distcp\n    ├── __1_0.distcp\n    ...\n    └── meta.pt\n\nThe “sharded” checkpoint format is the most efficient to save and load in Lightning.\n\n**Which checkpoint format should I use?**\n\n- ``state_dict_type=\"sharded\"``: Use for pre-training very large models. It is fast and uses less memory, but it is less portable. An extra step is needed to :doc:`convert the sharded checkpoint into a regular checkpoint file <../../common/checkpointing_expert>`.\n- ``state_dict_type=\"full\"``: Use when pre-training small to moderately large models (less than 10B parameters), when fine-tuning, and when portability is required.\n\n\n----\n\n\n*****************\nLoad a checkpoint\n*****************\n\nYou can easily :ref:`load checkpoints <checkpointing>` saved by Lightning to resume training:\n\n.. code-block:: python\n\n    trainer = L.Trainer(...)\n\n    # Restore the training progress, weights, and optimizer state\n    trainer.fit(model, ckpt_path=\"path/to/checkpoint/file\")\n\n\nThe Trainer will automatically recognize whether the provided path contains a checkpoint saved with ``state_dict_type=\"full\"`` or ``state_dict_type=\"sharded\"``.\nCheckpoints saved with ``state_dict_type=\"full\"`` can be loaded by all strategies, but sharded checkpoints can only be loaded by FSDP.\nRead :ref:`the checkpoints guide <checkpointing>` to explore more features.\n\n\n----\n\n\n**********************************\nAdvanced performance optimizations\n**********************************\n\nIf you’ve reached a good understanding of how the different FSDP settings impact the memory usage and speed of your model, here are a few more to squeeze out the last bit of performance.\nThese settings really depend on the specific use cases, so you will have to turn them on and off to see the impact on your model.\n\n\nDisable foreach in the optimizer\n================================\n\nThe commonly used optimizers in PyTorch have a setting ``foreach=True|False`` that speeds up the parameter and state updates when enabled.\nHowever, you might see a slight memory peak and the larger the model is, the more noticeable it can be.\nConsider disabling the ``foreach`` option if undesired memory patterns occur:\n\n.. code-block:: python\n\n    optimizer = torch.optim.AdamW(model.parameters(), foreach=False)\n\n`See the full list of optimizers that support this <https://pytorch.org/docs/stable/optim.html#algorithms>`_.\n\n\nLimit all-gathers\n=================\n\nIf you are running training close to the max.\nGPU memory limit, you might be getting so-called CUDA malloc retries.\nThis is essentially the GPU running out of memory but before crashing completely, it tries to find some unused or cached memory it can free.\nWhen they happen frequently, these retries can have a significant impact on speed.\nNormally, you would decrease the batch size slightly to avoid it.\nWith FSDP, you have one more knob you can tweak to combat the issue, by setting ``limit_all_gathers=True``:\n\n.. code-block:: python\n\n    strategy = FSDPStrategy(\n        # Default: The CPU will schedule the transfer of weights between GPUs\n        # at will, sometimes too aggressively\n        limit_all_gathers=False,\n        # Enable this if you are close to the max. GPU memory usage\n        limit_all_gathers=True,\n    )\n    trainer = L.Trainer(..., strategy=strategy)\n\nYou can monitor CUDA malloc retries in the output of ``torch.cuda.memory_summary()`` for example, or through the PyTorch profiler.\n\n\nManual wrapping\n===============\n\nManual wrapping can be useful to explore complex sharding strategies by applying ``wrap`` selectively to some parts of the model.\nTo activate parameter sharding with manual wrapping, you can wrap your model using the ``wrap`` function.\nInternally in Lightning, we enable a context manager around the :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to make sure the ``wrap`` parameters are passed correctly.\n\nHere is an example that uses ``wrap`` to create a model:\n\n.. code-block:: python\n\n    import torch\n    import torch.nn as nn\n    import lightning as L\n\n    from torch.distributed.fsdp.wrap import wrap\n\n\n    class MyModel(L.LightningModule):\n        def configure_model(self):\n            self.linear_layer = nn.Linear(32, 32)\n            self.block = nn.Sequential(nn.Linear(32, 32), nn.Linear(32, 32))\n\n            # Modules get sharded across processes as soon as they are wrapped with `wrap`.\n            linear_layer = wrap(self.linear_layer)\n\n            for i, layer in enumerate(self.block):\n                self.block[i] = wrap(layer)\n\n            self.model = nn.Sequential(linear_layer, nn.ReLU(), self.block)\n\n        def configure_optimizers(self):\n            return torch.optim.AdamW(self.model.parameters())\n\n\n    model = MyModel()\n    trainer = L.Trainer(accelerator=\"cuda\", devices=4, strategy=\"fsdp\", precision=16)\n    trainer.fit(model)\n\nWhen not using FSDP, these ``wrap`` calls are a no-op.\nThis means once the changes have been made, there is no need to remove the changes for other strategies.\nIn this case, Lightning will not re-wrap your model, so you don't need to set ``FSDPStrategy(auto_wrap_policy=...)``.\nCheck out `this tutorial <https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__ to learn more about it.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp.html", "url_rel_html": "advanced/model_parallel/tp.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_parallel/tp.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Tensor Parallelism¶", "rst_text": ""}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp_fsdp.html", "url_rel_html": "advanced/model_parallel/tp_fsdp.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "2D Parallelism (Tensor Parallelism + FSDP)¶", "rst_text": "##########################################\n2D Parallelism (Tensor Parallelism + FSDP)\n##########################################\n\n2D Parallelism combines Tensor Parallelism (TP) and Fully Sharded Data Parallelism (FSDP) to leverage the memory efficiency of FSDP and the computational scalability of TP.\nThis hybrid approach balances the trade-offs of each method, optimizing memory usage and minimizing communication overhead, enabling the training of extremely large models on large GPU clusters.\n\nThe :doc:`Tensor Parallelism documentation <tp>` and a general understanding of `FSDP <https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`_ are a prerequisite for this tutorial.\n\n.. raw:: html\n\n    <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/tensor-parallelism-supercharging-large-model-training-with-pytorch-lightning\">\n      <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\" style=\"width: auto; max-width: none;\"/>\n    </a>\n\n\n----\n\n\n*********************\nEnable 2D parallelism\n*********************\n\nWe will start off with the same feed forward example model as in the :doc:`Tensor Parallelism tutorial <tp>`.\n\n.. code-block:: python\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class FeedForward(nn.Module):\n        def __init__(self, dim, hidden_dim):\n            super().__init__()\n            self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n            self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n            self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n        def forward(self, x):\n            return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\nNext, we implement the LightningModule and override the :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` that applies the desired parallelism to our model.\n\n.. code-block:: python\n\n    import lightning as L\n    from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n    from torch.distributed.tensor.parallel import parallelize_module\n    from torch.distributed._composable.fsdp.fully_shard import fully_shard\n\n\n    class LitModel(L.LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.model = FeedForward(8192, 8192)\n\n        def configure_model(self):\n            # Lightning will set up a `self.device_mesh` for you\n            # Here, it is 2-dimensional\n            tp_mesh = self.device_mesh[\"tensor_parallel\"]\n            dp_mesh = self.device_mesh[\"data_parallel\"]\n\n            if tp_mesh.size() > 1:\n                # Use PyTorch's distributed tensor APIs to parallelize the model\n                plan = {\n                    \"w1\": ColwiseParallel(),\n                    \"w2\": RowwiseParallel(),\n                    \"w3\": ColwiseParallel(),\n                }\n                parallelize_module(self.model, tp_mesh, plan)\n\n            if dp_mesh.size() > 1:\n                # Use PyTorch's FSDP2 APIs to parallelize the model\n                fully_shard(self.model.w1, mesh=dp_mesh)\n                fully_shard(self.model.w2, mesh=dp_mesh)\n                fully_shard(self.model.w3, mesh=dp_mesh)\n                fully_shard(self.model, mesh=dp_mesh)\n\nBy writing the parallelization code in this special hook rather than hardcoding it into the model, we keep the original source code clean and maintainable.\nIn addition to the tensor-parallel code from the :doc:`Tensor Parallelism tutorial <tp>`, this implementation now also shards the model's parameters using FSDP along the data-parallel dimension.\n\nFinally, configure the :class:`~lightning.pytorch.strategies.model_parallel.ModelParallelStrategy` and configure the data-parallel and tensor-parallel sizes:\n\n.. code-block:: python\n\n    import lightning as L\n    from lightning.pytorch.strategies import ModelParallelStrategy\n\n    # 1. Create the strategy\n    strategy = ModelParallelStrategy(\n        # Define the size of the 2D parallelism\n        # Set these to \"auto\" (default) to apply TP intra-node and FSDP inter-node\n        data_parallel_size=2,\n        tensor_parallel_size=2,\n    )\n\n    # 2. Configure devices and set the strategy in Trainer\n    trainer = L.Trainer(accelerator=\"cuda\", devices=4, strategy=strategy)\n    trainer.fit(...)\n\n\nIn this example with 4 GPUs, the Trainer will create a device mesh that groups GPU 0-1 and GPU 2-3 (2 groups because ``data_parallel_size=2``, and 2 GPUs per group because ``tensor_parallel_size=2``).\nLater on when ``trainer.fit(model)`` is called, each layer wrapped with FSDP (``fully_shard``) will be split into two shards, one for the GPU 0-1 group, and one for the GPU 2-3 group.\nFinally, the tensor parallelism will apply to each group, splitting the sharded tensor across the GPUs within each group.\n\n.. collapse:: Full training example (requires at least 4 GPUs).\n\n    .. code-block:: python\n\n        import torch\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n        from torch.distributed.tensor.parallel import parallelize_module\n        from torch.distributed._composable.fsdp.fully_shard import fully_shard\n\n        import lightning as L\n        from lightning.pytorch.demos.boring_classes import RandomDataset\n        from lightning.pytorch.strategies import ModelParallelStrategy\n\n\n        class FeedForward(nn.Module):\n            def __init__(self, dim, hidden_dim):\n                super().__init__()\n                self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n                self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n                self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n            def forward(self, x):\n                return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\n        class LitModel(L.LightningModule):\n            def __init__(self):\n                super().__init__()\n                self.model = FeedForward(8192, 8192)\n\n            def configure_model(self):\n                if self.device_mesh is None:\n                    return\n\n                # Lightning will set up a `self.device_mesh` for you\n                # Here, it is 2-dimensional\n                tp_mesh = self.device_mesh[\"tensor_parallel\"]\n                dp_mesh = self.device_mesh[\"data_parallel\"]\n\n                if tp_mesh.size() > 1:\n                    # Use PyTorch's distributed tensor APIs to parallelize the model\n                    plan = {\n                        \"w1\": ColwiseParallel(),\n                        \"w2\": RowwiseParallel(),\n                        \"w3\": ColwiseParallel(),\n                    }\n                    parallelize_module(self.model, tp_mesh, plan)\n\n                if dp_mesh.size() > 1:\n                    # Use PyTorch's FSDP2 APIs to parallelize the model\n                    fully_shard(self.model.w1, mesh=dp_mesh)\n                    fully_shard(self.model.w2, mesh=dp_mesh)\n                    fully_shard(self.model.w3, mesh=dp_mesh)\n                    fully_shard(self.model, mesh=dp_mesh)\n\n\n            def training_step(self, batch):\n                output = self.model(batch)\n                loss = output.sum()\n                return loss\n\n            def configure_optimizers(self):\n                return torch.optim.AdamW(self.model.parameters(), lr=3e-3)\n\n            def train_dataloader(self):\n                # Trainer configures the sampler automatically for you such that\n                # all batches in a tensor-parallel group are identical\n                dataset = RandomDataset(8192, 64)\n                return torch.utils.data.DataLoader(dataset, batch_size=8, num_workers=2)\n\n\n        strategy = ModelParallelStrategy(\n            data_parallel_size=2,\n            tensor_parallel_size=2,\n        )\n        trainer = L.Trainer(\n            accelerator=\"cuda\",\n            devices=4,\n            strategy=strategy,\n            max_epochs=1,\n        )\n\n        model = LitModel()\n        trainer.fit(model)\n\n        trainer.print(f\"Peak memory usage: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n\n\n.. note:: 2D Parallelism in PyTorch Lightning as well as PyTorch is experimental. The APIs may change in the future.\n\nBeyond this toy example, we recommend you study our `LLM 2D Parallel Example (Llama 3) <https://github.com/Lightning-AI/pytorch-lightning/tree/master/examples/pytorch/tensor_parallel>`_.\n\n\n----\n\n\n*******************\nEffective use cases\n*******************\n\nIn the toy example above, the parallelization is configured to work within a single machine across multiple GPUs.\nHowever, in practice the main use case for 2D parallelism is in multi-node training, where one can effectively combine both methods to maximize throughput and model scale.\nSince tensor-parallelism requires blocking collective calls, fast GPU data transfers are essential to keep throughput high and therefore TP is typically applied across GPUs within a machine.\nOn the other hand, FSDP by design has the advantage that it can overlap GPU transfers with the computation (it can prefetch layers).\nHence, combining FSDP for inter-node parallelism and TP for intra-node parallelism is generally a good strategy to minimize both the latency and network bandwidth usage, making it possible to scale to much larger models than is possible with FSDP alone.\n\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import ModelParallelStrategy\n\n    strategy = ModelParallelStrategy(\n        # Default is \"auto\"\n        # Applies TP intra-node and DP inter-node\n        data_parallel_size=\"auto\",\n        tensor_parallel_size=\"auto\",\n    )\n\n\n----\n\n\n***************************\nData-loading considerations\n***************************\n\nIn a tensor-parallelized model, it is important that the model receives an identical input on each GPU that participates in the same tensor-parallel group.\nHowever, across the data-parallel dimension, the inputs should be different.\nIn other words, if TP is applied within a node, and FSDP across nodes, each node must receive a different batch, but every GPU within the node gets the same batch of data.\n\nIf you use a PyTorch data loader, the Trainer will automatically handle this for you by configuring the distributed sampler.\nHowever, when you shuffle data in your dataset or data loader, or when applying randomized transformations/augmentations in your data, you must still ensure that the seed is set appropriately.\n\n\n.. code-block:: python\n\n    import lightning as L\n\n    trainer = L.Trainer(...)\n\n    # Define dataset/dataloader\n    # If there is randomness/augmentation in the dataset, fix the seed\n    dataset = MyDataset(seed=42)\n    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n    # PyTorch Lightning configures the sampler automatically for you such that\n    # all batches in a tensor-parallel group are identical,\n    # while still sharding the dataset across the data-parallel group\n    trainer.fit(model, dataloader)\n\n    for i, batch in enumerate(dataloader):\n        ...\n\n\n----\n\n\n**********\nNext steps\n**********\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: LLM 2D Parallel Example\n    :description: Full example how to combine TP + FSDP in a large language model (Llama 3)\n    :col_css: col-md-4\n    :button_link: https://github.com/Lightning-AI/pytorch-lightning/tree/master/examples/pytorch/tensor_parallel\n    :height: 160\n    :tag: advanced\n\n.. displayitem::\n    :header: Pipeline Parallelism\n    :description: Coming soon\n    :col_css: col-md-4\n    :height: 160\n    :tag: advanced\n\n\n.. raw:: html\n\n        </div>\n    </div>\n\n|\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/post_training_quantization.html", "url_rel_html": "advanced/post_training_quantization.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/post_training_quantization.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Post-training Quantization¶", "rst_text": ":orphan:\n\n.. _post_training_quantization:\n\n##########################\nPost-training Quantization\n##########################\n\nMost deep learning applications are using 32-bits of floating-point precision for inference. But low precision data types, especially INT8, are attracting more attention due to significant performance margin. One of the essential concerns of adopting low precision is how to easily mitigate the possible accuracy loss and reach predefined accuracy requirements.\n\nIntel® Neural Compressor, is an open-source Python library that runs on Intel CPUs and GPUs, which could address the aforementioned concern by extending the PyTorch Lightning model with accuracy-driven automatic quantization tuning strategies to help users quickly find out the best-quantized model on Intel hardware. It also supports multiple popular network compression technologies such as sparse, pruning, and knowledge distillation.\n\n**Audience** : Machine learning engineers optimizing models for a better model inference speed and lower memory usage.\n\nVisit the Intel® Neural Compressor online document website at: `<https://github.com/intel/neural-compressor>`_.\n\n******************\nModel Quantization\n******************\n\nModel quantization is an efficient model optimization tool that can accelerate the model inference speed and decrease the memory load while still maintaining the model accuracy.\n\nIntel® Neural Compressor provides a convenient model quantization API to quantize the already-trained Lightning module with Post-training Quantization and Quantization Aware Training. This extension API exhibits the merits of an ease-of-use coding environment and multi-functional quantization options. The user can easily quantize their fine-tuned model by adding a few clauses to their original code.  We only introduce post-training quantization in this document.\n\nThere are two post-training quantization types in Intel® Neural Compressor, post-training static quantization and post-training dynamic quantization.  Post-training dynamic quantization is a recommended starting point because it provides reduced memory usage and faster computation without additional calibration datasets. This type of quantization statically quantizes only the weights from floating point to integer at conversion time. This optimization provides latencies close to post-training static quantization. But the outputs of ops are still stored with the floating point, so the increased speed of dynamic-quantized ops is less than a static-quantized computation.\n\nPost-training static quantization saves the output of ops via INT8 bit. It can tackle the accuracy and latency loss caused by \"quant\" and \"dequant\" operations. For Post-training static quantization, the user needs to estimate the min-max range of all FP32 tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations and model output cannot be calibrated unless the model run a few inference cycles. As a result, the converter requires a calibration dataset to estimate that range. This dataset can be a small subset (default 100 samples) of the training or the validation data.\n\n************\nInstallation\n************\n\nPrerequisites\n=============\n\nPython version: 3.9, 3.10\n\nInstall Intel® Neural Compressor\n================================\n\nRelease binary install:\n\n.. code-block:: bash\n\n    # Install stable basic version from pip\n    pip install neural-compressor\n    # Or install stable full version from pip (including GUI)\n    pip install neural-compressor-full\n\nMore installation methods can be found in the `Installation Guide <https://github.com/intel/neural-compressor/blob/master/docs/source/installation_guide.md>`_.\n\n*****\nUsage\n*****\n\nMinor code changes are required for the user to get started with Intel® Neural Compressor quantization API. To construct the quantization process, users can specify the below settings via the Python code:\n\n1. Calibration Dataloader (Needed for post-training static quantization)\n2. Evaluation Dataloader and Metric\n\nThe code changes that are required for Intel® Neural Compressor are highlighted with comments in the line above.\n\nPyTorch Lightning model\n=======================\n\nLoad the pretrained model with PyTorch Lightning:\n\n.. code-block:: python\n\n    import torch\n    from lightning.pytorch import LightningModule\n    from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n\n\n    # BERT Model definition\n    class GLUETransformer(LightningModule):\n        def __init__(self):\n            self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n            self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n\n        def forward(self, **inputs):\n            return self.model(**inputs)\n\n\n    model = GLUETransformer(model_name_or_path=\"Intel/bert-base-uncased-mrpc\")\n\nThe fine-tuned model from Intel could be downloaded from `Intel Hugging Face repository <https://huggingface.co/Intel>`_.\n\nAccuracy-driven quantization config\n===================================\n\nIntel® Neural Compressor supports accuracy-driven automatic tuning to generate the optimal INT8 model which meets a predefined accuracy goal. The default tolerance of accuracy loss in the accuracy criterion is 0.01. And the maximum trial number of quantization is 600. The user can specifically define their own criteria by:\n\n.. code-block:: python\n\n    from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n\n    accuracy_criterion = AccuracyCriterion(tolerable_loss=0.01)\n    tuning_criterion = TuningCriterion(max_trials=600)\n    conf = PostTrainingQuantConfig(\n        approach=\"static\", backend=\"default\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion\n    )\n\nThe \"approach\" parameter in PostTrainingQuantConfig is defined by the user to make a choice from post-training static quantization and post-training dynamic by \"static\" or \"dynamic\".\n\nQuantize the model\n==================\n\nThe model can be quantized by Intel® Neural Compressor with:\n\n.. code-block:: python\n\n    from neural_compressor.quantization import fit\n\n    q_model = fit(model=model.model, conf=conf, calib_dataloader=val_dataloader(), eval_func=eval_func)\n\nUsers can define the evaluation function \"eval_func\" by themselves.\n\nAt last, the quantized model can be saved by:\n\n.. code-block:: python\n\n    q_model.save(\"./saved_model/\")\n\n*****************\nHands-on Examples\n*****************\n\nBased on the `given example code <https://lightning.ai/docs/pytorch/2.1.0/notebooks/lightning_examples/text-transformers.html>`_, we show how Intel Neural Compressor conducts model quantization on PyTorch Lightning. We first define the basic config of the quantization process.\n\n.. code-block:: python\n\n    from neural_compressor.quantization import fit as fit\n    from neural_compressor.config import PostTrainingQuantConfig\n\n\n    def eval_func_for_nc(model_n, trainer_n):\n        setattr(model, \"model\", model_n)\n        result = trainer_n.validate(model=model, dataloaders=dm.val_dataloader())\n        return result[0][\"accuracy\"]\n\n\n    def eval_func(model):\n        return eval_func_for_nc(model, trainer)\n\n\n    conf = PostTrainingQuantConfig()\n    q_model = fit(model=model.model, conf=conf, calib_dataloader=dm.val_dataloader(), eval_func=eval_func)\n\n    q_model.save(\"./saved_model/\")\n\nWe define the evaluation function as:\n\n.. code-block:: python\n\n    def eval_func_for_nc(model_n, trainer_n):\n        setattr(model, \"model\", model_n)\n        result = trainer_n.validate(model=model, dataloaders=dm.val_dataloader())\n        return result[0][\"accuracy\"]\n\n\n    def eval_func(model):\n        return eval_func_for_nc(model, trainer)\n\nFollowing is the performance comparison between FP32 model and INT8 model:\n\n\n+-------------+-----------------+------------------+\n| Info Type   |  Baseline FP32  |  Quantized INT8  |\n+=============+=================+==================+\n| Accuracy    | 0.8603          | 0.8578           |\n+-------------+-----------------+------------------+\n| Duration(s) | 5.8973          | 3.5952           |\n+-------------+-----------------+------------------+\n| Memory(MB)  | 417.73          | 113.28           |\n+-------------+-----------------+------------------+\n\n\nFor more model quantization performance, please refer to `our model list <https://github.com/intel/neural-compressor/blob/master/docs/source/validated_model_list.md>`_\n\n*****************\nTechnical Support\n*****************\n\nWelcome to visit Intel® Neural Compressor website at: https://github.com/intel/neural-compressor to find technical support or contribute your code.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/pretrained.html", "url_rel_html": "advanced/pretrained.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/pretrained.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Transfer Learning¶", "rst_text": ".. include:: transfer_learning.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/pruning_quantization.html", "url_rel_html": "advanced/pruning_quantization.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/pruning_quantization.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Pruning and Quantization¶", "rst_text": ".. _pruning_quantization:\n\n########################\nPruning and Quantization\n########################\n\nPruning and Quantization are techniques to compress model size for deployment, allowing inference speed up and energy saving without significant accuracy losses.\n\n*******\nPruning\n*******\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\nPruning is a technique which focuses on eliminating some of the model weights to reduce the model size and decrease inference requirements.\n\nPruning has been shown to achieve significant efficiency improvements while minimizing the drop in model performance (prediction quality). Model pruning is recommended for cloud endpoints, deploying models on edge devices, or mobile inference (among others).\n\nTo enable pruning during training in Lightning, simply pass in the :class:`~lightning.pytorch.callbacks.ModelPruning` callback to the Lightning Trainer. PyTorch's native pruning implementation is used under the hood.\n\nThis callback supports multiple pruning functions: pass any `torch.nn.utils.prune <https://pytorch.org/docs/stable/nn.html#utilities>`_ function as a string to select which weights to prune (`random_unstructured <https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured>`_, `RandomStructured <https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured>`_, etc) or implement your own by subclassing `BasePruningMethod <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#extending-torch-nn-utils-prune-with-custom-pruning-functions>`_.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import ModelPruning\n\n    # set the amount to be the fraction of parameters to prune\n    trainer = Trainer(callbacks=[ModelPruning(\"l1_unstructured\", amount=0.5)])\n\nYou can also perform iterative pruning, apply the `lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`__, and more!\n\n.. code-block:: python\n\n    def compute_amount(epoch):\n        # the sum of all returned values needs to be smaller than 1\n        if epoch == 10:\n            return 0.5\n\n        elif epoch == 50:\n            return 0.25\n\n        elif 75 < epoch < 99:\n            return 0.01\n\n\n    # the amount can also be a callable\n    trainer = Trainer(callbacks=[ModelPruning(\"l1_unstructured\", amount=compute_amount)])\n\n\n\nPost-training Quantization\n==========================\n\nIf you want to quantize a fine-tuned model with PTQ, it is recommended to adopt a third party API names Intel® Neural Compressor, read more :doc:`here <./post_training_quantization>`, which provides a convenient tool for accelerating the model inference speed on Intel CPUs and GPUs.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/strategy_registry.html", "url_rel_html": "advanced/strategy_registry.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/strategy_registry.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Strategy Registry¶", "rst_text": "Strategy Registry\n=================\n\nLightning includes a registry that holds information about Training strategies and allows for the registration of new custom strategies.\n\nThe Strategies are assigned strings that identify them, such as \"ddp\", \"deepspeed_stage_2_offload\", and so on.\nIt also returns the optional description and parameters for initialising the Strategy that were defined during registration.\n\n\n.. code-block:: python\n\n    # Training with the DDP Strategy\n    trainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n    # Training with DeepSpeed ZeRO Stage 3 and CPU Offload\n    trainer = Trainer(strategy=\"deepspeed_stage_3_offload\", accelerator=\"gpu\", devices=3)\n\n    # Training with the TPU Spawn Strategy with `debug` as True\n    trainer = Trainer(strategy=\"xla_debug\", accelerator=\"tpu\", devices=8)\n\n\nAdditionally, you can pass your custom registered training strategies to the ``strategy`` argument.\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import DDPStrategy, StrategyRegistry, CheckpointIO\n\n\n    class CustomCheckpointIO(CheckpointIO):\n        def save_checkpoint(self, checkpoint: Dict[str, Any], path: Union[str, Path]) -> None:\n            ...\n\n        def load_checkpoint(self, path: Union[str, Path]) -> Dict[str, Any]:\n            ...\n\n\n    custom_checkpoint_io = CustomCheckpointIO()\n\n    # Register the DDP Strategy with your custom CheckpointIO plugin\n    StrategyRegistry.register(\n        \"ddp_custom_checkpoint_io\",\n        DDPStrategy,\n        description=\"DDP Strategy with custom checkpoint io plugin\",\n        checkpoint_io=custom_checkpoint_io,\n    )\n\n    trainer = Trainer(strategy=\"ddp_custom_checkpoint_io\", accelerator=\"gpu\", devices=2)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/tp.html", "url_rel_html": "advanced/tp.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/tp.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "", "rst_text": ""}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html", "url_rel_html": "advanced/training_tricks.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/training_tricks.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Effective Training Techniques¶", "rst_text": ".. testsetup:: *\n\n    from lightning.pytorch.callbacks import StochasticWeightAveraging\n\n.. _training_tricks:\n\n#############################\nEffective Training Techniques\n#############################\n\nLightning implements various techniques to help during training that can help make the training smoother.\n\n----------\n\n********************\nAccumulate Gradients\n********************\n\n.. include:: ../common/gradient_accumulation.rst\n\n----------\n\n*****************\nGradient Clipping\n*****************\n\nGradient clipping can be enabled to avoid exploding gradients. By default, this will clip the gradient norm by calling\n:func:`torch.nn.utils.clip_grad_norm_` computed over all model parameters together.\nIf the Trainer's ``gradient_clip_algorithm`` is set to ``'value'`` (``'norm'`` by default), this will use instead\n:func:`torch.nn.utils.clip_grad_value_` for each parameter instead.\n\n.. note::\n    If using mixed precision, the ``gradient_clip_val`` does not need to be changed as the gradients are unscaled\n    before applying the clipping function.\n\n.. seealso:: :class:`~lightning.pytorch.trainer.trainer.Trainer`\n\n.. testcode::\n\n    # DEFAULT (ie: don't clip)\n    trainer = Trainer(gradient_clip_val=0)\n\n    # clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\n    trainer = Trainer(gradient_clip_val=0.5)\n\n    # clip gradients' maximum magnitude to <=0.5\n    trainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n\nRead more about :ref:`Configuring Gradient Clipping <configure_gradient_clipping>` for advanced use cases.\n\n----------\n\n****************\nWeight Averaging\n****************\n\nWeight averaging methods such as Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA) can make your\nmodels generalize better at virtually no additional cost. Averaging smooths the loss landscape thus making it harder to\nend up in a local minimum during optimization.\n\nLightning provides two callbacks to facilitate weight averaging. :class:`~lightning.pytorch.callbacks.WeightAveraging`\nis a generic callback that wraps the\n`AveragedModel <https://pytorch.org/docs/stable/generated/torch.optim.swa_utils.AveragedModel.html>`__ class from\nPyTorch. It allows SWA, EMA, or a custom averaging strategy to be used. By default, it updates the weights after every\nstep, but it can be customized to update at specific steps or epochs by overriding the `should_update()` method.\n\nThe older :class:`~lightning.pytorch.callbacks.StochasticWeightAveraging` callback is specific to SWA. It starts the SWA\nprocedure after a certain number of epochs and always runs on every epoch. Additionally, it switches to a constant\nlearning rate schedule (`SWALR <https://pytorch.org/docs/stable/generated/torch.optim.swa_utils.SWALR.html>`__) when the\nprocedure starts.\n\n.. seealso::\n    For a more detailed explanation of SWA and how it works, read\n    `this post <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`__ by the PyTorch team.\n\n.. seealso::\n    The :class:`~lightning.pytorch.callbacks.WeightAveraging` callback and\n    :class:`~lightning.pytorch.callbacks.StochasticWeightAveraging` callback\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import StochasticWeightAveraging, WeightAveraging\n    from torch.optim.swa_utils import get_ema_avg_fn\n\n    # Enable Exponential Moving Average after 100 steps\n    class EMAWeightAveraging(WeightAveraging):\n        def __init__(self):\n            super().__init__(avg_fn=get_ema_avg_fn())\n        def should_update(self, step_idx=None, epoch_idx=None):\n            return (step_idx is not None) and (step_idx >= 100)\n    trainer = Trainer(callbacks=EMAWeightAveraging())\n\n    # Enable Stochastic Weight Averaging after 10 epochs with learning rate 0.01\n    trainer = Trainer(callbacks=StochasticWeightAveraging(swa_epoch_start=10, swa_lrs=0.01))\n\n----------\n\n.. _batch_size_finder:\n\n*****************\nBatch Size Finder\n*****************\n\nAuto-scaling of batch size can be enabled to find the largest batch size that fits into\nmemory. Large batch size often yields a better estimation of the gradients, but may also result in\nlonger training time. Inspired by https://github.com/BlackHC/toma.\n\n.. seealso:: :class:`~lightning.pytorch.tuner.tuning.Tuner`\n\n.. code-block:: python\n\n    from lightning.pytorch.tuner import Tuner\n\n    # Create a tuner for the trainer\n    trainer = Trainer(...)\n    tuner = Tuner(trainer)\n\n    # Auto-scale batch size by growing it exponentially (default)\n    tuner.scale_batch_size(model, mode=\"power\")\n\n    # Auto-scale batch size with binary search\n    tuner.scale_batch_size(model, mode=\"binsearch\")\n\n    # Fit as normal with new batch size\n    trainer.fit(model)\n\n\nCurrently, this feature supports two modes ``'power'`` scaling and ``'binsearch'``\nscaling. In ``'power'`` scaling, starting from a batch size of 1 keeps doubling\nthe batch size until an out-of-memory (OOM) error is encountered. Setting the\nargument to ``'binsearch'`` will initially also try doubling the batch size until\nit encounters an OOM, after which it will do a binary search that will finetune the\nbatch size. Additionally, it should be noted that the batch size scaler cannot\nsearch for batch sizes larger than the size of the training dataset.\n\n.. note::\n\n    This feature expects that a ``batch_size`` field is either located as a model attribute\n    i.e. ``model.batch_size`` or as a field in your ``hparams`` i.e. ``model.hparams.batch_size``.\n    Similarly it can work with datamodules too. The field should exist and will be updated by\n    the results of this algorithm. Additionally, your ``train_dataloader()`` method should depend\n    on this field for this feature to work i.e.\n\n    .. code-block:: python\n\n        # using LightningModule\n        class LitModel(LightningModule):\n            def __init__(self, batch_size):\n                super().__init__()\n                self.save_hyperparameters()\n                # or\n                self.batch_size = batch_size\n\n            def train_dataloader(self):\n                return DataLoader(train_dataset, batch_size=self.batch_size | self.hparams.batch_size)\n\n\n        model = LitModel(batch_size=32)\n        trainer = Trainer(...)\n        tuner = Tuner(trainer)\n        tuner.scale_batch_size(model)\n\n\n        # using LightningDataModule\n        class LitDataModule(LightningDataModule):\n            def __init__(self, batch_size):\n                super().__init__()\n                self.save_hyperparameters()\n                # or\n                self.batch_size = batch_size\n\n            def train_dataloader(self):\n                return DataLoader(train_dataset, batch_size=self.batch_size | self.hparams.batch_size)\n\n\n        model = MyModel()\n        datamodule = LitDataModule(batch_size=32)\n\n        trainer = Trainer(...)\n        tuner = Tuner(trainer)\n        tuner.scale_batch_size(model, datamodule=datamodule)\n\n    Note that the ``train_dataloader`` can be either part of\n    the ``LightningModule`` or ``LightningDataModule``\n    as shown above. If both the ``LightningModule``\n    and the ``LightningDataModule`` contain a ``train_dataloader``,\n    the ``LightningDataModule`` takes precedence.\n\nThe algorithm in short works by:\n    1. Dumping the current state of the model and trainer\n    2. Iteratively until convergence or maximum number of tries ``max_trials`` (default 25) has been reached:\n        - Call ``fit()`` method of trainer. This evaluates ``steps_per_trial`` (default 3) number of\n          optimization steps. Each training step can trigger an OOM error if the tensors\n          (training batch, weights, gradients, etc.) allocated during the steps have a\n          too large memory footprint.\n        - If an OOM error is encountered, decrease batch size else increase it.\n          How much the batch size is increased/decreased is determined by the chosen\n          strategy.\n    3. The found batch size is saved to either ``model.batch_size`` or ``model.hparams.batch_size``\n    4. Restore the initial state of model and trainer\n\n.. warning:: Batch size finder is not yet supported for DDP or any of its variations, it is coming soon.\n\n\nCustomizing Batch Size Finder\n=============================\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\n1. You can also customize the :class:`~lightning.pytorch.callbacks.batch_size_finder.BatchSizeFinder` callback to run\n   at different epochs. This feature is useful while fine-tuning models since you can't always use the same batch size after\n   unfreezing the backbone.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import BatchSizeFinder\n\n\n    class FineTuneBatchSizeFinder(BatchSizeFinder):\n        def __init__(self, milestones, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.milestones = milestones\n\n        def on_fit_start(self, *args, **kwargs):\n            return\n\n        def on_train_epoch_start(self, trainer, pl_module):\n            if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n                self.scale_batch_size(trainer, pl_module)\n\n\n    trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])\n    trainer.fit(...)\n\n\n2. Run batch size finder for ``validate``/``test``/``predict``.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import BatchSizeFinder\n\n\n    class EvalBatchSizeFinder(BatchSizeFinder):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def on_fit_start(self, *args, **kwargs):\n            return\n\n        def on_test_start(self, trainer, pl_module):\n            self.scale_batch_size(trainer, pl_module)\n\n\n    trainer = Trainer(callbacks=[EvalBatchSizeFinder()])\n    trainer.test(...)\n\n\n----------\n\n.. _learning_rate_finder:\n\n********************\nLearning Rate Finder\n********************\n\nFor training deep neural networks, selecting a good learning rate is essential\nfor both better performance and faster convergence. Even optimizers such as\n:class:`~torch.optim.Adam` that are self-adjusting the learning rate can benefit from more optimal\nchoices.\n\nTo reduce the amount of guesswork concerning choosing a good initial learning\nrate, a `learning rate finder` can be used. As described in `this paper <https://arxiv.org/abs/1506.01186>`_\na learning rate finder does a small run where the learning rate is increased\nafter each processed batch and the corresponding loss is logged. The result of\nthis is a ``lr`` vs. ``loss`` plot that can be used as guidance for choosing an optimal\ninitial learning rate.\n\n.. warning::\n\n    For the moment, this feature only works with models having a single optimizer.\n\n\n.. note::\n\n    With DDP: Since all the processes run in isolation, only process with ``global_rank=0`` will make the decision to stop the\n    learning rate finder and broadcast its results to all other ranks. That means, at the end of LR finder, each process will be running with\n    the learning rate found on ``global_rank=0``.\n\n\nUsing Lightning's built-in LR finder\n====================================\n\nTo enable the learning rate finder, your :doc:`lightning module <../common/lightning_module>` needs to\nhave a ``learning_rate`` or ``lr`` attribute (or as a field in your ``hparams`` i.e.\n``hparams.learning_rate`` or ``hparams.lr``). Then, create the :class:`~lightning.pytorch.tuner.tuning.Tuner` via ``tuner = Tuner(trainer)``\nand call ``tuner.lr_find(model)`` to run the LR finder.\nThe suggested ``learning_rate`` will be written to the console and will be automatically\nset to your :doc:`lightning module <../common/lightning_module>`, which can be accessed\nvia ``self.learning_rate`` or ``self.lr``.\n\n.. code-block:: python\n\n    from lightning.pytorch.tuner import Tuner\n\n\n    class LitModel(LightningModule):\n        def __init__(self, learning_rate):\n            super().__init__()\n            self.learning_rate = learning_rate\n            self.model = Model(...)\n\n        def configure_optimizers(self):\n            return Adam(self.parameters(), lr=(self.lr or self.learning_rate))\n\n\n    model = LitModel()\n    trainer = Trainer(...)\n\n    # Create a Tuner\n    tuner = Tuner(trainer)\n\n    # finds learning rate automatically\n    # sets hparams.lr or hparams.learning_rate to that learning rate\n    tuner.lr_find(model)\n\n\nIf your model is using an arbitrary value instead of ``self.lr`` or ``self.learning_rate``, set that value in ``lr_find``:\n\n.. code-block:: python\n\n    model = LitModel()\n    trainer = Trainer(...)\n    tuner = Tuner(trainer)\n\n    # to set to your own hparams.my_value\n    tuner.lr_find(model, attr_name=\"my_value\")\n\n\nYou can also inspect the results of the learning rate finder or just play around\nwith the parameters of the algorithm. A typical example of this would look like:\n\n.. code-block:: python\n\n    model = MyModelClass(hparams)\n    trainer = Trainer()\n    tuner = Tuner(trainer)\n\n    # Run learning rate finder\n    lr_finder = tuner.lr_find(model)\n\n    # Results can be found in\n    print(lr_finder.results)\n\n    # Plot with\n    fig = lr_finder.plot(suggest=True)\n    fig.show()\n\n    # Pick point based on plot, or get suggestion\n    new_lr = lr_finder.suggestion()\n\n    # update hparams of the model\n    model.hparams.lr = new_lr\n\n    # Fit model\n    trainer.fit(model)\n\nThe figure produced by ``lr_finder.plot()`` should look something like the figure\nbelow. It is recommended to not pick the learning rate that achieves the lowest\nloss, but instead something in the middle of the sharpest downward slope (red point).\nThis is the point returned py ``lr_finder.suggestion()``.\n\n\nCustomizing Learning Rate Finder\n================================\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\nYou can also customize the :class:`~lightning.pytorch.callbacks.lr_finder.LearningRateFinder` callback to run at different epochs. This feature is useful while fine-tuning models.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import LearningRateFinder\n\n\n    class FineTuneLearningRateFinder(LearningRateFinder):\n        def __init__(self, milestones, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.milestones = milestones\n\n        def on_fit_start(self, *args, **kwargs):\n            return\n\n        def on_train_epoch_start(self, trainer, pl_module):\n            if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n                self.lr_find(trainer, pl_module)\n\n\n    trainer = Trainer(callbacks=[FineTuneLearningRateFinder(milestones=(5, 10))])\n    trainer.fit(...)\n\n\n.. figure:: ../_static/images/trainer/lr_finder.png\n\n----------\n\n**************************\nAdvanced GPU Optimizations\n**************************\n\nWhen training on single or multiple GPU machines, Lightning offers a host of advanced optimizations to improve throughput, memory efficiency, and model scaling.\nRefer to :doc:`Advanced GPU Optimized Training <../advanced/model_parallel>` for more details.\n\n----------\n\n\n.. _ddp_spawn_shared_memory:\n\n******************************************\nSharing Datasets Across Process Boundaries\n******************************************\n\nThe :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class provides an organized way to decouple data loading from training logic, with :meth:`~lightning.pytorch.core.hooks.DataHooks.prepare_data` being used for downloading and pre-processing the dataset on a single process, and :meth:`~lightning.pytorch.core.hooks.DataHooks.setup` loading the pre-processed data for each process individually:\n\n.. code-block:: python\n\n    class MNISTDataModule(L.LightningDataModule):\n        def prepare_data(self):\n            MNIST(self.data_dir, download=True)\n\n        def setup(self, stage: str):\n            self.mnist = MNIST(self.data_dir)\n\n        def train_loader(self):\n            return DataLoader(self.mnist, batch_size=128)\n\nHowever, for in-memory datasets, that means that each process will hold a (redundant) replica of the dataset in memory, which may be impractical when using many processes while utilizing datasets that nearly fit into CPU memory, as the memory consumption will scale up linearly with the number of processes.\nFor example, when training Graph Neural Networks, a common strategy is to load the entire graph into CPU memory for fast access to the entire graph structure and its features, and to then perform neighbor sampling to obtain mini-batches that fit onto the GPU.\n\nA simple way to prevent redundant dataset replicas is to rely on :obj:`torch.multiprocessing` to share the `data automatically between spawned processes via shared memory <https://pytorch.org/docs/stable/notes/multiprocessing.html>`_.\nFor this, all data pre-loading should be done on the main process inside :meth:`DataModule.__init__`. As a result, all tensor-data will get automatically shared when using the ``'ddp_spawn'`` strategy.\n\n.. warning::\n\n    :obj:`torch.multiprocessing` will send a handle of each individual tensor to other processes.\n    In order to prevent any errors due to too many open file handles, try to reduce the number of tensors to share, *e.g.*, by stacking your data into a single tensor.\n\n.. code-block:: python\n\n    class MNISTDataModule(L.LightningDataModule):\n        def __init__(self, data_dir: str):\n            self.mnist = MNIST(data_dir, download=True, transform=T.ToTensor())\n\n        def train_loader(self):\n            return DataLoader(self.mnist, batch_size=128)\n\n\n    model = Model(...)\n    datamodule = MNISTDataModule(\"data/MNIST\")\n\n    trainer = Trainer(accelerator=\"gpu\", devices=2, strategy=\"ddp_spawn\")\n    trainer.fit(model, datamodule)\n\nSee the `graph-level <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pytorch_lightning/gin.py>`_ and `node-level <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pytorch_lightning/graph_sage.py>`_ prediction examples in PyTorch Geometric for practical use-cases.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/transfer_learning.html", "url_rel_html": "advanced/transfer_learning.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/transfer_learning.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Transfer Learning¶", "rst_text": "#################\nTransfer Learning\n#################\n**Audience**: Users looking to use pretrained models with Lightning.\n\n----\n\n*************************\nUse any PyTorch nn.Module\n*************************\nAny model that is a PyTorch nn.Module can be used with Lightning (because LightningModules are nn.Modules also).\n\n----\n\n********************************\nUse a pretrained LightningModule\n********************************\nLet's use the `AutoEncoder` as a feature extractor in a separate model.\n\n.. testcode::\n\n    class Encoder(torch.nn.Module):\n        ...\n\n\n    class AutoEncoder(LightningModule):\n        def __init__(self):\n            self.encoder = Encoder()\n            self.decoder = Decoder()\n\n\n    class CIFAR10Classifier(LightningModule):\n        def __init__(self):\n            # init the pretrained LightningModule\n            self.feature_extractor = AutoEncoder.load_from_checkpoint(PATH).encoder\n            self.feature_extractor.freeze()\n\n            # the autoencoder outputs a 100-dim representation and CIFAR-10 has 10 classes\n            self.classifier = nn.Linear(100, 10)\n\n        def forward(self, x):\n            representations = self.feature_extractor(x)\n            x = self.classifier(representations)\n            ...\n\nWe used our pretrained Autoencoder (a LightningModule) for transfer learning!\n\n----\n\n***********************************\nExample: Imagenet (Computer Vision)\n***********************************\n\n.. testcode::\n    :skipif: not _TORCHVISION_AVAILABLE\n\n    import torchvision.models as models\n\n\n    class ImagenetTransferLearning(LightningModule):\n        def __init__(self):\n            super().__init__()\n\n            # init a pretrained resnet\n            backbone = models.resnet50(weights=\"DEFAULT\")\n            num_filters = backbone.fc.in_features\n            layers = list(backbone.children())[:-1]\n            self.feature_extractor = nn.Sequential(*layers)\n            self.feature_extractor.eval()\n\n            # use the pretrained model to classify cifar-10 (10 image classes)\n            num_target_classes = 10\n            self.classifier = nn.Linear(num_filters, num_target_classes)\n\n        def forward(self, x):\n            with torch.no_grad():\n                representations = self.feature_extractor(x).flatten(1)\n            x = self.classifier(representations)\n            ...\n\nFinetune\n\n.. code-block:: python\n\n    model = ImagenetTransferLearning()\n    trainer = Trainer()\n    trainer.fit(model)\n\nAnd use it to predict your data of interest\n\n.. code-block:: python\n\n    model = ImagenetTransferLearning.load_from_checkpoint(PATH)\n    model.freeze()\n\n    x = some_images_from_cifar10()\n    predictions = model(x)\n\nWe used a pretrained model on imagenet, finetuned on CIFAR-10 to predict on CIFAR-10.\nIn the non-academic world we would finetune on a tiny dataset you have and predict on your dataset.\n\n----\n\n*******************\nExample: BERT (NLP)\n*******************\nLightning is completely agnostic to what's used for transfer learning so long\nas it is a `torch.nn.Module` subclass.\n\nHere's a model that uses `Huggingface transformers <https://github.com/huggingface/transformers>`_.\n\n.. testcode::\n\n    class BertMNLIFinetuner(LightningModule):\n        def __init__(self):\n            super().__init__()\n\n            self.bert = BertModel.from_pretrained(\"bert-base-cased\", output_attentions=True)\n            self.bert.train()\n            self.W = nn.Linear(bert.config.hidden_size, 3)\n            self.num_classes = 3\n\n        def forward(self, input_ids, attention_mask, token_type_ids):\n            h, _, attn = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n            h_cls = h[:, 0]\n            logits = self.W(h_cls)\n            return logits, attn\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/advanced/warnings.html", "url_rel_html": "advanced/warnings.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/advanced/warnings.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Warnings¶", "rst_text": "########\nWarnings\n########\n\nLightning warns users of possible misconfiguration, performance implications or potential mistakes through the ``PossibleUserWarning`` category.\nSometimes these warnings can be false positives, and you may want to suppress them to avoid cluttering the logs.\n\n\n.. warning::\n\n    Suppressing warnings is not recommended in general, because they may raise important issues that you should address.\n    Only suppress warnings if they are false.\n\n\n-----\n\n\n*********************************\nSuppress a single warning message\n*********************************\n\nSuppressing an individual warning message can be done through the :mod:`warnings` module:\n\n.. code-block:: python\n\n    import warnings\n\n    warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n\n\n-----\n\n\n*********************************************\nSuppress all instances of PossibleUserWarning\n*********************************************\n\nSuppressing all warnings of the ``PossibleUserWarning`` category can be done programmatically\n\n.. code-block:: python\n\n    from lightning.pytorch.utilities import disable_possible_user_warnings\n\n    # ignore all warnings that could be false positives\n    disable_possible_user_warnings()\n\nor through the environment variable ``POSSIBLE_USER_WARNINGS``:\n\n\n.. code-block:: bash\n\n    export POSSIBLE_USER_WARNINGS=off\n    # or\n    export POSSIBLE_USER_WARNINGS=0\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli.html", "url_rel_html": "cli/lightning_cli.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI¶", "rst_text": ":orphan:\n\n.. _lightning-cli:\n\n######################################\nConfigure hyperparameters from the CLI\n######################################\n\n*************\nWhy use a CLI\n*************\n\nWhen running deep learning experiments, there are a couple of good practices that are recommended to follow:\n\n- Separate configuration from source code\n- Guarantee reproducibility of experiments\n\nImplementing a command line interface (CLI) makes it possible to execute an experiment from a shell terminal. By having\na CLI, there is a clear separation between the Python source code and what hyperparameters are used for a particular\nexperiment. If the CLI corresponds to a stable version of the code, reproducing an experiment can be achieved by\ninstalling the same version of the code plus dependencies and running with the same configuration (CLI arguments).\n\n----\n\n*********\nBasic use\n*********\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: 1: Control it all from the CLI\n   :description: Learn to control a LightningModule and LightningDataModule from the CLI\n   :col_css: col-md-4\n   :button_link: lightning_cli_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: 2: Mix models, datasets and optimizers\n   :description: Support multiple models, datasets, optimizers and learning rate schedulers\n   :col_css: col-md-4\n   :button_link: lightning_cli_intermediate_2.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: 3: Control it all via YAML\n   :description: Enable composable YAMLs\n   :col_css: col-md-4\n   :button_link: lightning_cli_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n\n----\n\n************\nAdvanced use\n************\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: YAML for production\n   :description: Use the Lightning CLI with YAMLs for production environments\n   :col_css: col-md-4\n   :button_link: lightning_cli_advanced_2.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Customize for complex projects\n   :description: Learn how to implement CLIs for complex projects\n   :col_css: col-md-4\n   :button_link: lightning_cli_advanced_3.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Extend the Lightning CLI\n   :description: Customize the Lightning CLI\n   :col_css: col-md-4\n   :button_link: lightning_cli_expert.html\n   :height: 150\n   :tag: expert\n\n----\n\n*************\nMiscellaneous\n*************\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: FAQ\n   :description: Frequently asked questions about working with the Lightning CLI and YAML files\n   :col_css: col-md-6\n   :button_link: lightning_cli_faq.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced.html", "url_rel_html": "cli/lightning_cli_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Advanced)¶", "rst_text": ":orphan:\n\n#################################################\nConfigure hyperparameters from the CLI (Advanced)\n#################################################\n**Audience:** Users looking to modularize their code for a professional project.\n\n**Pre-reqs:** You must have read :doc:`(Mix models and datasets) <lightning_cli_intermediate_2>`.\n\nAs a project becomes more complex, the number of configurable options becomes very large, making it inconvenient to\ncontrol through individual command line arguments. To address this, CLIs implemented using\n:class:`~lightning.pytorch.cli.LightningCLI` always support receiving input from configuration files. The default format\nused for config files is YAML.\n\n.. tip::\n\n    If you are unfamiliar with YAML, it is recommended that you first read :ref:`what-is-a-yaml-config-file`.\n\n\n----\n\n***********************\nRun using a config file\n***********************\nTo run the CLI using a yaml config, do:\n\n.. code:: bash\n\n    python main.py fit --config config.yaml\n\nIndividual arguments can be given to override options in the config file:\n\n.. code:: bash\n\n    python main.py fit --config config.yaml --trainer.max_epochs 100\n\n----\n\n************************\nAutomatic save of config\n************************\n\nTo ease experiment reporting and reproducibility, by default ``LightningCLI`` automatically saves the full YAML\nconfiguration in the log directory. After multiple fit runs with different hyperparameters, each one will have in its\nrespective log directory a ``config.yaml`` file. These files can be used to trivially reproduce an experiment, e.g.:\n\n.. code:: bash\n\n    python main.py fit --config lightning_logs/version_7/config.yaml\n\nThe automatic saving of the config is done by the special callback :class:`~lightning.pytorch.cli.SaveConfigCallback`.\nThis callback is automatically added to the ``Trainer``. To disable the save of the config, instantiate ``LightningCLI``\nwith ``save_config_callback=None``.\n\n.. tip::\n\n    To change the file name of the saved configs to e.g. ``name.yaml``, do:\n\n    .. code:: python\n\n        cli = LightningCLI(..., save_config_kwargs={\"config_filename\": \"name.yaml\"})\n\nIt is also possible to extend the :class:`~lightning.pytorch.cli.SaveConfigCallback` class, for instance to additionally\nsave the config in a logger. An example of this is:\n\n    .. code:: python\n\n        class LoggerSaveConfigCallback(SaveConfigCallback):\n            def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\n                if isinstance(trainer.logger, Logger):\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\n                    trainer.logger.log_hyperparams({\"config\": config})\n\n\n        cli = LightningCLI(..., save_config_callback=LoggerSaveConfigCallback)\n\n.. tip::\n\n    If you want to disable the standard behavior of saving the config to the ``log_dir``, then you can either implement\n    ``__init__`` and call ``super().__init__(*args, save_to_log_dir=False, **kwargs)`` or instantiate the\n    ``LightningCLI`` as:\n\n    .. code:: python\n\n        cli = LightningCLI(..., save_config_kwargs={\"save_to_log_dir\": False})\n\n.. note::\n\n    The ``save_config`` method is only called on rank zero. This allows to implement a custom save config without having\n    to worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the process\n    hang waiting for a broadcast. If you need to make collective calls, implement the ``setup`` method instead.\n\n\n----\n\n*********************************\nPrepare a config file for the CLI\n*********************************\nThe ``--help`` option of the CLIs can be used to learn which configuration options are available and how to use them.\nHowever, writing a config from scratch can be time-consuming and error-prone. To alleviate this, the CLIs have the\n``--print_config`` argument, which prints to stdout the configuration without running the command.\n\nFor a CLI implemented as ``LightningCLI(DemoModel, BoringDataModule)``, executing:\n\n.. code:: bash\n\n    python main.py fit --print_config\n\ngenerates a config with all default values like the following:\n\n.. code:: bash\n\n    seed_everything: null\n    trainer:\n      logger: true\n      ...\n    model:\n      out_dim: 10\n      learning_rate: 0.02\n    data:\n      data_dir: ./\n    ckpt_path: null\n\nOther command line arguments can be given and considered in the printed configuration. A use case for this is CLIs that\naccept multiple models. By default, no model is selected, meaning the printed config will not include model settings. To\nget a config with the default values of a particular model would be:\n\n.. code:: bash\n\n    python main.py fit --model DemoModel --print_config\n\nwhich generates a config like:\n\n.. code:: bash\n\n    seed_everything: null\n    trainer:\n      ...\n    model:\n      class_path: lightning.pytorch.demos.boring_classes.DemoModel\n      init_args:\n        out_dim: 10\n        learning_rate: 0.02\n    ckpt_path: null\n\n.. tip::\n\n    A standard procedure to run experiments can be:\n\n    .. code:: bash\n\n        # Print a configuration to have as reference\n        python main.py fit --print_config > config.yaml\n        # Modify the config to your liking - you can remove all default arguments\n        nano config.yaml\n        # Fit your model using the edited configuration\n        python main.py fit --config config.yaml\n\nConfiguration items can be either simple Python objects such as int and str,\nor complex objects comprised of a ``class_path`` and ``init_args`` arguments. The ``class_path`` refers\nto the complete import path of the item class, while ``init_args`` are the arguments to be passed\nto the class constructor. For example, your model is defined as:\n\n.. code:: python\n\n    # model.py\n    class MyModel(L.LightningModule):\n        def __init__(self, criterion: torch.nn.Module):\n            self.criterion = criterion\n\nThen the config would be:\n\n.. code:: yaml\n\n    model:\n      class_path: model.MyModel\n      init_args:\n        criterion:\n          class_path: torch.nn.CrossEntropyLoss\n          init_args:\n            reduction: mean\n        ...\n\n``LightningCLI`` uses `jsonargparse <https://github.com/omni-us/jsonargparse>`_ under the hood for parsing\nconfiguration files and automatic creation of objects, so you don't need to do it yourself.\n\n.. note::\n\n    Lightning automatically registers all subclasses of :class:`~lightning.pytorch.core.LightningModule`,\n    so the complete import path is not required for them and can be replaced by the class name.\n\n.. note::\n\n    Parsers make a best effort to determine the correct names and types that the parser should accept.\n    However, there can be cases not yet supported or cases for which it would be impossible to support.\n    To somewhat overcome these limitations, there is a special key ``dict_kwargs`` that can be used\n    to provide arguments that will not be validated during parsing, but will be used for class instantiation.\n\n    For example, then using the ``lightning.pytorch.profilers.PyTorchProfiler`` profiler,\n    the ``profile_memory`` argument has a type that is determined dynamically. As a result, it's not possible\n    to know the expected type during parsing. To account for this, your config file should be set up like this:\n\n    .. code:: yaml\n\n        trainer:\n          profiler:\n            class_path: lightning.pytorch.profilers.PyTorchProfiler\n            dict_kwargs:\n              profile_memory: true\n\n----\n\n********************\nCompose config files\n********************\nMultiple config files can be provided, and they will be parsed sequentially. Let's say we have two configs with common\nsettings:\n\n.. code:: yaml\n\n    # config_1.yaml\n    trainer:\n      num_epochs: 10\n      ...\n\n    # config_2.yaml\n    trainer:\n      num_epochs: 20\n      ...\n\nThe value from the last config will be used, ``num_epochs = 20`` in this case:\n\n.. code-block:: bash\n\n    $ python main.py fit --config config_1.yaml --config config_2.yaml\n\n----\n\n*********************\nUse groups of options\n*********************\nGroups of options can also be given as independent config files. For configs like:\n\n.. code:: yaml\n\n    # trainer.yaml\n    num_epochs: 10\n\n    # model.yaml\n    out_dim: 7\n\n    # data.yaml\n    data_dir: ./data\n\na fit command can be run as:\n\n.. code-block:: bash\n\n    $ python main.py fit --trainer trainer.yaml --model model.yaml --data data.yaml [...]\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_2.html", "url_rel_html": "cli/lightning_cli_advanced_2.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Advanced)¶", "rst_text": ":orphan:\n\n.. testsetup:: *\n    :skipif: not _JSONARGPARSE_AVAILABLE\n\n    import torch\n    from unittest import mock\n    from typing import List\n    import lightning.pytorch.cli as pl_cli\n    from lightning.pytorch import LightningModule, LightningDataModule, Trainer, Callback\n\n\n    class NoFitTrainer(Trainer):\n        def fit(self, *_, **__):\n            pass\n\n\n    class LightningCLI(pl_cli.LightningCLI):\n        def __init__(self, *args, trainer_class=NoFitTrainer, run=False, **kwargs):\n            super().__init__(*args, trainer_class=trainer_class, run=run, **kwargs)\n\n\n    class MyModel(LightningModule):\n        def __init__(\n            self,\n            encoder_layers: int = 12,\n            decoder_layers: List[int] = [2, 4],\n            batch_size: int = 8,\n        ):\n            pass\n\n\n    class MyDataModule(LightningDataModule):\n        def __init__(self, batch_size: int = 8):\n            self.num_classes = 5\n\n\n    mock_argv = mock.patch(\"sys.argv\", [\"any.py\"])\n    mock_argv.start()\n\n.. testcleanup:: *\n\n    mock_argv.stop()\n\n#################################################\nConfigure hyperparameters from the CLI (Advanced)\n#################################################\n\n*********************************\nCustomize arguments by subcommand\n*********************************\nTo customize arguments by subcommand, pass the config *before* the subcommand:\n\n.. code-block:: bash\n\n    $ python main.py [before] [subcommand] [after]\n    $ python main.py  ...         fit       ...\n\nFor example, here we set the Trainer argument [max_steps = 100] for the full training routine and [max_steps = 10] for\ntesting:\n\n.. code-block:: bash\n\n    # config.yaml\n    fit:\n        trainer:\n            max_steps: 100\n    test:\n        trainer:\n            max_epochs: 10\n\nnow you can toggle this behavior by subcommand:\n\n.. code-block:: bash\n\n    # full routine with max_steps = 100\n    $ python main.py --config config.yaml fit\n\n    # test only with max_epochs = 10\n    $ python main.py --config config.yaml test\n\n----\n\n***************************\nRun from cloud yaml configs\n***************************\nFor certain enterprise workloads, Lightning CLI supports running from hosted configs:\n\n.. code-block:: bash\n\n    $ python main.py [subcommand] --config s3://bucket/config.yaml\n\nFor more options, refer to :doc:`Remote filesystems <../common/remote_fs>`.\n\n----\n\n**************************************\nUse a config via environment variables\n**************************************\nFor certain CI/CD systems, it's useful to pass in raw yaml config as environment variables:\n\n.. code-block:: bash\n\n    $ python main.py fit --trainer \"$TRAINER_CONFIG\" --model \"$MODEL_CONFIG\" [...]\n\n----\n\n***************************************\nRun from environment variables directly\n***************************************\nThe Lightning CLI can convert every possible CLI flag into an environment variable. To enable this, add to\n``parser_kwargs`` the ``default_env`` argument:\n\n.. code:: python\n\n    cli = LightningCLI(..., parser_kwargs={\"default_env\": True})\n\nnow use the ``--help`` CLI flag with any subcommand:\n\n.. code:: bash\n\n    $ python main.py fit --help\n\nwhich will show you ALL possible environment variables that can be set:\n\n.. code:: bash\n\n    usage: main.py [options] fit [-h] [-c CONFIG]\n                                ...\n\n    optional arguments:\n    ...\n    ARG:   --model.out_dim OUT_DIM\n    ENV:   PL_FIT__MODEL__OUT_DIM\n                            (type: int, default: 10)\n    ARG:   --model.learning_rate LEARNING_RATE\n    ENV:   PL_FIT__MODEL__LEARNING_RATE\n                            (type: float, default: 0.02)\n\nnow you can customize the behavior via environment variables:\n\n.. code:: bash\n\n    # set the options via env vars\n    $ export PL_FIT__MODEL__LEARNING_RATE=0.01\n    $ export PL_FIT__MODEL__OUT_DIM=5\n\n    $ python main.py fit\n\n----\n\n************************\nSet default config files\n************************\nTo set a path to a config file of defaults, use the ``default_config_files`` argument:\n\n.. testcode::\n\n    cli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"default_config_files\": [\"my_cli_defaults.yaml\"]})\n\nor if you want defaults per subcommand:\n\n.. testcode::\n\n    cli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"fit\": {\"default_config_files\": [\"my_fit_defaults.yaml\"]}})\n\n----\n\n*****************************\nEnable variable interpolation\n*****************************\nIn certain cases where multiple settings need to share a value, consider using variable interpolation. For instance:\n\n.. code-block:: yaml\n\n    model:\n      encoder_layers: 12\n      decoder_layers:\n      - ${model.encoder_layers}\n      - 4\n\nTo enable variable interpolation, first install omegaconf:\n\n.. code:: bash\n\n    pip install omegaconf\n\nThen set omegaconf when instantiating the ``LightningCLI`` class:\n\n.. code:: python\n\n    cli = LightningCLI(MyModel, parser_kwargs={\"parser_mode\": \"omegaconf\"})\n\nAfter this, the CLI will automatically perform interpolation in yaml files:\n\n.. code:: bash\n\n    python main.py --model.encoder_layers=12\n\nFor more details about the interpolation support and its limitations, have a look at the `jsonargparse\n<https://jsonargparse.readthedocs.io/en/stable/#variable-interpolation>`__ and the `omegaconf\n<https://omegaconf.readthedocs.io/en/2.1_branch/usage.html#variable-interpolation>`__ documentations.\n\n.. note::\n\n    There are many use cases in which variable interpolation is not the correct approach. When a parameter **must\n    always** be derived from other settings, it shouldn't be up to the CLI user to do this in a config file. For\n    example, if the data and model both require ``batch_size`` and must be the same value, then\n    :ref:`cli_link_arguments` should be used instead of interpolation.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_3.html", "url_rel_html": "cli/lightning_cli_advanced_3.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Advanced)¶", "rst_text": ":orphan:\n\n.. testsetup:: *\n    :skipif: not _JSONARGPARSE_AVAILABLE\n\n    import torch\n    from unittest import mock\n    from typing import List\n    import lightning.pytorch.cli as pl_cli\n    from lightning.pytorch import LightningModule, LightningDataModule, Trainer, Callback\n\n\n    class NoFitTrainer(Trainer):\n        def fit(self, *_, **__):\n            pass\n\n\n    class LightningCLI(pl_cli.LightningCLI):\n        def __init__(self, *args, trainer_class=NoFitTrainer, run=False, **kwargs):\n            super().__init__(*args, trainer_class=trainer_class, run=run, **kwargs)\n\n\n    class MyModel(LightningModule):\n        def __init__(\n            self,\n            encoder_layers: int = 12,\n            decoder_layers: List[int] = [2, 4],\n            batch_size: int = 8,\n        ):\n            pass\n\n\n    class MyDataModule(LightningDataModule):\n        def __init__(self, batch_size: int = 8):\n            self.num_classes = 5\n\n\n    MyModelBaseClass = MyModel\n    MyDataModuleBaseClass = MyDataModule\n\n    mock_argv = mock.patch(\"sys.argv\", [\"any.py\"])\n    mock_argv.start()\n\n.. testcleanup:: *\n\n    mock_argv.stop()\n\n#################################################\nConfigure hyperparameters from the CLI (Advanced)\n#################################################\n\nInstantiation only mode\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe CLI is designed to start fitting with minimal code changes. On class instantiation, the CLI will automatically call\nthe trainer function associated with the subcommand provided, so you don't have to do it. To avoid this, you can set the\nfollowing argument:\n\n.. testcode::\n\n    cli = LightningCLI(MyModel, run=False)  # True by default\n    # you'll have to call fit yourself:\n    cli.trainer.fit(cli.model)\n\nIn this mode, subcommands are **not** added to the parser. This can be useful to implement custom logic without having\nto subclass the CLI, but still, use the CLI's instantiation and argument parsing capabilities.\n\n\nTrainer Callbacks and arguments with class type\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA very important argument of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class is the ``callbacks``. In\ncontrast to simpler arguments that take numbers or strings, ``callbacks`` expects a list of instances of subclasses of\n:class:`~lightning.pytorch.callbacks.Callback`. To specify this kind of argument in a config file, each callback must be\ngiven as a dictionary, including a ``class_path`` entry with an import path of the class and optionally an ``init_args``\nentry with arguments to use to instantiate. Therefore, a simple configuration file that defines two callbacks is the\nfollowing:\n\n.. code-block:: yaml\n\n    trainer:\n      callbacks:\n        - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n          init_args:\n            save_weights_only: true\n        - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n          init_args:\n            logging_interval: 'epoch'\n\nSimilar to the callbacks, any parameter in :class:`~lightning.pytorch.trainer.trainer.Trainer` and user extended\n:class:`~lightning.pytorch.core.LightningModule` and\n:class:`~lightning.pytorch.core.datamodule.LightningDataModule` classes that have as type hint a class, can be\nconfigured the same way using ``class_path`` and ``init_args``. If the package that defines a subclass is imported\nbefore the :class:`~lightning.pytorch.cli.LightningCLI` class is run, the name can be used instead of the full import\npath.\n\nFrom command line the syntax is the following:\n\n.. code-block:: bash\n\n    $ python ... \\\n        --trainer.callbacks+={CALLBACK_1_NAME} \\\n        --trainer.callbacks.{CALLBACK_1_ARGS_1}=... \\\n        --trainer.callbacks.{CALLBACK_1_ARGS_2}=... \\\n        ...\n        --trainer.callbacks+={CALLBACK_N_NAME} \\\n        --trainer.callbacks.{CALLBACK_N_ARGS_1}=... \\\n        ...\n\nNote the use of ``+`` to append a new callback to the list and that the ``init_args`` are applied to the previous\ncallback appended. Here is an example:\n\n.. code-block:: bash\n\n    $ python ... \\\n        --trainer.callbacks+=EarlyStopping \\\n        --trainer.callbacks.patience=5 \\\n        --trainer.callbacks+=LearningRateMonitor \\\n        --trainer.callbacks.logging_interval=epoch\n\n.. note::\n\n    Serialized config files (e.g. ``--print_config`` or :class:`~lightning.pytorch.cli.SaveConfigCallback`) always have\n    the full ``class_path``, even when class name shorthand notation is used in the command line or in input config\n    files.\n\n\nMultiple models and/or datasets\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA CLI can be written such that a model and/or a datamodule is specified by an import path and init arguments. For\nexample, with a tool implemented as:\n\n.. code-block:: python\n\n    cli = LightningCLI(MyModelBaseClass, MyDataModuleBaseClass, subclass_mode_model=True, subclass_mode_data=True)\n\nA possible config file could be as follows:\n\n.. code-block:: yaml\n\n    model:\n      class_path: mycode.mymodels.MyModel\n      init_args:\n        decoder_layers:\n        - 2\n        - 4\n        encoder_layers: 12\n    data:\n      class_path: mycode.mydatamodules.MyDataModule\n      init_args:\n        ...\n    trainer:\n      callbacks:\n        - class_path: lightning.pytorch.callbacks.EarlyStopping\n          init_args:\n            patience: 5\n        ...\n\nOnly model classes that are a subclass of ``MyModelBaseClass`` would be allowed, and similarly, only subclasses of\n``MyDataModuleBaseClass``. If as base classes :class:`~lightning.pytorch.core.LightningModule` and\n:class:`~lightning.pytorch.core.datamodule.LightningDataModule` is given, then the CLI would allow any lightning module\nand data module.\n\n.. tip::\n\n    Note that with the subclass modes, the ``--help`` option does not show information for a specific subclass. To get\n    help for a subclass, the options ``--model.help`` and ``--data.help`` can be used, followed by the desired class\n    path. Similarly, ``--print_config`` does not include the settings for a particular subclass. To include them, the\n    class path should be given before the ``--print_config`` option. Examples for both help and print config are:\n\n    .. code-block:: bash\n\n        $ python trainer.py fit --model.help mycode.mymodels.MyModel\n        $ python trainer.py fit --model mycode.mymodels.MyModel --print_config\n\n\nModels with multiple submodules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMany use cases require to have several modules, each with its own configurable options. One possible way to handle this\nwith ``LightningCLI`` is to implement a single module having as init parameters each of the submodules. This is known as\n`dependency injection <https://en.wikipedia.org/wiki/Dependency_injection>`__ which is a good approach to improve\ndecoupling in your code base.\n\nSince the init parameters of the model have as a type hint a class, in the configuration, these would be specified with\n``class_path`` and ``init_args`` entries. For instance, a model could be implemented as:\n\n.. testcode::\n\n    class MyMainModel(LightningModule):\n        def __init__(self, encoder: nn.Module, decoder: nn.Module):\n            \"\"\"Example encoder-decoder submodules model\n\n            Args:\n                encoder: Instance of a module for encoding\n                decoder: Instance of a module for decoding\n            \"\"\"\n            super().__init__()\n            self.save_hyperparameters()\n            self.encoder = encoder\n            self.decoder = decoder\n\nIf the CLI is implemented as ``LightningCLI(MyMainModel)`` the configuration would be as follows:\n\n.. code-block:: yaml\n\n    model:\n      encoder:\n        class_path: mycode.myencoders.MyEncoder\n        init_args:\n          ...\n      decoder:\n        class_path: mycode.mydecoders.MyDecoder\n        init_args:\n          ...\n\nIt is also possible to combine ``subclass_mode_model=True`` and submodules, thereby having two levels of ``class_path``.\n\n.. tip::\n\n    By having ``self.save_hyperparameters()`` it becomes possible to load the model from a checkpoint. Simply do\n    ``ModelClass.load_from_checkpoint(\"path/to/checkpoint.ckpt\")``. In the case of using ``subclass_mode_model=True``,\n    then load it like ``LightningModule.load_from_checkpoint(\"path/to/checkpoint.ckpt\")``. ``save_hyperparameters`` is\n    optional and can be safely removed if there is no need to load from a checkpoint.\n\n\nFixed optimizer and scheduler\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn some cases, fixing the optimizer and/or learning scheduler might be desired instead of allowing multiple. For this,\nyou can manually add the arguments for specific classes by subclassing the CLI. The following code snippet shows how to\nimplement it:\n\n.. testcode::\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.add_optimizer_args(torch.optim.Adam)\n            parser.add_lr_scheduler_args(torch.optim.lr_scheduler.ExponentialLR)\n\nWith this, in the config, the ``optimizer`` and ``lr_scheduler`` groups would accept all of the options for the given\nclasses, in this example, ``Adam`` and ``ExponentialLR``. Therefore, the config file would be structured like:\n\n.. code-block:: yaml\n\n    optimizer:\n      lr: 0.01\n    lr_scheduler:\n      gamma: 0.2\n    model:\n      ...\n    trainer:\n      ...\n\nwhere the arguments can be passed directly through the command line without specifying the class. For example:\n\n.. code-block:: bash\n\n    $ python trainer.py fit --optimizer.lr=0.01 --lr_scheduler.gamma=0.2\n\n\nMultiple optimizers and schedulers\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, the CLIs support multiple optimizers and/or learning schedulers, automatically implementing\n``configure_optimizers``. This behavior can be disabled by providing ``auto_configure_optimizers=False`` on\ninstantiation of :class:`~lightning.pytorch.cli.LightningCLI`. This would be required for example to support multiple\noptimizers, for each selecting a particular optimizer class. Similar to multiple submodules, this can be done via\n`dependency injection <https://en.wikipedia.org/wiki/Dependency_injection>`__. Unlike the submodules, it is not possible\nto expect an instance of a class, because optimizers require the module's parameters to optimize, which are only\navailable after instantiation of the module. Learning schedulers are a similar situation, requiring an optimizer\ninstance. For these cases, dependency injection involves providing a function that instantiates the respective class\nwhen called.\n\nAn example of a model that uses two optimizers is the following:\n\n.. code-block:: python\n\n    from typing import Iterable\n    from torch.optim import Optimizer\n\n\n    OptimizerCallable = Callable[[Iterable], Optimizer]\n\n\n    class MyModel(LightningModule):\n        def __init__(self, optimizer1: OptimizerCallable, optimizer2: OptimizerCallable):\n            super().__init__()\n            self.save_hyperparameters()\n            self.optimizer1 = optimizer1\n            self.optimizer2 = optimizer2\n\n        def configure_optimizers(self):\n            optimizer1 = self.optimizer1(self.parameters())\n            optimizer2 = self.optimizer2(self.parameters())\n            return [optimizer1, optimizer2]\n\n\n    cli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n\nNote the type ``Callable[[Iterable], Optimizer]``, which denotes a function that receives a single argument, some\nlearnable parameters, and returns an optimizer instance. With this, from the command line it is possible to select the\nclass and init arguments for each of the optimizers, as follows:\n\n.. code-block:: bash\n\n    $ python trainer.py fit \\\n        --model.optimizer1=Adam \\\n        --model.optimizer1.lr=0.01 \\\n        --model.optimizer2=AdamW \\\n        --model.optimizer2.lr=0.0001\n\nIn the example above, the ``OptimizerCallable`` type alias was created to illustrate what the type hint means. For\nconvenience, this type alias and one for learning schedulers is available in the ``cli`` module. An example of a model\nthat uses dependency injection for an optimizer and a learning scheduler is:\n\n.. code-block:: python\n\n    from lightning.pytorch.cli import OptimizerCallable, LRSchedulerCallable, LightningCLI\n\n\n    class MyModel(LightningModule):\n        def __init__(\n            self,\n            optimizer: OptimizerCallable = torch.optim.Adam,\n            scheduler: LRSchedulerCallable = torch.optim.lr_scheduler.ConstantLR,\n        ):\n            super().__init__()\n            self.save_hyperparameters()\n            self.optimizer = optimizer\n            self.scheduler = scheduler\n\n        def configure_optimizers(self):\n            optimizer = self.optimizer(self.parameters())\n            scheduler = self.scheduler(optimizer)\n            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n\n    cli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n\nNote that for this example, classes are used as defaults. This is compatible with the type hints, since they are also\ncallables that receive the same first argument and return an instance of the class. Classes that have more than one\nrequired argument will not work as default. For these cases a lambda function can be used, e.g. ``optimizer:\nOptimizerCallable = lambda p: torch.optim.SGD(p, lr=0.01)``.\n\n\nRun from Python\n^^^^^^^^^^^^^^^\n\nEven though the :class:`~lightning.pytorch.cli.LightningCLI` class is designed to help in the implementation of command\nline tools, for some use cases it is desired to run directly from Python. To allow this there is the ``args`` parameter.\nAn example could be to first implement a normal CLI script, but adding an ``args`` parameter with default ``None`` to\nthe main function as follows:\n\n.. code:: python\n\n    from lightning.pytorch.cli import ArgsType, LightningCLI\n\n\n    def cli_main(args: ArgsType = None):\n        cli = LightningCLI(MyModel, ..., args=args)\n        ...\n\n\n    if __name__ == \"__main__\":\n        cli_main()\n\nThen it is possible to import the ``cli_main`` function to run it. Executing in a shell ``my_cli.py\n--trainer.max_epochs=100 --model.encoder_layers=24`` would be equivalent to:\n\n.. code:: python\n\n    from my_module.my_cli import cli_main\n\n    cli_main([\"--trainer.max_epochs=100\", \"--model.encoder_layers=24\"])\n\nAll the features that are supported from the command line can be used when giving ``args`` as a list of strings. It is\nalso possible to provide a ``dict`` or `jsonargparse.Namespace\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.Namespace>`__. For example in a jupyter notebook someone\nmight do:\n\n.. code:: python\n\n    args = {\n        \"trainer\": {\n            \"max_epochs\": 100,\n        },\n        \"model\": {},\n    }\n\n    args[\"model\"][\"encoder_layers\"] = 8\n    cli_main(args)\n    args[\"model\"][\"encoder_layers\"] = 12\n    cli_main(args)\n    args[\"trainer\"][\"max_epochs\"] = 200\n    cli_main(args)\n\n.. note::\n\n    The ``args`` parameter must be ``None`` when running from command line so that ``sys.argv`` is used as arguments.\n    Also, note that the purpose of ``trainer_defaults`` is different to ``args``. It is okay to use ``trainer_defaults``\n    in the ``cli_main`` function to modify the defaults of some trainer parameters.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_expert.html", "url_rel_html": "cli/lightning_cli_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Expert)¶", "rst_text": ":orphan:\n\n.. testsetup:: *\n    :skipif: not _JSONARGPARSE_AVAILABLE\n\n    import torch\n    from unittest import mock\n    from typing import List\n    import lightning.pytorch.cli as pl_cli\n    from lightning.pytorch import LightningModule, LightningDataModule, Trainer, Callback\n\n\n    class NoFitTrainer(Trainer):\n        def fit(self, *_, **__):\n            pass\n\n\n    class LightningCLI(pl_cli.LightningCLI):\n        def __init__(self, *args, trainer_class=NoFitTrainer, run=False, **kwargs):\n            super().__init__(*args, trainer_class=trainer_class, run=run, **kwargs)\n\n\n    class MyModel(LightningModule):\n        def __init__(\n            self,\n            encoder_layers: int = 12,\n            decoder_layers: List[int] = [2, 4],\n            batch_size: int = 8,\n        ):\n            pass\n\n\n    class MyClassModel(LightningModule):\n        def __init__(self, num_classes: int):\n            pass\n\n\n    class MyDataModule(LightningDataModule):\n        def __init__(self, batch_size: int = 8):\n            self.num_classes = 5\n\n\n    def send_email(address, message):\n        pass\n\n\n    mock_argv = mock.patch(\"sys.argv\", [\"any.py\"])\n    mock_argv.start()\n\n.. testcleanup:: *\n\n    mock_argv.stop()\n\n###############################################\nConfigure hyperparameters from the CLI (Expert)\n###############################################\n**Audience:** Users who already understand the LightningCLI and want to customize it.\n\n----\n\n**************************\nCustomize the LightningCLI\n**************************\n\nThe init parameters of the :class:`~lightning.pytorch.cli.LightningCLI` class can be used to customize some things,\ne.g., the description of the tool, enabling parsing of environment variables, and additional arguments to instantiate\nthe trainer and configuration parser.\n\nNevertheless, the init arguments are not enough for many use cases. For this reason, the class is designed so that it\ncan be extended to customize different parts of the command line tool. The argument parser class used by\n:class:`~lightning.pytorch.cli.LightningCLI` is :class:`~lightning.pytorch.cli.LightningArgumentParser`, which is an\nextension of python's argparse, thus adding arguments can be done using the :func:`add_argument` method. In contrast to\nargparse, it has additional methods to add arguments. For example :func:`add_class_arguments` add all arguments from the\ninit of a class. For more details, see the `respective documentation\n<https://jsonargparse.readthedocs.io/en/stable/#classes-methods-and-functions>`_.\n\nThe :class:`~lightning.pytorch.cli.LightningCLI` class has the\n:meth:`~lightning.pytorch.cli.LightningCLI.add_arguments_to_parser` method can be implemented to include more arguments.\nAfter parsing, the configuration is stored in the ``config`` attribute of the class instance. The\n:class:`~lightning.pytorch.cli.LightningCLI` class also has two methods that can be used to run code before and after\nthe trainer runs: ``before_<subcommand>`` and ``after_<subcommand>``. A realistic example of this would be to send an\nemail before and after the execution. The code for the ``fit`` subcommand would be something like this:\n\n.. testcode::\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.add_argument(\"--notification_email\", default=\"will@email.com\")\n\n        def before_fit(self):\n            send_email(address=self.config[\"notification_email\"], message=\"trainer.fit starting\")\n\n        def after_fit(self):\n            send_email(address=self.config[\"notification_email\"], message=\"trainer.fit finished\")\n\n\n    cli = MyLightningCLI(MyModel)\n\nNote that the config object ``self.config`` is a namespace whose keys are global options or groups of options. It has\nthe same structure as the YAML format described previously. This means that the parameters used for instantiating the\ntrainer class can be found in ``self.config['fit']['trainer']``.\n\n.. tip::\n\n    Have a look at the :class:`~lightning.pytorch.cli.LightningCLI` class API reference to learn about other methods\n    that can be extended to customize a CLI.\n\n----\n\n**************************\nConfigure forced callbacks\n**************************\nAs explained previously, any Lightning callback can be added by passing it through the command line or including it in\nthe config via ``class_path`` and ``init_args`` entries.\n\nHowever, certain callbacks **must** be coupled with a model so they are always present and configurable. This can be\nimplemented as follows:\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import EarlyStopping\n\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.add_lightning_class_args(EarlyStopping, \"my_early_stopping\")\n            parser.set_defaults({\"my_early_stopping.monitor\": \"val_loss\", \"my_early_stopping.patience\": 5})\n\n\n    cli = MyLightningCLI(MyModel)\n\nTo change the parameters for ``EarlyStopping`` in the config it would be:\n\n.. code-block:: yaml\n\n    model:\n      ...\n    trainer:\n      ...\n    my_early_stopping:\n      patience: 5\n\n.. note::\n\n    The example above overrides a default in ``add_arguments_to_parser``. This is included to show that defaults can be\n    changed if needed. However, note that overriding defaults in the source code is not intended to be used to store the\n    best hyperparameters for a task after experimentation. To guarantee reproducibility, the source code should be\n    stable. It is better to practice storing the best hyperparameters for a task in a configuration file independent\n    from the source code.\n\n----\n\n*******************\nClass type defaults\n*******************\n\nThe support for classes as type hints allows to try many possibilities with the same CLI. This is a useful feature, but\nit is tempting to use an instance of a class as a default. For example:\n\n.. testcode::\n\n    class MyMainModel(LightningModule):\n        def __init__(\n            self,\n            backbone: torch.nn.Module = MyModel(encoder_layers=24),  # BAD PRACTICE!\n        ):\n            super().__init__()\n            self.backbone = backbone\n\nNormally classes are mutable, as in this case. The instance of ``MyModel`` would be created the moment that the module\nthat defines ``MyMainModel`` is first imported. This means that the default of ``backbone`` will be initialized before\nthe CLI class runs ``seed_everything``, making it non-reproducible. Furthermore, if ``MyMainModel`` is used more than\nonce in the same Python process and the ``backbone`` parameter is not overridden, the same instance would be used in\nmultiple places. Most likely, this is not what the developer intended. Having an instance as default also makes it\nimpossible to generate the complete config file since it is not known which arguments were used to instantiate it for\narbitrary classes.\n\nAn excellent solution to these problems is not to have a default or set the default to a unique value (e.g., a string).\nThen check this value and instantiate it in the ``__init__`` body. If a class parameter has no default and the CLI is\nsubclassed, then a default can be set as follows:\n\n.. testcode::\n\n    default_backbone = {\n        \"class_path\": \"import.path.of.MyModel\",\n        \"init_args\": {\n            \"encoder_layers\": 24,\n        },\n    }\n\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.set_defaults({\"model.backbone\": default_backbone})\n\nA more compact version that avoids writing a dictionary would be:\n\n.. testcode::\n\n    from jsonargparse import lazy_instance\n\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.set_defaults({\"model.backbone\": lazy_instance(MyModel, encoder_layers=24)})\n\n----\n\n.. _cli_link_arguments:\n\n****************\nArgument linking\n****************\nAnother case in which it might be desired to extend :class:`~lightning.pytorch.cli.LightningCLI` is that the model and\ndata module depends on a common parameter. For example, in some cases, both classes require to know the ``batch_size``.\nIt is a burden and error-prone to give the same value twice in a config file. To avoid this, the parser can be\nconfigured so that a value is only given once and then propagated accordingly. With a tool implemented like the one\nshown below, the ``batch_size`` only has to be provided in the ``data`` section of the config.\n\n.. testcode::\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.link_arguments(\"data.batch_size\", \"model.batch_size\")\n\n\n    cli = MyLightningCLI(MyModel, MyDataModule)\n\nThe linking of arguments is observed in the help of the tool, which for this example would look like:\n\n.. code-block:: bash\n\n    $ python trainer.py fit --help\n      ...\n        --data.batch_size BATCH_SIZE\n                              Number of samples in a batch (type: int, default: 8)\n\n      Linked arguments:\n        data.batch_size --> model.batch_size\n                              Number of samples in a batch (type: int)\n\nSometimes a parameter value is only available after class instantiation. An example could be that your model requires\nthe number of classes to instantiate its fully connected layer (for a classification task). But the value is not\navailable until the data module has been instantiated. The code below illustrates how to address this.\n\n.. testcode::\n\n    class MyLightningCLI(LightningCLI):\n        def add_arguments_to_parser(self, parser):\n            parser.link_arguments(\"data.num_classes\", \"model.num_classes\", apply_on=\"instantiate\")\n\n\n    cli = MyLightningCLI(MyClassModel, MyDataModule)\n\nInstantiation links are used to automatically determine the order of instantiation, in this case data first.\n\n.. note::\n\n    The linking of arguments is intended for things that are meant to be non-configurable. This improves the CLI user\n    experience since it avoids the need to provide more parameters. A related concept is a variable interpolation that\n    keeps things configurable.\n\n.. tip::\n\n    The linking of arguments can be used for more complex cases. For example to derive a value via a function that takes\n    multiple settings as input. For more details have a look at the API of `link_arguments\n    <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentLinking.link_arguments>`_.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_faq.html", "url_rel_html": "cli/lightning_cli_faq.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_faq.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Frequently asked questions for LightningCLI¶", "rst_text": ":orphan:\n\n###########################################\nFrequently asked questions for LightningCLI\n###########################################\n\n************************\nWhat does CLI stand for?\n************************\nCLI is short for command line interface. This means it is a tool intended to be run from a terminal, similar to commands\nlike ``git``.\n\n----\n\n.. _what-is-a-yaml-config-file:\n\n***************************\nWhat is a yaml config file?\n***************************\nA YAML is a standard for configuration files used to describe parameters for sections of a program. It is a common tool\nin engineering and has recently started to gain popularity in machine learning. An example of a YAML file is the\nfollowing:\n\n.. code:: yaml\n\n    # file.yaml\n    car:\n        max_speed:100\n        max_passengers:2\n    plane:\n        fuel_capacity: 50\n    class_3:\n        option_1: 'x'\n        option_2: 'y'\n\nIf you are unfamiliar with YAML, the short introduction at `realpython.com#yaml-syntax\n<https://realpython.com/python-yaml/#yaml-syntax>`__ might be a good starting point.\n\n----\n\n*********************\nWhat is a subcommand?\n*********************\nA subcommand is what is the action the LightningCLI applies to the script:\n\n.. code:: bash\n\n    python main.py [subcommand]\n\nSee the Potential subcommands with:\n\n.. code:: bash\n\n    python main.py --help\n\nwhich prints:\n\n.. code:: bash\n\n        ...\n\n        fit                 Runs the full optimization routine.\n        validate            Perform one evaluation epoch over the validation set.\n        test                Perform one evaluation epoch over the test set.\n        predict             Run inference on your data.\n\nuse a subcommand as follows:\n\n.. code:: bash\n\n    python main.py fit\n    python main.py test\n\n----\n\n*******************************************************\nWhat is the relation between LightningCLI and argparse?\n*******************************************************\n\n:class:`~lightning.pytorch.cli.LightningCLI` makes use of `jsonargparse <https://github.com/omni-us/jsonargparse>`__\nwhich is an extension of `argparse <https://docs.python.org/3/library/argparse.html>`__. Due to this,\n:class:`~lightning.pytorch.cli.LightningCLI` follows the same arguments style as many POSIX command line tools. Long\noptions are prefixed with two dashes and its corresponding values are separated by space or an equal sign, as ``--option\nvalue`` or ``--option=value``. Command line options are parsed from left to right, therefore if a setting appears\nmultiple times, the value most to the right will override the previous ones.\n\n----\n\n*******************************************\nWhat is the override order of LightningCLI?\n*******************************************\n\nThe final configuration of CLIs implemented with :class:`~lightning.pytorch.cli.LightningCLI` can depend on default\nconfig files (if defined), environment variables (if enabled) and command line arguments. The override order between\nthese is the following:\n\n1. Defaults defined in the source code.\n2. Existing default config files in the order defined in ``default_config_files``, e.g. ``~/.myapp.yaml``.\n3. Entire config environment variable, e.g. ``PL_FIT__CONFIG``.\n4. Individual argument environment variables, e.g. ``PL_FIT__SEED_EVERYTHING``.\n5. Command line arguments in order left to right (might include config files).\n\n----\n\n****************************\nHow do I troubleshoot a CLI?\n****************************\nThe standard behavior for CLIs, when they fail, is to terminate the process with a non-zero exit code and a short\nmessage to hint the user about the cause. This is problematic while developing the CLI since there is no information to\ntrack down the root of the problem. To troubleshoot set the environment variable ``JSONARGPARSE_DEBUG`` to any value\nbefore running the CLI:\n\n.. code:: bash\n\n    export JSONARGPARSE_DEBUG=true\n    python main.py fit\n\n.. note::\n\n    When asking about problems and reporting issues, please set the ``JSONARGPARSE_DEBUG`` and include the stack trace\n    in your description. With this, users are more likely to help identify the cause without needing to create a\n    reproducible script.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_intermediate.html", "url_rel_html": "cli/lightning_cli_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Intermediate)¶", "rst_text": ":orphan:\n\n#####################################################\nConfigure hyperparameters from the CLI (Intermediate)\n#####################################################\n**Audience:** Users who want advanced modularity via a command line interface (CLI).\n\n**Pre-reqs:** You must already understand how to use the command line and :doc:`LightningDataModule <../data/datamodule>`.\n\n----\n\n*************************\nLightningCLI requirements\n*************************\n\nThe :class:`~lightning.pytorch.cli.LightningCLI` class is designed to significantly ease the implementation of CLIs. To\nuse this class, an additional Python requirement is necessary than the minimal installation of Lightning provides. To\nenable, either install all extras:\n\n.. code:: bash\n\n    pip install \"lightning[pytorch-extra]\"\n\nor if only interested in ``LightningCLI``, just install jsonargparse:\n\n.. code:: bash\n\n    pip install \"jsonargparse[signatures]\"\n\n----\n\n******************\nImplementing a CLI\n******************\nImplementing a CLI is as simple as instantiating a :class:`~lightning.pytorch.cli.LightningCLI` object giving as\narguments classes for a ``LightningModule`` and optionally a ``LightningDataModule``:\n\n.. code:: python\n\n    # main.py\n    from lightning.pytorch.cli import LightningCLI\n\n    # simple demo classes for your convenience\n    from lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\n    def cli_main():\n        cli = LightningCLI(DemoModel, BoringDataModule)\n        # note: don't call fit!!\n\n\n    if __name__ == \"__main__\":\n        cli_main()\n        # note: it is good practice to implement the CLI in a function and call it in the main if block\n\nNow your model can be managed via the CLI. To see the available commands type:\n\n.. code:: bash\n\n    $ python main.py --help\n\nwhich prints out:\n\n.. code:: bash\n\n    usage: main.py [-h] [-c CONFIG] [--print_config [={comments,skip_null,skip_default}+]]\n            {fit,validate,test,predict} ...\n\n    Lightning Trainer command line tool\n\n    optional arguments:\n    -h, --help            Show this help message and exit.\n    -c CONFIG, --config CONFIG\n                            Path to a configuration file in json or yaml format.\n    --print_config [={comments,skip_null,skip_default}+]\n                            Print configuration and exit.\n\n    subcommands:\n    For more details of each subcommand add it as argument followed by --help.\n\n    {fit,validate,test,predict}\n        fit                 Runs the full optimization routine.\n        validate            Perform one evaluation epoch over the validation set.\n        test                Perform one evaluation epoch over the test set.\n        predict             Run inference on your data.\n\n\nThe message tells us that we have a few available subcommands:\n\n.. code:: bash\n\n    python main.py [subcommand]\n\nwhich you can use depending on your use case:\n\n.. code:: bash\n\n    $ python main.py fit\n    $ python main.py validate\n    $ python main.py test\n    $ python main.py predict\n\n----\n\n**************************\nTrain a model with the CLI\n**************************\nTo train a model, use the ``fit`` subcommand:\n\n.. code:: bash\n\n    python main.py fit\n\nView all available options with the ``--help`` argument given after the subcommand:\n\n.. code:: bash\n\n    $ python main.py fit --help\n\n    usage: main.py [options] fit [-h] [-c CONFIG]\n                                [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n                                ...\n                                [--ckpt_path CKPT_PATH]\n        --trainer.logger LOGGER\n\n    optional arguments:\n    <class '__main__.DemoModel'>:\n        --model.out_dim OUT_DIM\n                                (type: int, default: 10)\n        --model.learning_rate LEARNING_RATE\n                                (type: float, default: 0.02)\n    <class 'lightning.pytorch.demos.boring_classes.BoringDataModule'>:\n    --data CONFIG         Path to a configuration file.\n    --data.data_dir DATA_DIR\n                            (type: str, default: ./)\n\nWith the Lightning CLI enabled, you can now change the parameters without touching your code:\n\n.. code:: bash\n\n    # change the learning_rate\n    python main.py fit --model.learning_rate 0.1\n\n    # change the output dimensions also\n    python main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n    # change trainer and data arguments too\n    python main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n\n.. tip::\n\n    The options that become available in the CLI are the ``__init__`` parameters of the ``LightningModule`` and\n    ``LightningDataModule`` classes. Thus, to make hyperparameters configurable, just add them to your class's\n    ``__init__``. It is highly recommended that these parameters are described in the docstring so that the CLI shows\n    them in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give\n    understandable error messages when incorrect values are given.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_intermediate_2.html", "url_rel_html": "cli/lightning_cli_intermediate_2.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI (Intermediate)¶", "rst_text": ":orphan:\n\n#####################################################\nConfigure hyperparameters from the CLI (Intermediate)\n#####################################################\n**Audience:** Users who have multiple models and datasets per project.\n\n**Pre-reqs:** You must have read :doc:`(Control it all from the CLI) <lightning_cli_intermediate>`.\n\n----\n\n***************************\nWhy mix models and datasets\n***************************\nLightning projects usually begin with one model and one dataset. As the project grows in complexity and you introduce\nmore models and more datasets, it becomes desirable to mix any model with any dataset directly from the command line\nwithout changing your code.\n\n.. code:: bash\n\n    # Mix and match anything\n    $ python main.py fit --model=GAN --data=MNIST\n    $ python main.py fit --model=Transformer --data=MNIST\n\n``LightningCLI`` makes this very simple. Otherwise, this kind of configuration requires a significant amount of\nboilerplate that often looks like this:\n\n.. code:: python\n\n    # choose model\n    if args.model == \"gan\":\n        model = GAN(args.feat_dim)\n    elif args.model == \"transformer\":\n        model = Transformer(args.feat_dim)\n    ...\n\n    # choose datamodule\n    if args.data == \"MNIST\":\n        datamodule = MNIST()\n    elif args.data == \"imagenet\":\n        datamodule = Imagenet()\n    ...\n\n    # mix them!\n    trainer.fit(model, datamodule)\n\nIt is highly recommended that you avoid writing this kind of boilerplate and use ``LightningCLI`` instead.\n\n----\n\n*************************\nMultiple LightningModules\n*************************\nTo support multiple models, when instantiating ``LightningCLI`` omit the ``model_class`` parameter:\n\n.. code:: python\n\n    # main.py\n    from lightning.pytorch.cli import LightningCLI\n    from lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\n    class Model1(DemoModel):\n        def configure_optimizers(self):\n            print(\"⚡\", \"using Model1\", \"⚡\")\n            return super().configure_optimizers()\n\n\n    class Model2(DemoModel):\n        def configure_optimizers(self):\n            print(\"⚡\", \"using Model2\", \"⚡\")\n            return super().configure_optimizers()\n\n\n    cli = LightningCLI(datamodule_class=BoringDataModule)\n\nNow you can choose between any model from the CLI:\n\n.. code:: bash\n\n    # use Model1\n    python main.py fit --model Model1\n\n    # use Model2\n    python main.py fit --model Model2\n\n.. tip::\n\n    Instead of omitting the ``model_class`` parameter, you can give a base class and ``subclass_mode_model=True``. This\n    will make the CLI only accept models which are a subclass of the given base class.\n\n----\n\n*****************************\nMultiple LightningDataModules\n*****************************\nTo support multiple data modules, when instantiating ``LightningCLI`` omit the ``datamodule_class`` parameter:\n\n.. code:: python\n\n    # main.py\n    import torch\n    from lightning.pytorch.cli import LightningCLI\n    from lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\n    class FakeDataset1(BoringDataModule):\n        def train_dataloader(self):\n            print(\"⚡\", \"using FakeDataset1\", \"⚡\")\n            return torch.utils.data.DataLoader(self.random_train)\n\n\n    class FakeDataset2(BoringDataModule):\n        def train_dataloader(self):\n            print(\"⚡\", \"using FakeDataset2\", \"⚡\")\n            return torch.utils.data.DataLoader(self.random_train)\n\n\n    cli = LightningCLI(DemoModel)\n\nNow you can choose between any dataset at runtime:\n\n.. code:: bash\n\n    # use Model1\n    python main.py fit --data FakeDataset1\n\n    # use Model2\n    python main.py fit --data FakeDataset2\n\n.. tip::\n\n    Instead of omitting the ``datamodule_class`` parameter, you can give a base class and ``subclass_mode_data=True``.\n    This will make the CLI only accept data modules that are a subclass of the given base class.\n\n----\n\n*******************\nMultiple optimizers\n*******************\nStandard optimizers from ``torch.optim`` work out of the box:\n\n.. code:: bash\n\n    python main.py fit --optimizer AdamW\n\nIf the optimizer you want needs other arguments, add them via the CLI (no need to change your code)!\n\n.. code:: bash\n\n    python main.py fit --optimizer SGD --optimizer.lr=0.01\n\nFurthermore, any custom subclass of :class:`torch.optim.Optimizer` can be used as an optimizer:\n\n.. code:: python\n\n    # main.py\n    import torch\n    from lightning.pytorch.cli import LightningCLI\n    from lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\n    class LitAdam(torch.optim.Adam):\n        def step(self, closure):\n            print(\"⚡\", \"using LitAdam\", \"⚡\")\n            super().step(closure)\n\n\n    class FancyAdam(torch.optim.Adam):\n        def step(self, closure):\n            print(\"⚡\", \"using FancyAdam\", \"⚡\")\n            super().step(closure)\n\n\n    cli = LightningCLI(DemoModel, BoringDataModule)\n\nNow you can choose between any optimizer at runtime:\n\n.. code:: bash\n\n    # use LitAdam\n    python main.py fit --optimizer LitAdam\n\n    # use FancyAdam\n    python main.py fit --optimizer FancyAdam\n\n----\n\n*******************\nMultiple schedulers\n*******************\nStandard learning rate schedulers from ``torch.optim.lr_scheduler``  work out of the box:\n\n.. code:: bash\n\n    python main.py fit --optimizer=Adam --lr_scheduler CosineAnnealingLR\n\nPlease note that ``--optimizer`` must be added for ``--lr_scheduler`` to have an effect.\n\nIf the scheduler you want needs other arguments, add them via the CLI (no need to change your code)!\n\n.. code:: bash\n\n    python main.py fit --optimizer=Adam --lr_scheduler=ReduceLROnPlateau --lr_scheduler.monitor=epoch\n\nFurthermore, any custom subclass of ``torch.optim.lr_scheduler.LRScheduler`` can be used as learning rate scheduler:\n\n.. code:: python\n\n    # main.py\n    import torch\n    from lightning.pytorch.cli import LightningCLI\n    from lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\n    class LitLRScheduler(torch.optim.lr_scheduler.CosineAnnealingLR):\n        def step(self):\n            print(\"⚡\", \"using LitLRScheduler\", \"⚡\")\n            super().step()\n\n\n    cli = LightningCLI(DemoModel, BoringDataModule)\n\nNow you can choose between any learning rate scheduler at runtime:\n\n.. code:: bash\n\n    # LitLRScheduler\n    python main.py fit --optimizer=Adam --lr_scheduler LitLRScheduler\n\n\n----\n\n************************\nClasses from any package\n************************\nIn the previous sections, custom classes to select were defined in the same python file where the ``LightningCLI`` class\nis run. To select classes from any package by using only the class name, import the respective package:\n\n.. code:: python\n\n    from lightning.pytorch.cli import LightningCLI\n    import my_code.models  # noqa: F401\n    import my_code.data_modules  # noqa: F401\n    import my_code.optimizers  # noqa: F401\n\n    cli = LightningCLI()\n\nNow use any of the classes:\n\n.. code:: bash\n\n    python main.py fit --model Model1 --data FakeDataset1 --optimizer LitAdam --lr_scheduler LitLRScheduler\n\nThe ``# noqa: F401`` comment avoids a linter warning that the import is unused.\n\nIt is also possible to select subclasses that have not been imported by giving the full import path:\n\n.. code:: bash\n\n    python main.py fit --model my_code.models.Model1\n\n----\n\n*************************\nHelp for specific classes\n*************************\nWhen multiple models or datasets are accepted, the main help of the CLI does not include their specific parameters. To\nshow this specific help, additional help arguments expect the class name or its import path. For example:\n\n.. code:: bash\n\n    python main.py fit --model.help Model1\n    python main.py fit --data.help FakeDataset2\n    python main.py fit --optimizer.help Adagrad\n    python main.py fit --lr_scheduler.help StepLR\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing.html", "url_rel_html": "common/checkpointing.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Checkpointing¶", "rst_text": ".. _checkpointing:\n\n#############\nCheckpointing\n#############\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Saving and loading checkpoints\n   :description: Learn to save and load checkpoints\n   :col_css: col-md-4\n   :button_link: checkpointing_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Customize checkpointing behavior\n   :description: Learn how to change the behavior of checkpointing\n   :col_css: col-md-4\n   :button_link: checkpointing_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Upgrading checkpoints\n   :description: Learn how to upgrade old checkpoints to the newest Lightning version\n   :col_css: col-md-4\n   :button_link: checkpointing_migration.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Cloud-based checkpoints\n   :description: Enable cloud-based checkpointing and composable checkpoints.\n   :col_css: col-md-4\n   :button_link: checkpointing_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Distributed checkpoints\n   :description: Save and load very large models efficiently with distributed checkpoints\n   :col_css: col-md-4\n   :button_link: checkpointing_expert.html\n   :height: 150\n   :tag: expert\n\n.. raw:: html\n\n        </div>\n    </div>\n\n----\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: ModelCheckpoint API\n   :description: Dig into the ModelCheckpoint API\n   :col_css: col-md-4\n   :button_link: ../api/lightning.pytorch.callbacks.ModelCheckpoint.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing_advanced.html", "url_rel_html": "common/checkpointing_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Cloud-based checkpoints (advanced)¶", "rst_text": ".. _checkpointing_advanced:\n\n##################################\nCloud-based checkpoints (advanced)\n##################################\n\n\n*****************\nCloud checkpoints\n*****************\nLightning is integrated with the major remote file systems including local filesystems and several cloud storage providers such as\n`S3 <https://aws.amazon.com/s3/>`_ on `AWS <https://aws.amazon.com/>`_, `GCS <https://cloud.google.com/storage>`_ on `Google Cloud <https://cloud.google.com/>`_,\nor `ADL <https://azure.microsoft.com/solutions/data-lake/>`_ on `Azure <https://azure.microsoft.com/>`_.\n\nPyTorch Lightning uses `fsspec <https://filesystem-spec.readthedocs.io/>`_ internally to handle all filesystem operations.\n\n----\n\nSave a cloud checkpoint\n=======================\n\nTo save to a remote filesystem, prepend a protocol like \"s3:/\" to the root_dir used for writing and reading model data.\n\n.. code-block:: python\n\n    # `default_root_dir` is the default path used for logs and checkpoints\n    trainer = Trainer(default_root_dir=\"s3://my_bucket/data/\")\n    trainer.fit(model)\n\n----\n\nResume training from a cloud checkpoint\n=======================================\nTo resume training from a cloud checkpoint use a cloud url.\n\n.. code-block:: python\n\n    trainer = Trainer(default_root_dir=tmpdir, max_steps=3)\n    trainer.fit(model, ckpt_path=\"s3://my_bucket/ckpts/classifier.ckpt\")\n\nPyTorch Lightning uses `fsspec <https://filesystem-spec.readthedocs.io/>`_ internally to handle all filesystem operations.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html", "url_rel_html": "common/checkpointing_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Saving and loading checkpoints (basic)¶", "rst_text": ":orphan:\n\n.. _checkpointing_basic:\n\n######################################\nSaving and loading checkpoints (basic)\n######################################\n**Audience:** All users\n\n----\n\n*********************\nWhat is a checkpoint?\n*********************\nWhen a model is training, the performance changes as it continues to see more data. It is a best practice to save the state of a model throughout the training process. This gives you a version of the model, *a checkpoint*, at each key point during the development of the model. Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n\nCheckpoints also enable your training to resume from where it was in case the training process is interrupted.\n\nPyTorch Lightning checkpoints are fully usable in plain PyTorch.\n\n----\n\n.. important::\n\n   **Important Update: Deprecated Method**\n\n   Starting from PyTorch Lightning v1.0.0, the `resume_from_checkpoint` argument has been deprecated. To resume training from a checkpoint, use the `ckpt_path` argument in the `fit()` method.\n   Please update your code accordingly to avoid potential compatibility issues.\n\n************************\nContents of a checkpoint\n************************\nA Lightning checkpoint contains a dump of the model's entire internal state. Unlike plain PyTorch, Lightning saves *everything* you need to restore a model even in the most complex distributed training environments.\n\nInside a Lightning checkpoint you'll find:\n\n- 16-bit scaling factor (if using 16-bit precision training)\n- Current epoch\n- Global step\n- LightningModule's state_dict\n- State of all optimizers\n- State of all learning rate schedulers\n- State of all callbacks (for stateful callbacks)\n- State of datamodule (for stateful datamodules)\n- The hyperparameters (init arguments) with which the model was created\n- The hyperparameters (init arguments) with which the datamodule was created\n- State of Loops\n\n----\n\n*****************\nSave a checkpoint\n*****************\nLightning automatically saves a checkpoint for you in your current working directory, with the state of your last training epoch. This makes sure you can resume training in case it was interrupted.\n\n.. code-block:: python\n\n    # simply by using the Trainer you get automatic checkpointing\n    trainer = Trainer()\n\n\nCheckpoint save location\n========================\n\nThe location where checkpoints are saved depends on whether you have configured a logger:\n\n**Without a logger**, checkpoints are saved to the ``default_root_dir``:\n\n.. code-block:: python\n\n    # saves checkpoints to 'some/path/checkpoints/'\n    trainer = Trainer(default_root_dir=\"some/path/\", logger=False)\n\n**With a logger**, checkpoints are saved to the logger's directory, **not** to ``default_root_dir``:\n\n.. code-block:: python\n\n    from lightning.pytorch.loggers import CSVLogger\n\n    # checkpoints will be saved to 'logs/my_experiment/version_0/checkpoints/'\n    # NOT to 'some/path/checkpoints/'\n    trainer = Trainer(\n        default_root_dir=\"some/path/\",  # This will be ignored for checkpoints!\n        logger=CSVLogger(\"logs\", \"my_experiment\")\n    )\n\nTo explicitly control the checkpoint location when using a logger, use the\n:class:`~lightning.pytorch.callbacks.ModelCheckpoint` callback:\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import ModelCheckpoint\n\n    # explicitly set checkpoint directory\n    checkpoint_callback = ModelCheckpoint(dirpath=\"my/custom/checkpoint/path/\")\n    trainer = Trainer(\n        logger=CSVLogger(\"logs\", \"my_experiment\"),\n        callbacks=[checkpoint_callback]\n    )\n\n\n----\n\n\n*******************************\nLightningModule from checkpoint\n*******************************\n\nTo load a LightningModule along with its weights and hyperparameters use the following method:\n\n.. code-block:: python\n\n    model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n\n    # disable randomness, dropout, etc...\n    model.eval()\n\n    # predict with the model\n    y_hat = model(x)\n\n----\n\nSave hyperparameters\n====================\nThe LightningModule allows you to automatically save all the hyperparameters passed to *init* simply by calling *self.save_hyperparameters()*.\n\n.. code-block:: python\n\n    class MyLightningModule(LightningModule):\n        def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n            super().__init__()\n            self.save_hyperparameters()\n\nThe hyperparameters are saved to the \"hyper_parameters\" key in the checkpoint\n\n.. code-block:: python\n\n    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n    print(checkpoint[\"hyper_parameters\"])\n    # {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n\nThe LightningModule also has access to the Hyperparameters\n\n.. code-block:: python\n\n    model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n    print(model.hparams.learning_rate)\n\n----\n\nInitialize with other parameters\n================================\nIf you used the *self.save_hyperparameters()* method in the *__init__* method of the LightningModule, you can override these and initialize the model with different hyperparameters.\n\n.. code-block:: python\n\n    # if you train and save the model like this it will use these values when loading\n    # the weights. But you can overwrite this\n    LitModel(in_dim=32, out_dim=10)\n\n    # uses in_dim=32, out_dim=10\n    model = LitModel.load_from_checkpoint(PATH)\n\n    # uses in_dim=128, out_dim=10\n    model = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n\nIn some cases, we may also pass entire PyTorch modules to the ``__init__`` method, which you don't want to save as hyperparameters due to their large size. If you didn't call ``self.save_hyperparameters()`` or ignore parameters via ``save_hyperparameters(ignore=...)``, then you must pass the missing positional arguments or keyword arguments when calling ``load_from_checkpoint`` method:\n\n\n.. code-block:: python\n\n    class LitAutoencoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            ...\n\n        ...\n\n\n    model = LitAutoEncoder.load_from_checkpoint(PATH, encoder=encoder, decoder=decoder)\n\n\n----\n\n\n*************************\nnn.Module from checkpoint\n*************************\nLightning checkpoints are fully compatible with plain torch nn.Modules.\n\n.. code-block:: python\n\n    checkpoint = torch.load(CKPT_PATH)\n    print(checkpoint.keys())\n\nFor example, let's pretend we created a LightningModule like so:\n\n.. code-block:: python\n\n    class Encoder(nn.Module):\n        ...\n\n\n    class Decoder(nn.Module):\n        ...\n\n\n    class Autoencoder(L.LightningModule):\n        def __init__(self, encoder, decoder, *args, **kwargs):\n            super().__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n\n\n    autoencoder = Autoencoder(Encoder(), Decoder())\n\nOnce the autoencoder has trained, pull out the relevant weights for your torch nn.Module:\n\n.. code-block:: python\n\n    checkpoint = torch.load(CKPT_PATH)\n    encoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"encoder.\")}\n    decoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"decoder.\")}\n\n\n----\n\n\n*********************\nDisable checkpointing\n*********************\n\nYou can disable checkpointing by passing:\n\n.. testcode::\n\n   trainer = Trainer(enable_checkpointing=False)\n\n----\n\n\n*********************\nResume training state\n*********************\n\nIf you don't just want to load weights, but instead restore the full training, do the following:\n\nCorrect usage:\n\n.. code-block:: python\n\n   model = LitModel()\n   trainer = Trainer()\n\n   # automatically restores model, epoch, step, LR schedulers, etc...\n   trainer.fit(model, ckpt_path=\"path/to/your/checkpoint.ckpt\")\n\n.. warning::\n\n   The argument `resume_from_checkpoint` has been deprecated in versions of PyTorch Lightning >= 1.0.0.\n   To resume training from a checkpoint, use the `ckpt_path` argument in the `fit()` method instead.\n\nIncorrect (deprecated) usage:\n\n.. code-block:: python\n\n   trainer = Trainer(resume_from_checkpoint=\"path/to/your/checkpoint.ckpt\")\n   trainer.fit(model)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing_expert.html", "url_rel_html": "common/checkpointing_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Distributed checkpoints (expert)¶", "rst_text": ":orphan:\n\n.. _checkpointing_expert:\n\n################################\nDistributed checkpoints (expert)\n################################\n\nGenerally, the bigger your model is, the longer it takes to save a checkpoint to disk.\nWith distributed checkpoints (sometimes called sharded checkpoints), you can save and load the state of your training script with multiple GPUs or nodes more efficiently, avoiding memory issues.\n\n\n----\n\n\n*****************************\nSave a distributed checkpoint\n*****************************\n\nThe distributed checkpoint format can be enabled when you train with the :doc:`FSDP strategy <../advanced/model_parallel/fsdp>`.\n\n.. code-block:: python\n\n    import lightning as L\n    from lightning.pytorch.strategies import FSDPStrategy\n\n    # 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\n    strategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n    # 2. Pass the strategy to the Trainer\n    trainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n    # 3. Run the trainer\n    trainer.fit(model)\n\n\nWith ``state_dict_type=\"sharded\"``, each process/GPU will save its own file into a folder at the given path.\nThis reduces memory peaks and speeds up the saving to disk.\n\n.. collapse:: Full example\n\n    .. code-block:: python\n\n        import lightning as L\n        from lightning.pytorch.strategies import FSDPStrategy\n        from lightning.pytorch.demos import LightningTransformer\n\n        model = LightningTransformer()\n\n        strategy = FSDPStrategy(state_dict_type=\"sharded\")\n        trainer = L.Trainer(\n            accelerator=\"cuda\",\n            devices=4,\n            strategy=strategy,\n            max_steps=3,\n        )\n        trainer.fit(model)\n\n\n    Check the contents of the checkpoint folder:\n\n    .. code-block:: bash\n\n        ls -a lightning_logs/version_0/checkpoints/epoch=0-step=3.ckpt/\n\n    .. code-block::\n\n        epoch=0-step=3.ckpt/\n        ├── __0_0.distcp\n        ├── __1_0.distcp\n        ├── __2_0.distcp\n        ├── __3_0.distcp\n        ├── .metadata\n        └── meta.pt\n\n    The ``.distcp`` files contain the tensor shards from each process/GPU. You can see that the size of these files\n    is roughly 1/4 of the total size of the checkpoint since the script distributes the model across 4 GPUs.\n\n\n----\n\n\n*****************************\nLoad a distributed checkpoint\n*****************************\n\nYou can easily load a distributed checkpoint in Trainer if your script uses :doc:`FSDP <../advanced/model_parallel/fsdp>`.\n\n.. code-block:: python\n\n    import lightning as L\n    from lightning.pytorch.strategies import FSDPStrategy\n\n    # 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\n    strategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n    # 2. Pass the strategy to the Trainer\n    trainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n    # 3. Set the checkpoint path to load\n    trainer.fit(model, ckpt_path=\"path/to/checkpoint\")\n\nNote that you can load the distributed checkpoint even if the world size has changed, i.e., you are running on a different number of GPUs than when you saved the checkpoint.\n\n.. collapse:: Full example\n\n    .. code-block:: python\n\n        import lightning as L\n        from lightning.pytorch.strategies import FSDPStrategy\n        from lightning.pytorch.demos import LightningTransformer\n\n        model = LightningTransformer()\n\n        strategy = FSDPStrategy(state_dict_type=\"sharded\")\n        trainer = L.Trainer(\n            accelerator=\"cuda\",\n            devices=2,\n            strategy=strategy,\n            max_steps=5,\n        )\n        trainer.fit(model, ckpt_path=\"lightning_logs/version_0/checkpoints/epoch=0-step=3.ckpt\")\n\n\n.. important::\n\n    If you want to load a distributed checkpoint into a script that doesn't use FSDP (or Trainer at all), then you will have to :ref:`convert it to a single-file checkpoint first <Convert dist-checkpoint>`.\n\n\n----\n\n\n.. _Convert dist-checkpoint:\n\n********************************\nConvert a distributed checkpoint\n********************************\n\nIt is possible to convert a distributed checkpoint to a regular, single-file checkpoint with this utility:\n\n.. code-block:: bash\n\n    python -m lightning.pytorch.utilities.consolidate_checkpoint path/to/my/checkpoint\n\nYou will need to do this for example if you want to load the checkpoint into a script that doesn't use FSDP, or need to export the checkpoint to a different format for deployment, evaluation, etc.\n\n.. note::\n\n    All tensors in the checkpoint will be converted to CPU tensors, and no GPUs are required to run the conversion command.\n    This function assumes you have enough free CPU memory to hold the entire checkpoint in memory.\n\n.. collapse:: Full example\n\n    Assuming you have saved a checkpoint ``epoch=0-step=3.ckpt`` using the examples above, run the following command to convert it:\n\n    .. code-block:: bash\n\n        cd lightning_logs/version_0/checkpoints\n        python -m lightning.pytorch.utilities.consolidate_checkpoint epoch=0-step=3.ckpt\n\n    This saves a new file ``epoch=0-step=3.ckpt.consolidated`` next to the sharded checkpoint which you can load normally in PyTorch:\n\n    .. code-block:: python\n\n        import torch\n\n        checkpoint = torch.load(\"epoch=0-step=3.ckpt.consolidated\")\n        print(list(checkpoint.keys()))\n        print(checkpoint[\"state_dict\"][\"model.transformer.decoder.layers.31.norm1.weight\"])\n\n\n|\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing_intermediate.html", "url_rel_html": "common/checkpointing_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Customize checkpointing behavior (intermediate)¶", "rst_text": ""}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/checkpointing_migration.html", "url_rel_html": "common/checkpointing_migration.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/checkpointing_migration.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Upgrading checkpoints (intermediate)¶", "rst_text": ":orphan:\n\n.. _checkpointing_intermediate_2:\n\n####################################\nUpgrading checkpoints (intermediate)\n####################################\n**Audience:** Users who are upgrading Lightning and their code and want to reuse their old checkpoints.\n\n----\n\n**************************************\nResume training from an old checkpoint\n**************************************\n\nNext to the model weights and trainer state, a Lightning checkpoint contains the version number of Lightning with which the checkpoint was saved.\nWhen you load a checkpoint file, either by resuming training\n\n.. code-block:: python\n\n    trainer = Trainer(...)\n    trainer.fit(model, ckpt_path=\"path/to/checkpoint.ckpt\")\n\nor by loading the state directly into your model,\n\n.. code-block:: python\n\n    model = LitModel.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n\nLightning will automatically recognize that it is from an older version and migrates the internal structure so it can be loaded properly.\nThis is done without any action required by the user.\n\n----\n\n************************************\nUpgrade checkpoint files permanently\n************************************\n\nWhen Lightning loads a checkpoint, it applies the version migration on-the-fly as explained above, but it does not modify your checkpoint files.\nYou can upgrade checkpoint files permanently with the following command\n\n.. code-block::\n\n    python -m lightning.pytorch.utilities.upgrade_checkpoint path/to/model.ckpt\n\n\nor a folder with multiple files:\n\n.. code-block::\n\n    python -m lightning.pytorch.utilities.upgrade_checkpoint /path/to/checkpoints/folder\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/console_logs.html", "url_rel_html": "common/console_logs.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/console_logs.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Console logging¶", "rst_text": "###############\nConsole logging\n###############\n**Audience:** Engineers looking to capture more visible logs.\n\n----\n\n*******************\nEnable console logs\n*******************\nLightning logs useful information about the training process and user warnings to the console.\nYou can retrieve the Lightning console logger and change it to your liking. For example, adjust the logging level\nor redirect output for certain modules to log files:\n\n.. testcode::\n\n    import logging\n\n    # configure logging at the root level of Lightning\n    logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n    # configure logging on module level, redirect to file\n    logger = logging.getLogger(\"lightning.pytorch.core\")\n    logger.addHandler(logging.FileHandler(\"core.log\"))\n\nRead more about custom Python logging `here <https://docs.python.org/3/library/logging.html>`_.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/early_stopping.html", "url_rel_html": "common/early_stopping.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/early_stopping.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Early Stopping¶", "rst_text": ".. testsetup:: *\n\n    from lightning.pytorch.callbacks.early_stopping import EarlyStopping, EarlyStoppingReason\n    from lightning.pytorch import Trainer, LightningModule\n\n.. _early_stopping:\n\n\n##############\nEarly Stopping\n##############\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/yt/Trainer+flags+19-+early+stopping_1.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/yt_thumbs/thumb_earlystop.png\n    :width: 400\n    :muted:\n\n\n***********************\nStopping an Epoch Early\n***********************\n\nYou can stop and skip the rest of the current epoch early by overriding :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_train_batch_start` to return ``-1`` when some condition is met.\n\nIf you do this repeatedly, for every epoch you had originally requested, then this will stop your entire training.\n\n\n**********************\nEarlyStopping Callback\n**********************\n\nThe :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback can be used to monitor a metric and stop the training when no improvement is observed.\n\nTo enable it:\n\n- Import :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback.\n- Log the metric you want to monitor using :meth:`~lightning.pytorch.core.LightningModule.log` method.\n- Init the callback, and set ``monitor`` to the logged metric of your choice.\n- Set the ``mode`` based on the metric needs to be monitored.\n- Pass the :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback to the :class:`~lightning.pytorch.trainer.trainer.Trainer` callbacks flag.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\n\n    class LitModel(LightningModule):\n        def validation_step(self, batch, batch_idx):\n            loss = ...\n            self.log(\"val_loss\", loss)\n\n\n    model = LitModel()\n    trainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n    trainer.fit(model)\n\nYou can customize the callbacks behaviour by changing its parameters.\n\n.. testcode::\n\n    early_stop_callback = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")\n    trainer = Trainer(callbacks=[early_stop_callback])\n\n\nAdditional parameters that stop training at extreme points:\n\n- ``stopping_threshold``: Stops training immediately once the monitored quantity reaches this threshold.\n  It is useful when we know that going beyond a certain optimal value does not further benefit us.\n- ``divergence_threshold``: Stops training as soon as the monitored quantity becomes worse than this threshold.\n  When reaching a value this bad, we believes the model cannot recover anymore and it is better to stop early and run with different initial conditions.\n- ``check_finite``: When turned on, it stops training if the monitored metric becomes NaN or infinite.\n- ``check_on_train_epoch_end``: When turned on, it checks the metric at the end of a training epoch. Use this only when you are monitoring any metric logged within\n  training-specific hooks on epoch-level.\n\nAfter training completes, you can programmatically check why early stopping occurred using the ``stopping_reason``\nattribute, which returns an ``EarlyStoppingReason`` enum value.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import EarlyStopping\n    from lightning.pytorch.callbacks.early_stopping import EarlyStoppingReason\n\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\n    trainer = Trainer(callbacks=[early_stopping])\n    trainer.fit(model)\n\n    # Check why training stopped\n    if early_stopping.stopping_reason == EarlyStoppingReason.PATIENCE_EXHAUSTED:\n        print(\"Training stopped due to patience exhaustion\")\n    elif early_stopping.stopping_reason == EarlyStoppingReason.STOPPING_THRESHOLD:\n        print(\"Training stopped due to reaching stopping threshold\")\n    elif early_stopping.stopping_reason == EarlyStoppingReason.NOT_STOPPED:\n        print(\"Training completed normally without early stopping\")\n\n    # Access human-readable message\n    if early_stopping.stopping_reason_message:\n        print(f\"Details: {early_stopping.stopping_reason_message}\")\n\nThe available stopping reasons are:\n\n- ``NOT_STOPPED``: Training completed normally without early stopping\n- ``STOPPING_THRESHOLD``: Training stopped because the monitored metric reached the stopping threshold\n- ``DIVERGENCE_THRESHOLD``: Training stopped because the monitored metric exceeded the divergence threshold\n- ``PATIENCE_EXHAUSTED``: Training stopped because the metric didn't improve for the specified patience\n- ``NON_FINITE_METRIC``: Training stopped because the monitored metric became NaN or infinite\n\nIn case you need early stopping in a different part of training, subclass :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping`\nand change where it is called:\n\n.. testcode::\n\n    class MyEarlyStopping(EarlyStopping):\n        def on_validation_end(self, trainer, pl_module):\n            # override this to disable early stopping at the end of val loop\n            pass\n\n        def on_train_end(self, trainer, pl_module):\n            # instead, do it at the end of training loop\n            self._run_early_stopping_check(trainer)\n\n.. note::\n   The :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback runs\n   at the end of every validation epoch by default. However, the frequency of validation\n   can be modified by setting various parameters in the :class:`~lightning.pytorch.trainer.trainer.Trainer`,\n   for example :paramref:`~lightning.pytorch.trainer.trainer.Trainer.check_val_every_n_epoch`\n   and :paramref:`~lightning.pytorch.trainer.trainer.Trainer.val_check_interval`.\n   It must be noted that the ``patience`` parameter counts the number of\n   validation checks with no improvement, and not the number of training epochs.\n   Therefore, with parameters ``check_val_every_n_epoch=10`` and ``patience=3``, the trainer\n   will perform at least 40 training epochs before being stopped.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/evaluation.html", "url_rel_html": "common/evaluation.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/evaluation.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Add validation and test datasets¶", "rst_text": ".. _val-test-dataset:\n\n********************************\nAdd validation and test datasets\n********************************\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Basic\n   :description: Add a validation and test loop to avoid overfitting.\n   :col_css: col-md-6\n   :button_link: evaluation_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn about more complex validation and test workflows\n   :col_css: col-md-6\n   :button_link: evaluation_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/evaluation_basic.html", "url_rel_html": "common/evaluation_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/evaluation_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Validate and test a model (basic)¶", "rst_text": ":orphan:\n\n#################################\nValidate and test a model (basic)\n#################################\n**Audience**: Users who want to add a validation loop to avoid overfitting\n\n----\n\n***************\nAdd a test loop\n***************\nTo make sure a model can generalize to an unseen dataset (ie: to publish a paper or in a production environment) a dataset is normally split into two parts, the *train* split and the *test* split.\n\nThe test set is **NOT** used during training, it is **ONLY** used once the model has been trained to see how the model will do in the real-world.\n\n----\n\nFind the train and test splits\n==============================\nDatasets come with two splits. Refer to the dataset documentation to find the *train* and *test* splits.\n\n.. code-block:: python\n\n   import torch.utils.data as data\n   from torchvision import datasets\n   import torchvision.transforms as transforms\n\n   # Load data sets\n   transform = transforms.ToTensor()\n   train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n   test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n\n----\n\nDefine the test loop\n====================\nTo add a test loop, implement the **test_step** method of the LightningModule\n\n.. code:: python\n\n    class LitAutoEncoder(L.LightningModule):\n        def training_step(self, batch, batch_idx):\n            ...\n\n        def test_step(self, batch, batch_idx):\n            # this is the test loop\n            x, _ = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            test_loss = F.mse_loss(x_hat, x)\n            self.log(\"test_loss\", test_loss)\n\n----\n\nTrain with the test loop\n========================\nOnce the model has finished training, call **.test**\n\n.. code-block:: python\n\n   from torch.utils.data import DataLoader\n\n   # initialize the Trainer\n   trainer = Trainer()\n\n   # test the model\n   trainer.test(model, dataloaders=DataLoader(test_set))\n\n----\n\n*********************\nAdd a validation loop\n*********************\nDuring training, it's common practice to use a small portion of the train split to determine when the model has finished training.\n\n----\n\nSplit the training data\n=======================\nAs a rule of thumb, we use 20% of the training set as the **validation set**. This number varies from dataset to dataset.\n\n.. code-block:: python\n\n   # use 20% of training data for validation\n   train_set_size = int(len(train_set) * 0.8)\n   valid_set_size = len(train_set) - train_set_size\n\n   # split the train set into two\n   seed = torch.Generator().manual_seed(42)\n   train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n\n----\n\nDefine the validation loop\n==========================\nTo add a validation loop, implement the **validation_step** method of the LightningModule\n\n.. code:: python\n\n    class LitAutoEncoder(L.LightningModule):\n        def training_step(self, batch, batch_idx):\n            ...\n\n        def validation_step(self, batch, batch_idx):\n            # this is the validation loop\n            x, _ = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            val_loss = F.mse_loss(x_hat, x)\n            self.log(\"val_loss\", val_loss)\n\n----\n\nTrain with the validation loop\n==============================\nTo run the validation loop, pass in the validation set to **.fit**\n\n.. code-block:: python\n\n   from torch.utils.data import DataLoader\n\n   train_loader = DataLoader(train_set)\n   valid_loader = DataLoader(valid_set)\n   model = LitAutoEncoder(...)\n\n   # train with both splits\n   trainer = L.Trainer()\n   trainer.fit(model, train_loader, valid_loader)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/evaluation_intermediate.html", "url_rel_html": "common/evaluation_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/evaluation_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Validate and test a model (intermediate)¶", "rst_text": ".. _test_set:\n\n:orphan:\n\n########################################\nValidate and test a model (intermediate)\n########################################\n\nDuring and after training we need a way to evaluate our models to make sure they are not overfitting while training and\ngeneralize well on unseen or real-world data. There are generally 2 stages of evaluation: validation and testing. To some\ndegree they serve the same purpose, to make sure models works on real data but they have some practical differences.\n\nValidation is usually done during training, traditionally after each training epoch. It can be used for hyperparameter optimization or tracking model performance during training.\nIt's a part of the training process.\n\nTesting is usually done once we are satisfied with the training and only with the best model selected from the validation metrics.\n\nLet's see how these can be performed with Lightning.\n\n*******\nTesting\n*******\n\nLightning allows the user to test their models with any compatible test dataloaders. This can be done before/after training\nand is completely agnostic to :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit` call. The logic used here is defined under\n:meth:`~lightning.pytorch.core.LightningModule.test_step`.\n\nTesting is performed using the ``Trainer`` object's ``.test()`` method.\n\n.. automethod:: lightning.pytorch.trainer.Trainer.test\n    :noindex:\n\n\nTest after Fit\n==============\n\nTo run the test set after training completes, use this method.\n\n.. code-block:: python\n\n    # run full training\n    trainer.fit(model)\n\n    # (1) load the best checkpoint automatically (lightning tracks this for you during .fit())\n    trainer.test(ckpt_path=\"best\")\n\n    # (2) load the last available checkpoint (only works if `ModelCheckpoint(save_last=True)`)\n    trainer.test(ckpt_path=\"last\")\n\n    # (3) test using a specific checkpoint\n    trainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\n\n    # (4) test with an explicit model (will use this model and not load a checkpoint)\n    trainer.test(model)\n\n.. warning::\n\n    It is recommended to test with ``Trainer(devices=1)`` since distributed strategies such as DDP\n    use :class:`~torch.utils.data.distributed.DistributedSampler` internally, which replicates some samples to\n    make sure all devices have same batch size in case of uneven inputs. This is helpful to make sure\n    benchmarking for research papers is done the right way.\n\n\nTest Multiple Models\n====================\n\nYou can run the test set on multiple models using the same trainer instance.\n\n.. code-block:: python\n\n    model1 = LitModel()\n    model2 = GANModel()\n\n    trainer = Trainer()\n    trainer.test(model1)\n    trainer.test(model2)\n\n\nTest Pre-Trained Model\n======================\n\nTo run the test set on a pre-trained model, use this method.\n\n.. code-block:: python\n\n    model = MyLightningModule.load_from_checkpoint(\n        checkpoint_path=\"/path/to/pytorch_checkpoint.ckpt\",\n        hparams_file=\"/path/to/experiment/version/hparams.yaml\",\n        map_location=None,\n    )\n\n    # init trainer with whatever options\n    trainer = Trainer(...)\n\n    # test (pass in the model)\n    trainer.test(model)\n\nIn this  case, the options you pass to trainer will be used when\nrunning the test set (ie: 16-bit, dp, ddp, etc...)\n\n\nTest with Additional DataLoaders\n================================\n\nYou can still run inference on a test dataset even if the :meth:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` method hasn't been\ndefined within your :doc:`lightning module <../common/lightning_module>` instance. This would be the case when your test data\nis not available at the time your model was declared.\n\n.. code-block:: python\n\n    # setup your data loader\n    test_dataloader = DataLoader(...)\n\n    # test (pass in the loader)\n    trainer.test(dataloaders=test_dataloader)\n\nYou can either pass in a single dataloader or a list of them. This optional named\nparameter can be used in conjunction with any of the above use cases. Additionally,\nyou can also pass in an :doc:`datamodules <../data/datamodule>` that have overridden the\n:ref:`datamodule_test_dataloader_label` method.\n\n.. code-block:: python\n\n    class MyDataModule(L.LightningDataModule):\n        ...\n\n        def test_dataloader(self):\n            return DataLoader(...)\n\n\n    # setup your datamodule\n    dm = MyDataModule(...)\n\n    # test (pass in datamodule)\n    trainer.test(datamodule=dm)\n\n\nTest with Multiple DataLoaders\n==============================\n\nWhen you need to evaluate your model on multiple test datasets simultaneously (e.g., different domains, conditions, or\nevaluation scenarios), PyTorch Lightning supports multiple test dataloaders out of the box.\n\nTo use multiple test dataloaders, simply return a list of dataloaders from your ``test_dataloader()`` method:\n\n.. code-block:: python\n\n    class LitModel(L.LightningModule):\n        def test_dataloader(self):\n            return [\n                DataLoader(clean_test_dataset, batch_size=32),\n                DataLoader(noisy_test_dataset, batch_size=32),\n                DataLoader(adversarial_test_dataset, batch_size=32),\n            ]\n\nWhen using multiple test dataloaders, your ``test_step`` method **must** include a ``dataloader_idx`` parameter:\n\n.. code-block:: python\n\n    def test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n\n        # Use dataloader_idx to handle different test scenarios\n        return {'test_loss': loss}\n\nLogging Metrics Per Dataloader\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLightning provides automatic support for logging metrics per dataloader:\n\n.. code-block:: python\n\n    def test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (y_hat.argmax(dim=1) == y).float().mean()\n\n        # Lightning automatically adds \"/dataloader_idx_X\" suffix\n        self.log('test_loss', loss, add_dataloader_idx=True)\n        self.log('test_acc', acc, add_dataloader_idx=True)\n\n        return loss\n\nThis will create metrics like ``test_loss/dataloader_idx_0``, ``test_loss/dataloader_idx_1``, etc.\n\nFor more meaningful metric names, you can use custom naming where you need to make sure that individual names are\nunique across dataloaders.\n\n.. code-block:: python\n\n    def test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n        # Define meaningful names for each dataloader\n        dataloader_names = {0: \"clean\", 1: \"noisy\", 2: \"adversarial\"}\n        dataset_name = dataloader_names.get(dataloader_idx, f\"dataset_{dataloader_idx}\")\n\n        # Log with custom names\n        self.log(f'test_loss_{dataset_name}', loss, add_dataloader_idx=False)\n        self.log(f'test_acc_{dataset_name}', acc, add_dataloader_idx=False)\n\nProcessing Entire Datasets Per Dataloader\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo perform calculations on the entire test dataset for each dataloader (e.g., computing overall metrics, creating\nvisualizations), accumulate results during ``test_step`` and process them in ``on_test_epoch_end``:\n\n.. code-block:: python\n\n    class LitModel(L.LightningModule):\n        def __init__(self):\n            super().__init__()\n            # Store outputs per dataloader\n            self.test_outputs = {}\n\n        def test_step(self, batch, batch_idx, dataloader_idx: int = 0):\n            x, y = batch\n            y_hat = self(x)\n            loss = F.cross_entropy(y_hat, y)\n\n            # Initialize and store results\n            if dataloader_idx not in self.test_outputs:\n                self.test_outputs[dataloader_idx] = {'predictions': [], 'targets': []}\n            self.test_outputs[dataloader_idx]['predictions'].append(y_hat)\n            self.test_outputs[dataloader_idx]['targets'].append(y)\n            return loss\n\n        def on_test_epoch_end(self):\n            for dataloader_idx, outputs in self.test_outputs.items():\n                # Concatenate all predictions and targets for this dataloader\n                all_predictions = torch.cat(outputs['predictions'], dim=0)\n                all_targets = torch.cat(outputs['targets'], dim=0)\n\n                # Calculate metrics on the entire dataset, log and create visualizations\n                overall_accuracy = (all_predictions.argmax(dim=1) == all_targets).float().mean()\n                self.log(f'test_overall_acc_dataloader_{dataloader_idx}', overall_accuracy)\n                self._save_results(all_predictions, all_targets, dataloader_idx)\n\n            self.test_outputs.clear()\n\n.. note::\n    When using multiple test dataloaders, ``trainer.test()`` returns a list of results, one for each dataloader:\n\n    .. code-block:: python\n\n        results = trainer.test(model)\n        print(f\"Results from {len(results)} test dataloaders:\")\n        for i, result in enumerate(results):\n            print(f\"Dataloader {i}: {result}\")\n\n----------\n\n**********\nValidation\n**********\n\nLightning allows the user to validate their models with any compatible ``val dataloaders``. This can be done before/after training.\nThe logic associated to the validation is defined within the :meth:`~lightning.pytorch.core.LightningModule.validation_step`.\n\nApart from this ``.validate`` has same API as ``.test``, but would rely respectively on :meth:`~lightning.pytorch.core.LightningModule.validation_step` and :meth:`~lightning.pytorch.core.LightningModule.test_step`.\n\n.. note::\n    ``.validate`` method uses the same validation logic being used under validation happening within\n    :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit` call.\n\n.. warning::\n\n    When using ``trainer.validate()``, it is recommended to use ``Trainer(devices=1)`` since distributed strategies such as DDP\n    uses :class:`~torch.utils.data.distributed.DistributedSampler` internally, which replicates some samples to\n    make sure all devices have same batch size in case of uneven inputs. This is helpful to make sure\n    benchmarking for research papers is done the right way.\n\n.. automethod:: lightning.pytorch.trainer.Trainer.validate\n    :noindex:\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/hooks.html", "url_rel_html": "common/hooks.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/hooks.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Hooks in PyTorch Lightning¶", "rst_text": "##########################\nHooks in PyTorch Lightning\n##########################\n\nHooks in Pytorch Lightning allow you to customize the training, validation, and testing logic of your models. They\nprovide a way to insert custom behavior at specific points during the training process without modifying the core\ntraining loop. There are several categories of hooks available in PyTorch Lightning:\n\n1. **Setup/Teardown Hooks**: Called at the beginning and end of training phases\n2. **Training Hooks**: Called during the training loop\n3. **Validation Hooks**: Called during validation\n4. **Test Hooks**: Called during testing\n5. **Prediction Hooks**: Called during prediction\n6. **Optimizer Hooks**: Called around optimizer operations\n7. **Checkpoint Hooks**: Called during checkpoint save/load operations\n8. **Exception Hooks**: Called when exceptions occur\n\nNearly all hooks can be implemented in three places within your code:\n\n- **LightningModule**: The main module where you define your model and training logic.\n- **Callbacks**: Custom classes that can be passed to the Trainer to handle specific events.\n- **Strategy**: Custom strategies for distributed training.\n\nImportantly, because logic can be place in the same hook but in different places the call order of hooks is in\nimportant to understand. The following order is always used:\n\n1. Callbacks, called in the order they are passed to the Trainer.\n2. ``LightningModule``\n3. Strategy\n\n.. testcode::\n\n    from lightning.pytorch import Trainer\n    from lightning.pytorch.callbacks import Callback\n    from lightning.pytorch.demos import BoringModel\n\n    class MyModel(BoringModel):\n        def on_train_start(self):\n            print(\"Model: Training is starting!\")\n\n    class MyCallback(Callback):\n        def on_train_start(self, trainer, pl_module):\n            print(\"Callback: Training is starting!\")\n\n    model = MyModel()\n    callback = MyCallback()\n    trainer = Trainer(callbacks=[callback], logger=False, max_epochs=1)\n    trainer.fit(model)\n\n.. testoutput::\n   :hide:\n   :options: +ELLIPSIS, +NORMALIZE_WHITESPACE\n\n    ┏━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n    ┃   ┃ Name  ┃ Type   ┃ Params ┃ Mode  ┃ FLOPs ┃\n    ┡━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n    │ 0 │ layer │ Linear │     66 │ train │     0 │\n    └───┴───────┴────────┴────────┴───────┴───────┘\n    ...\n    Callback: Training is starting!\n    Model: Training is starting!\n    Epoch 0/0  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64/64 ...\n\n\n.. note::\n   There are a few exceptions to this pattern:\n\n   - **on_train_epoch_end**: Non-monitoring callbacks are called first, then ``LightningModule``, then monitoring callbacks\n   - **Optimizer hooks** (on_before_backward, on_after_backward, on_before_optimizer_step): Only callbacks and ``LightningModule`` are called\n   - Some internal hooks may only call ``LightningModule`` or Strategy\n\n************************\nTraining Loop Hook Order\n************************\n\nThe following diagram shows the execution order of hooks during a typical training loop e.g. calling `trainer.fit()`,\nwith the source of each hook indicated:\n\n.. code-block:: text\n\n    Training Process Flow:\n\n    trainer.fit()\n    │\n    ├── setup(stage=\"fit\")\n    │   ├── [LightningDataModule]\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   ├── [LightningModule.configure_shared_model()]\n    │   ├── [LightningModule.configure_model()]\n    │   ├── Strategy.restore_checkpoint_before_setup\n    │   │   ├── [LightningModule.on_load_checkpoint()]\n    │   │   ├── [LightningModule.load_state_dict()]\n    │   │   ├── [LightningDataModule.load_state_dict()]\n    │   │   ├── [Callbacks.on_load_checkpoint()]\n    │   │   └── [Callbacks.load_state_dict()]\n    │   └── [Strategy]\n    │\n    ├── on_fit_start()\n    │   ├── [Callbacks]\n    │   └── [LightningModule]\n    │\n    ├── Strategy.restore_checkpoint_after_setup\n    │   ├── [LightningModule.on_load_checkpoint()]\n    │   ├── [LightningModule.load_state_dict()]\n    │   ├── [LightningDataModule.load_state_dict()]\n    │   ├── [Callbacks.on_load_checkpoint()]\n    │   └── [Callbacks.load_state_dict()]\n    │\n    ├── on_sanity_check_start()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │   ├── on_validation_start()\n    │   │   ├── [Callbacks]\n    │   │   ├── [LightningModule]\n    │   │   └── [Strategy]\n    │   ├── on_validation_epoch_start()\n    │   │   ├── [Callbacks]\n    │   │   ├── [LightningModule]\n    │   │   └── [Strategy]\n    │   │   ├── [for each validation batch]\n    │   │   │   ├── on_validation_batch_start()\n    │   │   │   │   ├── [Callbacks]\n    │   │   │   │   ├── [LightningModule]\n    │   │   │   │   └── [Strategy]\n    │   │   │   └── on_validation_batch_end()\n    │   │   │       ├── [Callbacks]\n    │   │   │       ├── [LightningModule]\n    │   │   │       └── [Strategy]\n    │   │   └── [end validation batches]\n    │   ├── on_validation_epoch_end()\n    │   │   ├── [Callbacks]\n    │   │   ├── [LightningModule]\n    │   │   └── [Strategy]\n    │   └── on_validation_end()\n    │       ├── [Callbacks]\n    │       ├── [LightningModule]\n    │       └── [Strategy]\n    ├── on_sanity_check_end()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │\n    ├── on_train_start()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │\n    ├── [Training Epochs Loop]\n    │   │\n    │   ├── on_train_epoch_start()\n    │   │   ├── [Callbacks]\n    │   │   └── [LightningModule]\n    │   │\n    │   ├── [Training Batches Loop]\n    │   │   │\n    │   │   ├── on_train_batch_start()\n    │   │   │   ├── [Callbacks]\n    │   │   │   ├── [LightningModule]\n    │   │   │   └── [Strategy]\n    │   │   │\n    │   │   ├── [Forward Pass - training_step()]\n    │   │   │   └── [Strategy only]\n    │   │   │\n    │   │   ├── on_before_zero_grad()\n    │   │   │   ├── [Callbacks]\n    │   │   │   └── [LightningModule]\n    │   │   │\n    │   │   ├── optimizer_zero_grad()\n    │   │   │   └── [LightningModule only - optimizer_zero_grad()]\n    │   │   │\n    │   │   ├── [Backward Pass - Strategy.backward()]\n    │   │   │   ├── on_before_backward()\n    │   │   │   │   ├── [Callbacks]\n    │   │   │   │   └── [LightningModule]\n    │   │   │   ├── LightningModule.backward()\n    │   │   │   └── on_after_backward()\n    │   │   │       ├── [Callbacks]\n    │   │   │       └── [LightningModule]\n    │   │   │\n    │   │   ├── on_before_optimizer_step()\n    │   │   │   ├── [Callbacks]\n    │   │   │   └── [LightningModule]\n    │   │   │\n    │   │   ├── [Optimizer Step]\n    │   │   │   └── [LightningModule only - optimizer_step()]\n    │   │   │\n    │   │   └── on_train_batch_end()\n    │   │       ├── [Callbacks]\n    │   │       └── [LightningModule]\n    │   │\n    │   │   [Optional: Validation during training]\n    │   │   ├── on_validation_start()\n    │   │   │   ├── [Callbacks]\n    │   │   │   ├── [LightningModule]\n    │   │   │   └── [Strategy]\n    │   │   ├── on_validation_epoch_start()\n    │   │   │   ├── [Callbacks]\n    │   │   │   ├── [LightningModule]\n    │   │   │   └── [Strategy]\n    │   │   │   ├── [for each validation batch]\n    │   │   │   │   ├── on_validation_batch_start()\n    │   │   │   │   │   ├── [Callbacks]\n    │   │   │   │   │   ├── [LightningModule]\n    │   │   │   │   │   └── [Strategy]\n    │   │   │   │   └── on_validation_batch_end()\n    │   │   │   │       ├── [Callbacks]\n    │   │   │   │       ├── [LightningModule]\n    │   │   │   │       └── [Strategy]\n    │   │   │   └── [end validation batches]\n    │   │   ├── on_validation_epoch_end()\n    │   │   │   ├── [Callbacks]\n    │   │   │   ├── [LightningModule]\n    │   │   │   └── [Strategy]\n    │   │   └── on_validation_end()\n    │   │       ├── [Callbacks]\n    │   │       ├── [LightningModule]\n    │   │       └── [Strategy]\n    │   │\n    │   └── on_train_epoch_end() **SPECIAL CASE**\n    │       ├── [Callbacks - Non-monitoring only]\n    │       ├── [LightningModule]\n    │       └── [Callbacks - Monitoring only]\n    │\n    ├── [End Training Epochs]\n    │\n    ├── on_train_end()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │\n    └── teardown(stage=\"fit\")\n        ├── [Strategy]\n        ├── on_fit_end()\n        │   ├── [Callbacks]\n        │   └── [LightningModule]\n        ├── [LightningDataModule]\n        ├── [Callbacks]\n        └── [LightningModule]\n\n***********************\nTesting Loop Hook Order\n***********************\n\nWhen running tests with ``trainer.test()``:\n\n.. code-block:: text\n\n    trainer.test()\n    │\n    ├── setup(stage=\"test\")\n    │   └── [Callbacks only]\n    ├── on_test_start()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │\n    ├── [Test Epochs Loop]\n    │   │\n    │   ├── on_test_epoch_start()\n    │   │   ├── [Callbacks]\n    │   │   ├── [LightningModule]\n    │   │   └── [Strategy]\n    │   │\n    │   ├── [Test Batches Loop]\n    │   │   │\n    │   │   ├── on_test_batch_start()\n    │   │   │   ├── [Callbacks]\n    │   │   │   ├── [LightningModule]\n    │   │   │   └── [Strategy]\n    │   │   │\n    │   │   └── on_test_batch_end()\n    │   │       ├── [Callbacks]\n    │   │       ├── [LightningModule]\n    │   │       └── [Strategy]\n    │   │\n    │   └── on_test_epoch_end()\n    │       ├── [Callbacks]\n    │       ├── [LightningModule]\n    │       └── [Strategy]\n    │\n    ├── on_test_end()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    └── teardown(stage=\"test\")\n        └── [Callbacks only]\n\n**************************\nPrediction Loop Hook Order\n**************************\n\nWhen running predictions with ``trainer.predict()``:\n\n.. code-block:: text\n\n    trainer.predict()\n    │\n    ├── setup(stage=\"predict\")\n    │   └── [Callbacks only]\n    ├── on_predict_start()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    │\n    ├── [Prediction Epochs Loop]\n    │   │\n    │   ├── on_predict_epoch_start()\n    │   │   ├── [Callbacks]\n    │   │   └── [LightningModule]\n    │   │\n    │   ├── [Prediction Batches Loop]\n    │   │   │\n    │   │   ├── on_predict_batch_start()\n    │   │   │   ├── [Callbacks]\n    │   │   │   └── [LightningModule]\n    │   │   │\n    │   │   └── on_predict_batch_end()\n    │   │       ├── [Callbacks]\n    │   │       └── [LightningModule]\n    │   │\n    │   └── on_predict_epoch_end()\n    │       ├── [Callbacks]\n    │       └── [LightningModule]\n    │\n    ├── on_predict_end()\n    │   ├── [Callbacks]\n    │   ├── [LightningModule]\n    │   └── [Strategy]\n    └── teardown(stage=\"predict\")\n        └── [Callbacks only]\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/hyperparameters.html", "url_rel_html": "common/hyperparameters.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/hyperparameters.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Configure hyperparameters from the CLI¶", "rst_text": ":orphan:\n\nConfigure hyperparameters from the CLI\n--------------------------------------\n\nYou can use any CLI tool you want with Lightning.\nFor beginners, we recommend using Python's built-in argument parser.\n\n\n----\n\n\nArgumentParser\n^^^^^^^^^^^^^^\n\nThe :class:`~argparse.ArgumentParser` is a built-in feature in Python that let's you build CLI programs.\nYou can use it to make hyperparameters and other training settings available from the command line:\n\n.. code-block:: python\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n\n    # Trainer arguments\n    parser.add_argument(\"--devices\", type=int, default=2)\n\n    # Hyperparameters for the model\n    parser.add_argument(\"--layer_1_dim\", type=int, default=128)\n\n    # Parse the user inputs and defaults (returns a argparse.Namespace)\n    args = parser.parse_args()\n\n    # Use the parsed arguments in your program\n    trainer = Trainer(devices=args.devices)\n    model = MyModel(layer_1_dim=args.layer_1_dim)\n\nThis allows you to call your program like so:\n\n.. code-block:: bash\n\n    python trainer.py --layer_1_dim 64 --devices 1\n\n----\n\n\nLightningCLI\n^^^^^^^^^^^^\n\nPython's argument parser works well for simple use cases, but it can become cumbersome to maintain for larger projects.\nFor example, every time you add, change, or delete an argument from your model, you will have to add, edit, or remove the corresponding ``parser.add_argument`` code.\nThe :doc:`Lightning CLI <../cli/lightning_cli>` provides a seamless integration with the Trainer and LightningModule for which the CLI arguments get generated automatically for you!\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/lightning_module.html", "url_rel_html": "common/lightning_module.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/lightning_module.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "LightningModule¶", "rst_text": ".. role:: hidden\n    :class: hidden-section\n\n.. _lightning_module:\n\n###############\nLightningModule\n###############\n\nA :class:`~lightning.pytorch.core.LightningModule` organizes your PyTorch code into 6 sections:\n\n- Initialization (``__init__`` and :meth:`~lightning.pytorch.core.hooks.ModelHooks.setup`).\n- Train Loop (:meth:`~lightning.pytorch.core.LightningModule.training_step`)\n- Validation Loop (:meth:`~lightning.pytorch.core.LightningModule.validation_step`)\n- Test Loop (:meth:`~lightning.pytorch.core.LightningModule.test_step`)\n- Prediction Loop (:meth:`~lightning.pytorch.core.LightningModule.predict_step`)\n- Optimizers and LR Schedulers (:meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`)\n\nWhen you convert to use Lightning, the code IS NOT abstracted - just organized.\nAll the other code that's not in the :class:`~lightning.pytorch.core.LightningModule`\nhas been automated for you by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\n\n|\n\n    .. code-block:: python\n\n        net = MyLightningModuleNet()\n        trainer = Trainer()\n        trainer.fit(net)\n\nThere are no ``.cuda()`` or ``.to(device)`` calls required. Lightning does these for you.\n\n|\n\n    .. code-block:: python\n\n        # don't do in Lightning\n        x = torch.Tensor(2, 3)\n        x = x.cuda()\n        x = x.to(device)\n\n        # do this instead\n        x = x  # leave it alone!\n\n        # or to init a new tensor\n        new_x = torch.Tensor(2, 3)\n        new_x = new_x.to(x)\n\nWhen running under a distributed strategy, Lightning handles the distributed sampler for you by default.\n\n|\n\n    .. code-block:: python\n\n        # Don't do in Lightning...\n        data = MNIST(...)\n        sampler = DistributedSampler(data)\n        DataLoader(data, sampler=sampler)\n\n        # do this instead\n        data = MNIST(...)\n        DataLoader(data)\n\nA :class:`~lightning.pytorch.core.LightningModule` is a :class:`torch.nn.Module` but with added functionality. Use it as such!\n\n|\n\n    .. code-block:: python\n\n        net = Net.load_from_checkpoint(PATH)\n        net.freeze()\n        out = net(x)\n\nThus, to use Lightning, you just need to organize your code which takes about 30 minutes,\n(and let's be real, you probably should do anyway).\n\n------------\n\n***************\nStarter Example\n***************\n\nHere are the only required methods.\n\n.. code-block:: python\n\n    import lightning as L\n    import torch\n\n    from lightning.pytorch.demos import Transformer\n\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n\n        def forward(self, inputs, target):\n            return self.model(inputs, target)\n\n        def training_step(self, batch, batch_idx):\n            inputs, target = batch\n            output = self(inputs, target)\n            loss = torch.nn.functional.nll_loss(output, target.view(-1))\n            return loss\n\n        def configure_optimizers(self):\n            return torch.optim.SGD(self.model.parameters(), lr=0.1)\n\nWhich you can train by doing:\n\n.. code-block:: python\n\n    from lightning.pytorch.demos import WikiText2\n    from torch.utils.data import DataLoader\n\n    dataset = WikiText2()\n    dataloader = DataLoader(dataset)\n    model = LightningTransformer(vocab_size=dataset.vocab_size)\n\n    trainer = L.Trainer(fast_dev_run=100)\n    trainer.fit(model=model, train_dataloaders=dataloader)\n\nThe LightningModule has many convenient methods, but the core ones you need to know about are:\n\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   * - Name\n     - Description\n   * - ``__init__`` and :meth:`~lightning.pytorch.core.hooks.ModelHooks.setup`\n     - Define initialization here\n   * - :meth:`~lightning.pytorch.core.LightningModule.forward`\n     - To run data through your model only (separate from ``training_step``)\n   * - :meth:`~lightning.pytorch.core.LightningModule.training_step`\n     - the complete training step\n   * - :meth:`~lightning.pytorch.core.LightningModule.validation_step`\n     - the complete validation step\n   * - :meth:`~lightning.pytorch.core.LightningModule.test_step`\n     - the complete test step\n   * - :meth:`~lightning.pytorch.core.LightningModule.predict_step`\n     - the complete prediction step\n   * - :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\n     - define optimizers and LR schedulers\n\n----------\n\n********\nTraining\n********\n\nTraining Loop\n=============\n\nTo activate the training loop, override the :meth:`~lightning.pytorch.core.LightningModule.training_step` method.\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n\n        def training_step(self, batch, batch_idx):\n            inputs, target = batch\n            output = self.model(inputs, target)\n            loss = torch.nn.functional.nll_loss(output, target.view(-1))\n            return loss\n\nUnder the hood, Lightning does the following (pseudocode):\n\n.. code-block:: python\n\n    # enable gradient calculation\n    torch.set_grad_enabled(True)\n\n    for batch_idx, batch in enumerate(train_dataloader):\n        loss = training_step(batch, batch_idx)\n\n        # clear gradients\n        optimizer.zero_grad()\n\n        # backward\n        loss.backward()\n\n        # update parameters\n        optimizer.step()\n\n\nTrain Epoch-level Metrics\n=========================\n\nIf you want to calculate epoch-level metrics and log them, use :meth:`~lightning.pytorch.core.LightningModule.log`.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n\n        # logs metrics for each training_step,\n        # and the average across the epoch, to the progress bar and logger\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\nThe :meth:`~lightning.pytorch.core.LightningModule.log` method automatically reduces the\nrequested metrics across a complete epoch and devices. Here's the pseudocode of what it does under the hood:\n\n.. code-block:: python\n\n    outs = []\n    for batch_idx, batch in enumerate(train_dataloader):\n        # forward\n        loss = training_step(batch, batch_idx)\n        outs.append(loss.detach())\n\n        # clear gradients\n        optimizer.zero_grad()\n        # backward\n        loss.backward()\n        # update parameters\n        optimizer.step()\n\n    # note: in reality, we do this incrementally, instead of keeping all outputs in memory\n    epoch_metric = torch.mean(torch.stack(outs))\n\nTrain Epoch-level Operations\n============================\n\nIn the case that you need to make use of all the outputs from each :meth:`~lightning.pytorch.LightningModule.training_step`,\noverride the :meth:`~lightning.pytorch.LightningModule.on_train_epoch_end` method.\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n            self.training_step_outputs = []\n\n        def training_step(self, batch, batch_idx):\n            inputs, target = batch\n            output = self.model(inputs, target)\n            loss = torch.nn.functional.nll_loss(output, target.view(-1))\n            preds = ...\n            self.training_step_outputs.append(preds)\n            return loss\n\n        def on_train_epoch_end(self):\n            all_preds = torch.stack(self.training_step_outputs)\n            # do something with all preds\n            ...\n            self.training_step_outputs.clear()  # free memory\n\n\n------------------\n\n**********\nValidation\n**********\n\nValidation Loop\n===============\n\nTo activate the validation loop while training, override the :meth:`~lightning.pytorch.core.LightningModule.validation_step` method.\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def validation_step(self, batch, batch_idx):\n            inputs, target = batch\n            output = self.model(inputs, target)\n            loss = F.cross_entropy(y_hat, y)\n            self.log(\"val_loss\", loss)\n\nUnder the hood, Lightning does the following (pseudocode):\n\n.. code-block:: python\n\n    # ...\n    for batch_idx, batch in enumerate(train_dataloader):\n        loss = model.training_step(batch, batch_idx)\n        loss.backward()\n        # ...\n\n        if validate_at_some_point:\n            # disable grads + batchnorm + dropout\n            torch.set_grad_enabled(False)\n            model.eval()\n\n            # ----------------- VAL LOOP ---------------\n            for val_batch_idx, val_batch in enumerate(val_dataloader):\n                val_out = model.validation_step(val_batch, val_batch_idx)\n            # ----------------- VAL LOOP ---------------\n\n            # enable grads + batchnorm + dropout\n            torch.set_grad_enabled(True)\n            model.train()\n\nYou can also run just the validation loop on your validation dataloaders by overriding :meth:`~lightning.pytorch.core.LightningModule.validation_step`\nand calling :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`.\n\n.. code-block:: python\n\n    model = LightningTransformer(vocab_size=dataset.vocab_size)\n    trainer = L.Trainer()\n    trainer.validate(model)\n\n.. note::\n\n    It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\n    This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\n    multi-device setting, samples could occur duplicated when :class:`~torch.utils.data.distributed.DistributedSampler`\n    is used, for eg. with ``strategy=\"ddp\"``. It replicates some samples on some devices to make sure all devices have\n    same batch size in case of uneven inputs.\n\n\nValidation Epoch-level Metrics\n==============================\n\nIn the case that you need to make use of all the outputs from each :meth:`~lightning.pytorch.LightningModule.validation_step`,\noverride the :meth:`~lightning.pytorch.LightningModule.on_validation_epoch_end` method.\nNote that this method is called before :meth:`~lightning.pytorch.LightningModule.on_train_epoch_end`.\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n            self.validation_step_outputs = []\n\n        def validation_step(self, batch, batch_idx):\n            x, y = batch\n            inputs, target = batch\n            output = self.model(inputs, target)\n            loss = torch.nn.functional.nll_loss(output, target.view(-1))\n            pred = ...\n            self.validation_step_outputs.append(pred)\n            return pred\n\n        def on_validation_epoch_end(self):\n            all_preds = torch.stack(self.validation_step_outputs)\n            # do something with all preds\n            ...\n            self.validation_step_outputs.clear()  # free memory\n\n----------------\n\n*******\nTesting\n*******\n\nTest Loop\n=========\n\nThe process for enabling a test loop is the same as the process for enabling a validation loop. Please refer to\nthe section above for details. For this you need to override the :meth:`~lightning.pytorch.core.LightningModule.test_step` method.\n\nThe only difference is that the test loop is only called when :meth:`~lightning.pytorch.trainer.trainer.Trainer.test` is used.\n\n.. code-block:: python\n\n    model = LightningTransformer(vocab_size=dataset.vocab_size)\n    dataloader = DataLoader(dataset)\n    trainer = L.Trainer()\n    trainer.fit(model=model, train_dataloaders=dataloader)\n\n    # automatically loads the best weights for you\n    trainer.test(model)\n\nThere are two ways to call ``test()``:\n\n.. code-block:: python\n\n    # call after training\n    trainer = L.Trainer()\n    trainer.fit(model=model, train_dataloaders=dataloader)\n\n    # automatically auto-loads the best weights from the previous run\n    trainer.test(dataloaders=test_dataloaders)\n\n    # or call with pretrained model\n    model = LightningTransformer.load_from_checkpoint(PATH)\n    dataset = WikiText2()\n    test_dataloader = DataLoader(dataset)\n    trainer = L.Trainer()\n    trainer.test(model, dataloaders=test_dataloader)\n\n.. note::\n    `WikiText2` is used in a manner that does not create a train, test, val split. This is done for illustrative purposes only.\n    A proper split can be created in :meth:`lightning.pytorch.core.LightningModule.setup` or :meth:`lightning.pytorch.core.LightningDataModule.setup`.\n\n.. note::\n\n    It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.\n    This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a\n    multi-device setting, samples could occur duplicated when :class:`~torch.utils.data.distributed.DistributedSampler`\n    is used, for eg. with ``strategy=\"ddp\"``. It replicates some samples on some devices to make sure all devices have\n    same batch size in case of uneven inputs.\n\n\n----------\n\n*********\nInference\n*********\n\nPrediction Loop\n===============\n\nBy default, the :meth:`~lightning.pytorch.core.LightningModule.predict_step` method runs the\n:meth:`~lightning.pytorch.core.LightningModule.forward` method. In order to customize this behaviour,\nsimply override the :meth:`~lightning.pytorch.core.LightningModule.predict_step` method.\n\nFor the example let's override ``predict_step``:\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n\n        def predict_step(self, batch):\n            inputs, target = batch\n            return self.model(inputs, target)\n\nUnder the hood, Lightning does the following (pseudocode):\n\n.. code-block:: python\n\n    # disable grads + batchnorm + dropout\n    torch.set_grad_enabled(False)\n    model.eval()\n    all_preds = []\n\n    for batch_idx, batch in enumerate(predict_dataloader):\n        pred = model.predict_step(batch, batch_idx)\n        all_preds.append(pred)\n\nThere are two ways to call ``predict()``:\n\n.. code-block:: python\n\n    # call after training\n    trainer = L.Trainer()\n    trainer.fit(model=model, train_dataloaders=dataloader)\n\n    # automatically auto-loads the best weights from the previous run\n    predictions = trainer.predict(dataloaders=predict_dataloader)\n\n    # or call with pretrained model\n    model = LightningTransformer.load_from_checkpoint(PATH)\n    dataset = WikiText2()\n    test_dataloader = DataLoader(dataset)\n    trainer = L.Trainer()\n    predictions = trainer.predict(model, dataloaders=test_dataloader)\n\nInference in Research\n=====================\n\nIf you want to perform inference with the system, you can add a ``forward`` method to the LightningModule.\n\n.. note:: When using forward, you are responsible to call :func:`~torch.nn.Module.eval` and use the :func:`~torch.no_grad` context manager.\n\n.. code-block:: python\n\n    class LightningTransformer(L.LightningModule):\n        def __init__(self, vocab_size):\n            super().__init__()\n            self.model = Transformer(vocab_size=vocab_size)\n\n        def forward(self, batch):\n            inputs, target = batch\n            return self.model(inputs, target)\n\n        def training_step(self, batch, batch_idx):\n            inputs, target = batch\n            output = self.model(inputs, target)\n            loss = torch.nn.functional.nll_loss(output, target.view(-1))\n            return loss\n\n        def configure_optimizers(self):\n            return torch.optim.SGD(self.model.parameters(), lr=0.1)\n\n\n    model = LightningTransformer(vocab_size=dataset.vocab_size)\n\n    model.eval()\n    with torch.no_grad():\n        batch = dataloader.dataset[0]\n        pred = model(batch)\n\nThe advantage of adding a forward is that in complex systems, you can do a much more involved inference procedure,\nsuch as text generation:\n\n.. code-block:: python\n\n    class Seq2Seq(L.LightningModule):\n        def forward(self, x):\n            embeddings = self(x)\n            hidden_states = self.encoder(embeddings)\n            for h in hidden_states:\n                # decode\n                ...\n            return decoded\n\nIn the case where you want to scale your inference, you should be using\n:meth:`~lightning.pytorch.core.LightningModule.predict_step`.\n\n.. code-block:: python\n\n    class Autoencoder(L.LightningModule):\n        def forward(self, x):\n            return self.decoder(x)\n\n        def predict_step(self, batch, batch_idx, dataloader_idx=0):\n            # this calls forward\n            return self(batch)\n\n\n    data_module = ...\n    model = Autoencoder()\n    trainer = Trainer(accelerator=\"gpu\", devices=2)\n    trainer.predict(model, data_module)\n\nInference in Production\n=======================\n\nFor cases like production, you might want to iterate different models inside a LightningModule.\n\n.. code-block:: python\n\n    from torchmetrics.functional import accuracy\n\n\n    class ClassificationTask(L.LightningModule):\n        def __init__(self, model):\n            super().__init__()\n            self.model = model\n\n        def training_step(self, batch, batch_idx):\n            x, y = batch\n            y_hat = self.model(x)\n            loss = F.cross_entropy(y_hat, y)\n            return loss\n\n        def validation_step(self, batch, batch_idx):\n            loss, acc = self._shared_eval_step(batch, batch_idx)\n            metrics = {\"val_acc\": acc, \"val_loss\": loss}\n            self.log_dict(metrics)\n            return metrics\n\n        def test_step(self, batch, batch_idx):\n            loss, acc = self._shared_eval_step(batch, batch_idx)\n            metrics = {\"test_acc\": acc, \"test_loss\": loss}\n            self.log_dict(metrics)\n            return metrics\n\n        def _shared_eval_step(self, batch, batch_idx):\n            x, y = batch\n            y_hat = self.model(x)\n            loss = F.cross_entropy(y_hat, y)\n            acc = accuracy(y_hat, y)\n            return loss, acc\n\n        def predict_step(self, batch, batch_idx, dataloader_idx=0):\n            x, y = batch\n            y_hat = self.model(x)\n            return y_hat\n\n        def configure_optimizers(self):\n            return torch.optim.Adam(self.model.parameters(), lr=0.02)\n\nThen pass in any arbitrary model to be fit with this task\n\n.. code-block:: python\n\n    for model in [resnet50(), vgg16(), BidirectionalRNN()]:\n        task = ClassificationTask(model)\n\n        trainer = Trainer(accelerator=\"gpu\", devices=2)\n        trainer.fit(task, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n\nTasks can be arbitrarily complex such as implementing GAN training, self-supervised or even RL.\n\n.. code-block:: python\n\n    class GANTask(L.LightningModule):\n        def __init__(self, generator, discriminator):\n            super().__init__()\n            self.generator = generator\n            self.discriminator = discriminator\n\n        ...\n\nWhen used like this, the model can be separated from the Task and thus used in production without needing to keep it in\na ``LightningModule``.\n\nThe following example shows how you can run inference in the Python runtime:\n\n.. code-block:: python\n\n    task = ClassificationTask(model)\n    trainer = Trainer(accelerator=\"gpu\", devices=2)\n    trainer.fit(task, train_dataloader, val_dataloader)\n    trainer.save_checkpoint(\"best_model.ckpt\")\n\n    # use model after training or load weights and drop into the production system\n    model = ClassificationTask.load_from_checkpoint(\"best_model.ckpt\")\n    x = ...\n    model.eval()\n    with torch.no_grad():\n        y_hat = model(x)\n\nCheck out :ref:`Inference in Production <production_inference>` guide to learn about the possible ways to perform inference in production.\n\n\n-----------\n\n\n********************\nSave Hyperparameters\n********************\n\nOften times we train many versions of a model. You might share that model or come back to it a few months later at which\npoint it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc...).\n\nLightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to\nimprove readability and reproducibility.\n\nsave_hyperparameters\n====================\n\nUse :meth:`~lightning.pytorch.core.LightningModule.save_hyperparameters` within your\n:class:`~lightning.pytorch.core.LightningModule`'s ``__init__`` method. It will enable Lightning to store all the\nprovided arguments under the ``self.hparams`` attribute. These hyperparameters will also be stored within the model\ncheckpoint, which simplifies model re-instantiation after training.\n\n.. code-block:: python\n\n    class LitMNIST(L.LightningModule):\n        def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n            super().__init__()\n            # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n            self.save_hyperparameters()\n\n            # equivalent\n            self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n\n            # Now possible to access layer_1_dim from hparams\n            self.hparams.layer_1_dim\n\n\nIn addition, loggers that support it will automatically log the contents of ``self.hparams``.\n\nExcluding hyperparameters\n=========================\n\nBy default, every parameter of the ``__init__`` method will be considered a hyperparameter to the LightningModule.\nHowever, sometimes some parameters need to be excluded from saving, for example when they are not serializable. Those\nparameters should be provided back when reloading the LightningModule. In this case, exclude them explicitly:\n\n.. code-block:: python\n\n    class LitMNIST(L.LightningModule):\n        def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n            super().__init__()\n            self.layer_1_dim = layer_1_dim\n            self.loss_fx = loss_fx\n\n            # call this to save only (layer_1_dim=128) to the checkpoint\n            self.save_hyperparameters(\"layer_1_dim\")\n\n            # equivalent\n            self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])\n\n\nload_from_checkpoint\n====================\n\nLightningModules that have hyperparameters automatically saved with\n:meth:`~lightning.pytorch.core.LightningModule.save_hyperparameters` can conveniently be loaded and instantiated\ndirectly from a checkpoint with :meth:`~lightning.pytorch.core.LightningModule.load_from_checkpoint`:\n\n.. code-block:: python\n\n    # to load specify the other args\n    model = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n\n\nIf parameters were excluded, they need to be provided at the time of loading:\n\n.. code-block:: python\n\n    # the excluded parameters were `loss_fx` and `generator_network`\n    model = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n\n\n-----------\n\n\n*************\nChild Modules\n*************\n\n.. include:: ../common/child_modules.rst\n\n-----------\n\n*******************\nLightningModule API\n*******************\n\n\nMethods\n=======\n\nall_gather\n~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.all_gather\n    :noindex:\n\nconfigure_callbacks\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.configure_callbacks\n    :noindex:\n\nconfigure_optimizers\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.configure_optimizers\n    :noindex:\n\nforward\n~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.forward\n    :noindex:\n\nfreeze\n~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.freeze\n    :noindex:\n\n.. _lm-log:\n\nlog\n~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.log\n    :noindex:\n\nlog_dict\n~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.log_dict\n    :noindex:\n\nlr_schedulers\n~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.lr_schedulers\n    :noindex:\n\nmanual_backward\n~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.manual_backward\n    :noindex:\n\noptimizers\n~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.optimizers\n    :noindex:\n\nprint\n~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.print\n    :noindex:\n\npredict_step\n~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.predict_step\n    :noindex:\n\nsave_hyperparameters\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.save_hyperparameters\n    :noindex:\n\ntoggle_optimizer\n~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.toggle_optimizer\n    :noindex:\n\ntest_step\n~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.test_step\n    :noindex:\n\nto_onnx\n~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.to_onnx\n    :noindex:\n\nto_torchscript\n~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.to_torchscript\n    :noindex:\n\ntraining_step\n~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.training_step\n    :noindex:\n\nunfreeze\n~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.unfreeze\n    :noindex:\n\nuntoggle_optimizer\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.untoggle_optimizer\n    :noindex:\n\nvalidation_step\n~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.validation_step\n    :noindex:\n\n-----------\n\nProperties\n==========\n\nThese are properties available in a LightningModule.\n\ncurrent_epoch\n~~~~~~~~~~~~~\n\nThe number of epochs run.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        if self.current_epoch == 0:\n            ...\n\ndevice\n~~~~~~\n\nThe device the module is on. Use it to keep your code device agnostic.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        z = torch.rand(2, 3, device=self.device)\n\nglobal_rank\n~~~~~~~~~~~\n\nThe ``global_rank`` is the index of the current process across all nodes and devices.\nLightning will perform some operations such as logging, weight checkpointing only when ``global_rank=0``. You\nusually do not need to use this property, but it is useful to know how to access it if needed.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        if self.global_rank == 0:\n            # do something only once across all the nodes\n            ...\n\nglobal_step\n~~~~~~~~~~~\n\nThe number of optimizer steps taken (does not reset each epoch).\nThis includes multiple optimizers (if enabled).\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        self.logger.experiment.log_image(..., step=self.global_step)\n\nhparams\n~~~~~~~\n\nThe arguments passed through ``LightningModule.__init__()`` and saved by calling\n:meth:`~lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin.save_hyperparameters` could be accessed by the ``hparams`` attribute.\n\n.. code-block:: python\n\n    def __init__(self, learning_rate):\n        self.save_hyperparameters()\n\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=self.hparams.learning_rate)\n\nlogger\n~~~~~~\n\nThe current logger being used (tensorboard or other supported logger)\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        # the generic logger (same no matter if tensorboard or other supported logger)\n        self.logger\n\n        # the particular logger\n        tensorboard_logger = self.logger.experiment\n\nloggers\n~~~~~~~\n\nThe list of loggers currently being used by the Trainer.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        # List of Logger objects\n        loggers = self.loggers\n        for logger in loggers:\n            logger.log_metrics({\"foo\": 1.0})\n\nlocal_rank\n~~~~~~~~~~~\n\nThe ``local_rank`` is the index of the current process across all the devices for the current node.\nYou usually do not need to use this property, but it is useful to know how to access it if needed.\nFor example, if using 10 machines (or nodes), the GPU at index 0 on each machine has local_rank = 0.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        if self.local_rank == 0:\n            # do something only once across each node\n            ...\n\nprecision\n~~~~~~~~~\n\nThe type of precision used:\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        if self.precision == \"16-true\":\n            ...\n\ntrainer\n~~~~~~~\n\nPointer to the trainer\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        max_steps = self.trainer.max_steps\n        any_flag = self.trainer.any_flag\n\nprepare_data_per_node\n~~~~~~~~~~~~~~~~~~~~~\n\nIf set to ``True`` will call ``prepare_data()`` on LOCAL_RANK=0 for every node.\nIf set to ``False`` will only call from NODE_RANK=0, LOCAL_RANK=0.\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.prepare_data_per_node = True\n\nautomatic_optimization\n~~~~~~~~~~~~~~~~~~~~~~\n\nWhen set to ``False``, Lightning does not automate the optimization process. This means you are responsible for handling\nyour optimizers. However, we do take care of precision and any accelerators used.\n\nSee :ref:`manual optimization <common/optimization:Manual optimization>` for details.\n\n.. code-block:: python\n\n    def __init__(self):\n        self.automatic_optimization = False\n\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers(use_pl_optimizer=True)\n\n        loss = ...\n        opt.zero_grad()\n        self.manual_backward(loss)\n        opt.step()\n\nManual optimization is most useful for research topics like reinforcement learning, sparse coding, and GAN research.\nIt is required when you are using 2+ optimizers because with automatic optimization, you can only use one optimizer.\n\n.. code-block:: python\n\n    def __init__(self):\n        self.automatic_optimization = False\n\n\n    def training_step(self, batch, batch_idx):\n        # access your optimizers with use_pl_optimizer=False. Default is True\n        opt_a, opt_b = self.optimizers(use_pl_optimizer=True)\n\n        gen_loss = ...\n        opt_a.zero_grad()\n        self.manual_backward(gen_loss)\n        opt_a.step()\n\n        disc_loss = ...\n        opt_b.zero_grad()\n        self.manual_backward(disc_loss)\n        opt_b.step()\n\nexample_input_array\n~~~~~~~~~~~~~~~~~~~\n\nSet and access example_input_array, which basically represents a single batch.\n\n.. code-block:: python\n\n    def __init__(self):\n        self.example_input_array = ...\n        self.generator = ...\n\n\n    def on_train_epoch_end(self):\n        # generate some images using the example_input_array\n        gen_images = self.generator(self.example_input_array)\n\n--------------\n\n.. _lightning_hooks:\n\nHooks\n=====\n\nThis is the pseudocode to describe the structure of :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`.\nThe inputs and outputs of each function are not represented for simplicity. Please check each function's API reference\nfor more information.\n\n.. code-block:: python\n\n    # runs on every device: devices can be GPUs, TPUs, ...\n    def fit(self):\n        configure_callbacks()\n\n        if local_rank == 0:\n            prepare_data()\n\n        setup(\"fit\")\n        configure_model()\n        configure_optimizers()\n\n        on_fit_start()\n\n        # the sanity check runs here\n\n        on_train_start()\n        for epoch in epochs:\n            fit_loop()\n        on_train_end()\n\n        on_fit_end()\n        teardown(\"fit\")\n\n\n    def fit_loop():\n        torch.set_grad_enabled(True)\n\n        on_train_epoch_start()\n\n        for batch_idx, batch in enumerate(train_dataloader()):\n            on_train_batch_start()\n\n            on_before_batch_transfer()\n            transfer_batch_to_device()\n            on_after_batch_transfer()\n\n            out = training_step()\n\n            on_before_zero_grad()\n            optimizer_zero_grad()\n\n            on_before_backward()\n            backward()\n            on_after_backward()\n\n            on_before_optimizer_step()\n            configure_gradient_clipping()\n            optimizer_step()\n\n            on_train_batch_end(out, batch, batch_idx)\n\n            if should_check_val:\n                val_loop()\n\n        on_train_epoch_end()\n\n\n    def val_loop():\n        on_validation_model_eval()  # calls `model.eval()`\n        torch.set_grad_enabled(False)\n\n        on_validation_start()\n        on_validation_epoch_start()\n\n        for batch_idx, batch in enumerate(val_dataloader()):\n            on_validation_batch_start(batch, batch_idx)\n\n            batch = on_before_batch_transfer(batch)\n            batch = transfer_batch_to_device(batch)\n            batch = on_after_batch_transfer(batch)\n\n            out = validation_step(batch, batch_idx)\n\n            on_validation_batch_end(out, batch, batch_idx)\n\n        on_validation_epoch_end()\n        on_validation_end()\n\n        # set up for train\n        on_validation_model_train()  # calls `model.train()`\n        torch.set_grad_enabled(True)\n\nbackward\n~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.backward\n    :noindex:\n\non_before_backward\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_before_backward\n    :noindex:\n\non_after_backward\n~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_after_backward\n    :noindex:\n\non_before_zero_grad\n~~~~~~~~~~~~~~~~~~~\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_before_zero_grad\n    :noindex:\n\non_fit_start\n~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_fit_start\n    :noindex:\n\non_fit_end\n~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_fit_end\n    :noindex:\n\n\non_load_checkpoint\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_load_checkpoint\n    :noindex:\n\non_save_checkpoint\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_save_checkpoint\n    :noindex:\n\nload_from_checkpoint\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.load_from_checkpoint\n    :noindex:\n\non_train_start\n~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_start\n    :noindex:\n\non_train_end\n~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_end\n    :noindex:\n\non_validation_start\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_start\n    :noindex:\n\non_validation_end\n~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_end\n    :noindex:\n\non_test_batch_start\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_batch_start\n    :noindex:\n\non_test_batch_end\n~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_batch_end\n    :noindex:\n\non_test_epoch_start\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_epoch_start\n    :noindex:\n\non_test_epoch_end\n~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_epoch_end\n    :noindex:\n\non_test_start\n~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_start\n    :noindex:\n\non_test_end\n~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_end\n    :noindex:\n\non_predict_batch_start\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_batch_start\n    :noindex:\n\non_predict_batch_end\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_batch_end\n    :noindex:\n\non_predict_epoch_start\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_epoch_start\n    :noindex:\n\non_predict_epoch_end\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_epoch_end\n    :noindex:\n\non_predict_start\n~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_start\n    :noindex:\n\non_predict_end\n~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_predict_end\n    :noindex:\n\non_train_batch_start\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_batch_start\n    :noindex:\n\non_train_batch_end\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_batch_end\n    :noindex:\n\non_train_epoch_start\n~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_epoch_start\n    :noindex:\n\non_train_epoch_end\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_train_epoch_end\n    :noindex:\n\non_validation_batch_start\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_batch_start\n    :noindex:\n\non_validation_batch_end\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_batch_end\n    :noindex:\n\non_validation_epoch_start\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_epoch_start\n    :noindex:\n\non_validation_epoch_end\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_epoch_end\n    :noindex:\n\nconfigure_model\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.configure_model\n    :noindex:\n\non_validation_model_eval\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_model_eval\n    :noindex:\n\non_validation_model_train\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_validation_model_train\n    :noindex:\n\non_test_model_eval\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_model_eval\n    :noindex:\n\non_test_model_train\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_test_model_train\n    :noindex:\n\non_before_optimizer_step\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_before_optimizer_step\n    :noindex:\n\nconfigure_gradient_clipping\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.configure_gradient_clipping\n    :noindex:\n\noptimizer_step\n~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.optimizer_step\n    :noindex:\n\noptimizer_zero_grad\n~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.optimizer_zero_grad\n    :noindex:\n\nprepare_data\n~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.prepare_data\n    :noindex:\n\nsetup\n~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.setup\n    :noindex:\n\nteardown\n~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.teardown\n    :noindex:\n\ntrain_dataloader\n~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.train_dataloader\n    :noindex:\n\nval_dataloader\n~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.val_dataloader\n    :noindex:\n\ntest_dataloader\n~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.test_dataloader\n    :noindex:\n\npredict_dataloader\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.predict_dataloader\n    :noindex:\n\ntransfer_batch_to_device\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.transfer_batch_to_device\n    :noindex:\n\non_before_batch_transfer\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_before_batch_transfer\n    :noindex:\n\non_after_batch_transfer\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_after_batch_transfer\n    :noindex:\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/notebooks.html", "url_rel_html": "common/notebooks.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/notebooks.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Interactive Notebooks (Jupyter, Colab, Kaggle)¶", "rst_text": ":orphan:\n\n.. _jupyter_notebooks:\n\n##############################################\nInteractive Notebooks (Jupyter, Colab, Kaggle)\n##############################################\n\n**Audience:** Users looking to train models in interactive notebooks (Jupyter, Colab, Kaggle, etc.).\n\n\n----\n\n\n**********************\nLightning in notebooks\n**********************\n\nYou can use the Lightning Trainer in interactive notebooks just like in a regular Python script, including multi-GPU training!\n\n.. code-block:: python\n\n    import lightning as L\n\n    # Works in Jupyter, Colab and Kaggle!\n    trainer = L.Trainer(accelerator=\"auto\", devices=\"auto\")\n\n\nYou can find many notebook examples on our :doc:`tutorials page <../tutorials>` too!\n\n\n----\n\n\n.. _jupyter_notebook_example:\n\n************\nFull example\n************\n\nPaste the following code block into a notebook cell:\n\n.. code-block:: python\n\n    import lightning as L\n    from torch import nn, optim, utils\n    import torchvision\n\n    encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n    decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n\n    class LitAutoEncoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            super().__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n\n        def training_step(self, batch, batch_idx):\n            x, _ = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            loss = nn.functional.mse_loss(x_hat, x)\n            self.log(\"train_loss\", loss)\n            return loss\n\n        def configure_optimizers(self):\n            return optim.Adam(self.parameters(), lr=1e-3)\n\n        def prepare_data(self):\n            torchvision.datasets.MNIST(\".\", download=True)\n\n        def train_dataloader(self):\n            dataset = torchvision.datasets.MNIST(\".\", transform=torchvision.transforms.ToTensor())\n            return utils.data.DataLoader(dataset, batch_size=64)\n\n\n    autoencoder = LitAutoEncoder(encoder, decoder)\n    trainer = L.Trainer(max_epochs=2, devices=\"auto\")\n    trainer.fit(model=autoencoder)\n\n\n----\n\n\n*********************\nMulti-GPU Limitations\n*********************\n\nThe multi-GPU capabilities in Jupyter are enabled by launching processes using the 'fork' start method.\nIt is the only supported way of multi-processing in notebooks, but also brings some limitations that you should be aware of.\n\nAvoid initializing CUDA before .fit()\n=====================================\n\nDon't run torch CUDA functions before calling ``trainer.fit()`` in any of the notebook cells beforehand, otherwise your code may hang or crash.\n\n.. code-block:: python\n\n    # BAD: Don't run CUDA-related code before `.fit()`\n    x = torch.tensor(1).cuda()\n    torch.cuda.empty_cache()\n    torch.cuda.is_available()\n\n    trainer = L.Trainer(accelerator=\"cuda\", devices=2)\n    trainer.fit(model)\n\n\nMove data loading code inside the hooks\n=======================================\n\nIf you define/load your data in the main process before calling ``trainer.fit()``, you may see a slowdown or crashes (segmentation fault, SIGSEV, etc.).\n\n.. code-block:: python\n\n    # BAD: Don't load data in the main process\n    dataset = MyDataset(\"data/\")\n    train_dataloader = torch.utils.data.DataLoader(dataset)\n\n    trainer = L.Trainer(accelerator=\"cuda\", devices=2)\n    trainer.fit(model, train_dataloader)\n\nThe best practice is to move your data loading code inside the ``*_dataloader()`` hooks in the :class:`~lightning.pytorch.core.LightningModule` or :class:`~lightning.pytorch.core.datamodule.LightningDataModule` as shown in the :ref:`example above <jupyter_notebook_example>`.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/precision.html", "url_rel_html": "common/precision.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/precision.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "N-Bit Precision¶", "rst_text": ":orphan:\n\n.. _precision:\n\n###############\nN-Bit Precision\n###############\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Basic\n   :description: Enable your models to train faster and save memory with different floating-point precision settings.\n   :col_css: col-md-4\n   :button_link: precision_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Enable state-of-the-art scaling with advanced mixed-precision settings.\n   :col_css: col-md-4\n   :button_link: precision_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Expert\n   :description: Create new precision techniques and enable them through Lightning.\n   :col_css: col-md-4\n   :button_link: precision_expert.html\n   :height: 150\n   :tag: expert\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/precision_basic.html", "url_rel_html": "common/precision_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/precision_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "N-Bit Precision (Basic)¶", "rst_text": ":orphan:\n\n.. _precision_basic:\n\n#######################\nN-Bit Precision (Basic)\n#######################\n**Audience:** Users looking to train models faster and consume less memory.\n\n----\n\nIf you're looking to run models faster or consume less memory, consider tweaking the precision settings of your models.\n\nLower precision, such as 16-bit floating-point, requires less memory and enables training and deploying larger models.\nHigher precision, such as the 64-bit floating-point, can be used for highly sensitive use-cases.\n\n----\n\n****************\n16-bit Precision\n****************\n\nUse 16-bit mixed precision to speed up training and inference.\nIf your GPUs are [`Tensor Core <https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html>`_] GPUs, you can expect a ~3x speed improvement.\n\n.. code-block:: python\n\n    Trainer(precision=\"16-mixed\")\n\n\nIn most cases, mixed precision uses FP16. Supported `PyTorch operations <https://pytorch.org/docs/stable/amp.html#op-specific-behavior>`__ automatically run in FP16, saving memory and improving throughput on the supported accelerators.\nSince computation happens in FP16, which has a very limited \"dynamic range\", there is a chance of numerical instability during training. This is handled internally by a dynamic grad scaler which skips invalid steps and adjusts the scaler to ensure subsequent steps fall within a finite range. For more information `see the autocast docs <https://pytorch.org/docs/stable/amp.html#gradient-scaling>`__.\n\n\nWith true 16-bit precision you can additionally lower your memory consumption by up to half so that you can train and deploy larger models.\nHowever, this setting can sometimes lead to unstable training.\n\n.. code-block:: python\n\n    Trainer(precision=\"16-true\")\n\n.. warning::\n\n    Float16 cannot represent values smaller than ~6e-5. Values like Adam's default ``eps=1e-8`` become zero, which can cause\n    NaN during training. Increase ``eps`` to 1e-4 or higher, and avoid extremely small values in your model weights and data.\n\n.. note::\n\n    BFloat16 (``\"bf16-mixed\"`` or ``\"bf16-true\"``) has better numerical stability with a wider dynamic range.\n\n----\n\n\n****************\n32-bit Precision\n****************\n\n32-bit precision is the default used across all models and research. This precision is known to be stable in contrast to lower precision settings.\n\n.. testcode::\n\n    Trainer(precision=\"32-true\")\n\n    # or (legacy)\n    Trainer(precision=\"32\")\n\n    # or (legacy)\n    Trainer(precision=32)\n\n----\n\n****************\n64-bit Precision\n****************\n\nFor certain scientific computations, 64-bit precision enables more accurate models. However, doubling the precision from 32 to 64 bit also doubles the memory requirements.\n\n.. testcode::\n\n    Trainer(precision=\"64-true\")\n\n    # or (legacy)\n    Trainer(precision=\"64\")\n\n    # or (legacy)\n    Trainer(precision=64)\n\nSince in deep learning, memory is always a bottleneck, especially when dealing with a large volume of data and with limited resources.\nIt is recommended using single precision for better speed. Although you can still use it if you want for your particular use-case.\n\nWhen working with complex numbers, instantiation of complex tensors should be done in the\n:meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook or under the\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager so that the `complex128` dtype\nis properly selected.\n\n.. code-block:: python\n\n    trainer = Trainer(precision=\"64-true\")\n\n    # init the model directly on the device and with parameters in full-precision\n    with trainer.init_module():\n        model = MyModel()\n\n    trainer.fit(model)\n\n\n----\n\n********************************\nPrecision support by accelerator\n********************************\n\n.. list-table:: Precision with Accelerators\n   :widths: 20 20 20 20\n   :header-rows: 1\n\n   * - Precision\n     - CPU\n     - GPU\n     - TPU\n   * - 16 Mixed\n     - No\n     - Yes\n     - No\n   * - BFloat16 Mixed\n     - Yes\n     - Yes\n     - Yes\n   * - 32 True\n     - Yes\n     - Yes\n     - Yes\n   * - 64 True\n     - Yes\n     - Yes\n     - No\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/precision_expert.html", "url_rel_html": "common/precision_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/precision_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "N-Bit Precision (Expert)¶", "rst_text": ":orphan:\n\n.. _precision_expert:\n\n########################\nN-Bit Precision (Expert)\n########################\n**Audience:** Researchers looking to integrate their new precision techniques into Lightning.\n\n\n*****************\nPrecision Plugins\n*****************\n\nYou can also customize and pass your own Precision Plugin by subclassing the :class:`~lightning.pytorch.plugins.precision.precision.Precision` class.\n\n- Perform pre and post backward/optimizer step operations such as scaling gradients.\n- Provide context managers for forward, training_step, etc.\n\n.. code-block:: python\n\n    class CustomPrecision(Precision):\n        precision = \"16-mixed\"\n\n        ...\n\n\n    trainer = Trainer(plugins=[CustomPrecision()])\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html", "url_rel_html": "common/precision_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/precision_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "N-Bit Precision (Intermediate)¶", "rst_text": ":orphan:\n\n.. _precision_intermediate:\n\n##############################\nN-Bit Precision (Intermediate)\n##############################\n**Audience:** Users looking to scale larger models or take advantage of optimized accelerators.\n\n----\n\n************************\nWhat is Mixed Precision?\n************************\n\nPyTorch, like most deep learning frameworks, trains on 32-bit floating-point (FP32) arithmetic by default. However, many deep learning models do not require this to reach complete accuracy. By conducting\noperations in half-precision format while keeping minimum information in single-precision to maintain as much information as possible in crucial areas of the network, mixed precision training delivers\nsignificant computational speedup. Switching to mixed precision has resulted in considerable training speedups since the introduction of Tensor Cores in the Volta and Turing architectures. It combines\nFP32 and lower-bit floating-points (such as FP16) to reduce memory footprint and increase performance during model training and evaluation. It accomplishes this by recognizing the steps that require\ncomplete accuracy and employing a 32-bit floating-point for those steps only, while using a 16-bit floating-point for the rest. When compared to complete precision training, mixed precision training\ndelivers all of these benefits while ensuring that no task-specific accuracy is lost. [`2 <https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html>`_].\n\n.. note::\n\n    In some cases, it is essential to remain in FP32 for numerical stability, so keep this in mind when using mixed precision.\n    For example, when running scatter operations during the forward (such as torchpoint3d), computation must remain in FP32.\n\n.. warning::\n\n    Do not cast anything to other dtypes manually using ``torch.autocast`` or ``tensor.half()`` when using native precision because\n    this can bring instability.\n\n    .. code-block:: python\n\n        class LitModel(LightningModule):\n            def training_step(self, batch, batch_idx):\n                outs = self(batch)\n\n                a_float32 = torch.rand((8, 8), device=self.device, dtype=self.dtype)\n                b_float32 = torch.rand((8, 4), device=self.device, dtype=self.dtype)\n\n                # casting to float16 manually\n                with torch.autocast(device_type=self.device.type):\n                    c_float16 = torch.mm(a_float32, b_float32)\n                    target = self.layer(c_float16.flatten()[None])\n\n                # here outs is of type float32 and target is of type float16\n                loss = torch.mm(target @ outs).float()\n                return loss\n\n\n        trainer = Trainer(accelerator=\"gpu\", devices=1, precision=32)\n\n----\n\n\n************************\nBFloat16 Mixed Precision\n************************\n\n.. warning::\n\n    BFloat16 may not provide significant speedups or memory improvements or offer better numerical stability.\n    For GPUs, the most significant benefits require `Ampere <https://en.wikipedia.org/wiki/Ampere_(microarchitecture)>`__ based GPUs or newer, such as A100s or 3090s.\n\nBFloat16 Mixed precision is similar to FP16 mixed precision, however, it maintains more of the \"dynamic range\" that FP32 offers. This means it is able to improve numerical stability than FP16 mixed precision. For more information, see `this TPU performance blogpost <https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus>`__.\n\nUnder the hood, we use `torch.autocast <https://pytorch.org/docs/stable/amp.html>`__ with the dtype set to ``bfloat16``, with no gradient scaling.\n\n.. testcode::\n    :skipif: not torch.cuda.is_available()\n\n    Trainer(accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\")\n\nIt is also possible to use BFloat16 mixed precision on the CPU, relying on MKLDNN under the hood.\n\n.. testcode::\n\n    Trainer(precision=\"bf16-mixed\")\n\n\n----\n\n\n*******************\nTrue Half Precision\n*******************\n\nAs mentioned before, for numerical stability mixed precision keeps the model weights in full float32 precision while casting only supported operations to lower bit precision.\nHowever, in some cases it is indeed possible to train completely in half precision. Similarly, for inference the model weights can often be cast to half precision without a loss in accuracy (even when trained with mixed precision).\n\n.. code-block:: python\n\n    # Select FP16 precision\n    trainer = Trainer(precision=\"16-true\")\n    trainer.fit(model)  # model gets cast to torch.float16\n\n    # Select BF16 precision\n    trainer = Trainer(precision=\"bf16-true\")\n    trainer.fit(model)  # model gets cast to torch.bfloat16\n\nTip: For faster initialization, you can create model parameters with the desired dtype directly on the device:\n\n.. code-block:: python\n\n    trainer = Trainer(precision=\"bf16-true\")\n\n    # init the model directly on the device and with parameters in half-precision\n    with trainer.init_module():\n        model = MyModel()\n\n    trainer.fit(model)\n\n\nSee also: :doc:`../advanced/model_init`\n\n\n----\n\n\n*****************************************************\nFloat8 Mixed Precision via Nvidia's TransformerEngine\n*****************************************************\n\n`Transformer Engine <https://github.com/NVIDIA/TransformerEngine>`__ (TE) is a library for accelerating models on the\nlatest NVIDIA GPUs using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower\nmemory utilization in both training and inference. It offers improved performance over half precision with no degradation in accuracy.\n\nUsing TE requires replacing some of the layers in your model. Fabric automatically replaces the :class:`torch.nn.Linear`\nand :class:`torch.nn.LayerNorm` layers in your model with their TE alternatives, however, TE also offers\n`fused layers <https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html>`__\nto squeeze out all the possible performance. If Fabric detects that any layer has been replaced already, automatic\nreplacement is not done.\n\nThis plugin is a combination of \"mixed\" and \"true\" precision. The computation is downcasted to FP8 precision on the fly, but\nthe model and inputs can be kept in true full or half precision.\n\n.. code-block:: python\n\n    # Select 8bit mixed precision via TransformerEngine, with model weights in bfloat16\n    trainer = Trainer(precision=\"transformer-engine\")\n\n    # Select 8bit mixed precision via TransformerEngine, with model weights in float16\n    trainer = Trainer(precision=\"transformer-engine-float16\")\n\n    # Customize the fp8 recipe or set a different base precision:\n    from lightning.trainer.plugins import TransformerEnginePrecision\n\n    recipe = {\"fp8_format\": \"HYBRID\", \"amax_history_len\": 16, \"amax_compute_algo\": \"max\"}\n    precision = TransformerEnginePrecision(weights_dtype=torch.bfloat16, recipe=recipe)\n    trainer = Trainer(plugins=precision)\n\n\nUnder the hood, we use `transformer_engine.pytorch.fp8_autocast <https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html#transformer_engine.pytorch.fp8_autocast>`__ with the default fp8 recipe.\n\n.. note::\n\n    This requires `Hopper <https://en.wikipedia.org/wiki/Hopper_(microarchitecture)>`_ based GPUs or newer, such the H100.\n\n\n----\n\n\n*****************************\nQuantization via Bitsandbytes\n*****************************\n\n`bitsandbytes <https://github.com/bitsandbytes-foundation/bitsandbytes>`__ (BNB) is a library that supports quantizing :class:`torch.nn.Linear` weights.\n\nBoth 4-bit (`paper reference <https://arxiv.org/abs/2305.14314v1>`__) and 8-bit (`paper reference <https://arxiv.org/abs/2110.02861>`__) quantization is supported.\nSpecifically, we support the following modes:\n\n* **nf4**: Uses the normalized float 4-bit data type. This is recommended over \"fp4\" based on the paper's experimental results and theoretical analysis.\n* **nf4-dq**: \"dq\" stands for \"Double Quantization\" which reduces the average memory footprint by quantizing the quantization constants. In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).\n* **fp4**: Uses regular float 4-bit data type.\n* **fp4-dq**: \"dq\" stands for \"Double Quantization\" which reduces the average memory footprint by quantizing the quantization constants. In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).\n* **int8**: Uses unsigned int8 data type.\n* **int8-training**: Meant for int8 activations with fp16 precision weights.\n\nWhile these techniques store weights in 4 or 8 bit, the computation still happens in 16 or 32-bit (float16, bfloat16, float32).\nThis is configurable via the dtype argument in the plugin.\nIf your model weights can fit on a single device with 16 bit precision, it's recommended that this plugin is not used as it will slow down training.\n\nQuantizing the model will dramatically reduce the weight's memory requirements but  may have a negative impact on the model's performance or runtime.\n\nThe :class:`~lightning.pytorch.plugins.precision.bitsandbytes.BitsandbytesPrecision` automatically replaces the :class:`torch.nn.Linear` layers in your model with their BNB alternatives.\n\n.. code-block:: python\n\n    from lightning.pytorch.plugins import BitsandbytesPrecision\n\n    # this will pick out the compute dtype automatically, by default `bfloat16`\n    precision = BitsandbytesPrecision(mode=\"nf4-dq\")\n    trainer = Trainer(plugins=precision)\n\n    # Customize the dtype, or skip some modules\n    precision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\n    trainer = Trainer(plugins=precision)\n\n\n    class MyModel(LightningModule):\n        def configure_model(self):\n            # instantiate your model in this hook\n            self.model = MyModel()\n\n\n.. note::\n\n    Only supports CUDA devices and the Linux operating system. Windows users should use\n    `WSL2 <https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl>`__.\n\n\nThis plugin does not take care of replacing your optimizer with an 8-bit optimizer e.g. ``bitsandbytes.optim.Adam8bit``.\nYou might want to do this for extra memory savings.\n\n.. code-block:: python\n\n    import bitsandbytes as bnb\n\n\n    class MyModel(LightningModule):\n        def configure_optimizers(self):\n            optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n\n            # (optional) force embedding layers to use 32 bit for numerical stability\n            # https://github.com/huggingface/transformers/issues/14819#issuecomment-1003445038\n            for module in model.modules():\n                if isinstance(module, torch.nn.Embedding):\n                    bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                        module, \"weight\", {\"optim_bits\": 32}\n                    )\n\n            return optimizer\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/progress_bar.html", "url_rel_html": "common/progress_bar.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/progress_bar.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Customize the progress bar¶", "rst_text": ".. testsetup:: *\n\n    from lightning.pytorch.trainer.trainer import Trainer\n\n.. _progress_bar:\n\n\nCustomize the progress bar\n==========================\n\nLightning supports two different types of progress bars (`tqdm <https://github.com/tqdm/tqdm>`_ and `rich <https://github.com/Textualize/rich>`_). :class:`~lightning.pytorch.callbacks.TQDMProgressBar` is used by default,\nbut you can override it by passing a custom :class:`~lightning.pytorch.callbacks.TQDMProgressBar` or :class:`~lightning.pytorch.callbacks.RichProgressBar` to the ``callbacks`` argument of the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\n\nYou could also use the :class:`~lightning.pytorch.callbacks.ProgressBar` class to implement your own progress bar.\n\n-------------\n\nTQDMProgressBar\n---------------\n\nThe :class:`~lightning.pytorch.callbacks.TQDMProgressBar` uses the `tqdm <https://github.com/tqdm/tqdm>`_ library internally and is the default progress bar used by Lightning.\nIt prints to ``stdout`` and shows up to four different bars:\n\n- **sanity check progress:** the progress during the sanity check run\n- **train progress:** shows the training progress. It will pause if validation starts and will resume when it ends, and also accounts for multiple validation runs during training when :paramref:`~lightning.pytorch.trainer.trainer.Trainer.val_check_interval` is used.\n- **validation progress:** only visible during validation; shows total progress over all validation datasets.\n- **test progress:** only active when testing; shows total progress over all test datasets.\n\nFor infinite datasets, the progress bar never ends.\n\nYou can update ``refresh_rate`` (rate (number of batches) at which the progress bar get updated) for :class:`~lightning.pytorch.callbacks.TQDMProgressBar` by:\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import TQDMProgressBar\n\n    trainer = Trainer(callbacks=[TQDMProgressBar(refresh_rate=10)])\n\n.. note::\n\n    The ``smoothing`` option has no effect when using the default implementation of :class:`~lightning.pytorch.callbacks.TQDMProgressBar`, as the progress bar is updated using the ``bar.refresh()`` method instead of ``bar.update()``. This can cause the progress bar to become desynchronized with the actual progress. To avoid this issue, you can use the ``bar.update()`` method instead, but this may require customizing the :class:`~lightning.pytorch.callbacks.TQDMProgressBar` class.\n\nBy default the training progress bar is reset (overwritten) at each new epoch.\nIf you wish for a new progress bar to be displayed at the end of every epoch, set\n:paramref:`TQDMProgressBar.leave <lightning.pytorch.callbacks.TQDMProgressBar.leave>` to ``True``.\n\n.. code-block:: python\n\n    trainer = Trainer(callbacks=[TQDMProgressBar(leave=True)])\n\nIf you want to customize the default :class:`~lightning.pytorch.callbacks.TQDMProgressBar` used by Lightning, you can override\nspecific methods of the callback class and pass your custom implementation to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\n\n.. code-block:: python\n\n    class LitProgressBar(TQDMProgressBar):\n        def init_validation_tqdm(self):\n            bar = super().init_validation_tqdm()\n            bar.set_description(\"running validation...\")\n            return bar\n\n\n    trainer = Trainer(callbacks=[LitProgressBar()])\n\n.. seealso::\n    - :class:`~lightning.pytorch.callbacks.TQDMProgressBar` docs.\n    - `tqdm library <https://github.com/tqdm/tqdm>`__\n\n----------------\n\nRichProgressBar\n---------------\n\n`Rich <https://github.com/Textualize/rich>`_ is a Python library for rich text and beautiful formatting in the terminal.\nTo use the :class:`~lightning.pytorch.callbacks.RichProgressBar` as your progress bar, first install the package:\n\n.. code-block:: bash\n\n    pip install rich\n\nThen configure the callback and pass it to the :class:`~lightning.pytorch.trainer.trainer.Trainer`:\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import RichProgressBar\n\n    trainer = Trainer(callbacks=[RichProgressBar()])\n\nCustomize the theme for your :class:`~lightning.pytorch.callbacks.RichProgressBar` like this:\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n    # create your own theme!\n    progress_bar = RichProgressBar(\n        theme=RichProgressBarTheme(\n            description=\"green_yellow\",\n            progress_bar=\"green1\",\n            progress_bar_finished=\"green1\",\n            progress_bar_pulse=\"#6206E0\",\n            batch_progress=\"green_yellow\",\n            time=\"grey82\",\n            processing_speed=\"grey82\",\n            metrics=\"grey82\",\n            metrics_text_delimiter=\"\\n\",\n            metrics_format=\".3e\",\n        )\n    )\n\n    trainer = Trainer(callbacks=progress_bar)\n\nYou can customize the components used within :class:`~lightning.pytorch.callbacks.RichProgressBar` with ease by overriding the\n:func:`~lightning.pytorch.callbacks.RichProgressBar.configure_columns` method.\n\n.. code-block:: python\n\n    from rich.progress import TextColumn\n\n    custom_column = TextColumn(\"[progress.description]Custom Rich Progress Bar!\")\n\n\n    class CustomRichProgressBar(RichProgressBar):\n        def configure_columns(self, trainer):\n            return [custom_column]\n\n\n    progress_bar = CustomRichProgressBar()\n\nIf you wish for a new progress bar to be displayed at the end of every epoch, you should enable\n:paramref:`RichProgressBar.leave <lightning.pytorch.callbacks.RichProgressBar.leave>` by passing ``True``\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import RichProgressBar\n\n    trainer = Trainer(callbacks=[RichProgressBar(leave=True)])\n\n.. seealso::\n    - :class:`~lightning.pytorch.callbacks.RichProgressBar` docs.\n    - :class:`~lightning.pytorch.callbacks.RichModelSummary` docs to customize the model summary table.\n    - `Rich library <https://github.com/Textualize/rich>`__.\n\n\n.. note::\n\n    Progress bar is automatically enabled with the Trainer, and to disable it, one should do this:\n\n    .. code-block:: python\n\n        trainer = Trainer(enable_progress_bar=False)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/remote_fs.html", "url_rel_html": "common/remote_fs.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/remote_fs.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Remote Filesystems¶", "rst_text": ".. _remote_fs:\n\n##################\nRemote Filesystems\n##################\n\nPyTorch Lightning enables working with data from a variety of filesystems, including local filesystems and several cloud storage providers such as\n`S3 <https://aws.amazon.com/s3/>`_ on `AWS <https://aws.amazon.com/>`_, `GCS <https://cloud.google.com/storage>`_ on `Google Cloud <https://cloud.google.com/>`_,\nor `ADL <https://azure.microsoft.com/solutions/data-lake/>`_ on `Azure <https://azure.microsoft.com/>`_.\n\nThis applies to saving and writing checkpoints, as well as for logging.\nWorking with different filesystems can be accomplished by appending a protocol like \"s3:/\" to file paths for writing and reading data.\n\n.. code-block:: python\n\n    # `default_root_dir` is the default path used for logs and checkpoints\n    trainer = Trainer(default_root_dir=\"s3://my_bucket/data/\")\n    trainer.fit(model)\n\n\nFor logging, remote filesystem support depends on the particular logger integration being used. Consult :ref:`the documentation of the individual logger <loggers-api-references>` for more details.\n\n.. code-block:: python\n\n    from lightning.pytorch.loggers import TensorBoardLogger\n\n    logger = TensorBoardLogger(save_dir=\"s3://my_bucket/logs/\")\n\n    trainer = Trainer(logger=logger)\n    trainer.fit(model)\n\nAdditionally, you could also resume training with a checkpoint stored at a remote filesystem.\n\n.. code-block:: python\n\n    trainer = Trainer(default_root_dir=tmpdir, max_steps=3)\n    trainer.fit(model, ckpt_path=\"s3://my_bucket/ckpts/classifier.ckpt\")\n\nPyTorch Lightning uses `fsspec <https://filesystem-spec.readthedocs.io/>`_ internally to handle all filesystem operations.\n\nThe most common filesystems supported by Lightning are:\n\n* Local filesystem: ``file://`` - It's the default and doesn't need any protocol to be used. It's installed by default in Lightning.\n* Amazon S3: ``s3://`` - Amazon S3 remote binary store, using the library `s3fs <https://s3fs.readthedocs.io/>`__. Run ``pip install fsspec[s3]`` to install it.\n* Google Cloud Storage: ``gcs://`` or ``gs://`` - Google Cloud Storage, using `gcsfs <https://gcsfs.readthedocs.io/en/stable/>`__. Run ``pip install fsspec[gcs]`` to install it.\n* Microsoft Azure Storage: ``adl://``, ``abfs://`` or ``az://`` - Microsoft Azure Storage, using `adlfs <https://github.com/fsspec/adlfs>`__. Run ``pip install fsspec[adl]`` to install it.\n* Hadoop File System: ``hdfs://`` - Hadoop Distributed File System. This uses `PyArrow <https://arrow.apache.org/docs/python/>`__ as the backend. Run ``pip install fsspec[hdfs]`` to install it.\n\nYou could learn more about the available filesystems with:\n\n.. code-block:: python\n\n    from fsspec.registry import known_implementations\n\n    print(known_implementations)\n\n\nYou could also look into :ref:`CheckpointIO Plugin <checkpointing_expert>` for more details on how to customize saving and loading checkpoints.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/tbptt.html", "url_rel_html": "common/tbptt.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/tbptt.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Truncated Backpropagation Through Time (TBPTT)¶", "rst_text": "##############################################\nTruncated Backpropagation Through Time (TBPTT)\n##############################################\n\nTruncated Backpropagation Through Time (TBPTT) performs backpropagation every k steps of\na much longer sequence. This is made possible by passing training batches\nsplit along the time-dimensions into splits of size k to the\n``training_step``. In order to keep the same forward propagation behavior, all\nhidden states should be kept in-between each time-dimension split.\n\n\n.. code-block:: python\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n\n    import lightning as L\n\n\n    class AverageDataset(Dataset):\n        def __init__(self, dataset_len=300, sequence_len=100):\n            self.dataset_len = dataset_len\n            self.sequence_len = sequence_len\n            self.input_seq = torch.randn(dataset_len, sequence_len, 10)\n            top, bottom = self.input_seq.chunk(2, -1)\n            self.output_seq = top + bottom.roll(shifts=1, dims=-1)\n\n        def __len__(self):\n            return self.dataset_len\n\n        def __getitem__(self, item):\n            return self.input_seq[item], self.output_seq[item]\n\n\n    class LitModel(L.LightningModule):\n\n        def __init__(self):\n            super().__init__()\n\n            self.batch_size = 10\n            self.in_features = 10\n            self.out_features = 5\n            self.hidden_dim = 20\n\n            # 1. Switch to manual optimization\n            self.automatic_optimization = False\n            self.truncated_bptt_steps = 10\n\n            self.rnn = nn.LSTM(self.in_features, self.hidden_dim, batch_first=True)\n            self.linear_out = nn.Linear(in_features=self.hidden_dim, out_features=self.out_features)\n\n        def forward(self, x, hs):\n            seq, hs = self.rnn(x, hs)\n            return self.linear_out(seq), hs\n\n        # 2. Remove the `hiddens` argument\n        def training_step(self, batch, batch_idx):\n            # 3. Split the batch in chunks along the time dimension\n            x, y = batch\n            split_x, split_y = [\n                x.tensor_split(self.truncated_bptt_steps, dim=1),\n                y.tensor_split(self.truncated_bptt_steps, dim=1)\n            ]\n\n            hiddens = None\n            optimizer = self.optimizers()\n            losses = []\n\n            # 4. Perform the optimization in a loop\n            for x, y in zip(split_x, split_y):\n                y_pred, hiddens = self(x, hiddens)\n                loss = F.mse_loss(y_pred, y)\n\n                optimizer.zero_grad()\n                self.manual_backward(loss)\n                optimizer.step()\n\n                # 5. \"Truncate\"\n                hiddens = [h.detach() for h in hiddens]\n                losses.append(loss.detach())\n\n            avg_loss = sum(losses) / len(losses)\n            self.log(\"train_loss\", avg_loss, prog_bar=True)\n\n            # 6. Remove the return of `hiddens`\n            # Returning loss in manual optimization is not needed\n            return None\n\n        def configure_optimizers(self):\n            return optim.Adam(self.parameters(), lr=0.001)\n\n        def train_dataloader(self):\n            return DataLoader(AverageDataset(), batch_size=self.batch_size)\n\n\n    if __name__ == \"__main__\":\n        model = LitModel()\n        trainer = L.Trainer(max_epochs=5)\n        trainer.fit(model)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common/trainer.html", "url_rel_html": "common/trainer.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common/trainer.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Trainer¶", "rst_text": ".. role:: hidden\n    :class: hidden-section\n\n.. testsetup:: *\n\n    import os\n    from lightning.pytorch import Trainer, LightningModule, seed_everything\n\n.. _trainer:\n\nTrainer\n=======\n\nOnce you've organized your PyTorch code into a :class:`~lightning.pytorch.core.LightningModule`, the ``Trainer`` automates everything else.\n\nThe ``Trainer`` achieves the following:\n\n1. You maintain control over all aspects via PyTorch code in your :class:`~lightning.pytorch.core.LightningModule`.\n\n2. The trainer uses best practices embedded by contributors and users\n   from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc...\n\n3. The trainer allows disabling any key part that you don't want automated.\n\n|\n\n-----------\n\nBasic use\n---------\n\nThis is the basic use of the trainer:\n\n.. code-block:: python\n\n    model = MyLightningModule()\n\n    trainer = Trainer()\n    trainer.fit(model, train_dataloader, val_dataloader)\n\n--------\n\nUnder the hood\n--------------\n\nThe Lightning ``Trainer`` does much more than just \"training\". Under the hood, it handles all loop details for you, some examples include:\n\n- Automatically enabling/disabling grads\n- Running the training, validation and test dataloaders\n- Calling the Callbacks at the appropriate times\n- Putting batches and computations on the correct devices\n\nHere's the pseudocode for what the trainer does under the hood (showing the train loop only)\n\n.. code-block:: python\n\n    # enable grads\n    torch.set_grad_enabled(True)\n\n    losses = []\n    for batch in train_dataloader:\n        # calls hooks like this one\n        on_train_batch_start()\n\n        # train step\n        loss = training_step(batch)\n\n        # clear gradients\n        optimizer.zero_grad()\n\n        # backward\n        loss.backward()\n\n        # update parameters\n        optimizer.step()\n\n        losses.append(loss)\n\n\n--------\n\nTrainer in Python scripts\n-------------------------\nIn Python scripts, it's recommended you use a main function to call the Trainer.\n\n.. code-block:: python\n\n    from argparse import ArgumentParser\n\n\n    def main(hparams):\n        model = LightningModule()\n        trainer = Trainer(accelerator=hparams.accelerator, devices=hparams.devices)\n        trainer.fit(model)\n\n\n    if __name__ == \"__main__\":\n        parser = ArgumentParser()\n        parser.add_argument(\"--accelerator\", default=None)\n        parser.add_argument(\"--devices\", default=None)\n        args = parser.parse_args()\n\n        main(args)\n\nSo you can run it like so:\n\n.. code-block:: bash\n\n    python main.py --accelerator 'gpu' --devices 2\n\n.. note::\n\n    Pro-tip: You don't need to define all flags manually.\n    You can let the :doc:`LightningCLI <../cli/lightning_cli>` create the Trainer and model with arguments supplied from the CLI.\n\n\nIf you want to stop a training run early, you can press \"Ctrl + C\" on your keyboard.\nThe trainer will catch the ``KeyboardInterrupt`` and attempt a graceful shutdown. The trainer object will also set\nan attribute ``interrupted`` to ``True`` in such cases. If you have a callback which shuts down compute\nresources, for example, you can conditionally run the shutdown logic for only uninterrupted runs by overriding :meth:`lightning.pytorch.Callback.on_exception`.\n\n------------\n\nValidation\n----------\nYou can perform an evaluation epoch over the validation set, outside of the training loop,\nusing :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`. This might be\nuseful if you want to collect new metrics from a model right at its initialization\nor after it has already been trained.\n\n.. code-block:: python\n\n    trainer.validate(model=model, dataloaders=val_dataloaders)\n\n------------\n\nTesting\n-------\nOnce you're done training, feel free to run the test set!\n(Only right before publishing your paper or pushing to production)\n\n.. code-block:: python\n\n    trainer.test(dataloaders=test_dataloaders)\n\n------------\n\nReproducibility\n---------------\n\nTo ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\nand set ``deterministic`` flag in ``Trainer``.\n\nExample::\n\n    from lightning.pytorch import Trainer, seed_everything\n\n    seed_everything(42, workers=True)\n    # sets seeds for numpy, torch and python.random.\n    model = Model()\n    trainer = Trainer(deterministic=True)\n\n\nBy setting ``workers=True`` in :func:`~lightning.pytorch.seed_everything`, Lightning derives\nunique seeds across all dataloader workers and processes for :mod:`torch`, :mod:`numpy` and stdlib\n:mod:`random` number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n\n-------\n\n.. _trainer_flags:\n\nTrainer flags\n-------------\n\naccelerator\n^^^^^^^^^^^\n\nSupports passing different accelerator types (``\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"auto\"``)\nas well as custom accelerator instances.\n\n.. code-block:: python\n\n    # CPU accelerator\n    trainer = Trainer(accelerator=\"cpu\")\n\n    # Training with GPU Accelerator using 2 GPUs\n    trainer = Trainer(devices=2, accelerator=\"gpu\")\n\n    # Training with TPU Accelerator using 8 tpu cores\n    trainer = Trainer(devices=8, accelerator=\"tpu\")\n\n    # Training with GPU Accelerator using the DistributedDataParallel strategy\n    trainer = Trainer(devices=4, accelerator=\"gpu\", strategy=\"ddp\")\n\n.. note:: The ``\"auto\"`` option recognizes the machine you are on, and selects the appropriate ``Accelerator``.\n\n.. code-block:: python\n\n    # If your machine has GPUs, it will use the GPU Accelerator for training\n    trainer = Trainer(devices=2, accelerator=\"auto\")\n\nYou can also modify hardware behavior by subclassing an existing accelerator to adjust for your needs.\n\nExample::\n\n    class MyOwnAcc(CPUAccelerator):\n        ...\n\n    Trainer(accelerator=MyOwnAcc())\n\n.. note::\n\n    If the ``devices`` flag is not defined, it will assume ``devices`` to be ``\"auto\"`` and fetch the ``auto_device_count``\n    from the accelerator.\n\n    .. code-block:: python\n\n        # This is part of the built-in `CUDAAccelerator`\n        class CUDAAccelerator(Accelerator):\n            \"\"\"Accelerator for GPU devices.\"\"\"\n\n            @staticmethod\n            def auto_device_count() -> int:\n                \"\"\"Get the devices when set to auto.\"\"\"\n                return torch.cuda.device_count()\n\n\n        # Training with GPU Accelerator using total number of gpus available on the system\n        Trainer(accelerator=\"gpu\")\n\naccumulate_grad_batches\n^^^^^^^^^^^^^^^^^^^^^^^\n\nAccumulates gradients over k batches before stepping the optimizer.\n\n.. testcode::\n\n    # default used by the Trainer (no accumulation)\n    trainer = Trainer(accumulate_grad_batches=1)\n\nExample::\n\n    # accumulate every 4 batches (effective batch size is batch*4)\n    trainer = Trainer(accumulate_grad_batches=4)\n\nSee also: :ref:`gradient_accumulation` to enable more fine-grained accumulation schedules.\n\n\nbarebones\n^^^^^^^^^\n\nWhether to run in \"barebones mode\", where all features that may impact raw speed are disabled. This is meant for\nanalyzing the Trainer overhead and is discouraged during regular training runs.\n\nWhen enabled, the following features are automatically deactivated:\n- Checkpointing: ``enable_checkpointing=False``\n- Logging: ``logger=False``, ``log_every_n_steps=0``\n- Progress bar: ``enable_progress_bar=False``\n- Model summary: ``enable_model_summary=False``\n- Sanity checking: ``num_sanity_val_steps=0``\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(barebones=False)\n\n    # enable barebones mode for speed analysis\n    trainer = Trainer(barebones=True)\n\nbenchmark\n^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/benchmark.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/benchmark.jpg\n    :width: 400\n    :muted:\n\nThe value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to. The value for\n``torch.backends.cudnn.benchmark`` set in the current session will be used (``False`` if not manually set).\nIf :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic` is set to ``True``, this will default to ``False``.\nYou can read more about the interaction of ``torch.backends.cudnn.benchmark`` and ``torch.backends.cudnn.deterministic``\n`here <https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking>`__\n\nSetting this flag to ``True`` can increase the speed of your system if your input sizes don't\nchange. However, if they do, then it might make your system slower. The CUDNN auto-tuner will try to find the best\nalgorithm for the hardware when a new input size is encountered. This might also increase the memory usage.\nRead more about it `here <https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936>`__.\n\nExample::\n\n    # Will use whatever the current value for torch.backends.cudnn.benchmark, normally False\n    trainer = Trainer(benchmark=None)  # default\n\n    # you can overwrite the value\n    trainer = Trainer(benchmark=True)\n\ndeterministic\n^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/deterministic.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/deterministic.jpg\n    :width: 400\n    :muted:\n\nThis flag sets the ``torch.backends.cudnn.deterministic`` flag.\nMight make your system slower, but ensures reproducibility.\n\nFor more info check `PyTorch docs <https://pytorch.org/docs/stable/notes/randomness.html>`_.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(deterministic=False)\n\ncallbacks\n^^^^^^^^^\n\nThis argument can be used to add a :class:`~lightning.pytorch.callbacks.callback.Callback` or a list of them.\nCallbacks run sequentially in the order defined here\nwith the exception of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks which run\nafter all others to ensure all states are saved to the checkpoints.\n\n.. code-block:: python\n\n    # single callback\n    trainer = Trainer(callbacks=PrintCallback())\n\n    # a list of callbacks\n    trainer = Trainer(callbacks=[PrintCallback()])\n\nExample::\n\n    from lightning.pytorch.callbacks import Callback\n\n    class PrintCallback(Callback):\n        def on_train_start(self, trainer, pl_module):\n            print(\"Training is started!\")\n        def on_train_end(self, trainer, pl_module):\n            print(\"Training is done.\")\n\n\nModel-specific callbacks can also be added inside the ``LightningModule`` through\n:meth:`~lightning.pytorch.core.LightningModule.configure_callbacks`.\nCallbacks returned in this hook will extend the list initially given to the ``Trainer`` argument, and replace\nthe trainer callbacks should there be two or more of the same type.\n:class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks always run last.\n\n\ncheck_val_every_n_epoch\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/check_val_every_n_epoch.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/check_val_every_n_epoch.jpg\n    :width: 400\n    :muted:\n\nCheck val every n train epochs.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(check_val_every_n_epoch=1)\n\n    # run val loop every 10 training epochs\n    trainer = Trainer(check_val_every_n_epoch=10)\n\n\ndefault_root_dir\n^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/default_root_dir.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/default%E2%80%A8_root_dir.jpg\n    :width: 400\n    :muted:\n\nDefault path for logs and weights when no logger or\n:class:`lightning.pytorch.callbacks.ModelCheckpoint` callback passed.  On\ncertain clusters you might want to separate where logs and checkpoints are\nstored. If you don't then use this argument for convenience. Paths can be local\npaths or remote paths such as ``s3://bucket/path`` or ``hdfs://path/``. Credentials\nwill need to be set up to use remote filepaths.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(default_root_dir=os.getcwd())\n\n\ndetect_anomaly\n^^^^^^^^^^^^^^\n\nEnable anomaly detection for the autograd engine. This will significantly slow down compute speed and is recommended\nonly for model debugging.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(detect_anomaly=False)\n\n    # enable anomaly detection for debugging\n    trainer = Trainer(detect_anomaly=True)\n\n\ndevices\n^^^^^^^\n\nNumber of devices to train on (``int``), which devices to train on (``list`` or ``str``), or ``\"auto\"``.\n\n.. code-block:: python\n\n    # Training with CPU Accelerator using 2 processes\n    trainer = Trainer(devices=2, accelerator=\"cpu\")\n\n    # Training with GPU Accelerator using GPUs 1 and 3\n    trainer = Trainer(devices=[1, 3], accelerator=\"gpu\")\n\n    # Training with TPU Accelerator using 8 tpu cores\n    trainer = Trainer(devices=8, accelerator=\"tpu\")\n\n.. tip:: The ``\"auto\"`` option recognizes the devices to train on, depending on the ``Accelerator`` being used.\n\n.. code-block:: python\n\n    # Use whatever hardware your machine has available\n    trainer = Trainer(devices=\"auto\", accelerator=\"auto\")\n\n    # Training with CPU Accelerator using 1 process\n    trainer = Trainer(devices=\"auto\", accelerator=\"cpu\")\n\n    # Training with TPU Accelerator using 8 tpu cores\n    trainer = Trainer(devices=\"auto\", accelerator=\"tpu\")\n\n.. note::\n\n    If the ``devices`` flag is not defined, it will assume ``devices`` to be ``\"auto\"`` and fetch the ``auto_device_count``\n    from the accelerator.\n\n    .. code-block:: python\n\n        # This is part of the built-in `CUDAAccelerator`\n        class CUDAAccelerator(Accelerator):\n            \"\"\"Accelerator for GPU devices.\"\"\"\n\n            @staticmethod\n            def auto_device_count() -> int:\n                \"\"\"Get the devices when set to auto.\"\"\"\n                return torch.cuda.device_count()\n\n\n        # Training with GPU Accelerator using total number of gpus available on the system\n        Trainer(accelerator=\"gpu\")\n\n\nenable_autolog_hparams\n^^^^^^^^^^^^^^^^^^^^^^\n\nWhether to log hyperparameters at the start of a run. Defaults to True.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(enable_autolog_hparams=True)\n\n    # disable logging hyperparams\n    trainer = Trainer(enable_autolog_hparams=False)\n\nWith the parameter set to false, you can add custom code to log hyperparameters.\n\n.. code-block:: python\n\n    model = LitModel()\n    trainer = Trainer(enable_autolog_hparams=False)\n    for logger in trainer.loggers:\n        if isinstance(logger, lightning.pytorch.loggers.CSVLogger):\n            logger.log_hyperparams(hparams_dict_1)\n        else:\n            logger.log_hyperparams(hparams_dict_2)\n\nYou can also use `self.logger.log_hyperparams(...)` inside `LightningModule` to log.\n\n\nenable_checkpointing\n^^^^^^^^^^^^^^^^^^^^\n\nBy default Lightning saves a checkpoint for you in your current working directory, with the state of your last training epoch,\nCheckpoints capture the exact value of all parameters used by a model.\nTo disable automatic checkpointing, set this to `False`.\n\n.. code-block:: python\n\n    # default used by Trainer, saves the most recent model to a single checkpoint after each epoch\n    trainer = Trainer(enable_checkpointing=True)\n\n    # turn off automatic checkpointing\n    trainer = Trainer(enable_checkpointing=False)\n\n\nYou can override the default behavior by initializing the :class:`~lightning.pytorch.callbacks.ModelCheckpoint`\ncallback, and adding it to the :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks` list.\nSee :doc:`Saving and Loading Checkpoints <../common/checkpointing>` for how to customize checkpointing.\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import ModelCheckpoint\n\n    # Init ModelCheckpoint callback, monitoring 'val_loss'\n    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n\n    # Add your callback to the callbacks list\n    trainer = Trainer(callbacks=[checkpoint_callback])\n\n\nenable_model_summary\n^^^^^^^^^^^^^^^^^^^^\n\nWhether to enable or disable the model summarization. Defaults to True.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(enable_model_summary=True)\n\n    # disable summarization\n    trainer = Trainer(enable_model_summary=False)\n\n    # enable custom summarization\n    from lightning.pytorch.callbacks import ModelSummary\n\n    trainer = Trainer(enable_model_summary=True, callbacks=[ModelSummary(max_depth=-1)])\n\n\nenable_progress_bar\n^^^^^^^^^^^^^^^^^^^\n\nWhether to enable or disable the progress bar. Defaults to True.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(enable_progress_bar=True)\n\n    # disable progress bar\n    trainer = Trainer(enable_progress_bar=False)\n\n\nfast_dev_run\n^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/fast_dev_run.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/fast_dev_run.jpg\n    :width: 400\n    :muted:\n\nRuns n if set to ``n`` (int) else 1 if set to ``True`` batch(es) to ensure your code will execute without errors. This\napplies to fitting, validating, testing, and predicting. This flag is **only** recommended for debugging purposes and\nshould not be used to limit the number of batches to run.\n\n.. code-block:: python\n\n    # default used by the Trainer\n    trainer = Trainer(fast_dev_run=False)\n\n    # runs only 1 training and 1 validation batch and the program ends\n    trainer = Trainer(fast_dev_run=True)\n    trainer.fit(...)\n\n    # runs 7 predict batches and program ends\n    trainer = Trainer(fast_dev_run=7)\n    trainer.predict(...)\n\nThis argument is different from ``limit_{train,val,test,predict}_batches`` because side effects are avoided to reduce the\nimpact to subsequent runs. These are the changes enabled:\n\n- Sets ``Trainer(max_epochs=1)``.\n- Sets ``Trainer(max_steps=...)`` to 1 or the number passed.\n- Sets ``Trainer(num_sanity_val_steps=0)``.\n- Sets ``Trainer(val_check_interval=1.0)``.\n- Sets ``Trainer(check_every_n_epoch=1)``.\n- Disables all loggers.\n- Disables passing logged metrics to loggers.\n- The :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks will not trigger.\n- The :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callbacks will not trigger.\n- Sets ``limit_{train,val,test,predict}_batches`` to 1 or the number passed.\n- Disables the tuning callbacks (:class:`~lightning.pytorch.callbacks.batch_size_finder.BatchSizeFinder`, :class:`~lightning.pytorch.callbacks.lr_finder.LearningRateFinder`).\n- If using the CLI, the configuration file is not saved.\n\n\ngradient_clip_algorithm\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"`` to clip by value, and\n``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will be set to ``\"norm\"``.\n\n.. testcode::\n\n    # default used by the Trainer (defaults to \"norm\" when gradient_clip_val is set)\n    trainer = Trainer(gradient_clip_algorithm=None)\n\n    # clip by value\n    trainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n\n    # clip by norm\n    trainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n\n\ngradient_clip_val\n^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/gradient_clip_val.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/gradient+_clip_val.jpg\n    :width: 400\n    :muted:\n\nGradient clipping value\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(gradient_clip_val=None)\n\n\ninference_mode\n^^^^^^^^^^^^^^\n\nWhether to use :func:`torch.inference_mode` or :func:`torch.no_grad` mode during evaluation\n(``validate``/``test``/``predict``)\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(inference_mode=True)\n\n    # Use `torch.no_grad` instead\n    trainer = Trainer(inference_mode=False)\n\n\nWith :func:`torch.inference_mode` disabled, you can enable the grad of your model layers if required.\n\n.. code-block:: python\n\n    class LitModel(LightningModule):\n        def validation_step(self, batch, batch_idx):\n            preds = self.layer1(batch)\n            with torch.enable_grad():\n                grad_preds = preds.requires_grad_()\n                preds2 = self.layer2(grad_preds)\n\n\n    model = LitModel()\n    trainer = Trainer(inference_mode=False)\n    trainer.validate(model)\n\n\nlimit_train_batches\n^^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_train_batches.jpg\n    :width: 400\n    :muted:\n\nHow much of training dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\nValue is per device.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(limit_train_batches=1.0)\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(limit_train_batches=1.0)\n\n    # run through only 25% of the training set each epoch\n    trainer = Trainer(limit_train_batches=0.25)\n\n    # run through only 10 batches of the training set each epoch\n    trainer = Trainer(limit_train_batches=10)\n\n\nlimit_predict_batches\n^^^^^^^^^^^^^^^^^^^^^\n\nHow much of prediction dataset to check. Value is per device.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(limit_predict_batches=1.0)\n\n    # run through only 25% of the prediction set\n    trainer = Trainer(limit_predict_batches=0.25)\n\n    # run for only 10 batches\n    trainer = Trainer(limit_predict_batches=10)\n\nIn the case of multiple prediction dataloaders, the limit applies to each dataloader individually.\n\n\nlimit_test_batches\n^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_test_batches.jpg\n    :width: 400\n    :muted:\n\nHow much of test dataset to check. Value is per device.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(limit_test_batches=1.0)\n\n    # run through only 25% of the test set each epoch\n    trainer = Trainer(limit_test_batches=0.25)\n\n    # run for only 10 batches\n    trainer = Trainer(limit_test_batches=10)\n\nIn the case of multiple test dataloaders, the limit applies to each dataloader individually.\n\nlimit_val_batches\n^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_val_batches.jpg\n    :width: 400\n    :muted:\n\nHow much of validation dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\nValue is per device.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(limit_val_batches=1.0)\n\n    # run through only 25% of the validation set each epoch\n    trainer = Trainer(limit_val_batches=0.25)\n\n    # run for only 10 batches\n    trainer = Trainer(limit_val_batches=10)\n\n    # disable validation\n    trainer = Trainer(limit_val_batches=0)\n\nIn the case of multiple validation dataloaders, the limit applies to each dataloader individually.\n\nlog_every_n_steps\n^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/log_every_n_steps.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/log_every_n_steps.jpg\n    :width: 400\n    :muted:\n\nHow often to add logging rows (does not write to disk)\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(log_every_n_steps=50)\n\nSee Also:\n    - :doc:`logging <../extensions/logging>`\n\nlogger\n^^^^^^\n\n:doc:`Logger <../visualize/loggers>` (or iterable collection of loggers) for experiment tracking. A ``True`` value uses the default ``TensorBoardLogger`` shown below. ``False`` will disable logging.\n\n.. testcode::\n    :skipif: not _TENSORBOARD_AVAILABLE and not _TENSORBOARDX_AVAILABLE\n\n    from lightning.pytorch.loggers import TensorBoardLogger\n\n    # default logger used by trainer (if tensorboard is installed)\n    logger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n    Trainer(logger=logger)\n\nmax_epochs\n^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_epochs.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/max_epochs.jpg\n    :width: 400\n    :muted:\n\nStop training once this number of epochs is reached\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(max_epochs=1000)\n\nIf both ``max_epochs`` and ``max_steps`` aren't specified, ``max_epochs`` will default to ``1000``.\nTo enable infinite training, set ``max_epochs = -1``.\n\nmin_epochs\n^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_epochs.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/min_epochs.jpg\n    :width: 400\n    :muted:\n\nForce training for at least these many epochs\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(min_epochs=1)\n\nmax_steps\n^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_steps.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/max_steps.jpg\n    :width: 400\n    :muted:\n\nStop training after this number of :ref:`global steps <common/trainer:global_step>`.\nTraining will stop if max_steps or max_epochs have reached (earliest).\n\n.. testcode::\n\n    # Default (disabled)\n    trainer = Trainer(max_steps=-1)\n\n    # Stop after 100 steps\n    trainer = Trainer(max_steps=100)\n\nIf ``max_steps`` is not specified, ``max_epochs`` will be used instead (and ``max_epochs`` defaults to\n``1000`` if ``max_epochs`` is not specified). To disable this default, set ``max_steps = -1``.\n\nmin_steps\n^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_steps.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/min_steps.jpg\n    :width: 400\n    :muted:\n\nForce training for at least this number of :ref:`global steps <common/trainer:global_step>`.\nTrainer will train model for at least min_steps or min_epochs (latest).\n\n.. testcode::\n\n    # Default (disabled)\n    trainer = Trainer(min_steps=None)\n\n    # Run at least for 100 steps (disable min_epochs)\n    trainer = Trainer(min_steps=100, min_epochs=0)\n\nmax_time\n^^^^^^^^\n\nSet the maximum amount of time for training. Training will get interrupted mid-epoch.\nFor customizable options use the :class:`~lightning.pytorch.callbacks.timer.Timer` callback.\n\n.. testcode::\n\n    # Default (disabled)\n    trainer = Trainer(max_time=None)\n\n    # Stop after 12 hours of training or when reaching 10 epochs (string)\n    trainer = Trainer(max_time=\"00:12:00:00\", max_epochs=10)\n\n    # Stop after 1 day and 5 hours (dict)\n    trainer = Trainer(max_time={\"days\": 1, \"hours\": 5})\n\nIn case ``max_time`` is used together with ``min_steps`` or ``min_epochs``, the ``min_*`` requirement\nalways has precedence.\n\n\nmodel_registry\n^^^^^^^^^^^^^^\n\nIf specified will upload the model to lightning model registry under the provided name.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(model_registry=None)\n\n    # specify model name for model hub upload\n    trainer = Trainer(model_registry=\"my-model-name\")\n\nSee `Lightning model registry docs <https://lightning.ai/docs/overview/finetune-models/model-registry>`_ for more info.\n\n\nnum_nodes\n^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/num_nodes.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/num_nodes.jpg\n    :width: 400\n    :muted:\n\nNumber of GPU nodes for distributed training.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(num_nodes=1)\n\n    # to train on 8 nodes\n    trainer = Trainer(num_nodes=8)\n\n\nnum_sanity_val_steps\n^^^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/num_sanity_val_steps.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/num_sanity%E2%80%A8_val_steps.jp\n    :width: 400\n    :muted:\n\nSanity check runs n batches of val before starting the training routine.\nThis catches any bugs in your validation without having to wait for the first validation check.\nThe Trainer uses 2 steps by default. Turn it off or modify it here.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(num_sanity_val_steps=2)\n\n    # turn it off\n    trainer = Trainer(num_sanity_val_steps=0)\n\n    # check all validation data\n    trainer = Trainer(num_sanity_val_steps=-1)\n\n\nThis option will reset the validation dataloader unless ``num_sanity_val_steps=0``.\n\noverfit_batches\n^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/overfit_batches.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/overfit_batches.jpg\n    :width: 400\n    :muted:\n\nUses this much data of the training & validation set.\nIf the training & validation dataloaders have ``shuffle=True``, Lightning will automatically disable it.\n\n* When set to a value > 0, sequential sampling (no shuffling) is used\n* Consistent batches are used for both training and validation across epochs, but training and validation use different sets of data\n\nUseful for quickly debugging or trying to overfit on purpose.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(overfit_batches=0.0)\n\n    # use only 1% of the train & val set\n    trainer = Trainer(overfit_batches=0.01)\n\n    # overfit on 10 consistent train batches & 10 consistent val batches\n    trainer = Trainer(overfit_batches=10)\n\n    # debug using a single consistent train batch and a single consistent val batch\n\nplugins\n^^^^^^^\n\nPlugins allow you to connect arbitrary backends, precision libraries, clusters etc. and modification of core lightning logic.\nExamples of plugin types:\n- :ref:`Checkpoint IO <checkpointing_expert>`\n- `TorchElastic <https://pytorch.org/elastic/0.2.2/index.html>`_\n- :ref:`Precision Plugins <precision_expert>`\n- :class:`~lightning.pytorch.plugins.environments.ClusterEnvironment`\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(plugins=None)\n\n    # example using built in slurm plugin\n    from lightning.fabric.plugins.environments import SLURMEnvironment\n    trainer = Trainer(plugins=[SLURMEnvironment()])\n\n\nTo define your own behavior, subclass the relevant class and pass it in. Here's an example linking up your own\n:class:`~lightning.pytorch.plugins.environments.ClusterEnvironment`.\n\n.. code-block:: python\n\n    from lightning.pytorch.plugins.environments import ClusterEnvironment\n\n\n    class MyCluster(ClusterEnvironment):\n        def main_address(self):\n            return your_main_address\n\n        def main_port(self):\n            return your_main_port\n\n        def world_size(self):\n            return the_world_size\n\n\n    trainer = Trainer(plugins=[MyCluster()], ...)\n\nprecision\n^^^^^^^^^\n\nThere are two different techniques to set the mixed precision. \"True\" precision and \"Mixed\" precision.\n\nLightning supports doing floating point operations in 64-bit precision (\"double\"), 32-bit precision (\"full\"), or 16-bit (\"half\") with both regular and `bfloat16 <https://pytorch.org/docs/1.10.0/generated/torch.Tensor.bfloat16.html>`_).\nThis selected precision will have a direct impact in the performance and memory usage based on your hardware.\nAutomatic mixed precision settings are denoted by a ``\"-mixed\"`` suffix, while \"true\" precision settings have a ``\"-true\"`` suffix:\n\n.. code-block:: python\n\n    # Default used by the Trainer\n    fabric = Fabric(precision=\"32-true\", devices=1)\n\n    # the same as:\n    trainer = Trainer(precision=\"32\", devices=1)\n\n    # 16-bit mixed precision (model weights remain in torch.float32)\n    trainer = Trainer(precision=\"16-mixed\", devices=1)\n\n    # 16-bit bfloat mixed precision (model weights remain in torch.float32)\n    trainer = Trainer(precision=\"bf16-mixed\", devices=1)\n\n    # 8-bit mixed precision via TransformerEngine (model weights get cast to torch.bfloat16)\n    trainer = Trainer(precision=\"transformer-engine\", devices=1)\n\n    # 16-bit precision (model weights get cast to torch.float16)\n    trainer = Trainer(precision=\"16-true\", devices=1)\n\n    # 16-bit bfloat precision (model weights get cast to torch.bfloat16)\n    trainer = Trainer(precision=\"bf16-true\", devices=1)\n\n    # 64-bit (double) precision (model weights get cast to torch.float64)\n    trainer = Trainer(precision=\"64-true\", devices=1)\n\n\nSee the :doc:`N-bit precision guide <../common/precision>` for more details.\n\n\nprofiler\n^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/profiler.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/profiler.jpg\n    :width: 400\n    :muted:\n\nTo profile individual steps during training and assist in identifying bottlenecks.\n\nSee the :doc:`profiler documentation <../tuning/profiler>` for more details.\n\n.. testcode::\n\n    from lightning.pytorch.profilers import SimpleProfiler, AdvancedProfiler\n\n    # default used by the Trainer\n    trainer = Trainer(profiler=None)\n\n    # to profile standard training events, equivalent to `profiler=SimpleProfiler()`\n    trainer = Trainer(profiler=\"simple\")\n\n    # advanced profiler for function-level stats, equivalent to `profiler=AdvancedProfiler()`\n    trainer = Trainer(profiler=\"advanced\")\n\n\nreload_dataloaders_every_n_epochs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/reload_dataloaders_every_epoch.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/reload_%E2%80%A8dataloaders_%E2%80%A8every_epoch.jpg\n    :width: 400\n    :muted:\n\nSet to a positive integer to reload dataloaders every n epochs from your currently used data source.\nDataSource can be a ``LightningModule`` or a ``LightningDataModule``.\n\n\n.. code-block:: python\n\n    # if 0 (default)\n    train_loader = model.train_dataloader()\n    # or if using data module: datamodule.train_dataloaders()\n    for epoch in epochs:\n        for batch in train_loader:\n            ...\n\n    # if a positive integer\n    for epoch in epochs:\n        if not epoch % reload_dataloaders_every_n_epochs:\n            train_loader = model.train_dataloader()\n            # or if using data module: datamodule.train_dataloader()\n        for batch in train_loader:\n            ...\n\nThe pseudocode applies also to the ``val_dataloader``.\n\n.. _replace-sampler-ddp:\n\n\nstrategy\n^^^^^^^^\n\nSupports passing different training strategies with aliases (ddp, fsdp, etc) as well as configured strategies.\n\n.. code-block:: python\n\n    # Data-parallel training with the DDP strategy on 4 GPUs\n    trainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n    # Model-parallel training with the FSDP strategy on 4 GPUs\n    trainer = Trainer(strategy=\"fsdp\", accelerator=\"gpu\", devices=4)\n\nAdditionally, you can pass a strategy object.\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import DDPStrategy\n\n    trainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=2)\n\nSee Also:\n    - :ref:`Multi GPU Training <multi_gpu>`.\n    - :doc:`Model Parallel GPU training guide <../advanced/model_parallel>`.\n    - :doc:`TPU training guide <../accelerators/tpu>`.\n\n\nsync_batchnorm\n^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/sync_batchnorm.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/sync_batchnorm.jpg\n    :width: 400\n    :muted:\n\nEnable synchronization between batchnorm layers across all GPUs.\n\n.. testcode::\n\n    trainer = Trainer(sync_batchnorm=True)\n\n\nuse_distributed_sampler\n^^^^^^^^^^^^^^^^^^^^^^^\n\nSee :paramref:`lightning.pytorch.trainer.Trainer.params.use_distributed_sampler`.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(use_distributed_sampler=True)\n\nBy setting to False, you have to add your own distributed sampler:\n\n.. code-block:: python\n\n    # in your LightningModule or LightningDataModule\n    def train_dataloader(self):\n        dataset = ...\n        # default used by the Trainer\n        sampler = torch.utils.data.DistributedSampler(dataset, shuffle=True)\n        dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n        return dataloader\n\n\nval_check_interval\n^^^^^^^^^^^^^^^^^^\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/val_check_interval.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/val_check_interval.jpg\n    :width: 400\n    :muted:\n\nHow often within one training epoch to check the validation set.\nCan specify as float, int, or a time-based duration.\n\n- pass a ``float`` in the range [0.0, 1.0] to check after a fraction of the training epoch.\n- pass an ``int`` to check after a fixed number of training batches. An ``int`` value can only be higher than the number of training\n  batches when ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches across epochs or iteration-based training.\n- pass a ``string`` duration in the format \"DD:HH:MM:SS\", a ``datetime.timedelta`` object, or a ``dictionary`` of keyword arguments that can be passed\n  to ``datetime.timedelta`` for time-based validation. When using a time-based duration, validation will trigger once the elapsed wall-clock time\n  since the last validation exceeds the interval. The validation check occurs after the current batch completes, the validation loop runs, and\n  the timer resets.\n\n**Time-based validation behavior with check_val_every_n_epoch:**  When used together with ``val_check_interval`` (time-based) and\n``check_val_every_n_epoch > 1``, validation is aligned to epoch multiples:\n\n- If the time-based interval elapses **before** the next multiple-N epoch, validation runs at the start of that epoch (after the first batch),\n  and the timer resets.\n- If the interval elapses **during** a multiple-N epoch, validation runs after the current batch.\n- For cases where ``check_val_every_n_epoch=None`` or ``1``, the time-based behavior of ``val_check_interval`` applies without additional alignment.\n\n.. testcode::\n\n    # default used by the Trainer\n    trainer = Trainer(val_check_interval=1.0)\n\n    # check validation set 4 times during a training epoch\n    trainer = Trainer(val_check_interval=0.25)\n\n    # check validation set every 1000 training batches in the current epoch\n    trainer = Trainer(val_check_interval=1000)\n\n    # check validation set every 1000 training batches across complete epochs or during iteration-based training\n    # use this when using iterableDataset and your dataset has no length\n    # (ie: production cases with streaming data)\n    trainer = Trainer(val_check_interval=1000, check_val_every_n_epoch=None)\n\n    # check validation every 15 minutes of wall-clock time using a string-based approach\n    trainer = Trainer(val_check_interval=\"00:00:15:00\")\n\n    # check validation every 15 minutes of wall-clock time using a dictionary-based approach\n    trainer = Trainer(val_check_interval={\"minutes\": 15})\n\n    # check validation every 1 hour of wall-clock time using a dictionary-based approach\n    trainer = Trainer(val_check_interval={\"hours\": 1})\n\n    # check validation every 1 hour of wall-clock time using a datetime.timedelta object\n    from datetime import timedelta\n    trainer = Trainer(val_check_interval=timedelta(hours=1))\n\n\n\n.. code-block:: python\n\n    # Here is the computation to estimate the total number of batches seen within an epoch.\n    # This logic applies when `val_check_interval` is specified as an integer or a float.\n\n    # Find the total number of train batches\n    total_train_batches = total_train_samples // (train_batch_size * world_size)\n\n    # Compute how many times we will call validation during the training loop\n    val_check_batch = max(1, int(total_train_batches * val_check_interval))\n    val_checks_per_epoch = total_train_batches / val_check_batch\n\n    # Find the total number of validation batches\n    total_val_batches = total_val_samples // (val_batch_size * world_size)\n\n    # Total number of batches run\n    total_fit_batches = total_train_batches + total_val_batches\n\n-----\n\nTrainer class API\n-----------------\n\nMethods\n^^^^^^^\n\ninit\n****\n\n.. automethod:: lightning.pytorch.trainer.Trainer.__init__\n   :noindex:\n\nfit\n****\n\n.. automethod:: lightning.pytorch.trainer.Trainer.fit\n   :noindex:\n\nvalidate\n********\n\n.. automethod:: lightning.pytorch.trainer.Trainer.validate\n   :noindex:\n\ntest\n****\n\n.. automethod:: lightning.pytorch.trainer.Trainer.test\n   :noindex:\n\npredict\n*******\n\n.. automethod:: lightning.pytorch.trainer.Trainer.predict\n   :noindex:\n\n\nProperties\n^^^^^^^^^^\n\ncallback_metrics\n****************\n\nThe metrics available to callbacks.\n\nThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log`.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        self.log(\"a_val\", 2.0)\n\n\n    callback_metrics = trainer.callback_metrics\n    assert callback_metrics[\"a_val\"] == 2.0\n\nlogged_metrics\n**************\n\nThe metrics sent to the loggers.\n\nThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\n:paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\n\nprogress_bar_metrics\n********************\n\nThe metrics sent to the progress bar.\n\nThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\n:paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\n\ncurrent_epoch\n*************\n\nThe current epoch, updated after the epoch end hooks are run.\n\ndatamodule\n**********\n\nThe current datamodule, which is used by the trainer.\n\n.. code-block:: python\n\n    used_datamodule = trainer.datamodule\n\nis_last_batch\n*************\n\nWhether trainer is executing the last batch.\n\nglobal_step\n***********\n\nThe number of optimizer steps taken (does not reset each epoch).\n\nThis includes multiple optimizers (if enabled).\n\nlogger\n*******\n\nThe first :class:`~lightning.pytorch.loggers.logger.Logger` being used.\n\nloggers\n********\n\nThe list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\n\n.. code-block:: python\n\n    for logger in trainer.loggers:\n        logger.log_metrics({\"foo\": 1.0})\n\nlog_dir\n*******\n\nThe directory for the current experiment. Use this to save images to, etc...\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        img = ...\n        save_img(img, self.trainer.log_dir)\n\nis_global_zero\n**************\n\nWhether this process is the global zero in multi-node training.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        if self.trainer.is_global_zero:\n            print(\"in node 0, accelerator 0\")\n\nestimated_stepping_batches\n**************************\n\nThe estimated number of batches that will ``optimizer.step()`` during training.\n\nThis accounts for gradient accumulation and the current trainer configuration. This might sets up your training\ndataloader if hadn't been set up already.\n\n.. code-block:: python\n\n    def configure_optimizers(self):\n        optimizer = ...\n        stepping_batches = self.trainer.estimated_stepping_batches\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n        }\n\nstate\n*****\n\nThe current state of the Trainer, including the current function that is running, the stage of\nexecution within that function, and the status of the Trainer.\n\n.. code-block:: python\n\n    # fn in (\"fit\", \"validate\", \"test\", \"predict\")\n    trainer.state.fn\n    # status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\n    trainer.state.status\n    # stage in (\"train\", \"sanity_check\", \"validate\", \"test\", \"predict\")\n    trainer.state.stage\n\nshould_stop\n***********\n\nIf you want to terminate the training during ``.fit``, you can set ``trainer.should_stop=True`` to terminate the training\nas soon as possible. Note that, it will respect the arguments ``min_steps`` and ``min_epochs`` to check whether to stop. If these\narguments are set and the ``current_epoch`` or ``global_step`` don't meet these minimum conditions, training will continue until\nboth conditions are met. If any of these arguments is not set, it won't be considered for the final decision.\n\n\n.. code-block:: python\n\n    # setting `trainer.should_stop` at any point of training will terminate it\n    class LitModel(LightningModule):\n        def training_step(self, *args, **kwargs):\n            self.trainer.should_stop = True\n\n\n    trainer = Trainer()\n    model = LitModel()\n    trainer.fit(model)\n\n.. code-block:: python\n\n    # setting `trainer.should_stop` will stop training only after at least 5 epochs have run\n    class LitModel(LightningModule):\n        def training_step(self, *args, **kwargs):\n            if self.current_epoch == 2:\n                self.trainer.should_stop = True\n\n\n    trainer = Trainer(min_epochs=5, max_epochs=100)\n    model = LitModel()\n    trainer.fit(model)\n\n.. code-block:: python\n\n    # setting `trainer.should_stop` will stop training only after at least 5 steps have run\n    class LitModel(LightningModule):\n        def training_step(self, *args, **kwargs):\n            if self.global_step == 2:\n                self.trainer.should_stop = True\n\n\n    trainer = Trainer(min_steps=5, max_epochs=100)\n    model = LitModel()\n    trainer.fit(model)\n\n.. code-block:: python\n\n    # setting `trainer.should_stop` at any until both min_steps and min_epochs are satisfied\n    class LitModel(LightningModule):\n        def training_step(self, *args, **kwargs):\n            if self.global_step == 7:\n                self.trainer.should_stop = True\n\n\n    trainer = Trainer(min_steps=5, min_epochs=5, max_epochs=100)\n    model = LitModel()\n    trainer.fit(model)\n\nsanity_checking\n***************\n\nIndicates if the trainer is currently running sanity checking. This property can be useful to disable some hooks,\nlogging or callbacks during the sanity checking.\n\n.. code-block:: python\n\n    def validation_step(self, batch, batch_idx):\n        ...\n        if not self.trainer.sanity_checking:\n            self.log(\"value\", value)\n\nnum_training_batches\n********************\n\nThe number of training batches that will be used during ``trainer.fit()``.\n\nnum_sanity_val_batches\n**********************\n\nThe number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\n\nnum_val_batches\n***************\n\nThe number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\n\nnum_test_batches\n****************\n\nThe number of test batches that will be used during ``trainer.test()``.\n\nnum_predict_batches\n*******************\n\nThe number of prediction batches that will be used during ``trainer.predict()``.\n\ntrain_dataloader\n****************\n\nThe training dataloader(s) used during ``trainer.fit()``.\n\nval_dataloaders\n***************\n\nThe validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\n\ntest_dataloaders\n****************\n\nThe test dataloader(s) used during ``trainer.test()``.\n\npredict_dataloaders\n*******************\n\nThe prediction dataloader(s) used during ``trainer.predict()``.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/common_usecases.html", "url_rel_html": "common_usecases.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/common_usecases.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Common Workflows¶", "rst_text": ":orphan:\n\n################\nCommon Workflows\n################\n\nCustomize and extend Lightning for things like custom hardware or distributed strategies.\n\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Avoid overfitting\n   :description: Add a training and test loop.\n   :col_css: col-md-12\n   :button_link: common/evaluation.html\n   :height: 100\n\n.. displayitem::\n   :header: Build a model\n   :description: Steps to build a model.\n   :col_css: col-md-12\n   :button_link: model/build_model.html\n   :height: 100\n\n.. displayitem::\n   :header: Configure hyperparameters from the CLI\n   :description: Enable basic CLI with Lightning.\n   :col_css: col-md-12\n   :button_link: common/hyperparameters.html\n   :height: 100\n\n.. displayitem::\n   :header: Customize the progress bar\n   :description: Change the progress bar behavior.\n   :col_css: col-md-12\n   :button_link: common/progress_bar.html\n   :height: 100\n\n.. displayitem::\n   :header: Deploy models into production\n   :description: Deploy models with different levels of scale.\n   :col_css: col-md-12\n   :button_link: deploy/production.html\n   :height: 100\n\n.. displayitem::\n   :header: Effective Training Techniques\n   :description: Explore advanced training techniques.\n   :col_css: col-md-12\n   :button_link: advanced/training_tricks.html\n   :height: 100\n\n.. displayitem::\n   :header: Eliminate config boilerplate\n   :description: Control your training via CLI and YAML.\n   :col_css: col-md-12\n   :button_link: cli/lightning_cli.html\n   :height: 100\n\n.. displayitem::\n   :header: Find bottlenecks in your code\n   :description: Learn to find bottlenecks in your code.\n   :col_css: col-md-12\n   :button_link: tuning/profiler.html\n   :height: 100\n\n.. displayitem::\n   :header: Finetune a model\n   :description: Learn to use pretrained models\n   :col_css: col-md-12\n   :button_link: advanced/transfer_learning.html\n   :height: 100\n\n.. displayitem::\n   :header: Manage Experiments\n   :description: Learn to track and visualize experiments\n   :col_css: col-md-12\n   :button_link: visualize/logging_intermediate.html\n   :height: 100\n\n.. displayitem::\n   :header: Run on a multi-node cluster\n   :description: Learn to run multi-node in the cloud or on your cluster\n   :col_css: col-md-12\n   :button_link: clouds/cluster.html\n   :height: 100\n\n.. displayitem::\n   :header: Save and load model progress\n   :description: Save and load progress with checkpoints.\n   :col_css: col-md-12\n   :button_link: common/checkpointing_basic.html\n   :height: 100\n\n.. displayitem::\n   :header: Save memory with half-precision\n   :description: Enable half-precision to train faster and save memory.\n   :col_css: col-md-12\n   :button_link: common/precision.html\n   :height: 100\n\n.. displayitem::\n   :header: Train models with billions of parameters\n   :description: Scale GPU training to models with billions of parameters\n   :col_css: col-md-12\n   :button_link: advanced/model_parallel/index.html\n   :height: 100\n\n.. displayitem::\n   :header: Train in a notebook\n   :description: Train models in interactive notebooks (Jupyter, Colab, Kaggle, etc.)\n   :col_css: col-md-12\n   :button_link: common/notebooks.html\n   :height: 100\n\n.. displayitem::\n   :header: Train on single or multiple GPUs\n   :description: Train models faster with GPUs.\n   :col_css: col-md-12\n   :button_link: accelerators/gpu.html\n   :height: 100\n\n.. displayitem::\n   :header: Train on single or multiple HPUs\n   :description: Train models faster with HPUs.\n   :col_css: col-md-12\n   :button_link: integrations/hpu/index.html\n   :height: 100\n\n.. displayitem::\n   :header: Train on single or multiple TPUs\n   :description: Train models faster with TPUs.\n   :col_css: col-md-12\n   :button_link: accelerators/tpu.html\n   :height: 100\n\n.. displayitem::\n   :header: Track and Visualize Experiments\n   :description: Learn to track and visualize experiments\n   :col_css: col-md-12\n   :button_link: visualize/logging_intermediate.html\n   :height: 100\n\n.. displayitem::\n   :header: Use a pure PyTorch training loop\n   :description: Run your pure PyTorch loop with Lightning.\n   :col_css: col-md-12\n   :button_link: model/own_your_loop.html\n   :height: 100\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/data/access.html", "url_rel_html": "data/access.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/data/access.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Accessing DataLoaders¶", "rst_text": ":orphan:\n\nAccessing DataLoaders\n=====================\n\nIn the case that you require access to the :class:`torch.utils.data.DataLoader` or :class:`torch.utils.data.Dataset` objects, DataLoaders for each step can be accessed\nvia the trainer properties :meth:`~lightning.pytorch.trainer.trainer.Trainer.train_dataloader`,\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.val_dataloaders`,\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.test_dataloaders`, and\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.predict_dataloaders`.\n\n.. code-block:: python\n\n    dataloaders = trainer.train_dataloader\n    dataloaders = trainer.val_dataloaders\n    dataloaders = trainer.test_dataloaders\n    dataloaders = trainer.predict_dataloaders\n\nThese properties will match exactly what was returned in your ``*_dataloader`` hooks or passed to the ``Trainer``,\nmeaning that if you returned a dictionary of dataloaders, these will return a dictionary of dataloaders.\n\nReplacing DataLoaders\n---------------------\n\nIf you are using a :class:`~lightning.pytorch.utilities.CombinedLoader`. A flattened list of DataLoaders can be accessed by doing:\n\n.. code-block:: python\n\n    from lightning.pytorch.utilities import CombinedLoader\n\n    iterables = {\"dl1\": dl1, \"dl2\": dl2}\n    combined_loader = CombinedLoader(iterables)\n    # access the original iterables\n    assert combined_loader.iterables is iterables\n    # the `.flattened` property can be convenient\n    assert combined_loader.flattened == [dl1, dl2]\n    # for example, to do a simple loop\n    updated = []\n    for dl in combined_loader.flattened:\n        new_dl = apply_some_transformation_to(dl)\n        updated.append(new_dl)\n    # it also allows you to easily replace the dataloaders\n    combined_loader.flattened = updated\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/data/alternatives.html", "url_rel_html": "data/alternatives.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/data/alternatives.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Using 3rd Party Data Iterables¶", "rst_text": ":orphan:\n\n.. _dataiters:\n\nUsing 3rd Party Data Iterables\n==============================\n\nWhen training a model on a specific task, data loading and preprocessing might become a bottleneck.\nLightning does not enforce a specific data loading approach nor does it try to control it.\nThe only assumption Lightning makes is that a valid iterable is provided.\n\nFor PyTorch-based programs, these iterables are typically instances of :class:`~torch.utils.data.DataLoader`.\nHowever, Lightning also supports other data types such as a list of batches, generators, or other custom iterables or\ncollections of the former.\n\n.. code-block:: python\n\n    # random list of batches\n    data = [(torch.rand(32, 3, 32, 32), torch.randint(0, 10, (32,))) for _ in range(100)]\n    model = LitClassifier()\n    trainer = Trainer()\n    trainer.fit(model, data)\n\nBelow we showcase Lightning examples with packages that compete with the generic PyTorch DataLoader and might be\nfaster depending on your use case. They might require custom data serialization, loading, and preprocessing that\nis often hardware accelerated.\n\nStreamingDataset\n^^^^^^^^^^^^^^^^\n\nAs datasets grow in size and the number of nodes scales, loading training data can become a significant challenge.\nThe `StreamingDataset <https://github.com/mosaicml/streaming>`__ can make training on large datasets from cloud storage\nas fast, cheap, and scalable as possible.\n\nThis library uses a custom built :class:`~torch.utils.data.IterableDataset`. The library recommends iterating through it\nvia a regular :class:`~torch.utils.data.DataLoader`. This means that support in the ``Trainer`` is seamless:\n\n.. code-block:: python\n\n    import lightning as L\n    from streaming import MDSWriter, StreamingDataset\n\n\n    class YourDataset(StreamingDataset):\n        ...\n\n\n    # you could do this in the `prepare_data` hook too\n    with MDSWriter(out=\"...\", columns=...) as out:\n        out.write(...)\n\n    train_dataset = YourDataset()\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n    model = ...\n    trainer = L.Trainer()\n    trainer.fit(model, train_dataloader)\n\nFFCV\n^^^^\n\nTaking the example from the `FFCV <https://github.com/libffcv/ffcv>`__ readme, we can use it with Lightning\nby just removing the hardcoded ``ToDevice(0)`` as Lightning takes care of GPU placement. In case you want to use some\ndata transformations on GPUs, change the ``ToDevice(0)`` to ``ToDevice(self.trainer.local_rank)`` to correctly map to\nthe desired GPU in your pipeline. When moving data to a specific device, you can always refer to\n``self.trainer.local_rank`` to get the accelerator used by the current process.\n\n.. code-block:: python\n\n    import lightning as L\n    from ffcv.loader import Loader, OrderOption\n    from ffcv.transforms import ToTensor, ToDevice, ToTorchImage, Cutout\n    from ffcv.fields.decoders import IntDecoder, RandomResizedCropRGBImageDecoder\n\n    # Random resized crop\n    decoder = RandomResizedCropRGBImageDecoder((224, 224))\n    # Data decoding and augmentation\n    image_pipeline = [decoder, Cutout(), ToTensor(), ToTorchImage()]\n    label_pipeline = [IntDecoder(), ToTensor()]\n    # Pipeline for each data field\n    pipelines = {\"image\": image_pipeline, \"label\": label_pipeline}\n    # Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\n    train_dataloader = Loader(\n        write_path, batch_size=bs, num_workers=num_workers, order=OrderOption.RANDOM, pipelines=pipelines\n    )\n\n    model = ...\n    trainer = L.Trainer()\n    trainer.fit(model, train_dataloader)\n\nWebDataset\n^^^^^^^^^^\n\nThe `WebDataset <https://github.com/webdataset/webdataset>`__ makes it easy to write I/O pipelines for large datasets.\nDatasets can be stored locally or in the cloud. ``WebDataset`` is just an instance of a standard IterableDataset.\nThe webdataset library contains a small wrapper (``WebLoader``) that adds a fluid interface to the DataLoader (and is otherwise identical).\n\n.. code-block:: python\n\n    import lightning as L\n    import webdataset as wds\n\n    dataset = wds.WebDataset(\n        urls,\n        # needed for multi-gpu or multi-node training\n        workersplitter=wds.shardlists.split_by_worker,\n        nodesplitter=wds.shardlists.split_by_node,\n    )\n    train_dataloader = wds.WebLoader(dataset)\n\n    model = ...\n    trainer = L.Trainer()\n    trainer.fit(model, train_dataloader)\n\nYou can find a complete example `here <https://github.com/webdataset/webdataset-lightning>`__.\n\nNVIDIA DALI\n^^^^^^^^^^^\n\nBy just changing ``device_id=0`` to ``device_id=self.trainer.local_rank`` we can also leverage DALI's GPU decoding:\n\n.. code-block:: python\n\n    import lightning as L\n    from nvidia.dali.pipeline import pipeline_def\n    import nvidia.dali.types as types\n    import nvidia.dali.fn as fn\n    from nvidia.dali.plugin.pytorch import DALIGenericIterator\n    import os\n\n    # To run with different data, see documentation of nvidia.dali.fn.readers.file\n    # points to https://github.com/NVIDIA/DALI_extra\n    data_root_dir = os.environ[\"DALI_EXTRA_PATH\"]\n    images_dir = os.path.join(data_root_dir, \"db\", \"single\", \"jpeg\")\n\n\n    @pipeline_def(num_threads=4, device_id=self.trainer.local_rank)\n    def get_dali_pipeline():\n        images, labels = fn.readers.file(file_root=images_dir, random_shuffle=True, name=\"Reader\")\n        # decode data on the GPU\n        images = fn.decoders.image_random_crop(images, device=\"mixed\", output_type=types.RGB)\n        # the rest of processing happens on the GPU as well\n        images = fn.resize(images, resize_x=256, resize_y=256)\n        images = fn.crop_mirror_normalize(\n            images,\n            crop_h=224,\n            crop_w=224,\n            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n            std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n            mirror=fn.random.coin_flip(),\n        )\n        return images, labels\n\n\n    train_dataloader = DALIGenericIterator(\n        [get_dali_pipeline(batch_size=16)],\n        [\"data\", \"label\"],\n        reader_name=\"Reader\",\n    )\n\n    model = ...\n    trainer = L.Trainer()\n    trainer.fit(model, train_dataloader)\n\nYou can find a complete tutorial `here <https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/frameworks/pytorch/pytorch-lightning.html>`__.\n\n\nLimitations\n------------\nLightning works with all kinds of custom data iterables as shown above. There are, however, a few features that cannot\nbe supported this way. These restrictions come from the fact that for their support,\nLightning needs to know a lot on the internals of these iterables.\n\n- In a distributed multi-GPU setting (ddp), Lightning wraps the DataLoader's sampler with a wrapper for distributed\n  support. This makes sure that each GPU sees a different part of the dataset. As sampling can be implemented in\n  arbitrary ways with custom iterables, Lightning might not be able to do this for you. If this is the case, you can use\n  the :paramref:`~lightning.pytorch.trainer.trainer.Trainer.use_distributed_sampler` argument to disable this logic and\n  set the distributed sampler yourself.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/data/data.html", "url_rel_html": "data/data.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/data/data.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Complex data uses¶", "rst_text": ".. _data:\n\nComplex data uses\n=================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: LightningDataModules\n   :description: Introduction to the LightningDataModule\n   :col_css: col-md-4\n   :button_link: datamodule.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Iterables\n   :description: What is an iterable? How do I use them?\n   :col_css: col-md-4\n   :button_link: iterables.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Access your data\n   :description: How to access your dataloaders\n   :col_css: col-md-4\n   :button_link: access.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Faster DataLoaders\n   :description: How alternative dataloader projects can be used with Lightning\n   :col_css: col-md-4\n   :button_link: alternatives.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/data/datamodule.html", "url_rel_html": "data/datamodule.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/data/datamodule.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "LightningDataModule¶", "rst_text": ".. _datamodules:\n\n###################\nLightningDataModule\n###################\nA datamodule is a shareable, reusable class that encapsulates all the steps needed to process data:\n\n.. video:: https://pl-public-data.s3.amazonaws.com/assets_lightning/pt_dm_vid.mp4\n    :width: 400\n    :autoplay:\n    :loop:\n    :muted:\n\nA datamodule encapsulates the five steps involved in data processing in PyTorch:\n\n1. Download / tokenize / process.\n2. Clean and (maybe) save to disk.\n3. Load inside :class:`~torch.utils.data.Dataset`.\n4. Apply transforms (rotate, tokenize, etc...).\n5. Wrap inside a :class:`~torch.utils.data.DataLoader`.\n\n|\n\nThis class can then be shared and used anywhere:\n\n.. code-block:: python\n\n    model = LitClassifier()\n    trainer = Trainer()\n\n    imagenet = ImagenetDataModule()\n    trainer.fit(model, datamodule=imagenet)\n\n    cifar10 = CIFAR10DataModule()\n    trainer.fit(model, datamodule=cifar10)\n\n---------------\n\n***************************\nWhy do I need a DataModule?\n***************************\nIn normal PyTorch code, the data cleaning/preparation is usually scattered across many files. This makes\nsharing and reusing the exact splits and transforms across projects impossible.\n\nDatamodules are for you if you ever asked the questions:\n\n- what splits did you use?\n- what transforms did you use?\n- what normalization did you use?\n- how did you prepare/tokenize the data?\n\n--------------\n\n*********************\nWhat is a DataModule?\n*********************\n\nThe :class:`~lightning.pytorch.core.datamodule.LightningDataModule`  is a convenient way to manage data in PyTorch Lightning.\nIt encapsulates training, validation, testing, and prediction dataloaders, as well as any necessary steps for data processing,\ndownloads, and transformations. By using a :class:`~lightning.pytorch.core.datamodule.LightningDataModule`, you can\neasily develop dataset-agnostic models, hot-swap different datasets, and share data splits and transformations across projects.\n\nHere's a simple PyTorch example:\n\n.. code-block:: python\n\n    # regular PyTorch\n    test_data = MNIST(my_path, train=False, download=True)\n    predict_data = MNIST(my_path, train=False, download=True)\n    train_data = MNIST(my_path, train=True, download=True)\n    train_data, val_data = random_split(train_data, [55000, 5000])\n\n    train_loader = DataLoader(train_data, batch_size=32)\n    val_loader = DataLoader(val_data, batch_size=32)\n    test_loader = DataLoader(test_data, batch_size=32)\n    predict_loader = DataLoader(predict_data, batch_size=32)\n\nThe equivalent DataModule just organizes the same exact code, but makes it reusable across projects.\n\n.. code-block:: python\n\n    class MNISTDataModule(L.LightningDataModule):\n        def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n            super().__init__()\n            self.data_dir = data_dir\n            self.batch_size = batch_size\n\n        def setup(self, stage: str):\n            self.mnist_test = MNIST(self.data_dir, train=False)\n            self.mnist_predict = MNIST(self.data_dir, train=False)\n            mnist_full = MNIST(self.data_dir, train=True)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n            )\n\n        def train_dataloader(self):\n            return DataLoader(self.mnist_train, batch_size=self.batch_size)\n\n        def val_dataloader(self):\n            return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n        def test_dataloader(self):\n            return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\n        def predict_dataloader(self):\n            return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n\n        def teardown(self, stage: str):\n            # Used to clean-up when the run is finished\n            ...\n\nBut now, as the complexity of your processing grows (transforms, multiple-GPU training), you can\nlet Lightning handle those details for you while making this dataset reusable so you can share with\ncolleagues or use in different projects.\n\n.. code-block:: python\n\n    mnist = MNISTDataModule(my_path)\n    model = LitClassifier()\n\n    trainer = Trainer()\n    trainer.fit(model, mnist)\n\nHere's a more realistic, complex DataModule that shows how much more reusable the datamodule is.\n\n.. code-block:: python\n\n    import lightning as L\n    from torch.utils.data import random_split, DataLoader\n\n    # Note - you must have torchvision installed for this example\n    from torchvision.datasets import MNIST\n    from torchvision import transforms\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def __init__(self, data_dir: str = \"./\"):\n            super().__init__()\n            self.data_dir = data_dir\n            self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n        def prepare_data(self):\n            # download\n            MNIST(self.data_dir, train=True, download=True)\n            MNIST(self.data_dir, train=False, download=True)\n\n        def setup(self, stage: str):\n            # Assign train/val datasets for use in dataloaders\n            if stage == \"fit\":\n                mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n                self.mnist_train, self.mnist_val = random_split(\n                    mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n                )\n\n            # Assign test dataset for use in dataloader(s)\n            if stage == \"test\":\n                self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n            if stage == \"predict\":\n                self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n\n        def train_dataloader(self):\n            return DataLoader(self.mnist_train, batch_size=32)\n\n        def val_dataloader(self):\n            return DataLoader(self.mnist_val, batch_size=32)\n\n        def test_dataloader(self):\n            return DataLoader(self.mnist_test, batch_size=32)\n\n        def predict_dataloader(self):\n            return DataLoader(self.mnist_predict, batch_size=32)\n\n\n---------------\n\n\n***********************\nLightningDataModule API\n***********************\nTo define a DataModule the following methods are used to create train/val/test/predict dataloaders:\n\n- :ref:`prepare_data<data/datamodule:prepare_data>` (how to download, tokenize, etc...)\n- :ref:`setup<data/datamodule:setup>` (how to split, define dataset, etc...)\n- :ref:`train_dataloader<data/datamodule:train_dataloader>`\n- :ref:`val_dataloader<data/datamodule:val_dataloader>`\n- :ref:`test_dataloader<data/datamodule:test_dataloader>`\n- :ref:`predict_dataloader<data/datamodule:predict_dataloader>`\n\n\nprepare_data\n============\nDownloading and saving data with multiple processes (distributed settings) will result in corrupted data. Lightning\nensures the :meth:`~lightning.pytorch.core.hooks.DataHooks.prepare_data` is called only within a single process on CPU,\nso you can safely add your downloading logic within. In case of multi-node training, the execution of this hook\ndepends upon :ref:`prepare_data_per_node<data/datamodule:prepare_data_per_node>`. :meth:`~lightning.pytorch.core.hooks.DataHooks.setup` is called after\n``prepare_data`` and there is a barrier in between which ensures that all the processes proceed to ``setup`` once the data is prepared and available for use.\n\n- download, i.e. download data only once on the disk from a single process\n- tokenize. Since it's a one time process, it is not recommended to do it on all processes\n- etc...\n\n.. code-block:: python\n\n    class MNISTDataModule(L.LightningDataModule):\n        def prepare_data(self):\n            # download\n            MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n            MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n\n\n.. warning::\n\n    ``prepare_data`` is called from the main process. It is not recommended to assign state here (e.g. ``self.x = y``) since it is called on a single process and if you assign\n    states here then they won't be available for other processes.\n\n\nsetup\n=====\nThere are also data operations you might want to perform on every GPU. Use :meth:`~lightning.pytorch.core.hooks.DataHooks.setup` to do things like:\n\n- count number of classes\n- build vocabulary\n- perform train/val/test splits\n- create datasets\n- apply transforms (defined explicitly in your datamodule)\n- etc...\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def setup(self, stage: str):\n            # Assign Train/val split(s) for use in Dataloaders\n            if stage == \"fit\":\n                mnist_full = MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n                self.mnist_train, self.mnist_val = random_split(\n                    mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n                )\n\n            # Assign Test split(s) for use in Dataloaders\n            if stage == \"test\":\n                self.mnist_test = MNIST(self.data_dir, train=False, download=True, transform=self.transform)\n\n\nFor eg., if you are working with NLP task where you need to tokenize the text and use it, then you can do something like as follows:\n\n.. code-block:: python\n\n    class LitDataModule(L.LightningDataModule):\n        def prepare_data(self):\n            dataset = load_Dataset(...)\n            train_dataset = ...\n            val_dataset = ...\n            # tokenize\n            # save it to disk\n\n        def setup(self, stage):\n            # load it back here\n            dataset = load_dataset_from_disk(...)\n\n\nThis method expects a ``stage`` argument.\nIt is used to separate setup logic for ``trainer.{fit,validate,test,predict}``.\n\n.. note:: :ref:`setup<data/datamodule:setup>` is called from every process across all the nodes. Setting state here is recommended.\n.. note:: :ref:`teardown<data/datamodule:teardown>` can be used to clean up the state. It is also called from every process across all the nodes.\n\n\ntrain_dataloader\n================\nUse the :meth:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` method to generate the training dataloader(s).\nUsually you just wrap the dataset you defined in :ref:`setup<data/datamodule:setup>`. This is the dataloader that the Trainer\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.fit` method uses.\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def train_dataloader(self):\n            return DataLoader(self.mnist_train, batch_size=64)\n\n.. _datamodule_val_dataloader_label:\n\nval_dataloader\n==============\nUse the :meth:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` method to generate the validation dataloader(s).\nUsually you just wrap the dataset you defined in :ref:`setup<data/datamodule:setup>`. This is the dataloader that the Trainer\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.fit` and :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate` methods uses.\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def val_dataloader(self):\n            return DataLoader(self.mnist_val, batch_size=64)\n\n\n.. _datamodule_test_dataloader_label:\n\ntest_dataloader\n===============\nUse the :meth:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` method to generate the test dataloader(s).\nUsually you just wrap the dataset you defined in :ref:`setup<data/datamodule:setup>`. This is the dataloader that the Trainer\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.test` method uses.\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def test_dataloader(self):\n            return DataLoader(self.mnist_test, batch_size=64)\n\n\npredict_dataloader\n==================\nUse the :meth:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` method to generate the prediction dataloader(s).\nUsually you just wrap the dataset you defined in :ref:`setup<data/datamodule:setup>`. This is the dataloader that the Trainer\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.predict` method uses.\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class MNISTDataModule(L.LightningDataModule):\n        def predict_dataloader(self):\n            return DataLoader(self.mnist_predict, batch_size=64)\n\n\ntransfer_batch_to_device\n========================\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.transfer_batch_to_device\n    :noindex:\n\non_before_batch_transfer\n========================\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.on_before_batch_transfer\n    :noindex:\n\non_after_batch_transfer\n=======================\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.on_after_batch_transfer\n    :noindex:\n\nload_state_dict\n===============\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.load_state_dict\n    :noindex:\n\nstate_dict\n==========\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.state_dict\n    :noindex:\n\nteardown\n========\n\n.. automethod:: lightning.pytorch.core.datamodule.LightningDataModule.teardown\n    :noindex:\n\nprepare_data_per_node\n=====================\nIf set to ``True`` will call ``prepare_data()`` on LOCAL_RANK=0 for every node.\nIf set to ``False`` will only call from NODE_RANK=0, LOCAL_RANK=0.\n\n.. testcode::\n\n    class LitDataModule(LightningDataModule):\n        def __init__(self):\n            super().__init__()\n            self.prepare_data_per_node = True\n\n\n------------------\n\n******************\nUsing a DataModule\n******************\n\nThe recommended way to use a DataModule is simply:\n\n.. code-block:: python\n\n    dm = MNISTDataModule()\n    model = Model()\n    trainer.fit(model, datamodule=dm)\n    trainer.test(datamodule=dm)\n    trainer.validate(datamodule=dm)\n    trainer.predict(datamodule=dm)\n\nIf you need information from the dataset to build your model, then run\n:ref:`prepare_data<data/datamodule:prepare_data>` and\n:ref:`setup<data/datamodule:setup>` manually (Lightning ensures\nthe method runs on the correct devices).\n\n.. code-block:: python\n\n    dm = MNISTDataModule()\n    dm.prepare_data()\n    dm.setup(stage=\"fit\")\n\n    model = Model(num_classes=dm.num_classes, width=dm.width, vocab=dm.vocab)\n    trainer.fit(model, dm)\n\n    dm.setup(stage=\"test\")\n    trainer.test(datamodule=dm)\n\nYou can access the current used datamodule of a trainer via ``trainer.datamodule`` and the current used\ndataloaders via the trainer properties :meth:`~lightning.pytorch.trainer.trainer.Trainer.train_dataloader`,\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.val_dataloaders`,\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.test_dataloaders`, and\n:meth:`~lightning.pytorch.trainer.trainer.Trainer.predict_dataloaders`.\n\n\n----------------\n\n*****************************\nDataModules without Lightning\n*****************************\nYou can of course use DataModules in plain PyTorch code as well.\n\n.. code-block:: python\n\n    # download, etc...\n    dm = MNISTDataModule()\n    dm.prepare_data()\n\n    # splits/transforms\n    dm.setup(stage=\"fit\")\n\n    # use data\n    for batch in dm.train_dataloader():\n        ...\n\n    for batch in dm.val_dataloader():\n        ...\n\n    dm.teardown(stage=\"fit\")\n\n    # lazy load test data\n    dm.setup(stage=\"test\")\n    for batch in dm.test_dataloader():\n        ...\n\n    dm.teardown(stage=\"test\")\n\nBut overall, DataModules encourage reproducibility by allowing all details of a dataset to be specified in a unified\nstructure.\n\n----------------\n\n******************************\nHyperparameters in DataModules\n******************************\nLike LightningModules, DataModules support hyperparameters with the same API.\n\n.. code-block:: python\n\n    import lightning as L\n\n\n    class CustomDataModule(L.LightningDataModule):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.save_hyperparameters()\n\n        def configure_optimizers(self):\n            # access the saved hyperparameters\n            opt = optim.Adam(self.parameters(), lr=self.hparams.lr)\n\nRefer to ``save_hyperparameters`` in :doc:`lightning module <../common/lightning_module>` for more details.\n\n\n----\n\n.. include:: ../extensions/datamodules_state.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/data/iterables.html", "url_rel_html": "data/iterables.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/data/iterables.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Arbitrary iterable support¶", "rst_text": ""}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/debug/debugging.html", "url_rel_html": "debug/debugging.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/debug/debugging.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Debug your model¶", "rst_text": ".. _debugging:\n\n################\nDebug your model\n################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of model debugging.\n   :col_css: col-md-4\n   :button_link: debugging_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn to debug machine learning operations\n   :col_css: col-md-4\n   :button_link: debugging_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Learn to debug distributed models\n   :col_css: col-md-4\n   :button_link: debugging_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n   </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/debug/debugging_advanced.html", "url_rel_html": "debug/debugging_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/debug/debugging_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Debug your model (advanced)¶", "rst_text": ":orphan:\n\n.. _debugging_advanced:\n\n###########################\nDebug your model (advanced)\n###########################\n**Audience**: Users who want to debug distributed models.\n\n----\n\n************************\nDebug distributed models\n************************\nTo debug a distributed model, we recommend you debug it locally by running the distributed version on CPUs:\n\n.. code-block:: python\n\n    trainer = Trainer(accelerator=\"cpu\", strategy=\"ddp\", devices=2)\n\nOn the CPU, you can use `pdb <https://docs.python.org/3/library/pdb.html>`_ or `breakpoint() <https://docs.python.org/3/library/functions.html#breakpoint>`_\nor use regular print statements.\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def training_step(self, batch, batch_idx):\n            debugging_message = ...\n            print(f\"RANK - {self.trainer.global_rank}: {debugging_message}\")\n\n            if self.trainer.global_rank == 0:\n                import pdb\n\n                pdb.set_trace()\n\n            # to prevent other processes from moving forward until all processes are in sync\n            self.trainer.strategy.barrier()\n\nWhen everything works, switch back to GPU by changing only the accelerator.\n\n.. code-block:: python\n\n    trainer = Trainer(accelerator=\"gpu\", strategy=\"ddp\", devices=2)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/debug/debugging_basic.html", "url_rel_html": "debug/debugging_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/debug/debugging_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Debug your model (basic)¶", "rst_text": ":orphan:\n\n.. _debugging_basic:\n\n########################\nDebug your model (basic)\n########################\n\n**Audience**: Users who want to learn the basics of debugging models.\n\n.. video:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/yt/Trainer+flags+7-+debugging_1.mp4\n    :poster: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/yt_thumbs/thumb_debugging.png\n    :width: 400\n    :muted:\n\n----\n\n**********************************\nHow does Lightning help me debug ?\n**********************************\nThe Lightning Trainer has *a lot* of arguments devoted to maximizing your debugging productivity.\n\n----\n\n****************\nSet a breakpoint\n****************\nA breakpoint stops your code execution so you can inspect variables, etc... and allow your code to execute one line at a time.\n\n.. code:: python\n\n    def function_to_debug():\n        x = 2\n\n        # set breakpoint\n        breakpoint()\n        y = x**2\n\nIn this example, the code will stop before executing the ``y = x**2`` line.\n\n----\n\n************************************\nRun all your model code once quickly\n************************************\nIf you've ever trained a model for days only to crash during validation or testing then this trainer argument is about to become your best friend.\n\nThe :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run` argument in the trainer runs 5 batch of training, validation, test and prediction data through your trainer to see if there are any bugs:\n\n.. code:: python\n\n    trainer = Trainer(fast_dev_run=True)\n\nTo change how many batches to use, change the argument to an integer. Here we run 7 batches of each:\n\n.. code:: python\n\n    trainer = Trainer(fast_dev_run=7)\n\n\n.. note::\n\n    This argument will disable tuner, checkpoint callbacks, early stopping callbacks,\n    loggers and logger callbacks like :class:`~lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor` and\n    :class:`~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor`.\n\n----\n\n************************\nShorten the epoch length\n************************\nSometimes it's helpful to only use a fraction of your training, val, test, or predict data (or a set number of batches).\nFor example, you can use 20% of the training set and 1% of the validation set.\n\nOn larger datasets like Imagenet, this can help you debug or test a few things faster than waiting for a full epoch.\n\n.. testcode::\n\n    # use only 10% of training data and 1% of val data\n    trainer = Trainer(limit_train_batches=0.1, limit_val_batches=0.01)\n\n    # use 10 batches of train and 5 batches of val\n    trainer = Trainer(limit_train_batches=10, limit_val_batches=5)\n\n----\n\n******************\nRun a Sanity Check\n******************\nLightning runs **2** steps of validation in the beginning of training.\nThis avoids crashing in the validation loop sometime deep into a lengthy training loop.\n\n(See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps`\nargument of :class:`~lightning.pytorch.trainer.trainer.Trainer`)\n\n.. testcode::\n\n    trainer = Trainer(num_sanity_val_steps=2)\n\n----\n\n*************************************\nPrint LightningModule weights summary\n*************************************\nWhenever the ``.fit()`` function gets called, the Trainer will print the weights summary for the LightningModule.\n\n.. code:: python\n\n    trainer.fit(...)\n\nthis generate a table like:\n\n.. code-block:: text\n\n      | Name  | Type        | Params | Mode\n    -------------------------------------------\n    0 | net   | Sequential  | 132 K  | train\n    1 | net.0 | Linear      | 131 K  | train\n    2 | net.1 | BatchNorm1d | 1.0 K  | train\n\nTo add the child modules to the summary add a :class:`~lightning.pytorch.callbacks.model_summary.ModelSummary`:\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import ModelSummary\n\n    trainer = Trainer(callbacks=[ModelSummary(max_depth=-1)])\n\nTo print the model summary if ``.fit()`` is not called:\n\n.. code-block:: python\n\n    from lightning.pytorch.utilities.model_summary import ModelSummary\n\n    model = LitModel()\n    summary = ModelSummary(model, max_depth=-1)\n    print(summary)\n\nTo turn off the autosummary use:\n\n.. code:: python\n\n    trainer = Trainer(enable_model_summary=False)\n\n----\n\n***********************************\nPrint input output layer dimensions\n***********************************\nAnother debugging tool is to  display the intermediate input- and output sizes of all your layers by setting the\n``example_input_array`` attribute in your LightningModule.\n\n.. code-block:: python\n\n    class LitModel(LightningModule):\n        def __init__(self, *args, **kwargs):\n            self.example_input_array = torch.Tensor(32, 1, 28, 28)\n\nWith the input array, the summary table will include the input and output layer dimensions:\n\n.. code-block:: text\n\n      | Name  | Type        | Params | Mode  | In sizes  | Out sizes\n    ----------------------------------------------------------------------\n    0 | net   | Sequential  | 132 K  | train | [10, 256] | [10, 512]\n    1 | net.0 | Linear      | 131 K  | train | [10, 256] | [10, 512]\n    2 | net.1 | BatchNorm1d | 1.0 K  | train | [10, 512] | [10, 512]\n\nwhen you call ``.fit()`` on the Trainer. This can help you find bugs in the composition of your layers.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/debug/debugging_intermediate.html", "url_rel_html": "debug/debugging_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/debug/debugging_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Debug your model (intermediate)¶", "rst_text": ":orphan:\n\n.. _debugging_intermediate:\n\n\n###############################\nDebug your model (intermediate)\n###############################\n**Audience**: Users who want to debug their ML code\n\n----\n\n***************************\nWhy should I debug ML code?\n***************************\nMachine learning code requires debugging mathematical correctness, which is not something non-ML code has to deal with. Lightning implements a few best-practice techniques to give all users, expert level ML debugging abilities.\n\n----\n\n**************************************\nOverfit your model on a Subset of Data\n**************************************\n\nA good debugging technique is to take a tiny portion of your data (say 2 samples per class),\nand try to get your model to overfit. If it can't, it's a sign it won't work with large datasets.\n\n(See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.overfit_batches`\nargument of :class:`~lightning.pytorch.trainer.trainer.Trainer`)\n\n.. testcode::\n\n    # use only 1% of training data\n    trainer = Trainer(overfit_batches=0.01)\n\n    # similar, but with a fixed 10 batches\n    trainer = Trainer(overfit_batches=10)\n\n    # equivalent to\n    trainer = Trainer(limit_train_batches=10, limit_val_batches=10)\n\nSetting ``overfit_batches`` is the same as setting ``limit_train_batches`` and ``limit_val_batches`` to the same value, but in addition will also turn off shuffling in the training dataloader.\n\n\n----\n\n********************************\nLook-out for exploding gradients\n********************************\nOne major problem that plagues models is exploding gradients.\nGradient clipping is one technique that can help keep gradients from exploding.\n\nYou can keep an eye on the gradient norm by logging it in your LightningModule:\n\n.. code-block:: python\n\n    from lightning.pytorch.utilities import grad_norm\n\n\n    def on_before_optimizer_step(self, optimizer):\n        # Compute the 2-norm for each layer\n        # If using mixed precision, the gradients are already unscaled here\n        norms = grad_norm(self.layer, norm_type=2)\n        self.log_dict(norms)\n\n\nThis will plot the 2-norm of each layer to your experiment manager.\nIf you notice the norm is going up, there's a good chance your gradients will explode.\n\nOne technique to stop exploding gradients is to clip the gradient when the norm is above a certain threshold:\n\n.. testcode::\n\n    # DEFAULT (ie: don't clip)\n    trainer = Trainer(gradient_clip_val=0)\n\n    # clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\n    trainer = Trainer(gradient_clip_val=0.5)\n\n    # clip gradients' maximum magnitude to <=0.5\n    trainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n\n----\n\n*************************\nDetect autograd anomalies\n*************************\nLightning helps you detect anomalies in the PyTorh autograd engine via PyTorch's built-in\n`Anomaly Detection Context-manager <https://pytorch.org/docs/stable/autograd.html#anomaly-detection>`_.\n\nEnable it via the **detect_anomaly** trainer argument:\n\n.. testcode::\n\n    trainer = Trainer(detect_anomaly=True)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/accelerator.html", "url_rel_html": "extensions/accelerator.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/accelerator.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Accelerator¶", "rst_text": ".. _accelerator:\n\n###########\nAccelerator\n###########\n\nThe Accelerator connects a Lightning Trainer to arbitrary hardware (CPUs, GPUs, TPUs, HPUs, MPS, ...).\nCurrently there are accelerators for:\n\n- CPU\n- :doc:`GPU <../accelerators/gpu>`\n- :doc:`TPU <../accelerators/tpu>`\n- :doc:`HPU <../integrations/hpu/index>`\n- :doc:`MPS <../accelerators/mps>`\n\nThe Accelerator is part of the Strategy which manages communication across multiple devices (distributed communication).\nWhenever the Trainer, the loops or any other component in Lightning needs to talk to hardware, it calls into the Strategy and the Strategy calls into the Accelerator.\n\n.. image:: https://pl-public-data.s3.amazonaws.com/docs/static/images/strategies/overview.jpeg\n    :alt: Illustration of the Strategy as a composition of the Accelerator and several plugins\n\nWe expose Accelerators and Strategies mainly for expert users who want to extend Lightning to work with new\nhardware and distributed training or clusters.\n\n\n----------\n\nCreate a Custom Accelerator\n---------------------------\n\n.. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\n\nHere is how you create a new Accelerator.\nLet's pretend we want to integrate the fictional XPU accelerator and we have access to its hardware through a library\n``xpulib``.\n\n.. code-block:: python\n\n    import xpulib\n\n\n    class XPUAccelerator(Accelerator):\n        \"\"\"Support for a hypothetical XPU, optimized for large-scale machine learning.\"\"\"\n\n        @staticmethod\n        def parse_devices(devices: Any) -> Any:\n            # Put parsing logic here how devices can be passed into the Trainer\n            # via the `devices` argument\n            return devices\n\n        @staticmethod\n        def get_parallel_devices(devices: Any) -> Any:\n            # Here, convert the device indices to actual device objects\n            return [torch.device(\"xpu\", idx) for idx in devices]\n\n        @staticmethod\n        def auto_device_count() -> int:\n            # Return a value for auto-device selection when `Trainer(devices=\"auto\")`\n            return xpulib.available_devices()\n\n        @staticmethod\n        def is_available() -> bool:\n            return xpulib.is_available()\n\n        def get_device_stats(self, device: Union[str, torch.device]) -> Dict[str, Any]:\n            # Return optional device statistics for loggers\n            return {}\n\n\nFinally, add the XPUAccelerator to the Trainer:\n\n.. code-block:: python\n\n    from lightning.pytorch import Trainer\n\n    accelerator = XPUAccelerator()\n    trainer = Trainer(accelerator=accelerator, devices=2)\n\n\n:doc:`Learn more about Strategies <../extensions/strategy>` and how they interact with the Accelerator.\n\n\n----------\n\nRegistering Accelerators\n------------------------\n\nIf you wish to switch to a custom accelerator from the CLI without code changes, you can implement the :meth:`~lightning.pytorch.accelerators.accelerator.Accelerator.register_accelerators` class method to register your new accelerator under a shorthand name like so:\n\n.. code-block:: python\n\n    class XPUAccelerator(Accelerator):\n        ...\n\n        @classmethod\n        def register_accelerators(cls, accelerator_registry):\n            accelerator_registry.register(\n                \"xpu\",\n                cls,\n                description=f\"XPU Accelerator - optimized for large-scale machine learning.\",\n            )\n\nNow, this is possible:\n\n.. code-block:: python\n\n    trainer = Trainer(accelerator=\"xpu\")\n\nOr if you are using the Lightning CLI, for example:\n\n.. code-block:: bash\n\n    python train.py fit --trainer.accelerator=xpu --trainer.devices=2\n\n\n----------\n\nAccelerator API\n---------------\n\n.. currentmodule:: lightning.pytorch.accelerators\n\n.. autosummary::\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Accelerator\n    CPUAccelerator\n    CUDAAccelerator\n    MPSAccelerator\n    XLAAccelerator\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html", "url_rel_html": "extensions/callbacks.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/callbacks.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Callback¶", "rst_text": ".. role:: hidden\n    :class: hidden-section\n\n.. _callbacks:\n\n########\nCallback\n########\n\nCallbacks allow you to add arbitrary self-contained programs to your training.\nAt specific points during the flow of execution (hooks), the Callback interface allows you to design programs that encapsulate a full set of functionality.\nIt de-couples functionality that does not need to be in the :doc:`lightning module <../common/lightning_module>` and can be shared across projects.\n\nLightning has a callback system to execute them when needed. Callbacks should capture NON-ESSENTIAL\nlogic that is NOT required for your :doc:`lightning module <../common/lightning_module>` to run.\n\nA complete list of Callback hooks can be found in :class:`~lightning.pytorch.callbacks.callback.Callback`.\n\nAn overall Lightning system should have:\n\n1. Trainer for all engineering\n2. LightningModule for all research code.\n3. Callbacks for non-essential code.\n\n|\n\nExample:\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import Callback\n\n\n    class MyPrintingCallback(Callback):\n        def on_train_start(self, trainer, pl_module):\n            print(\"Training is starting\")\n\n        def on_train_end(self, trainer, pl_module):\n            print(\"Training is ending\")\n\n\n    trainer = Trainer(callbacks=[MyPrintingCallback()])\n\nWe successfully extended functionality without polluting our super clean\n:doc:`lightning module <../common/lightning_module>` research code.\n\nYou can do pretty much anything with callbacks.\n\n--------------\n\n******************\nBuilt-in Callbacks\n******************\nLightning has a few built-in callbacks.\n\n.. note::\n    For a richer collection of callbacks, check out our\n    `bolts library <https://lightning-bolts.readthedocs.io/en/stable/index.html>`_.\n\n.. currentmodule:: lightning.pytorch.callbacks\n\n.. autosummary::\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BackboneFinetuning\n    BaseFinetuning\n    BasePredictionWriter\n    BatchSizeFinder\n    Callback\n    DeviceStatsMonitor\n    EarlyStopping\n    GradientAccumulationScheduler\n    LambdaCallback\n    LearningRateFinder\n    LearningRateMonitor\n    ModelCheckpoint\n    ModelPruning\n    ModelSummary\n    ProgressBar\n    RichModelSummary\n    RichProgressBar\n    StochasticWeightAveraging\n    Timer\n    TQDMProgressBar\n    WeightAveraging\n\n----------\n\n.. include:: callbacks_state.rst\n\n----------\n\n\n**************\nBest Practices\n**************\nThe following are best practices when using/designing callbacks.\n\n1. Callbacks should be isolated in their functionality.\n2. Your callback should not rely on the behavior of other callbacks in order to work properly.\n3. Do not manually call methods from the callback.\n4. Directly calling methods (eg. `on_validation_end`) is strongly discouraged.\n5. Whenever possible, your callbacks should not depend on the order in which they are executed.\n\n\n-----------\n\n.. include:: entry_points.rst\n\n-----------\n\n.. _callback_hooks:\n\n************\nCallback API\n************\nHere is the full API of methods available in the Callback base class.\n\nThe :class:`~lightning.pytorch.callbacks.Callback` class is the base for all the callbacks in Lightning just like the :class:`~lightning.pytorch.core.LightningModule` is the base for all models.\nIt defines a public interface that each callback implementation must follow, the key ones are:\n\nProperties\n==========\n\nstate_key\n^^^^^^^^^\n\n.. autoattribute:: lightning.pytorch.callbacks.Callback.state_key\n    :noindex:\n\n\nHooks\n=====\n\nsetup\n^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.setup\n    :noindex:\n\nteardown\n^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.teardown\n    :noindex:\n\non_fit_start\n^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_fit_start\n    :noindex:\n\non_fit_end\n^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_fit_end\n    :noindex:\n\non_sanity_check_start\n^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_sanity_check_start\n    :noindex:\n\non_sanity_check_end\n^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_sanity_check_end\n    :noindex:\n\non_train_batch_start\n^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_batch_start\n    :noindex:\n\non_train_batch_end\n^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_batch_end\n    :noindex:\n\non_train_epoch_start\n^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_epoch_start\n    :noindex:\n\non_train_epoch_end\n^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_epoch_end\n    :noindex:\n\non_validation_epoch_start\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_epoch_start\n    :noindex:\n\non_validation_epoch_end\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_epoch_end\n    :noindex:\n\non_test_epoch_start\n^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_epoch_start\n    :noindex:\n\non_test_epoch_end\n^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_epoch_end\n    :noindex:\n\non_predict_epoch_start\n^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_epoch_start\n    :noindex:\n\non_predict_epoch_end\n^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_epoch_end\n    :noindex:\n\non_validation_batch_start\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_batch_start\n    :noindex:\n\non_validation_batch_end\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_batch_end\n    :noindex:\n\non_test_batch_start\n^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_batch_start\n    :noindex:\n\non_test_batch_end\n^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_batch_end\n    :noindex:\n\non_predict_batch_start\n^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_batch_start\n    :noindex:\n\non_predict_batch_end\n^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_batch_end\n    :noindex:\n\non_train_start\n^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_start\n    :noindex:\n\non_train_end\n^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_train_end\n    :noindex:\n\non_validation_start\n^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_start\n    :noindex:\n\non_validation_end\n^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_validation_end\n    :noindex:\n\non_test_start\n^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_start\n    :noindex:\n\non_test_end\n^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_test_end\n    :noindex:\n\non_predict_start\n^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_start\n    :noindex:\n\non_predict_end\n^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_predict_end\n    :noindex:\n\n\non_exception\n^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_exception\n    :noindex:\n\nstate_dict\n^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.state_dict\n    :noindex:\n\non_save_checkpoint\n^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_save_checkpoint\n    :noindex:\n\nload_state_dict\n^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.load_state_dict\n    :noindex:\n\non_load_checkpoint\n^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_load_checkpoint\n    :noindex:\n\non_before_backward\n^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_before_backward\n    :noindex:\n\non_after_backward\n^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_after_backward\n    :noindex:\n\non_before_optimizer_step\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_before_optimizer_step\n    :noindex:\n\non_before_zero_grad\n^^^^^^^^^^^^^^^^^^^\n\n.. automethod:: lightning.pytorch.callbacks.Callback.on_before_zero_grad\n    :noindex:\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/callbacks_state.html", "url_rel_html": "extensions/callbacks_state.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/callbacks_state.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Save Callback state¶", "rst_text": "*******************\nSave Callback state\n*******************\n\nSome callbacks require internal state in order to function properly. You can optionally\nchoose to persist your callback's state as part of model checkpoint files using\n:meth:`~lightning.pytorch.callbacks.Callback.state_dict` and :meth:`~lightning.pytorch.callbacks.Callback.load_state_dict`.\nNote that the returned state must be able to be pickled.\n\nWhen your callback is meant to be used only as a singleton callback then implementing the above two hooks is enough\nto persist state effectively. However, if passing multiple instances of the callback to the Trainer is supported, then\nthe callback must define a :attr:`~lightning.pytorch.callbacks.Callback.state_key` property in order for Lightning\nto be able to distinguish the different states when loading the callback state. This concept is best illustrated by\nthe following example.\n\n.. testcode::\n\n    class Counter(Callback):\n        def __init__(self, what=\"epochs\", verbose=True):\n            self.what = what\n            self.verbose = verbose\n            self.state = {\"epochs\": 0, \"batches\": 0}\n\n        @property\n        def state_key(self) -> str:\n            # note: we do not include `verbose` here on purpose\n            return f\"Counter[what={self.what}]\"\n\n        def on_train_epoch_end(self, *args, **kwargs):\n            if self.what == \"epochs\":\n                self.state[\"epochs\"] += 1\n\n        def on_train_batch_end(self, *args, **kwargs):\n            if self.what == \"batches\":\n                self.state[\"batches\"] += 1\n\n        def load_state_dict(self, state_dict):\n            self.state.update(state_dict)\n\n        def state_dict(self):\n            return self.state.copy()\n\n\n    # two callbacks of the same type are being used\n    trainer = Trainer(callbacks=[Counter(what=\"epochs\"), Counter(what=\"batches\")])\n\nA Lightning checkpoint from this Trainer with the two stateful callbacks will include the following information:\n\n.. code-block::\n\n    {\n        \"state_dict\": ...,\n        \"callbacks\": {\n            \"Counter{'what': 'batches'}\": {\"batches\": 32, \"epochs\": 0},\n            \"Counter{'what': 'epochs'}\": {\"batches\": 0, \"epochs\": 2},\n            ...\n        }\n    }\n\nThe implementation of a :attr:`~lightning.pytorch.callbacks.Callback.state_key` is essential here. If it were missing,\nLightning would not be able to disambiguate the state for these two callbacks, and :attr:`~lightning.pytorch.callbacks.Callback.state_key`\nby default only defines the class name as the key, e.g., here ``Counter``.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/datamodules_state.html", "url_rel_html": "extensions/datamodules_state.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/datamodules_state.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Save DataModule state¶", "rst_text": "Save DataModule state\n=====================\nWhen a checkpoint is created, it asks every DataModule for their state. If your DataModule defines the *state_dict* and *load_state_dict* methods, the checkpoint will automatically track and restore your DataModules.\n\n.. code:: python\n\n    import lightning as L\n\n\n    class LitDataModule(L.LightningDataModule):\n        def state_dict(self):\n            # track whatever you want here\n            state = {\"current_train_batch_index\": self.current_train_batch_index}\n            return state\n\n        def load_state_dict(self, state_dict):\n            # restore the state based on what you tracked in (def state_dict)\n            self.current_train_batch_index = state_dict[\"current_train_batch_index\"]\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/logging.html", "url_rel_html": "extensions/logging.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/logging.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Logging¶", "rst_text": ":orphan:\n\n.. testsetup:: *\n\n    from lightning.pytorch import loggers as pl_loggers\n\n.. role:: hidden\n    :class: hidden-section\n\n.. _logging:\n\n\n#######\nLogging\n#######\n\n*****************\nSupported Loggers\n*****************\n\nThe following are loggers we support:\n\n.. currentmodule:: lightning.pytorch.loggers\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    CometLogger\n    CSVLogger\n    MLFlowLogger\n    NeptuneLogger\n    TensorBoardLogger\n    WandbLogger\n\n\nThe above loggers will normally plot an additional chart (**global_step VS epoch**). Depending on the loggers you use, there might be some additional charts too.\n\nBy default, Lightning uses ``TensorBoard`` logger under the hood, and stores the logs to a directory (by default in ``lightning_logs/``).\n\n.. testcode::\n\n    from lightning.pytorch import Trainer\n\n    # Automatically logs to a directory (by default ``lightning_logs/``)\n    trainer = Trainer()\n\nTo see your logs:\n\n.. code-block:: bash\n\n    tensorboard --logdir=lightning_logs/\n\nTo visualize tensorboard in a jupyter notebook environment, run the following command in a jupyter cell:\n\n.. code-block:: bash\n\n    %reload_ext tensorboard\n    %tensorboard --logdir=lightning_logs/\n\nYou can also pass a custom Logger to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\n\n.. testcode::\n    :skipif: not _TENSORBOARD_AVAILABLE and not _TENSORBOARDX_AVAILABLE\n\n    from lightning.pytorch import loggers as pl_loggers\n\n    tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\n    trainer = Trainer(logger=tb_logger)\n\nChoose from any of the others such as MLflow, Comet, Neptune, WandB, etc.\n\n.. code-block:: python\n\n    comet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\n    trainer = Trainer(logger=comet_logger)\n\nTo use multiple loggers, simply pass in a ``list`` or ``tuple`` of loggers.\n\n.. code-block:: python\n\n    tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\n    comet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\n    trainer = Trainer(logger=[tb_logger, comet_logger])\n\n.. note::\n\n    By default, Lightning logs every 50 steps. Use Trainer flags to :ref:`logging_frequency`.\n\n.. note::\n\n    By default, all loggers log to ``os.getcwd()``. You can change the logging path using\n    ``Trainer(default_root_dir=\"/your/path/to/save/checkpoints\")`` without instantiating a logger.\n\n----------\n\n******************************\nLogging from a LightningModule\n******************************\n\nLightning offers automatic log functionalities for logging scalars, or manual logging for anything else.\n\nAutomatic Logging\n=================\n\nUse the :meth:`~lightning.pytorch.core.LightningModule.log` or :meth:`~lightning.pytorch.core.LightningModule.log_dict`\nmethods to log from anywhere in a :doc:`LightningModule <../common/lightning_module>` and :doc:`callbacks <../extensions/callbacks>`.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        self.log(\"my_metric\", x)\n\n\n    # or a dict to log all metrics at once with individual plots\n    def training_step(self, batch, batch_idx):\n        self.log_dict({\"acc\": acc, \"recall\": recall})\n\n.. note::\n    Everything explained below applies to both :meth:`~lightning.pytorch.core.LightningModule.log` or :meth:`~lightning.pytorch.core.LightningModule.log_dict` methods.\n\n.. note::\n\n    When using TorchMetrics with Lightning, we recommend referring to the `TorchMetrics Lightning integration documentation <https://lightning.ai/docs/torchmetrics/stable/pages/lightning.html>`_ for logging best practices, common pitfalls, and proper usage patterns.\n\nDepending on where the :meth:`~lightning.pytorch.core.LightningModule.log` method is called, Lightning auto-determines\nthe correct logging mode for you. Of course you can override the default behavior by manually setting the\n:meth:`~lightning.pytorch.core.LightningModule.log` parameters.\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\nThe :meth:`~lightning.pytorch.core.LightningModule.log` method has a few options:\n\n* ``on_step``: Logs the metric at the current step.\n* ``on_epoch``: Automatically accumulates and logs at the end of the epoch.\n* ``prog_bar``: Logs to the progress bar (Default: ``False``).\n* ``logger``: Logs to the logger like ``Tensorboard``, or any other custom logger passed to the :class:`~lightning.pytorch.trainer.trainer.Trainer` (Default: ``True``).\n* ``reduce_fx``: Reduction function over step values for end of epoch. Uses :func:`torch.mean` by default and is not applied when a :class:`torchmetrics.Metric` is logged.\n* ``enable_graph``: If True, will not auto detach the graph.\n* ``sync_dist``: If True, averages the metric across devices. Use with care as this may lead to a significant communication overhead.\n* ``sync_dist_group``: The DDP group to sync across.\n* ``add_dataloader_idx``: If True, appends the index of the current dataloader to the name (when using multiple dataloaders). If False, user needs to give unique names for each dataloader to not mix the values.\n* ``batch_size``: Current batch size used for accumulating logs logged with ``on_epoch=True``. This will be directly inferred from the loaded batch, but for some data structures you might need to explicitly provide it.\n* ``rank_zero_only``: Set this to ``True`` only if you call ``self.log`` explicitly only from rank 0. If ``True`` you won't be able to access or specify this metric in callbacks (e.g. early stopping).\n\n.. list-table:: Default behavior of logging in Callback or LightningModule\n   :widths: 50 25 25\n   :header-rows: 1\n\n   * - Hook\n     - on_step\n     - on_epoch\n   * - on_train_start, on_train_epoch_start, on_train_epoch_end\n     - False\n     - True\n   * - on_before_backward, on_after_backward, on_before_optimizer_step, on_before_zero_grad\n     - True\n     - False\n   * - on_train_batch_start, on_train_batch_end, training_step\n     - True\n     - False\n   * - on_validation_start, on_validation_epoch_start, on_validation_epoch_end\n     - False\n     - True\n   * - on_validation_batch_start, on_validation_batch_end, validation_step\n     - False\n     - True\n\n\n.. note::\n\n    While logging tensor metrics with ``on_epoch=True`` inside step-level hooks and using mean-reduction (default) to accumulate the metrics across the current epoch, Lightning tries to extract the\n    batch size from the current batch. If multiple possible batch sizes are found, a warning is logged and if it fails to extract the batch size from the current batch, which is possible if\n    the batch is a custom structure/collection, then an error is raised. To avoid this, you can specify the ``batch_size`` inside the ``self.log(... batch_size=batch_size)`` call.\n\n    .. code-block:: python\n\n        def training_step(self, batch, batch_idx):\n            # extracts the batch size from `batch`\n            self.log(\"train_loss\", loss, on_epoch=True)\n\n\n        def validation_step(self, batch, batch_idx):\n            # uses `batch_size=10`\n            self.log(\"val_loss\", loss, batch_size=10)\n\n.. note::\n\n    - The above config for ``validation`` applies for ``test`` hooks as well.\n\n    -   Setting ``on_epoch=True`` will cache all your logged values during the full training epoch and perform a\n        reduction in ``on_train_epoch_end``. We recommend using `TorchMetrics <https://torchmetrics.readthedocs.io/>`_, when working with custom reduction.\n\n    -   Setting both ``on_step=True`` and ``on_epoch=True`` will create two keys per metric you log with\n        suffix ``_step`` and ``_epoch`` respectively. You can refer to these keys e.g. in the `monitor`\n        argument of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` or in the graphs plotted to the logger of your choice.\n\n\nIf your work requires to log in an unsupported method, please open an issue with a clear description of why it is blocking you.\n\n\nManual Logging Non-Scalar Artifacts\n===================================\n\nIf you want to log anything that is not a scalar, like histograms, text, images, etc., you may need to use the logger object directly.\n\n.. code-block:: python\n\n    def training_step(self):\n        ...\n        # the logger you used (in this case tensorboard)\n        tensorboard = self.logger.experiment\n        tensorboard.add_image()\n        tensorboard.add_histogram(...)\n        tensorboard.add_figure(...)\n\n\n----------\n\n********************\nMake a Custom Logger\n********************\n\nYou can implement your own logger by writing a class that inherits from :class:`~lightning.pytorch.loggers.logger.Logger`.\nUse the :func:`~lightning.pytorch.loggers.logger.rank_zero_experiment` and :func:`~lightning.pytorch.utilities.rank_zero.rank_zero_only` decorators to make sure that only the first process in DDP training creates the experiment and logs the data respectively.\n\n.. testcode::\n\n    from lightning.pytorch.loggers.logger import Logger, rank_zero_experiment\n    from lightning.pytorch.utilities import rank_zero_only\n\n\n    class MyLogger(Logger):\n        @property\n        def name(self):\n            return \"MyLogger\"\n\n        @property\n        def version(self):\n            # Return the experiment version, int or str.\n            return \"0.1\"\n\n        @rank_zero_only\n        def log_hyperparams(self, params):\n            # params is an argparse.Namespace\n            # your code to record hyperparameters goes here\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics, step):\n            # metrics is a dictionary of metric names and values\n            # your code to record metrics goes here\n            pass\n\n        @rank_zero_only\n        def save(self):\n            # Optional. Any code necessary to save logger data goes here\n            pass\n\n        @rank_zero_only\n        def finalize(self, status):\n            # Optional. Any code that needs to be run after training\n            # finishes goes here\n            pass\n\nIf you write a logger that may be useful to others, please send\na pull request to add it to Lightning!\n\n----------\n\n.. _logging_frequency:\n\n\n*************************\nControl Logging Frequency\n*************************\n\nLogging frequency\n=================\n\nIt may slow down training to log on every single batch. By default, Lightning logs every 50 rows, or 50 training steps.\nTo change this behaviour, set the ``log_every_n_steps`` :class:`~lightning.pytorch.trainer.trainer.Trainer` flag.\n\n.. testcode::\n\n   k = 10\n   trainer = Trainer(log_every_n_steps=k)\n\n\nLog Writing Frequency\n=====================\n\nIndividual logger implementations determine their flushing frequency. For example, on the\n:class:`~lightning.pytorch.loggers.csv_logs.CSVLogger` you can set the flag ``flush_logs_every_n_steps``.\n\n----------\n\n************\nProgress Bar\n************\n\nYou can add any metric to the progress bar using :meth:`~lightning.pytorch.core.LightningModule.log`\nmethod, setting ``prog_bar=True``.\n\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        self.log(\"my_loss\", loss, prog_bar=True)\n\n\nYou could learn more about progress bars supported by Lightning :doc:`here <../common/progress_bar>`.\n\nModifying the Progress Bar\n==========================\n\nThe progress bar by default already includes the training loss and version number of the experiment\nif you are using a logger. These defaults can be customized by overriding the\n:meth:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar.get_metrics` hook in your logger.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks.progress import TQDMProgressBar\n\n\n    class CustomProgressBar(TQDMProgressBar):\n        def get_metrics(self, *args, **kwargs):\n            # don't show the version number\n            items = super().get_metrics(*args, **kwargs)\n            items.pop(\"v_num\", None)\n            return items\n\n\n----------\n\n\n*************************\nConfigure Console Logging\n*************************\n\nLightning logs useful information about the training process and user warnings to the console.\nYou can retrieve the Lightning console logger and change it to your liking. For example, adjust the logging level\nor redirect output for certain modules to log files:\n\n.. testcode::\n\n    import logging\n\n    # configure logging at the root level of Lightning\n    logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n    # configure logging on module level, redirect to file\n    logger = logging.getLogger(\"lightning.pytorch.core\")\n    logger.addHandler(logging.FileHandler(\"core.log\"))\n\nRead more about custom Python logging `here <https://docs.python.org/3/library/logging.html>`_.\n\n\n----------\n\n***********************\nLogging Hyperparameters\n***********************\n\nWhen training a model, it is useful to know what hyperparams went into that model.\nWhen Lightning creates a checkpoint, it stores a key ``\"hyper_parameters\"`` with the hyperparams.\n\n.. code-block:: python\n\n    lightning_checkpoint = torch.load(filepath, map_location=lambda storage, loc: storage)\n    hyperparams = lightning_checkpoint[\"hyper_parameters\"]\n\nSome loggers also allow logging the hyperparams used in the experiment. For instance,\nwhen using the ``TensorBoardLogger``, all hyperparams will show\nin the hparams tab at :meth:`torch.utils.tensorboard.writer.SummaryWriter.add_hparams`.\n\n.. note::\n    If you want to track a metric in the tensorboard hparams tab, log scalars to the key ``hp_metric``. If tracking multiple metrics, initialize ``TensorBoardLogger`` with ``default_hp_metric=False`` and call ``log_hyperparams`` only once with your metric keys and initial values. Subsequent updates can simply be logged to the metric keys. Refer to the examples below for setting up proper hyperparams metrics tracking within the :doc:`LightningModule <../common/lightning_module>`.\n\n    .. code-block:: python\n\n        # Using default_hp_metric\n        def validation_step(self, batch, batch_idx):\n            self.log(\"hp_metric\", some_scalar)\n\n\n        # Using custom or multiple metrics (default_hp_metric=False)\n        def on_train_start(self):\n            self.logger.log_hyperparams(self.hparams, {\"hp/metric_1\": 0, \"hp/metric_2\": 0})\n\n\n        def validation_step(self, batch, batch_idx):\n            self.log(\"hp/metric_1\", some_scalar_1)\n            self.log(\"hp/metric_2\", some_scalar_2)\n\n    In the example, using ``\"hp/\"`` as a prefix allows for the metrics to be grouped under \"hp\" in the tensorboard scalar tab where you can collapse them.\n\n-----------\n\n***************************\nManaging Remote Filesystems\n***************************\n\nLightning supports saving logs to a variety of filesystems, including local filesystems and several cloud storage providers.\n\nCheck out the :doc:`Remote Filesystems <../common/remote_fs>` doc for more info.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/plugins.html", "url_rel_html": "extensions/plugins.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/plugins.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Plugins¶", "rst_text": ".. _plugins:\n\n#######\nPlugins\n#######\n\n.. include:: ../links.rst\n\nPlugins allow custom integrations to the internals of the Trainer such as custom precision, checkpointing or\ncluster environment implementation.\n\nUnder the hood, the Lightning Trainer is using plugins in the training routine, added automatically\ndepending on the provided Trainer arguments.\n\nThere are three types of plugins in Lightning with different responsibilities:\n\n- Precision plugins\n- CheckpointIO plugins\n- Cluster environments\n\nYou can make the Trainer use one or multiple plugins by adding it to the ``plugins`` argument like so:\n\n.. code-block:: python\n\n    trainer = Trainer(plugins=[plugin1, plugin2, ...])\n\n\nBy default, the plugins get selected based on the rest of the Trainer settings such as the ``strategy``.\n\n\n-----------\n\n.. _precision-plugins:\n\n*****************\nPrecision Plugins\n*****************\n\nWe provide precision plugins for you to benefit from numerical representations with lower precision than\n32-bit floating-point or higher precision, such as 64-bit floating-point.\n\n.. code-block:: python\n\n    # Training with 16-bit precision\n    trainer = Trainer(precision=16)\n\nThe full list of built-in precision plugins is listed below.\n\n.. currentmodule:: lightning.pytorch.plugins.precision\n\n.. autosummary::\n    :nosignatures:\n    :template: classtemplate.rst\n\n    DeepSpeedPrecision\n    DoublePrecision\n    HalfPrecision\n    FSDPPrecision\n    MixedPrecision\n    Precision\n    XLAPrecision\n    TransformerEnginePrecision\n    BitsandbytesPrecision\n\nMore information regarding precision with Lightning can be found :ref:`here <precision>`\n\n-----------\n\n\n.. _checkpoint_io_plugins:\n\n********************\nCheckpointIO Plugins\n********************\n\nAs part of our commitment to extensibility, we have abstracted Lightning's checkpointing logic into the :class:`~lightning.pytorch.plugins.io.CheckpointIO` plugin.\nWith this, you have the ability to customize the checkpointing logic to match the needs of your infrastructure.\n\nBelow is a list of built-in plugins for checkpointing.\n\n.. currentmodule:: lightning.pytorch.plugins.io\n\n.. autosummary::\n    :nosignatures:\n    :template: classtemplate.rst\n\n    AsyncCheckpointIO\n    CheckpointIO\n    TorchCheckpointIO\n    XLACheckpointIO\n\nLearn more about custom checkpointing with Lightning :ref:`here <checkpointing_expert>`.\n\n-----------\n\n\n.. _cluster_environment_plugins:\n\n********************\nCluster Environments\n********************\n\nYou can define the interface of your own cluster environment based on the requirements of your infrastructure.\n\n.. currentmodule:: lightning.pytorch.plugins.environments\n\n.. autosummary::\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ClusterEnvironment\n    KubeflowEnvironment\n    LightningEnvironment\n    LSFEnvironment\n    SLURMEnvironment\n    TorchElasticEnvironment\n    XLAEnvironment\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/extensions/strategy.html", "url_rel_html": "extensions/strategy.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/extensions/strategy.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "What is a Strategy?¶", "rst_text": "###################\nWhat is a Strategy?\n###################\n\nStrategy controls the model distribution across training, evaluation, and prediction to be used by the :doc:`Trainer <../common/trainer>`. It can be controlled by passing different\nstrategy with aliases (``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"`` and so on) as well as a custom strategy to the ``strategy`` parameter for Trainer.\n\nThe Strategy in PyTorch Lightning handles the following responsibilities:\n\n* Launch and teardown of training processes (if applicable).\n* Setup communication between processes (NCCL, GLOO, MPI, and so on).\n* Provide a unified communication interface for reduction, broadcast, and so on.\n* Owns the :class:`~lightning.pytorch.core.LightningModule`\n* Handles/owns optimizers and schedulers.\n\n\nStrategy is a composition of one :doc:`Accelerator <../extensions/accelerator>`, one :ref:`Precision Plugin <extensions/plugins:Precision Plugins>`, a :ref:`CheckpointIO <extensions/plugins:CheckpointIO Plugins>`\nplugin and other optional plugins such as the :ref:`ClusterEnvironment <extensions/plugins:Cluster Environments>`.\n\n.. image:: https://pl-public-data.s3.amazonaws.com/docs/static/images/strategies/overview.jpeg\n    :alt: Illustration of the Strategy as a composition of the Accelerator and several plugins\n\nWe expose Strategies mainly for expert users that want to extend Lightning for new hardware support or new distributed backends (e.g. a backend not yet supported by `PyTorch <https://pytorch.org/docs/stable/distributed.html#backends>`_ itself).\n\n\n----\n\n*****************************\nSelecting a Built-in Strategy\n*****************************\n\nBuilt-in strategies can be selected in two ways.\n\n1. Pass the shorthand name to the ``strategy`` Trainer argument\n2. Import a Strategy from :mod:`lightning.pytorch.strategies`, instantiate it and pass it to the ``strategy`` Trainer argument\n\nThe latter allows you to configure further options on the specific strategy.\nHere are some examples:\n\n.. code-block:: python\n\n    # Training with the DistributedDataParallel strategy on 4 GPUs\n    trainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n    # Training with the DistributedDataParallel strategy on 4 GPUs, with options configured\n    trainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=4)\n\n    # Training with the DDP Spawn strategy using auto accelerator selection\n    trainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"auto\", devices=4)\n\n    # Training with the DeepSpeed strategy on available GPUs\n    trainer = Trainer(strategy=\"deepspeed\", accelerator=\"gpu\", devices=\"auto\")\n\n    # Training with the DDP strategy using 3 CPU processes\n    trainer = Trainer(strategy=\"ddp\", accelerator=\"cpu\", devices=3)\n\n    # Training with the DDP Spawn strategy on 8 TPU cores\n    trainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"tpu\", devices=8)\n\nThe below table lists all relevant strategies available in Lightning with their corresponding short-hand name:\n\n.. list-table:: Strategy Classes and Nicknames\n   :widths: 20 20 20\n   :header-rows: 1\n\n   * - Name\n     - Class\n     - Description\n   * - fsdp\n     - :class:`~lightning.pytorch.strategies.FSDPStrategy`\n     - Strategy for Fully Sharded Data Parallel training. :doc:`Learn more. <../advanced/model_parallel/fsdp>`\n   * - ddp\n     - :class:`~lightning.pytorch.strategies.DDPStrategy`\n     - Strategy for multi-process single-device training on one or multiple nodes. :ref:`Learn more. <accelerators/gpu_intermediate:Distributed Data Parallel>`\n   * - ddp_spawn\n     - :class:`~lightning.pytorch.strategies.DDPStrategy`\n     - Same as \"ddp\" but launches processes using ``torch.multiprocessing.spawn`` method and joins processes after training finishes. :ref:`Learn more. <accelerators/gpu_intermediate:Distributed Data Parallel Spawn>`\n   * - deepspeed\n     - :class:`~lightning.pytorch.strategies.DeepSpeedStrategy`\n     - Provides capabilities to run training using the DeepSpeed library, with training optimizations for large billion parameter models. :doc:`Learn more. <../advanced/model_parallel/deepspeed>`\n   * - hpu_parallel\n     - ``HPUParallelStrategy``\n     - Strategy for distributed training on multiple HPU devices. :doc:`Learn more. <../integrations/hpu/index>`\n   * - hpu_single\n     - ``SingleHPUStrategy``\n     - Strategy for training on a single HPU device. :doc:`Learn more. <../integrations/hpu/index>`\n   * - xla\n     - :class:`~lightning.pytorch.strategies.XLAStrategy`\n     - Strategy for training on multiple TPU devices using the :func:`torch_xla.distributed.xla_multiprocessing.spawn` method. :doc:`Learn more. <../accelerators/tpu>`\n   * - single_xla\n     - :class:`~lightning.pytorch.strategies.SingleXLAStrategy`\n     - Strategy for training on a single XLA device, like TPUs. :doc:`Learn more. <../accelerators/tpu>`\n\n----\n\n\n**********************\nThird-party Strategies\n**********************\n\nThere are powerful third-party strategies that integrate well with Lightning but aren't maintained as part of the ``lightning`` package.\nCheckout the gallery over :doc:`here <../integrations/strategies/index>`.\n\n----\n\n\n************************\nCreate a Custom Strategy\n************************\n\nEvery strategy in Lightning is a subclass of one of the main base classes: :class:`~lightning.pytorch.strategies.Strategy`, :class:`~lightning.pytorch.strategies.SingleDeviceStrategy` or :class:`~lightning.pytorch.strategies.ParallelStrategy`.\n\n.. image:: https://pl-public-data.s3.amazonaws.com/docs/static/images/strategies/hierarchy.jpeg\n    :alt: Strategy base classes\n\nAs an expert user, you may choose to extend either an existing built-in Strategy or create a completely new one by\nsubclassing the base classes.\n\n.. code-block:: python\n\n    from lightning.pytorch.strategies import DDPStrategy\n\n\n    class CustomDDPStrategy(DDPStrategy):\n        def configure_ddp(self):\n            self.model = MyCustomDistributedDataParallel(\n                self.model,\n                device_ids=...,\n            )\n\n        def setup(self, trainer):\n            # you can access the accelerator and plugins directly\n            self.accelerator.setup()\n            self.precision_plugin.connect(...)\n\n\nThe custom strategy can then be passed into the ``Trainer`` directly via the ``strategy`` parameter.\n\n.. code-block:: python\n\n    # custom strategy\n    trainer = Trainer(strategy=CustomDDPStrategy())\n\n\nSince the strategy also hosts the Accelerator and various plugins, you can customize all of them to work together as you like:\n\n.. code-block:: python\n\n    # custom strategy, with new accelerator and plugins\n    accelerator = MyAccelerator()\n    precision_plugin = MyPrecisionPlugin()\n    strategy = CustomDDPStrategy(accelerator=accelerator, precision_plugin=precision_plugin)\n    trainer = Trainer(strategy=strategy)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/build_model.html", "url_rel_html": "model/build_model.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/build_model.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Build a Model¶", "rst_text": ":orphan:\n\n#############\nBuild a Model\n#############\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: 1: Train a model\n   :description: Build a model to learn the basic ideas of Lightning\n   :col_css: col-md-4\n   :button_link: train_model_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: 2: Validate and test a model\n   :description: Add a validation and test data split to avoid overfitting.\n   :col_css: col-md-4\n   :button_link: ../common/evaluation_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: 3: Supercharge training\n   :description: Enable state-of-the-art training techniques with the Trainer features.\n   :col_css: col-md-4\n   :button_link: build_model_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: LightningModule API\n   :description: Dig into LightningModule API in depth\n   :col_css: col-md-4\n   :button_link: ../common/lightning_module.html#lightningmodule-api\n   :height: 150\n\n.. displayitem::\n   :header: Trainer API\n   :description: Dig into Trainer API in depth\n   :col_css: col-md-4\n   :button_link: ../common/trainer.html#trainer-class-api\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/build_model_advanced.html", "url_rel_html": "model/build_model_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/build_model_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Own your loop (advanced)¶", "rst_text": ":orphan:\n\n########################\nOwn your loop (advanced)\n########################\n\n***********************\nCustomize training loop\n***********************\n\n.. image:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/custom_loop.png\n    :width: 600\n    :alt: Injecting custom code in a training loop\n\nInject custom code anywhere in the Training loop using any of the 20+ methods (:ref:`lightning_hooks`) available in the LightningModule.\n\n.. testcode::\n\n    import lightning as L\n\n\n    class LitModel(L.LightningModule):\n        def backward(self, loss):\n            loss.backward()\n\n----\n\n.. include:: manual_optimization.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/build_model_intermediate.html", "url_rel_html": "model/build_model_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/build_model_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Supercharge training (intermediate)¶", "rst_text": ":orphan:\n\n###################################\nSupercharge training (intermediate)\n###################################\n\n************************\nEnable training features\n************************\nEnable advanced training features using Trainer arguments. These are SOTA techniques that are automatically integrated into your training loop without changes to your code.\n\n.. code::\n\n   # train 1T+ parameter models with DeepSpeed/FSDP\n   trainer = Trainer(\n       devices=4,\n       accelerator=\"gpu\",\n       strategy=\"deepspeed_stage_2\",\n       precision=\"16-mixed\",\n    )\n\n   # 20+ helpful arguments for rapid idea iteration\n   trainer = Trainer(\n       max_epochs=10,\n       min_epochs=5,\n       overfit_batches=1\n    )\n\n   # access the latest state of the art techniques\n   trainer = Trainer(callbacks=[WeightAveraging(...)])\n\n----\n\n******************\nExtend the Trainer\n******************\n\n.. video:: https://pl-public-data.s3.amazonaws.com/assets_lightning/cb.mp4\n    :width: 600\n    :autoplay:\n    :loop:\n    :muted:\n\nIf you have multiple lines of code with similar functionalities, you can use *callbacks* to easily group them together and toggle all of those lines on or off at the same time.\n\n.. code::\n\n   trainer = Trainer(callbacks=[AWSCheckpoints()])\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/manual_optimization.html", "url_rel_html": "model/manual_optimization.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/manual_optimization.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Manual Optimization¶", "rst_text": "*******************\nManual Optimization\n*******************\n\nFor advanced research topics like reinforcement learning, sparse coding, or GAN research, it may be desirable to\nmanually manage the optimization process, especially when dealing with multiple optimizers at the same time.\n\nIn this mode, Lightning will handle only accelerator, precision and strategy logic.\nThe users are left with ``optimizer.zero_grad()``, gradient accumulation, optimizer toggling, etc..\n\nTo manually optimize, do the following:\n\n* Set ``self.automatic_optimization=False`` in your ``LightningModule``'s ``__init__``.\n* Use the following functions and call them manually:\n\n  * ``self.optimizers()`` to access your optimizers (one or multiple)\n  * ``optimizer.zero_grad()`` to clear the gradients from the previous training step\n  * ``self.manual_backward(loss)`` instead of ``loss.backward()``\n  * ``optimizer.step()`` to update your model parameters\n  * ``self.toggle_optimizer()`` and ``self.untoggle_optimizer()``, or ``self.toggled_optimizer()`` if needed\n\nHere is a minimal example of manual optimization.\n\n.. testcode:: python\n\n    from lightning.pytorch import LightningModule\n\n\n    class MyModel(LightningModule):\n        def __init__(self):\n            super().__init__()\n            # Important: This property activates manual optimization.\n            self.automatic_optimization = False\n\n        def training_step(self, batch, batch_idx):\n            opt = self.optimizers()\n            opt.zero_grad()\n            loss = self.compute_loss(batch)\n            self.manual_backward(loss)\n            opt.step()\n\n.. tip::\n   Be careful where you call ``optimizer.zero_grad()``, or your model won't converge.\n   It is good practice to call ``optimizer.zero_grad()`` before ``self.manual_backward(loss)``.\n\n\nAccess your Own Optimizer\n=========================\n\nThe provided ``optimizer`` is a :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` object wrapping your own optimizer\nconfigured in your :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`. You can access your own optimizer\nwith ``optimizer.optimizer``. However, if you use your own optimizer to perform a step, Lightning won't be able to\nsupport accelerators, precision and profiling for you.\n\n.. testcode:: python\n\n   class Model(LightningModule):\n       def __init__(self):\n           super().__init__()\n           self.automatic_optimization = False\n           ...\n\n       def training_step(self, batch, batch_idx):\n           optimizer = self.optimizers()\n\n           # `optimizer` is a `LightningOptimizer` wrapping the optimizer.\n           # To access it, do the following.\n           # However, it won't work on TPU, AMP, etc...\n           optimizer = optimizer.optimizer\n           ...\n\nGradient Accumulation\n=====================\n\nYou can accumulate gradients over batches similarly to ``accumulate_grad_batches`` argument in\n:ref:`Trainer <trainer>` for automatic optimization. To perform gradient accumulation with one optimizer\nafter every ``N`` steps, you can do as such.\n\n.. testcode:: python\n\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n\n        # scale losses by 1/N (for N batches of gradient accumulation)\n        loss = self.compute_loss(batch) / N\n        self.manual_backward(loss)\n\n        # accumulate gradients of N batches\n        if (batch_idx + 1) % N == 0:\n            opt.step()\n            opt.zero_grad()\n\nGradient Clipping\n=================\n\nYou can clip optimizer gradients during manual optimization similar to passing the ``gradient_clip_val`` and\n``gradient_clip_algorithm`` argument in :ref:`Trainer <trainer>` during automatic optimization.\nTo perform gradient clipping with one optimizer with manual optimization, you can do as such.\n\n.. testcode:: python\n\n    from lightning.pytorch import LightningModule\n\n\n    class SimpleModel(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.automatic_optimization = False\n\n        def training_step(self, batch, batch_idx):\n            opt = self.optimizers()\n\n            # compute loss\n            loss = self.compute_loss(batch)\n\n            opt.zero_grad()\n            self.manual_backward(loss)\n\n            # clip gradients\n            self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n\n            opt.step()\n\n.. warning::\n   * Note that ``configure_gradient_clipping()`` won't be called in Manual Optimization. Instead consider using ``self. clip_gradients()`` manually like in the example above.\n\n\nUse Multiple Optimizers (like GANs)\n===================================\n\nHere is an example training a simple GAN with multiple optimizers using manual optimization.\n\n.. testcode:: python\n\n    import torch\n    from torch import Tensor\n    from lightning.pytorch import LightningModule\n\n\n    class SimpleGAN(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.G = Generator()\n            self.D = Discriminator()\n\n            # Important: This property activates manual optimization.\n            self.automatic_optimization = False\n\n        def sample_z(self, n) -> Tensor:\n            sample = self._Z.sample((n,))\n            return sample\n\n        def sample_G(self, n) -> Tensor:\n            z = self.sample_z(n)\n            return self.G(z)\n\n        def training_step(self, batch, batch_idx):\n            # Implementation follows the PyTorch tutorial:\n            # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n            g_opt, d_opt = self.optimizers()\n\n            X, _ = batch\n            batch_size = X.shape[0]\n\n            real_label = torch.ones((batch_size, 1), device=self.device)\n            fake_label = torch.zeros((batch_size, 1), device=self.device)\n\n            g_X = self.sample_G(batch_size)\n\n            ##########################\n            # Optimize Discriminator #\n            ##########################\n            d_x = self.D(X)\n            errD_real = self.criterion(d_x, real_label)\n\n            d_z = self.D(g_X.detach())\n            errD_fake = self.criterion(d_z, fake_label)\n\n            errD = errD_real + errD_fake\n\n            d_opt.zero_grad()\n            self.manual_backward(errD)\n            d_opt.step()\n\n            ######################\n            # Optimize Generator #\n            ######################\n            d_z = self.D(g_X)\n            errG = self.criterion(d_z, real_label)\n\n            g_opt.zero_grad()\n            self.manual_backward(errG)\n            g_opt.step()\n\n            self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\n        def configure_optimizers(self):\n            g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n            d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n            return g_opt, d_opt\n\nLearning Rate Scheduling\n========================\n\nEvery optimizer you use can be paired with any\n`Learning Rate Scheduler <https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate>`_. Please see the\ndocumentation of :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` for all the available options\n\nYou can call ``lr_scheduler.step()`` at arbitrary intervals.\nUse ``self.lr_schedulers()`` in  your :class:`~lightning.pytorch.core.LightningModule` to access any learning rate schedulers\ndefined in your :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\n\n.. warning::\n   * ``lr_scheduler.step()`` can be called at arbitrary intervals by the user in case of manual optimization, or by Lightning if ``\"interval\"`` is defined in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` in case of automatic optimization.\n   * Note that the ``lr_scheduler_config`` keys, such as ``\"frequency\"`` and ``\"interval\"``, will be ignored even if they are provided in\n     your :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` during manual optimization.\n\nHere is an example calling ``lr_scheduler.step()`` every step.\n\n.. testcode:: python\n\n    # step every batch\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # do forward, backward, and optimization\n        ...\n\n        # single scheduler\n        sch = self.lr_schedulers()\n        sch.step()\n\n        # multiple schedulers\n        sch1, sch2 = self.lr_schedulers()\n        sch1.step()\n        sch2.step()\n\nIf you want to call ``lr_scheduler.step()`` every ``N`` steps/epochs, do the following.\n\n.. testcode:: python\n\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n        return [optimizer], [scheduler]\n\n\n    def training_step(self, batch, batch_idx):\n        # do forward, backward, and optimization\n        ...\n\n        sch = self.lr_schedulers()\n\n        # step every N batches\n        if (batch_idx + 1) % N == 0:\n            sch.step()\n\n        # step every N epochs\n        if self.trainer.is_last_batch and (self.trainer.current_epoch + 1) % N == 0:\n            sch.step()\n\nIf you want to call schedulers that require a metric value after each epoch, consider doing the following:\n\n.. testcode::\n\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n        return [optimizer], [scheduler]\n\n    def on_train_epoch_end(self):\n        sch = self.lr_schedulers()\n\n        sch.step(self.trainer.callback_metrics[\"loss\"])\n\n.. note::\n    :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` supports 6 different ways to define and return\n    optimizers and learning rate schedulers. Regardless of the way you define them, `self.optimizers()` will always return\n    either a single optimizer if you defined a single optimizer, or a list of optimizers if you defined multiple\n    optimizers. The same applies to the `self.lr_schedulers()` method, which will return a single scheduler\n    if you defined a single scheduler, or a list of schedulers if you defined multiple schedulers\n\n\nOptimizer Steps at Different Frequencies\n========================================\n\nIn manual optimization, you are free to ``step()`` one optimizer more often than another one.\nFor example, here we step the optimizer for the *discriminator* weights twice as often as the optimizer for the *generator*.\n\n.. testcode:: python\n\n    # Alternating schedule for optimizer steps (e.g. GANs)\n    def training_step(self, batch, batch_idx):\n        g_opt, d_opt = self.optimizers()\n        ...\n\n        # update discriminator every other step\n        d_opt.zero_grad()\n        self.manual_backward(errD)\n        if (batch_idx + 1) % 2 == 0:\n            d_opt.step()\n\n        ...\n\n        # update generator every step\n        g_opt.zero_grad()\n        self.manual_backward(errG)\n        g_opt.step()\n\n\nUse Closure for LBFGS-like Optimizers\n=====================================\n\nIt is a good practice to provide the optimizer with a closure function that performs a ``forward``, ``zero_grad`` and\n``backward`` of your model. It is optional for most optimizers, but makes your code compatible if you switch to an\noptimizer which requires a closure, such as :class:`~torch.optim.LBFGS`.\n\nSee `the PyTorch docs <https://pytorch.org/docs/stable/optim.html#optimizer-step-closure>`_ for more about the closure.\n\nHere is an example using a closure function.\n\n.. testcode:: python\n\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n\n    def configure_optimizers(self):\n        return torch.optim.LBFGS(...)\n\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n\n        def closure():\n            loss = self.compute_loss(batch)\n            opt.zero_grad()\n            self.manual_backward(loss)\n            return loss\n\n        opt.step(closure=closure)\n\n.. warning::\n   The :class:`~torch.optim.LBFGS` optimizer is not supported for AMP or DeepSpeed.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/own_your_loop.html", "url_rel_html": "model/own_your_loop.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/own_your_loop.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Use a pure PyTorch training loop¶", "rst_text": ":orphan:\n\n################################\nUse a pure PyTorch training loop\n################################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Enable manual optimization\n   :description: Gain control of the training loop with manual optimization and LightningModule methods.\n   :col_css: col-md-4\n   :button_link: build_model_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/model/train_model_basic.html", "url_rel_html": "model/train_model_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/model/train_model_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Train a model (basic)¶", "rst_text": ":orphan:\n\n#####################\nTrain a model (basic)\n#####################\n**Audience**: Users who need to train a model without coding their own training loops.\n\n----\n\n***********\nAdd imports\n***********\nAdd the relevant imports at the top of the file\n\n.. code:: python\n\n    import os\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n    from torchvision import transforms\n    from torchvision.datasets import MNIST\n    from torch.utils.data import DataLoader\n    import lightning as L\n\n----\n\n*****************************\nDefine the PyTorch nn.Modules\n*****************************\n\n.. code:: python\n\n    class Encoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n\n        def forward(self, x):\n            return self.l1(x)\n\n\n    class Decoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n        def forward(self, x):\n            return self.l1(x)\n\n----\n\n************************\nDefine a LightningModule\n************************\nThe LightningModule is the full **recipe** that defines how your nn.Modules interact.\n\n- The **training_step** defines how the *nn.Modules* interact together.\n- In the **configure_optimizers** define the optimizer(s) for your models.\n\n.. code:: python\n\n    class LitAutoEncoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            super().__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n\n        def training_step(self, batch, batch_idx):\n            # training_step defines the train loop.\n            x, _ = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            loss = F.mse_loss(x_hat, x)\n            return loss\n\n        def configure_optimizers(self):\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n            return optimizer\n\n----\n\n***************************\nDefine the training dataset\n***************************\nDefine a PyTorch :class:`~torch.utils.data.DataLoader` which contains your training dataset.\n\n.. code-block:: python\n\n    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n    train_loader = DataLoader(dataset)\n\n----\n\n***************\nTrain the model\n***************\nTo train the model use the Lightning :doc:`Trainer <../common/trainer>` which handles all the engineering and abstracts away all the complexity needed for scale.\n\n.. code-block:: python\n\n    # model\n    autoencoder = LitAutoEncoder(Encoder(), Decoder())\n\n    # train model\n    trainer = L.Trainer()\n    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\n----\n\n***************************\nEliminate the training loop\n***************************\nUnder the hood, the Lightning Trainer runs the following training loop on your behalf\n\n.. code:: python\n\n    autoencoder = LitAutoEncoder(Encoder(), Decoder())\n    optimizer = autoencoder.configure_optimizers()\n\n    for batch_idx, batch in enumerate(train_loader):\n        loss = autoencoder.training_step(batch, batch_idx)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\nThe power of Lightning comes when the training loop gets complicated as you add validation/test splits, schedulers, distributed training and all the latest SOTA techniques.\n\nWith Lightning, you can add mix all these techniques together without needing to rewrite a new loop every time.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/starter/converting.html", "url_rel_html": "starter/converting.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/starter/converting.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "How to Organize PyTorch Into Lightning¶", "rst_text": ".. _converting:\n\n######################################\nHow to Organize PyTorch Into Lightning\n######################################\n\nTo enable your code to work with Lightning, perform the following to organize PyTorch into Lightning.\n\n--------\n\n*******************************\n1. Keep Your Computational Code\n*******************************\n\nKeep your regular nn.Module architecture\n\n.. testcode::\n\n    import lightning as L\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class LitModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layer_1 = nn.Linear(28 * 28, 128)\n            self.layer_2 = nn.Linear(128, 10)\n\n        def forward(self, x):\n            x = x.view(x.size(0), -1)\n            x = self.layer_1(x)\n            x = F.relu(x)\n            x = self.layer_2(x)\n            return x\n\n--------\n\n***************************\n2. Configure Training Logic\n***************************\nIn the training_step of the LightningModule configure how your training routine behaves with a batch of training data:\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def __init__(self, encoder):\n            super().__init__()\n            self.encoder = encoder\n\n        def training_step(self, batch, batch_idx):\n            x, y = batch\n            y_hat = self.encoder(x)\n            loss = F.cross_entropy(y_hat, y)\n            return loss\n\n.. note:: If you need to fully own the training loop for complicated legacy projects, check out :doc:`Own your loop <../model/own_your_loop>`.\n\n----\n\n****************************************\n3. Move Optimizer(s) and LR Scheduler(s)\n****************************************\nMove your optimizers to the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` hook.\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def configure_optimizers(self):\n            optimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-3)\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n            return [optimizer], [lr_scheduler]\n\n--------\n\n***************************************\n4. Organize Validation Logic (optional)\n***************************************\nIf you need a validation loop, configure how your validation routine behaves with a batch of validation data:\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def validation_step(self, batch, batch_idx):\n            x, y = batch\n            y_hat = self.encoder(x)\n            val_loss = F.cross_entropy(y_hat, y)\n            self.log(\"val_loss\", val_loss)\n\n.. tip:: ``trainer.validate()`` loads the best checkpoint automatically by default if checkpointing was enabled during fitting.\n\n--------\n\n************************************\n5. Organize Testing Logic (optional)\n************************************\nIf you need a test loop, configure how your testing routine behaves with a batch of test data:\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def test_step(self, batch, batch_idx):\n            x, y = batch\n            y_hat = self.encoder(x)\n            test_loss = F.cross_entropy(y_hat, y)\n            self.log(\"test_loss\", test_loss)\n\n--------\n\n****************************************\n6. Configure Prediction Logic (optional)\n****************************************\nIf you need a prediction loop, configure how your prediction routine behaves with a batch of test data:\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def predict_step(self, batch, batch_idx):\n            x, y = batch\n            pred = self.encoder(x)\n            return pred\n\n--------\n\n******************************************\n7. Remove any .cuda() or .to(device) Calls\n******************************************\n\nYour :doc:`LightningModule <../common/lightning_module>` can automatically run on any hardware!\n\nIf you have any explicit calls to ``.cuda()`` or ``.to(device)``, you can remove them since Lightning makes sure that the data coming from :class:`~torch.utils.data.DataLoader`\nand all the :class:`~torch.nn.Module` instances initialized inside ``LightningModule.__init__`` are moved to the respective devices automatically.\nIf you still need to access the current device, you can use ``self.device`` anywhere in your ``LightningModule`` except in the ``__init__`` and ``setup`` methods.\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def training_step(self, batch, batch_idx):\n            z = torch.randn(4, 5, device=self.device)\n            ...\n\nHint: If you are initializing a :class:`~torch.Tensor` within the ``LightningModule.__init__`` method and want it to be moved to the device automatically you should call\n:meth:`~torch.nn.Module.register_buffer` to register it as a parameter.\n\n.. testcode::\n\n    class LitModel(L.LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.register_buffer(\"running_mean\", torch.zeros(num_features))\n\n--------\n\n********************\n8. Use your own data\n********************\nRegular PyTorch DataLoaders work with Lightning. For more modular and scalable datasets, check out :doc:`LightningDataModule <../data/datamodule>`.\n\n----\n\n************\nGood to know\n************\n\nAdditionally, you can run only the validation loop using :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate` method.\n\n.. code-block:: python\n\n    model = LitModel()\n    trainer.validate(model)\n\n.. note:: ``model.eval()`` and ``torch.no_grad()`` are called automatically for validation.\n\n\nThe test loop isn't used within :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`, therefore, you would need to explicitly call :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`.\n\n.. code-block:: python\n\n    model = LitModel()\n    trainer.test(model)\n\n.. note:: ``model.eval()`` and ``torch.no_grad()`` are called automatically for testing.\n\n.. tip:: ``trainer.test()`` loads the best checkpoint automatically by default if checkpointing is enabled.\n\n\nThe predict loop will not be used until you call :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\n\n.. code-block:: python\n\n    model = LitModel()\n    trainer.predict(model)\n\n.. note:: ``model.eval()`` and ``torch.no_grad()`` are called automatically for predicting.\n\n.. tip:: ``trainer.predict()`` loads the best checkpoint automatically by default if checkpointing is enabled.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/starter/installation.html", "url_rel_html": "starter/installation.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/starter/installation.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Installation¶", "rst_text": ":orphan:\n\n.. _installation:\n\n############\nInstallation\n############\n\n****************\nInstall with pip\n****************\n\nInstall lightning inside a virtual env or conda environment with pip\n\n.. code-block:: bash\n\n    python -m pip install lightning\n\n\n----\n\n\n******************\nInstall with Conda\n******************\n\nIf you don't have conda installed, follow the `Conda Installation Guide <https://docs.conda.io/projects/conda/en/latest/user-guide/install>`_.\nLightning can be installed with `conda <https://anaconda.org/conda-forge/pytorch-lightning>`_ using the following command:\n\n.. code-block:: bash\n\n    conda install lightning -c conda-forge\n\nYou can also use `Conda Environments <https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html>`_:\n\n.. code-block:: bash\n\n    conda activate my_env\n    conda install lightning -c conda-forge\n\n----\n\n\nIn case you face difficulty with pulling the GRPC package, please follow this `thread <https://stackoverflow.com/questions/66640705/how-can-i-install-grpcio-on-an-apple-m1-silicon-laptop>`_\n\n\n----\n\n*****************\nBuild from Source\n*****************\n\nInstall nightly from the source. Note that it contains all the bug fixes and newly released features that\nare not published yet. This is the bleeding edge, so use it at your own discretion.\n\n.. code-block:: bash\n\n    pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n\nInstall future patch releases from the source. Note that the patch release contains only the bug fixes for the recent major release.\n\n.. code-block:: bash\n\n    pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n\n\n\n^^^^^^^^^^^^^^^^^^^^^^\nCustom PyTorch Version\n^^^^^^^^^^^^^^^^^^^^^^\n\nTo use any PyTorch version visit the `PyTorch Installation Page <https://pytorch.org/get-started/locally/#start-locally>`_.\nYou can find the list of supported PyTorch versions in our :ref:`compatibility matrix <versioning:Compatibility matrix>`.\n\n----\n\n\n*******************************************\nOptimized for ML workflows (Lightning Apps)\n*******************************************\nIf you are deploying workflows built with Lightning in production and require fewer dependencies, try using the optimized ``lightning[apps]`` package:\n\n.. code-block:: bash\n\n    pip install lightning-app\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/starter/introduction.html", "url_rel_html": "starter/introduction.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/starter/introduction.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Lightning in 15 minutes¶", "rst_text": ":orphan:\n\n#######################\nLightning in 15 minutes\n#######################\n**Required background:** None\n\n**Goal:** In this guide, we'll walk you through the 7 key steps of a typical Lightning workflow.\n\nPyTorch Lightning is the deep learning framework with \"batteries included\" for professional AI researchers and machine learning engineers who need maximal flexibility while super-charging performance at scale.\n\nLightning organizes PyTorch code to remove boilerplate and unlock scalability.\n\n.. video:: https://pl-public-data.s3.amazonaws.com/assets_lightning/pl_readme_gif_2_0.mp4\n    :width: 800\n    :autoplay:\n    :loop:\n    :muted:\n\nBy organizing PyTorch code, lightning enables:\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Full flexibility\n   :description: Try any ideas using raw PyTorch without the boilerplate.\n   :col_css: col-md-3\n   :image_center: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/card_full_control.png\n   :height: 290\n\n.. displayitem::\n   :description: Decoupled research and engineering code enable reproducibility and better readability.\n   :header: Reproducible + Readable\n   :col_css: col-md-3\n   :image_center: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/card_no_boilerplate.png\n   :height: 290\n\n.. displayitem::\n   :description: Use multiple GPUs/TPUs/HPUs etc... without code changes.\n   :header: Simple multi-GPU training\n   :col_css: col-md-3\n   :image_center: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/card_hardware.png\n   :height: 290\n\n.. displayitem::\n   :description: We've done all the testing so you don't have to.\n   :header: Built-in testing\n   :col_css: col-md-3\n   :image_center: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/card_testing.png\n   :height: 290\n\n.. raw:: html\n\n        </div>\n    </div>\n\n.. End of callout item section\n\n----\n\n****************************\n1: Install PyTorch Lightning\n****************************\n.. raw:: html\n\n   <div class=\"row\" style='font-size: 16px'>\n      <div class='col-md-6'>\n\nFor `pip <https://pypi.org/project/pytorch-lightning/>`_ users\n\n.. code-block:: bash\n\n    pip install lightning\n\n.. raw:: html\n\n      </div>\n      <div class='col-md-6'>\n\nFor `conda <https://anaconda.org/conda-forge/pytorch-lightning>`_ users\n\n.. code-block:: bash\n\n    conda install lightning -c conda-forge\n\n.. raw:: html\n\n      </div>\n   </div>\n\nOr read the `advanced install guide <installation.html>`_\n\n----\n\n.. _new_project:\n\n***************************\n2: Define a LightningModule\n***************************\n\nA LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\n\n.. testcode::\n    :skipif: not _TORCHVISION_AVAILABLE\n\n    import os\n    from torch import optim, nn, utils, Tensor\n    from torchvision.datasets import MNIST\n    from torchvision.transforms import ToTensor\n    import lightning as L\n\n    # define any number of nn.Modules (or use your current ones)\n    encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n    decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n\n    # define the LightningModule\n    class LitAutoEncoder(L.LightningModule):\n        def __init__(self, encoder, decoder):\n            super().__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n\n        def training_step(self, batch, batch_idx):\n            # training_step defines the train loop.\n            # it is independent of forward\n            x, _ = batch\n            x = x.view(x.size(0), -1)\n            z = self.encoder(x)\n            x_hat = self.decoder(z)\n            loss = nn.functional.mse_loss(x_hat, x)\n            # Logging to TensorBoard (if installed) by default\n            self.log(\"train_loss\", loss)\n            return loss\n\n        def configure_optimizers(self):\n            optimizer = optim.Adam(self.parameters(), lr=1e-3)\n            return optimizer\n\n\n    # init the autoencoder\n    autoencoder = LitAutoEncoder(encoder, decoder)\n\n----\n\n*******************\n3: Define a dataset\n*******************\n\nLightning supports ANY iterable (:class:`~torch.utils.data.DataLoader`, numpy, etc...) for the train/val/test/predict splits.\n\n.. code-block:: python\n\n    # setup data\n    dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n    train_loader = utils.data.DataLoader(dataset)\n\n----\n\n******************\n4: Train the model\n******************\n\nThe Lightning :doc:`Trainer <../common/trainer>` \"mixes\" any :doc:`LightningModule <../common/lightning_module>` with any dataset and abstracts away all the engineering complexity needed for scale.\n\n.. code-block:: python\n\n    # train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n    trainer = L.Trainer(limit_train_batches=100, max_epochs=1)\n    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\nThe Lightning :doc:`Trainer <../common/trainer>` automates `40+ tricks <../common/trainer.html#trainer-flags>`_ including:\n\n* Epoch and batch iteration\n* ``optimizer.step()``, ``loss.backward()``, ``optimizer.zero_grad()`` calls\n* Calling of ``model.eval()``, enabling/disabling grads during evaluation\n* :doc:`Checkpoint Saving and Loading <../common/checkpointing>`\n* Tensorboard (see :doc:`loggers <../visualize/loggers>` options)\n* :doc:`Multi-GPU <../accelerators/gpu>` support\n* :doc:`TPU <../accelerators/tpu>`\n* :ref:`16-bit precision AMP <speed-amp>` support\n\n----\n\n\n****************\n5: Use the model\n****************\nOnce you've trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions.\n\n.. code:: python\n\n    # load checkpoint\n    checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\n    autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n\n    # choose your trained nn.Module\n    encoder = autoencoder.encoder\n    encoder.eval()\n\n    # embed 4 fake images!\n    fake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\n    embeddings = encoder(fake_image_batch)\n    print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)\n\n----\n\n*********************\n6: Visualize training\n*********************\nIf you have tensorboard installed, you can use it for visualizing experiments.\n\nRun this on your commandline and open your browser to **http://localhost:6006/**\n\n.. code:: bash\n\n    tensorboard --logdir .\n\n----\n\n***********************\n7: Supercharge training\n***********************\nEnable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code.\n\n.. code::\n\n   # train on 4 GPUs\n   trainer = L.Trainer(\n       devices=4,\n       accelerator=\"gpu\",\n    )\n\n   # train 1TB+ parameter models with Deepspeed/fsdp\n   trainer = L.Trainer(\n       devices=4,\n       accelerator=\"gpu\",\n       strategy=\"deepspeed_stage_2\",\n       precision=16\n    )\n\n   # 20+ helpful flags for rapid idea iteration\n   trainer = L.Trainer(\n       max_epochs=10,\n       min_epochs=5,\n       overfit_batches=1\n    )\n\n   # access the latest state of the art techniques\n   trainer = L.Trainer(callbacks=[WeightAveraging(...)])\n\n----\n\n********************\nMaximize flexibility\n********************\nLightning's core guiding principle is to always provide maximal flexibility **without ever hiding any of the PyTorch**.\n\nLightning offers 5 *added* degrees of flexibility depending on your project's complexity.\n\n----\n\nCustomize training loop\n=======================\n\n.. image:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/custom_loop.png\n    :width: 600\n    :alt: Injecting custom code in a training loop\n\nInject custom code anywhere in the Training loop using any of the 20+ methods (:ref:`lightning_hooks`) available in the LightningModule.\n\n.. testcode::\n\n    class LitAutoEncoder(L.LightningModule):\n        def backward(self, loss):\n            loss.backward()\n\n----\n\nExtend the Trainer\n==================\n\n.. video:: https://pl-public-data.s3.amazonaws.com/assets_lightning/cb.mp4\n    :width: 600\n    :autoplay:\n    :loop:\n    :muted:\n\nIf you have multiple lines of code with similar functionalities, you can use callbacks to easily group them together and toggle all of those lines on or off at the same time.\n\n.. code::\n\n   trainer = Trainer(callbacks=[AWSCheckpoints()])\n\n----\n\nUse a raw PyTorch loop\n======================\n\nFor certain types of work at the bleeding-edge of research, Lightning offers experts full control of optimization or the training loop in various ways.\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Manual optimization\n   :description: Automated training loop, but you own the optimization steps.\n   :col_css: col-md-4\n   :image_center: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/manual_opt.png\n   :button_link: ../model/build_model_advanced.html#manual-optimization\n   :image_height: 220px\n   :height: 320\n\n.. raw:: html\n\n        </div>\n    </div>\n\n.. End of callout item section\n\n----\n\n**********\nNext steps\n**********\nDepending on your use case, you might want to check one of these out next.\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Level 2: Add a validation and test set\n   :description: Add validation and test sets to avoid over/underfitting.\n   :button_link: ../levels/basic_level_2.html\n   :col_css: col-md-3\n   :height: 180\n   :tag: basic\n\n.. displayitem::\n   :header: See more examples\n   :description: See examples across computer vision, NLP, RL, etc...\n   :col_css: col-md-3\n   :button_link: ../tutorials.html\n   :height: 180\n   :tag: basic\n\n.. displayitem::\n   :header: Deploy your model\n   :description: Learn how to predict or put your model into production\n   :col_css: col-md-3\n   :button_link: ../deploy/production.html\n   :height: 180\n   :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/starter/style_guide.html", "url_rel_html": "starter/style_guide.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/starter/style_guide.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Style Guide¶", "rst_text": "###########\nStyle Guide\n###########\nThe main goal of PyTorch Lightning is to improve readability and reproducibility. Imagine looking into any GitHub repo or a research project,\nfinding a :class:`~lightning.pytorch.core.LightningModule`, and knowing exactly where to look to find the things you care about.\n\nThe goal of this style guide is to encourage Lightning code to be structured similarly.\n\n--------------\n\n***************\nLightningModule\n***************\n\nThese are best practices for structuring your :class:`~lightning.pytorch.core.LightningModule` class:\n\nSystems vs Models\n=================\n\n.. figure:: https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/model_system.png\n    :width: 400\n\nThe main principle behind a LightningModule is that a full system should be self-contained.\nIn Lightning, we differentiate between a system and a model.\n\nA model is something like a resnet18, RNN, and so on.\n\nA system defines how a collection of models interact with each other with user-defined training/evaluation logic. Examples of this are:\n\n* GANs\n* Seq2Seq\n* BERT\n* etc.\n\nA LightningModule can define both a system and a model:\n\nHere's a LightningModule that defines a system. This structure is what we recommend as a best practice. Keeping the model separate from the system improves\nmodularity, which eventually helps in better testing, reduces dependencies on the system and makes it easier to refactor.\n\n.. testcode::\n\n    class Encoder(nn.Module):\n        ...\n\n\n    class Decoder(nn.Module):\n        ...\n\n\n    class AutoEncoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.encoder = Encoder()\n            self.decoder = Decoder()\n\n        def forward(self, x):\n            return self.encoder(x)\n\n\n    class AutoEncoderSystem(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.auto_encoder = AutoEncoder()\n\n\nFor fast prototyping, it's often useful to define all the computations in a LightningModule. For reusability\nand scalability, it might be better to pass in the relevant backbones.\n\nHere's a LightningModule that defines a model. Although, we do not recommend to define a model like in the example.\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def __init__(self):\n            super().__init__()\n            self.layer_1 = nn.Linear()\n            self.layer_2 = nn.Linear()\n            self.layer_3 = nn.Linear()\n\n\nSelf-contained\n==============\n\nA Lightning module should be self-contained. To see how self-contained your model is, a good test is to ask\nyourself this question:\n\n\"Can someone drop this file into a Trainer without knowing anything about the internals?\"\n\nFor example, we couple the optimizer with a model because the majority of models require a specific optimizer with\na specific learning rate scheduler to work well.\n\nInit\n====\nThe first place where LightningModules tend to stop being self-contained is in the init. Try to define all the relevant\nsensible defaults in the init so that the user doesn't have to guess.\n\nHere's an example where a user will have to go hunt through files to figure out how to init this LightningModule.\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def __init__(self, params):\n            self.lr = params.lr\n            self.coef_x = params.coef_x\n\nModels defined as such leave you with many questions, such as what is ``coef_x``? Is it a string? A float? What is the range?\nInstead, be explicit in your init\n\n.. testcode::\n\n    class LitModel(LightningModule):\n        def __init__(self, encoder: nn.Module, coef_x: float = 0.2, lr: float = 1e-3):\n            ...\n\nNow the user doesn't have to guess. Instead, they know the value type, and the model has a sensible default where the\nuser can see the value immediately.\n\n\nMethod Order\n============\nThe only required methods in the LightningModule are:\n\n* init\n* training_step\n* configure_optimizers\n\nHowever, if you decide to implement the rest of the optional methods, the recommended order is:\n\n* model/system definition (init)\n* if doing inference, define forward\n* training hooks\n* validation hooks\n* test hooks\n* predict hooks\n* configure_optimizers\n* any other hooks\n\nIn practice, the code looks like this:\n\n.. code-block::\n\n    class LitModel(L.LightningModule):\n\n        def __init__(...):\n\n        def forward(...):\n\n        def training_step(...):\n\n        def on_train_epoch_end(...):\n\n        def validation_step(...):\n\n        def on_validation_epoch_end(...):\n\n        def test_step(...):\n\n        def on_test_epoch_end(...):\n\n        def configure_optimizers(...):\n\n        def any_extra_hook(...):\n\n\nForward vs training_step\n========================\n\nWe recommend using :meth:`~lightning.pytorch.core.LightningModule.forward` for inference/predictions and keeping\n:meth:`~lightning.pytorch.core.LightningModule.training_step` independent.\n\n.. code-block:: python\n\n    def forward(self, x):\n        embeddings = self.encoder(x)\n        return embeddings\n\n\n    def training_step(self, batch, batch_idx):\n        x, _ = batch\n        z = self.encoder(x)\n        pred = self.decoder(z)\n        ...\n\n\n--------------\n\n****\nData\n****\n\nThese are best practices for handling data.\n\nDataLoaders\n===========\n\nLightning uses :class:`~torch.utils.data.DataLoader` to handle all the data flow through the system. Whenever you structure dataloaders,\nmake sure to tune the number of workers for maximum efficiency.\n\n\nDataModules\n===========\n\nThe :class:`~lightning.pytorch.core.datamodule.LightningDataModule` is designed as a way of decoupling data-related\nhooks from the :class:`~lightning.pytorch.core.LightningModule` so you can develop dataset agnostic models. It makes it easy to hot swap different\ndatasets with your model, so you can test it and benchmark it across domains. It also makes sharing and reusing the exact data splits and transforms across projects possible.\n\nCheck out :ref:`data` document to understand data management within Lightning and its best practices.\n\n* What dataset splits were used?\n* How many samples does this dataset have overall and within each split?\n* Which transforms were used?\n\nIt's for this reason that we recommend you use datamodules. This is especially important when collaborating because\nit will save your team a lot of time as well.\n\nAll they need to do is drop a datamodule into the Trainer and not worry about what was done to the data.\n\nThis is true for both academic and corporate settings where data cleaning and ad-hoc instructions slow down the progress\nof iterating through ideas.\n\n- Check out the live examples to get your hands dirty:\n- `Introduction to PyTorch Lightning <https://lightning.ai/docs/pytorch/2.1.0/notebooks/lightning_examples/mnist-hello-world.html>`_\n- `Introduction to DataModules <https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/datamodules.html>`_\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/tuning/profiler.html", "url_rel_html": "tuning/profiler.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/tuning/profiler.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Find bottlenecks in your code¶", "rst_text": ".. _profiler:\n\n#############################\nFind bottlenecks in your code\n#############################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Basic\n   :description: Learn to find bottlenecks in the training loop.\n   :col_css: col-md-3\n   :button_link: profiler_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn to find bottlenecks in PyTorch operations.\n   :col_css: col-md-3\n   :button_link: profiler_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Learn to profile TPU code.\n   :col_css: col-md-3\n   :button_link: profiler_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Expert\n   :description: Learn to build your own profiler or profile custom pieces of code\n   :col_css: col-md-3\n   :button_link: profiler_expert.html\n   :height: 150\n   :tag: expert\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/tuning/profiler_advanced.html", "url_rel_html": "tuning/profiler_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/tuning/profiler_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Find bottlenecks in your code (advanced)¶", "rst_text": ":orphan:\n\n.. _profiler_advanced:\n\n########################################\nFind bottlenecks in your code (advanced)\n########################################\n**Audience**: Users who want to profile their TPU models to find bottlenecks and improve performance.\n\n----\n\n************************\nProfile cloud TPU models\n************************\nTo profile TPU models use the :class:`~lightning.pytorch.profilers.xla.XLAProfiler`\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import XLAProfiler\n\n    profiler = XLAProfiler(port=9001)\n    trainer = Trainer(profiler=profiler)\n\n----\n\n*************************************\nCapture profiling logs in Tensorboard\n*************************************\nTo capture profile logs in Tensorboard, follow these instructions:\n\n----\n\n0: Setup the required installs\n==============================\nUse this `guide <https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm#tpu-vm>`_ to help you with the Cloud TPU required installations.\n\n----\n\n1: Start Tensorboard\n====================\nStart the `TensorBoard <https://www.tensorflow.org/tensorboard>`_ server:\n\n.. code-block:: bash\n\n    tensorboard --logdir ./tensorboard --port 9001\n\nNow open the following url on your browser\n\n.. code-block:: bash\n\n    http://localhost:9001/#profile\n\n----\n\n2: Capture the profile\n======================\nOnce the code you want to profile is running:\n\n1. click on the ``CAPTURE PROFILE`` button.\n2. Enter ``localhost:9001`` (default port for XLA Profiler) as the Profile Service URL.\n3. Enter the number of milliseconds for the profiling duration\n4. Click ``CAPTURE``\n\n----\n\n3: Don't stop your code\n=======================\nMake sure the code is running while you are trying to capture the traces. It will lead to better performance insights if the profiling duration is longer than the step time.\n\n----\n\n4: View the profiling logs\n==========================\nOnce the capture is finished, the page will refresh and you can browse through the insights using the **Tools** dropdown at the top left\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/tuning/profiler_basic.html", "url_rel_html": "tuning/profiler_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/tuning/profiler_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Find bottlenecks in your code (basic)¶", "rst_text": ":orphan:\n\n.. _profiler_basic:\n\n#####################################\nFind bottlenecks in your code (basic)\n#####################################\n**Audience**: Users who want to learn the basics of removing bottlenecks from their code\n\n----\n\n************************\nWhy do I need profiling?\n************************\nProfiling helps you find bottlenecks in your code by capturing analytics such as how long a function takes or how much memory is used.\n\n------------\n\n******************************\nFind training loop bottlenecks\n******************************\nThe most basic profile measures all the key methods across **Callbacks**, **DataModules** and the **LightningModule** in the training loop.\n\n.. code-block:: python\n\n    trainer = Trainer(profiler=\"simple\")\n\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n.. code-block::\n\n    FIT Profiler Report\n\n    -------------------------------------------------------------------------------------------\n    |  Action                                          |  Mean duration (s) |  Total time (s) |\n    -------------------------------------------------------------------------------------------\n    |  [LightningModule]BoringModel.prepare_data       |  10.0001           |  20.00          |\n    |  run_training_epoch                              |  6.1558            |  6.1558         |\n    |  run_training_batch                              |  0.0022506         |  0.015754       |\n    |  [LightningModule]BoringModel.optimizer_step     |  0.0017477         |  0.012234       |\n    |  [LightningModule]BoringModel.val_dataloader     |  0.00024388        |  0.00024388     |\n    |  on_train_batch_start                            |  0.00014637        |  0.0010246      |\n    |  [LightningModule]BoringModel.teardown           |  2.15e-06          |  2.15e-06       |\n    |  [LightningModule]BoringModel.on_train_start     |  1.644e-06         |  1.644e-06      |\n    |  [LightningModule]BoringModel.on_train_end       |  1.516e-06         |  1.516e-06      |\n    |  [LightningModule]BoringModel.on_fit_end         |  1.426e-06         |  1.426e-06      |\n    |  [LightningModule]BoringModel.setup              |  1.403e-06         |  1.403e-06      |\n    |  [LightningModule]BoringModel.on_fit_start       |  1.226e-06         |  1.226e-06      |\n    -------------------------------------------------------------------------------------------\n\nIn this report we can see that the slowest function is **prepare_data**. Now you can figure out why data preparation is slowing down your training.\n\nThe simple profiler measures all the standard methods used in the training loop automatically, including:\n\n- on_train_epoch_start\n- on_train_epoch_end\n- on_train_batch_start\n- model_backward\n- on_after_backward\n- optimizer_step\n- on_train_batch_end\n- on_training_end\n- etc...\n\n----\n\n**************************************\nProfile the time within every function\n**************************************\nTo profile the time within every function, use the :class:`~lightning.pytorch.profilers.advanced.AdvancedProfiler` built on top of Python's `cProfiler <https://docs.python.org/3/library/profile.html#module-cProfile>`_.\n\n\n.. code-block:: python\n\n    trainer = Trainer(profiler=\"advanced\")\n\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n.. code-block::\n\n    Profiler Report\n\n    Profile stats for: get_train_batch\n            4869394 function calls (4863767 primitive calls) in 18.893 seconds\n    Ordered by: cumulative time\n    List reduced from 76 to 10 due to restriction <10>\n    ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n        1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n        1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n        1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n        1875     0.084    0.000   18.290    0.010 fetch.py:44(<listcomp>)\n        60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n        60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n        60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n        60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n        60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n\nIf the profiler report becomes too long, you can stream the report to a file:\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import AdvancedProfiler\n\n    profiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\n    trainer = Trainer(profiler=profiler)\n\n----\n\n*************************\nMeasure accelerator usage\n*************************\nAnother helpful technique to detect bottlenecks is to ensure that you're using the full capacity of your accelerator (GPU/TPU/HPU).\nThis can be measured with the :class:`~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor`:\n\n.. testcode::\n\n    from lightning.pytorch.callbacks import DeviceStatsMonitor\n\n    trainer = Trainer(callbacks=[DeviceStatsMonitor()])\n\nCPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set ``DeviceStatsMonitor(cpu_stats=True)``. To disable logging\nCPU metrics, you can specify ``DeviceStatsMonitor(cpu_stats=False)``.\n\n.. warning::\n\n    **Do not wrap** ``Trainer.fit()``, ``Trainer.validate()``, or other Trainer methods inside a manual\n    ``torch.profiler.profile`` context manager. This will cause unexpected crashes and cryptic errors due to\n    incompatibility between PyTorch Profiler's context management and Lightning's internal training loop.\n    Instead, always use the ``profiler`` argument in the ``Trainer`` constructor or the\n    :class:`~lightning.pytorch.profilers.pytorch.PyTorchProfiler` profiler class if you want to customize the profiling.\n\n    Example:\n\n    .. code-block:: python\n\n        from lightning.pytorch import Trainer\n        from lightning.pytorch.profilers import PytorchProfiler\n\n        trainer = Trainer(profiler=\"pytorch\")\n        # or\n        trainer = Trainer(profiler=PytorchProfiler(dirpath=\".\", filename=\"perf_logs\"))\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/tuning/profiler_expert.html", "url_rel_html": "tuning/profiler_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/tuning/profiler_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Find bottlenecks in your code (expert)¶", "rst_text": ":orphan:\n\n.. _profiler_expert:\n\n######################################\nFind bottlenecks in your code (expert)\n######################################\n**Audience**: Users who want to build their own profilers.\n\n----\n\n***********************\nBuild your own profiler\n***********************\nTo build your own profiler, subclass :class:`~lightning.pytorch.profilers.profiler.Profiler`\nand override some of its methods. Here is a simple example that profiles the first occurrence and total calls of each action:\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import Profiler\n    from collections import defaultdict\n    import time\n\n\n    class ActionCountProfiler(Profiler):\n        def __init__(self, dirpath=None, filename=None):\n            super().__init__(dirpath=dirpath, filename=filename)\n            self._action_count = defaultdict(int)\n            self._action_first_occurrence = {}\n\n        def start(self, action_name):\n            if action_name not in self._action_first_occurrence:\n                self._action_first_occurrence[action_name] = time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n\n        def stop(self, action_name):\n            self._action_count[action_name] += 1\n\n        def summary(self):\n            res = f\"\\nProfile Summary: \\n\"\n            max_len = max(len(x) for x in self._action_count)\n\n            for action_name in self._action_count:\n                # generate summary for actions called more than once\n                if self._action_count[action_name] > 1:\n                    res += (\n                        f\"{action_name:<{max_len}s} \\t \"\n                        + \"self._action_first_occurrence[action_name]} \\t \"\n                        + \"{self._action_count[action_name]} \\n\"\n                    )\n\n            return res\n\n        def teardown(self, stage):\n            self._action_count = {}\n            self._action_first_occurrence = {}\n            super().teardown(stage=stage)\n\n.. code-block:: python\n\n    trainer = Trainer(profiler=ActionCountProfiler())\n    trainer.fit(...)\n\n----\n\n**********************************\nProfile custom actions of interest\n**********************************\nTo profile a specific action of interest, reference a profiler in the LightningModule.\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\n\n    class MyModel(LightningModule):\n        def __init__(self, profiler=None):\n            self.profiler = profiler or PassThroughProfiler()\n\nTo profile in any part of your code, use the **self.profiler.profile()** function\n\n.. code-block:: python\n\n    class MyModel(LightningModule):\n        def custom_processing_step(self, data):\n            with self.profiler.profile(\"my_custom_action\"):\n                ...\n            return data\n\nHere's the full code:\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\n\n    class MyModel(LightningModule):\n        def __init__(self, profiler=None):\n            self.profiler = profiler or PassThroughProfiler()\n\n        def custom_processing_step(self, data):\n            with self.profiler.profile(\"my_custom_action\"):\n                ...\n            return data\n\n\n    profiler = SimpleProfiler()\n    model = MyModel(profiler)\n    trainer = Trainer(profiler=profiler, max_epochs=1)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/tuning/profiler_intermediate.html", "url_rel_html": "tuning/profiler_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/tuning/profiler_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Find bottlenecks in your code (intermediate)¶", "rst_text": ":orphan:\n\n.. _profiler_intermediate:\n\n############################################\nFind bottlenecks in your code (intermediate)\n############################################\n**Audience**: Users who want to see more granular profiling information\n\n----\n\n**************************\nProfile pytorch operations\n**************************\nTo understand the cost of each PyTorch operation, use the :class:`~lightning.pytorch.profilers.pytorch.PyTorchProfiler` built on top of the `PyTorch profiler <https://pytorch.org/docs/master/profiler.html>`__.\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import PyTorchProfiler\n\n    profiler = PyTorchProfiler()\n    trainer = Trainer(profiler=profiler)\n\nThe profiler will generate an output like this:\n\n.. code-block::\n\n    Profiler Report\n\n    Profile stats for: training_step\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Name                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    t                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\n    addmm                  32.32%           543.135us        32.69%           549.362us        549.362us\n    mse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\n    mean                   0.22%            3.694us          2.05%            34.523us         34.523us\n    div_                   0.64%            10.756us         1.90%            32.001us         16.000us\n    ones_like              0.21%            3.461us          0.81%            13.669us         13.669us\n    sum_out                0.45%            7.638us          0.74%            12.432us         12.432us\n    transpose              0.23%            3.786us          0.68%            11.393us         11.393us\n    as_strided             0.60%            10.060us         0.60%            10.060us         3.353us\n    to                     0.18%            3.059us          0.44%            7.464us          7.464us\n    empty_like             0.14%            2.387us          0.41%            6.859us          6.859us\n    empty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\n    fill_                  0.28%            4.782us          0.33%            5.566us          2.783us\n    expand                 0.20%            3.336us          0.28%            4.743us          4.743us\n    empty                  0.27%            4.456us          0.27%            4.456us          2.228us\n    copy_                  0.15%            2.526us          0.15%            2.526us          2.526us\n    broadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\n    size                   0.06%            0.967us          0.06%            0.967us          0.484us\n    is_complex             0.06%            0.961us          0.06%            0.961us          0.481us\n    stride                 0.03%            0.517us          0.03%            0.517us          0.517us\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Self CPU time total: 1.681ms\n\n.. note::\n    When using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\n    This is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\n    It is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\n    the ``SimpleProfiler``.\n\n----\n\n***************************\nProfile a distributed model\n***************************\nTo profile a distributed model, use the :class:`~lightning.pytorch.profilers.pytorch.PyTorchProfiler` with the *filename* argument which will save a report per rank.\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import PyTorchProfiler\n\n    profiler = PyTorchProfiler(filename=\"perf-logs\")\n    trainer = Trainer(profiler=profiler)\n\nWith two ranks, it will generate a report like so:\n\n.. code-block::\n\n    Profiler Report: rank 0\n\n    Profile stats for: training_step\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Name                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    t                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\n    addmm                  32.32%           543.135us        32.69%           549.362us        549.362us\n    mse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\n    mean                   0.22%            3.694us          2.05%            34.523us         34.523us\n    div_                   0.64%            10.756us         1.90%            32.001us         16.000us\n    ones_like              0.21%            3.461us          0.81%            13.669us         13.669us\n    sum_out                0.45%            7.638us          0.74%            12.432us         12.432us\n    transpose              0.23%            3.786us          0.68%            11.393us         11.393us\n    as_strided             0.60%            10.060us         0.60%            10.060us         3.353us\n    to                     0.18%            3.059us          0.44%            7.464us          7.464us\n    empty_like             0.14%            2.387us          0.41%            6.859us          6.859us\n    empty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\n    fill_                  0.28%            4.782us          0.33%            5.566us          2.783us\n    expand                 0.20%            3.336us          0.28%            4.743us          4.743us\n    empty                  0.27%            4.456us          0.27%            4.456us          2.228us\n    copy_                  0.15%            2.526us          0.15%            2.526us          2.526us\n    broadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\n    size                   0.06%            0.967us          0.06%            0.967us          0.484us\n    is_complex             0.06%            0.961us          0.06%            0.961us          0.481us\n    stride                 0.03%            0.517us          0.03%            0.517us          0.517us\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Self CPU time total: 1.681ms\n\n.. code-block::\n\n    Profiler Report: rank 1\n\n    Profile stats for: training_step\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Name                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    t                      42.10%           1.044ms          62.77%           1.055ms          1.055ms\n    addmm                  32.32%           543.135us        32.69%           549.362us        549.362us\n    mse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\n    mean                   0.22%            3.694us          2.05%            34.523us         34.523us\n    div_                   0.64%            10.756us         1.90%            32.001us         16.000us\n    ones_like              0.21%            3.461us          0.81%            13.669us         13.669us\n    sum_out                0.45%            7.638us          0.74%            12.432us         12.432us\n    transpose              0.23%            3.786us          0.68%            11.393us         11.393us\n    as_strided             0.60%            10.060us         0.60%            10.060us         3.353us\n    to                     0.18%            3.059us          0.44%            7.464us          7.464us\n    empty_like             0.14%            2.387us          0.41%            6.859us          6.859us\n    empty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\n    fill_                  0.28%            4.782us          0.33%            5.566us          2.783us\n    expand                 0.20%            3.336us          0.28%            4.743us          4.743us\n    empty                  0.27%            4.456us          0.27%            4.456us          2.228us\n    copy_                  0.15%            2.526us          0.15%            2.526us          2.526us\n    broadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\n    size                   0.06%            0.967us          0.06%            0.967us          0.484us\n    is_complex             0.06%            0.961us          0.06%            0.961us          0.481us\n    stride                 0.03%            0.517us          0.03%            0.517us          0.517us\n    ---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n    Self CPU time total: 1.681ms\n\nThis profiler will record ``training_step``, ``validation_step``, ``test_step``, and ``predict_step``.\nThe output above shows the profiling for the action ``training_step``.\n\n.. note::\n    When using the PyTorch Profiler, wall clock time will not be representative of the true wall clock time.\n    This is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously.\n    It is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use\n    the ``SimpleProfiler``.\n\n----\n\n*****************************\nVisualize profiled operations\n*****************************\nTo visualize the profiled operations, enable **emit_nvtx** in the :class:`~lightning.pytorch.profilers.pytorch.PyTorchProfiler`.\n\n.. code-block:: python\n\n    from lightning.pytorch.profilers import PyTorchProfiler\n\n    profiler = PyTorchProfiler(emit_nvtx=True)\n    trainer = Trainer(profiler=profiler)\n\nThen run as following:\n\n.. code-block::\n\n    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n\nTo visualize the profiled operation, you can either use **nvvp**:\n\n.. code-block::\n\n    nvvp trace_name.prof\n\nor python:\n\n.. code-block::\n\n    python -c 'import torch; print(torch.autograd.profiler.load_nvprof(\"trace_name.prof\"))'\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/experiment_managers.html", "url_rel_html": "visualize/experiment_managers.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/experiment_managers.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Manage Experiments¶", "rst_text": "******************\nManage Experiments\n******************\nTo track other artifacts, such as histograms or model topology graphs first select one of the many experiment managers (*loggers*) supported by Lightning\n\n.. code-block:: python\n\n    from lightning.pytorch import loggers as pl_loggers\n\n    tensorboard = pl_loggers.TensorBoardLogger()\n    trainer = Trainer(logger=tensorboard)\n\nthen access the logger's API directly\n\n.. code-block:: python\n\n    def training_step(self):\n        tensorboard = self.logger.experiment\n        tensorboard.add_image()\n        tensorboard.add_histogram(...)\n        tensorboard.add_figure(...)\n\n----\n\n.. include:: supported_exp_managers.rst\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/loggers.html", "url_rel_html": "visualize/loggers.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/loggers.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Track and Visualize Experiments¶", "rst_text": ".. _loggers:\n\n###############################\nTrack and Visualize Experiments\n###############################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Basic\n   :description: Learn how to track and visualize metrics, images and text.\n   :col_css: col-md-4\n   :button_link: logging_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Enable third-party experiment managers with advanced visualizations.\n   :col_css: col-md-4\n   :button_link: logging_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Optimize model speed with advanced self.log arguments and cloud logging.\n   :col_css: col-md-4\n   :button_link: logging_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Expert\n   :description: Make your own progress-bar or integrate a new experiment manager.\n   :col_css: col-md-4\n   :button_link: logging_expert.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: LightningModule.log API\n   :description: Dig into the LightningModule.log API in depth\n   :col_css: col-md-4\n   :button_link: ../common/lightning_module.html#log\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/logging_advanced.html", "url_rel_html": "visualize/logging_advanced.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/logging_advanced.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Track and Visualize Experiments (advanced)¶", "rst_text": ":orphan:\n\n.. _logging_advanced:\n\n##########################################\nTrack and Visualize Experiments (advanced)\n##########################################\n**Audience:** Users who want to do advanced speed optimizations by customizing the logging behavior.\n\n----\n\n****************************\nChange progress bar defaults\n****************************\nTo change the default values (ie: version number) shown in the progress bar, override the :meth:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar.get_metrics` method in your logger.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks.progress import Tqdm\n\n\n    class CustomProgressBar(Tqdm):\n        def get_metrics(self, *args, **kwargs):\n            # don't show the version number\n            items = super().get_metrics()\n            items.pop(\"v_num\", None)\n            return items\n\n----\n\n************************************\nCustomize tracking to speed up model\n************************************\n\n\nModify logging frequency\n========================\n\nLogging a metric on every single batch can slow down training. By default, Lightning logs every 50 rows, or 50 training steps.\nTo change this behaviour, set the *log_every_n_steps* :class:`~lightning.pytorch.trainer.trainer.Trainer` flag.\n\n.. testcode::\n\n   k = 10\n   trainer = Trainer(log_every_n_steps=k)\n\n----\n\nModify flushing frequency\n=========================\n\nSome loggers keep logged metrics in memory for N steps and only periodically flush them to disk to improve training efficiency.\nEvery logger handles this a bit differently. For example, here is how to fine-tune flushing for the TensorBoard logger:\n\n.. code-block:: python\n\n    # Default used by TensorBoard: Write to disk after 10 logging events or every two minutes\n    logger = TensorBoardLogger(..., max_queue=10, flush_secs=120)\n\n    # Faster training, more memory used\n    logger = TensorBoardLogger(..., max_queue=100)\n\n    # Slower training, less memory used\n    logger = TensorBoardLogger(..., max_queue=1)\n\n----\n\n******************\nCustomize self.log\n******************\n\nThe LightningModule *self.log* method offers many configurations to customize its behavior.\n\n----\n\nadd_dataloader_idx\n==================\n**Default:** True\n\nIf True, appends the index of the current dataloader to the name (when using multiple dataloaders). If False, user needs to give unique names for each dataloader to not mix the values.\n\n.. code-block:: python\n\n  self.log(add_dataloader_idx=True)\n\n----\n\nbatch_size\n==========\n**Default:** None\n\nCurrent batch size used for accumulating logs logged with ``on_epoch=True``. This will be directly inferred from the loaded batch, but for some data structures you might need to explicitly provide it.\n\n.. code-block:: python\n\n  self.log(batch_size=32)\n\n----\n\nenable_graph\n============\n**Default:** True\n\nIf True, will not auto detach the graph.\n\n.. code-block:: python\n\n  self.log(enable_graph=True)\n\n----\n\nlogger\n======\n**Default:** True\n\nSend logs to the logger like ``Tensorboard``, or any other custom logger passed to the :class:`~lightning.pytorch.trainer.trainer.Trainer` (Default: ``True``).\n\n.. code-block:: python\n\n  self.log(logger=True)\n\n----\n\non_epoch\n========\n**Default:** It varies\n\nIf this is True, that specific *self.log* call accumulates and reduces all metrics to the end of the epoch.\n\n.. code-block:: python\n\n  self.log(on_epoch=True)\n\nThe default value depends in which function this is called\n\n.. code-block:: python\n\n  def training_step(self, batch, batch_idx):\n      # Default: False\n      self.log(on_epoch=False)\n\n\n  def validation_step(self, batch, batch_idx):\n      # Default: True\n      self.log(on_epoch=True)\n\n\n  def test_step(self, batch, batch_idx):\n      # Default: True\n      self.log(on_epoch=True)\n\n----\n\non_step\n=======\n**Default:** It varies\n\nIf this is True, that specific *self.log* call will NOT accumulate metrics. Instead it will generate a timeseries across steps.\n\n.. code-block:: python\n\n  self.log(on_step=True)\n\nThe default value depends in which function this is called\n\n.. code-block:: python\n\n  def training_step(self, batch, batch_idx):\n      # Default: True\n      self.log(on_step=True)\n\n\n  def validation_step(self, batch, batch_idx):\n      # Default: False\n      self.log(on_step=False)\n\n\n  def test_step(self, batch, batch_idx):\n      # Default: False\n      self.log(on_step=False)\n\n\n----\n\nprog_bar\n========\n**Default:** False\n\nIf set to True, logs will be sent to the progress bar.\n\n.. code-block:: python\n\n  self.log(prog_bar=True)\n\n----\n\nrank_zero_only\n==============\n**Default:** False\n\nTells Lightning if you are calling ``self.log`` from every process (default) or only from rank 0.\nThis is for advanced users who want to reduce their metric manually across processes, but still want to benefit from automatic logging via ``self.log``.\n\n- Set ``False`` (default) if you are calling ``self.log`` from every process.\n- Set ``True`` if you are calling ``self.log`` from rank 0 only. Caveat: you won't be able to use this metric as a monitor in callbacks (e.g., early stopping).\n\n.. code-block:: python\n\n    # Default\n    self.log(..., rank_zero_only=False)\n\n    # If you call `self.log` on rank 0 only, you need to set `rank_zero_only=True`\n    if self.trainer.global_rank == 0:\n        self.log(..., rank_zero_only=True)\n\n    # DON'T do this, it will cause deadlocks!\n    self.log(..., rank_zero_only=True)\n\n\n----\n\nreduce_fx\n=========\n**Default:** :func:`torch.mean`\n\nReduction function over step values for end of epoch. Uses :func:`torch.mean` by default and is not applied when a :class:`torchmetrics.Metric` is logged.\n\n.. code-block:: python\n\n  self.log(..., reduce_fx=torch.mean)\n\n----\n\nsync_dist\n=========\n**Default:** False\n\nIf True, reduces the metric across devices. Use with care as this may lead to a significant communication overhead.\n\n.. code-block:: python\n\n  self.log(sync_dist=False)\n\n----\n\nsync_dist_group\n===============\n**Default:** None\n\nThe DDP group to sync across.\n\n.. code-block:: python\n\n  import torch.distributed as dist\n\n  group = dist.init_process_group(\"nccl\", rank=self.global_rank, world_size=self.world_size)\n  self.log(sync_dist_group=group)\n\n----\n\n***************************************\nEnable metrics for distributed training\n***************************************\nFor certain types of metrics that need complex aggregation, we recommended to build your metric using torchmetric which ensures all the complexities of metric aggregation in distributed environments is handled.\n\nFirst, implement your metric:\n\n.. code-block:: python\n\n  import torch\n  import torchmetrics\n\n\n  class MyAccuracy(Metric):\n      def __init__(self, dist_sync_on_step=False):\n          # call `self.add_state`for every internal state that is needed for the metrics computations\n          # dist_reduce_fx indicates the function that should be used to reduce\n          # state from multiple processes\n          super().__init__(dist_sync_on_step=dist_sync_on_step)\n\n          self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n          self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n      def update(self, preds: torch.Tensor, target: torch.Tensor):\n          # update metric states\n          preds, target = self._input_format(preds, target)\n          assert preds.shape == target.shape\n\n          self.correct += torch.sum(preds == target)\n          self.total += target.numel()\n\n      def compute(self):\n          # compute final result\n          return self.correct.float() / self.total\n\nTo use the metric inside Lightning, 1) initialize it in the init, 2) compute the metric, 3) pass it into *self.log*\n\n.. code-block:: python\n\n  class LitModel(LightningModule):\n      def __init__(self):\n          # 1. initialize the metric\n          self.accuracy = MyAccuracy()\n\n      def training_step(self, batch, batch_idx):\n          x, y = batch\n          preds = self(x)\n\n          # 2. compute the metric\n          self.accuracy(preds, y)\n\n          # 3. log it\n          self.log(\"train_acc_step\", self.accuracy)\n\n----\n\n********************************\nLog to a custom cloud filesystem\n********************************\nLightning is integrated with the major remote file systems including local filesystems and several cloud storage providers such as\n`S3 <https://aws.amazon.com/s3/>`_ on `AWS <https://aws.amazon.com/>`_, `GCS <https://cloud.google.com/storage>`_ on `Google Cloud <https://cloud.google.com/>`_,\nor `ADL <https://azure.microsoft.com/solutions/data-lake/>`_ on `Azure <https://azure.microsoft.com/>`_.\n\nPyTorch Lightning uses `fsspec <https://filesystem-spec.readthedocs.io/>`_ internally to handle all filesystem operations.\n\nTo save logs to a remote filesystem, prepend a protocol like \"s3:/\" to the root_dir used for writing and reading model data.\n\n.. code-block:: python\n\n    from lightning.pytorch.loggers import TensorBoardLogger\n\n    logger = TensorBoardLogger(save_dir=\"s3://my_bucket/logs/\")\n\n    trainer = Trainer(logger=logger)\n    trainer.fit(model)\n\n----\n\n*********************************\nTrack both step and epoch metrics\n*********************************\nTo track the timeseries over steps (*on_step*) as well as the accumulated epoch metric (*on_epoch*), set both to True\n\n.. code-block:: python\n\n  self.log(on_step=True, on_epoch=True)\n\nSetting both to True will generate two graphs with *_step* for the timeseries over steps and *_epoch* for the epoch metric.\n\n.. TODO:: show images of both\n\n----\n\n**************************************\nUnderstand self.log automatic behavior\n**************************************\nThis table shows the default values of *on_step* and *on_epoch* depending on the *LightningModule* or *Callback* method.\n\n----\n\nIn LightningModule\n==================\n\n.. list-table:: Default behavior of logging in ightningModule\n   :widths: 50 25 25\n   :header-rows: 1\n\n   * - Method\n     - on_step\n     - on_epoch\n   * - on_after_backward, on_before_backward, on_before_optimizer_step, optimizer_step, configure_gradient_clipping, on_before_zero_grad, training_step\n     - True\n     - False\n   * - test_step, validation_step\n     - False\n     - True\n\n----\n\nIn Callback\n===========\n\n.. list-table:: Default behavior of logging in Callback\n   :widths: 50 25 25\n   :header-rows: 1\n\n   * - Method\n     - on_step\n     - on_epoch\n   * - on_after_backward, on_before_backward, on_before_optimizer_step, on_before_zero_grad, on_train_batch_start, on_train_batch_end\n     - True\n     - False\n   * - on_train_epoch_start, on_train_epoch_end, on_train_start, on_validation_batch_start, on_validation_batch_end, on_validation_start, on_validation_epoch_start, on_validation_epoch_end\n     - False\n     - True\n\n.. note:: To add logging to an unsupported method, please open an issue with a clear description of why it is blocking you.\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/logging_basic.html", "url_rel_html": "visualize/logging_basic.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/logging_basic.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Track and Visualize Experiments (basic)¶", "rst_text": ":orphan:\n\n.. _logging_basic:\n\n#######################################\nTrack and Visualize Experiments (basic)\n#######################################\n**Audience:** Users who want to visualize and monitor their model development\n\n----\n\n*******************************\nWhy do I need to track metrics?\n*******************************\nIn model development, we track values of interest such as the *validation_loss* to visualize the learning process for our models. Model development is like driving a car without windows, charts and logs provide the *windows* to know where to drive the car.\n\nWith Lightning, you can visualize virtually anything you can think of: numbers, text, images, audio. Your creativity and imagination are the only limiting factor.\n\n----\n\n*************\nTrack metrics\n*************\nMetric visualization is the most basic but powerful way of understanding how your model is doing throughout the model development process.\n\nTo track a metric, simply use the *self.log* method available inside the *LightningModule*\n\n.. code-block:: python\n\n    class LitModel(L.LightningModule):\n        def training_step(self, batch, batch_idx):\n            value = ...\n            self.log(\"some_value\", value)\n\nTo log multiple metrics at once, use *self.log_dict*\n\n.. code-block:: python\n\n    values = {\"loss\": loss, \"acc\": acc, \"metric_n\": metric_n}  # add more items if needed\n    self.log_dict(values)\n\n.. TODO:: show plot of metric changing over time\n\n----\n\nView in the commandline\n=======================\n\nTo view metrics in the commandline progress bar, set the *prog_bar* argument to True.\n\n.. code-block:: python\n\n    self.log(..., prog_bar=True)\n\n\n.. code-block:: bash\n\n    Epoch 3:  33%|███▉        | 307/938 [00:01<00:02, 289.04it/s, loss=0.198, v_num=51, acc=0.211, metric_n=0.937]\n\n----\n\nView in the browser\n===================\nTo view metrics in the browser you need to use an *experiment manager* with these capabilities.\n\nBy Default, Lightning uses Tensorboard (if available) and a simple CSV logger otherwise.\n\n.. code-block:: python\n\n    # every trainer already has tensorboard enabled by default (if the dependency is available)\n    trainer = Trainer()\n\nTo launch the tensorboard dashboard run the following command on the commandline.\n\n.. code-block:: bash\n\n    tensorboard --logdir=lightning_logs/\n\nIf you're using a notebook environment such as *colab* or *kaggle* or *jupyter*, launch Tensorboard with this command\n\n.. code-block:: bash\n\n    %reload_ext tensorboard\n    %tensorboard --logdir=lightning_logs/\n\n----\n\nAccumulate a metric\n===================\nWhen *self.log* is called inside the *training_step*, it generates a timeseries showing how the metric behaves over time.\n\n.. figure:: https://pl-public-data.s3.amazonaws.com/assets_lightning/logging_basic/visualize_logging_basic_tensorboard_chart.png\n    :alt: TensorBoard chart of a metric logged with self.log\n    :width: 100 %\n\nHowever, For the validation and test sets we are not generally interested in plotting the metric values per batch of data. Instead, we want to compute a summary statistic (such as average, min or max) across the full split of data.\n\nWhen you call self.log inside the *validation_step* and *test_step*, Lightning automatically accumulates the metric and averages it once it's gone through the whole split (*epoch*).\n\n.. code-block:: python\n\n    def validation_step(self, batch, batch_idx):\n        value = batch_idx + 1\n        self.log(\"average_value\", value)\n\n.. figure:: https://pl-public-data.s3.amazonaws.com/assets_lightning/logging_basic/visualize_logging_basic_tensorboard_point.png\n    :alt: TensorBoard chart of a metric logged with self.log\n    :width: 100 %\n\nIf you don't want to average you can also choose from ``{min,max,sum}`` by passing the *reduce_fx* argument.\n\n.. code-block:: python\n\n    # default function\n    self.log(..., reduce_fx=\"mean\")\n\nFor other reductions, we recommend logging a :class:`torchmetrics.Metric` instance instead.\n\n----\n\n******************************\nConfigure the saving directory\n******************************\nBy default, anything that is logged is saved to the current working directory. To use a different directory, set the *default_root_dir* argument in the Trainer.\n\n.. code-block:: python\n\n    Trainer(default_root_dir=\"/your/custom/path\")\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/logging_expert.html", "url_rel_html": "visualize/logging_expert.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/logging_expert.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Track and Visualize Experiments (expert)¶", "rst_text": ":orphan:\n\n.. _logging_expert:\n\n########################################\nTrack and Visualize Experiments (expert)\n########################################\n**Audience:** Users who want to make their own progress bars or integrate new experiment managers.\n\n----\n\n***********************\nChange the progress bar\n***********************\n\nIf you'd like to change the way the progress bar displays information you can use some of our built-in progress bard or build your own.\n\n----\n\nUse the TQDMProgressBar\n=======================\nTo use the TQDMProgressBar pass it into the *callbacks* :class:`~lightning.pytorch.trainer.trainer.Trainer` argument.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import TQDMProgressBar\n\n    trainer = Trainer(callbacks=[TQDMProgressBar()])\n\n----\n\nUse the RichProgressBar\n=======================\nThe RichProgressBar can add custom colors and beautiful formatting for your progress bars. First, install the *`rich <https://github.com/Textualize/rich>`_*  library\n\n.. code-block:: bash\n\n    pip install rich\n\nThen pass the callback into the callbacks :class:`~lightning.pytorch.trainer.trainer.Trainer` argument:\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import RichProgressBar\n\n    trainer = Trainer(callbacks=[RichProgressBar()])\n\nThe rich progress bar can also have custom themes\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n    # create your own theme!\n    theme = RichProgressBarTheme(description=\"green_yellow\", progress_bar=\"green1\")\n\n    # init as normal\n    progress_bar = RichProgressBar(theme=theme)\n    trainer = Trainer(callbacks=progress_bar)\n\n----\n\n************************\nCustomize a progress bar\n************************\nTo customize either the  :class:`~lightning.pytorch.callbacks.TQDMProgressBar` or the  :class:`~lightning.pytorch.callbacks.RichProgressBar`, subclass it and override any of its methods.\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import TQDMProgressBar\n\n\n    class LitProgressBar(TQDMProgressBar):\n        def init_validation_tqdm(self):\n            bar = super().init_validation_tqdm()\n            bar.set_description(\"running validation...\")\n            return bar\n\n----\n\n***************************\nBuild your own progress bar\n***************************\nTo build your own progress bar, subclass :class:`~lightning.pytorch.callbacks.ProgressBar`\n\n.. code-block:: python\n\n    from lightning.pytorch.callbacks import ProgressBar\n\n\n    class LitProgressBar(ProgressBar):\n        def __init__(self):\n            super().__init__()  # don't forget this :)\n            self.enable = True\n\n        def disable(self):\n            self.enable = False\n\n        def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):\n            super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)\n            percent = (self.train_batch_idx / self.total_train_batches) * 100\n            sys.stdout.flush()\n            sys.stdout.write(f\"{percent:.01f} percent complete \\r\")\n\n\n    bar = LitProgressBar()\n    trainer = Trainer(callbacks=[bar])\n\n----\n\n*******************************\nIntegrate an experiment manager\n*******************************\nTo create an integration between a custom logger and Lightning, subclass :class:`~lightning.pytorch.loggers.Logger`\n\n.. code-block:: python\n\n    from lightning.pytorch.loggers import Logger\n\n\n    class LitLogger(Logger):\n        @property\n        def name(self) -> str:\n            return \"my-experiment\"\n\n        @property\n        def version(self):\n            return \"version_0\"\n\n        def log_metrics(self, metrics, step=None):\n            print(\"my logged metrics\", metrics)\n\n        def log_hyperparams(self, params, *args, **kwargs):\n            print(\"my logged hyperparameters\", params)\n"}
{"channel": "stable", "url_html": "https://lightning.ai/docs/pytorch/stable/visualize/logging_intermediate.html", "url_rel_html": "visualize/logging_intermediate.html", "url_rst": "https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/refs/heads/master/docs/source-pytorch/visualize/logging_intermediate.rst", "url_sources_txt": null, "source_used": "github_raw", "status": "ok", "section_title": "Track and Visualize Experiments (intermediate)¶", "rst_text": ".. _logging_intermediate:\n\n##############################################\nTrack and Visualize Experiments (intermediate)\n##############################################\n**Audience:** Users who want to track more complex outputs and use third-party experiment managers.\n\n----\n\n*******************************\nTrack audio and other artifacts\n*******************************\nTo track other artifacts, such as histograms or model topology graphs first select one of the many loggers supported by Lightning\n\n.. code-block:: python\n\n    from lightning.pytorch import loggers as pl_loggers\n\n    tensorboard = pl_loggers.TensorBoardLogger(save_dir=\"\")\n    trainer = Trainer(logger=tensorboard)\n\nthen access the logger's API directly\n\n.. code-block:: python\n\n    def training_step(self):\n        tensorboard = self.logger.experiment\n        tensorboard.add_image()\n        tensorboard.add_histogram(...)\n        tensorboard.add_figure(...)\n\n----\n\n.. include:: supported_exp_managers.rst\n\n----\n\n*********************\nTrack hyperparameters\n*********************\nTo track hyperparameters, first call *save_hyperparameters* from the LightningModule init:\n\n.. code-block:: python\n\n    class MyLightningModule(LightningModule):\n        def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n            super().__init__()\n            self.save_hyperparameters()\n\nIf your logger supports tracked hyperparameters, the hyperparameters will automatically show up on the logger dashboard.\n\n.. TODO:: show tracked hyperparameters.\n\n----\n\n********************\nTrack model topology\n********************\nMultiple loggers support visualizing the model topology. Here's an example that tracks the model topology using Tensorboard.\n\n.. code-block:: python\n\n    def any_lightning_module_function_or_hook(self):\n        tensorboard_logger = self.logger\n\n        prototype_array = torch.Tensor(32, 1, 28, 27)\n        tensorboard_logger.log_graph(model=self, input_array=prototype_array)\n\n.. TODO:: show tensorboard topology.\n"}
