{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47700563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 2240\n"
     ]
    }
   ],
   "source": [
    "# put all json files into one list\n",
    "import json\n",
    "import os\n",
    "doc_links = []\n",
    "doc_dir = \"../data/final_data\"\n",
    "doc_jsons = []\n",
    "# handle documentation json\n",
    "with open(os.path.join(doc_dir, \"lightning_docs_cleaned.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_jsons.append(data)\n",
    "    doc_links += [d[\"url_html\"] for d in data]\n",
    "    doc_docs = [f\"{d['title']}\\n{d['text']}\" for d in data]\n",
    "\n",
    "# handle discussions json\n",
    "with open(os.path.join(doc_dir, \"discussions.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"url\"] for d in data]\n",
    "    disc_docs = [f\"{d['title']}\\n {d['bodyText']} \\n Answer: {d[\"answer\"][\"bodyText\"]}\" for d in data]\n",
    "\n",
    "# handle src code json\n",
    "with open(os.path.join(doc_dir, \"src_filtered_data.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    doc_links += [d[\"file\"] for d in data]\n",
    "    src_docs = [f\"{d['text']}\" for d in data]\n",
    "\n",
    "\n",
    "documents = doc_docs + disc_docs + src_docs\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a516494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkafle/291a_project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 70/70 [00:12<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# embed all documents\n",
    "embeddings = model.encode(documents, convert_to_numpy=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefefcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2240 documents.\n"
     ]
    }
   ],
   "source": [
    "d = embeddings.shape[1]  # embedding dimension\n",
    "index = faiss.IndexFlatL2(d)  # simple L2 distance index\n",
    "index.add(embeddings)\n",
    "print(f\"Indexed {index.ntotal} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c972dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../requests/richa_requests.json\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "query_texts = [q[\"query\"] for q in queries]\n",
    "\n",
    "query_embeddings = model.encode(query_texts, convert_to_numpy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ecc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Training is taking a long time. How do I speed up the training for multiple datasets?\n",
      "  Rank 1: Multiple Sequential trainings slows down speed\n",
      " Hi.\n",
      "I have a task where I need to run a training script multiple time with a for-loop like this:\n",
      "for d...\n",
      "  Rank 2: Find bottlenecks in your code (basic)\n",
      "## Find bottlenecks in your code (basic)\n",
      "**Audience**: Users who want to learn the basics of removing bottleneck...\n",
      "  Rank 3: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\trainer\\trainer.py\n",
      "Function Name: estimated_stepping_batches\n",
      "Language: python\n",
      "Pa...\n",
      "  Rank 4: Training seems to pause every N steps\n",
      " I am doing feature extraction using an efficientnet_b0 model. The training process works fine but it seems to p...\n",
      "  Rank 5: Debug your model (intermediate)\n",
      "## Debug your model (intermediate)\n",
      "**Audience**: Users who want to debug their ML code\n",
      "\n",
      "----\n",
      "\n",
      "### Why should I debug M...\n",
      "\n",
      "Query: What do I need to pass into LightningCLI() to get it working? Also, please give an example.\n",
      "  Rank 1: Configure hyperparameters from the CLI (Intermediate)\n",
      "## Configure hyperparameters from the CLI (Intermediate)\n",
      "**Audience:** Users who want advanced m...\n",
      "  Rank 2: Configure hyperparameters from the CLI (Expert)\n",
      "## Configure hyperparameters from the CLI (Expert)\n",
      "**Audience:** Users who already understand the Ligh...\n",
      "  Rank 3: Lightning in 15 minutes\n",
      "## Lightning in 15 minutes\n",
      "**Required background:** None\n",
      "\n",
      "**Goal:** In this guide, we'll walk you through the 7 key steps of a...\n",
      "  Rank 4: ValueError(f\"{func} does not have a return type annotation\") when implementing LightningCLI\n",
      " Dear All,\n",
      "I am trying to implement the LightningCLI class...\n",
      "  Rank 5: Configure hyperparameters from the CLI (Advanced)\n",
      "## Configure hyperparameters from the CLI (Advanced)\n",
      "\n",
      "### Instantiation only mode\n",
      "\n",
      "The CLI is design...\n",
      "\n",
      "Query: For defining a training step function, when should I use manual backward and what issues can arrise from using it?\n",
      "  Rank 1: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\n",
      "Function Name: _make_backward_fn\n",
      "Language: pytho...\n",
      "  Rank 2: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\n",
      "Function Name: _make_backward_fn\n",
      "Language: pytho...\n",
      "  Rank 3: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\n",
      "Function Name: _make_backward_fn\n",
      "Language: pytho...\n",
      "  Rank 4: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\core\\module.py\n",
      "Function Name: backward\n",
      "Language: python\n",
      "Partition: train\n",
      "\n",
      "--- Do...\n",
      "  Rank 5: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\core\\module.py\n",
      "Function Name: backward\n",
      "Language: python\n",
      "Partition: train\n",
      "\n",
      "--- Do...\n",
      "\n",
      "Query: init_meta_context() isn't available. What are some alternitives that have similar functionality?\n",
      "  Rank 1: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\strategies\\strategy.py\n",
      "Function Name: model_sharded_context\n",
      "Language: python\n",
      "Par...\n",
      "  Rank 2: Current best practices to initialize massive (50B parameter+) models\n",
      " Hi, I am working with GPT-style models and need to intitialize a model at the GP...\n",
      "  Rank 3: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\strategies\\strategy.py\n",
      "Function Name: _setup_optimizer\n",
      "Language: python\n",
      "Partitio...\n",
      "  Rank 4: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\utilities\\parsing.py\n",
      "Function Name: get_init_args\n",
      "Language: python\n",
      "Partition: te...\n",
      "  Rank 5: --- Meta Data ---\n",
      "Repo: pytorch-lightning\n",
      "Path: src\\lightning\\pytorch\\utilities\\parsing.py\n",
      "Function Name: get_init_args\n",
      "Language: python\n",
      "Partition: te...\n"
     ]
    }
   ],
   "source": [
    "k = 5  # number of results per query\n",
    "D, I = index.search(query_embeddings, k)  # D = distances, I = indices\n",
    "for q_idx, query in enumerate(query_texts):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for rank, doc_idx in enumerate(I[q_idx]):\n",
    "        print(f\"  Rank {rank+1}: {documents[doc_idx][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(I, ground_truth, k):\n",
    "    recalls = []\n",
    "    for q_idx, retrieved in enumerate(I):\n",
    "        gt = set(ground_truth[q_idx])\n",
    "        hit = any(doc in gt for doc in retrieved[:k])\n",
    "        recalls.append(hit)\n",
    "    return sum(recalls)/len(recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for q_idx, query in enumerate(query_texts):\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"top_docs\": [documents[i] for i in I[q_idx].tolist()],\n",
    "        \"doc_ids\": I[q_idx].tolist(),\n",
    "        \"doc_links\": [doc_links[i]]\n",
    "        \"distances\": D[q_idx].tolist()\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"retrieval_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
