[
  {
    "query": "Training is taking a long time. How do I speed up the training for multiple datasets?",
    "top_docs": [
      "Multiple Sequential trainings slows down speed\n Hi.\nI have a task where I need to run a training script multiple time with a for-loop like this:\nfor dataset in datasets:\n      ... Training script with datamodule, model, Trainer Init and trainer.fit()\n\nHowever, after each loop the the training it self slows down incrementally (epoch / sec). I am thinking it is because I need to reset something but I have not been able to find that information. I am using Torch-cpu 2.0.0\nAny idea on what I am doing wrong? :( \n Answer: I tried to remove the neptune logger and model saving functionality and it seemed to have solved the issue",
      "Find bottlenecks in your code (basic)\n## Find bottlenecks in your code (basic)\n**Audience**: Users who want to learn the basics of removing bottlenecks from their code\n\n----\n\n### Why do I need profiling?\nProfiling helps you find bottlenecks in your code by capturing analytics such as how long a function takes or how much memory is used.\n\n------------\n\n### Find training loop bottlenecks\nThe most basic profile measures all the key methods across **Callbacks**, **DataModules** and the **LightningModule** in the training loop.\n\n```python\ntrainer = Trainer(profiler=\"simple\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nFIT Profiler Report\n\n-------------------------------------------------------------------------------------------\n|  Action                                          |  Mean duration (s) |  Total time (s) |\n-------------------------------------------------------------------------------------------\n|  [LightningModule]BoringModel.prepare_data       |  10.0001           |  20.00          |\n|  run_training_epoch                              |  6.1558            |  6.1558         |\n|  run_training_batch                              |  0.0022506         |  0.015754       |\n|  [LightningModule]BoringModel.optimizer_step     |  0.0017477         |  0.012234       |\n|  [LightningModule]BoringModel.val_dataloader     |  0.00024388        |  0.00024388     |\n|  on_train_batch_start                            |  0.00014637        |  0.0010246      |\n|  [LightningModule]BoringModel.teardown           |  2.15e-06          |  2.15e-06       |\n|  [LightningModule]BoringModel.on_train_start     |  1.644e-06         |  1.644e-06      |\n|  [LightningModule]BoringModel.on_train_end       |  1.516e-06         |  1.516e-06      |\n|  [LightningModule]BoringModel.on_fit_end         |  1.426e-06         |  1.426e-06      |\n|  [LightningModule]BoringModel.setup              |  1.403e-06         |  1.403e-06      |\n|  [LightningModule]BoringModel.on_fit_start       |  1.226e-06         |  1.226e-06      |\n-------------------------------------------------------------------------------------------\n```\nIn this report we can see that the slowest function is **prepare_data**. Now you can figure out why data preparation is slowing down your training.\n\nThe simple profiler measures all the standard methods used in the training loop automatically, including:\n\n- on_train_epoch_start\n- on_train_epoch_end\n- on_train_batch_start\n- model_backward\n- on_after_backward\n- optimizer_step\n- on_train_batch_end\n- on_training_end\n- etc...\n\n----\n\n### Profile the time within every function\nTo profile the time within every function, use the ~lightning.pytorch.profilers.advanced.AdvancedProfiler built on top of Python's cProfiler.\n\n```python\ntrainer = Trainer(profiler=\"advanced\")\n```\nOnce the **.fit()** function has completed, you'll see an output like this:\n\n```\nProfiler Report\n\nProfile stats for: get_train_batch\n        4869394 function calls (4863767 primitive calls) in 18.893 seconds\nOrdered by: cumulative time\nList reduced from 76 to 10 due to restriction <10>\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n    1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n    1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n    1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n    1875     0.084    0.000   18.290    0.010 fetch.py:44(<listcomp>)\n    60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n    60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n    60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n    60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n    60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n```\nIf the profiler report becomes too long, you can stream the report to a file:\n\n```python\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nprofiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\ntrainer = Trainer(profiler=profiler)\n```\n----\n\n### Measure accelerator usage\nAnother helpful technique to detect bottlenecks is to ensure that you're using the full capacity of your accelerator (GPU/TPU/HPU).\nThis can be measured with the ~lightning.pytorch.callbacks.device_stats_monitor.DeviceStatsMonitor:\n\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\n\ntrainer = Trainer(callbacks=[DeviceStatsMonitor()])\nCPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set DeviceStatsMonitor(cpu_stats=True). To disable logging\nCPU metrics, you can specify DeviceStatsMonitor(cpu_stats=False).\n\n**Do not wrap** Trainer.fit(), Trainer.validate(), or other Trainer methods inside a manual\ntorch.profiler.profile context manager. This will cause unexpected crashes and cryptic errors due to\nincompatibility between PyTorch Profiler's context management and Lightning's internal training loop.\nInstead, always use the profiler argument in the Trainer constructor or the\n~lightning.pytorch.profilers.pytorch.PyTorchProfiler profiler class if you want to customize the profiling.\n\nExample:\n\n.. code-block:: python\n\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.profilers import PytorchProfiler\n\ntrainer = Trainer(profiler=\"pytorch\")\n# or\ntrainer = Trainer(profiler=PytorchProfiler(dirpath=\".\", filename=\"perf_logs\"))",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\trainer\\trainer.py\nFunction Name: estimated_stepping_batches\nLanguage: python\nPartition: train\n\n--- Docstring ---\ninfinite training iterable dataset\n\n--- Code ---\ndef estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps\n\n--- Original String ---\ndef estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps",
      "Training seems to pause every N steps\n I am doing feature extraction using an efficientnet_b0 model. The training process works fine but it seems to pause every once in a while. I verified this using nvidia-smi dmon. There are spikes of a few seconds where the GPU utilization is anywhere between 50% and 100%, followed by a few seconds where the GPU utilization is 0%.\nRight now I am training with 4 Tesla T4, but I verified the same issue with a single GPU (T4 and V100).\nI am using a batch size of 200 (per GPU).  I have 48 CPUs and their usage is pretty low (I'd say 20-40%).\nI noticed the training pausing at epoch 48, 96, 144,... So it pauses every 48 steps.\nI thought that the pause were caused by logging so in my Trainer I set  log_every_n_steps=500 and I also initialize my logger with TensorBoardLogger(\"tb_logs\", name=\"vehicles\", max_queue=1000, flush_secs=120). I can that the processes pauses more frequently than 120 seconds.\nOriginally, I thought it was a PyTorch \"issue\". So I opened a post here https://discuss.pytorch.org/t/gpu-usage-is-not-constant-during-training/154718 . However I am wondering whether this could be caused by torch lightning.\nThank you \n Answer: I analyzed the problem a little bit more. I noticed that I have 48 CPUs and 48 workers. That makes the training process pausing every 48 steps. If use 12 workers, the pause happens every 12 steps.\nI'd like to increase the number of workers but the RAM usage is crazy high. With 48 workers I am almost using all the 180Gb of RAM available. Is this normal for simply loading images of a few Kbytes?\nAny suggestion on how to speed this up?\nEDIT: I think I am facing this issue pytorch/pytorch#13246 (comment) even though I am not entirely sure. My memory consumption is of about 100-150 gb right after the training starts. I tried to used a numpy array to store the huge list of integers containing the IDs of the record in the dataset. However, this didn't reduce the RAM usage.\nSuppose my dataset has a property myobject of type MyObject, and that myobject internally references a list of integers. Should I convert this list of integers to a numpy array too?",
      "Debug your model (intermediate)\n## Debug your model (intermediate)\n**Audience**: Users who want to debug their ML code\n\n----\n\n### Why should I debug ML code?\nMachine learning code requires debugging mathematical correctness, which is not something non-ML code has to deal with. Lightning implements a few best-practice techniques to give all users, expert level ML debugging abilities.\n\n----\n\n### Overfit your model on a Subset of Data\n\nA good debugging technique is to take a tiny portion of your data (say 2 samples per class),\nand try to get your model to overfit. If it can't, it's a sign it won't work with large datasets.\n\n(See: ~lightning.pytorch.trainer.trainer.Trainer.overfit_batches\nargument of ~lightning.pytorch.trainer.trainer.Trainer)\n\n# use only 1% of training data\ntrainer = Trainer(overfit_batches=0.01)\n\n# similar, but with a fixed 10 batches\ntrainer = Trainer(overfit_batches=10)\n\n# equivalent to\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=10)\nSetting overfit_batches is the same as setting limit_train_batches and limit_val_batches to the same value, but in addition will also turn off shuffling in the training dataloader.\n\n----\n\n### Look-out for exploding gradients\nOne major problem that plagues models is exploding gradients.\nGradient clipping is one technique that can help keep gradients from exploding.\n\nYou can keep an eye on the gradient norm by logging it in your LightningModule:\n\n```python\nfrom lightning.pytorch.utilities import grad_norm\n\ndef on_before_optimizer_step(self, optimizer):\n    # Compute the 2-norm for each layer\n    # If using mixed precision, the gradients are already unscaled here\n    norms = grad_norm(self.layer, norm_type=2)\n    self.log_dict(norms)\n```\nThis will plot the 2-norm of each layer to your experiment manager.\nIf you notice the norm is going up, there's a good chance your gradients will explode.\n\nOne technique to stop exploding gradients is to clip the gradient when the norm is above a certain threshold:\n\n# DEFAULT (ie: don't clip)\ntrainer = Trainer(gradient_clip_val=0)\n\n# clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\ntrainer = Trainer(gradient_clip_val=0.5)\n\n# clip gradients' maximum magnitude to <=0.5\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n----\n\n### Detect autograd anomalies\nLightning helps you detect anomalies in the PyTorh autograd engine via PyTorch's built-in\nAnomaly Detection Context-manager.\n\nEnable it via the **detect_anomaly** trainer argument:\n\ntrainer = Trainer(detect_anomaly=True)"
    ],
    "distances": [
      0.873259961605072,
      1.0658683776855469,
      1.0932085514068604,
      1.1008440256118774,
      1.126258373260498
    ]
  },
  {
    "query": "What do I need to pass into LightningCLI() to get it working? Also, please give an example.",
    "top_docs": [
      "Configure hyperparameters from the CLI (Intermediate)\n## Configure hyperparameters from the CLI (Intermediate)\n**Audience:** Users who want advanced modularity via a command line interface (CLI).\n\n**Pre-reqs:** You must already understand how to use the command line and LightningDataModule <../data/datamodule>.\n\n----\n\n### LightningCLI requirements\n\nThe ~lightning.pytorch.cli.LightningCLI class is designed to significantly ease the implementation of CLIs. To\nuse this class, an additional Python requirement is necessary than the minimal installation of Lightning provides. To\nenable, either install all extras:\n\n```bash\npip install \"lightning[pytorch-extra]\"\n```\nor if only interested in LightningCLI, just install jsonargparse:\n\n```bash\npip install \"jsonargparse[signatures]\"\n```\n----\n\n### Implementing a CLI\nImplementing a CLI is as simple as instantiating a ~lightning.pytorch.cli.LightningCLI object giving as\narguments classes for a LightningModule and optionally a LightningDataModule:\n\n```python\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\n\n# simple demo classes for your convenience\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\ndef cli_main():\n    cli = LightningCLI(DemoModel, BoringDataModule)\n    # note: don't call fit!!\n\nif __name__ == \"__main__\":\n    cli_main()\n    # note: it is good practice to implement the CLI in a function and call it in the main if block\n```\nNow your model can be managed via the CLI. To see the available commands type:\n\n```bash\n$ python main.py --help\n```\nwhich prints out:\n\n```bash\nusage: main.py [-h] [-c CONFIG] [--print_config [={comments,skip_null,skip_default}+]]\n        {fit,validate,test,predict} ...\n\nLightning Trainer command line tool\n\noptional arguments:\n-h, --help            Show this help message and exit.\n-c CONFIG, --config CONFIG\n                        Path to a configuration file in json or yaml format.\n--print_config [={comments,skip_null,skip_default}+]\n                        Print configuration and exit.\n\nsubcommands:\nFor more details of each subcommand add it as argument followed by --help.\n\n{fit,validate,test,predict}\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n```\nThe message tells us that we have a few available subcommands:\n\n```bash\npython main.py [subcommand]\n```\nwhich you can use depending on your use case:\n\n```bash\n$ python main.py fit\n$ python main.py validate\n$ python main.py test\n$ python main.py predict\n```\n----\n\n### Train a model with the CLI\nTo train a model, use the fit subcommand:\n\n```bash\npython main.py fit\n```\nView all available options with the --help argument given after the subcommand:\n\n```bash\n$ python main.py fit --help\n\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n                            ...\n                            [--ckpt_path CKPT_PATH]\n    --trainer.logger LOGGER\n\noptional arguments:\n<class '__main__.DemoModel'>:\n    --model.out_dim OUT_DIM\n                            (type: int, default: 10)\n    --model.learning_rate LEARNING_RATE\n                            (type: float, default: 0.02)\n<class 'lightning.pytorch.demos.boring_classes.BoringDataModule'>:\n--data CONFIG         Path to a configuration file.\n--data.data_dir DATA_DIR\n                        (type: str, default: ./)\n```\nWith the Lightning CLI enabled, you can now change the parameters without touching your code:\n\n```bash\n# change the learning_rate\npython main.py fit --model.learning_rate 0.1\n\n# change the output dimensions also\npython main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n# change trainer and data arguments too\npython main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n```\nThe options that become available in the CLI are the __init__ parameters of the LightningModule and\nLightningDataModule classes. Thus, to make hyperparameters configurable, just add them to your class's\n__init__. It is highly recommended that these parameters are described in the docstring so that the CLI shows\nthem in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give\nunderstandable error messages when incorrect values are given.",
      "Configure hyperparameters from the CLI (Expert)\n## Configure hyperparameters from the CLI (Expert)\n**Audience:** Users who already understand the LightningCLI and want to customize it.\n\n----\n\n### Customize the LightningCLI\n\nThe init parameters of the ~lightning.pytorch.cli.LightningCLI class can be used to customize some things,\ne.g., the description of the tool, enabling parsing of environment variables, and additional arguments to instantiate\nthe trainer and configuration parser.\n\nNevertheless, the init arguments are not enough for many use cases. For this reason, the class is designed so that it\ncan be extended to customize different parts of the command line tool. The argument parser class used by\n~lightning.pytorch.cli.LightningCLI is ~lightning.pytorch.cli.LightningArgumentParser, which is an\nextension of python's argparse, thus adding arguments can be done using the add_argument method. In contrast to\nargparse, it has additional methods to add arguments. For example add_class_arguments add all arguments from the\ninit of a class. For more details, see the `respective documentation\n<https://jsonargparse.readthedocs.io/en/stable/#classes-methods-and-functions>`_.\n\nThe ~lightning.pytorch.cli.LightningCLI class has the\n~lightning.pytorch.cli.LightningCLI.add_arguments_to_parser method can be implemented to include more arguments.\nAfter parsing, the configuration is stored in the config attribute of the class instance. The\n~lightning.pytorch.cli.LightningCLI class also has two methods that can be used to run code before and after\nthe trainer runs: before_<subcommand> and after_<subcommand>. A realistic example of this would be to send an\nemail before and after the execution. The code for the fit subcommand would be something like this:\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_argument(\"--notification_email\", default=\"will@email.com\")\n\ndef before_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit starting\")\n\ndef after_fit(self):\nsend_email(address=self.config[\"notification_email\"], message=\"trainer.fit finished\")\n\ncli = MyLightningCLI(MyModel)\nNote that the config object self.config is a namespace whose keys are global options or groups of options. It has\nthe same structure as the YAML format described previously. This means that the parameters used for instantiating the\ntrainer class can be found in self.config['fit']['trainer'].\n\nHave a look at the ~lightning.pytorch.cli.LightningCLI class API reference to learn about other methods\nthat can be extended to customize a CLI.\n----\n\n### Configure forced callbacks\nAs explained previously, any Lightning callback can be added by passing it through the command line or including it in\nthe config via class_path and init_args entries.\n\nHowever, certain callbacks **must** be coupled with a model so they are always present and configurable. This can be\nimplemented as follows:\n\nfrom lightning.pytorch.callbacks import EarlyStopping\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_lightning_class_args(EarlyStopping, \"my_early_stopping\")\nparser.set_defaults({\"my_early_stopping.monitor\": \"val_loss\", \"my_early_stopping.patience\": 5})\n\ncli = MyLightningCLI(MyModel)\nTo change the parameters for EarlyStopping in the config it would be:\n\n```yaml\nmodel:\n  ...\ntrainer:\n  ...\nmy_early_stopping:\n  patience: 5\n```\nThe example above overrides a default in add_arguments_to_parser. This is included to show that defaults can be\nchanged if needed. However, note that overriding defaults in the source code is not intended to be used to store the\nbest hyperparameters for a task after experimentation. To guarantee reproducibility, the source code should be\nstable. It is better to practice storing the best hyperparameters for a task in a configuration file independent\nfrom the source code.\n----\n\n### Class type defaults\n\nThe support for classes as type hints allows to try many possibilities with the same CLI. This is a useful feature, but\nit is tempting to use an instance of a class as a default. For example:\n\nclass MyMainModel(LightningModule):\ndef __init__(\nself,\nbackbone: torch.nn.Module = MyModel(encoder_layers=24), # BAD PRACTICE!\n):\nsuper().__init__()\nself.backbone = backbone\nNormally classes are mutable, as in this case. The instance of MyModel would be created the moment that the module\nthat defines MyMainModel is first imported. This means that the default of backbone will be initialized before\nthe CLI class runs seed_everything, making it non-reproducible. Furthermore, if MyMainModel is used more than\nonce in the same Python process and the backbone parameter is not overridden, the same instance would be used in\nmultiple places. Most likely, this is not what the developer intended. Having an instance as default also makes it\nimpossible to generate the complete config file since it is not known which arguments were used to instantiate it for\narbitrary classes.\n\nAn excellent solution to these problems is not to have a default or set the default to a unique value (e.g., a string).\nThen check this value and instantiate it in the __init__ body. If a class parameter has no default and the CLI is\nsubclassed, then a default can be set as follows:\n\ndefault_backbone = {\n\"class_path\": \"import.path.of.MyModel\",\n\"init_args\": {\n\"encoder_layers\": 24,\n},\n}\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": default_backbone})\nA more compact version that avoids writing a dictionary would be:\n\nfrom jsonargparse import lazy_instance\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.set_defaults({\"model.backbone\": lazy_instance(MyModel, encoder_layers=24)})\n----\n\n### Argument linking\nAnother case in which it might be desired to extend ~lightning.pytorch.cli.LightningCLI is that the model and\ndata module depends on a common parameter. For example, in some cases, both classes require to know the batch_size.\nIt is a burden and error-prone to give the same value twice in a config file. To avoid this, the parser can be\nconfigured so that a value is only given once and then propagated accordingly. With a tool implemented like the one\nshown below, the batch_size only has to be provided in the data section of the config.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.batch_size\", \"model.batch_size\")\n\ncli = MyLightningCLI(MyModel, MyDataModule)\nThe linking of arguments is observed in the help of the tool, which for this example would look like:\n\n```bash\n$ python trainer.py fit --help\n  ...\n    --data.batch_size BATCH_SIZE\n                          Number of samples in a batch (type: int, default: 8)\n\n  Linked arguments:\n    data.batch_size --> model.batch_size\n                          Number of samples in a batch (type: int)\n```\nSometimes a parameter value is only available after class instantiation. An example could be that your model requires\nthe number of classes to instantiate its fully connected layer (for a classification task). But the value is not\navailable until the data module has been instantiated. The code below illustrates how to address this.\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.link_arguments(\"data.num_classes\", \"model.num_classes\", apply_on=\"instantiate\")\n\ncli = MyLightningCLI(MyClassModel, MyDataModule)\nInstantiation links are used to automatically determine the order of instantiation, in this case data first.\n\nThe linking of arguments is intended for things that are meant to be non-configurable. This improves the CLI user\nexperience since it avoids the need to provide more parameters. A related concept is a variable interpolation that\nkeeps things configurable.\nThe linking of arguments can be used for more complex cases. For example to derive a value via a function that takes\nmultiple settings as input. For more details have a look at the API of `link_arguments\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentLinking.link_arguments>`_.",
      "Lightning in 15 minutes\n## Lightning in 15 minutes\n**Required background:** None\n\n**Goal:** In this guide, we'll walk you through the 7 key steps of a typical Lightning workflow.\n\nPyTorch Lightning is the deep learning framework with \"batteries included\" for professional AI researchers and machine learning engineers who need maximal flexibility while super-charging performance at scale.\n\nLightning organizes PyTorch code to remove boilerplate and unlock scalability.\n\nBy organizing PyTorch code, lightning enables:\n\n.. Add callout items below this line\n\n.. End of callout item section\n\n----\n\n### 1: Install PyTorch Lightning\nFor pip users\n\n```bash\npip install lightning\n```\nFor conda users\n\n```bash\nconda install lightning -c conda-forge\n```\nOr read the advanced install guide\n\n----\n\n### 2: Define a LightningModule\n\nA LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\n\nimport os\nfrom torch import optim, nn, utils, Tensor\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport lightning as L\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n# define the LightningModule\nclass LitAutoEncoder(L.LightningModule):\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder\nself.decoder = decoder\n\ndef training_step(self, batch, batch_idx):\n# training_step defines the train loop.\n# it is independent of forward\nx, _ = batch\nx = x.view(x.size(0), -1)\nz = self.encoder(x)\nx_hat = self.decoder(z)\nloss = nn.functional.mse_loss(x_hat, x)\n# Logging to TensorBoard (if installed) by default\nself.log(\"train_loss\", loss)\nreturn loss\n\ndef configure_optimizers(self):\noptimizer = optim.Adam(self.parameters(), lr=1e-3)\nreturn optimizer\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n----\n\n### 3: Define a dataset\n\nLightning supports ANY iterable (~torch.utils.data.DataLoader, numpy, etc...) for the train/val/test/predict splits.\n\n```python\n# setup data\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\ntrain_loader = utils.data.DataLoader(dataset)\n```\n----\n\n### 4: Train the model\n\nThe Lightning Trainer <../common/trainer> \"mixes\" any LightningModule <../common/lightning_module> with any dataset and abstracts away all the engineering complexity needed for scale.\n\n```python\n# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\ntrainer = L.Trainer(limit_train_batches=100, max_epochs=1)\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n```\nThe Lightning Trainer <../common/trainer> automates 40+ tricks including:\n\n* Epoch and batch iteration\n* optimizer.step(), loss.backward(), optimizer.zero_grad() calls\n* Calling of model.eval(), enabling/disabling grads during evaluation\n* Checkpoint Saving and Loading <../common/checkpointing>\n* Tensorboard (see loggers <../visualize/loggers> options)\n* Multi-GPU <../accelerators/gpu> support\n* TPU <../accelerators/tpu>\n* 16-bit precision AMP <speed-amp> support\n\n----\n\n### 5: Use the model\nOnce you've trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions.\n\n```python\n# load checkpoint\ncheckpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\nautoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n\n# choose your trained nn.Module\nencoder = autoencoder.encoder\nencoder.eval()\n\n# embed 4 fake images!\nfake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\nembeddings = encoder(fake_image_batch)\nprint(\"\u26a1\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20)\n```\n----\n\n### 6: Visualize training\nIf you have tensorboard installed, you can use it for visualizing experiments.\n\nRun this on your commandline and open your browser to **http://localhost:6006/**\n\n```bash\ntensorboard --logdir .\n```\n----\n\n### 7: Supercharge training\nEnable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code.\n\n```\n# train on 4 GPUs\ntrainer = L.Trainer(\n   devices=4,\n   accelerator=\"gpu\",\n)\n\n# train 1TB+ parameter models with Deepspeed/fsdp\ntrainer = L.Trainer(\n   devices=4,\n   accelerator=\"gpu\",\n   strategy=\"deepspeed_stage_2\",\n   precision=16\n)\n\n# 20+ helpful flags for rapid idea iteration\ntrainer = L.Trainer(\n   max_epochs=10,\n   min_epochs=5,\n   overfit_batches=1\n)\n\n# access the latest state of the art techniques\ntrainer = L.Trainer(callbacks=[WeightAveraging(...)])\n```\n----\n\n### Maximize flexibility\nLightning's core guiding principle is to always provide maximal flexibility **without ever hiding any of the PyTorch**.\n\nLightning offers 5 *added* degrees of flexibility depending on your project's complexity.\n\n----\n\n# Customize training loop\n\nInject custom code anywhere in the Training loop using any of the 20+ methods (lightning_hooks) available in the LightningModule.\n\nclass LitAutoEncoder(L.LightningModule):\ndef backward(self, loss):\nloss.backward()\n----\n\n# Extend the Trainer\n\nIf you have multiple lines of code with similar functionalities, you can use callbacks to easily group them together and toggle all of those lines on or off at the same time.\n\n```\ntrainer = Trainer(callbacks=[AWSCheckpoints()])\n```\n----\n\n# Use a raw PyTorch loop\n\nFor certain types of work at the bleeding-edge of research, Lightning offers experts full control of optimization or the training loop in various ways.\n\n.. Add callout items below this line\n\n.. End of callout item section\n\n----\n\n### Next steps\nDepending on your use case, you might want to check one of these out next.\n\n.. Add callout items below this line",
      "ValueError(f\"{func} does not have a return type annotation\") when implementing LightningCLI\n Dear All,\nI am trying to implement the LightningCLI class, but it tells me my model does not have any Type annotation, although I added class function type return type annotation.\nWhat am I doing wrong?\nThank you for your input!\nLG\nMax \n Answer: Hi Mauvilsa,\nthank you for your reply. While trying to come up with a minimal reproduction example I actually found the solution to my problem:\nLightningCLI() does not take the instance of the model as a parameter but the class itself (as I guess instantiation is then done by the pipeline itself). To be more clear:\nWrong / Not Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nmodel = MyFancyModelClass()\ndatamodule = MyFancyDataModuleClass()\n\nLightningCLI = LightningCLI(model, datamodule)\n\nCorrect / Working:\nfrom lightning.pytorch.cli import LightningCLI\nfrom MyFancyModel import MyFancyModelClass\nfrom MyFancyDataModule import MyFancyDataModuleClass\n\nLightningCLI = LightningCLI(MyFancyModelClass, MyFancyDataModuleClass)\n\nHope that also helps others!\nCheers,\nMax",
      "Configure hyperparameters from the CLI (Advanced)\n## Configure hyperparameters from the CLI (Advanced)\n\n### Instantiation only mode\n\nThe CLI is designed to start fitting with minimal code changes. On class instantiation, the CLI will automatically call\nthe trainer function associated with the subcommand provided, so you don't have to do it. To avoid this, you can set the\nfollowing argument:\n\ncli = LightningCLI(MyModel, run=False) # True by default\n# you'll have to call fit yourself:\ncli.trainer.fit(cli.model)\nIn this mode, subcommands are **not** added to the parser. This can be useful to implement custom logic without having\nto subclass the CLI, but still, use the CLI's instantiation and argument parsing capabilities.\n\n### Trainer Callbacks and arguments with class type\n\nA very important argument of the ~lightning.pytorch.trainer.trainer.Trainer class is the callbacks. In\ncontrast to simpler arguments that take numbers or strings, callbacks expects a list of instances of subclasses of\n~lightning.pytorch.callbacks.Callback. To specify this kind of argument in a config file, each callback must be\ngiven as a dictionary, including a class_path entry with an import path of the class and optionally an init_args\nentry with arguments to use to instantiate. Therefore, a simple configuration file that defines two callbacks is the\nfollowing:\n\n```yaml\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n      init_args:\n        save_weights_only: true\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: 'epoch'\n```\nSimilar to the callbacks, any parameter in ~lightning.pytorch.trainer.trainer.Trainer and user extended\n~lightning.pytorch.core.LightningModule and\n~lightning.pytorch.core.datamodule.LightningDataModule classes that have as type hint a class, can be\nconfigured the same way using class_path and init_args. If the package that defines a subclass is imported\nbefore the ~lightning.pytorch.cli.LightningCLI class is run, the name can be used instead of the full import\npath.\n\nFrom command line the syntax is the following:\n\n```bash\n$ python ... \\\n    --trainer.callbacks+={CALLBACK_1_NAME} \\\n    --trainer.callbacks.{CALLBACK_1_ARGS_1}=... \\\n    --trainer.callbacks.{CALLBACK_1_ARGS_2}=... \\\n    ...\n    --trainer.callbacks+={CALLBACK_N_NAME} \\\n    --trainer.callbacks.{CALLBACK_N_ARGS_1}=... \\\n    ...\n```\nNote the use of + to append a new callback to the list and that the init_args are applied to the previous\ncallback appended. Here is an example:\n\n```bash\n$ python ... \\\n    --trainer.callbacks+=EarlyStopping \\\n    --trainer.callbacks.patience=5 \\\n    --trainer.callbacks+=LearningRateMonitor \\\n    --trainer.callbacks.logging_interval=epoch\n```\nSerialized config files (e.g. --print_config or ~lightning.pytorch.cli.SaveConfigCallback) always have\nthe full class_path, even when class name shorthand notation is used in the command line or in input config\nfiles.\n### Multiple models and/or datasets\n\nA CLI can be written such that a model and/or a datamodule is specified by an import path and init arguments. For\nexample, with a tool implemented as:\n\n```python\ncli = LightningCLI(MyModelBaseClass, MyDataModuleBaseClass, subclass_mode_model=True, subclass_mode_data=True)\n```\nA possible config file could be as follows:\n\n```yaml\nmodel:\n  class_path: mycode.mymodels.MyModel\n  init_args:\n    decoder_layers:\n    - 2\n    - 4\n    encoder_layers: 12\ndata:\n  class_path: mycode.mydatamodules.MyDataModule\n  init_args:\n    ...\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        patience: 5\n    ...\n```\nOnly model classes that are a subclass of MyModelBaseClass would be allowed, and similarly, only subclasses of\nMyDataModuleBaseClass. If as base classes ~lightning.pytorch.core.LightningModule and\n~lightning.pytorch.core.datamodule.LightningDataModule is given, then the CLI would allow any lightning module\nand data module.\n\nNote that with the subclass modes, the --help option does not show information for a specific subclass. To get\nhelp for a subclass, the options --model.help and --data.help can be used, followed by the desired class\npath. Similarly, --print_config does not include the settings for a particular subclass. To include them, the\nclass path should be given before the --print_config option. Examples for both help and print config are:\n\n.. code-block:: bash\n\n$ python trainer.py fit --model.help mycode.mymodels.MyModel\n$ python trainer.py fit --model mycode.mymodels.MyModel --print_config\n### Models with multiple submodules\n\nMany use cases require to have several modules, each with its own configurable options. One possible way to handle this\nwith LightningCLI is to implement a single module having as init parameters each of the submodules. This is known as\ndependency injection_ which is a good approach to improve\ndecoupling in your code base.\n\nSince the init parameters of the model have as a type hint a class, in the configuration, these would be specified with\nclass_path and init_args entries. For instance, a model could be implemented as:\n\nclass MyMainModel(LightningModule):\ndef __init__(self, encoder: nn.Module, decoder: nn.Module):\n\"\"\"Example encoder-decoder submodules model\n\nArgs:\nencoder: Instance of a module for encoding\ndecoder: Instance of a module for decoding\n\"\"\"\nsuper().__init__()\nself.save_hyperparameters()\nself.encoder = encoder\nself.decoder = decoder\nIf the CLI is implemented as LightningCLI(MyMainModel) the configuration would be as follows:\n\n```yaml\nmodel:\n  encoder:\n    class_path: mycode.myencoders.MyEncoder\n    init_args:\n      ...\n  decoder:\n    class_path: mycode.mydecoders.MyDecoder\n    init_args:\n      ...\n```\nIt is also possible to combine subclass_mode_model=True and submodules, thereby having two levels of class_path.\n\nBy having self.save_hyperparameters() it becomes possible to load the model from a checkpoint. Simply do\nModelClass.load_from_checkpoint(\"path/to/checkpoint.ckpt\"). In the case of using subclass_mode_model=True,\nthen load it like LightningModule.load_from_checkpoint(\"path/to/checkpoint.ckpt\"). save_hyperparameters is\noptional and can be safely removed if there is no need to load from a checkpoint.\n### Fixed optimizer and scheduler\n\nIn some cases, fixing the optimizer and/or learning scheduler might be desired instead of allowing multiple. For this,\nyou can manually add the arguments for specific classes by subclassing the CLI. The following code snippet shows how to\nimplement it:\n\nclass MyLightningCLI(LightningCLI):\ndef add_arguments_to_parser(self, parser):\nparser.add_optimizer_args(torch.optim.Adam)\nparser.add_lr_scheduler_args(torch.optim.lr_scheduler.ExponentialLR)\nWith this, in the config, the optimizer and lr_scheduler groups would accept all of the options for the given\nclasses, in this example, Adam and ExponentialLR. Therefore, the config file would be structured like:\n\n```yaml\noptimizer:\n  lr: 0.01\nlr_scheduler:\n  gamma: 0.2\nmodel:\n  ...\ntrainer:\n  ...\n```\nwhere the arguments can be passed directly through the command line without specifying the class. For example:\n\n```bash\n$ python trainer.py fit --optimizer.lr=0.01 --lr_scheduler.gamma=0.2\n```\n### Multiple optimizers and schedulers\n\nBy default, the CLIs support multiple optimizers and/or learning schedulers, automatically implementing\nconfigure_optimizers. This behavior can be disabled by providing auto_configure_optimizers=False on\ninstantiation of ~lightning.pytorch.cli.LightningCLI. This would be required for example to support multiple\noptimizers, for each selecting a particular optimizer class. Similar to multiple submodules, this can be done via\ndependency injection_. Unlike the submodules, it is not possible\nto expect an instance of a class, because optimizers require the module's parameters to optimize, which are only\navailable after instantiation of the module. Learning schedulers are a similar situation, requiring an optimizer\ninstance. For these cases, dependency injection involves providing a function that instantiates the respective class\nwhen called.\n\nAn example of a model that uses two optimizers is the following:\n\n```python\nfrom typing import Iterable\nfrom torch.optim import Optimizer\n\nOptimizerCallable = Callable[[Iterable], Optimizer]\n\nclass MyModel(LightningModule):\n    def __init__(self, optimizer1: OptimizerCallable, optimizer2: OptimizerCallable):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer1 = optimizer1\n        self.optimizer2 = optimizer2\n\n    def configure_optimizers(self):\n        optimizer1 = self.optimizer1(self.parameters())\n        optimizer2 = self.optimizer2(self.parameters())\n        return [optimizer1, optimizer2]\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\nNote the type Callable[[Iterable], Optimizer], which denotes a function that receives a single argument, some\nlearnable parameters, and returns an optimizer instance. With this, from the command line it is possible to select the\nclass and init arguments for each of the optimizers, as follows:\n\n```bash\n$ python trainer.py fit \\\n    --model.optimizer1=Adam \\\n    --model.optimizer1.lr=0.01 \\\n    --model.optimizer2=AdamW \\\n    --model.optimizer2.lr=0.0001\n```\nIn the example above, the OptimizerCallable type alias was created to illustrate what the type hint means. For\nconvenience, this type alias and one for learning schedulers is available in the cli module. An example of a model\nthat uses dependency injection for an optimizer and a learning scheduler is:\n\n```python\nfrom lightning.pytorch.cli import OptimizerCallable, LRSchedulerCallable, LightningCLI\n\nclass MyModel(LightningModule):\n    def __init__(\n        self,\n        optimizer: OptimizerCallable = torch.optim.Adam,\n        scheduler: LRSchedulerCallable = torch.optim.lr_scheduler.ConstantLR,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer(self.parameters())\n        scheduler = self.scheduler(optimizer)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\nNote that for this example, classes are used as defaults. This is compatible with the type hints, since they are also\ncallables that receive the same first argument and return an instance of the class. Classes that have more than one\nrequired argument will not work as default. For these cases a lambda function can be used, e.g. ``optimizer:\nOptimizerCallable = lambda p: torch.optim.SGD(p, lr=0.01)``.\n\n### Run from Python\n\nEven though the ~lightning.pytorch.cli.LightningCLI class is designed to help in the implementation of command\nline tools, for some use cases it is desired to run directly from Python. To allow this there is the args parameter.\nAn example could be to first implement a normal CLI script, but adding an args parameter with default None to\nthe main function as follows:\n\n```python\nfrom lightning.pytorch.cli import ArgsType, LightningCLI\n\ndef cli_main(args: ArgsType = None):\n    cli = LightningCLI(MyModel, ..., args=args)\n    ...\n\nif __name__ == \"__main__\":\n    cli_main()\n```\nThen it is possible to import the cli_main function to run it. Executing in a shell ``my_cli.py\n--trainer.max_epochs=100 --model.encoder_layers=24`` would be equivalent to:\n\n```python\nfrom my_module.my_cli import cli_main\n\ncli_main([\"--trainer.max_epochs=100\", \"--model.encoder_layers=24\"])\n```\nAll the features that are supported from the command line can be used when giving args as a list of strings. It is\nalso possible to provide a dict or `jsonargparse.Namespace\n<https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.Namespace>`__. For example in a jupyter notebook someone\nmight do:\n\n```python\nargs = {\n    \"trainer\": {\n        \"max_epochs\": 100,\n    },\n    \"model\": {},\n}\n\nargs[\"model\"][\"encoder_layers\"] = 8\ncli_main(args)\nargs[\"model\"][\"encoder_layers\"] = 12\ncli_main(args)\nargs[\"trainer\"][\"max_epochs\"] = 200\ncli_main(args)\n```\nThe args parameter must be None when running from command line so that sys.argv is used as arguments.\nAlso, note that the purpose of trainer_defaults is different to args. It is okay to use trainer_defaults\nin the cli_main function to modify the defaults of some trainer parameters."
    ],
    "distances": [
      0.9697234630584717,
      1.117997646331787,
      1.141939640045166,
      1.188499093055725,
      1.2370579242706299
    ]
  },
  {
    "query": "For defining a training step function, when should I use manual backward and what issues can arrise from using it?",
    "top_docs": [
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\nFunction Name: _make_backward_fn\nLanguage: python\nPartition: train\n\n--- Docstring ---\nBuild a `backward` function that handles back-propagation through the output produced by the `training_step`\n\n--- Code ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn\n\n--- Original String ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\nFunction Name: _make_backward_fn\nLanguage: python\nPartition: train\n\n--- Docstring ---\nBuild a `backward` function that handles back-propagation through the output produced by the `training_step`\n\n--- Code ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn\n\n--- Original String ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\loops\\optimization\\automatic.py\nFunction Name: _make_backward_fn\nLanguage: python\nPartition: train\n\n--- Docstring ---\nBuild a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\n\n--- Code ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn\n\n--- Original String ---\ndef _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\core\\module.py\nFunction Name: backward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\n\n--- Code ---\ndef backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)\n\n--- Original String ---\ndef backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\core\\module.py\nFunction Name: backward\nLanguage: python\nPartition: train\n\n--- Docstring ---\nCalled to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\n\n--- Code ---\ndef backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)\n\n--- Original String ---\ndef backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)"
    ],
    "distances": [
      0.8684632182121277,
      0.8684632182121277,
      0.8798747658729553,
      0.9703735113143921,
      0.9929947853088379
    ]
  },
  {
    "query": "init_meta_context() isn't available. What are some alternitives that have similar functionality?",
    "top_docs": [
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\strategies\\strategy.py\nFunction Name: model_sharded_context\nLanguage: python\nPartition: valid\n\n--- Docstring ---\nProvide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\n\n--- Code ---\ndef model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield\n\n--- Original String ---\ndef model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield",
      "Current best practices to initialize massive (50B parameter+) models\n Hi, I am working with GPT-style models and need to intitialize a model at the GPT-3 scale.  Unfortunately, this means the model will run out of memory during initialization on CPU (or take an eternity to initialize layer-by-layer on cpu before shipping to GPU).  In vanilla Pytorch I solved this using FSDP by initializing my models on the \"meta\" device, with full initialization on GPU afterward.  What is the current best, most performant method to accomplish this with lightning?\nNote: I found this, which references an init_meta_context(), but my pytorch-lightning (v1.9.0) has no such functionality:\nhttps://devblog.pytorchlightning.ai/experiment-with-billion-parameter-models-faster-using-deepspeed-and-meta-tensors-2e9c255edd71 \n Answer: Hi! The init_meta_context functionality was replaced with a torchdistx integration in #13868. You can do the following:\nfrom torchdistx.deferred_init import deferred_init\n\nmodel = deferred_init(YourLightningModule)\nAnd we'll materialize it for you in the Trainer. This is very experimental, and you might encounter installation issues.\nIn the long term, we'll adopt the fake tensor mode from PyTorch: #16448.\nOtherwise, for a stable(r) solution, you can use the DeepSpeed integration: https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed-zero-stage-3",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\strategies\\strategy.py\nFunction Name: _setup_optimizer\nLanguage: python\nPartition: train\n\n--- Docstring ---\nPerforms setup for the optimizer, e.g., by wrapping it by another class.\n\n--- Code ---\ndef _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return optimizer\n\n--- Original String ---\ndef _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return optimizer",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\utilities\\parsing.py\nFunction Name: get_init_args\nLanguage: python\nPartition: test\n\n--- Docstring ---\nFor backwards compatibility: #16369.\n\n--- Code ---\ndef get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args\n\n--- Original String ---\ndef get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args",
      "--- Meta Data ---\nRepo: pytorch-lightning\nPath: src\\lightning\\pytorch\\utilities\\parsing.py\nFunction Name: get_init_args\nLanguage: python\nPartition: test\n\n--- Docstring ---\nFor backwards compatibility: #16369.\n\n--- Code ---\ndef get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args\n\n--- Original String ---\ndef get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args"
    ],
    "distances": [
      1.1898105144500732,
      1.2218058109283447,
      1.2764865159988403,
      1.2924325466156006,
      1.2924325466156006
    ]
  }
]