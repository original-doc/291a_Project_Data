[
  {
    "query_id": 1,
    "query": "How to fix 'RuntimeError: CUDA out of memory' error when training with PyTorch Lightning?",
    "relevant_docs": [
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "accelerator",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Accelerator",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "strategy",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Strategy",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "devices",
          ":",
          "Union",
          "[",
          "list",
          "[",
          "int",
          "]",
          ",",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "precision",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "32",
          "-",
          "true",
          "\"",
          ",",
          "plugins",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "callbacks",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "list",
          "[",
          "Any",
          "]",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "loggers",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "Logger",
          ",",
          "list",
          "[",
          "Logger",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          "max_epochs",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "1000",
          ",",
          "max_steps",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "None",
          ",",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "1",
          ",",
          "limit_train_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "limit_val_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "validation_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          "use_distributed_sampler",
          ":",
          "bool",
          "=",
          "True",
          ",",
          "checkpoint_dir",
          ":",
          "str",
          "=",
          "\"",
          ".",
          "/",
          "checkpoints",
          "\"",
          ",",
          "checkpoint_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Exemplary",
          "Trainer",
          "with",
          "Fabric",
          ".",
          "This",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          ".",
          "As",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          ",",
          "we",
          "recommend",
          "using",
          "the",
          ":",
          "class",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "Trainer",
          "`",
          ".",
          "Args",
          ":",
          "accelerator",
          ":",
          "The",
          "hardware",
          "to",
          "run",
          "on",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "cpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "cuda",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "mps",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "gpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "tpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "strategy",
          ":",
          "Strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "dp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp_spawn",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "deepspeed",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "fsdp",
          "\"",
          "`",
          "`",
          ".",
          "devices",
          ":",
          "Number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "int",
          "`",
          "`",
          ")",
          ",",
          "which",
          "GPUs",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "list",
          "`",
          "`",
          "or",
          "`",
          "`",
          "str",
          "`",
          "`",
          ")",
          ",",
          "or",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "The",
          "value",
          "applies",
          "per",
          "node",
          ".",
          "precision",
          ":",
          "Double",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "64",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "full",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "32",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "half",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "or",
          "bfloat16",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "bf16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ".",
          "plugins",
          ":",
          "One",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          ":",
          "A",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          ".",
          "The",
          "following",
          "hooks",
          "are",
          "supported",
          ":",
          "-",
          "on_train_epoch_start",
          "-",
          "on",
          "train_epoch_end",
          "-",
          "on_train_batch_start",
          "-",
          "on_train_batch_end",
          "-",
          "on_before_backward",
          "-",
          "on_after_backward",
          "-",
          "on_before_zero_grad",
          "-",
          "on_before_optimizer_step",
          "-",
          "on_validation_model_eval",
          "-",
          "on_validation_model_train",
          "-",
          "on_validation_epoch_start",
          "-",
          "on_validation_epoch_end",
          "-",
          "on_validation_batch_start",
          "-",
          "on_validation_batch_end",
          "loggers",
          ":",
          "A",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          ".",
          "See",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "fabric",
          ".",
          "fabric",
          ".",
          "Fabric",
          ".",
          "log",
          "`",
          "for",
          "more",
          "information",
          ".",
          "max_epochs",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "(",
          "optimizer",
          ")",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          ":",
          "How",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "limit_val_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "validation_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          ".",
          "use_distributed_sampler",
          ":",
          "Wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "-",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          ".",
          "checkpoint_dir",
          ":",
          "Directory",
          "to",
          "store",
          "checkpoints",
          "to",
          ".",
          "checkpoint_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          ".",
          "Warning",
          ":",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "(",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          ")",
          ",",
          "won",
          "'",
          "t",
          "work",
          "!",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          "=",
          "L",
          ".",
          "Fabric",
          "(",
          "accelerator",
          "=",
          "accelerator",
          ",",
          "strategy",
          "=",
          "strategy",
          ",",
          "devices",
          "=",
          "devices",
          ",",
          "precision",
          "=",
          "precision",
          ",",
          "plugins",
          "=",
          "plugins",
          ",",
          "callbacks",
          "=",
          "callbacks",
          ",",
          "loggers",
          "=",
          "loggers",
          ",",
          ")",
          "self",
          ".",
          "global_step",
          "=",
          "0",
          "self",
          ".",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "grad_accum_steps",
          "self",
          ".",
          "current_epoch",
          "=",
          "0",
          "self",
          ".",
          "max_epochs",
          "=",
          "max_epochs",
          "self",
          ".",
          "max_steps",
          "=",
          "max_steps",
          "self",
          ".",
          "should_stop",
          "=",
          "False",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_train_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_train_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_val_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_val_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "self",
          ".",
          "limit_train_batches",
          "=",
          "limit_train_batches",
          "self",
          ".",
          "limit_val_batches",
          "=",
          "limit_val_batches",
          "self",
          ".",
          "validation_frequency",
          "=",
          "validation_frequency",
          "self",
          ".",
          "use_distributed_sampler",
          "=",
          "use_distributed_sampler",
          "self",
          ".",
          "_current_train_return",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "_current_val_return",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "checkpoint_dir",
          "=",
          "checkpoint_dir",
          "self",
          ".",
          "checkpoint_frequency",
          "=",
          "checkpoint_frequency"
        ],
        "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!",
        "docstring_tokens": [
          "exemplary",
          "trainer",
          "with",
          "fabric",
          "this",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          "as",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          "we",
          "recommend",
          "using",
          "the",
          "class",
          "lightning",
          "pytorch",
          "trainer",
          "args",
          "accelerator",
          "the",
          "hardware",
          "to",
          "run",
          "on",
          "possible",
          "choices",
          "are",
          "cpu",
          "cuda",
          "mps",
          "gpu",
          "tpu",
          "auto",
          "strategy",
          "strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          "possible",
          "choices",
          "are",
          "dp",
          "ddp",
          "ddp_spawn",
          "deepspeed",
          "fsdp",
          "devices",
          "number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "int",
          "which",
          "gpus",
          "to",
          "train",
          "on",
          "list",
          "or",
          "str",
          "or",
          "auto",
          "the",
          "value",
          "applies",
          "per",
          "node",
          "precision",
          "double",
          "precision",
          "64",
          "full",
          "precision",
          "32",
          "half",
          "precision",
          "amp",
          "16",
          "mixed",
          "or",
          "bfloat16",
          "precision",
          "amp",
          "bf16",
          "mixed",
          "plugins",
          "one",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          "a",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          "the",
          "following",
          "hooks",
          "are",
          "supported",
          "on_train_epoch_start",
          "on",
          "train_epoch_end",
          "on_train_batch_start",
          "on_train_batch_end",
          "on_before_backward",
          "on_after_backward",
          "on_before_zero_grad",
          "on_before_optimizer_step",
          "on_validation_model_eval",
          "on_validation_model_train",
          "on_validation_epoch_start",
          "on_validation_epoch_end",
          "on_validation_batch_start",
          "on_validation_batch_end",
          "loggers",
          "a",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          "see",
          "meth",
          "lightning",
          "fabric",
          "fabric",
          "fabric",
          "log",
          "for",
          "more",
          "information",
          "max_epochs",
          "the",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          "the",
          "maximum",
          "number",
          "of",
          "optimizer",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          "how",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          "limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "limit_val_batches",
          "limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "validation_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          "use_distributed_sampler",
          "wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          "checkpoint_dir",
          "directory",
          "to",
          "store",
          "checkpoints",
          "to",
          "checkpoint_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          "warning",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          "won",
          "t",
          "work"
        ],
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 18,
        "end_line": 121,
        "hash": "0bbe0adf3ec60b2f294e15b93da70f56",
        "complexity": 3,
        "parameters": [
          "accelerator",
          "Accelerator]",
          "strategy",
          "Strategy]",
          "devices",
          "str",
          "int]",
          "precision",
          "int]",
          "plugins",
          "Any]]",
          "callbacks",
          "Any]]",
          "loggers",
          "list[Logger]]]",
          "max_epochs",
          "max_steps",
          "grad_accum_steps",
          "limit_train_batches",
          "float]",
          "limit_val_batches",
          "float]",
          "validation_frequency",
          "use_distributed_sampler",
          "checkpoint_dir",
          "checkpoint_frequency"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "env",
          ":",
          "str",
          ",",
          "gamma",
          ":",
          "float",
          "=",
          "0",
          ".",
          "99",
          ",",
          "lam",
          ":",
          "float",
          "=",
          "0",
          ".",
          "95",
          ",",
          "lr_actor",
          ":",
          "float",
          "=",
          "3e",
          "-",
          "4",
          ",",
          "lr_critic",
          ":",
          "float",
          "=",
          "1e",
          "-",
          "3",
          ",",
          "max_episode_len",
          ":",
          "float",
          "=",
          "200",
          ",",
          "batch_size",
          ":",
          "int",
          "=",
          "512",
          ",",
          "steps_per_epoch",
          ":",
          "int",
          "=",
          "2048",
          ",",
          "nb_optim_iters",
          ":",
          "int",
          "=",
          "4",
          ",",
          "clip_ratio",
          ":",
          "float",
          "=",
          "0",
          ".",
          "2",
          ",",
          "*",
          "*",
          "kwargs",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Args",
          ":",
          "env",
          ":",
          "gym",
          "environment",
          "tag",
          "gamma",
          ":",
          "discount",
          "factor",
          "lam",
          ":",
          "advantage",
          "discount",
          "factor",
          "(",
          "lambda",
          "in",
          "the",
          "paper",
          ")",
          "lr_actor",
          ":",
          "learning",
          "rate",
          "of",
          "actor",
          "network",
          "lr_critic",
          ":",
          "learning",
          "rate",
          "of",
          "critic",
          "network",
          "max_episode_len",
          ":",
          "maximum",
          "number",
          "interactions",
          "(",
          "actions",
          ")",
          "in",
          "an",
          "episode",
          "batch_size",
          ":",
          "batch_size",
          "when",
          "training",
          "network",
          "-",
          "can",
          "simulate",
          "number",
          "of",
          "policy",
          "updates",
          "performed",
          "per",
          "epoch",
          "steps_per_epoch",
          ":",
          "how",
          "many",
          "action",
          "-",
          "state",
          "pairs",
          "to",
          "rollout",
          "for",
          "trajectory",
          "collection",
          "per",
          "epoch",
          "nb_optim_iters",
          ":",
          "how",
          "many",
          "steps",
          "of",
          "gradient",
          "descent",
          "to",
          "perform",
          "on",
          "each",
          "batch",
          "clip_ratio",
          ":",
          "hyperparameter",
          "for",
          "clipping",
          "in",
          "the",
          "policy",
          "objective",
          "\"",
          "\"",
          "\"",
          "super",
          "(",
          ")",
          ".",
          "__init__",
          "(",
          ")",
          "self",
          ".",
          "lr_actor",
          "=",
          "lr_actor",
          "self",
          ".",
          "lr_critic",
          "=",
          "lr_critic",
          "self",
          ".",
          "steps_per_epoch",
          "=",
          "steps_per_epoch",
          "self",
          ".",
          "nb_optim_iters",
          "=",
          "nb_optim_iters",
          "self",
          ".",
          "batch_size",
          "=",
          "batch_size",
          "self",
          ".",
          "gamma",
          "=",
          "gamma",
          "self",
          ".",
          "lam",
          "=",
          "lam",
          "self",
          ".",
          "max_episode_len",
          "=",
          "max_episode_len",
          "self",
          ".",
          "clip_ratio",
          "=",
          "clip_ratio",
          "self",
          ".",
          "save_hyperparameters",
          "(",
          ")",
          "self",
          ".",
          "automatic_optimization",
          "=",
          "False",
          "self",
          ".",
          "env",
          "=",
          "gym",
          ".",
          "make",
          "(",
          "env",
          ")",
          "self",
          ".",
          "critic",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "1",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ",",
          "gym",
          ".",
          "spaces",
          ".",
          "box",
          ".",
          "Box",
          ")",
          ":",
          "act_dim",
          "=",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ".",
          "shape",
          "[",
          "0",
          "]",
          "actor_mlp",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "act_dim",
          ")",
          "self",
          ".",
          "actor",
          "=",
          "ActorContinuous",
          "(",
          "actor_mlp",
          ",",
          "act_dim",
          ")",
          "elif",
          "isinstance",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ",",
          "gym",
          ".",
          "spaces",
          ".",
          "discrete",
          ".",
          "Discrete",
          ")",
          ":",
          "actor_mlp",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ".",
          "n",
          ")",
          "self",
          ".",
          "actor",
          "=",
          "ActorCategorical",
          "(",
          "actor_mlp",
          ")",
          "else",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "Env",
          "action",
          "space",
          "should",
          "be",
          "of",
          "type",
          "Box",
          "(",
          "continuous",
          ")",
          "or",
          "Discrete",
          "(",
          "categorical",
          ")",
          ".",
          "\"",
          "f",
          "\"",
          "Got",
          "type",
          ":",
          "{",
          "type",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ")",
          "}",
          "\"",
          ")",
          "self",
          ".",
          "batch_states",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_actions",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_adv",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_qvals",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_logp",
          "=",
          "[",
          "]",
          "self",
          ".",
          "ep_rewards",
          "=",
          "[",
          "]",
          "self",
          ".",
          "ep_values",
          "=",
          "[",
          "]",
          "self",
          ".",
          "epoch_rewards",
          "=",
          "[",
          "]",
          "self",
          ".",
          "episode_step",
          "=",
          "0",
          "self",
          ".",
          "avg_ep_reward",
          "=",
          "0",
          "self",
          ".",
          "avg_ep_len",
          "=",
          "0",
          "self",
          ".",
          "avg_reward",
          "=",
          "0",
          "self",
          ".",
          "state",
          "=",
          "torch",
          ".",
          "FloatTensor",
          "(",
          "self",
          ".",
          "env",
          ".",
          "reset",
          "(",
          ")",
          ")"
        ],
        "docstring": "Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective",
        "docstring_tokens": [
          "args",
          "env",
          "gym",
          "environment",
          "tag",
          "gamma",
          "discount",
          "factor",
          "lam",
          "advantage",
          "discount",
          "factor",
          "lambda",
          "in",
          "the",
          "paper",
          "lr_actor",
          "learning",
          "rate",
          "of",
          "actor",
          "network",
          "lr_critic",
          "learning",
          "rate",
          "of",
          "critic",
          "network",
          "max_episode_len",
          "maximum",
          "number",
          "interactions",
          "actions",
          "in",
          "an",
          "episode",
          "batch_size",
          "batch_size",
          "when",
          "training",
          "network",
          "can",
          "simulate",
          "number",
          "of",
          "policy",
          "updates",
          "performed",
          "per",
          "epoch",
          "steps_per_epoch",
          "how",
          "many",
          "action",
          "state",
          "pairs",
          "to",
          "rollout",
          "for",
          "trajectory",
          "collection",
          "per",
          "epoch",
          "nb_optim_iters",
          "how",
          "many",
          "steps",
          "of",
          "gradient",
          "descent",
          "to",
          "perform",
          "on",
          "each",
          "batch",
          "clip_ratio",
          "hyperparameter",
          "for",
          "clipping",
          "in",
          "the",
          "policy",
          "objective"
        ],
        "docstring_summary": "Args:",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "PPOLightning",
        "start_line": 154,
        "end_line": 229,
        "hash": "4c413b034484f7c6c2f3c02d3031e0ef",
        "complexity": 3,
        "parameters": [
          "env",
          "gamma",
          "lam",
          "lr_actor",
          "lr_critic",
          "max_episode_len",
          "batch_size",
          "steps_per_epoch",
          "nb_optim_iters",
          "clip_ratio",
          "**kwargs"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "fit",
        "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "language": "python",
        "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "code_tokens": [
          "def",
          "fit",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "val_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "ckpt_path",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          ",",
          "triggering",
          "the",
          "actual",
          "training",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          ".",
          "Can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          ":",
          "attr",
          ":",
          "`",
          "callbacks",
          "`",
          "(",
          "see",
          ":",
          "meth",
          ":",
          "`",
          "MyCustomTrainer",
          ".",
          "__init__",
          "`",
          ")",
          ".",
          "train_loader",
          ":",
          "the",
          "training",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "val_loader",
          ":",
          "the",
          "validation",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "If",
          "not",
          "specified",
          ",",
          "no",
          "validation",
          "will",
          "run",
          ".",
          "ckpt_path",
          ":",
          "Path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          ".",
          "If",
          "specified",
          ",",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "launch",
          "(",
          ")",
          "train_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "train_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          ":",
          "val_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "val_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "fabric",
          ".",
          "strategy",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "strategies",
          ".",
          "fsdp",
          ".",
          "FSDPStrategy",
          ")",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "currently",
          "does",
          "not",
          "support",
          "FSDP",
          "\"",
          ")",
          "optimizer",
          ",",
          "scheduler_cfg",
          "=",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "model",
          ".",
          "configure_optimizers",
          "(",
          ")",
          ")",
          "assert",
          "optimizer",
          "is",
          "not",
          "None",
          "model",
          ",",
          "optimizer",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup",
          "(",
          "model",
          ",",
          "optimizer",
          ")",
          "state",
          "=",
          "{",
          "\"",
          "model",
          "\"",
          ":",
          "model",
          ",",
          "\"",
          "optim",
          "\"",
          ":",
          "optimizer",
          ",",
          "\"",
          "scheduler",
          "\"",
          ":",
          "scheduler_cfg",
          "}",
          "if",
          "ckpt_path",
          "is",
          "not",
          "None",
          "and",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "ckpt_path",
          ")",
          ":",
          "latest_checkpoint_path",
          "=",
          "self",
          ".",
          "get_latest_checkpoint",
          "(",
          "self",
          ".",
          "checkpoint_dir",
          ")",
          "if",
          "latest_checkpoint_path",
          "is",
          "not",
          "None",
          ":",
          "self",
          ".",
          "load",
          "(",
          "state",
          ",",
          "latest_checkpoint_path",
          ")",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "while",
          "not",
          "self",
          ".",
          "should_stop",
          ":",
          "self",
          ".",
          "train_loop",
          "(",
          "model",
          ",",
          "optimizer",
          ",",
          "train_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_train_batches",
          ",",
          "scheduler_cfg",
          "=",
          "scheduler_cfg",
          ")",
          "if",
          "self",
          ".",
          "should_validate",
          ":",
          "self",
          ".",
          "val_loop",
          "(",
          "model",
          ",",
          "val_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_val_batches",
          ")",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "epoch",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "current_epoch",
          ")",
          "self",
          ".",
          "current_epoch",
          "+",
          "=",
          "1",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "self",
          ".",
          "save",
          "(",
          "state",
          ")",
          "self",
          ".",
          "should_stop",
          "=",
          "False"
        ],
        "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.",
        "docstring_tokens": [
          "the",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          "triggering",
          "the",
          "actual",
          "training",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          "attr",
          "callbacks",
          "see",
          "meth",
          "mycustomtrainer",
          "__init__",
          "train_loader",
          "the",
          "training",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "val_loader",
          "the",
          "validation",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "if",
          "not",
          "specified",
          "no",
          "validation",
          "will",
          "run",
          "ckpt_path",
          "path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          "if",
          "specified",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory"
        ],
        "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 123,
        "end_line": 191,
        "hash": "f92144ac7f0974541f275cce4fcc2e5f",
        "complexity": 12,
        "parameters": [
          "model",
          "train_loader",
          "val_loader",
          "ckpt_path"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "train_loop",
        "original_string": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")",
        "language": "python",
        "code": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")",
        "code_tokens": [
          "def",
          "train_loop",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "optimizer",
          ":",
          "torch",
          ".",
          "optim",
          ".",
          "Optimizer",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "limit_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "scheduler_cfg",
          ":",
          "Optional",
          "[",
          "Mapping",
          "[",
          "str",
          ",",
          "Union",
          "[",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "LRScheduler",
          ",",
          "bool",
          ",",
          "str",
          ",",
          "int",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "training",
          "loop",
          "running",
          "a",
          "single",
          "training",
          "epoch",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          "optimizer",
          ":",
          "the",
          "optimizer",
          ",",
          "optimizing",
          "the",
          "LightningModule",
          ".",
          "train_loader",
          ":",
          "The",
          "dataloader",
          "yielding",
          "the",
          "training",
          "batches",
          ".",
          "limit_batches",
          ":",
          "Limits",
          "the",
          "batches",
          "during",
          "this",
          "training",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "`",
          "`",
          "train_loader",
          "`",
          "`",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "scheduler_cfg",
          ":",
          "The",
          "learning",
          "rate",
          "scheduler",
          "configuration",
          ".",
          "Have",
          "a",
          "look",
          "at",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "pytorch",
          ".",
          "core",
          ".",
          "LightningModule",
          ".",
          "configure_optimizers",
          "`",
          "for",
          "supported",
          "values",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_epoch_start",
          "\"",
          ")",
          "iterable",
          "=",
          "self",
          ".",
          "progbar_wrapper",
          "(",
          "train_loader",
          ",",
          "total",
          "=",
          "min",
          "(",
          "len",
          "(",
          "train_loader",
          ")",
          ",",
          "limit_batches",
          ")",
          ",",
          "desc",
          "=",
          "f",
          "\"",
          "Epoch",
          "{",
          "self",
          ".",
          "current_epoch",
          "}",
          "\"",
          ")",
          "for",
          "batch_idx",
          ",",
          "batch",
          "in",
          "enumerate",
          "(",
          "iterable",
          ")",
          ":",
          "if",
          "self",
          ".",
          "should_stop",
          "or",
          "batch_idx",
          ">",
          "=",
          "limit_batches",
          ":",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_batch_start",
          "\"",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "should_optim_step",
          "=",
          "self",
          ".",
          "global_step",
          "%",
          "self",
          ".",
          "grad_accum_steps",
          "=",
          "=",
          "0",
          "if",
          "should_optim_step",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_before_optimizer_step",
          "\"",
          ",",
          "optimizer",
          ")",
          "optimizer",
          ".",
          "step",
          "(",
          "partial",
          "(",
          "self",
          ".",
          "training_step",
          ",",
          "model",
          "=",
          "model",
          ",",
          "batch",
          "=",
          "batch",
          ",",
          "batch_idx",
          "=",
          "batch_idx",
          ")",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_before_zero_grad",
          "\"",
          ",",
          "optimizer",
          ")",
          "optimizer",
          ".",
          "zero_grad",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "training_step",
          "(",
          "model",
          "=",
          "model",
          ",",
          "batch",
          "=",
          "batch",
          ",",
          "batch_idx",
          "=",
          "batch_idx",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_batch_end",
          "\"",
          ",",
          "self",
          ".",
          "_current_train_return",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "if",
          "should_optim_step",
          ":",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "step",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "global_step",
          ")",
          "self",
          ".",
          "_format_iterable",
          "(",
          "iterable",
          ",",
          "self",
          ".",
          "_current_train_return",
          ",",
          "\"",
          "train",
          "\"",
          ")",
          "self",
          ".",
          "global_step",
          "+",
          "=",
          "int",
          "(",
          "should_optim_step",
          ")",
          "if",
          "self",
          ".",
          "max_steps",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "global_step",
          ">",
          "=",
          "self",
          ".",
          "max_steps",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_epoch_end",
          "\"",
          ")"
        ],
        "docstring": "The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.",
        "docstring_tokens": [
          "the",
          "training",
          "loop",
          "running",
          "a",
          "single",
          "training",
          "epoch",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "optimizer",
          "the",
          "optimizer",
          "optimizing",
          "the",
          "lightningmodule",
          "train_loader",
          "the",
          "dataloader",
          "yielding",
          "the",
          "training",
          "batches",
          "limit_batches",
          "limits",
          "the",
          "batches",
          "during",
          "this",
          "training",
          "epoch",
          "if",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "train_loader",
          "this",
          "has",
          "no",
          "effect",
          "scheduler_cfg",
          "the",
          "learning",
          "rate",
          "scheduler",
          "configuration",
          "have",
          "a",
          "look",
          "at",
          "meth",
          "lightning",
          "pytorch",
          "core",
          "lightningmodule",
          "configure_optimizers",
          "for",
          "supported",
          "values"
        ],
        "docstring_summary": "The training loop running a single training epoch.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 193,
        "end_line": 259,
        "hash": "7dd4ca71b837d6f688303a3e5eb5fe82",
        "complexity": 8,
        "parameters": [
          "model",
          "optimizer",
          "train_loader",
          "limit_batches",
          "float]",
          "scheduler_cfg",
          "Union[L.fabric.utilities.types.LRScheduler",
          "bool",
          "str",
          "int]]]"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "_parse_optimizers_schedulers",
        "original_string": "def _parse_optimizers_schedulers(\r\n        self, configure_optim_output\r\n    ) -> tuple[\r\n        Optional[L.fabric.utilities.types.Optimizable],\r\n        Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n    ]:\r\n        \"\"\"Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        _lr_sched_defaults = {\"interval\": \"epoch\", \"frequency\": 1, \"monitor\": \"val_loss\"}\r\n\r\n        # single optimizer\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.Optimizable):\r\n            return configure_optim_output, None\r\n\r\n        # single lr scheduler\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.LRScheduler):\r\n            return None, _lr_sched_defaults.update(scheduler=configure_optim_output)\r\n\r\n        # single lr scheduler config\r\n        if isinstance(configure_optim_output, Mapping):\r\n            _lr_sched_defaults.update(configure_optim_output)\r\n            return None, _lr_sched_defaults\r\n\r\n        # list or tuple\r\n        if isinstance(configure_optim_output, (list, tuple)):\r\n            if all(isinstance(_opt_cand, L.fabric.utilities.types.Optimizable) for _opt_cand in configure_optim_output):\r\n                # single optimizer in list\r\n                if len(configure_optim_output) == 1:\r\n                    return configure_optim_output[0][0], None\r\n\r\n                raise NotImplementedError(\"BYOT only supports a single optimizer\")\r\n\r\n            if all(\r\n                isinstance(_lr_cand, (L.fabric.utilities.types.LRScheduler, Mapping))\r\n                for _lr_cand in configure_optim_output\r\n            ):\r\n                # single scheduler in list\r\n                if len(configure_optim_output) == 1:\r\n                    return None, self._parse_optimizers_schedulers(configure_optim_output[0])[1]\r\n\r\n            # optimizer and lr scheduler\r\n            elif len(configure_optim_output) == 2:\r\n                opt_cands, lr_cands = (\r\n                    self._parse_optimizers_schedulers(configure_optim_output[0])[0],\r\n                    self._parse_optimizers_schedulers(configure_optim_output[1])[1],\r\n                )\r\n                return opt_cands, lr_cands\r\n\r\n        return None, None",
        "language": "python",
        "code": "def _parse_optimizers_schedulers(\r\n        self, configure_optim_output\r\n    ) -> tuple[\r\n        Optional[L.fabric.utilities.types.Optimizable],\r\n        Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n    ]:\r\n        \"\"\"Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        _lr_sched_defaults = {\"interval\": \"epoch\", \"frequency\": 1, \"monitor\": \"val_loss\"}\r\n\r\n        # single optimizer\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.Optimizable):\r\n            return configure_optim_output, None\r\n\r\n        # single lr scheduler\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.LRScheduler):\r\n            return None, _lr_sched_defaults.update(scheduler=configure_optim_output)\r\n\r\n        # single lr scheduler config\r\n        if isinstance(configure_optim_output, Mapping):\r\n            _lr_sched_defaults.update(configure_optim_output)\r\n            return None, _lr_sched_defaults\r\n\r\n        # list or tuple\r\n        if isinstance(configure_optim_output, (list, tuple)):\r\n            if all(isinstance(_opt_cand, L.fabric.utilities.types.Optimizable) for _opt_cand in configure_optim_output):\r\n                # single optimizer in list\r\n                if len(configure_optim_output) == 1:\r\n                    return configure_optim_output[0][0], None\r\n\r\n                raise NotImplementedError(\"BYOT only supports a single optimizer\")\r\n\r\n            if all(\r\n                isinstance(_lr_cand, (L.fabric.utilities.types.LRScheduler, Mapping))\r\n                for _lr_cand in configure_optim_output\r\n            ):\r\n                # single scheduler in list\r\n                if len(configure_optim_output) == 1:\r\n                    return None, self._parse_optimizers_schedulers(configure_optim_output[0])[1]\r\n\r\n            # optimizer and lr scheduler\r\n            elif len(configure_optim_output) == 2:\r\n                opt_cands, lr_cands = (\r\n                    self._parse_optimizers_schedulers(configure_optim_output[0])[0],\r\n                    self._parse_optimizers_schedulers(configure_optim_output[1])[1],\r\n                )\r\n                return opt_cands, lr_cands\r\n\r\n        return None, None",
        "code_tokens": [
          "def",
          "_parse_optimizers_schedulers",
          "(",
          "self",
          ",",
          "configure_optim_output",
          ")",
          "-",
          ">",
          "tuple",
          "[",
          "Optional",
          "[",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "Optimizable",
          "]",
          ",",
          "Optional",
          "[",
          "Mapping",
          "[",
          "str",
          ",",
          "Union",
          "[",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "LRScheduler",
          ",",
          "bool",
          ",",
          "str",
          ",",
          "int",
          "]",
          "]",
          "]",
          ",",
          "]",
          ":",
          "\"",
          "\"",
          "\"",
          "Recursively",
          "parses",
          "the",
          "output",
          "of",
          ":",
          "meth",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "LightningModule",
          ".",
          "configure_optimizers",
          "`",
          ".",
          "Args",
          ":",
          "configure_optim_output",
          ":",
          "The",
          "output",
          "of",
          "`",
          "`",
          "configure_optimizers",
          "`",
          "`",
          ".",
          "For",
          "supported",
          "values",
          ",",
          "please",
          "refer",
          "to",
          ":",
          "meth",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "LightningModule",
          ".",
          "configure_optimizers",
          "`",
          ".",
          "\"",
          "\"",
          "\"",
          "_lr_sched_defaults",
          "=",
          "{",
          "\"",
          "interval",
          "\"",
          ":",
          "\"",
          "epoch",
          "\"",
          ",",
          "\"",
          "frequency",
          "\"",
          ":",
          "1",
          ",",
          "\"",
          "monitor",
          "\"",
          ":",
          "\"",
          "val_loss",
          "\"",
          "}",
          "if",
          "isinstance",
          "(",
          "configure_optim_output",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "Optimizable",
          ")",
          ":",
          "return",
          "configure_optim_output",
          ",",
          "None",
          "if",
          "isinstance",
          "(",
          "configure_optim_output",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "LRScheduler",
          ")",
          ":",
          "return",
          "None",
          ",",
          "_lr_sched_defaults",
          ".",
          "update",
          "(",
          "scheduler",
          "=",
          "configure_optim_output",
          ")",
          "if",
          "isinstance",
          "(",
          "configure_optim_output",
          ",",
          "Mapping",
          ")",
          ":",
          "_lr_sched_defaults",
          ".",
          "update",
          "(",
          "configure_optim_output",
          ")",
          "return",
          "None",
          ",",
          "_lr_sched_defaults",
          "if",
          "isinstance",
          "(",
          "configure_optim_output",
          ",",
          "(",
          "list",
          ",",
          "tuple",
          ")",
          ")",
          ":",
          "if",
          "all",
          "(",
          "isinstance",
          "(",
          "_opt_cand",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "Optimizable",
          ")",
          "for",
          "_opt_cand",
          "in",
          "configure_optim_output",
          ")",
          ":",
          "if",
          "len",
          "(",
          "configure_optim_output",
          ")",
          "=",
          "=",
          "1",
          ":",
          "return",
          "configure_optim_output",
          "[",
          "0",
          "]",
          "[",
          "0",
          "]",
          ",",
          "None",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "only",
          "supports",
          "a",
          "single",
          "optimizer",
          "\"",
          ")",
          "if",
          "all",
          "(",
          "isinstance",
          "(",
          "_lr_cand",
          ",",
          "(",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "LRScheduler",
          ",",
          "Mapping",
          ")",
          ")",
          "for",
          "_lr_cand",
          "in",
          "configure_optim_output",
          ")",
          ":",
          "if",
          "len",
          "(",
          "configure_optim_output",
          ")",
          "=",
          "=",
          "1",
          ":",
          "return",
          "None",
          ",",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "configure_optim_output",
          "[",
          "0",
          "]",
          ")",
          "[",
          "1",
          "]",
          "elif",
          "len",
          "(",
          "configure_optim_output",
          ")",
          "=",
          "=",
          "2",
          ":",
          "opt_cands",
          ",",
          "lr_cands",
          "=",
          "(",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "configure_optim_output",
          "[",
          "0",
          "]",
          ")",
          "[",
          "0",
          "]",
          ",",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "configure_optim_output",
          "[",
          "1",
          "]",
          ")",
          "[",
          "1",
          "]",
          ",",
          ")",
          "return",
          "opt_cands",
          ",",
          "lr_cands",
          "return",
          "None",
          ",",
          "None"
        ],
        "docstring": "Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.",
        "docstring_tokens": [
          "recursively",
          "parses",
          "the",
          "output",
          "of",
          "meth",
          "lightning",
          "pytorch",
          "lightningmodule",
          "configure_optimizers",
          "args",
          "configure_optim_output",
          "the",
          "output",
          "of",
          "configure_optimizers",
          "for",
          "supported",
          "values",
          "please",
          "refer",
          "to",
          "meth",
          "lightning",
          "pytorch",
          "lightningmodule",
          "configure_optimizers"
        ],
        "docstring_summary": "Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 466,
        "end_line": 519,
        "hash": "49e88a8be6c27b795bf5a766f60a9454",
        "complexity": 12,
        "parameters": [
          "configure_optim_output"
        ]
      }
    ]
  },
  {
    "query_id": 2,
    "query": "Why is my validation loss not decreasing in Lightning Trainer?",
    "relevant_docs": [
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "fit",
        "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "language": "python",
        "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "code_tokens": [
          "def",
          "fit",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "val_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "ckpt_path",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          ",",
          "triggering",
          "the",
          "actual",
          "training",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          ".",
          "Can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          ":",
          "attr",
          ":",
          "`",
          "callbacks",
          "`",
          "(",
          "see",
          ":",
          "meth",
          ":",
          "`",
          "MyCustomTrainer",
          ".",
          "__init__",
          "`",
          ")",
          ".",
          "train_loader",
          ":",
          "the",
          "training",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "val_loader",
          ":",
          "the",
          "validation",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "If",
          "not",
          "specified",
          ",",
          "no",
          "validation",
          "will",
          "run",
          ".",
          "ckpt_path",
          ":",
          "Path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          ".",
          "If",
          "specified",
          ",",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "launch",
          "(",
          ")",
          "train_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "train_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          ":",
          "val_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "val_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "fabric",
          ".",
          "strategy",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "strategies",
          ".",
          "fsdp",
          ".",
          "FSDPStrategy",
          ")",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "currently",
          "does",
          "not",
          "support",
          "FSDP",
          "\"",
          ")",
          "optimizer",
          ",",
          "scheduler_cfg",
          "=",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "model",
          ".",
          "configure_optimizers",
          "(",
          ")",
          ")",
          "assert",
          "optimizer",
          "is",
          "not",
          "None",
          "model",
          ",",
          "optimizer",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup",
          "(",
          "model",
          ",",
          "optimizer",
          ")",
          "state",
          "=",
          "{",
          "\"",
          "model",
          "\"",
          ":",
          "model",
          ",",
          "\"",
          "optim",
          "\"",
          ":",
          "optimizer",
          ",",
          "\"",
          "scheduler",
          "\"",
          ":",
          "scheduler_cfg",
          "}",
          "if",
          "ckpt_path",
          "is",
          "not",
          "None",
          "and",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "ckpt_path",
          ")",
          ":",
          "latest_checkpoint_path",
          "=",
          "self",
          ".",
          "get_latest_checkpoint",
          "(",
          "self",
          ".",
          "checkpoint_dir",
          ")",
          "if",
          "latest_checkpoint_path",
          "is",
          "not",
          "None",
          ":",
          "self",
          ".",
          "load",
          "(",
          "state",
          ",",
          "latest_checkpoint_path",
          ")",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "while",
          "not",
          "self",
          ".",
          "should_stop",
          ":",
          "self",
          ".",
          "train_loop",
          "(",
          "model",
          ",",
          "optimizer",
          ",",
          "train_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_train_batches",
          ",",
          "scheduler_cfg",
          "=",
          "scheduler_cfg",
          ")",
          "if",
          "self",
          ".",
          "should_validate",
          ":",
          "self",
          ".",
          "val_loop",
          "(",
          "model",
          ",",
          "val_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_val_batches",
          ")",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "epoch",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "current_epoch",
          ")",
          "self",
          ".",
          "current_epoch",
          "+",
          "=",
          "1",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "self",
          ".",
          "save",
          "(",
          "state",
          ")",
          "self",
          ".",
          "should_stop",
          "=",
          "False"
        ],
        "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.",
        "docstring_tokens": [
          "the",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          "triggering",
          "the",
          "actual",
          "training",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          "attr",
          "callbacks",
          "see",
          "meth",
          "mycustomtrainer",
          "__init__",
          "train_loader",
          "the",
          "training",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "val_loader",
          "the",
          "validation",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "if",
          "not",
          "specified",
          "no",
          "validation",
          "will",
          "run",
          "ckpt_path",
          "path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          "if",
          "specified",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory"
        ],
        "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 123,
        "end_line": 191,
        "hash": "f92144ac7f0974541f275cce4fcc2e5f",
        "complexity": 12,
        "parameters": [
          "model",
          "train_loader",
          "val_loader",
          "ckpt_path"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "accelerator",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Accelerator",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "strategy",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Strategy",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "devices",
          ":",
          "Union",
          "[",
          "list",
          "[",
          "int",
          "]",
          ",",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "precision",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "32",
          "-",
          "true",
          "\"",
          ",",
          "plugins",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "callbacks",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "list",
          "[",
          "Any",
          "]",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "loggers",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "Logger",
          ",",
          "list",
          "[",
          "Logger",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          "max_epochs",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "1000",
          ",",
          "max_steps",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "None",
          ",",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "1",
          ",",
          "limit_train_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "limit_val_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "validation_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          "use_distributed_sampler",
          ":",
          "bool",
          "=",
          "True",
          ",",
          "checkpoint_dir",
          ":",
          "str",
          "=",
          "\"",
          ".",
          "/",
          "checkpoints",
          "\"",
          ",",
          "checkpoint_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Exemplary",
          "Trainer",
          "with",
          "Fabric",
          ".",
          "This",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          ".",
          "As",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          ",",
          "we",
          "recommend",
          "using",
          "the",
          ":",
          "class",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "Trainer",
          "`",
          ".",
          "Args",
          ":",
          "accelerator",
          ":",
          "The",
          "hardware",
          "to",
          "run",
          "on",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "cpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "cuda",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "mps",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "gpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "tpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "strategy",
          ":",
          "Strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "dp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp_spawn",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "deepspeed",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "fsdp",
          "\"",
          "`",
          "`",
          ".",
          "devices",
          ":",
          "Number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "int",
          "`",
          "`",
          ")",
          ",",
          "which",
          "GPUs",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "list",
          "`",
          "`",
          "or",
          "`",
          "`",
          "str",
          "`",
          "`",
          ")",
          ",",
          "or",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "The",
          "value",
          "applies",
          "per",
          "node",
          ".",
          "precision",
          ":",
          "Double",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "64",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "full",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "32",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "half",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "or",
          "bfloat16",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "bf16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ".",
          "plugins",
          ":",
          "One",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          ":",
          "A",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          ".",
          "The",
          "following",
          "hooks",
          "are",
          "supported",
          ":",
          "-",
          "on_train_epoch_start",
          "-",
          "on",
          "train_epoch_end",
          "-",
          "on_train_batch_start",
          "-",
          "on_train_batch_end",
          "-",
          "on_before_backward",
          "-",
          "on_after_backward",
          "-",
          "on_before_zero_grad",
          "-",
          "on_before_optimizer_step",
          "-",
          "on_validation_model_eval",
          "-",
          "on_validation_model_train",
          "-",
          "on_validation_epoch_start",
          "-",
          "on_validation_epoch_end",
          "-",
          "on_validation_batch_start",
          "-",
          "on_validation_batch_end",
          "loggers",
          ":",
          "A",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          ".",
          "See",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "fabric",
          ".",
          "fabric",
          ".",
          "Fabric",
          ".",
          "log",
          "`",
          "for",
          "more",
          "information",
          ".",
          "max_epochs",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "(",
          "optimizer",
          ")",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          ":",
          "How",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "limit_val_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "validation_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          ".",
          "use_distributed_sampler",
          ":",
          "Wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "-",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          ".",
          "checkpoint_dir",
          ":",
          "Directory",
          "to",
          "store",
          "checkpoints",
          "to",
          ".",
          "checkpoint_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          ".",
          "Warning",
          ":",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "(",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          ")",
          ",",
          "won",
          "'",
          "t",
          "work",
          "!",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          "=",
          "L",
          ".",
          "Fabric",
          "(",
          "accelerator",
          "=",
          "accelerator",
          ",",
          "strategy",
          "=",
          "strategy",
          ",",
          "devices",
          "=",
          "devices",
          ",",
          "precision",
          "=",
          "precision",
          ",",
          "plugins",
          "=",
          "plugins",
          ",",
          "callbacks",
          "=",
          "callbacks",
          ",",
          "loggers",
          "=",
          "loggers",
          ",",
          ")",
          "self",
          ".",
          "global_step",
          "=",
          "0",
          "self",
          ".",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "grad_accum_steps",
          "self",
          ".",
          "current_epoch",
          "=",
          "0",
          "self",
          ".",
          "max_epochs",
          "=",
          "max_epochs",
          "self",
          ".",
          "max_steps",
          "=",
          "max_steps",
          "self",
          ".",
          "should_stop",
          "=",
          "False",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_train_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_train_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_val_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_val_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "self",
          ".",
          "limit_train_batches",
          "=",
          "limit_train_batches",
          "self",
          ".",
          "limit_val_batches",
          "=",
          "limit_val_batches",
          "self",
          ".",
          "validation_frequency",
          "=",
          "validation_frequency",
          "self",
          ".",
          "use_distributed_sampler",
          "=",
          "use_distributed_sampler",
          "self",
          ".",
          "_current_train_return",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "_current_val_return",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "checkpoint_dir",
          "=",
          "checkpoint_dir",
          "self",
          ".",
          "checkpoint_frequency",
          "=",
          "checkpoint_frequency"
        ],
        "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!",
        "docstring_tokens": [
          "exemplary",
          "trainer",
          "with",
          "fabric",
          "this",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          "as",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          "we",
          "recommend",
          "using",
          "the",
          "class",
          "lightning",
          "pytorch",
          "trainer",
          "args",
          "accelerator",
          "the",
          "hardware",
          "to",
          "run",
          "on",
          "possible",
          "choices",
          "are",
          "cpu",
          "cuda",
          "mps",
          "gpu",
          "tpu",
          "auto",
          "strategy",
          "strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          "possible",
          "choices",
          "are",
          "dp",
          "ddp",
          "ddp_spawn",
          "deepspeed",
          "fsdp",
          "devices",
          "number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "int",
          "which",
          "gpus",
          "to",
          "train",
          "on",
          "list",
          "or",
          "str",
          "or",
          "auto",
          "the",
          "value",
          "applies",
          "per",
          "node",
          "precision",
          "double",
          "precision",
          "64",
          "full",
          "precision",
          "32",
          "half",
          "precision",
          "amp",
          "16",
          "mixed",
          "or",
          "bfloat16",
          "precision",
          "amp",
          "bf16",
          "mixed",
          "plugins",
          "one",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          "a",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          "the",
          "following",
          "hooks",
          "are",
          "supported",
          "on_train_epoch_start",
          "on",
          "train_epoch_end",
          "on_train_batch_start",
          "on_train_batch_end",
          "on_before_backward",
          "on_after_backward",
          "on_before_zero_grad",
          "on_before_optimizer_step",
          "on_validation_model_eval",
          "on_validation_model_train",
          "on_validation_epoch_start",
          "on_validation_epoch_end",
          "on_validation_batch_start",
          "on_validation_batch_end",
          "loggers",
          "a",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          "see",
          "meth",
          "lightning",
          "fabric",
          "fabric",
          "fabric",
          "log",
          "for",
          "more",
          "information",
          "max_epochs",
          "the",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          "the",
          "maximum",
          "number",
          "of",
          "optimizer",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          "how",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          "limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "limit_val_batches",
          "limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "validation_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          "use_distributed_sampler",
          "wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          "checkpoint_dir",
          "directory",
          "to",
          "store",
          "checkpoints",
          "to",
          "checkpoint_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          "warning",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          "won",
          "t",
          "work"
        ],
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 18,
        "end_line": 121,
        "hash": "0bbe0adf3ec60b2f294e15b93da70f56",
        "complexity": 3,
        "parameters": [
          "accelerator",
          "Accelerator]",
          "strategy",
          "Strategy]",
          "devices",
          "str",
          "int]",
          "precision",
          "int]",
          "plugins",
          "Any]]",
          "callbacks",
          "Any]]",
          "loggers",
          "list[Logger]]]",
          "max_epochs",
          "max_steps",
          "grad_accum_steps",
          "limit_train_batches",
          "float]",
          "limit_val_batches",
          "float]",
          "validation_frequency",
          "use_distributed_sampler",
          "checkpoint_dir",
          "checkpoint_frequency"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "val_loop",
        "original_string": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)",
        "language": "python",
        "code": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)",
        "code_tokens": [
          "def",
          "val_loop",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "val_loader",
          ":",
          "Optional",
          "[",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          "]",
          ",",
          "limit_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "validation",
          "loop",
          "running",
          "a",
          "single",
          "validation",
          "epoch",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "evaluate",
          "val_loader",
          ":",
          "The",
          "dataloader",
          "yielding",
          "the",
          "validation",
          "batches",
          ".",
          "limit_batches",
          ":",
          "Limits",
          "the",
          "batches",
          "during",
          "this",
          "validation",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "`",
          "`",
          "val_loader",
          "`",
          "`",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "\"",
          "\"",
          "\"",
          "if",
          "val_loader",
          "is",
          "None",
          ":",
          "return",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          "and",
          "not",
          "is_overridden",
          "(",
          "\"",
          "validation_step",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "rank_zero_warn",
          "(",
          "\"",
          "Your",
          "LightningModule",
          "does",
          "not",
          "have",
          "a",
          "validation_step",
          "implemented",
          ",",
          "\"",
          "\"",
          "but",
          "you",
          "passed",
          "a",
          "validation",
          "dataloder",
          ".",
          "Skipping",
          "Validation",
          ".",
          "\"",
          ")",
          "return",
          "if",
          "not",
          "is_overridden",
          "(",
          "\"",
          "on_validation_model_eval",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "model",
          ".",
          "eval",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_model_eval",
          "\"",
          ")",
          "torch",
          ".",
          "set_grad_enabled",
          "(",
          "False",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_epoch_start",
          "\"",
          ")",
          "iterable",
          "=",
          "self",
          ".",
          "progbar_wrapper",
          "(",
          "val_loader",
          ",",
          "total",
          "=",
          "min",
          "(",
          "len",
          "(",
          "val_loader",
          ")",
          ",",
          "limit_batches",
          ")",
          ",",
          "desc",
          "=",
          "\"",
          "Validation",
          "\"",
          ")",
          "for",
          "batch_idx",
          ",",
          "batch",
          "in",
          "enumerate",
          "(",
          "iterable",
          ")",
          ":",
          "if",
          "self",
          ".",
          "should_stop",
          "or",
          "batch_idx",
          ">",
          "=",
          "limit_batches",
          ":",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_batch_start",
          "\"",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "out",
          "=",
          "model",
          ".",
          "validation_step",
          "(",
          "batch",
          ",",
          "batch_idx",
          ")",
          "out",
          "=",
          "apply_to_collection",
          "(",
          "out",
          ",",
          "torch",
          ".",
          "Tensor",
          ",",
          "lambda",
          "x",
          ":",
          "x",
          ".",
          "detach",
          "(",
          ")",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_batch_end",
          "\"",
          ",",
          "out",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "self",
          ".",
          "_current_val_return",
          "=",
          "out",
          "self",
          ".",
          "_format_iterable",
          "(",
          "iterable",
          ",",
          "self",
          ".",
          "_current_val_return",
          ",",
          "\"",
          "val",
          "\"",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_epoch_end",
          "\"",
          ")",
          "if",
          "not",
          "is_overridden",
          "(",
          "\"",
          "on_validation_model_train",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "model",
          ".",
          "train",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_model_train",
          "\"",
          ")",
          "torch",
          ".",
          "set_grad_enabled",
          "(",
          "True",
          ")"
        ],
        "docstring": "The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.",
        "docstring_tokens": [
          "the",
          "validation",
          "loop",
          "running",
          "a",
          "single",
          "validation",
          "epoch",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "evaluate",
          "val_loader",
          "the",
          "dataloader",
          "yielding",
          "the",
          "validation",
          "batches",
          "limit_batches",
          "limits",
          "the",
          "batches",
          "during",
          "this",
          "validation",
          "epoch",
          "if",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "val_loader",
          "this",
          "has",
          "no",
          "effect"
        ],
        "docstring_summary": "The validation loop running a single validation epoch.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 261,
        "end_line": 321,
        "hash": "81622f9eb3301a1e1cd783d25137baf0",
        "complexity": 9,
        "parameters": [
          "model",
          "val_loader",
          "limit_batches",
          "float]"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": ".actions\\assistant.py",
        "func_name": "load_readme_description",
        "original_string": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r",
        "language": "python",
        "code": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r",
        "code_tokens": [
          "def",
          "load_readme_description",
          "(",
          "path_dir",
          ":",
          "str",
          ",",
          "homepage",
          ":",
          "str",
          ",",
          "version",
          ":",
          "str",
          ")",
          "-",
          ">",
          "str",
          ":",
          "\"",
          "\"",
          "\"",
          "Load",
          "readme",
          "as",
          "decribtion",
          ".",
          ">",
          ">",
          ">",
          "load_readme_description",
          "(",
          "_PROJECT_ROOT",
          ",",
          "\"",
          "\"",
          ",",
          "\"",
          "\"",
          ")",
          "'",
          ".",
          ".",
          ".",
          "PyTorch",
          "Lightning",
          "is",
          "just",
          "organized",
          "PyTorch",
          ".",
          ".",
          ".",
          "'",
          "\"",
          "\"",
          "\"",
          "path_readme",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "path_dir",
          ",",
          "\"",
          "README",
          ".",
          "md",
          "\"",
          ")",
          "with",
          "open",
          "(",
          "path_readme",
          ",",
          "encoding",
          "=",
          "\"",
          "utf",
          "-",
          "8",
          "\"",
          ")",
          "as",
          "fopen",
          ":",
          "text",
          "=",
          "fopen",
          ".",
          "read",
          "(",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "!",
          "[",
          "PT",
          "to",
          "PL",
          "]",
          "(",
          "docs",
          "/",
          "source",
          "-",
          "pytorch",
          "/",
          "_static",
          "/",
          "images",
          "/",
          "general",
          "/",
          "pl_quick_start_full_compressed",
          ".",
          "gif",
          ")",
          "\"",
          ",",
          "\"",
          "\"",
          ")",
          "github_source_url",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "homepage",
          ",",
          "\"",
          "raw",
          "\"",
          ",",
          "version",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "docs",
          "/",
          "source",
          "-",
          "pytorch",
          "/",
          "_static",
          "/",
          "\"",
          ",",
          "f",
          "\"",
          "{",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "github_source_url",
          ",",
          "'",
          "docs",
          "/",
          "source",
          "-",
          "app",
          "/",
          "_static",
          "/",
          "'",
          ")",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "badge",
          "/",
          "?",
          "version",
          "=",
          "stable",
          "\"",
          ",",
          "f",
          "\"",
          "badge",
          "/",
          "?",
          "version",
          "=",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "pytorch",
          "-",
          "lightning",
          ".",
          "readthedocs",
          ".",
          "io",
          "/",
          "en",
          "/",
          "stable",
          "/",
          "\"",
          ",",
          "f",
          "\"",
          "pytorch",
          "-",
          "lightning",
          ".",
          "readthedocs",
          ".",
          "io",
          "/",
          "en",
          "/",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "/",
          "branch",
          "/",
          "master",
          "/",
          "graph",
          "/",
          "badge",
          ".",
          "svg",
          "\"",
          ",",
          "f",
          "\"",
          "/",
          "release",
          "/",
          "{",
          "version",
          "}",
          "/",
          "graph",
          "/",
          "badge",
          ".",
          "svg",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "badge",
          ".",
          "svg",
          "?",
          "branch",
          "=",
          "master",
          "&",
          "event",
          "=",
          "push",
          "\"",
          ",",
          "f",
          "\"",
          "badge",
          ".",
          "svg",
          "?",
          "tag",
          "=",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "?",
          "branchName",
          "=",
          "master",
          "\"",
          ",",
          "f",
          "\"",
          "?",
          "branchName",
          "=",
          "refs",
          "%",
          "2Ftags",
          "%",
          "2F",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "skip_begin",
          "=",
          "r",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "following",
          "section",
          "will",
          "be",
          "skipped",
          "from",
          "PyPI",
          "description",
          "-",
          "-",
          ">",
          "\"",
          "skip_end",
          "=",
          "r",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "end",
          "skipping",
          "PyPI",
          "description",
          "-",
          "-",
          ">",
          "\"",
          "return",
          "re",
          ".",
          "sub",
          "(",
          "rf",
          "\"",
          "{",
          "skip_begin",
          "}",
          ".",
          "+",
          "?",
          "{",
          "skip_end",
          "}",
          "\"",
          ",",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "-",
          "-",
          ">",
          "\"",
          ",",
          "text",
          ",",
          "flags",
          "=",
          "re",
          ".",
          "IGNORECASE",
          "+",
          "re",
          ".",
          "DOTALL",
          ")"
        ],
        "docstring": "Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'",
        "docstring_tokens": [
          "load",
          "readme",
          "as",
          "decribtion",
          "load_readme_description",
          "_project_root",
          "doctest",
          "ellipsis",
          "normalize_whitespace",
          "pytorch",
          "lightning",
          "is",
          "just",
          "organized",
          "pytorch"
        ],
        "docstring_summary": "Load readme as decribtion.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py",
        "partition": "train",
        "function_type": "function",
        "start_line": 148,
        "end_line": 190,
        "hash": "0e616e657902720df027768a224df289",
        "complexity": 2,
        "parameters": [
          "path_dir",
          "homepage",
          "version"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "docs\\source-pytorch\\conf.py",
        "func_name": "package_list_from_file",
        "original_string": "def package_list_from_file(file):\r\n    \"\"\"List up package name (not containing version and extras) from a package list file.\"\"\"\r\n    mocked_packages = []\r\n    with open(file) as fp:\r\n        for ln in fp.readlines():\r\n            # Example: `tqdm>=4.41.0` => `tqdm`\r\n            # `[` is for package with extras\r\n            found = [ln.index(ch) for ch in list(\",=<>#[\") if ch in ln]\r\n            pkg = ln[: min(found)] if found else ln\r\n            if pkg.rstrip():\r\n                mocked_packages.append(pkg.rstrip())\r\n    return mocked_packages",
        "language": "python",
        "code": "def package_list_from_file(file):\r\n    \"\"\"List up package name (not containing version and extras) from a package list file.\"\"\"\r\n    mocked_packages = []\r\n    with open(file) as fp:\r\n        for ln in fp.readlines():\r\n            # Example: `tqdm>=4.41.0` => `tqdm`\r\n            # `[` is for package with extras\r\n            found = [ln.index(ch) for ch in list(\",=<>#[\") if ch in ln]\r\n            pkg = ln[: min(found)] if found else ln\r\n            if pkg.rstrip():\r\n                mocked_packages.append(pkg.rstrip())\r\n    return mocked_packages",
        "code_tokens": [
          "def",
          "package_list_from_file",
          "(",
          "file",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "List",
          "up",
          "package",
          "name",
          "(",
          "not",
          "containing",
          "version",
          "and",
          "extras",
          ")",
          "from",
          "a",
          "package",
          "list",
          "file",
          ".",
          "\"",
          "\"",
          "\"",
          "mocked_packages",
          "=",
          "[",
          "]",
          "with",
          "open",
          "(",
          "file",
          ")",
          "as",
          "fp",
          ":",
          "for",
          "ln",
          "in",
          "fp",
          ".",
          "readlines",
          "(",
          ")",
          ":",
          "found",
          "=",
          "[",
          "ln",
          ".",
          "index",
          "(",
          "ch",
          ")",
          "for",
          "ch",
          "in",
          "list",
          "(",
          "\"",
          ",",
          "=",
          "<",
          ">",
          "pkg",
          "=",
          "ln",
          "[",
          ":",
          "min",
          "(",
          "found",
          ")",
          "]",
          "if",
          "found",
          "else",
          "ln",
          "if",
          "pkg",
          ".",
          "rstrip",
          "(",
          ")",
          ":",
          "mocked_packages",
          ".",
          "append",
          "(",
          "pkg",
          ".",
          "rstrip",
          "(",
          ")",
          ")",
          "return",
          "mocked_packages"
        ],
        "docstring": "List up package name (not containing version and extras) from a package list file.",
        "docstring_tokens": [
          "list",
          "up",
          "package",
          "name",
          "not",
          "containing",
          "version",
          "and",
          "extras",
          "from",
          "a",
          "package",
          "list",
          "file"
        ],
        "docstring_summary": "List up package name (not containing version and extras) from a package list file.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/docs\\source-pytorch\\conf.py",
        "partition": "train",
        "function_type": "function",
        "start_line": 545,
        "end_line": 556,
        "hash": "dc734e60cda9c82d8febdd77a234a635",
        "complexity": 7,
        "parameters": [
          "file"
        ]
      }
    ]
  },
  {
    "query_id": 3,
    "query": "What parameters does the Lightning Trainer accept?",
    "relevant_docs": [
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "accelerator",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Accelerator",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "strategy",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Strategy",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "devices",
          ":",
          "Union",
          "[",
          "list",
          "[",
          "int",
          "]",
          ",",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "precision",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "32",
          "-",
          "true",
          "\"",
          ",",
          "plugins",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "callbacks",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "list",
          "[",
          "Any",
          "]",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "loggers",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "Logger",
          ",",
          "list",
          "[",
          "Logger",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          "max_epochs",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "1000",
          ",",
          "max_steps",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "None",
          ",",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "1",
          ",",
          "limit_train_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "limit_val_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "validation_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          "use_distributed_sampler",
          ":",
          "bool",
          "=",
          "True",
          ",",
          "checkpoint_dir",
          ":",
          "str",
          "=",
          "\"",
          ".",
          "/",
          "checkpoints",
          "\"",
          ",",
          "checkpoint_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Exemplary",
          "Trainer",
          "with",
          "Fabric",
          ".",
          "This",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          ".",
          "As",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          ",",
          "we",
          "recommend",
          "using",
          "the",
          ":",
          "class",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "Trainer",
          "`",
          ".",
          "Args",
          ":",
          "accelerator",
          ":",
          "The",
          "hardware",
          "to",
          "run",
          "on",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "cpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "cuda",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "mps",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "gpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "tpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "strategy",
          ":",
          "Strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "dp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp_spawn",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "deepspeed",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "fsdp",
          "\"",
          "`",
          "`",
          ".",
          "devices",
          ":",
          "Number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "int",
          "`",
          "`",
          ")",
          ",",
          "which",
          "GPUs",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "list",
          "`",
          "`",
          "or",
          "`",
          "`",
          "str",
          "`",
          "`",
          ")",
          ",",
          "or",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "The",
          "value",
          "applies",
          "per",
          "node",
          ".",
          "precision",
          ":",
          "Double",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "64",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "full",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "32",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "half",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "or",
          "bfloat16",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "bf16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ".",
          "plugins",
          ":",
          "One",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          ":",
          "A",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          ".",
          "The",
          "following",
          "hooks",
          "are",
          "supported",
          ":",
          "-",
          "on_train_epoch_start",
          "-",
          "on",
          "train_epoch_end",
          "-",
          "on_train_batch_start",
          "-",
          "on_train_batch_end",
          "-",
          "on_before_backward",
          "-",
          "on_after_backward",
          "-",
          "on_before_zero_grad",
          "-",
          "on_before_optimizer_step",
          "-",
          "on_validation_model_eval",
          "-",
          "on_validation_model_train",
          "-",
          "on_validation_epoch_start",
          "-",
          "on_validation_epoch_end",
          "-",
          "on_validation_batch_start",
          "-",
          "on_validation_batch_end",
          "loggers",
          ":",
          "A",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          ".",
          "See",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "fabric",
          ".",
          "fabric",
          ".",
          "Fabric",
          ".",
          "log",
          "`",
          "for",
          "more",
          "information",
          ".",
          "max_epochs",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "(",
          "optimizer",
          ")",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          ":",
          "How",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "limit_val_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "validation_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          ".",
          "use_distributed_sampler",
          ":",
          "Wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "-",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          ".",
          "checkpoint_dir",
          ":",
          "Directory",
          "to",
          "store",
          "checkpoints",
          "to",
          ".",
          "checkpoint_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          ".",
          "Warning",
          ":",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "(",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          ")",
          ",",
          "won",
          "'",
          "t",
          "work",
          "!",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          "=",
          "L",
          ".",
          "Fabric",
          "(",
          "accelerator",
          "=",
          "accelerator",
          ",",
          "strategy",
          "=",
          "strategy",
          ",",
          "devices",
          "=",
          "devices",
          ",",
          "precision",
          "=",
          "precision",
          ",",
          "plugins",
          "=",
          "plugins",
          ",",
          "callbacks",
          "=",
          "callbacks",
          ",",
          "loggers",
          "=",
          "loggers",
          ",",
          ")",
          "self",
          ".",
          "global_step",
          "=",
          "0",
          "self",
          ".",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "grad_accum_steps",
          "self",
          ".",
          "current_epoch",
          "=",
          "0",
          "self",
          ".",
          "max_epochs",
          "=",
          "max_epochs",
          "self",
          ".",
          "max_steps",
          "=",
          "max_steps",
          "self",
          ".",
          "should_stop",
          "=",
          "False",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_train_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_train_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_val_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_val_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "self",
          ".",
          "limit_train_batches",
          "=",
          "limit_train_batches",
          "self",
          ".",
          "limit_val_batches",
          "=",
          "limit_val_batches",
          "self",
          ".",
          "validation_frequency",
          "=",
          "validation_frequency",
          "self",
          ".",
          "use_distributed_sampler",
          "=",
          "use_distributed_sampler",
          "self",
          ".",
          "_current_train_return",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "_current_val_return",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "checkpoint_dir",
          "=",
          "checkpoint_dir",
          "self",
          ".",
          "checkpoint_frequency",
          "=",
          "checkpoint_frequency"
        ],
        "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!",
        "docstring_tokens": [
          "exemplary",
          "trainer",
          "with",
          "fabric",
          "this",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          "as",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          "we",
          "recommend",
          "using",
          "the",
          "class",
          "lightning",
          "pytorch",
          "trainer",
          "args",
          "accelerator",
          "the",
          "hardware",
          "to",
          "run",
          "on",
          "possible",
          "choices",
          "are",
          "cpu",
          "cuda",
          "mps",
          "gpu",
          "tpu",
          "auto",
          "strategy",
          "strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          "possible",
          "choices",
          "are",
          "dp",
          "ddp",
          "ddp_spawn",
          "deepspeed",
          "fsdp",
          "devices",
          "number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "int",
          "which",
          "gpus",
          "to",
          "train",
          "on",
          "list",
          "or",
          "str",
          "or",
          "auto",
          "the",
          "value",
          "applies",
          "per",
          "node",
          "precision",
          "double",
          "precision",
          "64",
          "full",
          "precision",
          "32",
          "half",
          "precision",
          "amp",
          "16",
          "mixed",
          "or",
          "bfloat16",
          "precision",
          "amp",
          "bf16",
          "mixed",
          "plugins",
          "one",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          "a",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          "the",
          "following",
          "hooks",
          "are",
          "supported",
          "on_train_epoch_start",
          "on",
          "train_epoch_end",
          "on_train_batch_start",
          "on_train_batch_end",
          "on_before_backward",
          "on_after_backward",
          "on_before_zero_grad",
          "on_before_optimizer_step",
          "on_validation_model_eval",
          "on_validation_model_train",
          "on_validation_epoch_start",
          "on_validation_epoch_end",
          "on_validation_batch_start",
          "on_validation_batch_end",
          "loggers",
          "a",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          "see",
          "meth",
          "lightning",
          "fabric",
          "fabric",
          "fabric",
          "log",
          "for",
          "more",
          "information",
          "max_epochs",
          "the",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          "the",
          "maximum",
          "number",
          "of",
          "optimizer",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          "how",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          "limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "limit_val_batches",
          "limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "validation_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          "use_distributed_sampler",
          "wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          "checkpoint_dir",
          "directory",
          "to",
          "store",
          "checkpoints",
          "to",
          "checkpoint_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          "warning",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          "won",
          "t",
          "work"
        ],
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 18,
        "end_line": 121,
        "hash": "0bbe0adf3ec60b2f294e15b93da70f56",
        "complexity": 3,
        "parameters": [
          "accelerator",
          "Accelerator]",
          "strategy",
          "Strategy]",
          "devices",
          "str",
          "int]",
          "precision",
          "int]",
          "plugins",
          "Any]]",
          "callbacks",
          "Any]]",
          "loggers",
          "list[Logger]]]",
          "max_epochs",
          "max_steps",
          "grad_accum_steps",
          "limit_train_batches",
          "float]",
          "limit_val_batches",
          "float]",
          "validation_frequency",
          "use_distributed_sampler",
          "checkpoint_dir",
          "checkpoint_frequency"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "fit",
        "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "language": "python",
        "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "code_tokens": [
          "def",
          "fit",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "val_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "ckpt_path",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          ",",
          "triggering",
          "the",
          "actual",
          "training",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          ".",
          "Can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          ":",
          "attr",
          ":",
          "`",
          "callbacks",
          "`",
          "(",
          "see",
          ":",
          "meth",
          ":",
          "`",
          "MyCustomTrainer",
          ".",
          "__init__",
          "`",
          ")",
          ".",
          "train_loader",
          ":",
          "the",
          "training",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "val_loader",
          ":",
          "the",
          "validation",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "If",
          "not",
          "specified",
          ",",
          "no",
          "validation",
          "will",
          "run",
          ".",
          "ckpt_path",
          ":",
          "Path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          ".",
          "If",
          "specified",
          ",",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "launch",
          "(",
          ")",
          "train_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "train_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          ":",
          "val_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "val_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "fabric",
          ".",
          "strategy",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "strategies",
          ".",
          "fsdp",
          ".",
          "FSDPStrategy",
          ")",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "currently",
          "does",
          "not",
          "support",
          "FSDP",
          "\"",
          ")",
          "optimizer",
          ",",
          "scheduler_cfg",
          "=",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "model",
          ".",
          "configure_optimizers",
          "(",
          ")",
          ")",
          "assert",
          "optimizer",
          "is",
          "not",
          "None",
          "model",
          ",",
          "optimizer",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup",
          "(",
          "model",
          ",",
          "optimizer",
          ")",
          "state",
          "=",
          "{",
          "\"",
          "model",
          "\"",
          ":",
          "model",
          ",",
          "\"",
          "optim",
          "\"",
          ":",
          "optimizer",
          ",",
          "\"",
          "scheduler",
          "\"",
          ":",
          "scheduler_cfg",
          "}",
          "if",
          "ckpt_path",
          "is",
          "not",
          "None",
          "and",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "ckpt_path",
          ")",
          ":",
          "latest_checkpoint_path",
          "=",
          "self",
          ".",
          "get_latest_checkpoint",
          "(",
          "self",
          ".",
          "checkpoint_dir",
          ")",
          "if",
          "latest_checkpoint_path",
          "is",
          "not",
          "None",
          ":",
          "self",
          ".",
          "load",
          "(",
          "state",
          ",",
          "latest_checkpoint_path",
          ")",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "while",
          "not",
          "self",
          ".",
          "should_stop",
          ":",
          "self",
          ".",
          "train_loop",
          "(",
          "model",
          ",",
          "optimizer",
          ",",
          "train_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_train_batches",
          ",",
          "scheduler_cfg",
          "=",
          "scheduler_cfg",
          ")",
          "if",
          "self",
          ".",
          "should_validate",
          ":",
          "self",
          ".",
          "val_loop",
          "(",
          "model",
          ",",
          "val_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_val_batches",
          ")",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "epoch",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "current_epoch",
          ")",
          "self",
          ".",
          "current_epoch",
          "+",
          "=",
          "1",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "self",
          ".",
          "save",
          "(",
          "state",
          ")",
          "self",
          ".",
          "should_stop",
          "=",
          "False"
        ],
        "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.",
        "docstring_tokens": [
          "the",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          "triggering",
          "the",
          "actual",
          "training",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          "attr",
          "callbacks",
          "see",
          "meth",
          "mycustomtrainer",
          "__init__",
          "train_loader",
          "the",
          "training",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "val_loader",
          "the",
          "validation",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "if",
          "not",
          "specified",
          "no",
          "validation",
          "will",
          "run",
          "ckpt_path",
          "path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          "if",
          "specified",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory"
        ],
        "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 123,
        "end_line": 191,
        "hash": "f92144ac7f0974541f275cce4fcc2e5f",
        "complexity": 12,
        "parameters": [
          "model",
          "train_loader",
          "val_loader",
          "ckpt_path"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "train_loop",
        "original_string": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")",
        "language": "python",
        "code": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")",
        "code_tokens": [
          "def",
          "train_loop",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "optimizer",
          ":",
          "torch",
          ".",
          "optim",
          ".",
          "Optimizer",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "limit_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "scheduler_cfg",
          ":",
          "Optional",
          "[",
          "Mapping",
          "[",
          "str",
          ",",
          "Union",
          "[",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "types",
          ".",
          "LRScheduler",
          ",",
          "bool",
          ",",
          "str",
          ",",
          "int",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "training",
          "loop",
          "running",
          "a",
          "single",
          "training",
          "epoch",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          "optimizer",
          ":",
          "the",
          "optimizer",
          ",",
          "optimizing",
          "the",
          "LightningModule",
          ".",
          "train_loader",
          ":",
          "The",
          "dataloader",
          "yielding",
          "the",
          "training",
          "batches",
          ".",
          "limit_batches",
          ":",
          "Limits",
          "the",
          "batches",
          "during",
          "this",
          "training",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "`",
          "`",
          "train_loader",
          "`",
          "`",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "scheduler_cfg",
          ":",
          "The",
          "learning",
          "rate",
          "scheduler",
          "configuration",
          ".",
          "Have",
          "a",
          "look",
          "at",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "pytorch",
          ".",
          "core",
          ".",
          "LightningModule",
          ".",
          "configure_optimizers",
          "`",
          "for",
          "supported",
          "values",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_epoch_start",
          "\"",
          ")",
          "iterable",
          "=",
          "self",
          ".",
          "progbar_wrapper",
          "(",
          "train_loader",
          ",",
          "total",
          "=",
          "min",
          "(",
          "len",
          "(",
          "train_loader",
          ")",
          ",",
          "limit_batches",
          ")",
          ",",
          "desc",
          "=",
          "f",
          "\"",
          "Epoch",
          "{",
          "self",
          ".",
          "current_epoch",
          "}",
          "\"",
          ")",
          "for",
          "batch_idx",
          ",",
          "batch",
          "in",
          "enumerate",
          "(",
          "iterable",
          ")",
          ":",
          "if",
          "self",
          ".",
          "should_stop",
          "or",
          "batch_idx",
          ">",
          "=",
          "limit_batches",
          ":",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_batch_start",
          "\"",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "should_optim_step",
          "=",
          "self",
          ".",
          "global_step",
          "%",
          "self",
          ".",
          "grad_accum_steps",
          "=",
          "=",
          "0",
          "if",
          "should_optim_step",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_before_optimizer_step",
          "\"",
          ",",
          "optimizer",
          ")",
          "optimizer",
          ".",
          "step",
          "(",
          "partial",
          "(",
          "self",
          ".",
          "training_step",
          ",",
          "model",
          "=",
          "model",
          ",",
          "batch",
          "=",
          "batch",
          ",",
          "batch_idx",
          "=",
          "batch_idx",
          ")",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_before_zero_grad",
          "\"",
          ",",
          "optimizer",
          ")",
          "optimizer",
          ".",
          "zero_grad",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "training_step",
          "(",
          "model",
          "=",
          "model",
          ",",
          "batch",
          "=",
          "batch",
          ",",
          "batch_idx",
          "=",
          "batch_idx",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_batch_end",
          "\"",
          ",",
          "self",
          ".",
          "_current_train_return",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "if",
          "should_optim_step",
          ":",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "step",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "global_step",
          ")",
          "self",
          ".",
          "_format_iterable",
          "(",
          "iterable",
          ",",
          "self",
          ".",
          "_current_train_return",
          ",",
          "\"",
          "train",
          "\"",
          ")",
          "self",
          ".",
          "global_step",
          "+",
          "=",
          "int",
          "(",
          "should_optim_step",
          ")",
          "if",
          "self",
          ".",
          "max_steps",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "global_step",
          ">",
          "=",
          "self",
          ".",
          "max_steps",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_train_epoch_end",
          "\"",
          ")"
        ],
        "docstring": "The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.",
        "docstring_tokens": [
          "the",
          "training",
          "loop",
          "running",
          "a",
          "single",
          "training",
          "epoch",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "optimizer",
          "the",
          "optimizer",
          "optimizing",
          "the",
          "lightningmodule",
          "train_loader",
          "the",
          "dataloader",
          "yielding",
          "the",
          "training",
          "batches",
          "limit_batches",
          "limits",
          "the",
          "batches",
          "during",
          "this",
          "training",
          "epoch",
          "if",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "train_loader",
          "this",
          "has",
          "no",
          "effect",
          "scheduler_cfg",
          "the",
          "learning",
          "rate",
          "scheduler",
          "configuration",
          "have",
          "a",
          "look",
          "at",
          "meth",
          "lightning",
          "pytorch",
          "core",
          "lightningmodule",
          "configure_optimizers",
          "for",
          "supported",
          "values"
        ],
        "docstring_summary": "The training loop running a single training epoch.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 193,
        "end_line": 259,
        "hash": "7dd4ca71b837d6f688303a3e5eb5fe82",
        "complexity": 8,
        "parameters": [
          "model",
          "optimizer",
          "train_loader",
          "limit_batches",
          "float]",
          "scheduler_cfg",
          "Union[L.fabric.utilities.types.LRScheduler",
          "bool",
          "str",
          "int]]]"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "val_loop",
        "original_string": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)",
        "language": "python",
        "code": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)",
        "code_tokens": [
          "def",
          "val_loop",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "val_loader",
          ":",
          "Optional",
          "[",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          "]",
          ",",
          "limit_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "validation",
          "loop",
          "running",
          "a",
          "single",
          "validation",
          "epoch",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "evaluate",
          "val_loader",
          ":",
          "The",
          "dataloader",
          "yielding",
          "the",
          "validation",
          "batches",
          ".",
          "limit_batches",
          ":",
          "Limits",
          "the",
          "batches",
          "during",
          "this",
          "validation",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "`",
          "`",
          "val_loader",
          "`",
          "`",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "\"",
          "\"",
          "\"",
          "if",
          "val_loader",
          "is",
          "None",
          ":",
          "return",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          "and",
          "not",
          "is_overridden",
          "(",
          "\"",
          "validation_step",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "L",
          ".",
          "fabric",
          ".",
          "utilities",
          ".",
          "rank_zero_warn",
          "(",
          "\"",
          "Your",
          "LightningModule",
          "does",
          "not",
          "have",
          "a",
          "validation_step",
          "implemented",
          ",",
          "\"",
          "\"",
          "but",
          "you",
          "passed",
          "a",
          "validation",
          "dataloder",
          ".",
          "Skipping",
          "Validation",
          ".",
          "\"",
          ")",
          "return",
          "if",
          "not",
          "is_overridden",
          "(",
          "\"",
          "on_validation_model_eval",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "model",
          ".",
          "eval",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_model_eval",
          "\"",
          ")",
          "torch",
          ".",
          "set_grad_enabled",
          "(",
          "False",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_epoch_start",
          "\"",
          ")",
          "iterable",
          "=",
          "self",
          ".",
          "progbar_wrapper",
          "(",
          "val_loader",
          ",",
          "total",
          "=",
          "min",
          "(",
          "len",
          "(",
          "val_loader",
          ")",
          ",",
          "limit_batches",
          ")",
          ",",
          "desc",
          "=",
          "\"",
          "Validation",
          "\"",
          ")",
          "for",
          "batch_idx",
          ",",
          "batch",
          "in",
          "enumerate",
          "(",
          "iterable",
          ")",
          ":",
          "if",
          "self",
          ".",
          "should_stop",
          "or",
          "batch_idx",
          ">",
          "=",
          "limit_batches",
          ":",
          "break",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_batch_start",
          "\"",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "out",
          "=",
          "model",
          ".",
          "validation_step",
          "(",
          "batch",
          ",",
          "batch_idx",
          ")",
          "out",
          "=",
          "apply_to_collection",
          "(",
          "out",
          ",",
          "torch",
          ".",
          "Tensor",
          ",",
          "lambda",
          "x",
          ":",
          "x",
          ".",
          "detach",
          "(",
          ")",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_batch_end",
          "\"",
          ",",
          "out",
          ",",
          "batch",
          ",",
          "batch_idx",
          ")",
          "self",
          ".",
          "_current_val_return",
          "=",
          "out",
          "self",
          ".",
          "_format_iterable",
          "(",
          "iterable",
          ",",
          "self",
          ".",
          "_current_val_return",
          ",",
          "\"",
          "val",
          "\"",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_epoch_end",
          "\"",
          ")",
          "if",
          "not",
          "is_overridden",
          "(",
          "\"",
          "on_validation_model_train",
          "\"",
          ",",
          "_unwrap_objects",
          "(",
          "model",
          ")",
          ")",
          ":",
          "model",
          ".",
          "train",
          "(",
          ")",
          "else",
          ":",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_validation_model_train",
          "\"",
          ")",
          "torch",
          ".",
          "set_grad_enabled",
          "(",
          "True",
          ")"
        ],
        "docstring": "The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.",
        "docstring_tokens": [
          "the",
          "validation",
          "loop",
          "running",
          "a",
          "single",
          "validation",
          "epoch",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "evaluate",
          "val_loader",
          "the",
          "dataloader",
          "yielding",
          "the",
          "validation",
          "batches",
          "limit_batches",
          "limits",
          "the",
          "batches",
          "during",
          "this",
          "validation",
          "epoch",
          "if",
          "greater",
          "than",
          "the",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "val_loader",
          "this",
          "has",
          "no",
          "effect"
        ],
        "docstring_summary": "The validation loop running a single validation epoch.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 261,
        "end_line": 321,
        "hash": "81622f9eb3301a1e1cd783d25137baf0",
        "complexity": 9,
        "parameters": [
          "model",
          "val_loader",
          "limit_batches",
          "float]"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "training_step",
        "original_string": "def training_step(self, model: L.LightningModule, batch: Any, batch_idx: int) -> torch.Tensor:\r\n        \"\"\"A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch\r\n\r\n        \"\"\"\r\n        outputs: Union[torch.Tensor, Mapping[str, Any]] = model.training_step(batch, batch_idx=batch_idx)\r\n\r\n        loss = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n\r\n        self.fabric.call(\"on_before_backward\", loss)\r\n        self.fabric.backward(loss)\r\n        self.fabric.call(\"on_after_backward\")\r\n\r\n        # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n        self._current_train_return = apply_to_collection(outputs, dtype=torch.Tensor, function=lambda x: x.detach())\r\n\r\n        return loss",
        "language": "python",
        "code": "def training_step(self, model: L.LightningModule, batch: Any, batch_idx: int) -> torch.Tensor:\r\n        \"\"\"A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch\r\n\r\n        \"\"\"\r\n        outputs: Union[torch.Tensor, Mapping[str, Any]] = model.training_step(batch, batch_idx=batch_idx)\r\n\r\n        loss = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n\r\n        self.fabric.call(\"on_before_backward\", loss)\r\n        self.fabric.backward(loss)\r\n        self.fabric.call(\"on_after_backward\")\r\n\r\n        # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n        self._current_train_return = apply_to_collection(outputs, dtype=torch.Tensor, function=lambda x: x.detach())\r\n\r\n        return loss",
        "code_tokens": [
          "def",
          "training_step",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "batch",
          ":",
          "Any",
          ",",
          "batch_idx",
          ":",
          "int",
          ")",
          "-",
          ">",
          "torch",
          ".",
          "Tensor",
          ":",
          "\"",
          "\"",
          "\"",
          "A",
          "single",
          "training",
          "step",
          ",",
          "running",
          "forward",
          "and",
          "backward",
          ".",
          "The",
          "optimizer",
          "step",
          "is",
          "called",
          "separately",
          ",",
          "as",
          "this",
          "is",
          "given",
          "as",
          "a",
          "closure",
          "to",
          "the",
          "optimizer",
          "step",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "lightning",
          "module",
          "to",
          "train",
          "batch",
          ":",
          "the",
          "batch",
          "to",
          "run",
          "the",
          "forward",
          "on",
          "batch_idx",
          ":",
          "index",
          "of",
          "the",
          "current",
          "batch",
          "w",
          ".",
          "r",
          ".",
          "t",
          "the",
          "current",
          "epoch",
          "\"",
          "\"",
          "\"",
          "outputs",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "model",
          ".",
          "training_step",
          "(",
          "batch",
          ",",
          "batch_idx",
          "=",
          "batch_idx",
          ")",
          "loss",
          "=",
          "outputs",
          "if",
          "isinstance",
          "(",
          "outputs",
          ",",
          "torch",
          ".",
          "Tensor",
          ")",
          "else",
          "outputs",
          "[",
          "\"",
          "loss",
          "\"",
          "]",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_before_backward",
          "\"",
          ",",
          "loss",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "backward",
          "(",
          "loss",
          ")",
          "self",
          ".",
          "fabric",
          ".",
          "call",
          "(",
          "\"",
          "on_after_backward",
          "\"",
          ")",
          "self",
          ".",
          "_current_train_return",
          "=",
          "apply_to_collection",
          "(",
          "outputs",
          ",",
          "dtype",
          "=",
          "torch",
          ".",
          "Tensor",
          ",",
          "function",
          "=",
          "lambda",
          "x",
          ":",
          "x",
          ".",
          "detach",
          "(",
          ")",
          ")",
          "return",
          "loss"
        ],
        "docstring": "A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch",
        "docstring_tokens": [
          "a",
          "single",
          "training",
          "step",
          "running",
          "forward",
          "and",
          "backward",
          "the",
          "optimizer",
          "step",
          "is",
          "called",
          "separately",
          "as",
          "this",
          "is",
          "given",
          "as",
          "a",
          "closure",
          "to",
          "the",
          "optimizer",
          "step",
          "args",
          "model",
          "the",
          "lightning",
          "module",
          "to",
          "train",
          "batch",
          "the",
          "batch",
          "to",
          "run",
          "the",
          "forward",
          "on",
          "batch_idx",
          "index",
          "of",
          "the",
          "current",
          "batch",
          "w",
          "r",
          "t",
          "the",
          "current",
          "epoch"
        ],
        "docstring_summary": "A single training step, running forward and backward. The optimizer step is called separately, as this is",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 323,
        "end_line": 344,
        "hash": "4b5f46df8382a263e27e1522f29db870",
        "complexity": 2,
        "parameters": [
          "model",
          "batch",
          "batch_idx"
        ]
      }
    ]
  },
  {
    "query_id": 4,
    "query": "How to use early stopping callback in PyTorch Lightning?",
    "relevant_docs": [
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "accelerator",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Accelerator",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "strategy",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Strategy",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "devices",
          ":",
          "Union",
          "[",
          "list",
          "[",
          "int",
          "]",
          ",",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "precision",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "32",
          "-",
          "true",
          "\"",
          ",",
          "plugins",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "callbacks",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "list",
          "[",
          "Any",
          "]",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "loggers",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "Logger",
          ",",
          "list",
          "[",
          "Logger",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          "max_epochs",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "1000",
          ",",
          "max_steps",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "None",
          ",",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "1",
          ",",
          "limit_train_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "limit_val_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "validation_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          "use_distributed_sampler",
          ":",
          "bool",
          "=",
          "True",
          ",",
          "checkpoint_dir",
          ":",
          "str",
          "=",
          "\"",
          ".",
          "/",
          "checkpoints",
          "\"",
          ",",
          "checkpoint_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Exemplary",
          "Trainer",
          "with",
          "Fabric",
          ".",
          "This",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          ".",
          "As",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          ",",
          "we",
          "recommend",
          "using",
          "the",
          ":",
          "class",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "Trainer",
          "`",
          ".",
          "Args",
          ":",
          "accelerator",
          ":",
          "The",
          "hardware",
          "to",
          "run",
          "on",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "cpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "cuda",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "mps",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "gpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "tpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "strategy",
          ":",
          "Strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "dp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp_spawn",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "deepspeed",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "fsdp",
          "\"",
          "`",
          "`",
          ".",
          "devices",
          ":",
          "Number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "int",
          "`",
          "`",
          ")",
          ",",
          "which",
          "GPUs",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "list",
          "`",
          "`",
          "or",
          "`",
          "`",
          "str",
          "`",
          "`",
          ")",
          ",",
          "or",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "The",
          "value",
          "applies",
          "per",
          "node",
          ".",
          "precision",
          ":",
          "Double",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "64",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "full",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "32",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "half",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "or",
          "bfloat16",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "bf16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ".",
          "plugins",
          ":",
          "One",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          ":",
          "A",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          ".",
          "The",
          "following",
          "hooks",
          "are",
          "supported",
          ":",
          "-",
          "on_train_epoch_start",
          "-",
          "on",
          "train_epoch_end",
          "-",
          "on_train_batch_start",
          "-",
          "on_train_batch_end",
          "-",
          "on_before_backward",
          "-",
          "on_after_backward",
          "-",
          "on_before_zero_grad",
          "-",
          "on_before_optimizer_step",
          "-",
          "on_validation_model_eval",
          "-",
          "on_validation_model_train",
          "-",
          "on_validation_epoch_start",
          "-",
          "on_validation_epoch_end",
          "-",
          "on_validation_batch_start",
          "-",
          "on_validation_batch_end",
          "loggers",
          ":",
          "A",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          ".",
          "See",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "fabric",
          ".",
          "fabric",
          ".",
          "Fabric",
          ".",
          "log",
          "`",
          "for",
          "more",
          "information",
          ".",
          "max_epochs",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "(",
          "optimizer",
          ")",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          ":",
          "How",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "limit_val_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "validation_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          ".",
          "use_distributed_sampler",
          ":",
          "Wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "-",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          ".",
          "checkpoint_dir",
          ":",
          "Directory",
          "to",
          "store",
          "checkpoints",
          "to",
          ".",
          "checkpoint_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          ".",
          "Warning",
          ":",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "(",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          ")",
          ",",
          "won",
          "'",
          "t",
          "work",
          "!",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          "=",
          "L",
          ".",
          "Fabric",
          "(",
          "accelerator",
          "=",
          "accelerator",
          ",",
          "strategy",
          "=",
          "strategy",
          ",",
          "devices",
          "=",
          "devices",
          ",",
          "precision",
          "=",
          "precision",
          ",",
          "plugins",
          "=",
          "plugins",
          ",",
          "callbacks",
          "=",
          "callbacks",
          ",",
          "loggers",
          "=",
          "loggers",
          ",",
          ")",
          "self",
          ".",
          "global_step",
          "=",
          "0",
          "self",
          ".",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "grad_accum_steps",
          "self",
          ".",
          "current_epoch",
          "=",
          "0",
          "self",
          ".",
          "max_epochs",
          "=",
          "max_epochs",
          "self",
          ".",
          "max_steps",
          "=",
          "max_steps",
          "self",
          ".",
          "should_stop",
          "=",
          "False",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_train_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_train_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_val_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_val_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "self",
          ".",
          "limit_train_batches",
          "=",
          "limit_train_batches",
          "self",
          ".",
          "limit_val_batches",
          "=",
          "limit_val_batches",
          "self",
          ".",
          "validation_frequency",
          "=",
          "validation_frequency",
          "self",
          ".",
          "use_distributed_sampler",
          "=",
          "use_distributed_sampler",
          "self",
          ".",
          "_current_train_return",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "_current_val_return",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "checkpoint_dir",
          "=",
          "checkpoint_dir",
          "self",
          ".",
          "checkpoint_frequency",
          "=",
          "checkpoint_frequency"
        ],
        "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!",
        "docstring_tokens": [
          "exemplary",
          "trainer",
          "with",
          "fabric",
          "this",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          "as",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          "we",
          "recommend",
          "using",
          "the",
          "class",
          "lightning",
          "pytorch",
          "trainer",
          "args",
          "accelerator",
          "the",
          "hardware",
          "to",
          "run",
          "on",
          "possible",
          "choices",
          "are",
          "cpu",
          "cuda",
          "mps",
          "gpu",
          "tpu",
          "auto",
          "strategy",
          "strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          "possible",
          "choices",
          "are",
          "dp",
          "ddp",
          "ddp_spawn",
          "deepspeed",
          "fsdp",
          "devices",
          "number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "int",
          "which",
          "gpus",
          "to",
          "train",
          "on",
          "list",
          "or",
          "str",
          "or",
          "auto",
          "the",
          "value",
          "applies",
          "per",
          "node",
          "precision",
          "double",
          "precision",
          "64",
          "full",
          "precision",
          "32",
          "half",
          "precision",
          "amp",
          "16",
          "mixed",
          "or",
          "bfloat16",
          "precision",
          "amp",
          "bf16",
          "mixed",
          "plugins",
          "one",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          "a",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          "the",
          "following",
          "hooks",
          "are",
          "supported",
          "on_train_epoch_start",
          "on",
          "train_epoch_end",
          "on_train_batch_start",
          "on_train_batch_end",
          "on_before_backward",
          "on_after_backward",
          "on_before_zero_grad",
          "on_before_optimizer_step",
          "on_validation_model_eval",
          "on_validation_model_train",
          "on_validation_epoch_start",
          "on_validation_epoch_end",
          "on_validation_batch_start",
          "on_validation_batch_end",
          "loggers",
          "a",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          "see",
          "meth",
          "lightning",
          "fabric",
          "fabric",
          "fabric",
          "log",
          "for",
          "more",
          "information",
          "max_epochs",
          "the",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          "the",
          "maximum",
          "number",
          "of",
          "optimizer",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          "how",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          "limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "limit_val_batches",
          "limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "validation_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          "use_distributed_sampler",
          "wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          "checkpoint_dir",
          "directory",
          "to",
          "store",
          "checkpoints",
          "to",
          "checkpoint_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          "warning",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          "won",
          "t",
          "work"
        ],
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 18,
        "end_line": 121,
        "hash": "0bbe0adf3ec60b2f294e15b93da70f56",
        "complexity": 3,
        "parameters": [
          "accelerator",
          "Accelerator]",
          "strategy",
          "Strategy]",
          "devices",
          "str",
          "int]",
          "precision",
          "int]",
          "plugins",
          "Any]]",
          "callbacks",
          "Any]]",
          "loggers",
          "list[Logger]]]",
          "max_epochs",
          "max_steps",
          "grad_accum_steps",
          "limit_train_batches",
          "float]",
          "limit_val_batches",
          "float]",
          "validation_frequency",
          "use_distributed_sampler",
          "checkpoint_dir",
          "checkpoint_frequency"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": ".actions\\assistant.py",
        "func_name": "load_readme_description",
        "original_string": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r",
        "language": "python",
        "code": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r",
        "code_tokens": [
          "def",
          "load_readme_description",
          "(",
          "path_dir",
          ":",
          "str",
          ",",
          "homepage",
          ":",
          "str",
          ",",
          "version",
          ":",
          "str",
          ")",
          "-",
          ">",
          "str",
          ":",
          "\"",
          "\"",
          "\"",
          "Load",
          "readme",
          "as",
          "decribtion",
          ".",
          ">",
          ">",
          ">",
          "load_readme_description",
          "(",
          "_PROJECT_ROOT",
          ",",
          "\"",
          "\"",
          ",",
          "\"",
          "\"",
          ")",
          "'",
          ".",
          ".",
          ".",
          "PyTorch",
          "Lightning",
          "is",
          "just",
          "organized",
          "PyTorch",
          ".",
          ".",
          ".",
          "'",
          "\"",
          "\"",
          "\"",
          "path_readme",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "path_dir",
          ",",
          "\"",
          "README",
          ".",
          "md",
          "\"",
          ")",
          "with",
          "open",
          "(",
          "path_readme",
          ",",
          "encoding",
          "=",
          "\"",
          "utf",
          "-",
          "8",
          "\"",
          ")",
          "as",
          "fopen",
          ":",
          "text",
          "=",
          "fopen",
          ".",
          "read",
          "(",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "!",
          "[",
          "PT",
          "to",
          "PL",
          "]",
          "(",
          "docs",
          "/",
          "source",
          "-",
          "pytorch",
          "/",
          "_static",
          "/",
          "images",
          "/",
          "general",
          "/",
          "pl_quick_start_full_compressed",
          ".",
          "gif",
          ")",
          "\"",
          ",",
          "\"",
          "\"",
          ")",
          "github_source_url",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "homepage",
          ",",
          "\"",
          "raw",
          "\"",
          ",",
          "version",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "docs",
          "/",
          "source",
          "-",
          "pytorch",
          "/",
          "_static",
          "/",
          "\"",
          ",",
          "f",
          "\"",
          "{",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "github_source_url",
          ",",
          "'",
          "docs",
          "/",
          "source",
          "-",
          "app",
          "/",
          "_static",
          "/",
          "'",
          ")",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "badge",
          "/",
          "?",
          "version",
          "=",
          "stable",
          "\"",
          ",",
          "f",
          "\"",
          "badge",
          "/",
          "?",
          "version",
          "=",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "pytorch",
          "-",
          "lightning",
          ".",
          "readthedocs",
          ".",
          "io",
          "/",
          "en",
          "/",
          "stable",
          "/",
          "\"",
          ",",
          "f",
          "\"",
          "pytorch",
          "-",
          "lightning",
          ".",
          "readthedocs",
          ".",
          "io",
          "/",
          "en",
          "/",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "/",
          "branch",
          "/",
          "master",
          "/",
          "graph",
          "/",
          "badge",
          ".",
          "svg",
          "\"",
          ",",
          "f",
          "\"",
          "/",
          "release",
          "/",
          "{",
          "version",
          "}",
          "/",
          "graph",
          "/",
          "badge",
          ".",
          "svg",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "badge",
          ".",
          "svg",
          "?",
          "branch",
          "=",
          "master",
          "&",
          "event",
          "=",
          "push",
          "\"",
          ",",
          "f",
          "\"",
          "badge",
          ".",
          "svg",
          "?",
          "tag",
          "=",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "text",
          "=",
          "text",
          ".",
          "replace",
          "(",
          "\"",
          "?",
          "branchName",
          "=",
          "master",
          "\"",
          ",",
          "f",
          "\"",
          "?",
          "branchName",
          "=",
          "refs",
          "%",
          "2Ftags",
          "%",
          "2F",
          "{",
          "version",
          "}",
          "\"",
          ")",
          "skip_begin",
          "=",
          "r",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "following",
          "section",
          "will",
          "be",
          "skipped",
          "from",
          "PyPI",
          "description",
          "-",
          "-",
          ">",
          "\"",
          "skip_end",
          "=",
          "r",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "end",
          "skipping",
          "PyPI",
          "description",
          "-",
          "-",
          ">",
          "\"",
          "return",
          "re",
          ".",
          "sub",
          "(",
          "rf",
          "\"",
          "{",
          "skip_begin",
          "}",
          ".",
          "+",
          "?",
          "{",
          "skip_end",
          "}",
          "\"",
          ",",
          "\"",
          "<",
          "!",
          "-",
          "-",
          "-",
          "-",
          ">",
          "\"",
          ",",
          "text",
          ",",
          "flags",
          "=",
          "re",
          ".",
          "IGNORECASE",
          "+",
          "re",
          ".",
          "DOTALL",
          ")"
        ],
        "docstring": "Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'",
        "docstring_tokens": [
          "load",
          "readme",
          "as",
          "decribtion",
          "load_readme_description",
          "_project_root",
          "doctest",
          "ellipsis",
          "normalize_whitespace",
          "pytorch",
          "lightning",
          "is",
          "just",
          "organized",
          "pytorch"
        ],
        "docstring_summary": "Load readme as decribtion.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py",
        "partition": "train",
        "function_type": "function",
        "start_line": 148,
        "end_line": 190,
        "hash": "0e616e657902720df027768a224df289",
        "complexity": 2,
        "parameters": [
          "path_dir",
          "homepage",
          "version"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": ".actions\\assistant.py",
        "func_name": "_replace_imports",
        "original_string": "def _replace_imports(lines: list[str], mapping: list[tuple[str, str]], lightning_by: str = \"\") -> list[str]:\r\n    \"\"\"Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']\r\n\r\n    \"\"\"\r\n    out = lines[:]\r\n    for source_import, target_import in mapping:\r\n        for i, ln in enumerate(out):\r\n            out[i] = re.sub(\r\n                rf\"([^_/@]|^){source_import}([^_\\w/]|$)\",\r\n                rf\"\\1{target_import}\\2\",\r\n                ln,\r\n            )\r\n            if lightning_by:  # in addition, replace base package\r\n                out[i] = out[i].replace(\"from lightning import \", f\"from {lightning_by} import \")\r\n                out[i] = out[i].replace(\"import lightning \", f\"import {lightning_by} \")\r\n    return out",
        "language": "python",
        "code": "def _replace_imports(lines: list[str], mapping: list[tuple[str, str]], lightning_by: str = \"\") -> list[str]:\r\n    \"\"\"Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']\r\n\r\n    \"\"\"\r\n    out = lines[:]\r\n    for source_import, target_import in mapping:\r\n        for i, ln in enumerate(out):\r\n            out[i] = re.sub(\r\n                rf\"([^_/@]|^){source_import}([^_\\w/]|$)\",\r\n                rf\"\\1{target_import}\\2\",\r\n                ln,\r\n            )\r\n            if lightning_by:  # in addition, replace base package\r\n                out[i] = out[i].replace(\"from lightning import \", f\"from {lightning_by} import \")\r\n                out[i] = out[i].replace(\"import lightning \", f\"import {lightning_by} \")\r\n    return out",
        "code_tokens": [
          "def",
          "_replace_imports",
          "(",
          "lines",
          ":",
          "list",
          "[",
          "str",
          "]",
          ",",
          "mapping",
          ":",
          "list",
          "[",
          "tuple",
          "[",
          "str",
          ",",
          "str",
          "]",
          "]",
          ",",
          "lightning_by",
          ":",
          "str",
          "=",
          "\"",
          "\"",
          ")",
          "-",
          ">",
          "list",
          "[",
          "str",
          "]",
          ":",
          "\"",
          "\"",
          "\"",
          "Replace",
          "imports",
          "of",
          "standalone",
          "package",
          "to",
          "lightning",
          ".",
          ">",
          ">",
          ">",
          "lns",
          "=",
          "[",
          ".",
          ".",
          ".",
          "'",
          "\"",
          "lightning_app",
          "\"",
          "'",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "lightning_app",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "lightning_app",
          "/",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "delete_cloud_lightning_apps",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "from",
          "lightning_app",
          "import",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "lightning_apps",
          "=",
          "[",
          "]",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "lightning_app",
          "and",
          "pytorch_lightning",
          "are",
          "ours",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "def",
          "_lightning_app",
          "(",
          ")",
          ":",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          ":",
          "class",
          ":",
          "`",
          "~",
          "lightning_app",
          ".",
          "core",
          ".",
          "flow",
          ".",
          "LightningFlow",
          "`",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "http",
          ":",
          "/",
          "/",
          "pytorch_lightning",
          ".",
          "ai",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "from",
          "lightning",
          "import",
          "__version__",
          "\"",
          ",",
          ".",
          ".",
          ".",
          "\"",
          "@",
          "lightning",
          ".",
          "ai",
          "\"",
          ".",
          ".",
          ".",
          "]",
          ">",
          ">",
          ">",
          "mapping",
          "=",
          "[",
          "(",
          "\"",
          "lightning_app",
          "\"",
          ",",
          "\"",
          "lightning",
          ".",
          "app",
          "\"",
          ")",
          ",",
          "(",
          "\"",
          "pytorch_lightning",
          "\"",
          ",",
          "\"",
          "lightning",
          ".",
          "pytorch",
          "\"",
          ")",
          "]",
          ">",
          ">",
          ">",
          "_replace_imports",
          "(",
          "lns",
          ",",
          "mapping",
          ",",
          "lightning_by",
          "=",
          "\"",
          "lightning_fabric",
          "\"",
          ")",
          "[",
          "'",
          "\"",
          "lightning",
          ".",
          "app",
          "\"",
          "'",
          ",",
          "\\",
          "'",
          "lightning",
          ".",
          "app",
          "'",
          ",",
          "\\",
          "'",
          "lightning_app",
          "/",
          "'",
          ",",
          "\\",
          "'",
          "delete_cloud_lightning_apps",
          "'",
          ",",
          "\\",
          "'",
          "from",
          "lightning",
          ".",
          "app",
          "import",
          "'",
          ",",
          "\\",
          "'",
          "lightning_apps",
          "=",
          "[",
          "]",
          "'",
          ",",
          "\\",
          "'",
          "lightning",
          ".",
          "app",
          "and",
          "lightning",
          ".",
          "pytorch",
          "are",
          "ours",
          "'",
          ",",
          "\\",
          "'",
          "def",
          "_lightning_app",
          "(",
          ")",
          ":",
          "'",
          ",",
          "\\",
          "'",
          ":",
          "class",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "app",
          ".",
          "core",
          ".",
          "flow",
          ".",
          "LightningFlow",
          "`",
          "'",
          ",",
          "\\",
          "'",
          "http",
          ":",
          "/",
          "/",
          "pytorch_lightning",
          ".",
          "ai",
          "'",
          ",",
          "\\",
          "'",
          "from",
          "lightning_fabric",
          "import",
          "__version__",
          "'",
          ",",
          "\\",
          "'",
          "@",
          "lightning",
          ".",
          "ai",
          "'",
          "]",
          "\"",
          "\"",
          "\"",
          "out",
          "=",
          "lines",
          "[",
          ":",
          "]",
          "for",
          "source_import",
          ",",
          "target_import",
          "in",
          "mapping",
          ":",
          "for",
          "i",
          ",",
          "ln",
          "in",
          "enumerate",
          "(",
          "out",
          ")",
          ":",
          "out",
          "[",
          "i",
          "]",
          "=",
          "re",
          ".",
          "sub",
          "(",
          "rf",
          "\"",
          "(",
          "[",
          "^",
          "_",
          "/",
          "@",
          "]",
          "|",
          "^",
          ")",
          "{",
          "source_import",
          "}",
          "(",
          "[",
          "^",
          "_",
          "\\",
          "w",
          "/",
          "]",
          "|",
          "$",
          ")",
          "\"",
          ",",
          "rf",
          "\"",
          "\\",
          "1",
          "{",
          "target_import",
          "}",
          "\\",
          "2",
          "\"",
          ",",
          "ln",
          ",",
          ")",
          "if",
          "lightning_by",
          ":",
          "out",
          "[",
          "i",
          "]",
          "=",
          "out",
          "[",
          "i",
          "]",
          ".",
          "replace",
          "(",
          "\"",
          "from",
          "lightning",
          "import",
          "\"",
          ",",
          "f",
          "\"",
          "from",
          "{",
          "lightning_by",
          "}",
          "import",
          "\"",
          ")",
          "out",
          "[",
          "i",
          "]",
          "=",
          "out",
          "[",
          "i",
          "]",
          ".",
          "replace",
          "(",
          "\"",
          "import",
          "lightning",
          "\"",
          ",",
          "f",
          "\"",
          "import",
          "{",
          "lightning_by",
          "}",
          "\"",
          ")",
          "return",
          "out"
        ],
        "docstring": "Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']",
        "docstring_tokens": [
          "replace",
          "imports",
          "of",
          "standalone",
          "package",
          "to",
          "lightning",
          "lns",
          "lightning_app",
          "lightning_app",
          "lightning_app",
          "delete_cloud_lightning_apps",
          "from",
          "lightning_app",
          "import",
          "lightning_apps",
          "lightning_app",
          "and",
          "pytorch_lightning",
          "are",
          "ours",
          "def",
          "_lightning_app",
          "class",
          "lightning_app",
          "core",
          "flow",
          "lightningflow",
          "http",
          "pytorch_lightning",
          "ai",
          "from",
          "lightning",
          "import",
          "__version__",
          "lightning",
          "ai",
          "mapping",
          "lightning_app",
          "lightning",
          "app",
          "pytorch_lightning",
          "lightning",
          "pytorch",
          "_replace_imports",
          "lns",
          "mapping",
          "lightning_by",
          "lightning_fabric",
          "doctest",
          "normalize_whitespace",
          "lightning",
          "app",
          "lightning",
          "app",
          "lightning_app",
          "delete_cloud_lightning_apps",
          "from",
          "lightning",
          "app",
          "import",
          "lightning_apps",
          "lightning",
          "app",
          "and",
          "lightning",
          "pytorch",
          "are",
          "ours",
          "def",
          "_lightning_app",
          "class",
          "lightning",
          "app",
          "core",
          "flow",
          "lightningflow",
          "http",
          "pytorch_lightning",
          "ai",
          "from",
          "lightning_fabric",
          "import",
          "__version__",
          "lightning",
          "ai"
        ],
        "docstring_summary": "Replace imports of standalone package to lightning.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py",
        "partition": "train",
        "function_type": "function",
        "start_line": 235,
        "end_line": 279,
        "hash": "570e98693d738c4b47c1d9b3e8a039cc",
        "complexity": 4,
        "parameters": [
          "lines",
          "mapping",
          "str]]",
          "lightning_by"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": ".actions\\assistant.py",
        "func_name": "pull_docs_files",
        "original_string": "def pull_docs_files(\r\n        gh_user_repo: str,\r\n        target_dir: str = \"docs/source-pytorch/XXX\",\r\n        checkout: str = \"refs/tags/1.0.0\",\r\n        source_dir: str = \"docs/source\",\r\n        single_page: Optional[str] = None,\r\n        as_orphan: bool = False,\r\n    ) -> None:\r\n        \"\"\"Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page\r\n\r\n        \"\"\"\r\n        import zipfile\r\n\r\n        zip_url = f\"https://github.com/{gh_user_repo}/archive/{checkout}.zip\"\r\n\r\n        with tempfile.TemporaryDirectory() as tmp:\r\n            zip_file = os.path.join(tmp, \"repo.zip\")\r\n            try:\r\n                urllib.request.urlretrieve(zip_url, zip_file)\r\n            except urllib.error.HTTPError:\r\n                raise RuntimeError(f\"Requesting file '{zip_url}' does not exist or it is just unavailable.\")\r\n\r\n            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\r\n                zip_ref.extractall(tmp)\r\n\r\n            zip_dirs = [d for d in glob.glob(os.path.join(tmp, \"*\")) if os.path.isdir(d)]\r\n            # check that the extracted archive has only repo folder\r\n            assert len(zip_dirs) == 1\r\n            repo_dir = zip_dirs[0]\r\n\r\n            if single_page:  # special case for copying single page\r\n                single_page = os.path.join(repo_dir, source_dir, single_page)\r\n                assert os.path.isfile(single_page), f\"File '{single_page}' does not exist.\"\r\n                name = re.sub(r\"lightning[-_]?\", \"\", gh_user_repo.split(\"/\")[-1])\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, f\"{name}.rst\")\r\n                AssistantCLI._copy_rst(single_page, new_rst, as_orphan=as_orphan)\r\n                return\r\n            # continue with copying all pages\r\n            ls_pages = glob.glob(os.path.join(repo_dir, source_dir, \"*.rst\"))\r\n            ls_pages += glob.glob(os.path.join(repo_dir, source_dir, \"**\", \"*.rst\"))\r\n            for rst in ls_pages:\r\n                rel_rst = rst.replace(os.path.join(repo_dir, source_dir) + os.path.sep, \"\")\r\n                rel_dir = os.path.dirname(rel_rst)\r\n                os.makedirs(os.path.join(_PROJECT_ROOT, target_dir, rel_dir), exist_ok=True)\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, rel_rst)\r\n                if os.path.isfile(new_rst):\r\n                    logging.warning(f\"Page {new_rst} already exists in the local tree so it will be skipped.\")\r\n                    continue\r\n                AssistantCLI._copy_rst(rst, new_rst, as_orphan=as_orphan)",
        "language": "python",
        "code": "def pull_docs_files(\r\n        gh_user_repo: str,\r\n        target_dir: str = \"docs/source-pytorch/XXX\",\r\n        checkout: str = \"refs/tags/1.0.0\",\r\n        source_dir: str = \"docs/source\",\r\n        single_page: Optional[str] = None,\r\n        as_orphan: bool = False,\r\n    ) -> None:\r\n        \"\"\"Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page\r\n\r\n        \"\"\"\r\n        import zipfile\r\n\r\n        zip_url = f\"https://github.com/{gh_user_repo}/archive/{checkout}.zip\"\r\n\r\n        with tempfile.TemporaryDirectory() as tmp:\r\n            zip_file = os.path.join(tmp, \"repo.zip\")\r\n            try:\r\n                urllib.request.urlretrieve(zip_url, zip_file)\r\n            except urllib.error.HTTPError:\r\n                raise RuntimeError(f\"Requesting file '{zip_url}' does not exist or it is just unavailable.\")\r\n\r\n            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\r\n                zip_ref.extractall(tmp)\r\n\r\n            zip_dirs = [d for d in glob.glob(os.path.join(tmp, \"*\")) if os.path.isdir(d)]\r\n            # check that the extracted archive has only repo folder\r\n            assert len(zip_dirs) == 1\r\n            repo_dir = zip_dirs[0]\r\n\r\n            if single_page:  # special case for copying single page\r\n                single_page = os.path.join(repo_dir, source_dir, single_page)\r\n                assert os.path.isfile(single_page), f\"File '{single_page}' does not exist.\"\r\n                name = re.sub(r\"lightning[-_]?\", \"\", gh_user_repo.split(\"/\")[-1])\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, f\"{name}.rst\")\r\n                AssistantCLI._copy_rst(single_page, new_rst, as_orphan=as_orphan)\r\n                return\r\n            # continue with copying all pages\r\n            ls_pages = glob.glob(os.path.join(repo_dir, source_dir, \"*.rst\"))\r\n            ls_pages += glob.glob(os.path.join(repo_dir, source_dir, \"**\", \"*.rst\"))\r\n            for rst in ls_pages:\r\n                rel_rst = rst.replace(os.path.join(repo_dir, source_dir) + os.path.sep, \"\")\r\n                rel_dir = os.path.dirname(rel_rst)\r\n                os.makedirs(os.path.join(_PROJECT_ROOT, target_dir, rel_dir), exist_ok=True)\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, rel_rst)\r\n                if os.path.isfile(new_rst):\r\n                    logging.warning(f\"Page {new_rst} already exists in the local tree so it will be skipped.\")\r\n                    continue\r\n                AssistantCLI._copy_rst(rst, new_rst, as_orphan=as_orphan)",
        "code_tokens": [
          "def",
          "pull_docs_files",
          "(",
          "gh_user_repo",
          ":",
          "str",
          ",",
          "target_dir",
          ":",
          "str",
          "=",
          "\"",
          "docs",
          "/",
          "source",
          "-",
          "pytorch",
          "/",
          "XXX",
          "\"",
          ",",
          "checkout",
          ":",
          "str",
          "=",
          "\"",
          "refs",
          "/",
          "tags",
          "/",
          "1",
          ".",
          "0",
          ".",
          "0",
          "\"",
          ",",
          "source_dir",
          ":",
          "str",
          "=",
          "\"",
          "docs",
          "/",
          "source",
          "\"",
          ",",
          "single_page",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          "as_orphan",
          ":",
          "bool",
          "=",
          "False",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Pull",
          "docs",
          "pages",
          "from",
          "external",
          "source",
          "and",
          "append",
          "to",
          "local",
          "docs",
          ".",
          "Args",
          ":",
          "gh_user_repo",
          ":",
          "standard",
          "GitHub",
          "user",
          "/",
          "repo",
          "string",
          "target_dir",
          ":",
          "relative",
          "location",
          "inside",
          "the",
          "docs",
          "folder",
          "checkout",
          ":",
          "specific",
          "tag",
          "or",
          "branch",
          "to",
          "checkout",
          "source_dir",
          ":",
          "relative",
          "location",
          "inside",
          "the",
          "remote",
          "/",
          "external",
          "repo",
          "single_page",
          ":",
          "copy",
          "only",
          "single",
          "page",
          "from",
          "the",
          "remote",
          "repo",
          "and",
          "name",
          "it",
          "as",
          "the",
          "repo",
          "name",
          "as_orphan",
          ":",
          "append",
          "orphan",
          "statement",
          "to",
          "the",
          "page",
          "\"",
          "\"",
          "\"",
          "import",
          "zipfile",
          "zip_url",
          "=",
          "f",
          "\"",
          "https",
          ":",
          "/",
          "/",
          "github",
          ".",
          "com",
          "/",
          "{",
          "gh_user_repo",
          "}",
          "/",
          "archive",
          "/",
          "{",
          "checkout",
          "}",
          ".",
          "zip",
          "\"",
          "with",
          "tempfile",
          ".",
          "TemporaryDirectory",
          "(",
          ")",
          "as",
          "tmp",
          ":",
          "zip_file",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "tmp",
          ",",
          "\"",
          "repo",
          ".",
          "zip",
          "\"",
          ")",
          "try",
          ":",
          "urllib",
          ".",
          "request",
          ".",
          "urlretrieve",
          "(",
          "zip_url",
          ",",
          "zip_file",
          ")",
          "except",
          "urllib",
          ".",
          "error",
          ".",
          "HTTPError",
          ":",
          "raise",
          "RuntimeError",
          "(",
          "f",
          "\"",
          "Requesting",
          "file",
          "'",
          "{",
          "zip_url",
          "}",
          "'",
          "does",
          "not",
          "exist",
          "or",
          "it",
          "is",
          "just",
          "unavailable",
          ".",
          "\"",
          ")",
          "with",
          "zipfile",
          ".",
          "ZipFile",
          "(",
          "zip_file",
          ",",
          "\"",
          "r",
          "\"",
          ")",
          "as",
          "zip_ref",
          ":",
          "zip_ref",
          ".",
          "extractall",
          "(",
          "tmp",
          ")",
          "zip_dirs",
          "=",
          "[",
          "d",
          "for",
          "d",
          "in",
          "glob",
          ".",
          "glob",
          "(",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "tmp",
          ",",
          "\"",
          "*",
          "\"",
          ")",
          ")",
          "if",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "d",
          ")",
          "]",
          "assert",
          "len",
          "(",
          "zip_dirs",
          ")",
          "=",
          "=",
          "1",
          "repo_dir",
          "=",
          "zip_dirs",
          "[",
          "0",
          "]",
          "if",
          "single_page",
          ":",
          "single_page",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "repo_dir",
          ",",
          "source_dir",
          ",",
          "single_page",
          ")",
          "assert",
          "os",
          ".",
          "path",
          ".",
          "isfile",
          "(",
          "single_page",
          ")",
          ",",
          "f",
          "\"",
          "File",
          "'",
          "{",
          "single_page",
          "}",
          "'",
          "does",
          "not",
          "exist",
          ".",
          "\"",
          "name",
          "=",
          "re",
          ".",
          "sub",
          "(",
          "r",
          "\"",
          "lightning",
          "[",
          "-",
          "_",
          "]",
          "?",
          "\"",
          ",",
          "\"",
          "\"",
          ",",
          "gh_user_repo",
          ".",
          "split",
          "(",
          "\"",
          "/",
          "\"",
          ")",
          "[",
          "-",
          "1",
          "]",
          ")",
          "new_rst",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "_PROJECT_ROOT",
          ",",
          "target_dir",
          ",",
          "f",
          "\"",
          "{",
          "name",
          "}",
          ".",
          "rst",
          "\"",
          ")",
          "AssistantCLI",
          ".",
          "_copy_rst",
          "(",
          "single_page",
          ",",
          "new_rst",
          ",",
          "as_orphan",
          "=",
          "as_orphan",
          ")",
          "return",
          "ls_pages",
          "=",
          "glob",
          ".",
          "glob",
          "(",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "repo_dir",
          ",",
          "source_dir",
          ",",
          "\"",
          "*",
          ".",
          "rst",
          "\"",
          ")",
          ")",
          "ls_pages",
          "+",
          "=",
          "glob",
          ".",
          "glob",
          "(",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "repo_dir",
          ",",
          "source_dir",
          ",",
          "\"",
          "*",
          "*",
          "\"",
          ",",
          "\"",
          "*",
          ".",
          "rst",
          "\"",
          ")",
          ")",
          "for",
          "rst",
          "in",
          "ls_pages",
          ":",
          "rel_rst",
          "=",
          "rst",
          ".",
          "replace",
          "(",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "repo_dir",
          ",",
          "source_dir",
          ")",
          "+",
          "os",
          ".",
          "path",
          ".",
          "sep",
          ",",
          "\"",
          "\"",
          ")",
          "rel_dir",
          "=",
          "os",
          ".",
          "path",
          ".",
          "dirname",
          "(",
          "rel_rst",
          ")",
          "os",
          ".",
          "makedirs",
          "(",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "_PROJECT_ROOT",
          ",",
          "target_dir",
          ",",
          "rel_dir",
          ")",
          ",",
          "exist_ok",
          "=",
          "True",
          ")",
          "new_rst",
          "=",
          "os",
          ".",
          "path",
          ".",
          "join",
          "(",
          "_PROJECT_ROOT",
          ",",
          "target_dir",
          ",",
          "rel_rst",
          ")",
          "if",
          "os",
          ".",
          "path",
          ".",
          "isfile",
          "(",
          "new_rst",
          ")",
          ":",
          "logging",
          ".",
          "warning",
          "(",
          "f",
          "\"",
          "Page",
          "{",
          "new_rst",
          "}",
          "already",
          "exists",
          "in",
          "the",
          "local",
          "tree",
          "so",
          "it",
          "will",
          "be",
          "skipped",
          ".",
          "\"",
          ")",
          "continue",
          "AssistantCLI",
          ".",
          "_copy_rst",
          "(",
          "rst",
          ",",
          "new_rst",
          ",",
          "as_orphan",
          "=",
          "as_orphan",
          ")"
        ],
        "docstring": "Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page",
        "docstring_tokens": [
          "pull",
          "docs",
          "pages",
          "from",
          "external",
          "source",
          "and",
          "append",
          "to",
          "local",
          "docs",
          "args",
          "gh_user_repo",
          "standard",
          "github",
          "user",
          "repo",
          "string",
          "target_dir",
          "relative",
          "location",
          "inside",
          "the",
          "docs",
          "folder",
          "checkout",
          "specific",
          "tag",
          "or",
          "branch",
          "to",
          "checkout",
          "source_dir",
          "relative",
          "location",
          "inside",
          "the",
          "remote",
          "external",
          "repo",
          "single_page",
          "copy",
          "only",
          "single",
          "page",
          "from",
          "the",
          "remote",
          "repo",
          "and",
          "name",
          "it",
          "as",
          "the",
          "repo",
          "name",
          "as_orphan",
          "append",
          "orphan",
          "statement",
          "to",
          "the",
          "page"
        ],
        "docstring_summary": "Pull docs pages from external source and append to local docs.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "AssistantCLI",
        "start_line": 359,
        "end_line": 415,
        "hash": "7cfddef78c4d6be65753dcdcf884b2b6",
        "complexity": 9,
        "parameters": [
          "gh_user_repo",
          "target_dir",
          "checkout",
          "source_dir",
          "single_page",
          "as_orphan"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "fit",
        "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "language": "python",
        "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "code_tokens": [
          "def",
          "fit",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "val_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "ckpt_path",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          ",",
          "triggering",
          "the",
          "actual",
          "training",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          ".",
          "Can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          ":",
          "attr",
          ":",
          "`",
          "callbacks",
          "`",
          "(",
          "see",
          ":",
          "meth",
          ":",
          "`",
          "MyCustomTrainer",
          ".",
          "__init__",
          "`",
          ")",
          ".",
          "train_loader",
          ":",
          "the",
          "training",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "val_loader",
          ":",
          "the",
          "validation",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "If",
          "not",
          "specified",
          ",",
          "no",
          "validation",
          "will",
          "run",
          ".",
          "ckpt_path",
          ":",
          "Path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          ".",
          "If",
          "specified",
          ",",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "launch",
          "(",
          ")",
          "train_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "train_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          ":",
          "val_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "val_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "fabric",
          ".",
          "strategy",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "strategies",
          ".",
          "fsdp",
          ".",
          "FSDPStrategy",
          ")",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "currently",
          "does",
          "not",
          "support",
          "FSDP",
          "\"",
          ")",
          "optimizer",
          ",",
          "scheduler_cfg",
          "=",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "model",
          ".",
          "configure_optimizers",
          "(",
          ")",
          ")",
          "assert",
          "optimizer",
          "is",
          "not",
          "None",
          "model",
          ",",
          "optimizer",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup",
          "(",
          "model",
          ",",
          "optimizer",
          ")",
          "state",
          "=",
          "{",
          "\"",
          "model",
          "\"",
          ":",
          "model",
          ",",
          "\"",
          "optim",
          "\"",
          ":",
          "optimizer",
          ",",
          "\"",
          "scheduler",
          "\"",
          ":",
          "scheduler_cfg",
          "}",
          "if",
          "ckpt_path",
          "is",
          "not",
          "None",
          "and",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "ckpt_path",
          ")",
          ":",
          "latest_checkpoint_path",
          "=",
          "self",
          ".",
          "get_latest_checkpoint",
          "(",
          "self",
          ".",
          "checkpoint_dir",
          ")",
          "if",
          "latest_checkpoint_path",
          "is",
          "not",
          "None",
          ":",
          "self",
          ".",
          "load",
          "(",
          "state",
          ",",
          "latest_checkpoint_path",
          ")",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "while",
          "not",
          "self",
          ".",
          "should_stop",
          ":",
          "self",
          ".",
          "train_loop",
          "(",
          "model",
          ",",
          "optimizer",
          ",",
          "train_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_train_batches",
          ",",
          "scheduler_cfg",
          "=",
          "scheduler_cfg",
          ")",
          "if",
          "self",
          ".",
          "should_validate",
          ":",
          "self",
          ".",
          "val_loop",
          "(",
          "model",
          ",",
          "val_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_val_batches",
          ")",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "epoch",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "current_epoch",
          ")",
          "self",
          ".",
          "current_epoch",
          "+",
          "=",
          "1",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "self",
          ".",
          "save",
          "(",
          "state",
          ")",
          "self",
          ".",
          "should_stop",
          "=",
          "False"
        ],
        "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.",
        "docstring_tokens": [
          "the",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          "triggering",
          "the",
          "actual",
          "training",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          "attr",
          "callbacks",
          "see",
          "meth",
          "mycustomtrainer",
          "__init__",
          "train_loader",
          "the",
          "training",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "val_loader",
          "the",
          "validation",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "if",
          "not",
          "specified",
          "no",
          "validation",
          "will",
          "run",
          "ckpt_path",
          "path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          "if",
          "specified",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory"
        ],
        "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 123,
        "end_line": 191,
        "hash": "f92144ac7f0974541f275cce4fcc2e5f",
        "complexity": 12,
        "parameters": [
          "model",
          "train_loader",
          "val_loader",
          "ckpt_path"
        ]
      }
    ]
  },
  {
    "query_id": 5,
    "query": "How to implement custom metrics in LightningModule?",
    "relevant_docs": [
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "accelerator",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Accelerator",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "strategy",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "Strategy",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "devices",
          ":",
          "Union",
          "[",
          "list",
          "[",
          "int",
          "]",
          ",",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "auto",
          "\"",
          ",",
          "precision",
          ":",
          "Union",
          "[",
          "str",
          ",",
          "int",
          "]",
          "=",
          "\"",
          "32",
          "-",
          "true",
          "\"",
          ",",
          "plugins",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "callbacks",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "list",
          "[",
          "Any",
          "]",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "None",
          ",",
          "loggers",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "Logger",
          ",",
          "list",
          "[",
          "Logger",
          "]",
          "]",
          "]",
          "=",
          "None",
          ",",
          "max_epochs",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "1000",
          ",",
          "max_steps",
          ":",
          "Optional",
          "[",
          "int",
          "]",
          "=",
          "None",
          ",",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "1",
          ",",
          "limit_train_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "limit_val_batches",
          ":",
          "Union",
          "[",
          "int",
          ",",
          "float",
          "]",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          ",",
          "validation_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          "use_distributed_sampler",
          ":",
          "bool",
          "=",
          "True",
          ",",
          "checkpoint_dir",
          ":",
          "str",
          "=",
          "\"",
          ".",
          "/",
          "checkpoints",
          "\"",
          ",",
          "checkpoint_frequency",
          ":",
          "int",
          "=",
          "1",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Exemplary",
          "Trainer",
          "with",
          "Fabric",
          ".",
          "This",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          ".",
          "As",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          ",",
          "we",
          "recommend",
          "using",
          "the",
          ":",
          "class",
          ":",
          "`",
          "lightning",
          ".",
          "pytorch",
          ".",
          "Trainer",
          "`",
          ".",
          "Args",
          ":",
          "accelerator",
          ":",
          "The",
          "hardware",
          "to",
          "run",
          "on",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "cpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "cuda",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "mps",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "gpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "tpu",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "strategy",
          ":",
          "Strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          ".",
          "Possible",
          "choices",
          "are",
          ":",
          "`",
          "`",
          "\"",
          "dp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "ddp_spawn",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "deepspeed",
          "\"",
          "`",
          "`",
          ",",
          "`",
          "`",
          "\"",
          "fsdp",
          "\"",
          "`",
          "`",
          ".",
          "devices",
          ":",
          "Number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "int",
          "`",
          "`",
          ")",
          ",",
          "which",
          "GPUs",
          "to",
          "train",
          "on",
          "(",
          "`",
          "`",
          "list",
          "`",
          "`",
          "or",
          "`",
          "`",
          "str",
          "`",
          "`",
          ")",
          ",",
          "or",
          "`",
          "`",
          "\"",
          "auto",
          "\"",
          "`",
          "`",
          ".",
          "The",
          "value",
          "applies",
          "per",
          "node",
          ".",
          "precision",
          ":",
          "Double",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "64",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "full",
          "precision",
          "(",
          "`",
          "`",
          "\"",
          "32",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "half",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ",",
          "or",
          "bfloat16",
          "precision",
          "AMP",
          "(",
          "`",
          "`",
          "\"",
          "bf16",
          "-",
          "mixed",
          "\"",
          "`",
          "`",
          ")",
          ".",
          "plugins",
          ":",
          "One",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          ":",
          "A",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          ".",
          "The",
          "following",
          "hooks",
          "are",
          "supported",
          ":",
          "-",
          "on_train_epoch_start",
          "-",
          "on",
          "train_epoch_end",
          "-",
          "on_train_batch_start",
          "-",
          "on_train_batch_end",
          "-",
          "on_before_backward",
          "-",
          "on_after_backward",
          "-",
          "on_before_zero_grad",
          "-",
          "on_before_optimizer_step",
          "-",
          "on_validation_model_eval",
          "-",
          "on_validation_model_train",
          "-",
          "on_validation_epoch_start",
          "-",
          "on_validation_epoch_end",
          "-",
          "on_validation_batch_start",
          "-",
          "on_validation_batch_end",
          "loggers",
          ":",
          "A",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          ".",
          "See",
          ":",
          "meth",
          ":",
          "`",
          "~",
          "lightning",
          ".",
          "fabric",
          ".",
          "fabric",
          ".",
          "Fabric",
          ".",
          "log",
          "`",
          "for",
          "more",
          "information",
          ".",
          "max_epochs",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          ":",
          "The",
          "maximum",
          "number",
          "of",
          "(",
          "optimizer",
          ")",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          ":",
          "How",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "limit_val_batches",
          ":",
          "Limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          ".",
          "If",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          ",",
          "this",
          "has",
          "no",
          "effect",
          ".",
          "validation_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          ".",
          "use_distributed_sampler",
          ":",
          "Wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "-",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          ".",
          "checkpoint_dir",
          ":",
          "Directory",
          "to",
          "store",
          "checkpoints",
          "to",
          ".",
          "checkpoint_frequency",
          ":",
          "How",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          ".",
          "Warning",
          ":",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "(",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          ")",
          ",",
          "won",
          "'",
          "t",
          "work",
          "!",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          "=",
          "L",
          ".",
          "Fabric",
          "(",
          "accelerator",
          "=",
          "accelerator",
          ",",
          "strategy",
          "=",
          "strategy",
          ",",
          "devices",
          "=",
          "devices",
          ",",
          "precision",
          "=",
          "precision",
          ",",
          "plugins",
          "=",
          "plugins",
          ",",
          "callbacks",
          "=",
          "callbacks",
          ",",
          "loggers",
          "=",
          "loggers",
          ",",
          ")",
          "self",
          ".",
          "global_step",
          "=",
          "0",
          "self",
          ".",
          "grad_accum_steps",
          ":",
          "int",
          "=",
          "grad_accum_steps",
          "self",
          ".",
          "current_epoch",
          "=",
          "0",
          "self",
          ".",
          "max_epochs",
          "=",
          "max_epochs",
          "self",
          ".",
          "max_steps",
          "=",
          "max_steps",
          "self",
          ".",
          "should_stop",
          "=",
          "False",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_train_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_train_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "if",
          "not",
          "isinstance",
          "(",
          "limit_val_batches",
          ",",
          "int",
          ")",
          ":",
          "assert",
          "limit_val_batches",
          "=",
          "=",
          "float",
          "(",
          "\"",
          "inf",
          "\"",
          ")",
          "self",
          ".",
          "limit_train_batches",
          "=",
          "limit_train_batches",
          "self",
          ".",
          "limit_val_batches",
          "=",
          "limit_val_batches",
          "self",
          ".",
          "validation_frequency",
          "=",
          "validation_frequency",
          "self",
          ".",
          "use_distributed_sampler",
          "=",
          "use_distributed_sampler",
          "self",
          ".",
          "_current_train_return",
          ":",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "_current_val_return",
          ":",
          "Optional",
          "[",
          "Union",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "Mapping",
          "[",
          "str",
          ",",
          "Any",
          "]",
          "]",
          "]",
          "=",
          "{",
          "}",
          "self",
          ".",
          "checkpoint_dir",
          "=",
          "checkpoint_dir",
          "self",
          ".",
          "checkpoint_frequency",
          "=",
          "checkpoint_frequency"
        ],
        "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!",
        "docstring_tokens": [
          "exemplary",
          "trainer",
          "with",
          "fabric",
          "this",
          "is",
          "a",
          "very",
          "simple",
          "trainer",
          "focused",
          "on",
          "readability",
          "but",
          "with",
          "reduced",
          "featureset",
          "as",
          "a",
          "trainer",
          "with",
          "more",
          "included",
          "features",
          "we",
          "recommend",
          "using",
          "the",
          "class",
          "lightning",
          "pytorch",
          "trainer",
          "args",
          "accelerator",
          "the",
          "hardware",
          "to",
          "run",
          "on",
          "possible",
          "choices",
          "are",
          "cpu",
          "cuda",
          "mps",
          "gpu",
          "tpu",
          "auto",
          "strategy",
          "strategy",
          "for",
          "how",
          "to",
          "run",
          "across",
          "multiple",
          "devices",
          "possible",
          "choices",
          "are",
          "dp",
          "ddp",
          "ddp_spawn",
          "deepspeed",
          "fsdp",
          "devices",
          "number",
          "of",
          "devices",
          "to",
          "train",
          "on",
          "int",
          "which",
          "gpus",
          "to",
          "train",
          "on",
          "list",
          "or",
          "str",
          "or",
          "auto",
          "the",
          "value",
          "applies",
          "per",
          "node",
          "precision",
          "double",
          "precision",
          "64",
          "full",
          "precision",
          "32",
          "half",
          "precision",
          "amp",
          "16",
          "mixed",
          "or",
          "bfloat16",
          "precision",
          "amp",
          "bf16",
          "mixed",
          "plugins",
          "one",
          "or",
          "several",
          "custom",
          "plugins",
          "callbacks",
          "a",
          "single",
          "callback",
          "or",
          "a",
          "list",
          "of",
          "callbacks",
          "the",
          "following",
          "hooks",
          "are",
          "supported",
          "on_train_epoch_start",
          "on",
          "train_epoch_end",
          "on_train_batch_start",
          "on_train_batch_end",
          "on_before_backward",
          "on_after_backward",
          "on_before_zero_grad",
          "on_before_optimizer_step",
          "on_validation_model_eval",
          "on_validation_model_train",
          "on_validation_epoch_start",
          "on_validation_epoch_end",
          "on_validation_batch_start",
          "on_validation_batch_end",
          "loggers",
          "a",
          "single",
          "logger",
          "or",
          "a",
          "list",
          "of",
          "loggers",
          "see",
          "meth",
          "lightning",
          "fabric",
          "fabric",
          "fabric",
          "log",
          "for",
          "more",
          "information",
          "max_epochs",
          "the",
          "maximum",
          "number",
          "of",
          "epochs",
          "to",
          "train",
          "max_steps",
          "the",
          "maximum",
          "number",
          "of",
          "optimizer",
          "steps",
          "to",
          "train",
          "grad_accum_steps",
          "how",
          "many",
          "batches",
          "to",
          "process",
          "before",
          "each",
          "optimizer",
          "step",
          "limit_train_batches",
          "limits",
          "the",
          "number",
          "of",
          "train",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "limit_val_batches",
          "limits",
          "the",
          "number",
          "of",
          "validation",
          "batches",
          "per",
          "epoch",
          "if",
          "greater",
          "than",
          "number",
          "of",
          "batches",
          "in",
          "the",
          "dataloader",
          "this",
          "has",
          "no",
          "effect",
          "validation_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "validation",
          "epoch",
          "use_distributed_sampler",
          "wraps",
          "the",
          "sampler",
          "of",
          "each",
          "dataloader",
          "with",
          "a",
          "respective",
          "distributed",
          "aware",
          "sampler",
          "in",
          "case",
          "of",
          "distributed",
          "training",
          "checkpoint_dir",
          "directory",
          "to",
          "store",
          "checkpoints",
          "to",
          "checkpoint_frequency",
          "how",
          "many",
          "epochs",
          "to",
          "run",
          "before",
          "each",
          "checkpoint",
          "is",
          "written",
          "warning",
          "callbacks",
          "written",
          "for",
          "the",
          "lightning",
          "trainer",
          "especially",
          "making",
          "assumptions",
          "on",
          "the",
          "trainer",
          "won",
          "t",
          "work"
        ],
        "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 18,
        "end_line": 121,
        "hash": "0bbe0adf3ec60b2f294e15b93da70f56",
        "complexity": 3,
        "parameters": [
          "accelerator",
          "Accelerator]",
          "strategy",
          "Strategy]",
          "devices",
          "str",
          "int]",
          "precision",
          "int]",
          "plugins",
          "Any]]",
          "callbacks",
          "Any]]",
          "loggers",
          "list[Logger]]]",
          "max_epochs",
          "max_steps",
          "grad_accum_steps",
          "limit_train_batches",
          "float]",
          "limit_val_batches",
          "float]",
          "validation_frequency",
          "use_distributed_sampler",
          "checkpoint_dir",
          "checkpoint_frequency"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\build_your_own_trainer\\trainer.py",
        "func_name": "fit",
        "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "language": "python",
        "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False",
        "code_tokens": [
          "def",
          "fit",
          "(",
          "self",
          ",",
          "model",
          ":",
          "L",
          ".",
          "LightningModule",
          ",",
          "train_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "val_loader",
          ":",
          "torch",
          ".",
          "utils",
          ".",
          "data",
          ".",
          "DataLoader",
          ",",
          "ckpt_path",
          ":",
          "Optional",
          "[",
          "str",
          "]",
          "=",
          "None",
          ",",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "The",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          ",",
          "triggering",
          "the",
          "actual",
          "training",
          ".",
          "Args",
          ":",
          "model",
          ":",
          "the",
          "LightningModule",
          "to",
          "train",
          ".",
          "Can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          ":",
          "attr",
          ":",
          "`",
          "callbacks",
          "`",
          "(",
          "see",
          ":",
          "meth",
          ":",
          "`",
          "MyCustomTrainer",
          ".",
          "__init__",
          "`",
          ")",
          ".",
          "train_loader",
          ":",
          "the",
          "training",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "val_loader",
          ":",
          "the",
          "validation",
          "dataloader",
          ".",
          "Has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          ".",
          "If",
          "not",
          "specified",
          ",",
          "no",
          "validation",
          "will",
          "run",
          ".",
          "ckpt_path",
          ":",
          "Path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          ".",
          "If",
          "specified",
          ",",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory",
          ".",
          "\"",
          "\"",
          "\"",
          "self",
          ".",
          "fabric",
          ".",
          "launch",
          "(",
          ")",
          "train_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "train_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "val_loader",
          "is",
          "not",
          "None",
          ":",
          "val_loader",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup_dataloaders",
          "(",
          "val_loader",
          ",",
          "use_distributed_sampler",
          "=",
          "self",
          ".",
          "use_distributed_sampler",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "fabric",
          ".",
          "strategy",
          ",",
          "L",
          ".",
          "fabric",
          ".",
          "strategies",
          ".",
          "fsdp",
          ".",
          "FSDPStrategy",
          ")",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "BYOT",
          "currently",
          "does",
          "not",
          "support",
          "FSDP",
          "\"",
          ")",
          "optimizer",
          ",",
          "scheduler_cfg",
          "=",
          "self",
          ".",
          "_parse_optimizers_schedulers",
          "(",
          "model",
          ".",
          "configure_optimizers",
          "(",
          ")",
          ")",
          "assert",
          "optimizer",
          "is",
          "not",
          "None",
          "model",
          ",",
          "optimizer",
          "=",
          "self",
          ".",
          "fabric",
          ".",
          "setup",
          "(",
          "model",
          ",",
          "optimizer",
          ")",
          "state",
          "=",
          "{",
          "\"",
          "model",
          "\"",
          ":",
          "model",
          ",",
          "\"",
          "optim",
          "\"",
          ":",
          "optimizer",
          ",",
          "\"",
          "scheduler",
          "\"",
          ":",
          "scheduler_cfg",
          "}",
          "if",
          "ckpt_path",
          "is",
          "not",
          "None",
          "and",
          "os",
          ".",
          "path",
          ".",
          "isdir",
          "(",
          "ckpt_path",
          ")",
          ":",
          "latest_checkpoint_path",
          "=",
          "self",
          ".",
          "get_latest_checkpoint",
          "(",
          "self",
          ".",
          "checkpoint_dir",
          ")",
          "if",
          "latest_checkpoint_path",
          "is",
          "not",
          "None",
          ":",
          "self",
          ".",
          "load",
          "(",
          "state",
          ",",
          "latest_checkpoint_path",
          ")",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "while",
          "not",
          "self",
          ".",
          "should_stop",
          ":",
          "self",
          ".",
          "train_loop",
          "(",
          "model",
          ",",
          "optimizer",
          ",",
          "train_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_train_batches",
          ",",
          "scheduler_cfg",
          "=",
          "scheduler_cfg",
          ")",
          "if",
          "self",
          ".",
          "should_validate",
          ":",
          "self",
          ".",
          "val_loop",
          "(",
          "model",
          ",",
          "val_loader",
          ",",
          "limit_batches",
          "=",
          "self",
          ".",
          "limit_val_batches",
          ")",
          "self",
          ".",
          "step_scheduler",
          "(",
          "model",
          ",",
          "scheduler_cfg",
          ",",
          "level",
          "=",
          "\"",
          "epoch",
          "\"",
          ",",
          "current_value",
          "=",
          "self",
          ".",
          "current_epoch",
          ")",
          "self",
          ".",
          "current_epoch",
          "+",
          "=",
          "1",
          "if",
          "self",
          ".",
          "max_epochs",
          "is",
          "not",
          "None",
          "and",
          "self",
          ".",
          "current_epoch",
          ">",
          "=",
          "self",
          ".",
          "max_epochs",
          ":",
          "self",
          ".",
          "should_stop",
          "=",
          "True",
          "self",
          ".",
          "save",
          "(",
          "state",
          ")",
          "self",
          ".",
          "should_stop",
          "=",
          "False"
        ],
        "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.",
        "docstring_tokens": [
          "the",
          "main",
          "entrypoint",
          "of",
          "the",
          "trainer",
          "triggering",
          "the",
          "actual",
          "training",
          "args",
          "model",
          "the",
          "lightningmodule",
          "to",
          "train",
          "can",
          "have",
          "the",
          "same",
          "hooks",
          "as",
          "attr",
          "callbacks",
          "see",
          "meth",
          "mycustomtrainer",
          "__init__",
          "train_loader",
          "the",
          "training",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "val_loader",
          "the",
          "validation",
          "dataloader",
          "has",
          "to",
          "be",
          "an",
          "iterable",
          "returning",
          "batches",
          "if",
          "not",
          "specified",
          "no",
          "validation",
          "will",
          "run",
          "ckpt_path",
          "path",
          "to",
          "previous",
          "checkpoints",
          "to",
          "resume",
          "training",
          "from",
          "if",
          "specified",
          "will",
          "always",
          "look",
          "for",
          "the",
          "latest",
          "checkpoint",
          "within",
          "the",
          "given",
          "directory"
        ],
        "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "MyCustomTrainer",
        "start_line": 123,
        "end_line": 191,
        "hash": "f92144ac7f0974541f275cce4fcc2e5f",
        "complexity": 12,
        "parameters": [
          "model",
          "train_loader",
          "val_loader",
          "ckpt_path"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\fabric\\tensor_parallel\\model.py",
        "func_name": "init_weights",
        "original_string": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )",
        "language": "python",
        "code": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )",
        "code_tokens": [
          "def",
          "init_weights",
          "(",
          "self",
          ")",
          ":",
          "\"",
          "\"",
          "\"",
          "[",
          "Note",
          ":",
          "On",
          "`",
          "`",
          "init_weights",
          "`",
          "`",
          "vs",
          ".",
          "`",
          "`",
          "reset_parameters",
          "`",
          "`",
          "]",
          "Modules",
          "may",
          "define",
          "`",
          "`",
          "reset_parameters",
          "`",
          "`",
          "to",
          "initialize",
          "parameter",
          "values",
          ".",
          "`",
          "`",
          "reset_parameters",
          "`",
          "`",
          "is",
          "meant",
          "to",
          "only",
          "initialize",
          "directly",
          "owned",
          "parameters",
          "/",
          "buffers",
          ",",
          "not",
          "those",
          "of",
          "their",
          "child",
          "modules",
          ",",
          "and",
          "it",
          "can",
          "be",
          "used",
          "to",
          "give",
          "the",
          "initial",
          "values",
          "for",
          "these",
          "tensors",
          ".",
          "Separately",
          ",",
          "users",
          "may",
          "want",
          "custom",
          "initialization",
          "for",
          "their",
          "modules",
          ",",
          "different",
          "from",
          "that",
          "in",
          "`",
          "`",
          "reset_parameters",
          "`",
          "`",
          ".",
          "For",
          "this",
          ",",
          "we",
          "define",
          "`",
          "`",
          "init_weights",
          "`",
          "`",
          ".",
          "We",
          "only",
          "call",
          "it",
          "in",
          "the",
          "constructor",
          "of",
          "this",
          "`",
          "`",
          "Transformer",
          "`",
          "`",
          "root",
          "module",
          "to",
          "avoid",
          "reinitializing",
          "tensors",
          ".",
          "\"",
          "\"",
          "\"",
          "with",
          "torch",
          ".",
          "device",
          "(",
          "self",
          ".",
          "freqs_cis",
          ".",
          "device",
          ")",
          ":",
          "self",
          ".",
          "freqs_cis",
          "=",
          "self",
          ".",
          "_precompute_freqs_cis",
          "(",
          ")",
          "nn",
          ".",
          "init",
          ".",
          "normal_",
          "(",
          "self",
          ".",
          "tok_embeddings",
          ".",
          "weight",
          ")",
          "for",
          "layer",
          "in",
          "self",
          ".",
          "layers",
          ".",
          "values",
          "(",
          ")",
          ":",
          "layer",
          ".",
          "init_weights",
          "(",
          ")",
          "self",
          ".",
          "norm",
          ".",
          "reset_parameters",
          "(",
          ")",
          "final_out_std",
          "=",
          "self",
          ".",
          "model_args",
          ".",
          "dim",
          "*",
          "*",
          "-",
          "0",
          ".",
          "5",
          "cutoff_factor",
          "=",
          "3",
          "nn",
          ".",
          "init",
          ".",
          "trunc_normal_",
          "(",
          "self",
          ".",
          "output",
          ".",
          "weight",
          ",",
          "mean",
          "=",
          "0",
          ".",
          "0",
          ",",
          "std",
          "=",
          "final_out_std",
          ",",
          "a",
          "=",
          "-",
          "cutoff_factor",
          "*",
          "final_out_std",
          ",",
          "b",
          "=",
          "cutoff_factor",
          "*",
          "final_out_std",
          ",",
          ")"
        ],
        "docstring": "[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.",
        "docstring_tokens": [
          "note",
          "on",
          "init_weights",
          "vs",
          "reset_parameters",
          "modules",
          "may",
          "define",
          "reset_parameters",
          "to",
          "initialize",
          "parameter",
          "values",
          "reset_parameters",
          "is",
          "meant",
          "to",
          "only",
          "initialize",
          "directly",
          "owned",
          "parameters",
          "buffers",
          "not",
          "those",
          "of",
          "their",
          "child",
          "modules",
          "and",
          "it",
          "can",
          "be",
          "used",
          "to",
          "give",
          "the",
          "initial",
          "values",
          "for",
          "these",
          "tensors",
          "separately",
          "users",
          "may",
          "want",
          "custom",
          "initialization",
          "for",
          "their",
          "modules",
          "different",
          "from",
          "that",
          "in",
          "reset_parameters",
          "for",
          "this",
          "we",
          "define",
          "init_weights",
          "we",
          "only",
          "call",
          "it",
          "in",
          "the",
          "constructor",
          "of",
          "this",
          "transformer",
          "root",
          "module",
          "to",
          "avoid",
          "reinitializing",
          "tensors"
        ],
        "docstring_summary": "[Note: On ``init_weights`` vs.",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "Transformer",
        "start_line": 386,
        "end_line": 414,
        "hash": "94da5e7255f0ac07771c9b7856d1f3af",
        "complexity": 3,
        "parameters": []
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py",
        "func_name": "__init__",
        "original_string": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())",
        "language": "python",
        "code": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())",
        "code_tokens": [
          "def",
          "__init__",
          "(",
          "self",
          ",",
          "env",
          ":",
          "str",
          ",",
          "gamma",
          ":",
          "float",
          "=",
          "0",
          ".",
          "99",
          ",",
          "lam",
          ":",
          "float",
          "=",
          "0",
          ".",
          "95",
          ",",
          "lr_actor",
          ":",
          "float",
          "=",
          "3e",
          "-",
          "4",
          ",",
          "lr_critic",
          ":",
          "float",
          "=",
          "1e",
          "-",
          "3",
          ",",
          "max_episode_len",
          ":",
          "float",
          "=",
          "200",
          ",",
          "batch_size",
          ":",
          "int",
          "=",
          "512",
          ",",
          "steps_per_epoch",
          ":",
          "int",
          "=",
          "2048",
          ",",
          "nb_optim_iters",
          ":",
          "int",
          "=",
          "4",
          ",",
          "clip_ratio",
          ":",
          "float",
          "=",
          "0",
          ".",
          "2",
          ",",
          "*",
          "*",
          "kwargs",
          ",",
          ")",
          "-",
          ">",
          "None",
          ":",
          "\"",
          "\"",
          "\"",
          "Args",
          ":",
          "env",
          ":",
          "gym",
          "environment",
          "tag",
          "gamma",
          ":",
          "discount",
          "factor",
          "lam",
          ":",
          "advantage",
          "discount",
          "factor",
          "(",
          "lambda",
          "in",
          "the",
          "paper",
          ")",
          "lr_actor",
          ":",
          "learning",
          "rate",
          "of",
          "actor",
          "network",
          "lr_critic",
          ":",
          "learning",
          "rate",
          "of",
          "critic",
          "network",
          "max_episode_len",
          ":",
          "maximum",
          "number",
          "interactions",
          "(",
          "actions",
          ")",
          "in",
          "an",
          "episode",
          "batch_size",
          ":",
          "batch_size",
          "when",
          "training",
          "network",
          "-",
          "can",
          "simulate",
          "number",
          "of",
          "policy",
          "updates",
          "performed",
          "per",
          "epoch",
          "steps_per_epoch",
          ":",
          "how",
          "many",
          "action",
          "-",
          "state",
          "pairs",
          "to",
          "rollout",
          "for",
          "trajectory",
          "collection",
          "per",
          "epoch",
          "nb_optim_iters",
          ":",
          "how",
          "many",
          "steps",
          "of",
          "gradient",
          "descent",
          "to",
          "perform",
          "on",
          "each",
          "batch",
          "clip_ratio",
          ":",
          "hyperparameter",
          "for",
          "clipping",
          "in",
          "the",
          "policy",
          "objective",
          "\"",
          "\"",
          "\"",
          "super",
          "(",
          ")",
          ".",
          "__init__",
          "(",
          ")",
          "self",
          ".",
          "lr_actor",
          "=",
          "lr_actor",
          "self",
          ".",
          "lr_critic",
          "=",
          "lr_critic",
          "self",
          ".",
          "steps_per_epoch",
          "=",
          "steps_per_epoch",
          "self",
          ".",
          "nb_optim_iters",
          "=",
          "nb_optim_iters",
          "self",
          ".",
          "batch_size",
          "=",
          "batch_size",
          "self",
          ".",
          "gamma",
          "=",
          "gamma",
          "self",
          ".",
          "lam",
          "=",
          "lam",
          "self",
          ".",
          "max_episode_len",
          "=",
          "max_episode_len",
          "self",
          ".",
          "clip_ratio",
          "=",
          "clip_ratio",
          "self",
          ".",
          "save_hyperparameters",
          "(",
          ")",
          "self",
          ".",
          "automatic_optimization",
          "=",
          "False",
          "self",
          ".",
          "env",
          "=",
          "gym",
          ".",
          "make",
          "(",
          "env",
          ")",
          "self",
          ".",
          "critic",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "1",
          ")",
          "if",
          "isinstance",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ",",
          "gym",
          ".",
          "spaces",
          ".",
          "box",
          ".",
          "Box",
          ")",
          ":",
          "act_dim",
          "=",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ".",
          "shape",
          "[",
          "0",
          "]",
          "actor_mlp",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "act_dim",
          ")",
          "self",
          ".",
          "actor",
          "=",
          "ActorContinuous",
          "(",
          "actor_mlp",
          ",",
          "act_dim",
          ")",
          "elif",
          "isinstance",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ",",
          "gym",
          ".",
          "spaces",
          ".",
          "discrete",
          ".",
          "Discrete",
          ")",
          ":",
          "actor_mlp",
          "=",
          "create_mlp",
          "(",
          "self",
          ".",
          "env",
          ".",
          "observation_space",
          ".",
          "shape",
          ",",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ".",
          "n",
          ")",
          "self",
          ".",
          "actor",
          "=",
          "ActorCategorical",
          "(",
          "actor_mlp",
          ")",
          "else",
          ":",
          "raise",
          "NotImplementedError",
          "(",
          "\"",
          "Env",
          "action",
          "space",
          "should",
          "be",
          "of",
          "type",
          "Box",
          "(",
          "continuous",
          ")",
          "or",
          "Discrete",
          "(",
          "categorical",
          ")",
          ".",
          "\"",
          "f",
          "\"",
          "Got",
          "type",
          ":",
          "{",
          "type",
          "(",
          "self",
          ".",
          "env",
          ".",
          "action_space",
          ")",
          "}",
          "\"",
          ")",
          "self",
          ".",
          "batch_states",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_actions",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_adv",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_qvals",
          "=",
          "[",
          "]",
          "self",
          ".",
          "batch_logp",
          "=",
          "[",
          "]",
          "self",
          ".",
          "ep_rewards",
          "=",
          "[",
          "]",
          "self",
          ".",
          "ep_values",
          "=",
          "[",
          "]",
          "self",
          ".",
          "epoch_rewards",
          "=",
          "[",
          "]",
          "self",
          ".",
          "episode_step",
          "=",
          "0",
          "self",
          ".",
          "avg_ep_reward",
          "=",
          "0",
          "self",
          ".",
          "avg_ep_len",
          "=",
          "0",
          "self",
          ".",
          "avg_reward",
          "=",
          "0",
          "self",
          ".",
          "state",
          "=",
          "torch",
          ".",
          "FloatTensor",
          "(",
          "self",
          ".",
          "env",
          ".",
          "reset",
          "(",
          ")",
          ")"
        ],
        "docstring": "Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective",
        "docstring_tokens": [
          "args",
          "env",
          "gym",
          "environment",
          "tag",
          "gamma",
          "discount",
          "factor",
          "lam",
          "advantage",
          "discount",
          "factor",
          "lambda",
          "in",
          "the",
          "paper",
          "lr_actor",
          "learning",
          "rate",
          "of",
          "actor",
          "network",
          "lr_critic",
          "learning",
          "rate",
          "of",
          "critic",
          "network",
          "max_episode_len",
          "maximum",
          "number",
          "interactions",
          "actions",
          "in",
          "an",
          "episode",
          "batch_size",
          "batch_size",
          "when",
          "training",
          "network",
          "can",
          "simulate",
          "number",
          "of",
          "policy",
          "updates",
          "performed",
          "per",
          "epoch",
          "steps_per_epoch",
          "how",
          "many",
          "action",
          "state",
          "pairs",
          "to",
          "rollout",
          "for",
          "trajectory",
          "collection",
          "per",
          "epoch",
          "nb_optim_iters",
          "how",
          "many",
          "steps",
          "of",
          "gradient",
          "descent",
          "to",
          "perform",
          "on",
          "each",
          "batch",
          "clip_ratio",
          "hyperparameter",
          "for",
          "clipping",
          "in",
          "the",
          "policy",
          "objective"
        ],
        "docstring_summary": "Args:",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "PPOLightning",
        "start_line": 154,
        "end_line": 229,
        "hash": "4c413b034484f7c6c2f3c02d3031e0ef",
        "complexity": 3,
        "parameters": [
          "env",
          "gamma",
          "lam",
          "lr_actor",
          "lr_critic",
          "max_episode_len",
          "batch_size",
          "steps_per_epoch",
          "nb_optim_iters",
          "clip_ratio",
          "**kwargs"
        ]
      },
      {
        "repo": "pytorch-lightning",
        "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py",
        "func_name": "training_step",
        "original_string": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\r\n        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics\r\n\r\n        \"\"\"\r\n        device = self.get_device(batch)\r\n        epsilon = max(self.eps_end, self.eps_start - (self.global_step + 1) / self.eps_last_frame)\r\n\r\n        # step through environment with agent\r\n        reward, done = self.agent.play_step(self.net, epsilon, device)\r\n        self.episode_reward += reward\r\n\r\n        # calculates training loss\r\n        loss = self.dqn_mse_loss(batch)\r\n\r\n        if done:\r\n            self.total_reward = self.episode_reward\r\n            self.episode_reward = 0\r\n\r\n        # Soft update of target network\r\n        if self.global_step % self.sync_rate == 0:\r\n            self.target_net.load_state_dict(self.net.state_dict())\r\n\r\n        log = {\r\n            \"total_reward\": torch.tensor(self.total_reward).to(device),\r\n            \"reward\": torch.tensor(reward).to(device),\r\n            \"steps\": torch.tensor(self.global_step).to(device),\r\n        }\r\n\r\n        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": log})",
        "language": "python",
        "code": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\r\n        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics\r\n\r\n        \"\"\"\r\n        device = self.get_device(batch)\r\n        epsilon = max(self.eps_end, self.eps_start - (self.global_step + 1) / self.eps_last_frame)\r\n\r\n        # step through environment with agent\r\n        reward, done = self.agent.play_step(self.net, epsilon, device)\r\n        self.episode_reward += reward\r\n\r\n        # calculates training loss\r\n        loss = self.dqn_mse_loss(batch)\r\n\r\n        if done:\r\n            self.total_reward = self.episode_reward\r\n            self.episode_reward = 0\r\n\r\n        # Soft update of target network\r\n        if self.global_step % self.sync_rate == 0:\r\n            self.target_net.load_state_dict(self.net.state_dict())\r\n\r\n        log = {\r\n            \"total_reward\": torch.tensor(self.total_reward).to(device),\r\n            \"reward\": torch.tensor(reward).to(device),\r\n            \"steps\": torch.tensor(self.global_step).to(device),\r\n        }\r\n\r\n        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": log})",
        "code_tokens": [
          "def",
          "training_step",
          "(",
          "self",
          ",",
          "batch",
          ":",
          "tuple",
          "[",
          "torch",
          ".",
          "Tensor",
          ",",
          "torch",
          ".",
          "Tensor",
          "]",
          ",",
          "nb_batch",
          ")",
          "-",
          ">",
          "OrderedDict",
          ":",
          "\"",
          "\"",
          "\"",
          "Carries",
          "out",
          "a",
          "single",
          "step",
          "through",
          "the",
          "environment",
          "to",
          "update",
          "the",
          "replay",
          "buffer",
          ".",
          "Then",
          "calculates",
          "loss",
          "based",
          "on",
          "the",
          "minibatch",
          "received",
          ".",
          "Args",
          ":",
          "batch",
          ":",
          "current",
          "mini",
          "batch",
          "of",
          "replay",
          "data",
          "nb_batch",
          ":",
          "batch",
          "number",
          "Returns",
          ":",
          "Training",
          "loss",
          "and",
          "log",
          "metrics",
          "\"",
          "\"",
          "\"",
          "device",
          "=",
          "self",
          ".",
          "get_device",
          "(",
          "batch",
          ")",
          "epsilon",
          "=",
          "max",
          "(",
          "self",
          ".",
          "eps_end",
          ",",
          "self",
          ".",
          "eps_start",
          "-",
          "(",
          "self",
          ".",
          "global_step",
          "+",
          "1",
          ")",
          "/",
          "self",
          ".",
          "eps_last_frame",
          ")",
          "reward",
          ",",
          "done",
          "=",
          "self",
          ".",
          "agent",
          ".",
          "play_step",
          "(",
          "self",
          ".",
          "net",
          ",",
          "epsilon",
          ",",
          "device",
          ")",
          "self",
          ".",
          "episode_reward",
          "+",
          "=",
          "reward",
          "loss",
          "=",
          "self",
          ".",
          "dqn_mse_loss",
          "(",
          "batch",
          ")",
          "if",
          "done",
          ":",
          "self",
          ".",
          "total_reward",
          "=",
          "self",
          ".",
          "episode_reward",
          "self",
          ".",
          "episode_reward",
          "=",
          "0",
          "if",
          "self",
          ".",
          "global_step",
          "%",
          "self",
          ".",
          "sync_rate",
          "=",
          "=",
          "0",
          ":",
          "self",
          ".",
          "target_net",
          ".",
          "load_state_dict",
          "(",
          "self",
          ".",
          "net",
          ".",
          "state_dict",
          "(",
          ")",
          ")",
          "log",
          "=",
          "{",
          "\"",
          "total_reward",
          "\"",
          ":",
          "torch",
          ".",
          "tensor",
          "(",
          "self",
          ".",
          "total_reward",
          ")",
          ".",
          "to",
          "(",
          "device",
          ")",
          ",",
          "\"",
          "reward",
          "\"",
          ":",
          "torch",
          ".",
          "tensor",
          "(",
          "reward",
          ")",
          ".",
          "to",
          "(",
          "device",
          ")",
          ",",
          "\"",
          "steps",
          "\"",
          ":",
          "torch",
          ".",
          "tensor",
          "(",
          "self",
          ".",
          "global_step",
          ")",
          ".",
          "to",
          "(",
          "device",
          ")",
          ",",
          "}",
          "return",
          "OrderedDict",
          "(",
          "{",
          "\"",
          "loss",
          "\"",
          ":",
          "loss",
          ",",
          "\"",
          "log",
          "\"",
          ":",
          "log",
          ",",
          "\"",
          "progress_bar",
          "\"",
          ":",
          "log",
          "}",
          ")"
        ],
        "docstring": "Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics",
        "docstring_tokens": [
          "carries",
          "out",
          "a",
          "single",
          "step",
          "through",
          "the",
          "environment",
          "to",
          "update",
          "the",
          "replay",
          "buffer",
          "then",
          "calculates",
          "loss",
          "based",
          "on",
          "the",
          "minibatch",
          "received",
          "args",
          "batch",
          "current",
          "mini",
          "batch",
          "of",
          "replay",
          "data",
          "nb_batch",
          "batch",
          "number",
          "returns",
          "training",
          "loss",
          "and",
          "log",
          "metrics"
        ],
        "docstring_summary": "Carries out a single step through the environment to update the replay buffer. Then calculates loss based on",
        "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py",
        "partition": "train",
        "function_type": "class_method",
        "class_name": "DQNLightning",
        "start_line": 321,
        "end_line": 357,
        "hash": "84b3939a1240065a02cb6e28f5ccfedb",
        "complexity": 3,
        "parameters": [
          "batch",
          "torch.Tensor]",
          "nb_batch"
        ]
      }
    ]
  }
]