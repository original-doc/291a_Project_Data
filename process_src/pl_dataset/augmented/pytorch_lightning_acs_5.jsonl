{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_0", "original_string": "def _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]", "language": "python", "code": "def _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]", "code_tokens": ["def", "_get_supported_strategies", "(", ")", "-", ">", "list", "[", "str", "]", ":", "STRING", "available_strategies", "=", "STRATEGY_REGISTRY", ".", "available_strategies", "(", ")", "excluded", "=", "rSTRING", "return", "[", "strategy", "for", "strategy", "in", "available_strategies", "if", "not", "re", ".", "match", "(", "excluded", ",", "strategy", ")", "]"], "docstring": "Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the", "docstring_tokens": ["returns", "strategy", "choices", "from", "the", "registry", "with", "the", "ones", "removed", "that", "are", "incompatible", "to", "be", "launched", "from", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 39, "end_line": 44, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_1", "original_string": "def _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)", "language": "python", "code": "def _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)", "code_tokens": ["def", "_run", "(", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "script_args", "=", "list", "(", "kwargs", ".", "pop", "(", "STRING", ",", "[", "]", ")", ")", "main", "(", "args", "=", "Namespace", "(", "*", "*", "kwargs", ")", ",", "script_args", "=", "script_args", ")"], "docstring": "Run a Lightning Fabric script.", "docstring_tokens": ["run", "a", "lightning", "fabric", "script"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 126, "end_line": 136, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_2", "original_string": "def _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)", "language": "python", "code": "def _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)", "code_tokens": ["def", "_consolidate", "(", "checkpoint_folder", ":", "str", ",", "output_file", ":", "Optional", "[", "str", "]", ")", "-", ">", "None", ":", "STRING", "args", "=", "Namespace", "(", "checkpoint_folder", "=", "checkpoint_folder", ",", "output_file", "=", "output_file", ")", "config", "=", "_process_cli_args", "(", "args", ")", "checkpoint", "=", "_load_distributed_checkpoint", "(", "config", ".", "checkpoint_folder", ")", "torch", ".", "save", "(", "checkpoint", ",", "config", ".", "output_file", ")"], "docstring": "Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.", "docstring_tokens": ["convert", "a", "distributed", "sharded", "checkpoint", "into", "a", "single", "file", "that", "can", "be", "loaded", "with", "torch", "load"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 158, "end_line": 167, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_3", "original_string": "def _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)", "language": "python", "code": "def _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)", "code_tokens": ["def", "_set_env_variables", "(", "args", ":", "Namespace", ")", "-", ">", "None", ":", "STRING", "os", ".", "environ", "[", "STRING", "]", "=", "STRING", "if", "args", ".", "accelerator", "is", "not", "None", ":", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "args", ".", "accelerator", ")", "if", "args", ".", "strategy", "is", "not", "None", ":", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "args", ".", "strategy", ")", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "args", ".", "devices", ")", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "args", ".", "num_nodes", ")", "if", "args", ".", "precision", "is", "not", "None", ":", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "args", ".", "precision", ")"], "docstring": "Set the environment variables for the new processes.", "docstring_tokens": ["set", "the", "environment", "variables", "for", "the", "new", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 170, "end_line": 184, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_4", "original_string": "def _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0", "language": "python", "code": "def _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0", "code_tokens": ["def", "_get_num_processes", "(", "accelerator", ":", "str", ",", "devices", ":", "str", ")", "-", ">", "int", ":", "STRING", "if", "accelerator", "=", "=", "STRING", "or", "accelerator", "is", "None", ":", "accelerator", "=", "_select_auto_accelerator", "(", ")", "if", "devices", "=", "=", "STRING", ":", "if", "accelerator", "=", "=", "STRING", "or", "accelerator", "=", "=", "STRING", "or", "accelerator", "=", "=", "STRING", ":", "devices", "=", "STRING", "else", ":", "raise", "ValueError", "(", "fSTRING", ")", "if", "accelerator", "=", "=", "STRING", ":", "parsed_devices", "=", "_parse_gpu_ids", "(", "devices", ",", "include_cuda", "=", "True", ",", "include_mps", "=", "True", ")", "elif", "accelerator", "=", "=", "STRING", ":", "parsed_devices", "=", "CUDAAccelerator", ".", "parse_devices", "(", "devices", ")", "elif", "accelerator", "=", "=", "STRING", ":", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "elif", "accelerator", "=", "=", "STRING", ":", "raise", "ValueError", "(", "STRING", ")", "else", ":", "return", "CPUAccelerator", ".", "parse_devices", "(", "devices", ")", "return", "len", "(", "parsed_devices", ")", "if", "parsed_devices", "is", "not", "None", "else", "0"], "docstring": "Parse the `devices` argument to determine how many processes need to be launched on the current machine.", "docstring_tokens": ["parse", "the", "devices", "argument", "to", "determine", "how", "many", "processes", "need", "to", "be", "launched", "on", "the", "current", "machine"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 187, "end_line": 207, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "function_5", "original_string": "def _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)", "language": "python", "code": "def _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)", "code_tokens": ["def", "_torchrun_launch", "(", "args", ":", "Namespace", ",", "script_args", ":", "list", "[", "str", "]", ")", "-", ">", "None", ":", "STRING", "import", "torch", ".", "distributed", ".", "run", "as", "torchrun", "num_processes", "=", "1", "if", "args", ".", "strategy", "=", "=", "STRING", "else", "_get_num_processes", "(", "args", ".", "accelerator", ",", "args", ".", "devices", ")", "torchrun_args", "=", "[", "fSTRING", ",", "fSTRING", ",", "fSTRING", ",", "fSTRING", ",", "fSTRING", ",", "args", ".", "script", ",", "]", "torchrun_args", ".", "extend", "(", "script_args", ")", "os", ".", "environ", ".", "setdefault", "(", "STRING", ",", "str", "(", "_suggested_max_num_threads", "(", ")", ")", ")", "torchrun", ".", "main", "(", "torchrun_args", ")"], "docstring": "This will invoke `torchrun` programmatically to launch the given script in new processes.", "docstring_tokens": ["this", "will", "invoke", "torchrun", "programmatically", "to", "launch", "the", "given", "script", "in", "new", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\cli.py", "start_line": 210, "end_line": 228, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "function_6", "original_string": "def _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.fabric.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, dp, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._registered_accelerators\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._registered_accelerators)}.\"\r\n            )\r\n\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_dp_str = isinstance(strategy, str) and \"dp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_dp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_input = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_instance = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                else:\r\n                    raise TypeError(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise ValueError(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_input is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_input}` and `plugins={self._precision_instance}`. Choose one.\"\r\n                )\r\n\r\n        self._precision_input = \"32-true\" if precision_input is None else precision_input\r\n\r\n        if isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise ValueError(\"accelerator set through both strategy class and accelerator flag, choose one\")\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision:\r\n                if self._precision_instance:\r\n                    raise ValueError(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_instance = self._strategy_flag._precision\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise ValueError(\"checkpoint_io set through both strategy class and plugins, choose one\")\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise ValueError(\"cluster_environment set through both strategy class and plugins, choose one\")\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise ValueError(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise ValueError(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices", "language": "python", "code": "def _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.fabric.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, dp, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._registered_accelerators\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._registered_accelerators)}.\"\r\n            )\r\n\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_dp_str = isinstance(strategy, str) and \"dp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_dp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_input = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_instance = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                else:\r\n                    raise TypeError(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise ValueError(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_input is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_input}` and `plugins={self._precision_instance}`. Choose one.\"\r\n                )\r\n\r\n        self._precision_input = \"32-true\" if precision_input is None else precision_input\r\n\r\n        if isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise ValueError(\"accelerator set through both strategy class and accelerator flag, choose one\")\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision:\r\n                if self._precision_instance:\r\n                    raise ValueError(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_instance = self._strategy_flag._precision\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise ValueError(\"checkpoint_io set through both strategy class and plugins, choose one\")\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise ValueError(\"cluster_environment set through both strategy class and plugins, choose one\")\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise ValueError(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise ValueError(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices", "code_tokens": ["def", "_check_config_and_set_final_flags", "(", "self", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", ",", "precision", ":", "Optional", "[", "_PRECISION_INPUT", "]", ",", "plugins", ":", "Optional", "[", "Union", "[", "_PLUGIN_INPUT", ",", "Iterable", "[", "_PLUGIN_INPUT", "]", "]", "]", ",", ")", "-", ">", "None", ":", "STRING", "if", "plugins", "is", "not", "None", ":", "plugins", "=", "[", "plugins", "]", "if", "not", "isinstance", "(", "plugins", ",", "Iterable", ")", "else", "plugins", "if", "isinstance", "(", "strategy", ",", "str", ")", ":", "strategy", "=", "strategy", ".", "lower", "(", ")", "self", ".", "_strategy_flag", "=", "strategy", "if", "strategy", "!", "=", "STRING", "and", "strategy", "not", "in", "self", ".", "_registered_strategies", "and", "not", "isinstance", "(", "strategy", ",", "Strategy", ")", ":", "raise", "ValueError", "(", "fSTRING", "STRING", "STRING", "STRING", ")", "if", "(", "accelerator", "not", "in", "self", ".", "_registered_accelerators", "and", "accelerator", "not", "in", "(", "STRING", ",", "STRING", ")", "and", "not", "isinstance", "(", "accelerator", ",", "Accelerator", ")", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "is_ddp_str", "=", "isinstance", "(", "strategy", ",", "str", ")", "and", "STRING", "in", "strategy", "is_dp_str", "=", "isinstance", "(", "strategy", ",", "str", ")", "and", "STRING", "in", "strategy", "is_deepspeed_str", "=", "isinstance", "(", "strategy", ",", "str", ")", "and", "STRING", "in", "strategy", "is_parallel_strategy", "=", "isinstance", "(", "strategy", ",", "ParallelStrategy", ")", "or", "is_ddp_str", "or", "is_dp_str", "or", "is_deepspeed_str", "is_mps_accelerator", "=", "MPSAccelerator", ".", "is_available", "(", ")", "and", "(", "accelerator", "in", "(", "STRING", ",", "STRING", ",", "STRING", ",", "None", ")", "or", "isinstance", "(", "accelerator", ",", "MPSAccelerator", ")", ")", "if", "is_mps_accelerator", "and", "is_parallel_strategy", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "self", ".", "_accelerator_flag", "=", "accelerator", "precision_input", "=", "_convert_precision_to_unified_args", "(", "precision", ")", "if", "plugins", ":", "plugins_flags_types", ":", "dict", "[", "str", ",", "int", "]", "=", "Counter", "(", ")", "for", "plugin", "in", "plugins", ":", "if", "isinstance", "(", "plugin", ",", "Precision", ")", ":", "self", ".", "_precision_instance", "=", "plugin", "plugins_flags_types", "[", "Precision", ".", "__name__", "]", "+", "=", "1", "elif", "isinstance", "(", "plugin", ",", "CheckpointIO", ")", ":", "self", ".", "checkpoint_io", "=", "plugin", "plugins_flags_types", "[", "CheckpointIO", ".", "__name__", "]", "+", "=", "1", "elif", "isinstance", "(", "plugin", ",", "ClusterEnvironment", ")", ":", "self", ".", "_cluster_environment_flag", "=", "plugin", "plugins_flags_types", "[", "ClusterEnvironment", ".", "__name__", "]", "+", "=", "1", "else", ":", "raise", "TypeError", "(", "fSTRING", "STRING", ")", "duplicated_plugin_key", "=", "[", "k", "for", "k", ",", "v", "in", "plugins_flags_types", ".", "items", "(", ")", "if", "v", ">", "1", "]", "if", "duplicated_plugin_key", ":", "raise", "ValueError", "(", "fSTRING", "STRING", ")", "if", "plugins_flags_types", ".", "get", "(", "Precision", ".", "__name__", ")", "and", "precision_input", "is", "not", "None", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "_precision_input", "=", "STRING", "if", "precision_input", "is", "None", "else", "precision_input", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", ":", "if", "self", ".", "_strategy_flag", ".", "_accelerator", ":", "if", "self", ".", "_accelerator_flag", "!", "=", "STRING", ":", "raise", "ValueError", "(", "STRING", ")", "self", ".", "_accelerator_flag", "=", "self", ".", "_strategy_flag", ".", "_accelerator", "if", "self", ".", "_strategy_flag", ".", "_precision", ":", "if", "self", ".", "_precision_instance", ":", "raise", "ValueError", "(", "STRING", ")", "self", ".", "_precision_instance", "=", "self", ".", "_strategy_flag", ".", "_precision", "if", "self", ".", "_strategy_flag", ".", "_checkpoint_io", ":", "if", "self", ".", "checkpoint_io", ":", "raise", "ValueError", "(", "STRING", ")", "self", ".", "checkpoint_io", "=", "self", ".", "_strategy_flag", ".", "_checkpoint_io", "if", "getattr", "(", "self", ".", "_strategy_flag", ",", "STRING", ",", "None", ")", ":", "if", "self"], "docstring": "This method checks:", "docstring_tokens": ["this", "method", "checks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\connector.py", "start_line": 163, "end_line": 295, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "function_7", "original_string": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"", "language": "python", "code": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"", "code_tokens": ["def", "_choose_auto_accelerator", "(", ")", "-", ">", "str", ":", "STRING", "if", "XLAAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "if", "MPSAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "if", "CUDAAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "return", "STRING"], "docstring": "Choose the accelerator type (str) based on availability when ``accelerator='auto'``.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability", "when", "accelerator", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\connector.py", "start_line": 316, "end_line": 324, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "function_8", "original_string": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "language": "python", "code": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "code_tokens": ["def", "_check_strategy_and_fallback", "(", "self", ")", "-", ">", "None", ":", "STRING", "strategy_flag", "=", "STRING", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", "else", "self", ".", "_strategy_flag", "if", "strategy_flag", "=", "=", "STRING", "and", "self", ".", "_accelerator_flag", "=", "=", "STRING", ":", "strategy_flag", "=", "STRING", "if", "strategy_flag", "=", "=", "STRING", "and", "self", ".", "_accelerator_flag", "=", "=", "STRING", ":", "rank_zero_warn", "(", "fSTRING", ")", "strategy_flag", "=", "STRING", "if", "strategy_flag", "in", "_DDP_FORK_ALIASES", "and", "STRING", "not", "in", "torch", ".", "multiprocessing", ".", "get_all_start_methods", "(", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "if", "(", "strategy_flag", "in", "_FSDP_ALIASES", "or", "type", "(", "self", ".", "_strategy_flag", ")", "is", "FSDPStrategy", ")", "and", "self", ".", "_accelerator_flag", "not", "in", "(", "STRING", ",", "STRING", ")", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "if", "strategy_flag", ":", "self", ".", "_strategy_flag", "=", "strategy_flag"], "docstring": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different", "docstring_tokens": ["checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", "and", "we", "need", "to", "fall", "back", "to", "a", "different"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\connector.py", "start_line": 414, "end_line": 440, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "function_9", "original_string": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "language": "python", "code": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "code_tokens": ["def", "_init_strategy", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "isinstance", "(", "self", ".", "_strategy_flag", ",", "(", "str", ",", "Strategy", ")", ")", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "str", ")", ":", "self", ".", "strategy", "=", "STRATEGY_REGISTRY", ".", "get", "(", "self", ".", "_strategy_flag", ")", "else", ":", "self", ".", "strategy", "=", "self", ".", "_strategy_flag"], "docstring": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "docstring_tokens": ["instantiate", "the", "strategy", "given", "depending", "on", "the", "setting", "of", "_strategy_flag"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\connector.py", "start_line": 442, "end_line": 449, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "function_10", "original_string": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )", "language": "python", "code": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )", "code_tokens": ["def", "_lazy_init_strategy", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "strategy", ".", "accelerator", "=", "self", ".", "accelerator", "if", "self", ".", "precision", ":", "self", ".", "strategy", ".", "precision", "=", "self", ".", "precision", "if", "self", ".", "checkpoint_io", ":", "self", ".", "strategy", ".", "checkpoint_io", "=", "self", ".", "checkpoint_io", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "if", "self", ".", "strategy", ".", "cluster_environment", "is", "None", ":", "self", ".", "strategy", ".", "cluster_environment", "=", "self", ".", "cluster_environment", "self", ".", "cluster_environment", "=", "self", ".", "strategy", ".", "cluster_environment", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "if", "self", ".", "strategy", ".", "parallel_devices", ":", "self", ".", "_parallel_devices", "=", "self", ".", "strategy", ".", "parallel_devices", "else", ":", "self", ".", "strategy", ".", "parallel_devices", "=", "self", ".", "_parallel_devices", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "self", ".", "strategy", ".", "_num_nodes", "=", "self", ".", "_num_nodes_flag", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "self", ".", "strategy", ".", "_set_world_ranks", "(", ")", "self", ".", "strategy", ".", "_configure_launcher", "(", ")", "if", "_IS_INTERACTIVE", "and", "self", ".", "strategy", ".", "launcher", "and", "not", "self", ".", "strategy", ".", "launcher", ".", "is_interactive_compatible", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", "fSTRING", "STRING", "STRING", ")", "if", "isinstance", "(", "self", ".", "accelerator", ",", "XLAAccelerator", ")", "and", "not", "isinstance", "(", "self", ".", "strategy", ",", "(", "SingleDeviceXLAStrategy", ",", "XLAStrategy", ",", "XLAFSDPStrategy", ")", ")", ":", "raise", "ValueError", "(", "STRING", "fSTRING", ")"], "docstring": "Lazily set missing attributes on the previously instantiated strategy.", "docstring_tokens": ["lazily", "set", "missing", "attributes", "on", "the", "previously", "instantiated", "strategy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\connector.py", "start_line": 499, "end_line": 538, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_11", "original_string": "def device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device", "language": "python", "code": "def device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device", "code_tokens": ["def", "device", "(", "self", ")", "-", ">", "torch", ".", "device", ":", "STRING", "return", "self", ".", "_strategy", ".", "root_device"], "docstring": "The current device this process runs on.", "docstring_tokens": ["the", "current", "device", "this", "process", "runs", "on"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 178, "end_line": 184, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_12", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The global index of the current process across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"global_rank\", 0)", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The global index of the current process across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"global_rank\", 0)", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "getattr", "(", "self", ".", "_strategy", ",", "STRING", ",", "0", ")"], "docstring": "The global index of the current process across all devices and nodes.", "docstring_tokens": ["the", "global", "index", "of", "the", "current", "process", "across", "all", "devices", "and", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 187, "end_line": 189, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_13", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The index of the current process among the processes running on the local node.\"\"\"\r\n        return getattr(self._strategy, \"local_rank\", 0)", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The index of the current process among the processes running on the local node.\"\"\"\r\n        return getattr(self._strategy, \"local_rank\", 0)", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "getattr", "(", "self", ".", "_strategy", ",", "STRING", ",", "0", ")"], "docstring": "The index of the current process among the processes running on the local node.", "docstring_tokens": ["the", "index", "of", "the", "current", "process", "among", "the", "processes", "running", "on", "the", "local", "node"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 192, "end_line": 194, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_14", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The index of the current node.\"\"\"\r\n        return getattr(self._strategy, \"node_rank\", 0)", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The index of the current node.\"\"\"\r\n        return getattr(self._strategy, \"node_rank\", 0)", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "getattr", "(", "self", ".", "_strategy", ",", "STRING", ",", "0", ")"], "docstring": "The index of the current node.", "docstring_tokens": ["the", "index", "of", "the", "current", "node"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 197, "end_line": 199, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_15", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The total number of processes running across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"world_size\", 1)", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The total number of processes running across all devices and nodes.\"\"\"\r\n        return getattr(self._strategy, \"world_size\", 1)", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "getattr", "(", "self", ".", "_strategy", ",", "STRING", ",", "1", ")"], "docstring": "The total number of processes running across all devices and nodes.", "docstring_tokens": ["the", "total", "number", "of", "processes", "running", "across", "all", "devices", "and", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 202, "end_line": 204, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_16", "original_string": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this rank is rank zero.\"\"\"\r\n        return self._strategy.is_global_zero", "language": "python", "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this rank is rank zero.\"\"\"\r\n        return self._strategy.is_global_zero", "code_tokens": ["def", "is_global_zero", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_strategy", ".", "is_global_zero"], "docstring": "Whether this rank is rank zero.", "docstring_tokens": ["whether", "this", "rank", "is", "rank", "zero"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 207, "end_line": 209, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_17", "original_string": "def loggers(self) -> list[Logger]:\r\n        \"\"\"Returns all loggers passed to Fabric.\"\"\"\r\n        return self._loggers", "language": "python", "code": "def loggers(self) -> list[Logger]:\r\n        \"\"\"Returns all loggers passed to Fabric.\"\"\"\r\n        return self._loggers", "code_tokens": ["def", "loggers", "(", "self", ")", "-", ">", "list", "[", "Logger", "]", ":", "STRING", "return", "self", ".", "_loggers"], "docstring": "Returns all loggers passed to Fabric.", "docstring_tokens": ["returns", "all", "loggers", "passed", "to", "fabric"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 212, "end_line": 214, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_18", "original_string": "def logger(self) -> Logger:\r\n        \"\"\"Returns the first logger in the list passed to Fabric, which is considered the main logger.\"\"\"\r\n        return self._loggers[0]", "language": "python", "code": "def logger(self) -> Logger:\r\n        \"\"\"Returns the first logger in the list passed to Fabric, which is considered the main logger.\"\"\"\r\n        return self._loggers[0]", "code_tokens": ["def", "logger", "(", "self", ")", "-", ">", "Logger", ":", "STRING", "return", "self", ".", "_loggers", "[", "0", "]"], "docstring": "Returns the first logger in the list passed to Fabric, which is considered the main logger.", "docstring_tokens": ["returns", "the", "first", "logger", "in", "the", "list", "passed", "to", "fabric", "which", "is", "considered", "the", "main", "logger"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 217, "end_line": 219, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_19", "original_string": "def run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"", "language": "python", "code": "def run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"", "code_tokens": ["def", "run", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING"], "docstring": "All the code inside this run method gets accelerated by Fabric.", "docstring_tokens": ["all", "the", "code", "inside", "this", "run", "method", "gets", "accelerated", "by", "fabric"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 221, "end_line": 226, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_20", "original_string": "def setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module", "language": "python", "code": "def setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module", "code_tokens": ["def", "setup", "(", "self", ",", "module", ":", "nn", ".", "Module", ",", "*", "optimizers", ":", "Optimizer", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "move_to_device", ":", "bool", "=", "True", ",", "_reapply_compile", ":", "bool", "=", "True", ",", ")", "-", ">", "Any", ":", "#", "no", "specific", "return", "because", "the", "way", "we", "want", "our", "API", "to", "look", "does", "not", "play", "well", "with", "mypy", "rSTRING", "self", ".", "_validate_setup", "(", "module", ",", "optimizers", ")", "module", ",", "compile_kwargs", "=", "_unwrap_compiled", "(", "module", ")", "if", "_reapply_compile", "else", "(", "module", ",", "None", ")", "original_module", "=", "module", "module", "=", "self", ".", "_precision", ".", "convert_module", "(", "module", ")", "if", "move_to_device", ":", "module", "=", "self", ".", "_move_model_to_device", "(", "model", "=", "module", ",", "optimizers", "=", "list", "(", "optimizers", ")", ")", "if", "optimizers", ":", "module", ",", "optimizers", ",", "scheduler", "=", "self", ".", "_strategy", ".", "setup_module_and_optimizers", "(", "#", "type", ":", "ignore", "[", "assignment", "]", "module", ",", "list", "(", "optimizers", ")", ",", "scheduler", ")", "else", ":", "module", "=", "self", ".", "_strategy", ".", "setup_module", "(", "module", ")", "if", "compile_kwargs", "is", "not", "None", ":", "module", "=", "_to_compiled", "(", "module", ",", "compile_kwargs", ")", "module", "=", "_FabricModule", "(", "module", ",", "self", ".", "_strategy", ",", "original_module", "=", "original_module", ")", "_update_properties", "(", "module", ",", "device", "=", "self", ".", "device", "if", "move_to_device", "else", "next", "(", "module", ".", "parameters", "(", ")", ",", "torch", ".", "tensor", "(", "0", ")", ")", ".", "device", ")", "optimizers", "=", "[", "_FabricOptimizer", "(", "optimizer", ",", "self", ".", "_strategy", ",", "self", ".", "_callbacks", ")", "for", "optimizer", "in", "optimizers", "]", "self", ".", "_models_setup", "+", "=", "1", "if", "hasattr", "(", "original_module", ",", "STRING", ")", ":", "#", "this", "is", "probably", "a", "LightningModule", "original_module", ".", "_fabric", "=", "self", "original_module", ".", "_fabric_optimizers", "=", "optimizers", "if", "original_module", "not", "in", "self", ".", "_callbacks", ":", "self", ".", "_callbacks", ".", "append", "(", "original_module", ")", "self", ".", "call", "(", "STRING", ",", "fabric", "=", "self", ",", "module", "=", "module", ")", "if", "optimizers", ":", "return", "(", "module", ",", "*", "optimizers", ",", "scheduler", ")", "if", "scheduler", "is", "not", "None", "else", "(", "module", ",", "*", "optimizers", ")", "return", "module"], "docstring": "r\"\"\"Set up a model and its optimizers for accelerated training.", "docstring_tokens": ["r", "set", "up", "a", "model", "and", "its", "optimizers", "for", "accelerated", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 228, "end_line": 312, "has_examples": true, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_21", "original_string": "def setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            model = fabric.setup_module(model)\r\n\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module", "language": "python", "code": "def setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            model = fabric.setup_module(model)\r\n\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "nn", ".", "Module", ",", "move_to_device", ":", "bool", "=", "True", ",", "_reapply_compile", ":", "bool", "=", "True", ")", "-", ">", "_FabricModule", ":", "rSTRING", "self", ".", "_validate_setup_module", "(", "module", ")", "module", ",", "compile_kwargs", "=", "_unwrap_compiled", "(", "module", ")", "if", "_reapply_compile", "else", "(", "module", ",", "None", ")", "original_module", "=", "module", "module", "=", "self", ".", "_precision", ".", "convert_module", "(", "module", ")", "if", "move_to_device", ":", "module", "=", "self", ".", "_move_model_to_device", "(", "model", "=", "module", ",", "optimizers", "=", "[", "]", ")", "module", "=", "self", ".", "_strategy", ".", "setup_module", "(", "module", ")", "if", "compile_kwargs", "is", "not", "None", ":", "module", "=", "_to_compiled", "(", "module", ",", "compile_kwargs", ")", "module", "=", "_FabricModule", "(", "module", ",", "self", ".", "_strategy", ",", "original_module", "=", "original_module", ")", "_update_properties", "(", "module", ",", "device", "=", "self", ".", "device", "if", "move_to_device", "else", "next", "(", "module", ".", "parameters", "(", ")", ",", "torch", ".", "tensor", "(", "0", ")", ")", ".", "device", ")", "if", "hasattr", "(", "original_module", ",", "STRING", ")", ":", "#", "this", "is", "probably", "a", "LightningModule", "original_module", ".", "_fabric", "=", "self", "if", "original_module", "not", "in", "self", ".", "_callbacks", ":", "self", ".", "_callbacks", ".", "append", "(", "original_module", ")", "self", ".", "_models_setup", "+", "=", "1", "return", "module"], "docstring": "r\"\"\"Set up a model for accelerated training or inference.", "docstring_tokens": ["r", "set", "up", "a", "model", "for", "accelerated", "training", "or", "inference"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 314, "end_line": 373, "has_examples": true, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_22", "original_string": "def setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)", "language": "python", "code": "def setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "*", "optimizers", ":", "Optimizer", ")", "-", ">", "Union", "[", "_FabricOptimizer", ",", "tuple", "[", "_FabricOptimizer", ",", ".", ".", ".", "]", "]", ":", "rSTRING", "self", ".", "_validate_setup_optimizers", "(", "optimizers", ")", "optimizers", "=", "[", "self", ".", "_strategy", ".", "setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "optimizers", "=", "[", "_FabricOptimizer", "(", "optimizer", "=", "optimizer", ",", "strategy", "=", "self", ".", "_strategy", ",", "callbacks", "=", "self", ".", "_callbacks", ")", "for", "optimizer", "in", "optimizers", "]", "return", "optimizers", "[", "0", "]", "if", "len", "(", "optimizers", ")", "=", "=", "1", "else", "tuple", "(", "optimizers", ")"], "docstring": "r\"\"\"Set up one or more optimizers for accelerated training.", "docstring_tokens": ["r", "set", "up", "one", "or", "more", "optimizers", "for", "accelerated", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 375, "end_line": 409, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_23", "original_string": "def setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders", "language": "python", "code": "def setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders", "code_tokens": ["def", "setup_dataloaders", "(", "self", ",", "*", "dataloaders", ":", "DataLoader", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "move_to_device", ":", "bool", "=", "True", ")", "-", ">", "Union", "[", "DataLoader", ",", "list", "[", "DataLoader", "]", "]", ":", "rSTRING", "self", ".", "_validate_setup_dataloaders", "(", "dataloaders", ")", "dataloaders", "=", "[", "self", ".", "_setup_dataloader", "(", "dataloader", ",", "use_distributed_sampler", "=", "use_distributed_sampler", ",", "move_to_device", "=", "move_to_device", ")", "for", "dataloader", "in", "dataloaders", "]", "return", "dataloaders", "[", "0", "]", "if", "len", "(", "dataloaders", ")", "=", "=", "1", "else", "dataloaders"], "docstring": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each", "docstring_tokens": ["r", "set", "up", "one", "or", "multiple", "dataloaders", "for", "accelerated", "training", "if", "you", "need", "different", "settings", "for", "each"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 411, "end_line": 446, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_24", "original_string": "def _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader", "language": "python", "code": "def _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader", "code_tokens": ["def", "_setup_dataloader", "(", "self", ",", "dataloader", ":", "DataLoader", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "move_to_device", ":", "bool", "=", "True", ")", "-", ">", "DataLoader", ":", "rSTRING", "if", "use_distributed_sampler", "and", "self", ".", "_requires_distributed_sampler", "(", "dataloader", ")", ":", "sampler", "=", "self", ".", "_get_distributed_sampler", "(", "dataloader", ",", "*", "*", "self", ".", "_strategy", ".", "distributed_sampler_kwargs", ")", "dataloader", "=", "_update_dataloader", "(", "dataloader", ",", "sampler", ")", "_auto_add_worker_init_fn", "(", "dataloader", ",", "self", ".", "global_rank", ")", "dataloader", "=", "self", ".", "_strategy", ".", "process_dataloader", "(", "dataloader", ")", "device", "=", "self", ".", "device", "if", "move_to_device", "and", "not", "isinstance", "(", "self", ".", "_strategy", ",", "XLAStrategy", ")", "else", "None", "fabric_dataloader", "=", "_FabricDataLoader", "(", "dataloader", "=", "dataloader", ",", "device", "=", "device", ")", "fabric_dataloader", "=", "cast", "(", "DataLoader", ",", "fabric_dataloader", ")", "return", "fabric_dataloader"], "docstring": "r\"\"\"Set up a single dataloader for accelerated training.", "docstring_tokens": ["r", "set", "up", "a", "single", "dataloader", "for", "accelerated", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 448, "end_line": 479, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_25", "original_string": "def backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False", "language": "python", "code": "def backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "model", ":", "Optional", "[", "_FabricModule", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "module", "=", "model", ".", "_forward_module", "if", "model", "is", "not", "None", "else", "model", "module", ",", "_", "=", "_unwrap_compiled", "(", "module", ")", "if", "isinstance", "(", "self", ".", "_strategy", ",", "DeepSpeedStrategy", ")", ":", "if", "model", "is", "None", ":", "if", "self", ".", "_models_setup", "=", "=", "0", ":", "raise", "RuntimeError", "(", "STRING", ")", "if", "self", ".", "_models_setup", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "module", "=", "self", ".", "_strategy", ".", "model", "else", ":", "self", ".", "_strategy", ".", "_deepspeed_engine", "=", "module", "lightning", ".", "fabric", ".", "wrappers", ".", "_in_fabric_backward", "=", "True", "try", ":", "self", ".", "_strategy", ".", "backward", "(", "tensor", ",", "module", ",", "*", "args", ",", "*", "*", "kwargs", ")", "finally", ":", "lightning", ".", "fabric", ".", "wrappers", ".", "_in_fabric_backward", "=", "False"], "docstring": "r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.", "docstring_tokens": ["r", "replaces", "loss", "backward", "in", "your", "training", "loop", "handles", "precision", "automatically", "for", "you"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 481, "end_line": 524, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_26", "original_string": "def clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "module", ":", "Union", "[", "torch", ".", "nn", ".", "Module", ",", "_FabricModule", "]", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "_FabricOptimizer", "]", ",", "clip_val", ":", "Optional", "[", "Union", "[", "float", ",", "int", "]", "]", "=", "None", ",", "max_norm", ":", "Optional", "[", "Union", "[", "float", ",", "int", "]", "]", "=", "None", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Optional", "[", "torch", ".", "Tensor", "]", ":", "STRING", "if", "clip_val", "is", "not", "None", "and", "max_norm", "is", "not", "None", ":", "raise", "ValueError", "(", "STRING", ")", "if", "clip_val", "is", "not", "None", ":", "self", ".", "strategy", ".", "clip_gradients_value", "(", "_unwrap_objects", "(", "module", ")", ",", "_unwrap_objects", "(", "optimizer", ")", ",", "clip_val", "=", "clip_val", ")", "return", "None", "if", "max_norm", "is", "not", "None", ":", "return", "self", ".", "strategy", ".", "clip_gradients_norm", "(", "_unwrap_objects", "(", "module", ")", ",", "_unwrap_objects", "(", "optimizer", ")", ",", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ",", "error_if_nonfinite", "=", "error_if_nonfinite", ",", ")", "raise", "ValueError", "(", "STRING", ")"], "docstring": "Clip the gradients of the model to a given max value or max norm.", "docstring_tokens": ["clip", "the", "gradients", "of", "the", "model", "to", "a", "given", "max", "value", "or", "max", "norm"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 526, "end_line": 580, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_27", "original_string": "def autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()", "language": "python", "code": "def autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()", "code_tokens": ["def", "autocast", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "self", ".", "_precision", ".", "forward_context", "(", ")"], "docstring": "A context manager to automatically convert operations for the chosen precision.", "docstring_tokens": ["a", "context", "manager", "to", "automatically", "convert", "operations", "for", "the", "chosen", "precision"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 582, "end_line": 589, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_28", "original_string": "def to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)", "language": "python", "code": "def to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)", "code_tokens": ["def", "to_device", "(", "self", ",", "obj", ":", "Union", "[", "nn", ".", "Module", ",", "Tensor", ",", "Any", "]", ")", "-", ">", "Union", "[", "nn", ".", "Module", ",", "Tensor", ",", "Any", "]", ":", "rSTRING", "if", "isinstance", "(", "obj", ",", "nn", ".", "Module", ")", ":", "self", ".", "_accelerator", ".", "setup_device", "(", "self", ".", "device", ")", "self", ".", "_strategy", ".", "module_to_device", "(", "obj", ")", "return", "obj", "return", "move_data_to_device", "(", "obj", ",", "device", "=", "self", ".", "device", ")"], "docstring": "r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on", "docstring_tokens": ["r", "move", "a", "class", "torch", "nn", "module", "or", "a", "collection", "of", "tensors", "to", "the", "current", "device", "if", "it", "is", "not", "already", "on"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 600, "end_line": 616, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_29", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "if", "self", ".", "local_rank", "=", "=", "0", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first", "docstring_tokens": ["r", "print", "something", "only", "on", "the", "first", "process", "if", "running", "on", "multiple", "machines", "it", "will", "print", "from", "the", "first"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 618, "end_line": 626, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_30", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "_validate_launched", "(", ")", "self", ".", "_strategy", ".", "barrier", "(", "name", "=", "name", ")"], "docstring": "Wait for all processes to enter this call.", "docstring_tokens": ["wait", "for", "all", "processes", "to", "enter", "this", "call"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 628, "end_line": 637, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_31", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "rSTRING", "self", ".", "_validate_launched", "(", ")", "return", "self", ".", "_strategy", ".", "broadcast", "(", "obj", ",", "src", "=", "src", ")"], "docstring": "r\"\"\"Send a tensor from one process to all others.", "docstring_tokens": ["r", "send", "a", "tensor", "from", "one", "process", "to", "all", "others"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 639, "end_line": 654, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_32", "original_string": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "STRING", "self", ".", "_validate_launched", "(", ")", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "self", ".", "_strategy", ".", "all_gather", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "Gather tensors or collections of tensors from multiple processes.", "docstring_tokens": ["gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 656, "end_line": 678, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_33", "original_string": "def all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)", "language": "python", "code": "def all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)", "code_tokens": ["def", "all_reduce", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "STRING", "self", ".", "_validate_launched", "(", ")", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "self", ".", "_strategy", ".", "all_reduce", ",", "group", "=", "group", ",", "reduce_op", "=", "reduce_op", ")"], "docstring": "Reduce tensors or collections of tensors from multiple processes.", "docstring_tokens": ["reduce", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 680, "end_line": 707, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_34", "original_string": "def rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()", "language": "python", "code": "def rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()", "code_tokens": ["def", "rank_zero_first", "(", "self", ",", "local", ":", "bool", "=", "False", ")", "-", ">", "Generator", ":", "rSTRING", "rank", "=", "self", ".", "local_rank", "if", "local", "else", "self", ".", "global_rank", "with", "_InfiniteBarrier", "(", ")", "as", "barrier", ":", "if", "rank", ">", "0", ":", "barrier", "(", ")", "yield", "if", "rank", "=", "=", "0", ":", "barrier", "(", ")"], "docstring": "r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when", "docstring_tokens": ["r", "the", "code", "block", "under", "this", "context", "manager", "gets", "executed", "first", "on", "the", "main", "process", "rank", "0", "and", "only", "when"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 710, "end_line": 730, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_35", "original_string": "def no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)", "language": "python", "code": "def no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "_FabricModule", ",", "enabled", ":", "bool", "=", "True", ")", "-", ">", "AbstractContextManager", ":", "rSTRING", "module", ",", "_", "=", "_unwrap_compiled", "(", "module", ")", "if", "not", "isinstance", "(", "module", ",", "_FabricModule", ")", ":", "raise", "TypeError", "(", "STRING", "STRING", ")", "if", "isinstance", "(", "self", ".", "_strategy", ",", "(", "SingleDeviceStrategy", ",", "XLAStrategy", ")", ")", ":", "return", "nullcontext", "(", ")", "if", "self", ".", "_strategy", ".", "_backward_sync_control", "is", "None", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "nullcontext", "(", ")", "forward_module", ",", "_", "=", "_unwrap_compiled", "(", "module", ".", "_forward_module", ")", "return", "self", ".", "_strategy", ".", "_backward_sync_control", ".", "no_backward_sync", "(", "forward_module", ",", "enabled", ")"], "docstring": "r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.", "docstring_tokens": ["r", "skip", "gradient", "synchronization", "during", "backward", "to", "avoid", "redundant", "communication", "overhead"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 732, "end_line": 785, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_36", "original_string": "def sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()", "language": "python", "code": "def sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()", "code_tokens": ["def", "sharded_model", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "rSTRING", "rank_zero_deprecation", "(", "STRING", ")", "self", ".", "_validate_launched", "(", ")", "if", "isinstance", "(", "self", ".", "strategy", ",", "_Sharded", ")", ":", "return", "self", ".", "strategy", ".", "module_sharded_context", "(", ")", "return", "nullcontext", "(", ")"], "docstring": "r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.", "docstring_tokens": ["r", "instantiate", "a", "model", "under", "this", "context", "manager", "to", "prepare", "it", "for", "model", "parallel", "sharding"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 787, "end_line": 797, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_37", "original_string": "def init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()", "language": "python", "code": "def init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()", "code_tokens": ["def", "init_tensor", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "self", ".", "_strategy", ".", "tensor_init_context", "(", ")"], "docstring": "Tensors that you instantiate under this context manager will be created on the device right away and have", "docstring_tokens": ["tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 799, "end_line": 802, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_38", "original_string": "def init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)", "language": "python", "code": "def init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)", "code_tokens": ["def", "init_module", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "AbstractContextManager", ":", "STRING", "self", ".", "_validate_launched", "(", ")", "return", "self", ".", "_strategy", ".", "module_init_context", "(", "empty_init", "=", "empty_init", ")"], "docstring": "Instantiate the model and its parameters under this context manager to reduce peak memory usage.", "docstring_tokens": ["instantiate", "the", "model", "and", "its", "parameters", "under", "this", "context", "manager", "to", "reduce", "peak", "memory", "usage"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 804, "end_line": 817, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_39", "original_string": "def save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()", "language": "python", "code": "def save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()", "code_tokens": ["def", "save", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "nn", ".", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "rSTRING", "if", "filter", "is", "not", "None", ":", "if", "not", "isinstance", "(", "filter", ",", "dict", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "if", "not", "set", "(", "filter", ")", ".", "issubset", "(", "state", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "for", "k", ",", "v", "in", "filter", ".", "items", "(", ")", ":", "if", "not", "callable", "(", "v", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "self", ".", "_strategy", ".", "save_checkpoint", "(", "path", "=", "path", ",", "state", "=", "_unwrap_objects", "(", "state", ")", ",", "filter", "=", "filter", ")", "self", ".", "barrier", "(", ")"], "docstring": "r\"\"\"Save checkpoint contents to a file.", "docstring_tokens": ["r", "save", "checkpoint", "contents", "to", "a", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 819, "end_line": 866, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_40", "original_string": "def load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder", "language": "python", "code": "def load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder", "code_tokens": ["def", "load", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "state", ":", "Optional", "[", "dict", "[", "str", ",", "Union", "[", "nn", ".", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "unwrapped_state", "=", "_unwrap_objects", "(", "state", ")", "remainder", "=", "self", ".", "_strategy", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "unwrapped_state", ",", "strict", "=", "strict", ")", "self", ".", "barrier", "(", ")", "if", "state", "is", "not", "None", ":", "for", "k", "in", "list", "(", "unwrapped_state", ".", "keys", "(", ")", ")", ":", "obj", ",", "_", "=", "_unwrap_compiled", "(", "state", "[", "k", "]", ")", "if", "isinstance", "(", "obj", ",", "(", "_FabricModule", ",", "_FabricOptimizer", ",", "_FabricDataLoader", ")", ")", ":", "continue", "state", "[", "k", "]", "=", "unwrapped_state", "[", "k", "]", "return", "remainder"], "docstring": "Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)", "docstring_tokens": ["load", "a", "checkpoint", "from", "a", "file", "and", "restore", "the", "state", "of", "objects", "modules", "optimizers", "etc"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 868, "end_line": 911, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_41", "original_string": "def load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)", "language": "python", "code": "def load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)", "code_tokens": ["def", "load_raw", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "obj", ":", "Union", "[", "nn", ".", "Module", ",", "Optimizer", "]", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "obj", "=", "_unwrap_objects", "(", "obj", ")", "self", ".", "_strategy", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "obj", ",", "strict", "=", "strict", ")"], "docstring": "Load the state of a module or optimizer from a single state-dict file.", "docstring_tokens": ["load", "the", "state", "of", "a", "module", "or", "optimizer", "from", "a", "single", "state", "dict", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 913, "end_line": 928, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_42", "original_string": "def launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", "[", "[", "STRING", "]", ",", "Any", "]", "=", "_do_nothing", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "if", "_is_using_cli", "(", ")", ":", "raise", "RuntimeError", "(", "STRING", "STRING", ")", "if", "function", "is", "not", "_do_nothing", ":", "if", "not", "callable", "(", "function", ")", ":", "raise", "TypeError", "(", "fSTRING", "STRING", ")", "if", "not", "inspect", ".", "signature", "(", "function", ")", ".", "parameters", ":", "raise", "TypeError", "(", "fSTRING", "STRING", ")", "elif", "isinstance", "(", "self", ".", "strategy", ".", "launcher", ",", "(", "_MultiProcessingLauncher", ",", "_XLALauncher", ")", ")", ":", "raise", "TypeError", "(", "fSTRING", "STRING", ")", "return", "self", ".", "_wrap_and_launch", "(", "function", ",", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Launch and initialize all the processes needed for distributed execution.", "docstring_tokens": ["launch", "and", "initialize", "all", "the", "processes", "needed", "for", "distributed", "execution"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 930, "end_line": 985, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_43", "original_string": "def call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r", "language": "python", "code": "def call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r", "code_tokens": ["def", "call", "(", "self", ",", "hook_name", ":", "str", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "for", "callback", "in", "self", ".", "_callbacks", ":", "method", "=", "getattr", "(", "callback", ",", "hook_name", ",", "None", ")", "if", "method", "is", "None", ":", "continue", "if", "not", "callable", "(", "method", ")", ":", "rank_zero_warn", "(", "fSTRING", ")", "continue", "method", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Trigger the callback methods with the given name and arguments.", "docstring_tokens": ["r", "trigger", "the", "callback", "methods", "with", "the", "given", "name", "and", "arguments"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 987, "end_line": 1024, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_44", "original_string": "def log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)", "language": "python", "code": "def log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)", "code_tokens": ["def", "log", "(", "self", ",", "name", ":", "str", ",", "value", ":", "Any", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "log_dict", "(", "metrics", "=", "{", "name", ":", "value", "}", ",", "step", "=", "step", ")"], "docstring": "Log a scalar to all loggers that were added to Fabric.", "docstring_tokens": ["log", "a", "scalar", "to", "all", "loggers", "that", "were", "added", "to", "fabric"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 1026, "end_line": 1037, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_45", "original_string": "def log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)", "language": "python", "code": "def log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)", "code_tokens": ["def", "log_dict", "(", "self", ",", "metrics", ":", "Mapping", "[", "str", ",", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "metrics", "=", "convert_tensors_to_scalars", "(", "metrics", ")", "for", "logger", "in", "self", ".", "_loggers", ":", "logger", ".", "log_metrics", "(", "metrics", "=", "metrics", ",", "step", "=", "step", ")"], "docstring": "Log multiple scalars at once to all loggers that were added to Fabric.", "docstring_tokens": ["log", "multiple", "scalars", "at", "once", "to", "all", "loggers", "that", "were", "added", "to", "fabric"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 1039, "end_line": 1051, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "function_46", "original_string": "def seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)", "language": "python", "code": "def seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)", "code_tokens": ["def", "seed_everything", "(", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "workers", ":", "Optional", "[", "bool", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ")", "-", ">", "int", ":", "rSTRING", "if", "workers", "is", "None", ":", "workers", "=", "True", "return", "seed_everything", "(", "seed", "=", "seed", ",", "workers", "=", "workers", ",", "verbose", "=", "verbose", ")"], "docstring": "r\"\"\"Helper function to seed everything without explicitly importing Lightning.", "docstring_tokens": ["r", "helper", "function", "to", "seed", "everything", "without", "explicitly", "importing", "lightning"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\fabric.py", "start_line": 1054, "end_line": 1064, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_47", "original_string": "def __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})", "language": "python", "code": "def __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})", "code_tokens": ["def", "__init__", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "strategy", ":", "Strategy", ",", "callbacks", ":", "Optional", "[", "list", "[", "Callable", "]", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "_optimizer", "=", "optimizer", "self", ".", "_strategy", "=", "strategy", "self", ".", "_callbacks", "=", "callbacks", "or", "[", "]", "self", ".", "__class__", "=", "type", "(", "STRING", "+", "optimizer", ".", "__class__", ".", "__name__", ",", "(", "self", ".", "__class__", ",", "optimizer", ".", "__class__", ")", ",", "{", "}", ")"], "docstring": "FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer", "docstring_tokens": ["fabricoptimizer", "is", "a", "thin", "wrapper", "around", "the", "class", "torch", "optim", "optimizer", "that", "delegates", "the", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 52, "end_line": 67, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_48", "original_string": "def __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True", "language": "python", "code": "def __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True", "code_tokens": ["def", "__init__", "(", "self", ",", "forward_module", ":", "nn", ".", "Module", ",", "strategy", ":", "Strategy", ",", "original_module", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "_forward_module", "=", "forward_module", "self", ".", "_original_module", "=", "original_module", "or", "forward_module", "self", ".", "_strategy", "=", "strategy", "self", ".", "_forward_methods", "=", "set", "(", "_LIGHTNING_MODULE_STEP_METHODS", ")", "self", ".", "_fabric_module_initialized", "=", "True"], "docstring": "The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast", "docstring_tokens": ["the", "fabricmodule", "is", "a", "thin", "wrapper", "around", "the", "class", "torch", "nn", "module", "and", "handles", "precision", "autocast"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 101, "end_line": 122, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_49", "original_string": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output", "language": "python", "code": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output", "code_tokens": ["def", "forward", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "precision", "=", "self", ".", "_strategy", ".", "precision", "args", ",", "kwargs", "=", "precision", ".", "convert_input", "(", "(", "args", ",", "kwargs", ")", ")", "with", "precision", ".", "forward_context", "(", ")", ":", "output", "=", "self", ".", "_forward_module", "(", "*", "args", ",", "*", "*", "kwargs", ")", "output", "=", "precision", ".", "convert_output", "(", "output", ")", "apply_to_collection", "(", "output", ",", "dtype", "=", "Tensor", ",", "function", "=", "self", ".", "_register_backward_hook", ")", "return", "output"], "docstring": "Casts all inputs to the right precision and handles autocast for operations in the module forward method.", "docstring_tokens": ["casts", "all", "inputs", "to", "the", "right", "precision", "and", "handles", "autocast", "for", "operations", "in", "the", "module", "forward", "method"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 129, "end_line": 140, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_50", "original_string": "def mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)", "language": "python", "code": "def mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)", "code_tokens": ["def", "mark_forward_method", "(", "self", ",", "method", ":", "Union", "[", "MethodType", ",", "str", "]", ")", "-", ">", "None", ":", "STRING", "if", "not", "isinstance", "(", "method", ",", "(", "MethodType", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "name", "=", "method", "if", "isinstance", "(", "method", ",", "str", ")", "else", "method", ".", "__name__", "if", "name", "=", "=", "STRING", ":", "raise", "ValueError", "(", "STRING", ")", "if", "not", "isinstance", "(", "getattr", "(", "self", ".", "_original_module", ",", "name", ",", "None", ")", ",", "MethodType", ")", ":", "raise", "AttributeError", "(", "fSTRING", "fSTRING", ")", "self", ".", "_forward_methods", ".", "add", "(", "name", ")"], "docstring": "Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).", "docstring_tokens": ["mark", "a", "method", "as", "a", "forward", "method", "to", "prevent", "it", "bypassing", "the", "strategy", "wrapper", "e", "g", "ddp"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 164, "end_line": 176, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_51", "original_string": "def _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method", "language": "python", "code": "def _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method", "code_tokens": ["def", "_wrap_method_with_module_call_tracker", "(", "self", ",", "method", ":", "Callable", ",", "name", ":", "str", ")", "-", ">", "Callable", ":", "STRING", "module_called", "=", "False", "def", "hook", "(", "*", "_", ":", "Any", ",", "*", "*", "__", ":", "Any", ")", "-", ">", "None", ":", "nonlocal", "module_called", "module_called", "=", "True", "@", "wraps", "(", "method", ")", "def", "_wrapped_method", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "handles", "=", "[", "]", "for", "module", "in", "self", ".", "_original_module", ".", "modules", "(", ")", ":", "handles", ".", "append", "(", "module", ".", "register_forward_hook", "(", "hook", ")", ")", "output", "=", "method", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "module_called", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", "fSTRING", ")", "for", "handle", "in", "handles", ":", "handle", ".", "remove", "(", ")", "return", "output", "return", "_wrapped_method"], "docstring": "Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by", "docstring_tokens": ["tracks", "whether", "any", "submodule", "in", "self", "_original_module", "was", "called", "during", "the", "execution", "of", "method", "by"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 200, "end_line": 227, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_52", "original_string": "def __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0", "language": "python", "code": "def __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0", "code_tokens": ["def", "__init__", "(", "self", ",", "dataloader", ":", "DataLoader", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "__dict__", ".", "update", "(", "dataloader", ".", "__dict__", ")", "self", ".", "_dataloader", "=", "dataloader", "self", ".", "_device", "=", "device", "self", ".", "_num_iter_calls", "=", "0"], "docstring": "The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the", "docstring_tokens": ["the", "fabricdataloader", "is", "a", "wrapper", "for", "the", "class", "torch", "utils", "data", "dataloader", "it", "moves", "the", "data", "to", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 293, "end_line": 306, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_53", "original_string": "def _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None", "language": "python", "code": "def _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None", "code_tokens": ["def", "_unwrap_compiled", "(", "obj", ":", "Union", "[", "Any", ",", "OptimizedModule", "]", ")", "-", ">", "tuple", "[", "Union", "[", "Any", ",", "nn", ".", "Module", "]", ",", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "]", ":", "STRING", "if", "isinstance", "(", "obj", ",", "OptimizedModule", ")", ":", "if", "(", "compile_kwargs", ":", "=", "getattr", "(", "obj", ",", "STRING", ",", "None", ")", ")", "is", "None", ":", "raise", "RuntimeError", "(", "STRING", "STRING", ")", "return", "obj", ".", "_orig_mod", ",", "compile_kwargs", "return", "obj", ",", "None"], "docstring": "Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.", "docstring_tokens": ["removes", "the", "class", "torch", "_dynamo", "optimizedmodule", "around", "the", "object", "if", "it", "is", "wrapped"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 347, "end_line": 360, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_54", "original_string": "def is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))", "language": "python", "code": "def is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))", "code_tokens": ["def", "is_wrapped", "(", "obj", ":", "object", ")", "-", ">", "bool", ":", "STRING", "obj", ",", "_", "=", "_unwrap_compiled", "(", "obj", ")", "return", "isinstance", "(", "obj", ",", "(", "_FabricModule", ",", "_FabricOptimizer", ",", "_FabricDataLoader", ")", ")"], "docstring": "Checks if an object was set up by Fabric.", "docstring_tokens": ["checks", "if", "an", "object", "was", "set", "up", "by", "fabric"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 375, "end_line": 387, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "function_55", "original_string": "def _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture", "language": "python", "code": "def _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture", "code_tokens": ["def", "_capture_compile_kwargs", "(", "compile_fn", ":", "Callable", ")", "-", ">", "Callable", ":", "STRING", "@", "wraps", "(", "compile_fn", ")", "def", "_capture", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "if", "not", "args", "or", "not", "isinstance", "(", "args", "[", "0", "]", ",", "nn", ".", "Module", ")", ":", "return", "compile_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "model", "=", "args", "[", "0", "]", "compiled_model", "=", "compile_fn", "(", "model", ",", "*", "*", "kwargs", ")", "compiled_model", ".", "_compile_kwargs", "=", "deepcopy", "(", "kwargs", ")", "return", "compiled_model", "return", "_capture"], "docstring": "Wraps the ``torch.compile`` function and captures the compile arguments.", "docstring_tokens": ["wraps", "the", "torch", "compile", "function", "and", "captures", "the", "compile", "arguments"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\wrappers.py", "start_line": 390, "end_line": 412, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_56", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"Create and prepare the device for the current process.\"\"\"", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"Create and prepare the device for the current process.\"\"\"", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING"], "docstring": "Create and prepare the device for the current process.", "docstring_tokens": ["create", "and", "prepare", "the", "device", "for", "the", "current", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 31, "end_line": 32, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_57", "original_string": "def teardown(self) -> None:\r\n        \"\"\"Clean up any state created by the accelerator.\"\"\"", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"Clean up any state created by the accelerator.\"\"\"", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Clean up any state created by the accelerator.", "docstring_tokens": ["clean", "up", "any", "state", "created", "by", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 35, "end_line": 36, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_58", "original_string": "def parse_devices(devices: Any) -> Any:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"", "language": "python", "code": "def parse_devices(devices: Any) -> Any:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Any", ")", "-", ">", "Any", ":", "STRING"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 40, "end_line": 41, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_59", "original_string": "def get_parallel_devices(devices: Any) -> Any:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"", "language": "python", "code": "def get_parallel_devices(devices: Any) -> Any:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Any", ")", "-", ">", "Any", ":", "STRING"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 45, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_60", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the device count when set to auto.\"\"\"", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the device count when set to auto.\"\"\"", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING"], "docstring": "Get the device count when set to auto.", "docstring_tokens": ["get", "the", "device", "count", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 50, "end_line": 51, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "func_name": "function_61", "original_string": "def is_available() -> bool:\r\n        \"\"\"Detect if the hardware is available.\"\"\"", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"Detect if the hardware is available.\"\"\"", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Detect if the hardware is available.", "docstring_tokens": ["detect", "if", "the", "hardware", "is", "available"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\accelerator.py", "start_line": 55, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_62", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "ValueError", "(", "fSTRING", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 26, "end_line": 33, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_63", "original_string": "def parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)", "language": "python", "code": "def parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "int", ":", "STRING", "return", "_parse_cpu_cores", "(", "devices", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 41, "end_line": 43, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_64", "original_string": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "devices", "=", "_parse_cpu_cores", "(", "devices", ")", "return", "[", "torch", ".", "device", "(", "STRING", ")", "]", "*", "devices"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 47, "end_line": 50, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_65", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "1"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 54, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_66", "original_string": "def is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "STRING", "return", "True"], "docstring": "CPU is always available for execution.", "docstring_tokens": ["cpu", "is", "always", "available", "for", "execution"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 60, "end_line": 62, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "function_67", "original_string": "def _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores", "language": "python", "code": "def _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores", "code_tokens": ["def", "_parse_cpu_cores", "(", "cpu_cores", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "int", ":", "STRING", "if", "isinstance", "(", "cpu_cores", ",", "str", ")", "and", "cpu_cores", ".", "strip", "(", ")", ".", "isdigit", "(", ")", ":", "cpu_cores", "=", "int", "(", "cpu_cores", ")", "if", "not", "isinstance", "(", "cpu_cores", ",", "int", ")", "or", "cpu_cores", "<", "=", "0", ":", "raise", "TypeError", "(", "STRING", ")", "return", "cpu_cores"], "docstring": "Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the", "docstring_tokens": ["parses", "the", "cpu_cores", "given", "in", "the", "format", "as", "accepted", "by", "the", "devices", "argument", "in", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cpu.py", "start_line": 74, "end_line": 95, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_68", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "ValueError", "(", "fSTRING", ")", "_check_cuda_matmul_precision", "(", "device", ")", "torch", ".", "cuda", ".", "set_device", "(", "device", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 28, "end_line": 37, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_69", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "STRING", "from", "lightning", ".", "fabric", ".", "utilities", ".", "device_parser", "import", "_parse_gpu_ids", "return", "_parse_gpu_ids", "(", "devices", ",", "include_cuda", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 45, "end_line": 49, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_70", "original_string": "def get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]", "language": "python", "code": "def get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "list", "[", "int", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "return", "[", "torch", ".", "device", "(", "STRING", ",", "i", ")", "for", "i", "in", "devices", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 53, "end_line": 55, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_71", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "num_cuda_devices", "(", ")"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 59, "end_line": 61, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_72", "original_string": "def find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices", "language": "python", "code": "def find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices", "code_tokens": ["def", "find_usable_cuda_devices", "(", "num_devices", ":", "int", "=", "-", "1", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "if", "num_devices", "=", "=", "0", ":", "return", "[", "]", "visible_devices", "=", "_get_all_visible_cuda_devices", "(", ")", "if", "not", "visible_devices", ":", "raise", "ValueError", "(", "fSTRING", ")", "if", "num_devices", ">", "len", "(", "visible_devices", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "available_devices", "=", "[", "]", "unavailable_devices", "=", "[", "]", "for", "gpu_idx", "in", "visible_devices", ":", "try", ":", "torch", ".", "tensor", "(", "0", ",", "device", "=", "torch", ".", "device", "(", "STRING", ",", "gpu_idx", ")", ")", "except", "RuntimeError", ":", "unavailable_devices", ".", "append", "(", "gpu_idx", ")", "continue", "available_devices", ".", "append", "(", "gpu_idx", ")", "if", "len", "(", "available_devices", ")", "=", "=", "num_devices", ":", "break", "if", "num_devices", "!", "=", "-", "1", "and", "len", "(", "available_devices", ")", "!", "=", "num_devices", ":", "raise", "RuntimeError", "(", "fSTRING", "fSTRING", ")", "return", "available_devices"], "docstring": "Returns a list of all available and usable CUDA GPU devices.", "docstring_tokens": ["returns", "a", "list", "of", "all", "available", "and", "usable", "cuda", "gpu", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 78, "end_line": 128, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_73", "original_string": "def _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))", "language": "python", "code": "def _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))", "code_tokens": ["def", "_get_all_visible_cuda_devices", "(", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "return", "list", "(", "range", "(", "num_cuda_devices", "(", ")", ")", ")"], "docstring": "Returns a list of all visible CUDA GPU devices.", "docstring_tokens": ["returns", "a", "list", "of", "all", "visible", "cuda", "gpu", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 131, "end_line": 139, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_74", "original_string": "def num_cuda_devices() -> int:\r\n    \"\"\"Returns the number of available CUDA devices.\"\"\"\r\n    return torch.cuda.device_count()", "language": "python", "code": "def num_cuda_devices() -> int:\r\n    \"\"\"Returns the number of available CUDA devices.\"\"\"\r\n    return torch.cuda.device_count()", "code_tokens": ["def", "num_cuda_devices", "(", ")", "-", ">", "int", ":", "STRING", "return", "torch", ".", "cuda", ".", "device_count", "(", ")"], "docstring": "Returns the number of available CUDA devices.", "docstring_tokens": ["returns", "the", "number", "of", "available", "cuda", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 142, "end_line": 144, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "function_75", "original_string": "def is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    return torch.cuda.is_available()", "language": "python", "code": "def is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    return torch.cuda.is_available()", "code_tokens": ["def", "is_cuda_available", "(", ")", "-", ">", "bool", ":", "STRING", "return", "torch", ".", "cuda", ".", "is_available", "(", ")"], "docstring": "Returns a bool indicating if CUDA is currently available.", "docstring_tokens": ["returns", "a", "bool", "indicating", "if", "cuda", "is", "currently", "available"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\cuda.py", "start_line": 147, "end_line": 150, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_76", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "ValueError", "(", "fSTRING", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 33, "end_line": 40, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_77", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "STRING", "from", "lightning", ".", "fabric", ".", "utilities", ".", "device_parser", "import", "_parse_gpu_ids", "return", "_parse_gpu_ids", "(", "devices", ",", "include_mps", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 48, "end_line": 52, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_78", "original_string": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "assert", "parsed_devices", "is", "not", "None", "return", "[", "torch", ".", "device", "(", "STRING", ",", "i", ")", "for", "i", "in", "range", "(", "len", "(", "parsed_devices", ")", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 56, "end_line": 60, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_79", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "1"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 64, "end_line": 66, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_80", "original_string": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "STRING", "mps_disabled", "=", "os", ".", "getenv", "(", "STRING", ",", "STRING", ")", "=", "=", "STRING", "return", "not", "mps_disabled", "and", "torch", ".", "backends", ".", "mps", ".", "is_available", "(", ")", "and", "platform", ".", "processor", "(", ")", "in", "(", "STRING", ",", "STRING", ")"], "docstring": "MPS is only available on a machine with the ARM-based Apple Silicon processors.", "docstring_tokens": ["mps", "is", "only", "available", "on", "a", "machine", "with", "the", "arm", "based", "apple", "silicon", "processors"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 71, "end_line": 74, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "function_81", "original_string": "def _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []", "language": "python", "code": "def _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []", "code_tokens": ["def", "_get_all_available_mps_gpus", "(", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "return", "[", "0", "]", "if", "MPSAccelerator", ".", "is_available", "(", ")", "else", "[", "]"], "docstring": "Returns:", "docstring_tokens": ["returns"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\mps.py", "start_line": 86, "end_line": 91, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "function_82", "original_string": "def register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register", "language": "python", "code": "def register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register", "code_tokens": ["def", "register", "(", "self", ",", "name", ":", "str", ",", "accelerator", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "description", ":", "str", "=", "STRING", ",", "override", ":", "bool", "=", "False", ",", "*", "*", "init_params", ":", "Any", ",", ")", "-", ">", "Callable", ":", "STRING", "if", "not", "(", "name", "is", "None", "or", "isinstance", "(", "name", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "if", "name", "in", "self", "and", "not", "override", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "data", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "data", "[", "STRING", "]", "=", "description", "data", "[", "STRING", "]", "=", "init_params", "def", "do_register", "(", "accelerator", ":", "Callable", ")", "-", ">", "Callable", ":", "data", "[", "STRING", "]", "=", "accelerator", "data", "[", "STRING", "]", "=", "name", "self", "[", "name", "]", "=", "data", "return", "accelerator", "if", "accelerator", "is", "not", "None", ":", "return", "do_register", "(", "accelerator", ")", "return", "do_register"], "docstring": "Registers a accelerator mapped to a name and with required metadata.", "docstring_tokens": ["registers", "a", "accelerator", "mapped", "to", "a", "name", "and", "with", "required", "metadata"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\registry.py", "start_line": 46, "end_line": 84, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "function_83", "original_string": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))", "language": "python", "code": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))", "code_tokens": ["def", "get", "(", "self", ",", "name", ":", "str", ",", "default", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "Any", ":", "STRING", "if", "name", "in", "self", ":", "data", "=", "self", "[", "name", "]", "return", "data", "[", "STRING", "]", "(", "*", "*", "data", "[", "STRING", "]", ")", "if", "default", "is", "not", "None", ":", "return", "default", "err_msg", "=", "STRING", "available_names", "=", "self", ".", "available_accelerators", "(", ")", "raise", "KeyError", "(", "err_msg", ".", "format", "(", "name", ",", "available_names", ")", ")"], "docstring": "Calls the registered accelerator with the required parameters and returns the accelerator object.", "docstring_tokens": ["calls", "the", "registered", "accelerator", "with", "the", "required", "parameters", "and", "returns", "the", "accelerator", "object"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\registry.py", "start_line": 87, "end_line": 103, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "function_84", "original_string": "def remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered accelerator by name.\"\"\"\r\n        self.pop(name)", "language": "python", "code": "def remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered accelerator by name.\"\"\"\r\n        self.pop(name)", "code_tokens": ["def", "remove", "(", "self", ",", "name", ":", "str", ")", "-", ">", "None", ":", "STRING", "self", ".", "pop", "(", "name", ")"], "docstring": "Removes the registered accelerator by name.", "docstring_tokens": ["removes", "the", "registered", "accelerator", "by", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\registry.py", "start_line": 105, "end_line": 107, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "function_85", "original_string": "def available_accelerators(self) -> set[str]:\r\n        \"\"\"Returns a set of registered accelerators.\"\"\"\r\n        return set(self.keys())", "language": "python", "code": "def available_accelerators(self) -> set[str]:\r\n        \"\"\"Returns a set of registered accelerators.\"\"\"\r\n        return set(self.keys())", "code_tokens": ["def", "available_accelerators", "(", "self", ")", "-", ">", "set", "[", "str", "]", ":", "STRING", "return", "set", "(", "self", ".", "keys", "(", ")", ")"], "docstring": "Returns a set of registered accelerators.", "docstring_tokens": ["returns", "a", "set", "of", "registered", "accelerators"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\registry.py", "start_line": 109, "end_line": 111, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "function_86", "original_string": "def call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)", "language": "python", "code": "def call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)", "code_tokens": ["def", "call_register_accelerators", "(", "registry", ":", "_AcceleratorRegistry", ",", "base_module", ":", "str", ")", "-", ">", "None", ":", "#", "pragma", ":", "no", "-", "cover", "STRING", "import", "importlib", "module", "=", "importlib", ".", "import_module", "(", "base_module", ")", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "accelerator", "import", "Accelerator", "_register_classes", "(", "registry", ",", "STRING", ",", "module", ",", "Accelerator", ")"], "docstring": "Legacy.", "docstring_tokens": ["legacy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\registry.py", "start_line": 117, "end_line": 128, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "function_87", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_tpu_devices(devices)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_tpu_devices(devices)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Union", "[", "int", ",", "list", "[", "int", "]", "]", ":", "STRING", "return", "_parse_tpu_devices", "(", "devices", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\xla.py", "start_line": 49, "end_line": 51, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "function_88", "original_string": "def get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        return [torch.device(\"xla\", devices[0])]\r", "language": "python", "code": "def get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        return [torch.device(\"xla\", devices[0])]\r", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "devices", "=", "_parse_tpu_devices", "(", "devices", ")", "if", "isinstance", "(", "devices", ",", "int", ")", ":", "return", "[", "torch", ".", "device", "(", "STRING", ",", "i", ")", "for", "i", "in", "range", "(", "devices", ")", "]", "return", "[", "torch", ".", "device", "(", "STRING", ",", "devices", "[", "0", "]", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\xla.py", "start_line": 55, "end_line": 64, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "function_89", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "if", "not", "_XLA_AVAILABLE", ":", "return", "0", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", ".", "_internal", "import", "tpu", "return", "tpu", ".", "num_available_devices", "(", ")", "from", "torch_xla", ".", "experimental", "import", "tpu", "device_count_on_version", "=", "{", "2", ":", "8", ",", "3", ":", "8", ",", "4", ":", "4", "}", "return", "device_count_on_version", ".", "get", "(", "tpu", ".", "version", "(", ")", ",", "8", ")"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\xla.py", "start_line": 71, "end_line": 82, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "function_90", "original_string": "def _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices", "language": "python", "code": "def _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices", "code_tokens": ["def", "_parse_tpu_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Union", "[", "int", ",", "list", "[", "int", "]", "]", ":", "STRING", "_check_data_type", "(", "devices", ")", "if", "isinstance", "(", "devices", ",", "str", ")", ":", "devices", "=", "_parse_tpu_devices_str", "(", "devices", ")", "_check_tpu_devices_valid", "(", "devices", ")", "return", "devices"], "docstring": "Parses the TPU devices given in the format as accepted by the", "docstring_tokens": ["parses", "the", "tpu", "devices", "given", "in", "the", "format", "as", "accepted", "by", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\accelerators\\xla.py", "start_line": 124, "end_line": 141, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_91", "original_string": "def name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "language": "python", "code": "def name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_name"], "docstring": "Gets the name of the experiment.", "docstring_tokens": ["gets", "the", "name", "of", "the", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 80, "end_line": 87, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_92", "original_string": "def version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "language": "python", "code": "def version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "str", "]", ":", "STRING", "if", "self", ".", "_version", "is", "None", ":", "self", ".", "_version", "=", "self", ".", "_get_next_version", "(", ")", "return", "self", ".", "_version"], "docstring": "Gets the version of the experiment.", "docstring_tokens": ["gets", "the", "version", "of", "the", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 91, "end_line": 100, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_93", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the versioned CSV experiments are saved.\"\"\"\r\n        return self._root_dir", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the versioned CSV experiments are saved.\"\"\"\r\n        return self._root_dir", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_root_dir"], "docstring": "Gets the save directory where the versioned CSV experiments are saved.", "docstring_tokens": ["gets", "the", "save", "directory", "where", "the", "versioned", "csv", "experiments", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 104, "end_line": 106, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_94", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "fSTRING", "return", "os", ".", "path", ".", "join", "(", "self", ".", "_root_dir", ",", "self", ".", "name", ",", "version", ")"], "docstring": "The log directory for this run.", "docstring_tokens": ["the", "log", "directory", "for", "this", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 110, "end_line": 119, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_95", "original_string": "def experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "STRING", ":", "STRING", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "os", ".", "makedirs", "(", "self", ".", "_root_dir", ",", "exist_ok", "=", "True", ")", "self", ".", "_experiment", "=", "_ExperimentWriter", "(", "log_dir", "=", "self", ".", "log_dir", ")", "return", "self", ".", "_experiment"], "docstring": "Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.", "docstring_tokens": ["actual", "experimentwriter", "object", "to", "use", "experimentwriter", "features", "anywhere", "in", "your", "code", "do", "the", "following"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 123, "end_line": 136, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_96", "original_string": "def log_metrics(self, metrics_dict: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Record metrics.\"\"\"\r\n\r\n        def _handle_value(value: Union[Tensor, Any]) -> Any:\r\n            if isinstance(value, Tensor):\r\n                return value.item()\r\n            return value\r\n\r\n        if step is None:\r\n            step = len(self.metrics)\r\n\r\n        metrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n        metrics[\"step\"] = step\r\n        self.metrics.append(metrics)", "language": "python", "code": "def log_metrics(self, metrics_dict: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Record metrics.\"\"\"\r\n\r\n        def _handle_value(value: Union[Tensor, Any]) -> Any:\r\n            if isinstance(value, Tensor):\r\n                return value.item()\r\n            return value\r\n\r\n        if step is None:\r\n            step = len(self.metrics)\r\n\r\n        metrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n        metrics[\"step\"] = step\r\n        self.metrics.append(metrics)", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics_dict", ":", "dict", "[", "str", ",", "float", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "def", "_handle_value", "(", "value", ":", "Union", "[", "Tensor", ",", "Any", "]", ")", "-", ">", "Any", ":", "if", "isinstance", "(", "value", ",", "Tensor", ")", ":", "return", "value", ".", "item", "(", ")", "return", "value", "if", "step", "is", "None", ":", "step", "=", "len", "(", "self", ".", "metrics", ")", "metrics", "=", "{", "k", ":", "_handle_value", "(", "v", ")", "for", "k", ",", "v", "in", "metrics_dict", ".", "items", "(", ")", "}", "metrics", "[", "STRING", "]", "=", "step", "self", ".", "metrics", ".", "append", "(", "metrics", ")"], "docstring": "Record metrics.", "docstring_tokens": ["record", "metrics"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 212, "end_line": 225, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_97", "original_string": "def save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset\r", "language": "python", "code": "def save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset\r", "code_tokens": ["def", "save", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "metrics", ":", "return", "new_keys", "=", "self", ".", "_record_new_keys", "(", ")", "file_exists", "=", "self", ".", "_fs", ".", "isfile", "(", "self", ".", "metrics_file_path", ")", "if", "new_keys", "and", "file_exists", ":", "self", ".", "_rewrite_with_new_header", "(", "self", ".", "metrics_keys", ")", "with", "self", ".", "_fs", ".", "open", "(", "self", ".", "metrics_file_path", ",", "mode", "=", "(", "STRING", "if", "file_exists", "else", "STRING", ")", ",", "newline", "=", "STRING", ")", "as", "file", ":", "writer", "=", "csv", ".", "DictWriter", "(", "file", ",", "fieldnames", "=", "self", ".", "metrics_keys", ")", "if", "not", "file_exists", ":", "writer", ".", "writeheader", "(", ")", "writer", ".", "writerows", "(", "self", ".", "metrics", ")", "self", ".", "metrics", "=", "[", "]", "#", "reset"], "docstring": "Save recorded metrics into files.", "docstring_tokens": ["save", "recorded", "metrics", "into", "files"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 227, "end_line": 246, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "function_98", "original_string": "def _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys", "language": "python", "code": "def _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys", "code_tokens": ["def", "_record_new_keys", "(", "self", ")", "-", ">", "set", "[", "str", "]", ":", "STRING", "current_keys", "=", "set", "(", ")", ".", "union", "(", "*", "self", ".", "metrics", ")", "new_keys", "=", "current_keys", "-", "set", "(", "self", ".", "metrics_keys", ")", "self", ".", "metrics_keys", ".", "extend", "(", "new_keys", ")", "self", ".", "metrics_keys", ".", "sort", "(", ")", "return", "new_keys"], "docstring": "Records new keys that have not been logged before.", "docstring_tokens": ["records", "new", "keys", "that", "have", "not", "been", "logged", "before"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "start_line": 248, "end_line": 254, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_99", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name.\"\"\"", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name.\"\"\"", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING"], "docstring": "Return the experiment name.", "docstring_tokens": ["return", "the", "experiment", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 31, "end_line": 32, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_100", "original_string": "def version(self) -> Optional[Union[int, str]]:\r\n        \"\"\"Return the experiment version.\"\"\"", "language": "python", "code": "def version(self) -> Optional[Union[int, str]]:\r\n        \"\"\"Return the experiment version.\"\"\"", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "Union", "[", "int", ",", "str", "]", "]", ":", "STRING"], "docstring": "Return the experiment version.", "docstring_tokens": ["return", "the", "experiment", "version"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 36, "end_line": 37, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_101", "original_string": "def root_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where all versions of an experiment get saved, or `None` if the logger does not\r\n        save data locally.\"\"\"\r\n        return None", "language": "python", "code": "def root_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where all versions of an experiment get saved, or `None` if the logger does not\r\n        save data locally.\"\"\"\r\n        return None", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "None"], "docstring": "Return the root directory where all versions of an experiment get saved, or `None` if the logger does not", "docstring_tokens": ["return", "the", "root", "directory", "where", "all", "versions", "of", "an", "experiment", "get", "saved", "or", "none", "if", "the", "logger", "does", "not"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 40, "end_line": 43, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_102", "original_string": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"Return directory the current version of the experiment gets saved, or `None` if the logger does not save\r\n        data locally.\"\"\"\r\n        return None", "language": "python", "code": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"Return directory the current version of the experiment gets saved, or `None` if the logger does not save\r\n        data locally.\"\"\"\r\n        return None", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "None"], "docstring": "Return directory the current version of the experiment gets saved, or `None` if the logger does not save", "docstring_tokens": ["return", "directory", "the", "current", "version", "of", "the", "experiment", "gets", "saved", "or", "none", "if", "the", "logger", "does", "not", "save"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 46, "end_line": 49, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_103", "original_string": "def group_separator(self) -> str:\r\n        \"\"\"Return the default separator used by the logger to group the data into subfolders.\"\"\"\r\n        return \"/\"", "language": "python", "code": "def group_separator(self) -> str:\r\n        \"\"\"Return the default separator used by the logger to group the data into subfolders.\"\"\"\r\n        return \"/\"", "code_tokens": ["def", "group_separator", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "STRING"], "docstring": "Return the default separator used by the logger to group the data into subfolders.", "docstring_tokens": ["return", "the", "default", "separator", "used", "by", "the", "logger", "to", "group", "the", "data", "into", "subfolders"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 52, "end_line": 54, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_104", "original_string": "def log_metrics(self, metrics: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Records metrics. This method logs metrics as soon as it received them.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def log_metrics(self, metrics: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Records metrics. This method logs metrics as soon as it received them.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "float", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Records metrics. This method logs metrics as soon as it received them.", "docstring_tokens": ["records", "metrics", "this", "method", "logs", "metrics", "as", "soon", "as", "it", "received", "them"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 57, "end_line": 65, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_105", "original_string": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Record hyperparameters.\r\n\r\n        Args:\r\n            params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\r\n            args: Optional positional arguments, depends on the specific logger being used\r\n            kwargs: Optional keyword arguments, depends on the specific logger being used\r\n\r\n        \"\"\"", "language": "python", "code": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Record hyperparameters.\r\n\r\n        Args:\r\n            params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\r\n            args: Optional positional arguments, depends on the specific logger being used\r\n            kwargs: Optional keyword arguments, depends on the specific logger being used\r\n\r\n        \"\"\"", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING"], "docstring": "Record hyperparameters.", "docstring_tokens": ["record", "hyperparameters"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 68, "end_line": 76, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_106", "original_string": "def log_graph(self, model: Module, input_array: Optional[Tensor] = None) -> None:\r\n        \"\"\"Record model graph.\r\n\r\n        Args:\r\n            model: the model with an implementation of ``forward``.\r\n            input_array: input passes to `model.forward`\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def log_graph(self, model: Module, input_array: Optional[Tensor] = None) -> None:\r\n        \"\"\"Record model graph.\r\n\r\n        Args:\r\n            model: the model with an implementation of ``forward``.\r\n            input_array: input passes to `model.forward`\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "log_graph", "(", "self", ",", "model", ":", "Module", ",", "input_array", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Record model graph.", "docstring_tokens": ["record", "model", "graph"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 78, "end_line": 86, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_107", "original_string": "def save(self) -> None:\r\n        \"\"\"Save log data.\"\"\"", "language": "python", "code": "def save(self) -> None:\r\n        \"\"\"Save log data.\"\"\"", "code_tokens": ["def", "save", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Save log data.", "docstring_tokens": ["save", "log", "data"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 88, "end_line": 89, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_108", "original_string": "def finalize(self, status: str) -> None:\r\n        \"\"\"Do any processing that is necessary to finalize an experiment.\r\n\r\n        Args:\r\n            status: Status that the experiment finished with (e.g. success, failed, aborted)\r\n\r\n        \"\"\"\r\n        self.save()", "language": "python", "code": "def finalize(self, status: str) -> None:\r\n        \"\"\"Do any processing that is necessary to finalize an experiment.\r\n\r\n        Args:\r\n            status: Status that the experiment finished with (e.g. success, failed, aborted)\r\n\r\n        \"\"\"\r\n        self.save()", "code_tokens": ["def", "finalize", "(", "self", ",", "status", ":", "str", ")", "-", ">", "None", ":", "STRING", "self", ".", "save", "(", ")"], "docstring": "Do any processing that is necessary to finalize an experiment.", "docstring_tokens": ["do", "any", "processing", "that", "is", "necessary", "to", "finalize", "an", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 91, "end_line": 98, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_109", "original_string": "def rank_zero_experiment(fn: Callable) -> Callable:\r\n    \"\"\"Returns the real experiment on rank 0 and otherwise the _DummyExperiment.\"\"\"\r\n\r\n    @wraps(fn)\r\n    def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)\r\n\r\n    return experiment", "language": "python", "code": "def rank_zero_experiment(fn: Callable) -> Callable:\r\n    \"\"\"Returns the real experiment on rank 0 and otherwise the _DummyExperiment.\"\"\"\r\n\r\n    @wraps(fn)\r\n    def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)\r\n\r\n    return experiment", "code_tokens": ["def", "rank_zero_experiment", "(", "fn", ":", "Callable", ")", "-", ">", "Callable", ":", "STRING", "@", "wraps", "(", "fn", ")", "def", "experiment", "(", "self", ":", "Logger", ")", "-", ">", "Union", "[", "Any", ",", "_DummyExperiment", "]", ":", "STRING", "if", "rank_zero_only", ".", "rank", ">", "0", ":", "return", "_DummyExperiment", "(", ")", "return", "fn", "(", "self", ")", "return", "experiment"], "docstring": "Returns the real experiment on rank 0 and otherwise the _DummyExperiment.", "docstring_tokens": ["returns", "the", "real", "experiment", "on", "rank", "0", "and", "otherwise", "the", "_dummyexperiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 101, "end_line": 119, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "function_110", "original_string": "def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)", "language": "python", "code": "def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)", "code_tokens": ["def", "experiment", "(", "self", ":", "Logger", ")", "-", ">", "Union", "[", "Any", ",", "_DummyExperiment", "]", ":", "STRING", "if", "rank_zero_only", ".", "rank", ">", "0", ":", "return", "_DummyExperiment", "(", ")", "return", "fn", "(", "self", ")"], "docstring": "Note:", "docstring_tokens": ["note"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\logger.py", "start_line": 105, "end_line": 117, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_111", "original_string": "def name(self) -> str:\r\n        \"\"\"Get the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "language": "python", "code": "def name(self) -> str:\r\n        \"\"\"Get the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_name"], "docstring": "Get the name of the experiment.", "docstring_tokens": ["get", "the", "name", "of", "the", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 112, "end_line": 119, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_112", "original_string": "def version(self) -> Union[int, str]:\r\n        \"\"\"Get the experiment version.\r\n\r\n        Returns:\r\n            The experiment version if specified else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "language": "python", "code": "def version(self) -> Union[int, str]:\r\n        \"\"\"Get the experiment version.\r\n\r\n        Returns:\r\n            The experiment version if specified else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "str", "]", ":", "STRING", "if", "self", ".", "_version", "is", "None", ":", "self", ".", "_version", "=", "self", ".", "_get_next_version", "(", ")", "return", "self", ".", "_version"], "docstring": "Get the experiment version.", "docstring_tokens": ["get", "the", "experiment", "version"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 123, "end_line": 132, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_113", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_root_dir"], "docstring": "Gets the save directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 136, "end_line": 143, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_114", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, self.name, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, self.name, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "fSTRING", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "self", ".", "name", ",", "version", ")", "if", "isinstance", "(", "self", ".", "sub_dir", ",", "str", ")", ":", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "self", ".", "sub_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expandvars", "(", "log_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "log_dir", ")", "return", "log_dir"], "docstring": "The directory for this run's tensorboard checkpoint.", "docstring_tokens": ["the", "directory", "for", "this", "run", "s", "tensorboard", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 147, "end_line": 160, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_115", "original_string": "def sub_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the sub directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the sub directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._sub_dir", "language": "python", "code": "def sub_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the sub directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the sub directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._sub_dir", "code_tokens": ["def", "sub_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_sub_dir"], "docstring": "Gets the sub directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "sub", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 163, "end_line": 170, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_116", "original_string": "def experiment(self) -> \"SummaryWriter\":\r\n        \"\"\"Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            logger.experiment.some_tensorboard_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        assert rank_zero_only.rank == 0, \"tried to init log dirs in non global_rank=0\"\r\n        if self.root_dir:\r\n            self._fs.makedirs(self.root_dir, exist_ok=True)\r\n\r\n        if _TENSORBOARD_AVAILABLE:\r\n            from torch.utils.tensorboard import SummaryWriter\r\n        else:\r\n            from tensorboardX import SummaryWriter  # type: ignore[no-redef]\r\n\r\n        self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> \"SummaryWriter\":\r\n        \"\"\"Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            logger.experiment.some_tensorboard_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        assert rank_zero_only.rank == 0, \"tried to init log dirs in non global_rank=0\"\r\n        if self.root_dir:\r\n            self._fs.makedirs(self.root_dir, exist_ok=True)\r\n\r\n        if _TENSORBOARD_AVAILABLE:\r\n            from torch.utils.tensorboard import SummaryWriter\r\n        else:\r\n            from tensorboardX import SummaryWriter  # type: ignore[no-redef]\r\n\r\n        self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "STRING", ":", "STRING", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "assert", "rank_zero_only", ".", "rank", "=", "=", "0", ",", "STRING", "if", "self", ".", "root_dir", ":", "self", ".", "_fs", ".", "makedirs", "(", "self", ".", "root_dir", ",", "exist_ok", "=", "True", ")", "if", "_TENSORBOARD_AVAILABLE", ":", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "else", ":", "from", "tensorboardX", "import", "SummaryWriter", "#", "type", ":", "ignore", "[", "no", "-", "redef", "]", "self", ".", "_experiment", "=", "SummaryWriter", "(", "log_dir", "=", "self", ".", "log_dir", ",", "*", "*", "self", ".", "_kwargs", ")", "return", "self", ".", "_experiment"], "docstring": "Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.", "docstring_tokens": ["actual", "tensorboard", "object", "to", "use", "tensorboard", "features", "anywhere", "in", "your", "code", "do", "the", "following"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 174, "end_line": 195, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "function_117", "original_string": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        params = _convert_params(params)\r\n\r\n        params = _flatten_dict(params)\r\n        params = self._sanitize_params(params)\r\n\r\n        if metrics is None:\r\n            if self._default_hp_metric:\r\n                metrics = {\"hp_metric\": -1}\r\n        elif not isinstance(metrics, dict):\r\n            metrics = {\"hp_metric\": metrics}\r\n\r\n        if metrics:\r\n            self.log_metrics(metrics, step)\r\n\r\n            if _TENSORBOARD_AVAILABLE:\r\n                from torch.utils.tensorboard.summary import hparams\r\n            else:\r\n                from tensorboardX.summary import hparams  # type: ignore[no-redef]\r\n\r\n            exp, ssi, sei = hparams(params, metrics)\r\n            writer = self.experiment._get_file_writer()\r\n            writer.add_summary(exp, step)\r\n            writer.add_summary(ssi, step)\r\n            writer.add_summary(sei, step)", "language": "python", "code": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        params = _convert_params(params)\r\n\r\n        params = _flatten_dict(params)\r\n        params = self._sanitize_params(params)\r\n\r\n        if metrics is None:\r\n            if self._default_hp_metric:\r\n                metrics = {\"hp_metric\": -1}\r\n        elif not isinstance(metrics, dict):\r\n            metrics = {\"hp_metric\": metrics}\r\n\r\n        if metrics:\r\n            self.log_metrics(metrics, step)\r\n\r\n            if _TENSORBOARD_AVAILABLE:\r\n                from torch.utils.tensorboard.summary import hparams\r\n            else:\r\n                from tensorboardX.summary import hparams  # type: ignore[no-redef]\r\n\r\n            exp, ssi, sei = hparams(params, metrics)\r\n            writer = self.experiment._get_file_writer()\r\n            writer.add_summary(exp, step)\r\n            writer.add_summary(ssi, step)\r\n            writer.add_summary(sei, step)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "metrics", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "params", "=", "_convert_params", "(", "params", ")", "params", "=", "_flatten_dict", "(", "params", ")", "params", "=", "self", ".", "_sanitize_params", "(", "params", ")", "if", "metrics", "is", "None", ":", "if", "self", ".", "_default_hp_metric", ":", "metrics", "=", "{", "STRING", ":", "-", "1", "}", "elif", "not", "isinstance", "(", "metrics", ",", "dict", ")", ":", "metrics", "=", "{", "STRING", ":", "metrics", "}", "if", "metrics", ":", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")", "if", "_TENSORBOARD_AVAILABLE", ":", "from", "torch", ".", "utils", ".", "tensorboard", ".", "summary", "import", "hparams", "else", ":", "from", "tensorboardX", ".", "summary", "import", "hparams", "#", "type", ":", "ignore", "[", "no", "-", "redef", "]", "exp", ",", "ssi", ",", "sei", "=", "hparams", "(", "params", ",", "metrics", ")", "writer", "=", "self", ".", "experiment", ".", "_get_file_writer", "(", ")", "writer", ".", "add_summary", "(", "exp", ",", "step", ")", "writer", ".", "add_summary", "(", "ssi", ",", "step", ")", "writer", ".", "add_summary", "(", "sei", ",", "step", ")"], "docstring": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the", "docstring_tokens": ["record", "hyperparameters", "tensorboard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "start_line": 221, "end_line": 261, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\collectives\\collective.py", "func_name": "function_118", "original_string": "def world_size(self) -> int:\r\n        \"\"\"World size.\"\"\"", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"World size.\"\"\"", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "World size.", "docstring_tokens": ["world", "size"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\collectives\\collective.py", "start_line": 28, "end_line": 29, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\collectives\\collective.py", "func_name": "function_119", "original_string": "def create_group(self, **kwargs: Any) -> Self:\r\n        \"\"\"Create a group.\r\n\r\n        This assumes that :meth:`~lightning.fabric.plugins.collectives.Collective.init_group` has been\r\n        called already by the user.\r\n\r\n        \"\"\"\r\n        if self._group is not None:\r\n            raise RuntimeError(f\"`{type(self).__name__}` already owns a group.\")\r\n        self._group = self.new_group(**kwargs)\r\n        return self", "language": "python", "code": "def create_group(self, **kwargs: Any) -> Self:\r\n        \"\"\"Create a group.\r\n\r\n        This assumes that :meth:`~lightning.fabric.plugins.collectives.Collective.init_group` has been\r\n        called already by the user.\r\n\r\n        \"\"\"\r\n        if self._group is not None:\r\n            raise RuntimeError(f\"`{type(self).__name__}` already owns a group.\")\r\n        self._group = self.new_group(**kwargs)\r\n        return self", "code_tokens": ["def", "create_group", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Self", ":", "STRING", "if", "self", ".", "_group", "is", "not", "None", ":", "raise", "RuntimeError", "(", "fSTRING", ")", "self", ".", "_group", "=", "self", ".", "new_group", "(", "*", "*", "kwargs", ")", "return", "self"], "docstring": "Create a group.", "docstring_tokens": ["create", "a", "group"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\collectives\\collective.py", "start_line": 101, "end_line": 111, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_120", "original_string": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Whether the environment creates the subprocesses or not.\"\"\"", "language": "python", "code": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Whether the environment creates the subprocesses or not.\"\"\"", "code_tokens": ["def", "creates_processes_externally", "(", "self", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Whether the environment creates the subprocesses or not.", "docstring_tokens": ["whether", "the", "environment", "creates", "the", "subprocesses", "or", "not"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 21, "end_line": 22, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_121", "original_string": "def main_address(self) -> str:\r\n        \"\"\"The main address through which all processes connect and communicate.\"\"\"", "language": "python", "code": "def main_address(self) -> str:\r\n        \"\"\"The main address through which all processes connect and communicate.\"\"\"", "code_tokens": ["def", "main_address", "(", "self", ")", "-", ">", "str", ":", "STRING"], "docstring": "The main address through which all processes connect and communicate.", "docstring_tokens": ["the", "main", "address", "through", "which", "all", "processes", "connect", "and", "communicate"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 26, "end_line": 27, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_122", "original_string": "def main_port(self) -> int:\r\n        \"\"\"An open and configured port in the main node through which all processes communicate.\"\"\"", "language": "python", "code": "def main_port(self) -> int:\r\n        \"\"\"An open and configured port in the main node through which all processes communicate.\"\"\"", "code_tokens": ["def", "main_port", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "An open and configured port in the main node through which all processes communicate.", "docstring_tokens": ["an", "open", "and", "configured", "port", "in", "the", "main", "node", "through", "which", "all", "processes", "communicate"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 31, "end_line": 32, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_123", "original_string": "def detect() -> bool:\r\n        \"\"\"Detects the environment settings corresponding to this cluster and returns ``True`` if they match.\"\"\"", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Detects the environment settings corresponding to this cluster and returns ``True`` if they match.\"\"\"", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Detects the environment settings corresponding to this cluster and returns ``True`` if they match.", "docstring_tokens": ["detects", "the", "environment", "settings", "corresponding", "to", "this", "cluster", "and", "returns", "true", "if", "they", "match"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 36, "end_line": 37, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_124", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and nodes.\"\"\"", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and nodes.\"\"\"", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "The number of processes across all devices and nodes.", "docstring_tokens": ["the", "number", "of", "processes", "across", "all", "devices", "and", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 40, "end_line": 41, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_125", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all nodes and devices.\"\"\"", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all nodes and devices.\"\"\"", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "The rank (index) of the currently running process across all nodes and devices.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "across", "all", "nodes", "and", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 48, "end_line": 49, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_126", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current node.\"\"\"", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current node.\"\"\"", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "The rank (index) of the currently running process inside of the current node.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "inside", "of", "the", "current", "node"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 56, "end_line": 57, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_127", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the node on which the current process runs.\"\"\"", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the node on which the current process runs.\"\"\"", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "STRING"], "docstring": "The rank (index) of the node on which the current process runs.", "docstring_tokens": ["the", "rank", "index", "of", "the", "node", "on", "which", "the", "current", "process", "runs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 60, "end_line": 61, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_128", "original_string": "def validate_settings(self, num_devices: int, num_nodes: int) -> None:\r\n        \"\"\"Validates settings configured in the script against the environment, and raises an exception if there is an\r\n        inconsistency.\"\"\"\r\n        pass", "language": "python", "code": "def validate_settings(self, num_devices: int, num_nodes: int) -> None:\r\n        \"\"\"Validates settings configured in the script against the environment, and raises an exception if there is an\r\n        inconsistency.\"\"\"\r\n        pass", "code_tokens": ["def", "validate_settings", "(", "self", ",", "num_devices", ":", "int", ",", "num_nodes", ":", "int", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Validates settings configured in the script against the environment, and raises an exception if there is an", "docstring_tokens": ["validates", "settings", "configured", "in", "the", "script", "against", "the", "environment", "and", "raises", "an", "exception", "if", "there", "is", "an"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 63, "end_line": 66, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "function_129", "original_string": "def teardown(self) -> None:\r\n        \"\"\"Clean up any state set after execution finishes.\"\"\"\r\n        pass", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"Clean up any state set after execution finishes.\"\"\"\r\n        pass", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Clean up any state set after execution finishes.", "docstring_tokens": ["clean", "up", "any", "state", "set", "after", "execution", "finishes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "start_line": 68, "end_line": 70, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "func_name": "function_130", "original_string": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Returns whether the cluster creates the processes or not.\r\n\r\n        If at least :code:`LOCAL_RANK` is available as environment variable, Lightning assumes the user acts as the\r\n        process launcher/job scheduler and Lightning will not launch new processes.\r\n\r\n        \"\"\"\r\n        return \"LOCAL_RANK\" in os.environ", "language": "python", "code": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Returns whether the cluster creates the processes or not.\r\n\r\n        If at least :code:`LOCAL_RANK` is available as environment variable, Lightning assumes the user acts as the\r\n        process launcher/job scheduler and Lightning will not launch new processes.\r\n\r\n        \"\"\"\r\n        return \"LOCAL_RANK\" in os.environ", "code_tokens": ["def", "creates_processes_externally", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "STRING", "in", "os", ".", "environ"], "docstring": "Returns whether the cluster creates the processes or not.", "docstring_tokens": ["returns", "whether", "the", "cluster", "creates", "the", "processes", "or", "not"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "start_line": 47, "end_line": 54, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "func_name": "function_131", "original_string": "def find_free_network_port() -> int:\r\n    \"\"\"Finds a free port on localhost.\r\n\r\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\r\n    `MASTER_PORT` environment variable.\r\n\r\n    \"\"\"\r\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    s.bind((\"\", 0))\r\n    port = s.getsockname()[1]\r\n    s.close()\r\n    return port", "language": "python", "code": "def find_free_network_port() -> int:\r\n    \"\"\"Finds a free port on localhost.\r\n\r\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\r\n    `MASTER_PORT` environment variable.\r\n\r\n    \"\"\"\r\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    s.bind((\"\", 0))\r\n    port = s.getsockname()[1]\r\n    s.close()\r\n    return port", "code_tokens": ["def", "find_free_network_port", "(", ")", "-", ">", "int", ":", "STRING", "s", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "s", ".", "bind", "(", "(", "STRING", ",", "0", ")", ")", "port", "=", "s", ".", "getsockname", "(", ")", "[", "1", "]", "s", ".", "close", "(", ")", "return", "port"], "docstring": "Finds a free port on localhost.", "docstring_tokens": ["finds", "a", "free", "port", "on", "localhost"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "start_line": 107, "end_line": 118, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_132", "original_string": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.\"\"\"\r\n        return True", "language": "python", "code": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.\"\"\"\r\n        return True", "code_tokens": ["def", "creates_processes_externally", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "True"], "docstring": "LSF creates subprocesses, i.e., PyTorch Lightning does not need to spawn them.", "docstring_tokens": ["lsf", "creates", "subprocesses", "i", "e", "pytorch", "lightning", "does", "not", "need", "to", "spawn", "them"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 66, "end_line": 68, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_133", "original_string": "def main_address(self) -> str:\r\n        \"\"\"The main address is read from an OpenMPI host rank file in the environment variable\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._main_address", "language": "python", "code": "def main_address(self) -> str:\r\n        \"\"\"The main address is read from an OpenMPI host rank file in the environment variable\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._main_address", "code_tokens": ["def", "main_address", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_main_address"], "docstring": "The main address is read from an OpenMPI host rank file in the environment variable", "docstring_tokens": ["the", "main", "address", "is", "read", "from", "an", "openmpi", "host", "rank", "file", "in", "the", "environment", "variable"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 72, "end_line": 75, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_134", "original_string": "def main_port(self) -> int:\r\n        \"\"\"The main port is calculated from the LSF job ID.\"\"\"\r\n        return self._main_port", "language": "python", "code": "def main_port(self) -> int:\r\n        \"\"\"The main port is calculated from the LSF job ID.\"\"\"\r\n        return self._main_port", "code_tokens": ["def", "main_port", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "_main_port"], "docstring": "The main port is calculated from the LSF job ID.", "docstring_tokens": ["the", "main", "port", "is", "calculated", "from", "the", "lsf", "job", "id"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 79, "end_line": 81, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_135", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the ``jsrun`` command.\"\"\"\r\n        required_env_vars = {\"LSB_JOBID\", \"LSB_DJOB_RANKFILE\", \"JSM_NAMESPACE_LOCAL_RANK\", \"JSM_NAMESPACE_SIZE\"}\r\n        return required_env_vars.issubset(os.environ.keys())", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the ``jsrun`` command.\"\"\"\r\n        required_env_vars = {\"LSB_JOBID\", \"LSB_DJOB_RANKFILE\", \"JSM_NAMESPACE_LOCAL_RANK\", \"JSM_NAMESPACE_SIZE\"}\r\n        return required_env_vars.issubset(os.environ.keys())", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "STRING", "required_env_vars", "=", "{", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", "}", "return", "required_env_vars", ".", "issubset", "(", "os", ".", "environ", ".", "keys", "(", ")", ")"], "docstring": "Returns ``True`` if the current process was launched using the ``jsrun`` command.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "using", "the", "jsrun", "command"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 85, "end_line": 88, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_136", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.\"\"\"\r\n        world_size = os.environ.get(\"JSM_NAMESPACE_SIZE\")\r\n        if world_size is None:\r\n            raise ValueError(\r\n                \"Cannot determine world size. Environment variable `JSM_NAMESPACE_SIZE` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(world_size)", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.\"\"\"\r\n        world_size = os.environ.get(\"JSM_NAMESPACE_SIZE\")\r\n        if world_size is None:\r\n            raise ValueError(\r\n                \"Cannot determine world size. Environment variable `JSM_NAMESPACE_SIZE` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(world_size)", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "STRING", "world_size", "=", "os", ".", "environ", ".", "get", "(", "STRING", ")", "if", "world_size", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "return", "int", "(", "world_size", ")"], "docstring": "The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.", "docstring_tokens": ["the", "world", "size", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_size"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 91, "end_line": 99, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_137", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.\"\"\"\r\n        global_rank = os.environ.get(\"JSM_NAMESPACE_RANK\")\r\n        if global_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine global rank. Environment variable `JSM_NAMESPACE_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(global_rank)", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.\"\"\"\r\n        global_rank = os.environ.get(\"JSM_NAMESPACE_RANK\")\r\n        if global_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine global rank. Environment variable `JSM_NAMESPACE_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(global_rank)", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "global_rank", "=", "os", ".", "environ", ".", "get", "(", "STRING", ")", "if", "global_rank", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "return", "int", "(", "global_rank", ")"], "docstring": "The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.", "docstring_tokens": ["the", "world", "size", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_rank"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 106, "end_line": 114, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_138", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.\"\"\"\r\n        local_rank = os.environ.get(\"JSM_NAMESPACE_LOCAL_RANK\")\r\n        if local_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine local rank. Environment variable `JSM_NAMESPACE_LOCAL_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(local_rank)", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.\"\"\"\r\n        local_rank = os.environ.get(\"JSM_NAMESPACE_LOCAL_RANK\")\r\n        if local_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine local rank. Environment variable `JSM_NAMESPACE_LOCAL_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(local_rank)", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "local_rank", "=", "os", ".", "environ", ".", "get", "(", "STRING", ")", "if", "local_rank", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "return", "int", "(", "local_rank", ")"], "docstring": "The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.", "docstring_tokens": ["the", "local", "rank", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_local_rank"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 121, "end_line": 129, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_139", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._node_rank", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._node_rank", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "_node_rank"], "docstring": "The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in", "docstring_tokens": ["the", "node", "rank", "is", "determined", "by", "the", "position", "of", "the", "current", "hostname", "in", "the", "openmpi", "host", "rank", "file", "stored", "in"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 132, "end_line": 135, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_140", "original_string": "def _get_node_rank(self) -> int:\r\n        \"\"\"A helper method for getting the node rank.\r\n\r\n        The node rank is determined by the position of the current node in the list of hosts used in the job. This is\r\n        calculated by reading all hosts from ``LSB_DJOB_RANKFILE`` and finding this node's hostname in the list.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        count: dict[str, int] = {}\r\n        for host in hosts:\r\n            if host not in count:\r\n                count[host] = len(count)\r\n        return count[socket.gethostname()]", "language": "python", "code": "def _get_node_rank(self) -> int:\r\n        \"\"\"A helper method for getting the node rank.\r\n\r\n        The node rank is determined by the position of the current node in the list of hosts used in the job. This is\r\n        calculated by reading all hosts from ``LSB_DJOB_RANKFILE`` and finding this node's hostname in the list.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        count: dict[str, int] = {}\r\n        for host in hosts:\r\n            if host not in count:\r\n                count[host] = len(count)\r\n        return count[socket.gethostname()]", "code_tokens": ["def", "_get_node_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "hosts", "=", "self", ".", "_read_hosts", "(", ")", "count", ":", "dict", "[", "str", ",", "int", "]", "=", "{", "}", "for", "host", "in", "hosts", ":", "if", "host", "not", "in", "count", ":", "count", "[", "host", "]", "=", "len", "(", "count", ")", "return", "count", "[", "socket", ".", "gethostname", "(", ")", "]"], "docstring": "A helper method for getting the node rank.", "docstring_tokens": ["a", "helper", "method", "for", "getting", "the", "node", "rank"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 137, "end_line": 149, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_141", "original_string": "def _read_hosts() -> list[str]:\r\n        \"\"\"Read compute hosts that are a part of the compute job.\r\n\r\n        LSF uses the Job Step Manager (JSM) to manage job steps. Job steps are executed by the JSM from \"launch\" nodes.\r\n        Each job is assigned a launch node. This launch node will be the first node in the list contained in\r\n        ``LSB_DJOB_RANKFILE``.\r\n\r\n        \"\"\"\r\n        var = \"LSB_DJOB_RANKFILE\"\r\n        rankfile = os.environ.get(var)\r\n        if rankfile is None:\r\n            raise ValueError(\"Did not find the environment variable `LSB_DJOB_RANKFILE`\")\r\n        if not rankfile:\r\n            raise ValueError(\"The environment variable `LSB_DJOB_RANKFILE` is empty\")\r\n\r\n        fs = get_filesystem(rankfile)\r\n        with fs.open(rankfile, \"r\") as f:\r\n            ret = [line.strip() for line in f]\r\n        return ret[1:]", "language": "python", "code": "def _read_hosts() -> list[str]:\r\n        \"\"\"Read compute hosts that are a part of the compute job.\r\n\r\n        LSF uses the Job Step Manager (JSM) to manage job steps. Job steps are executed by the JSM from \"launch\" nodes.\r\n        Each job is assigned a launch node. This launch node will be the first node in the list contained in\r\n        ``LSB_DJOB_RANKFILE``.\r\n\r\n        \"\"\"\r\n        var = \"LSB_DJOB_RANKFILE\"\r\n        rankfile = os.environ.get(var)\r\n        if rankfile is None:\r\n            raise ValueError(\"Did not find the environment variable `LSB_DJOB_RANKFILE`\")\r\n        if not rankfile:\r\n            raise ValueError(\"The environment variable `LSB_DJOB_RANKFILE` is empty\")\r\n\r\n        fs = get_filesystem(rankfile)\r\n        with fs.open(rankfile, \"r\") as f:\r\n            ret = [line.strip() for line in f]\r\n        return ret[1:]", "code_tokens": ["def", "_read_hosts", "(", ")", "-", ">", "list", "[", "str", "]", ":", "STRING", "var", "=", "STRING", "rankfile", "=", "os", ".", "environ", ".", "get", "(", "var", ")", "if", "rankfile", "is", "None", ":", "raise", "ValueError", "(", "STRING", ")", "if", "not", "rankfile", ":", "raise", "ValueError", "(", "STRING", ")", "fs", "=", "get_filesystem", "(", "rankfile", ")", "with", "fs", ".", "open", "(", "rankfile", ",", "STRING", ")", "as", "f", ":", "ret", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "f", "]", "return", "ret", "[", "1", ":", "]"], "docstring": "Read compute hosts that are a part of the compute job.", "docstring_tokens": ["read", "compute", "hosts", "that", "are", "a", "part", "of", "the", "compute", "job"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 152, "end_line": 171, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_142", "original_string": "def _get_main_address(self) -> str:\r\n        \"\"\"A helper for getting the main address.\r\n\r\n        The main address is assigned to the first node in the list of nodes used for the job.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        return hosts[0]", "language": "python", "code": "def _get_main_address(self) -> str:\r\n        \"\"\"A helper for getting the main address.\r\n\r\n        The main address is assigned to the first node in the list of nodes used for the job.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        return hosts[0]", "code_tokens": ["def", "_get_main_address", "(", "self", ")", "-", ">", "str", ":", "STRING", "hosts", "=", "self", ".", "_read_hosts", "(", ")", "return", "hosts", "[", "0", "]"], "docstring": "A helper for getting the main address.", "docstring_tokens": ["a", "helper", "for", "getting", "the", "main", "address"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 173, "end_line": 180, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "function_143", "original_string": "def _get_main_port() -> int:\r\n        \"\"\"A helper function for accessing the main port.\r\n\r\n        Uses the LSF job ID so all ranks can compute the main port.\r\n\r\n        \"\"\"\r\n        if \"MASTER_PORT\" in os.environ:\r\n            log.debug(f\"Using externally specified main port: {os.environ['MASTER_PORT']}\")\r\n            return int(os.environ[\"MASTER_PORT\"])\r\n        if \"LSB_JOBID\" in os.environ:\r\n            port = int(os.environ[\"LSB_JOBID\"])\r\n            port = port % 1000 + 10000\r\n            log.debug(f\"calculated LSF main port: {port}\")\r\n            return port\r\n        raise ValueError(\"Could not find job id in environment variable LSB_JOBID\")", "language": "python", "code": "def _get_main_port() -> int:\r\n        \"\"\"A helper function for accessing the main port.\r\n\r\n        Uses the LSF job ID so all ranks can compute the main port.\r\n\r\n        \"\"\"\r\n        if \"MASTER_PORT\" in os.environ:\r\n            log.debug(f\"Using externally specified main port: {os.environ['MASTER_PORT']}\")\r\n            return int(os.environ[\"MASTER_PORT\"])\r\n        if \"LSB_JOBID\" in os.environ:\r\n            port = int(os.environ[\"LSB_JOBID\"])\r\n            port = port % 1000 + 10000\r\n            log.debug(f\"calculated LSF main port: {port}\")\r\n            return port\r\n        raise ValueError(\"Could not find job id in environment variable LSB_JOBID\")", "code_tokens": ["def", "_get_main_port", "(", ")", "-", ">", "int", ":", "STRING", "if", "STRING", "in", "os", ".", "environ", ":", "log", ".", "debug", "(", "fSTRING", ")", "return", "int", "(", "os", ".", "environ", "[", "STRING", "]", ")", "if", "STRING", "in", "os", ".", "environ", ":", "port", "=", "int", "(", "os", ".", "environ", "[", "STRING", "]", ")", "port", "=", "port", "%", "1000", "+", "10000", "log", ".", "debug", "(", "fSTRING", ")", "return", "port", "raise", "ValueError", "(", "STRING", ")"], "docstring": "A helper function for accessing the main port.", "docstring_tokens": ["a", "helper", "function", "for", "accessing", "the", "main", "port"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "start_line": 183, "end_line": 199, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\mpi.py", "func_name": "function_144", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.\"\"\"\r\n        if not _MPI4PY_AVAILABLE:\r\n            return False\r\n\r\n        from mpi4py import MPI\r\n\r\n        return MPI.COMM_WORLD.Get_size() > 1", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.\"\"\"\r\n        if not _MPI4PY_AVAILABLE:\r\n            return False\r\n\r\n        from mpi4py import MPI\r\n\r\n        return MPI.COMM_WORLD.Get_size() > 1", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "STRING", "if", "not", "_MPI4PY_AVAILABLE", ":", "return", "False", "from", "mpi4py", "import", "MPI", "return", "MPI", ".", "COMM_WORLD", ".", "Get_size", "(", ")", ">", "1"], "docstring": "Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.", "docstring_tokens": ["returns", "true", "if", "the", "mpi4py", "package", "is", "installed", "and", "mpi", "returns", "a", "world", "size", "greater", "than", "1"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\mpi.py", "start_line": 70, "end_line": 77, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "function_145", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched on a SLURM cluster.\r\n\r\n        It is possible to use the SLURM scheduler to request resources and then launch processes manually using a\r\n        different environment. For this, the user can set the job name in SLURM to 'bash' or 'interactive' (srun --job-\r\n        name=interactive). This will then avoid the detection of ``SLURMEnvironment`` and another environment can be\r\n        detected automatically.\r\n\r\n        \"\"\"\r\n        SLURMEnvironment._validate_srun_used()\r\n        return _is_srun_used()", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched on a SLURM cluster.\r\n\r\n        It is possible to use the SLURM scheduler to request resources and then launch processes manually using a\r\n        different environment. For this, the user can set the job name in SLURM to 'bash' or 'interactive' (srun --job-\r\n        name=interactive). This will then avoid the detection of ``SLURMEnvironment`` and another environment can be\r\n        detected automatically.\r\n\r\n        \"\"\"\r\n        SLURMEnvironment._validate_srun_used()\r\n        return _is_srun_used()", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "STRING", "SLURMEnvironment", ".", "_validate_srun_used", "(", ")", "return", "_is_srun_used", "(", ")"], "docstring": "Returns ``True`` if the current process was launched on a SLURM cluster.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "on", "a", "slurm", "cluster"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "start_line": 102, "end_line": 112, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "function_146", "original_string": "def resolve_root_node_address(nodes: str) -> str:\r\n        \"\"\"The node selection format in SLURM supports several formats.\r\n\r\n        This function selects the first host name from\r\n\r\n        - a space-separated list of host names, e.g., 'host0 host1 host3' yields 'host0' as the root\r\n        - a comma-separated list of host names, e.g., 'host0,host1,host3' yields 'host0' as the root\r\n        - the range notation with brackets, e.g., 'host[5-9]' yields 'host5' as the root\r\n\r\n        \"\"\"\r\n        nodes = re.sub(r\"\\[(.*?)[,-].*\\]\", \"\\\\1\", nodes)  # Take the first node of every node range\r\n        nodes = re.sub(r\"\\[(.*?)\\]\", \"\\\\1\", nodes)  # handle special case where node range is single number\r\n        return nodes.split(\" \")[0].split(\",\")[0]", "language": "python", "code": "def resolve_root_node_address(nodes: str) -> str:\r\n        \"\"\"The node selection format in SLURM supports several formats.\r\n\r\n        This function selects the first host name from\r\n\r\n        - a space-separated list of host names, e.g., 'host0 host1 host3' yields 'host0' as the root\r\n        - a comma-separated list of host names, e.g., 'host0,host1,host3' yields 'host0' as the root\r\n        - the range notation with brackets, e.g., 'host[5-9]' yields 'host5' as the root\r\n\r\n        \"\"\"\r\n        nodes = re.sub(r\"\\[(.*?)[,-].*\\]\", \"\\\\1\", nodes)  # Take the first node of every node range\r\n        nodes = re.sub(r\"\\[(.*?)\\]\", \"\\\\1\", nodes)  # handle special case where node range is single number\r\n        return nodes.split(\" \")[0].split(\",\")[0]", "code_tokens": ["def", "resolve_root_node_address", "(", "nodes", ":", "str", ")", "-", ">", "str", ":", "STRING", "nodes", "=", "re", ".", "sub", "(", "rSTRING", ",", "STRING", ",", "nodes", ")", "#", "Take", "the", "first", "node", "of", "every", "node", "range", "nodes", "=", "re", ".", "sub", "(", "rSTRING", ",", "STRING", ",", "nodes", ")", "#", "handle", "special", "case", "where", "node", "range", "is", "single", "number", "return", "nodes", ".", "split", "(", "STRING", ")", "[", "0", "]", ".", "split", "(", "STRING", ")", "[", "0", "]"], "docstring": "The node selection format in SLURM supports several formats.", "docstring_tokens": ["the", "node", "selection", "format", "in", "slurm", "supports", "several", "formats"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "start_line": 174, "end_line": 186, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "function_147", "original_string": "def _validate_srun_used() -> None:\r\n        \"\"\"Checks if the `srun` command is available and used.\r\n\r\n        Parallel jobs (multi-GPU, multi-node) in SLURM are launched by prepending `srun` in front of the Python command.\r\n        Not doing so will result in processes hanging, which is a frequent user error. Lightning will emit a warning if\r\n        `srun` is found but not used.\r\n\r\n        \"\"\"\r\n        if _IS_WINDOWS:\r\n            return\r\n\r\n        srun_exists = shutil.which(\"srun\") is not None\r\n        if srun_exists and not _is_srun_used():\r\n            hint = \" \".join([\"srun\", os.path.basename(sys.executable), *sys.argv])[:64]\r\n            rank_zero_warn(\r\n                \"The `srun` command is available on your system but is not used. HINT: If your intention is to run\"\r\n                f\" Lightning on SLURM, prepend your python command with `srun` like so: {hint} ...\",\r\n                category=PossibleUserWarning,\r\n            )", "language": "python", "code": "def _validate_srun_used() -> None:\r\n        \"\"\"Checks if the `srun` command is available and used.\r\n\r\n        Parallel jobs (multi-GPU, multi-node) in SLURM are launched by prepending `srun` in front of the Python command.\r\n        Not doing so will result in processes hanging, which is a frequent user error. Lightning will emit a warning if\r\n        `srun` is found but not used.\r\n\r\n        \"\"\"\r\n        if _IS_WINDOWS:\r\n            return\r\n\r\n        srun_exists = shutil.which(\"srun\") is not None\r\n        if srun_exists and not _is_srun_used():\r\n            hint = \" \".join([\"srun\", os.path.basename(sys.executable), *sys.argv])[:64]\r\n            rank_zero_warn(\r\n                \"The `srun` command is available on your system but is not used. HINT: If your intention is to run\"\r\n                f\" Lightning on SLURM, prepend your python command with `srun` like so: {hint} ...\",\r\n                category=PossibleUserWarning,\r\n            )", "code_tokens": ["def", "_validate_srun_used", "(", ")", "-", ">", "None", ":", "STRING", "if", "_IS_WINDOWS", ":", "return", "srun_exists", "=", "shutil", ".", "which", "(", "STRING", ")", "is", "not", "None", "if", "srun_exists", "and", "not", "_is_srun_used", "(", ")", ":", "hint", "=", "STRING", ".", "join", "(", "[", "STRING", ",", "os", ".", "path", ".", "basename", "(", "sys", ".", "executable", ")", ",", "*", "sys", ".", "argv", "]", ")", "[", ":", "64", "]", "rank_zero_warn", "(", "STRING", "fSTRING", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Checks if the `srun` command is available and used.", "docstring_tokens": ["checks", "if", "the", "srun", "command", "is", "available", "and", "used"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "start_line": 189, "end_line": 207, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "function_148", "original_string": "def _validate_srun_variables() -> None:\r\n        \"\"\"Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.\r\n\r\n        Right now, we only check for the most common user errors. See\r\n        `the srun docs <https://slurm.schedmd.com/srun.html>`_\r\n        for a complete list of supported srun variables.\r\n\r\n        \"\"\"\r\n        ntasks = int(os.environ.get(\"SLURM_NTASKS\", \"1\"))\r\n        if ntasks > 1 and \"SLURM_NTASKS_PER_NODE\" not in os.environ:\r\n            raise RuntimeError(\r\n                f\"You set `--ntasks={ntasks}` in your SLURM bash script, but this variable is not supported.\"\r\n                f\" HINT: Use `--ntasks-per-node={ntasks}` instead.\"\r\n            )", "language": "python", "code": "def _validate_srun_variables() -> None:\r\n        \"\"\"Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.\r\n\r\n        Right now, we only check for the most common user errors. See\r\n        `the srun docs <https://slurm.schedmd.com/srun.html>`_\r\n        for a complete list of supported srun variables.\r\n\r\n        \"\"\"\r\n        ntasks = int(os.environ.get(\"SLURM_NTASKS\", \"1\"))\r\n        if ntasks > 1 and \"SLURM_NTASKS_PER_NODE\" not in os.environ:\r\n            raise RuntimeError(\r\n                f\"You set `--ntasks={ntasks}` in your SLURM bash script, but this variable is not supported.\"\r\n                f\" HINT: Use `--ntasks-per-node={ntasks}` instead.\"\r\n            )", "code_tokens": ["def", "_validate_srun_variables", "(", ")", "-", ">", "None", ":", "STRING", "ntasks", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "STRING", ",", "STRING", ")", ")", "if", "ntasks", ">", "1", "and", "STRING", "not", "in", "os", ".", "environ", ":", "raise", "RuntimeError", "(", "fSTRING", "fSTRING", ")"], "docstring": "Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.", "docstring_tokens": ["checks", "for", "conflicting", "or", "incorrectly", "set", "variables", "set", "through", "srun", "and", "raises", "a", "useful", "error", "message"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "start_line": 210, "end_line": 223, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\torchelastic.py", "func_name": "function_149", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"\r\n        return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"\r\n        return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "STRING", "return", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_torchelastic_launched", "(", ")"], "docstring": "Returns ``True`` if the current process was launched using the torchelastic command.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "using", "the", "torchelastic", "command"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\torchelastic.py", "start_line": 55, "end_line": 58, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "function_150", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and hosts.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.world_size()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.xrt_world_size()", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and hosts.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.world_size()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.xrt_world_size()", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "STRING", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "world_size", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "xrt_world_size", "(", ")"], "docstring": "The number of processes across all devices and hosts.", "docstring_tokens": ["the", "number", "of", "processes", "across", "all", "devices", "and", "hosts"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "start_line": 62, "end_line": 75, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "function_151", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all host and devices.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.global_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_ordinal()", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all host and devices.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.global_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_ordinal()", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "global_ordinal", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "get_ordinal", "(", ")"], "docstring": "The rank (index) of the currently running process across all host and devices.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "across", "all", "host", "and", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "start_line": 83, "end_line": 96, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "function_152", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current host.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.local_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_local_ordinal()", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current host.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.local_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_local_ordinal()", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "local_ordinal", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "get_local_ordinal", "(", ")"], "docstring": "The rank (index) of the currently running process inside of the current host.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "inside", "of", "the", "current", "host"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "start_line": 104, "end_line": 117, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "function_153", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the host on which the current process runs.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.host_index()\r\n        import torch_xla.core.xla_env_vars as xenv\r\n        from torch_xla.utils.utils import getenv_as\r\n\r\n        return getenv_as(xenv.HOST_ORDINAL, int, 0)", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the host on which the current process runs.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.host_index()\r\n        import torch_xla.core.xla_env_vars as xenv\r\n        from torch_xla.utils.utils import getenv_as\r\n\r\n        return getenv_as(xenv.HOST_ORDINAL, int, 0)", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "host_index", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_env_vars", "as", "xenv", "from", "torch_xla", ".", "utils", ".", "utils", "import", "getenv_as", "return", "getenv_as", "(", "xenv", ".", "HOST_ORDINAL", ",", "int", ",", "0", ")"], "docstring": "The rank (index) of the host on which the current process runs.", "docstring_tokens": ["the", "rank", "index", "of", "the", "host", "on", "which", "the", "current", "process", "runs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "start_line": 121, "end_line": 134, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "function_154", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: Optional parameters when saving the model/training states.\r\n\r\n        \"\"\"", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: Optional parameters when saving the model/training states.\r\n\r\n        \"\"\"", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "STRING"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "start_line": 38, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "function_155", "original_string": "def load_checkpoint(self, path: _PATH, map_location: Optional[Any] = None) -> dict[str, Any]:\r\n        \"\"\"Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        \"\"\"", "language": "python", "code": "def load_checkpoint(self, path: _PATH, map_location: Optional[Any] = None) -> dict[str, Any]:\r\n        \"\"\"Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        \"\"\"", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "map_location", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING"], "docstring": "Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.", "docstring_tokens": ["load", "checkpoint", "from", "a", "path", "when", "resuming", "or", "loading", "ckpt", "for", "test", "validate", "predict", "stages"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "start_line": 49, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "function_156", "original_string": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"", "language": "python", "code": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "path", ":", "_PATH", ")", "-", ">", "None", ":", "STRING"], "docstring": "Remove checkpoint file from the filesystem.", "docstring_tokens": ["remove", "checkpoint", "file", "from", "the", "filesystem"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "start_line": 62, "end_line": 68, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "function_157", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the process.\"\"\"", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the process.\"\"\"", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "This method is called to teardown the process.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "start_line": 70, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "function_158", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``TorchCheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        _atomic_save(checkpoint, path)", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``TorchCheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        _atomic_save(checkpoint, path)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "STRING", ")", "fs", "=", "get_filesystem", "(", "path", ")", "fs", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ",", "exist_ok", "=", "True", ")", "_atomic_save", "(", "checkpoint", ",", "path", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "start_line": 36, "end_line": 57, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "function_159", "original_string": "def load_checkpoint(\r\n        self, path: _PATH, map_location: Optional[Callable] = lambda storage, loc: storage\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        Raises:\r\n            FileNotFoundError: If ``path`` is not found by the ``fsspec`` filesystem\r\n\r\n        \"\"\"\r\n\r\n        fs = get_filesystem(path)\r\n        if not fs.exists(path):\r\n            raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\r\n\r\n        return pl_load(path, map_location=map_location)", "language": "python", "code": "def load_checkpoint(\r\n        self, path: _PATH, map_location: Optional[Callable] = lambda storage, loc: storage\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        Raises:\r\n            FileNotFoundError: If ``path`` is not found by the ``fsspec`` filesystem\r\n\r\n        \"\"\"\r\n\r\n        fs = get_filesystem(path)\r\n        if not fs.exists(path):\r\n            raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\r\n\r\n        return pl_load(path, map_location=map_location)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "map_location", ":", "Optional", "[", "Callable", "]", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "fs", "=", "get_filesystem", "(", "path", ")", "if", "not", "fs", ".", "exists", "(", "path", ")", ":", "raise", "FileNotFoundError", "(", "fSTRING", ")", "return", "pl_load", "(", "path", ",", "map_location", "=", "map_location", ")"], "docstring": "Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.", "docstring_tokens": ["loads", "checkpoint", "using", "func", "torch", "load", "with", "additional", "handling", "for", "fsspec", "remote", "loading", "of", "files"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "start_line": 60, "end_line": 82, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "function_160", "original_string": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"\r\n        fs = get_filesystem(path)\r\n        if fs.exists(path):\r\n            fs.rm(path, recursive=True)\r\n            log.debug(f\"Removed checkpoint: {path}\")", "language": "python", "code": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"\r\n        fs = get_filesystem(path)\r\n        if fs.exists(path):\r\n            fs.rm(path, recursive=True)\r\n            log.debug(f\"Removed checkpoint: {path}\")", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "path", ":", "_PATH", ")", "-", ">", "None", ":", "STRING", "fs", "=", "get_filesystem", "(", "path", ")", "if", "fs", ".", "exists", "(", "path", ")", ":", "fs", ".", "rm", "(", "path", ",", "recursive", "=", "True", ")", "log", ".", "debug", "(", "fSTRING", ")"], "docstring": "Remove checkpoint file from the filesystem.", "docstring_tokens": ["remove", "checkpoint", "file", "from", "the", "filesystem"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "start_line": 85, "end_line": 95, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\xla.py", "func_name": "function_161", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``XLACheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        if RequirementCache(\"omegaconf\"):\r\n            from omegaconf import DictConfig, ListConfig, OmegaConf\r\n\r\n            checkpoint = apply_to_collection(checkpoint, (DictConfig, ListConfig), OmegaConf.to_container)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        cpu_data = xm._maybe_convert_to_cpu(checkpoint, convert=True)\r\n        log.debug(f\"Saving checkpoint: {path}\")\r\n        torch.save(cpu_data, path)", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``XLACheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        if RequirementCache(\"omegaconf\"):\r\n            from omegaconf import DictConfig, ListConfig, OmegaConf\r\n\r\n            checkpoint = apply_to_collection(checkpoint, (DictConfig, ListConfig), OmegaConf.to_container)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        cpu_data = xm._maybe_convert_to_cpu(checkpoint, convert=True)\r\n        log.debug(f\"Saving checkpoint: {path}\")\r\n        torch.save(cpu_data, path)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "STRING", ")", "fs", "=", "get_filesystem", "(", "path", ")", "fs", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ",", "exist_ok", "=", "True", ")", "if", "RequirementCache", "(", "STRING", ")", ":", "from", "omegaconf", "import", "DictConfig", ",", "ListConfig", ",", "OmegaConf", "checkpoint", "=", "apply_to_collection", "(", "checkpoint", ",", "(", "DictConfig", ",", "ListConfig", ")", ",", "OmegaConf", ".", "to_container", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "cpu_data", "=", "xm", ".", "_maybe_convert_to_cpu", "(", "checkpoint", ",", "convert", "=", "True", ")", "log", ".", "debug", "(", "fSTRING", ")", "torch", ".", "save", "(", "cpu_data", ",", "path", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\io\\xla.py", "start_line": 43, "end_line": 73, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\amp.py", "func_name": "function_162", "original_string": "def _optimizer_handles_unscaling(optimizer: Any) -> bool:\r\n    \"\"\"Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the\r\n    :class:`torch.cuda.amp.GradScaler`.\r\n\r\n    Since, the current implementation of this function checks a PyTorch internal variable on the optimizer, the return\r\n    value will only be reliable for built-in PyTorch optimizers.\r\n\r\n    \"\"\"\r\n    return getattr(optimizer, \"_step_supports_amp_scaling\", False)", "language": "python", "code": "def _optimizer_handles_unscaling(optimizer: Any) -> bool:\r\n    \"\"\"Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the\r\n    :class:`torch.cuda.amp.GradScaler`.\r\n\r\n    Since, the current implementation of this function checks a PyTorch internal variable on the optimizer, the return\r\n    value will only be reliable for built-in PyTorch optimizers.\r\n\r\n    \"\"\"\r\n    return getattr(optimizer, \"_step_supports_amp_scaling\", False)", "code_tokens": ["def", "_optimizer_handles_unscaling", "(", "optimizer", ":", "Any", ")", "-", ">", "bool", ":", "STRING", "return", "getattr", "(", "optimizer", ",", "STRING", ",", "False", ")"], "docstring": "Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the", "docstring_tokens": ["determines", "whether", "a", "pytorch", "optimizer", "handles", "unscaling", "gradients", "in", "the", "step", "method", "rather", "than", "through", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\amp.py", "start_line": 115, "end_line": 123, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\bitsandbytes.py", "func_name": "function_163", "original_string": "def quantize_(self, weight: Optional[torch.Tensor] = None, device: Optional[torch.device] = None) -> None:\r\n            \"\"\"Inplace quantize.\"\"\"\r\n            if weight is None:\r\n                weight = self.weight.data\r\n            if weight.data.dtype == torch.int8:\r\n                return\r\n            assert isinstance(self.weight, bnb.nn.Int8Params)\r\n            self.weight = self.quantize(self.weight, weight, device)", "language": "python", "code": "def quantize_(self, weight: Optional[torch.Tensor] = None, device: Optional[torch.device] = None) -> None:\r\n            \"\"\"Inplace quantize.\"\"\"\r\n            if weight is None:\r\n                weight = self.weight.data\r\n            if weight.data.dtype == torch.int8:\r\n                return\r\n            assert isinstance(self.weight, bnb.nn.Int8Params)\r\n            self.weight = self.quantize(self.weight, weight, device)", "code_tokens": ["def", "quantize_", "(", "self", ",", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "weight", "is", "None", ":", "weight", "=", "self", ".", "weight", ".", "data", "if", "weight", ".", "data", ".", "dtype", "=", "=", "torch", ".", "int8", ":", "return", "assert", "isinstance", "(", "self", ".", "weight", ",", "bnb", ".", "nn", ".", "Int8Params", ")", "self", ".", "weight", "=", "self", ".", "quantize", "(", "self", ".", "weight", ",", "weight", ",", "device", ")"], "docstring": "Inplace quantize.", "docstring_tokens": ["inplace", "quantize"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\bitsandbytes.py", "start_line": 236, "end_line": 244, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\bitsandbytes.py", "func_name": "function_164", "original_string": "def quantize_(self, weight: Optional[torch.Tensor] = None, device: Optional[torch.device] = None) -> None:\r\n            \"\"\"Inplace quantize.\"\"\"\r\n            if weight is None:\r\n                weight = self.weight.data\r\n            if weight.data.dtype == torch.uint8:\r\n                return\r\n            assert isinstance(self.weight, bnb.nn.Params4bit)\r\n            self.weight = self.quantize(self.weight, weight, device)\r\n            self.weight.bnb_quantized = True", "language": "python", "code": "def quantize_(self, weight: Optional[torch.Tensor] = None, device: Optional[torch.device] = None) -> None:\r\n            \"\"\"Inplace quantize.\"\"\"\r\n            if weight is None:\r\n                weight = self.weight.data\r\n            if weight.data.dtype == torch.uint8:\r\n                return\r\n            assert isinstance(self.weight, bnb.nn.Params4bit)\r\n            self.weight = self.quantize(self.weight, weight, device)\r\n            self.weight.bnb_quantized = True", "code_tokens": ["def", "quantize_", "(", "self", ",", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "weight", "is", "None", ":", "weight", "=", "self", ".", "weight", ".", "data", "if", "weight", ".", "data", ".", "dtype", "=", "=", "torch", ".", "uint8", ":", "return", "assert", "isinstance", "(", "self", ".", "weight", ",", "bnb", ".", "nn", ".", "Params4bit", ")", "self", ".", "weight", "=", "self", ".", "quantize", "(", "self", ".", "weight", ",", "weight", ",", "device", ")", "self", ".", "weight", ".", "bnb_quantized", "=", "True"], "docstring": "Inplace quantize.", "docstring_tokens": ["inplace", "quantize"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\bitsandbytes.py", "start_line": 323, "end_line": 332, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\deepspeed.py", "func_name": "function_165", "original_string": "def backward(self, tensor: Tensor, model: \"DeepSpeedEngine\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs back-propagation using DeepSpeed's engine.\"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "language": "python", "code": "def backward(self, tensor: Tensor, model: \"DeepSpeedEngine\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs back-propagation using DeepSpeed's engine.\"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "STRING", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "model", ".", "backward", "(", "tensor", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Performs back-propagation using DeepSpeed's engine.", "docstring_tokens": ["performs", "back", "propagation", "using", "deepspeed", "s", "engine"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\deepspeed.py", "start_line": 88, "end_line": 90, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_166", "original_string": "def convert_module(self, module: Module) -> Module:\r\n        \"\"\"Convert the module parameters to the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return module", "language": "python", "code": "def convert_module(self, module: Module) -> Module:\r\n        \"\"\"Convert the module parameters to the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return module", "code_tokens": ["def", "convert_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "return", "module"], "docstring": "Convert the module parameters to the precision type this plugin handles.", "docstring_tokens": ["convert", "the", "module", "parameters", "to", "the", "precision", "type", "this", "plugin", "handles"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 47, "end_line": 53, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_167", "original_string": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        return nullcontext()", "language": "python", "code": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        return nullcontext()", "code_tokens": ["def", "tensor_init_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "nullcontext", "(", ")"], "docstring": "Controls how tensors get created (device, dtype).", "docstring_tokens": ["controls", "how", "tensors", "get", "created", "device", "dtype"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 55, "end_line": 57, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_168", "original_string": "def module_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Instantiate module parameters or tensors in the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return nullcontext()", "language": "python", "code": "def module_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Instantiate module parameters or tensors in the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return nullcontext()", "code_tokens": ["def", "module_init_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "nullcontext", "(", ")"], "docstring": "Instantiate module parameters or tensors in the precision type this plugin handles.", "docstring_tokens": ["instantiate", "module", "parameters", "or", "tensors", "in", "the", "precision", "type", "this", "plugin", "handles"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 59, "end_line": 65, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_169", "original_string": "def forward_context(self) -> AbstractContextManager:\r\n        \"\"\"A contextmanager for managing model forward/training_step/evaluation_step/predict_step.\"\"\"\r\n        return nullcontext()", "language": "python", "code": "def forward_context(self) -> AbstractContextManager:\r\n        \"\"\"A contextmanager for managing model forward/training_step/evaluation_step/predict_step.\"\"\"\r\n        return nullcontext()", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "nullcontext", "(", ")"], "docstring": "A contextmanager for managing model forward/training_step/evaluation_step/predict_step.", "docstring_tokens": ["a", "contextmanager", "for", "managing", "model", "forward", "training_step", "evaluation_step", "predict_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 67, "end_line": 69, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_170", "original_string": "def convert_input(self, data: Any) -> Any:\r\n        \"\"\"Convert model inputs (forward) to the floating point precision type of this plugin.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "language": "python", "code": "def convert_input(self, data: Any) -> Any:\r\n        \"\"\"Convert model inputs (forward) to the floating point precision type of this plugin.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "code_tokens": ["def", "convert_input", "(", "self", ",", "data", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "return", "data"], "docstring": "Convert model inputs (forward) to the floating point precision type of this plugin.", "docstring_tokens": ["convert", "model", "inputs", "forward", "to", "the", "floating", "point", "precision", "type", "of", "this", "plugin"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 71, "end_line": 78, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_171", "original_string": "def convert_output(self, data: Any) -> Any:\r\n        \"\"\"Convert outputs to the floating point precision type expected after model's forward.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "language": "python", "code": "def convert_output(self, data: Any) -> Any:\r\n        \"\"\"Convert outputs to the floating point precision type expected after model's forward.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "code_tokens": ["def", "convert_output", "(", "self", ",", "data", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "return", "data"], "docstring": "Convert outputs to the floating point precision type expected after model's forward.", "docstring_tokens": ["convert", "outputs", "to", "the", "floating", "point", "precision", "type", "expected", "after", "model", "s", "forward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 80, "end_line": 87, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_172", "original_string": "def pre_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs before precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "language": "python", "code": "def pre_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs before precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "code_tokens": ["def", "pre_backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ")", "-", ">", "Any", ":", "STRING"], "docstring": "Runs before precision plugin executes backward.", "docstring_tokens": ["runs", "before", "precision", "plugin", "executes", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 89, "end_line": 96, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_173", "original_string": "def backward(self, tensor: Tensor, model: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            model: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"\r\n        tensor.backward(*args, **kwargs)", "language": "python", "code": "def backward(self, tensor: Tensor, model: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            model: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"\r\n        tensor.backward(*args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "Optional", "[", "Module", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "tensor", ".", "backward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Performs the actual backpropagation.", "docstring_tokens": ["performs", "the", "actual", "backpropagation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 98, "end_line": 106, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_174", "original_string": "def post_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs after precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "language": "python", "code": "def post_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs after precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "code_tokens": ["def", "post_backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ")", "-", ">", "Any", ":", "STRING"], "docstring": "Runs after precision plugin executes backward.", "docstring_tokens": ["runs", "after", "precision", "plugin", "executes", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 108, "end_line": 115, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_175", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        return optimizer.step(**kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        return optimizer.step(**kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "STRING", "return", "optimizer", ".", "step", "(", "*", "*", "kwargs", ")"], "docstring": "Hook to run the optimizer step.", "docstring_tokens": ["hook", "to", "run", "the", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 117, "end_line": 123, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_176", "original_string": "def main_params(self, optimizer: Optimizer) -> _PARAMETERS:\r\n        \"\"\"The main params of the model.\r\n\r\n        Returns the plain model params here. Maybe different in other precision plugins.\r\n\r\n        \"\"\"\r\n        for group in optimizer.param_groups:\r\n            yield from group[\"params\"]", "language": "python", "code": "def main_params(self, optimizer: Optimizer) -> _PARAMETERS:\r\n        \"\"\"The main params of the model.\r\n\r\n        Returns the plain model params here. Maybe different in other precision plugins.\r\n\r\n        \"\"\"\r\n        for group in optimizer.param_groups:\r\n            yield from group[\"params\"]", "code_tokens": ["def", "main_params", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "_PARAMETERS", ":", "STRING", "for", "group", "in", "optimizer", ".", "param_groups", ":", "yield", "from", "group", "[", "STRING", "]"], "docstring": "The main params of the model.", "docstring_tokens": ["the", "main", "params", "of", "the", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 125, "end_line": 132, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_177", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate precision plugin state_dict.\r\n\r\n        Returns:\r\n            A dictionary containing precision plugin state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate precision plugin state_dict.\r\n\r\n        Returns:\r\n            A dictionary containing precision plugin state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate precision plugin state_dict.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "precision", "plugin", "state_dict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 137, "end_line": 144, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_178", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload precision plugin state given precision plugin\r\n        state_dict.\r\n\r\n        Args:\r\n            state_dict: the precision plugin state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload precision plugin state given precision plugin\r\n        state_dict.\r\n\r\n        Args:\r\n            state_dict: the precision plugin state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when loading a checkpoint, implement to reload precision plugin state given precision plugin", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "precision", "plugin", "state", "given", "precision", "plugin"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 146, "end_line": 154, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "function_179", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "This method is called to teardown the training process.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "start_line": 156, "end_line": 161, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "function_180", "original_string": "def setup_module(self, module: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self._determine_ddp_device_ids()\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=module, device_ids=device_ids, **self._ddp_kwargs)", "language": "python", "code": "def setup_module(self, module: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self._determine_ddp_device_ids()\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=module, device_ids=device_ids, **self._ddp_kwargs)", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "DistributedDataParallel", ":", "STRING", "device_ids", "=", "self", ".", "_determine_ddp_device_ids", "(", ")", "ctx", "=", "torch", ".", "cuda", ".", "stream", "(", "torch", ".", "cuda", ".", "Stream", "(", ")", ")", "if", "device_ids", "is", "not", "None", "else", "nullcontext", "(", ")", "with", "ctx", ":", "return", "DistributedDataParallel", "(", "module", "=", "module", ",", "device_ids", "=", "device_ids", ",", "*", "*", "self", ".", "_ddp_kwargs", ")"], "docstring": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\ddp.py", "start_line": 123, "end_line": 129, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "function_181", "original_string": "def all_reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def all_reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ")", "-", ">", "Tensor", ":", "STRING", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\ddp.py", "start_line": 136, "end_line": 153, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "function_182", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n\r\n        if not isinstance(module, DistributedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `DistributedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n\r\n        if not isinstance(module, DistributedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `DistributedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "STRING", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "if", "not", "isinstance", "(", "module", ",", "DistributedDataParallel", ")", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "fSTRING", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\ddp.py", "start_line": 247, "end_line": 259, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_183", "original_string": "def __init__(\r\n        self,\r\n        accelerator: Optional[Accelerator] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Optional[int] = None,\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. `For more information: https://pytorch-\r\n        lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        `For more information: https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training`.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either ``precision=\"16-mixed\"`` or\r\n                ``precision=\"bf16-mixed\"``.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size.\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise ImportError(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision=precision,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._backward_sync_control = None  # DeepSpeed handles gradient accumulation internally\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale\r\n\r\n        self._deepspeed_engine: Optional[DeepSpeedEngine] = None", "language": "python", "code": "def __init__(\r\n        self,\r\n        accelerator: Optional[Accelerator] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Optional[int] = None,\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. `For more information: https://pytorch-\r\n        lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        `For more information: https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training`.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either ``precision=\"16-mixed\"`` or\r\n                ``precision=\"bf16-mixed\"``.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size.\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise ImportError(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision=precision,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._backward_sync_control = None  # DeepSpeed handles gradient accumulation internally\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale\r\n\r\n        self._deepspeed_engine: Optional[DeepSpeedEngine] = None", "code_tokens": ["def", "__init__", "(", "self", ",", "accelerator", ":", "Optional", "[", "Accelerator", "]", "=", "None", ",", "zero_optimization", ":", "bool", "=", "True", ",", "stage", ":", "int", "=", "2", ",", "remote_device", ":", "Optional", "[", "str", "]", "=", "None", ",", "offload_optimizer", ":", "bool", "=", "False", ",", "offload_parameters", ":", "bool", "=", "False", ",", "offload_params_device", ":", "str", "=", "STRING", ",", "nvme_path", ":", "str", "=", "STRING", ",", "params_buffer_count", ":", "int", "=", "5", ",", "params_buffer_size", ":", "int", "=", "100_000_000", ",", "max_in_cpu", ":", "int", "=", "1_000_000_000", ",", "offload_optimizer_device", ":", "str", "=", "STRING", ",", "optimizer_buffer_count", ":", "int", "=", "4", ",", "block_size", ":", "int", "=", "1048576", ",", "queue_depth", ":", "int", "=", "8", ",", "single_submit", ":", "bool", "=", "False", ",", "overlap_events", ":", "bool", "=", "True", ",", "thread_count", ":", "int", "=", "1", ",", "pin_memory", ":", "bool", "=", "False", ",", "sub_group_size", ":", "int", "=", "1_000_000_000_000", ",", "contiguous_gradients", ":", "bool", "=", "True", ",", "overlap_comm", ":", "bool", "=", "True", ",", "allgather_partitions", ":", "bool", "=", "True", ",", "reduce_scatter", ":", "bool", "=", "True", ",", "allgather_bucket_size", ":", "int", "=", "200_000_000", ",", "reduce_bucket_size", ":", "int", "=", "200_000_000", ",", "zero_allow_untested_optimizer", ":", "bool", "=", "True", ",", "logging_batch_size_per_gpu", ":", "Optional", "[", "int", "]", "=", "None", ",", "config", ":", "Optional", "[", "Union", "[", "_PATH", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "logging_level", ":", "int", "=", "logging", ".", "WARN", ",", "parallel_devices", ":", "Optional", "[", "list", "[", "torch", ".", "device", "]", "]", "=", "None", ",", "cluster_environment", ":", "Optional", "[", "ClusterEnvironment", "]", "=", "None", ",", "loss_scale", ":", "float", "=", "0", ",", "initial_scale_power", ":", "int", "=", "16", ",", "loss_scale_window", ":", "int", "=", "1000", ",", "hysteresis", ":", "int", "=", "2", ",", "min_loss_scale", ":", "int", "=", "1", ",", "partition_activations", ":", "bool", "=", "False", ",", "cpu_checkpointing", ":", "bool", "=", "False", ",", "contiguous_memory_optimization", ":", "bool", "=", "False", ",", "synchronize_checkpoint_boundary", ":", "bool", "=", "False", ",", "load_full_weights", ":", "bool", "=", "False", ",", "precision", ":", "Optional", "[", "Precision", "]", "=", "None", ",", "process_group_backend", ":", "Optional", "[", "str", "]", "=", "None", ",", "timeout", ":", "Optional", "[", "timedelta", "]", "=", "default_pg_timeout", ",", "exclude_frozen_parameters", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "ImportError", "(", "STRING", "STRING", ")", "if", "_TORCH_GREATER_EQUAL_2_6", "and", "not", "_DEEPSPEED_GREATER_EQUAL_0_16", ":", "import", "deepspeed", "deepspeed_version", "=", "deepspeed", ".", "__version__", "raise", "ImportError", "(", "fSTRING", "fSTRING", "STRING", ")", "super", "(", ")", ".", "__init__", "(", "accelerator", "=", "accelerator", ",", "parallel_devices", "=", "parallel_devices", ",", "cluster_environment", "=", "cluster_environment", ",", "precision", "=", "precision", ",", "process_group_backend", "=", "process_group_backend", ",", ")", "self", ".", "_backward_sync_control", "=", "None", "#", "DeepSpeed", "handles", "gradient", "accumulation", "internally", "self", ".", "_timeout", ":", "Optional", "[", "timedelta", "]", "=", "timeout", "self", ".", "config", "=", "self", ".", "_load_config", "(", "config", ")", "if", "self", ".", "config", "is", "None", ":", "self", ".", "config", "=", "self", ".", "_create_default_config", "(", "zero_optimization", ",", "zero_allow_untested_optimizer", ",", "logging_batch_size_per_gpu", ",", "offload_optimizer", "=", "offload_optimizer", ",", "offload_parameters", "=", "offload_parameters", ",", "nvme_path", "=", "nvme_path", ",", "offload_params_device", "=", "offload_params_device", ",", "params_buffer_count", "=", "params_buffer_count", ",", "params_buffer_size", "=", "params_buffer_size", ",", "max_in_cpu", "=", "max_in_cpu", ",", "pin_memory", "=", "pin_memory", ",", "offload_optimizer_device", "=", "offload_optimizer_device", ",", "optimizer_buffer_count", "=", "optimizer_buffer_count", ",", "block_size", "=", "block_size", ",", "queue_depth", "=", "queue_depth", ",", "single_submit", "=", "single_submit", ",", "overlap_events", "=", "overlap_events", ",", "thread_count", "=", "thread_count", ",", "partition_activations", "=", "partition_activations", ",", "cpu_checkpointing", "=", "cpu_checkpointing", ",", "contiguous_memory_optimization", "=", "contiguous_memory_optimization", ","], "docstring": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large", "docstring_tokens": ["provides", "capabilities", "to", "run", "training", "using", "the", "deepspeed", "library", "with", "training", "optimizations", "for", "large"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 57, "end_line": 318, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_184", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", list[Optimizer], Any]:\r\n        \"\"\"Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,\r\n        only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine`, a list with a single\r\n            deepspeed optimizer, and an optional learning rate scheduler.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        self._deepspeed_engine, optimizer, scheduler = self._initialize_engine(module, optimizers[0], scheduler)\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self._deepspeed_engine, [optimizer], scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", list[Optimizer], Any]:\r\n        \"\"\"Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,\r\n        only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine`, a list with a single\r\n            deepspeed optimizer, and an optional learning rate scheduler.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        self._deepspeed_engine, optimizer, scheduler = self._initialize_engine(module, optimizers[0], scheduler)\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self._deepspeed_engine, [optimizer], scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "tuple", "[", "STRING", ",", "list", "[", "Optimizer", "]", ",", "Any", "]", ":", "STRING", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "_deepspeed_engine", ",", "optimizer", ",", "scheduler", "=", "self", ".", "_initialize_engine", "(", "module", ",", "optimizers", "[", "0", "]", ",", "scheduler", ")", "self", ".", "_set_deepspeed_activation_checkpointing", "(", ")", "return", "self", ".", "_deepspeed_engine", ",", "[", "optimizer", "]", ",", "scheduler"], "docstring": "Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,", "docstring_tokens": ["set", "up", "a", "model", "and", "multiple", "optimizers", "together", "along", "with", "an", "optional", "learning", "rate", "scheduler", "currently"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 336, "end_line": 354, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_185", "original_string": "def setup_module(self, module: Module) -> \"DeepSpeedEngine\":\r\n        \"\"\"Set up a module for inference (no optimizers).\r\n\r\n        For training, see :meth:`setup_module_and_optimizers`.\r\n\r\n        \"\"\"\r\n        self._deepspeed_engine, _, _ = self._initialize_engine(module)\r\n        return self._deepspeed_engine", "language": "python", "code": "def setup_module(self, module: Module) -> \"DeepSpeedEngine\":\r\n        \"\"\"Set up a module for inference (no optimizers).\r\n\r\n        For training, see :meth:`setup_module_and_optimizers`.\r\n\r\n        \"\"\"\r\n        self._deepspeed_engine, _, _ = self._initialize_engine(module)\r\n        return self._deepspeed_engine", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "STRING", ":", "STRING", "self", ".", "_deepspeed_engine", ",", "_", ",", "_", "=", "self", ".", "_initialize_engine", "(", "module", ")", "return", "self", ".", "_deepspeed_engine"], "docstring": "Set up a module for inference (no optimizers).", "docstring_tokens": ["set", "up", "a", "module", "for", "inference", "no", "optimizers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 357, "end_line": 364, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_186", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Optimizers can only be set up jointly with the model in this strategy.\r\n\r\n        Please use :meth:`setup_module_and_optimizers` to set up both module and optimizer together.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError(self._err_msg_joint_setup_required())", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Optimizers can only be set up jointly with the model in this strategy.\r\n\r\n        Please use :meth:`setup_module_and_optimizers` to set up both module and optimizer together.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError(self._err_msg_joint_setup_required())", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "STRING", "raise", "NotImplementedError", "(", "self", ".", "_err_msg_joint_setup_required", "(", ")", ")"], "docstring": "Optimizers can only be set up jointly with the model in this strategy.", "docstring_tokens": ["optimizers", "can", "only", "be", "set", "up", "jointly", "with", "the", "model", "in", "this", "strategy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 367, "end_line": 373, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_187", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in a checkpoint directory.\r\n\r\n        Args:\r\n            path: A path to where the files should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Unused by this strategy, since it doesn't use a ``CheckpointIO`` plugin.\r\n            filter: Unsupported.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If the unused ``storage_options`` gets passed.\r\n            ValueError:\r\n                When no :class:`deepspeed.DeepSpeedEngine` objects were found in the state, or when multiple\r\n                :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., filter=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` manages the state serialization internally.\"\r\n            )\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving checkpoints with DeepSpeed is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        path = self.broadcast(path)\r\n\r\n        excluded_objects = (engine, engine.optimizer) if engine.optimizer is not None else (engine,)\r\n        state = {k: v for k, v in state.items() if v not in excluded_objects}\r\n        _validate_state_keys(state)\r\n        state = self._convert_stateful_objects_in_state(state, filter={})\r\n        engine.save_checkpoint(\r\n            path, client_state=state, tag=\"checkpoint\", exclude_frozen_parameters=self.exclude_frozen_parameters\r\n        )", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in a checkpoint directory.\r\n\r\n        Args:\r\n            path: A path to where the files should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Unused by this strategy, since it doesn't use a ``CheckpointIO`` plugin.\r\n            filter: Unsupported.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If the unused ``storage_options`` gets passed.\r\n            ValueError:\r\n                When no :class:`deepspeed.DeepSpeedEngine` objects were found in the state, or when multiple\r\n                :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., filter=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` manages the state serialization internally.\"\r\n            )\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving checkpoints with DeepSpeed is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        path = self.broadcast(path)\r\n\r\n        excluded_objects = (engine, engine.optimizer) if engine.optimizer is not None else (engine,)\r\n        state = {k: v for k, v in state.items() if v not in excluded_objects}\r\n        _validate_state_keys(state)\r\n        state = self._convert_stateful_objects_in_state(state, filter={})\r\n        engine.save_checkpoint(\r\n            path, client_state=state, tag=\"checkpoint\", exclude_frozen_parameters=self.exclude_frozen_parameters\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "STRING", ")", "if", "filter", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "STRING", ")", "engines", "=", "_get_deepspeed_engines_from_state", "(", "state", ")", "if", "len", "(", "engines", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "if", "len", "(", "engines", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "engine", "=", "engines", "[", "0", "]", "path", "=", "self", ".", "broadcast", "(", "path", ")", "excluded_objects", "=", "(", "engine", ",", "engine", ".", "optimizer", ")", "if", "engine", ".", "optimizer", "is", "not", "None", "else", "(", "engine", ",", ")", "state", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", "if", "v", "not", "in", "excluded_objects", "}", "_validate_state_keys", "(", "state", ")", "state", "=", "self", ".", "_convert_stateful_objects_in_state", "(", "state", ",", "filter", "=", "{", "}", ")", "engine", ".", "save_checkpoint", "(", "path", ",", "client_state", "=", "state", ",", "tag", "=", "STRING", ",", "exclude_frozen_parameters", "=", "self", ".", "exclude_frozen_parameters", ")"], "docstring": "Save model, optimizer, and other state in a checkpoint directory.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "in", "a", "checkpoint", "directory"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 403, "end_line": 467, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_188", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                This should contain exactly one model, and the model must already be set up by DeepSpeed.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            Dictionary with the state inside DeepSpeed's engine\r\n\r\n        Raises:\r\n            ValueError:\r\n                If no state is provided, when no :class:`deepspeed.DeepSpeedEngine` objects were found in the\r\n                state, or when multiple :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n            RuntimeError:\r\n                If DeepSpeed was unable to load the checkpoint due to missing files or because the checkpoint is\r\n                not in the expected DeepSpeed format.\r\n\r\n        \"\"\"\r\n        if isinstance(state, (Module, Optimizer)) or self.load_full_weights and self.zero_stage_3:\r\n            path = self.broadcast(path)\r\n            return super().load_checkpoint(path=path, state=state, strict=strict)\r\n\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got DeepSpeedStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" DeepSpeedStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        _validate_checkpoint_directory(path)\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving and loading checkpoints\"\r\n                \" with DeepSpeed is currently limited to a single model per checkpoint. To load multiple model\"\r\n                \" states, call the load method for each model checkpoint separately.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        from deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\r\n\r\n        optimzer_state_requested = any(isinstance(item, (Optimizer, DeepSpeedOptimizer)) for item in state.values())\r\n\r\n        torch.cuda.empty_cache()\r\n        _, client_state = engine.load_checkpoint(\r\n            path,\r\n            tag=\"checkpoint\",\r\n            load_optimizer_states=optimzer_state_requested,\r\n            load_lr_scheduler_states=False,\r\n            load_module_strict=strict,\r\n        )\r\n\r\n        if client_state is None:\r\n            raise RuntimeError(\r\n                \"DeepSpeed was unable to load the checkpoint. Ensure you passed in a DeepSpeed compatible checkpoint\"\r\n                \" or a single checkpoint file by setting `DeepSpeedStrategy(..., load_full_weights=True)`.\"\r\n            )\r\n\r\n        keys = set(client_state) & set(state) - {\"optimizer\", \"lr_scheduler\"}\r\n        _move_state_into(source=client_state, destination=state, keys=keys)\r\n        return client_state", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                This should contain exactly one model, and the model must already be set up by DeepSpeed.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            Dictionary with the state inside DeepSpeed's engine\r\n\r\n        Raises:\r\n            ValueError:\r\n                If no state is provided, when no :class:`deepspeed.DeepSpeedEngine` objects were found in the\r\n                state, or when multiple :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n            RuntimeError:\r\n                If DeepSpeed was unable to load the checkpoint due to missing files or because the checkpoint is\r\n                not in the expected DeepSpeed format.\r\n\r\n        \"\"\"\r\n        if isinstance(state, (Module, Optimizer)) or self.load_full_weights and self.zero_stage_3:\r\n            path = self.broadcast(path)\r\n            return super().load_checkpoint(path=path, state=state, strict=strict)\r\n\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got DeepSpeedStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" DeepSpeedStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        _validate_checkpoint_directory(path)\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving and loading checkpoints\"\r\n                \" with DeepSpeed is currently limited to a single model per checkpoint. To load multiple model\"\r\n                \" states, call the load method for each model checkpoint separately.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        from deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\r\n\r\n        optimzer_state_requested = any(isinstance(item, (Optimizer, DeepSpeedOptimizer)) for item in state.values())\r\n\r\n        torch.cuda.empty_cache()\r\n        _, client_state = engine.load_checkpoint(\r\n            path,\r\n            tag=\"checkpoint\",\r\n            load_optimizer_states=optimzer_state_requested,\r\n            load_lr_scheduler_states=False,\r\n            load_module_strict=strict,\r\n        )\r\n\r\n        if client_state is None:\r\n            raise RuntimeError(\r\n                \"DeepSpeed was unable to load the checkpoint. Ensure you passed in a DeepSpeed compatible checkpoint\"\r\n                \" or a single checkpoint file by setting `DeepSpeedStrategy(..., load_full_weights=True)`.\"\r\n            )\r\n\r\n        keys = set(client_state) & set(state) - {\"optimizer\", \"lr_scheduler\"}\r\n        _move_state_into(source=client_state, destination=state, keys=keys)\r\n        return client_state", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "isinstance", "(", "state", ",", "(", "Module", ",", "Optimizer", ")", ")", "or", "self", ".", "load_full_weights", "and", "self", ".", "zero_stage_3", ":", "path", "=", "self", ".", "broadcast", "(", "path", ")", "return", "super", "(", ")", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "strict", "=", "strict", ")", "if", "not", "state", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", "STRING", ")", "_validate_checkpoint_directory", "(", "path", ")", "engines", "=", "_get_deepspeed_engines_from_state", "(", "state", ")", "if", "len", "(", "engines", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "if", "len", "(", "engines", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "engine", "=", "engines", "[", "0", "]", "from", "deepspeed", ".", "runtime", ".", "base_optimizer", "import", "DeepSpeedOptimizer", "optimzer_state_requested", "=", "any", "(", "isinstance", "(", "item", ",", "(", "Optimizer", ",", "DeepSpeedOptimizer", ")", ")", "for", "item", "in", "state", ".", "values", "(", ")", ")", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "_", ",", "client_state", "=", "engine", ".", "load_checkpoint", "(", "path", ",", "tag", "=", "STRING", ",", "load_optimizer_states", "=", "optimzer_state_requested", ",", "load_lr_scheduler_states", "=", "False", ",", "load_module_strict", "=", "strict", ",", ")", "if", "client_state", "is", "None", ":", "raise", "RuntimeError", "(", "STRING", "STRING", ")", "keys", "=", "set", "(", "client_state", ")", "&", "set", "(", "state", ")", "-", "{", "STRING", ",", "STRING", "}", "_move_state_into", "(", "source", "=", "client_state", ",", "destination", "=", "state", ",", "keys", "=", "keys", ")", "return", "client_state"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 470, "end_line": 548, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_189", "original_string": "def _initialize_engine(\r\n        self, model: Module, optimizer: Optional[Optimizer] = None, scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", Optimizer, Any]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, deepspeed_scheduler = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer, deepspeed_scheduler", "language": "python", "code": "def _initialize_engine(\r\n        self, model: Module, optimizer: Optional[Optimizer] = None, scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", Optimizer, Any]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, deepspeed_scheduler = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer, deepspeed_scheduler", "code_tokens": ["def", "_initialize_engine", "(", "self", ",", "model", ":", "Module", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", "=", "None", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "tuple", "[", "STRING", ",", "Optimizer", ",", "Any", "]", ":", "STRING", "import", "deepspeed", "model_parameters", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "_", ",", "deepspeed_scheduler", "=", "deepspeed", ".", "initialize", "(", "args", "=", "argparse", ".", "Namespace", "(", "device_rank", "=", "self", ".", "root_device", ".", "index", ")", ",", "config", "=", "self", ".", "config", ",", "model", "=", "model", ",", "model_parameters", "=", "model_parameters", ",", "optimizer", "=", "optimizer", ",", "lr_scheduler", "=", "scheduler", ",", "dist_init_required", "=", "False", ",", ")", "return", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "deepspeed_scheduler"], "docstring": "Initialize one model and one optimizer with an optional learning rate scheduler.", "docstring_tokens": ["initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 614, "end_line": 634, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_190", "original_string": "def _restore_zero_state(self, module: Module, ckpt: Mapping[str, Any]) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=True,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(module, prefix=\"\")", "language": "python", "code": "def _restore_zero_state(self, module: Module, ckpt: Mapping[str, Any]) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=True,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(module, prefix=\"\")", "code_tokens": ["def", "_restore_zero_state", "(", "self", ",", "module", ":", "Module", ",", "ckpt", ":", "Mapping", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "import", "deepspeed", "def", "load", "(", "module", ":", "torch", ".", "nn", ".", "Module", ",", "prefix", ":", "str", "=", "STRING", ")", "-", ">", "None", ":", "missing_keys", ":", "list", "[", "str", "]", "=", "[", "]", "unexpected_keys", ":", "list", "[", "str", "]", "=", "[", "]", "error_msgs", ":", "list", "[", "str", "]", "=", "[", "]", "state_dict", "=", "ckpt", "[", "STRING", "]", "metadata", "=", "getattr", "(", "state_dict", ",", "STRING", ",", "None", ")", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "if", "metadata", "is", "not", "None", ":", "state_dict", ".", "_metadata", "=", "metadata", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "with", "deepspeed", ".", "zero", ".", "GatheredParameters", "(", "list", "(", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ")", ",", "modifier_rank", "=", "0", ")", ":", "if", "self", ".", "is_global_zero", ":", "module", ".", "_load_from_state_dict", "(", "state_dict", "=", "state_dict", ",", "prefix", "=", "prefix", ",", "local_metadata", "=", "local_metadata", ",", "strict", "=", "True", ",", "missing_keys", "=", "missing_keys", ",", "unexpected_keys", "=", "unexpected_keys", ",", "error_msgs", "=", "error_msgs", ",", ")", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "if", "child", "is", "not", "None", ":", "load", "(", "child", ",", "prefix", "+", "name", "+", "STRING", ")", "load", "(", "module", ",", "prefix", "=", "STRING", ")"], "docstring": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded", "docstring_tokens": ["overrides", "the", "normal", "load_state_dict", "behaviour", "in", "pytorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 781, "end_line": 824, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_191", "original_string": "def _is_deepspeed_checkpoint(path: Path) -> bool:\r\n    \"\"\"Heuristic check whether the path points to a top-level DeepSpeed checkpoint directory.\"\"\"\r\n    return path.is_dir() and (path / \"checkpoint\").is_dir()", "language": "python", "code": "def _is_deepspeed_checkpoint(path: Path) -> bool:\r\n    \"\"\"Heuristic check whether the path points to a top-level DeepSpeed checkpoint directory.\"\"\"\r\n    return path.is_dir() and (path / \"checkpoint\").is_dir()", "code_tokens": ["def", "_is_deepspeed_checkpoint", "(", "path", ":", "Path", ")", "-", ">", "bool", ":", "STRING", "return", "path", ".", "is_dir", "(", ")", "and", "(", "path", "/", "STRING", ")", ".", "is_dir", "(", ")"], "docstring": "Heuristic check whether the path points to a top-level DeepSpeed checkpoint directory.", "docstring_tokens": ["heuristic", "check", "whether", "the", "path", "points", "to", "a", "top", "level", "deepspeed", "checkpoint", "directory"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 886, "end_line": 888, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "function_192", "original_string": "def _validate_checkpoint_directory(path: _PATH) -> None:\r\n    \"\"\"Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.\"\"\"\r\n\r\n    path = Path(path)\r\n    path_is_ds_checkpoint = _is_deepspeed_checkpoint(path)\r\n    default_message = f\"The provided path is not a valid DeepSpeed checkpoint: {path}\"\r\n\r\n    if not path_is_ds_checkpoint:\r\n        parent_is_ds_checkpoint = _is_deepspeed_checkpoint(path.parent)\r\n        if parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a subfolder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent}\"\r\n            )\r\n        parent_parent_is_ds_checkpoint = path.is_file() and _is_deepspeed_checkpoint(path.parent.parent)\r\n        if parent_parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a file inside a DeepSpeed checkpoint folder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent.parent}\"\r\n            )\r\n        raise FileNotFoundError(default_message)", "language": "python", "code": "def _validate_checkpoint_directory(path: _PATH) -> None:\r\n    \"\"\"Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.\"\"\"\r\n\r\n    path = Path(path)\r\n    path_is_ds_checkpoint = _is_deepspeed_checkpoint(path)\r\n    default_message = f\"The provided path is not a valid DeepSpeed checkpoint: {path}\"\r\n\r\n    if not path_is_ds_checkpoint:\r\n        parent_is_ds_checkpoint = _is_deepspeed_checkpoint(path.parent)\r\n        if parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a subfolder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent}\"\r\n            )\r\n        parent_parent_is_ds_checkpoint = path.is_file() and _is_deepspeed_checkpoint(path.parent.parent)\r\n        if parent_parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a file inside a DeepSpeed checkpoint folder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent.parent}\"\r\n            )\r\n        raise FileNotFoundError(default_message)", "code_tokens": ["def", "_validate_checkpoint_directory", "(", "path", ":", "_PATH", ")", "-", ">", "None", ":", "STRING", "path", "=", "Path", "(", "path", ")", "path_is_ds_checkpoint", "=", "_is_deepspeed_checkpoint", "(", "path", ")", "default_message", "=", "fSTRING", "if", "not", "path_is_ds_checkpoint", ":", "parent_is_ds_checkpoint", "=", "_is_deepspeed_checkpoint", "(", "path", ".", "parent", ")", "if", "parent_is_ds_checkpoint", ":", "raise", "FileNotFoundError", "(", "fSTRING", "fSTRING", ")", "parent_parent_is_ds_checkpoint", "=", "path", ".", "is_file", "(", ")", "and", "_is_deepspeed_checkpoint", "(", "path", ".", "parent", ".", "parent", ")", "if", "parent_parent_is_ds_checkpoint", ":", "raise", "FileNotFoundError", "(", "fSTRING", "fSTRING", ")", "raise", "FileNotFoundError", "(", "default_message", ")"], "docstring": "Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.", "docstring_tokens": ["validates", "that", "the", "path", "points", "to", "a", "deepspeed", "checkpoint", "directory", "and", "suggests", "fixes", "for", "user", "error"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "start_line": 891, "end_line": 923, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\dp.py", "func_name": "function_193", "original_string": "def setup_module(self, module: Module) -> DataParallel:\r\n        \"\"\"Wraps the given model into a :class:`~torch.nn.DataParallel` module.\"\"\"\r\n        return DataParallel(module=module, device_ids=self.parallel_devices)", "language": "python", "code": "def setup_module(self, module: Module) -> DataParallel:\r\n        \"\"\"Wraps the given model into a :class:`~torch.nn.DataParallel` module.\"\"\"\r\n        return DataParallel(module=module, device_ids=self.parallel_devices)", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "DataParallel", ":", "STRING", "return", "DataParallel", "(", "module", "=", "module", ",", "device_ids", "=", "self", ".", "parallel_devices", ")"], "docstring": "Wraps the given model into a :class:`~torch.nn.DataParallel` module.", "docstring_tokens": ["wraps", "the", "given", "model", "into", "a", "class", "torch", "nn", "dataparallel", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\dp.py", "start_line": 61, "end_line": 63, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_194", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module and sets `use_orig_params=True` to keep the reference to the original parameters in the optimizer.\"\"\"\r\n        use_orig_params = self._fsdp_kwargs.get(\"use_orig_params\")\r\n        if use_orig_params is False:\r\n            raise ValueError(\r\n                f\"You set `{type(self).__name__}(use_orig_params=False)` but this is not supported when\"\r\n                \" setting the model and optimizer up jointly. Either set it to `True` or set the objects\"\r\n                \" up in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n                \" call `setup_optimizer`.\"\r\n            )\r\n        module = self.setup_module(module)\r\n        return module, optimizers, scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module and sets `use_orig_params=True` to keep the reference to the original parameters in the optimizer.\"\"\"\r\n        use_orig_params = self._fsdp_kwargs.get(\"use_orig_params\")\r\n        if use_orig_params is False:\r\n            raise ValueError(\r\n                f\"You set `{type(self).__name__}(use_orig_params=False)` but this is not supported when\"\r\n                \" setting the model and optimizer up jointly. Either set it to `True` or set the objects\"\r\n                \" up in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n                \" call `setup_optimizer`.\"\r\n            )\r\n        module = self.setup_module(module)\r\n        return module, optimizers, scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "STRING", "]", "]", ":", "STRING", "use_orig_params", "=", "self", ".", "_fsdp_kwargs", ".", "get", "(", "STRING", ")", "if", "use_orig_params", "is", "False", ":", "raise", "ValueError", "(", "fSTRING", "STRING", "STRING", "STRING", ")", "module", "=", "self", ".", "setup_module", "(", "module", ")", "return", "module", ",", "optimizers", ",", "scheduler"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 263, "end_line": 277, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_195", "original_string": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in module.modules()):\r\n            if _has_meta_device_parameters_or_buffers(module):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self._fsdp_kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self._fsdp_kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            module = FullyShardedDataParallel(\r\n                module=module,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self._fsdp_kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(module, self.root_device)\r\n\r\n        _setup_activation_checkpointing(module, self._activation_checkpointing_kwargs)\r\n\r\n        return module", "language": "python", "code": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in module.modules()):\r\n            if _has_meta_device_parameters_or_buffers(module):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self._fsdp_kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self._fsdp_kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            module = FullyShardedDataParallel(\r\n                module=module,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self._fsdp_kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(module, self.root_device)\r\n\r\n        _setup_activation_checkpointing(module, self._activation_checkpointing_kwargs)\r\n\r\n        return module", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "if", "any", "(", "isinstance", "(", "mod", ",", "FullyShardedDataParallel", ")", "for", "mod", "in", "module", ".", "modules", "(", ")", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "module", ")", ":", "rank_zero_warn", "(", "STRING", ")", "if", "STRING", "in", "self", ".", "_fsdp_kwargs", ":", "rank_zero_warn", "(", "STRING", ")", "del", "self", ".", "_fsdp_kwargs", "[", "STRING", "]", "else", ":", "module", "=", "FullyShardedDataParallel", "(", "module", "=", "module", ",", "cpu_offload", "=", "self", ".", "cpu_offload", ",", "mixed_precision", "=", "self", ".", "mixed_precision_config", ",", "sharding_strategy", "=", "self", ".", "sharding_strategy", ",", "device_id", "=", "self", ".", "root_device", ".", "index", ",", "*", "*", "self", ".", "_fsdp_kwargs", ",", ")", "_move_torchmetrics_to_device", "(", "module", ",", "self", ".", "root_device", ")", "_setup_activation_checkpointing", "(", "module", ",", "self", ".", "_activation_checkpointing_kwargs", ")", "return", "module"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 280, "end_line": 311, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_196", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with FSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if self._fsdp_kwargs.get(\"use_orig_params\"):\r\n            return super().setup_optimizer(optimizer)\r\n        if not _optimizer_has_flat_params(optimizer):\r\n            raise ValueError(\r\n                \"The optimizer does not seem to reference any FSDP parameters. HINT: Make sure to create the optimizer\"\r\n                \" after setting up the model.\"\r\n            )\r\n        return optimizer", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with FSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if self._fsdp_kwargs.get(\"use_orig_params\"):\r\n            return super().setup_optimizer(optimizer)\r\n        if not _optimizer_has_flat_params(optimizer):\r\n            raise ValueError(\r\n                \"The optimizer does not seem to reference any FSDP parameters. HINT: Make sure to create the optimizer\"\r\n                \" after setting up the model.\"\r\n            )\r\n        return optimizer", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "STRING", "if", "self", ".", "_fsdp_kwargs", ".", "get", "(", "STRING", ")", ":", "return", "super", "(", ")", ".", "setup_optimizer", "(", "optimizer", ")", "if", "not", "_optimizer_has_flat_params", "(", "optimizer", ")", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "return", "optimizer"], "docstring": "Set up an optimizer for a model wrapped with FSDP.", "docstring_tokens": ["set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "fsdp"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 314, "end_line": 330, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_197", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            raise TypeError(\r\n                \"Gradient clipping with FSDP is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.clip_gradients_norm` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        self.precision.unscale_gradients(optimizer)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            raise TypeError(\r\n                \"Gradient clipping with FSDP is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.clip_gradients_norm` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        self.precision.unscale_gradients(optimizer)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Tensor", ":", "STRING", "from", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", "import", "FullyShardedDataParallel", "if", "not", "isinstance", "(", "module", ",", "FullyShardedDataParallel", ")", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "fSTRING", ")", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "return", "module", ".", "clip_grad_norm_", "(", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 391, "end_line": 410, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_198", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If the state-dict-type is ``'full'``, the checkpoint will be written to a single file containing the weights,\r\n        optimizer state and other metadata. If the state-dict-type is ``'sharded'``, the checkpoint gets saved as a\r\n        directory containing one file per process, with model- and optimizer shards stored per file. Additionally, it\r\n        creates a metadata file `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`FSDPStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `FSDPStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._state_dict_type == \"sharded\":\r\n            raise NotImplementedError(\r\n                \"FSDP doesn't support loading sharded filtered checkpoints, so saving them is disabled.\"\r\n            )\r\n\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and self._state_dict_type == \"full\" and not _is_sharded_checkpoint(path):\r\n            raise IsADirectoryError(f\"The checkpoint path exists and is a directory: {path}\")\r\n\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n        modules = [module for module in state.values() if _has_fsdp_modules(module)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple FSDP models in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        module = modules[0]\r\n\r\n        if self._state_dict_type == \"sharded\":\r\n            if path.is_file():\r\n                path.unlink()\r\n            path.mkdir(parents=True, exist_ok=True)\r\n\r\n            state_dict_ctx = _get_sharded_state_dict_context(module)\r\n\r\n            converted_state: dict[str, Any] = {}\r\n            metadata: dict[str, Any] = {}\r\n            with state_dict_ctx:\r\n                for key, obj in state.items():\r\n                    converted: Any\r\n                    if isinstance(obj, Module):\r\n                        converted = obj.state_dict()\r\n                        target_dict = converted_state\r\n                    elif isinstance(obj, Optimizer):\r\n                        converted = FSDP.optim_state_dict(module, obj)\r\n                        target_dict = converted_state\r\n                    else:  # everything not a module or optimizer is considered metadata\r\n                        converted = obj.state_dict() if isinstance(obj, _Stateful) else obj\r\n                        target_dict = metadata\r\n                    _apply_filter(key, filter or {}, converted, target_dict)\r\n\r\n            _distributed_checkpoint_save(converted_state, path)\r\n\r\n            if self.global_rank == 0:\r\n                torch.save(metadata, path / _METADATA_FILENAME)\r\n\r\n        elif self._state_dict_type == \"full\":\r\n            if _is_sharded_checkpoint(path):\r\n                shutil.rmtree(path)\r\n\r\n            state_dict_ctx = _get_full_state_dict_context(module, world_size=self.world_size)\r\n            full_state: dict[str, Any] = {}\r\n            with state_dict_ctx:\r\n                for key, obj in state.items():\r\n                    if isinstance(obj, Module):\r\n                        converted = obj.state_dict()\r\n                    elif isinstance(obj, Optimizer):\r\n                        converted = FSDP.optim_state_dict(module, obj)\r\n                    else:  # everything not a module or optimizer is considered metadata\r\n                        converted = obj.state_dict() if isinstance(obj, _Stateful) else obj\r\n                    _apply_filter(key, filter or {}, converted, full_state)\r\n\r\n            if self.global_rank == 0:\r\n                torch.save(full_state, path)\r\n        else:\r\n            raise ValueError(f\"Unknown state_dict_type: {self._state_dict_type}\")", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If the state-dict-type is ``'full'``, the checkpoint will be written to a single file containing the weights,\r\n        optimizer state and other metadata. If the state-dict-type is ``'sharded'``, the checkpoint gets saved as a\r\n        directory containing one file per process, with model- and optimizer shards stored per file. Additionally, it\r\n        creates a metadata file `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`FSDPStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `FSDPStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._state_dict_type == \"sharded\":\r\n            raise NotImplementedError(\r\n                \"FSDP doesn't support loading sharded filtered checkpoints, so saving them is disabled.\"\r\n            )\r\n\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and self._state_dict_type == \"full\" and not _is_sharded_checkpoint(path):\r\n            raise IsADirectoryError(f\"The checkpoint path exists and is a directory: {path}\")\r\n\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n        modules = [module for module in state.values() if _has_fsdp_modules(module)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple FSDP models in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        module = modules[0]\r\n\r\n        if self._state_dict_type == \"sharded\":\r\n            if path.is_file():\r\n                path.unlink()\r\n            path.mkdir(parents=True, exist_ok=True)\r\n\r\n            state_dict_ctx = _get_sharded_state_dict_context(module)\r\n\r\n            converted_state: dict[str, Any] = {}\r\n            metadata: dict[str, Any] = {}\r\n            with state_dict_ctx:\r\n                for key, obj in state.items():\r\n                    converted: Any\r\n                    if isinstance(obj, Module):\r\n                        converted = obj.state_dict()\r\n                        target_dict = converted_state\r\n                    elif isinstance(obj, Optimizer):\r\n                        converted = FSDP.optim_state_dict(module, obj)\r\n                        target_dict = converted_state\r\n                    else:  # everything not a module or optimizer is considered metadata\r\n                        converted = obj.state_dict() if isinstance(obj, _Stateful) else obj\r\n                        target_dict = metadata\r\n                    _apply_filter(key, filter or {}, converted, target_dict)\r\n\r\n            _distributed_checkpoint_save(converted_state, path)\r\n\r\n            if self.global_rank == 0:\r\n                torch.save(metadata, path / _METADATA_FILENAME)\r\n\r\n        elif self._state_dict_type == \"full\":\r\n            if _is_sharded_checkpoint(path):\r\n                shutil.rmtree(path)\r\n\r\n            state_dict_ctx = _get_full_state_dict_context(module, world_size=self.world_size)\r\n            full_state: dict[str, Any] = {}\r\n            with state_dict_ctx:\r\n                for key, obj in state.items():\r\n                    if isinstance(obj, Module):\r\n                        converted = obj.state_dict()\r\n                    elif isinstance(obj, Optimizer):\r\n                        converted = FSDP.optim_state_dict(module, obj)\r\n                    else:  # everything not a module or optimizer is considered metadata\r\n                        converted = obj.state_dict() if isinstance(obj, _Stateful) else obj\r\n                    _apply_filter(key, filter or {}, converted, full_state)\r\n\r\n            if self.global_rank == 0:\r\n                torch.save(full_state, path)\r\n        else:\r\n            raise ValueError(f\"Unknown state_dict_type: {self._state_dict_type}\")", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "STRING", ")", "if", "filter", "is", "not", "None", "and", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "raise", "NotImplementedError", "(", "STRING", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "path", ".", "is_dir", "(", ")", "and", "self", ".", "_state_dict_type", "=", "=", "STRING", "and", "not", "_is_sharded_checkpoint", "(", "path", ")", ":", "raise", "IsADirectoryError", "(", "fSTRING", ")", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "modules", "=", "[", "module", "for", "module", "in", "state", ".", "values", "(", ")", "if", "_has_fsdp_modules", "(", "module", ")", "]", "if", "len", "(", "modules", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "if", "len", "(", "modules", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "module", "=", "modules", "[", "0", "]", "if", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "if", "path", ".", "is_file", "(", ")", ":", "path", ".", "unlink", "(", ")", "path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "state_dict_ctx", "=", "_get_sharded_state_dict_context", "(", "module", ")", "converted_state", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "metadata", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "with", "state_dict_ctx", ":", "for", "key", ",", "obj", "in", "state", ".", "items", "(", ")", ":", "converted", ":", "Any", "if", "isinstance", "(", "obj", ",", "Module", ")", ":", "converted", "=", "obj", ".", "state_dict", "(", ")", "target_dict", "=", "converted_state", "elif", "isinstance", "(", "obj", ",", "Optimizer", ")", ":", "converted", "=", "FSDP", ".", "optim_state_dict", "(", "module", ",", "obj", ")", "target_dict", "=", "converted_state", "else", ":", "#", "everything", "not", "a", "module", "or", "optimizer", "is", "considered", "metadata", "converted", "=", "obj", ".", "state_dict", "(", ")", "if", "isinstance", "(", "obj", ",", "_Stateful", ")", "else", "obj", "target_dict", "=", "metadata", "_apply_filter", "(", "key", ",", "filter", "or", "{", "}", ",", "converted", ",", "target_dict", ")", "_distributed_checkpoint_save", "(", "converted_state", ",", "path", ")", "if", "self", ".", "global_rank", "=", "=", "0", ":", "torch", ".", "save", "(", "metadata", ",", "path", "/", "_METADATA_FILENAME", ")", "elif", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "if", "_is_sharded_checkpoint", "(", "path", ")", ":", "shutil", ".", "rmtree", "(", "path", ")", "state_dict_ctx", "=", "_get_full_state_dict_context", "(", "module", ",", "world_size", "=", "self", ".", "world_size", ")", "full_state", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "with", "state_dict_ctx", ":", "for", "key", ",", "obj", "in", "state", ".", "items", "(", ")", ":", "if", "isinstance", "(", "obj", ",", "Module", ")", ":", "converted", "=", "obj", ".", "state_dict", "(", ")", "elif", "isinstance", "(", "obj", ",", "Optimizer", ")", ":", "converted", "=", "FSDP", ".", "optim_state_dict", "(", "module", ",", "obj", ")", "else", ":", "#", "everything", "not", "a", "module", "or", "optimizer", "is", "considered", "metadata", "converted", "=", "obj", ".", "state_dict", "(", ")", "if", "isinstance", "(", "obj", ",", "_Stateful", ")", "else", "obj", "_apply_filter", "(", "key", ",", "filter", "or", "{", "}", ",", "converted", ",", "full_state", ")", "if", "self", "."], "docstring": "Save model, optimizer, and other state to a checkpoint on disk.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "to", "a", "checkpoint", "on", "disk"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 413, "end_line": 510, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_199", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got FSDPStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" FSDPStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            from lightning.fabric.strategies.model_parallel import _load_raw_module_state_from_path\r\n\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                \"Loading a single optimizer object from a checkpoint is not supported yet with the FSDP strategy.\"\r\n            )\r\n\r\n        from torch.distributed.checkpoint.optimizer import load_sharded_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n        modules = {key: module for key, module in state.items() if _has_fsdp_modules(module)}\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple FSDP models in the given state. Loading checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To load multiple models, call the\"\r\n                \" load method for each model separately with a different path.\"\r\n            )\r\n        module_key, module = list(modules.items())[0]\r\n\r\n        if _is_sharded_checkpoint(path):\r\n            state_dict_ctx = _get_sharded_state_dict_context(module)\r\n\r\n            with state_dict_ctx:\r\n                module_state = {module_key: module.state_dict()}\r\n                _distributed_checkpoint_load(module_state, path)\r\n                module.load_state_dict(module_state[module_key], strict=strict)\r\n\r\n                if optimizers:\r\n                    from torch.distributed.checkpoint import FileSystemReader\r\n\r\n                    reader = FileSystemReader(path=path)\r\n                    for optim_key, optim in optimizers.items():\r\n                        optim_state = load_sharded_optimizer_state_dict(\r\n                            model_state_dict=module_state[module_key],\r\n                            optimizer_key=optim_key,\r\n                            storage_reader=reader,\r\n                        )\r\n                        flattened_osd = FSDP.optim_state_dict_to_load(\r\n                            optim_state_dict=optim_state[optim_key],\r\n                            model=module,\r\n                            optim=optim,\r\n                        )\r\n                        optim.load_state_dict(flattened_osd)\r\n\r\n            metadata = torch.load(path / _METADATA_FILENAME)\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, metadata.keys(), strict=strict)\r\n            for key in requested_metadata_keys:\r\n                if key not in metadata:\r\n                    continue\r\n                state[key] = metadata.pop(key)\r\n\r\n            return metadata\r\n\r\n        if _is_full_checkpoint(path):\r\n            checkpoint = _lazy_load(path)\r\n\r\n            from lightning.fabric.strategies.model_parallel import (\r\n                _load_raw_module_state,\r\n                _rekey_optimizer_state_if_needed,\r\n            )\r\n\r\n            _load_raw_module_state(checkpoint.pop(module_key), module=module, world_size=self.world_size, strict=strict)\r\n\r\n            if isinstance(state, Module):\r\n                return {}\r\n\r\n            checkpoint = _materialize_tensors(checkpoint)\r\n\r\n            for optim_key, optim in optimizers.items():\r\n                with _get_full_state_dict_context(module, world_size=self.world_size, rank0_only=False):\r\n                    temp_state_dict = _rekey_optimizer_state_if_needed(checkpoint.pop(optim_key), module)\r\n                    optim_state_dict = FSDP.optim_state_dict_to_load(\r\n                        optim_state_dict=temp_state_dict,\r\n                        model=module,\r\n                        optim=optim,\r\n                    )\r\n                    optim.load_state_dict(optim_state_dict)\r\n\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, checkpoint.keys(), strict=strict)\r\n\r\n            _move_state_into(source=checkpoint, destination=state, keys=requested_metadata_keys)\r\n\r\n            return checkpoint\r\n\r\n        raise ValueError(\r\n            f\"The path {str(path)!r} does not point to a valid checkpoint. Make sure the path points to either a\"\r\n            \" directory with FSDP checkpoint shards, or a single file with a full checkpoint.\"\r\n        )", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got FSDPStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" FSDPStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            from lightning.fabric.strategies.model_parallel import _load_raw_module_state_from_path\r\n\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                \"Loading a single optimizer object from a checkpoint is not supported yet with the FSDP strategy.\"\r\n            )\r\n\r\n        from torch.distributed.checkpoint.optimizer import load_sharded_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n        modules = {key: module for key, module in state.items() if _has_fsdp_modules(module)}\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple FSDP models in the given state. Loading checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To load multiple models, call the\"\r\n                \" load method for each model separately with a different path.\"\r\n            )\r\n        module_key, module = list(modules.items())[0]\r\n\r\n        if _is_sharded_checkpoint(path):\r\n            state_dict_ctx = _get_sharded_state_dict_context(module)\r\n\r\n            with state_dict_ctx:\r\n                module_state = {module_key: module.state_dict()}\r\n                _distributed_checkpoint_load(module_state, path)\r\n                module.load_state_dict(module_state[module_key], strict=strict)\r\n\r\n                if optimizers:\r\n                    from torch.distributed.checkpoint import FileSystemReader\r\n\r\n                    reader = FileSystemReader(path=path)\r\n                    for optim_key, optim in optimizers.items():\r\n                        optim_state = load_sharded_optimizer_state_dict(\r\n                            model_state_dict=module_state[module_key],\r\n                            optimizer_key=optim_key,\r\n                            storage_reader=reader,\r\n                        )\r\n                        flattened_osd = FSDP.optim_state_dict_to_load(\r\n                            optim_state_dict=optim_state[optim_key],\r\n                            model=module,\r\n                            optim=optim,\r\n                        )\r\n                        optim.load_state_dict(flattened_osd)\r\n\r\n            metadata = torch.load(path / _METADATA_FILENAME)\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, metadata.keys(), strict=strict)\r\n            for key in requested_metadata_keys:\r\n                if key not in metadata:\r\n                    continue\r\n                state[key] = metadata.pop(key)\r\n\r\n            return metadata\r\n\r\n        if _is_full_checkpoint(path):\r\n            checkpoint = _lazy_load(path)\r\n\r\n            from lightning.fabric.strategies.model_parallel import (\r\n                _load_raw_module_state,\r\n                _rekey_optimizer_state_if_needed,\r\n            )\r\n\r\n            _load_raw_module_state(checkpoint.pop(module_key), module=module, world_size=self.world_size, strict=strict)\r\n\r\n            if isinstance(state, Module):\r\n                return {}\r\n\r\n            checkpoint = _materialize_tensors(checkpoint)\r\n\r\n            for optim_key, optim in optimizers.items():\r\n                with _get_full_state_dict_context(module, world_size=self.world_size, rank0_only=False):\r\n                    temp_state_dict = _rekey_optimizer_state_if_needed(checkpoint.pop(optim_key), module)\r\n                    optim_state_dict = FSDP.optim_state_dict_to_load(\r\n                        optim_state_dict=temp_state_dict,\r\n                        model=module,\r\n                        optim=optim,\r\n                    )\r\n                    optim.load_state_dict(optim_state_dict)\r\n\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, checkpoint.keys(), strict=strict)\r\n\r\n            _move_state_into(source=checkpoint, destination=state, keys=requested_metadata_keys)\r\n\r\n            return checkpoint\r\n\r\n        raise ValueError(\r\n            f\"The path {str(path)!r} does not point to a valid checkpoint. Make sure the path points to either a\"\r\n            \" directory with FSDP checkpoint shards, or a single file with a full checkpoint.\"\r\n        )", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "state", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", "STRING", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "from", "lightning", ".", "fabric", ".", "strategies", ".", "model_parallel", "import", "_load_raw_module_state_from_path", "_load_raw_module_state_from_path", "(", "path", ",", "module", "=", "state", ",", "world_size", "=", "self", ".", "world_size", ",", "strict", "=", "strict", ")", "return", "{", "}", "if", "isinstance", "(", "state", ",", "Optimizer", ")", ":", "raise", "NotImplementedError", "(", "STRING", ")", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "optimizer", "import", "load_sharded_optimizer_state_dict", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "modules", "=", "{", "key", ":", "module", "for", "key", ",", "module", "in", "state", ".", "items", "(", ")", "if", "_has_fsdp_modules", "(", "module", ")", "}", "if", "len", "(", "modules", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "optimizers", "=", "{", "key", ":", "optim", "for", "key", ",", "optim", "in", "state", ".", "items", "(", ")", "if", "isinstance", "(", "optim", ",", "Optimizer", ")", "}", "if", "len", "(", "modules", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "module_key", ",", "module", "=", "list", "(", "modules", ".", "items", "(", ")", ")", "[", "0", "]", "if", "_is_sharded_checkpoint", "(", "path", ")", ":", "state_dict_ctx", "=", "_get_sharded_state_dict_context", "(", "module", ")", "with", "state_dict_ctx", ":", "module_state", "=", "{", "module_key", ":", "module", ".", "state_dict", "(", ")", "}", "_distributed_checkpoint_load", "(", "module_state", ",", "path", ")", "module", ".", "load_state_dict", "(", "module_state", "[", "module_key", "]", ",", "strict", "=", "strict", ")", "if", "optimizers", ":", "from", "torch", ".", "distributed", ".", "checkpoint", "import", "FileSystemReader", "reader", "=", "FileSystemReader", "(", "path", "=", "path", ")", "for", "optim_key", ",", "optim", "in", "optimizers", ".", "items", "(", ")", ":", "optim_state", "=", "load_sharded_optimizer_state_dict", "(", "model_state_dict", "=", "module_state", "[", "module_key", "]", ",", "optimizer_key", "=", "optim_key", ",", "storage_reader", "=", "reader", ",", ")", "flattened_osd", "=", "FSDP", ".", "optim_state_dict_to_load", "(", "optim_state_dict", "=", "optim_state", "[", "optim_key", "]", ",", "model", "=", "module", ",", "optim", "=", "optim", ",", ")", "optim", ".", "load_state_dict", "(", "flattened_osd", ")", "metadata", "=", "torch", ".", "load", "(", "path", "/", "_METADATA_FILENAME", ")", "requested_metadata_keys", "=", "state", ".", "keys", "(", ")", "-", "modules", ".", "keys", "(", ")", "-", "optimizers", ".", "keys", "(", ")", "_validate_keys_for_strict_loading", "(", "requested_metadata_keys", ",", "metadata", ".", "keys", "(", ")", ",", "strict", "=", "strict", ")", "for", "key", "in", "requested_metadata_keys", ":", "if", "key", "not", "in", "metadata", ":", "continue", "state", "[", "key", "]", "=", "metadata", ".", "pop", "(", "key", ")", "return", "metadata", "if", "_is_full_checkpoint", "(", "path", ")", ":", "checkpoint", "=", "_lazy_load", "(", "path", ")", "from", "lightning", ".", "fabric", ".", "strategies", ".", "model_parallel", "import", "(", "_load_raw_module_state", ",", "_rekey_optimizer_state_if_needed", ",", ")", "_load_raw_module_state", "(", "checkpoint", ".", "pop", "(", "module_key", ")", ",", "module", "=", "module", ",", "world_size", "=", "self", ".", "world_size", ",", "strict", "=", "strict", ")", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "return", "{", "}", "checkpoint", "=", "_materialize_tensors", "(", "checkpoint", ")", "for", "optim_key", ",", "optim", "in", "optimizers", ".", "items", "(", ")", ":", "with", "_get_full_state_dict_context", "("], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 513, "end_line": 640, "has_examples": false, "num_comments": 10, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_200", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.no_backward_sync` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.no_backward_sync` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "STRING", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "from", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", "import", "FullyShardedDataParallel", "if", "not", "isinstance", "(", "module", ",", "FullyShardedDataParallel", ")", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "fSTRING", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch", "distributed", "fsdp", "fullyshardeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 745, "end_line": 759, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "function_201", "original_string": "def _is_sharded_checkpoint(path: Path) -> bool:\r\n    \"\"\"A heuristic check to determine whether the path points to a directory with checkpoint shards.\"\"\"\r\n    return path.is_dir() and (path / _METADATA_FILENAME).is_file()", "language": "python", "code": "def _is_sharded_checkpoint(path: Path) -> bool:\r\n    \"\"\"A heuristic check to determine whether the path points to a directory with checkpoint shards.\"\"\"\r\n    return path.is_dir() and (path / _METADATA_FILENAME).is_file()", "code_tokens": ["def", "_is_sharded_checkpoint", "(", "path", ":", "Path", ")", "-", ">", "bool", ":", "STRING", "return", "path", ".", "is_dir", "(", ")", "and", "(", "path", "/", "_METADATA_FILENAME", ")", ".", "is_file", "(", ")"], "docstring": "A heuristic check to determine whether the path points to a directory with checkpoint shards.", "docstring_tokens": ["a", "heuristic", "check", "to", "determine", "whether", "the", "path", "points", "to", "a", "directory", "with", "checkpoint", "shards"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\fsdp.py", "start_line": 831, "end_line": 833, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_202", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If distributed checkpointing is enabled (default), the checkpoint gets saved as a directory containing one file\r\n        per process, with model- and optimizer shards stored per file. Additionally, it creates a metadata file\r\n        `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n        If distributed checkpointing is disabled (``save_distributed_checkpoint=False``), the checkpoint will be\r\n        written to a single file containing the weights, optimizer state and other metadata.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                f\"`{type(self).__name__}.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                f\" `{type(self).__name__}` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._save_distributed_checkpoint:\r\n            raise NotImplementedError(\r\n                f\"{type(self).__name__} doesn't support loading distributed filtered checkpoints,\"\r\n                \" so saving them is disabled.\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n        _save_checkpoint(\r\n            path=path,\r\n            state=state,\r\n            full_state_dict=(not self._save_distributed_checkpoint),\r\n            rank=self.global_rank,\r\n            filter=filter,\r\n        )", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If distributed checkpointing is enabled (default), the checkpoint gets saved as a directory containing one file\r\n        per process, with model- and optimizer shards stored per file. Additionally, it creates a metadata file\r\n        `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n        If distributed checkpointing is disabled (``save_distributed_checkpoint=False``), the checkpoint will be\r\n        written to a single file containing the weights, optimizer state and other metadata.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                f\"`{type(self).__name__}.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                f\" `{type(self).__name__}` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._save_distributed_checkpoint:\r\n            raise NotImplementedError(\r\n                f\"{type(self).__name__} doesn't support loading distributed filtered checkpoints,\"\r\n                \" so saving them is disabled.\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n        _save_checkpoint(\r\n            path=path,\r\n            state=state,\r\n            full_state_dict=(not self._save_distributed_checkpoint),\r\n            rank=self.global_rank,\r\n            filter=filter,\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "fSTRING", "fSTRING", ")", "if", "filter", "is", "not", "None", "and", "self", ".", "_save_distributed_checkpoint", ":", "raise", "NotImplementedError", "(", "fSTRING", "STRING", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "_save_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "rank", "=", "self", ".", "global_rank", ",", "filter", "=", "filter", ",", ")"], "docstring": "Save model, optimizer, and other state to a checkpoint on disk.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "to", "a", "checkpoint", "on", "disk"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 234, "end_line": 269, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_203", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got {type(self).__name__}.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                f\" {type(self).__name__}.load_checkpoint(..., state={{'model': model, ...}})\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                f\"Loading a single optimizer object from a checkpoint is not supported yet with {type(self).__name__}.\"\r\n            )\r\n\r\n        return _load_checkpoint(path=path, state=state, strict=strict)", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got {type(self).__name__}.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                f\" {type(self).__name__}.load_checkpoint(..., state={{'model': model, ...}})\"\r\n            )\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                f\"Loading a single optimizer object from a checkpoint is not supported yet with {type(self).__name__}.\"\r\n            )\r\n\r\n        return _load_checkpoint(path=path, state=state, strict=strict)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "state", ":", "raise", "ValueError", "(", "fSTRING", "STRING", "fSTRING", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "_load_raw_module_state_from_path", "(", "path", ",", "module", "=", "state", ",", "world_size", "=", "self", ".", "world_size", ",", "strict", "=", "strict", ")", "return", "{", "}", "if", "isinstance", "(", "state", ",", "Optimizer", ")", ":", "raise", "NotImplementedError", "(", "fSTRING", ")", "return", "_load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "strict", "=", "strict", ")"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 272, "end_line": 297, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_204", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the FSDP2 modules.\"\"\"\r\n        return _FSDPNoSync(module=module, enabled=enabled)", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the FSDP2 modules.\"\"\"\r\n        return _FSDPNoSync(module=module, enabled=enabled)", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "STRING", "return", "_FSDPNoSync", "(", "module", "=", "module", ",", "enabled", "=", "enabled", ")"], "docstring": "Blocks gradient synchronization inside the FSDP2 modules.", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "fsdp2", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 323, "end_line": 325, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_205", "original_string": "def _load_raw_module_state_from_path(path: Path, module: Module, world_size: int, strict: bool = True) -> None:\r\n    \"\"\"Loads the state dict from a file path into the FSDP module.\"\"\"\r\n    if not _is_full_checkpoint(path):\r\n        raise ValueError(\r\n            \"Failed to load checkpoint directly into the model. The given path must be a single file containing the\"\r\n            f\" full state dict: {path}\"\r\n        )\r\n    state_dict = torch.load(path, mmap=True, map_location=\"cpu\") if _TORCH_GREATER_EQUAL_2_3 else _lazy_load(path)\r\n    _load_raw_module_state(state_dict=state_dict, module=module, world_size=world_size, strict=strict)", "language": "python", "code": "def _load_raw_module_state_from_path(path: Path, module: Module, world_size: int, strict: bool = True) -> None:\r\n    \"\"\"Loads the state dict from a file path into the FSDP module.\"\"\"\r\n    if not _is_full_checkpoint(path):\r\n        raise ValueError(\r\n            \"Failed to load checkpoint directly into the model. The given path must be a single file containing the\"\r\n            f\" full state dict: {path}\"\r\n        )\r\n    state_dict = torch.load(path, mmap=True, map_location=\"cpu\") if _TORCH_GREATER_EQUAL_2_3 else _lazy_load(path)\r\n    _load_raw_module_state(state_dict=state_dict, module=module, world_size=world_size, strict=strict)", "code_tokens": ["def", "_load_raw_module_state_from_path", "(", "path", ":", "Path", ",", "module", ":", "Module", ",", "world_size", ":", "int", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "if", "not", "_is_full_checkpoint", "(", "path", ")", ":", "raise", "ValueError", "(", "STRING", "fSTRING", ")", "state_dict", "=", "torch", ".", "load", "(", "path", ",", "mmap", "=", "True", ",", "map_location", "=", "STRING", ")", "if", "_TORCH_GREATER_EQUAL_2_3", "else", "_lazy_load", "(", "path", ")", "_load_raw_module_state", "(", "state_dict", "=", "state_dict", ",", "module", "=", "module", ",", "world_size", "=", "world_size", ",", "strict", "=", "strict", ")"], "docstring": "Loads the state dict from a file path into the FSDP module.", "docstring_tokens": ["loads", "the", "state", "dict", "from", "a", "file", "path", "into", "the", "fsdp", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 529, "end_line": 538, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_206", "original_string": "def _load_raw_module_state(\r\n    state_dict: dict[str, Any], module: Module, world_size: int = 1, strict: bool = True\r\n) -> None:\r\n    \"\"\"Loads the state dict into the module by gathering all weights first and then and writing back to each shard.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n    if _has_dtensor_modules(module):\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, set_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(\r\n            broadcast_from_rank0=True,\r\n            full_state_dict=True,\r\n            strict=False,\r\n        )\r\n\r\n        for submodule_name, submodule in module.named_modules():\r\n            for param_name, _ in _named_parameters_and_buffers_to_load(submodule):\r\n                full_param_name = f\"{submodule_name}{'.' if submodule_name else ''}{param_name}\"\r\n                if full_param_name not in state_dict:\r\n                    if not strict:\r\n                        continue\r\n                    raise KeyError(\r\n                        f\"The model contains a key '{full_param_name}' that does not exist in the loaded checkpoint.\"\r\n                        \" To disable strict loading, set `strict=False`.\"\r\n                    )\r\n                local_state_dict = {param_name: state_dict[full_param_name]}\r\n                set_model_state_dict(submodule, local_state_dict, options=state_dict_options)\r\n\r\n    elif isinstance(module, FSDP):\r\n        with _get_full_state_dict_context(module, world_size=world_size, rank0_only=False):\r\n            module.load_state_dict(state_dict, strict=strict)\r\n    else:\r\n        module.load_state_dict(state_dict, strict=strict)", "language": "python", "code": "def _load_raw_module_state(\r\n    state_dict: dict[str, Any], module: Module, world_size: int = 1, strict: bool = True\r\n) -> None:\r\n    \"\"\"Loads the state dict into the module by gathering all weights first and then and writing back to each shard.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n    if _has_dtensor_modules(module):\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, set_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(\r\n            broadcast_from_rank0=True,\r\n            full_state_dict=True,\r\n            strict=False,\r\n        )\r\n\r\n        for submodule_name, submodule in module.named_modules():\r\n            for param_name, _ in _named_parameters_and_buffers_to_load(submodule):\r\n                full_param_name = f\"{submodule_name}{'.' if submodule_name else ''}{param_name}\"\r\n                if full_param_name not in state_dict:\r\n                    if not strict:\r\n                        continue\r\n                    raise KeyError(\r\n                        f\"The model contains a key '{full_param_name}' that does not exist in the loaded checkpoint.\"\r\n                        \" To disable strict loading, set `strict=False`.\"\r\n                    )\r\n                local_state_dict = {param_name: state_dict[full_param_name]}\r\n                set_model_state_dict(submodule, local_state_dict, options=state_dict_options)\r\n\r\n    elif isinstance(module, FSDP):\r\n        with _get_full_state_dict_context(module, world_size=world_size, rank0_only=False):\r\n            module.load_state_dict(state_dict, strict=strict)\r\n    else:\r\n        module.load_state_dict(state_dict, strict=strict)", "code_tokens": ["def", "_load_raw_module_state", "(", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ",", "module", ":", "Module", ",", "world_size", ":", "int", "=", "1", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "if", "_has_dtensor_modules", "(", "module", ")", ":", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "set_model_state_dict", "state_dict_options", "=", "StateDictOptions", "(", "broadcast_from_rank0", "=", "True", ",", "full_state_dict", "=", "True", ",", "strict", "=", "False", ",", ")", "for", "submodule_name", ",", "submodule", "in", "module", ".", "named_modules", "(", ")", ":", "for", "param_name", ",", "_", "in", "_named_parameters_and_buffers_to_load", "(", "submodule", ")", ":", "full_param_name", "=", "fSTRING", "if", "full_param_name", "not", "in", "state_dict", ":", "if", "not", "strict", ":", "continue", "raise", "KeyError", "(", "fSTRING", "STRING", ")", "local_state_dict", "=", "{", "param_name", ":", "state_dict", "[", "full_param_name", "]", "}", "set_model_state_dict", "(", "submodule", ",", "local_state_dict", ",", "options", "=", "state_dict_options", ")", "elif", "isinstance", "(", "module", ",", "FSDP", ")", ":", "with", "_get_full_state_dict_context", "(", "module", ",", "world_size", "=", "world_size", ",", "rank0_only", "=", "False", ")", ":", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "else", ":", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")"], "docstring": "Loads the state dict into the module by gathering all weights first and then and writing back to each shard.", "docstring_tokens": ["loads", "the", "state", "dict", "into", "the", "module", "by", "gathering", "all", "weights", "first", "and", "then", "and", "writing", "back", "to", "each", "shard"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 541, "end_line": 574, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_207", "original_string": "def _named_parameters_and_buffers_to_load(module: Module) -> Generator:\r\n    \"\"\"Returns parameters and buffers, with non-persistent buffers excluded.\"\"\"\r\n    for param_name, param in itertools.chain(\r\n        module.named_buffers(recurse=False),\r\n        module.named_parameters(recurse=False),\r\n    ):\r\n        if param_name in module._non_persistent_buffers_set:\r\n            continue\r\n        yield param_name, param", "language": "python", "code": "def _named_parameters_and_buffers_to_load(module: Module) -> Generator:\r\n    \"\"\"Returns parameters and buffers, with non-persistent buffers excluded.\"\"\"\r\n    for param_name, param in itertools.chain(\r\n        module.named_buffers(recurse=False),\r\n        module.named_parameters(recurse=False),\r\n    ):\r\n        if param_name in module._non_persistent_buffers_set:\r\n            continue\r\n        yield param_name, param", "code_tokens": ["def", "_named_parameters_and_buffers_to_load", "(", "module", ":", "Module", ")", "-", ">", "Generator", ":", "STRING", "for", "param_name", ",", "param", "in", "itertools", ".", "chain", "(", "module", ".", "named_buffers", "(", "recurse", "=", "False", ")", ",", "module", ".", "named_parameters", "(", "recurse", "=", "False", ")", ",", ")", ":", "if", "param_name", "in", "module", ".", "_non_persistent_buffers_set", ":", "continue", "yield", "param_name", ",", "param"], "docstring": "Returns parameters and buffers, with non-persistent buffers excluded.", "docstring_tokens": ["returns", "parameters", "and", "buffers", "with", "non", "persistent", "buffers", "excluded"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 577, "end_line": 585, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "function_208", "original_string": "def _rekey_optimizer_state_if_needed(optimizer_state_dict: dict[str, Any], module: Module) -> dict[str, Any]:\r\n    \"\"\"Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter\r\n    names.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n    from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n    if isinstance(list(optimizer_state_dict[\"state\"].keys())[0], int):\r\n        optimizer_state_dict = FSDP.rekey_optim_state_dict(optimizer_state_dict, OptimStateKeyType.PARAM_NAME, module)\r\n    return optimizer_state_dict", "language": "python", "code": "def _rekey_optimizer_state_if_needed(optimizer_state_dict: dict[str, Any], module: Module) -> dict[str, Any]:\r\n    \"\"\"Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter\r\n    names.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n    from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n    if isinstance(list(optimizer_state_dict[\"state\"].keys())[0], int):\r\n        optimizer_state_dict = FSDP.rekey_optim_state_dict(optimizer_state_dict, OptimStateKeyType.PARAM_NAME, module)\r\n    return optimizer_state_dict", "code_tokens": ["def", "_rekey_optimizer_state_if_needed", "(", "optimizer_state_dict", ":", "dict", "[", "str", ",", "Any", "]", ",", "module", ":", "Module", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "from", "torch", ".", "distributed", ".", "fsdp", "import", "OptimStateKeyType", "if", "isinstance", "(", "list", "(", "optimizer_state_dict", "[", "STRING", "]", ".", "keys", "(", ")", ")", "[", "0", "]", ",", "int", ")", ":", "optimizer_state_dict", "=", "FSDP", ".", "rekey_optim_state_dict", "(", "optimizer_state_dict", ",", "OptimStateKeyType", ".", "PARAM_NAME", ",", "module", ")", "return", "optimizer_state_dict"], "docstring": "Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter", "docstring_tokens": ["handles", "the", "case", "where", "the", "optimizer", "state", "is", "saved", "from", "a", "normal", "optimizer", "and", "converts", "the", "keys", "to", "parameter"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "start_line": 588, "end_line": 596, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\parallel.py", "func_name": "function_209", "original_string": "def distributed_sampler_kwargs(self) -> Optional[dict[str, Any]]:\r\n        \"\"\"Arguments for the ``DistributedSampler``.\r\n\r\n        If this method is not defined, or it returns ``None``, then the ``DistributedSampler`` will not be used.\r\n\r\n        \"\"\"\r\n        return {\"num_replicas\": self.world_size, \"rank\": self.global_rank}", "language": "python", "code": "def distributed_sampler_kwargs(self) -> Optional[dict[str, Any]]:\r\n        \"\"\"Arguments for the ``DistributedSampler``.\r\n\r\n        If this method is not defined, or it returns ``None``, then the ``DistributedSampler`` will not be used.\r\n\r\n        \"\"\"\r\n        return {\"num_replicas\": self.world_size, \"rank\": self.global_rank}", "code_tokens": ["def", "distributed_sampler_kwargs", "(", "self", ")", "-", ">", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", ":", "STRING", "return", "{", "STRING", ":", "self", ".", "world_size", ",", "STRING", ":", "self", ".", "global_rank", "}"], "docstring": "Arguments for the ``DistributedSampler``.", "docstring_tokens": ["arguments", "for", "the", "distributedsampler"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\parallel.py", "start_line": 74, "end_line": 80, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\parallel.py", "func_name": "function_210", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return _all_gather_ddp_if_available(tensor, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return _all_gather_ddp_if_available(tensor, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "return", "_all_gather_ddp_if_available", "(", "tensor", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "Perform a all_gather on all processes.", "docstring_tokens": ["perform", "a", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\parallel.py", "start_line": 83, "end_line": 85, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\parallel.py", "func_name": "function_211", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.all_reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.all_reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "STRING", "decision", "=", "torch", ".", "tensor", "(", "int", "(", "decision", ")", ",", "device", "=", "self", ".", "root_device", ")", "decision", "=", "self", ".", "all_reduce", "(", "decision", ",", "reduce_op", "=", "ReduceOp", ".", "SUM", ",", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", ")", "decision", "=", "bool", "(", "decision", "=", "=", "self", ".", "world_size", ")", "if", "all", "else", "bool", "(", "decision", ")", "return", "decision"], "docstring": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard", "docstring_tokens": ["reduces", "a", "boolean", "decision", "over", "distributed", "processes", "by", "default", "is", "analogous", "to", "all", "from", "the", "standard"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\parallel.py", "start_line": 88, "end_line": 107, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "function_212", "original_string": "def register(\r\n        self,\r\n        name: str,\r\n        strategy: Optional[Callable] = None,\r\n        description: Optional[str] = None,\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a strategy mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n            strategy : strategy class\r\n            description : strategy description\r\n            override : overrides the registered strategy, if True\r\n            init_params: parameters to initialize the strategy\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise ValueError(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n        data[\"description\"] = description if description is not None else \"\"\r\n\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(strategy: Callable) -> Callable:\r\n            data[\"strategy\"] = strategy\r\n            data[\"strategy_name\"] = name\r\n            self[name] = data\r\n            return strategy\r\n\r\n        if strategy is not None:\r\n            return do_register(strategy)\r\n\r\n        return do_register", "language": "python", "code": "def register(\r\n        self,\r\n        name: str,\r\n        strategy: Optional[Callable] = None,\r\n        description: Optional[str] = None,\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a strategy mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n            strategy : strategy class\r\n            description : strategy description\r\n            override : overrides the registered strategy, if True\r\n            init_params: parameters to initialize the strategy\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise ValueError(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n        data[\"description\"] = description if description is not None else \"\"\r\n\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(strategy: Callable) -> Callable:\r\n            data[\"strategy\"] = strategy\r\n            data[\"strategy_name\"] = name\r\n            self[name] = data\r\n            return strategy\r\n\r\n        if strategy is not None:\r\n            return do_register(strategy)\r\n\r\n        return do_register", "code_tokens": ["def", "register", "(", "self", ",", "name", ":", "str", ",", "strategy", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "description", ":", "Optional", "[", "str", "]", "=", "None", ",", "override", ":", "bool", "=", "False", ",", "*", "*", "init_params", ":", "Any", ",", ")", "-", ">", "Callable", ":", "STRING", "if", "not", "(", "name", "is", "None", "or", "isinstance", "(", "name", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "if", "name", "in", "self", "and", "not", "override", ":", "raise", "ValueError", "(", "fSTRING", ")", "data", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "data", "[", "STRING", "]", "=", "description", "if", "description", "is", "not", "None", "else", "STRING", "data", "[", "STRING", "]", "=", "init_params", "def", "do_register", "(", "strategy", ":", "Callable", ")", "-", ">", "Callable", ":", "data", "[", "STRING", "]", "=", "strategy", "data", "[", "STRING", "]", "=", "name", "self", "[", "name", "]", "=", "data", "return", "strategy", "if", "strategy", "is", "not", "None", ":", "return", "do_register", "(", "strategy", ")", "return", "do_register"], "docstring": "Registers a strategy mapped to a name and with required metadata.", "docstring_tokens": ["registers", "a", "strategy", "mapped", "to", "a", "name", "and", "with", "required", "metadata"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\registry.py", "start_line": 43, "end_line": 81, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "function_213", "original_string": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered strategy with the required parameters and returns the strategy object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"strategy\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = \", \".join(sorted(self.keys())) or \"none\"\r\n        raise KeyError(err_msg.format(name, available_names))", "language": "python", "code": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered strategy with the required parameters and returns the strategy object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"strategy\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = \", \".join(sorted(self.keys())) or \"none\"\r\n        raise KeyError(err_msg.format(name, available_names))", "code_tokens": ["def", "get", "(", "self", ",", "name", ":", "str", ",", "default", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "Any", ":", "STRING", "if", "name", "in", "self", ":", "data", "=", "self", "[", "name", "]", "return", "data", "[", "STRING", "]", "(", "*", "*", "data", "[", "STRING", "]", ")", "if", "default", "is", "not", "None", ":", "return", "default", "err_msg", "=", "STRING", "available_names", "=", "STRING", ".", "join", "(", "sorted", "(", "self", ".", "keys", "(", ")", ")", ")", "or", "STRING", "raise", "KeyError", "(", "err_msg", ".", "format", "(", "name", ",", "available_names", ")", ")"], "docstring": "Calls the registered strategy with the required parameters and returns the strategy object.", "docstring_tokens": ["calls", "the", "registered", "strategy", "with", "the", "required", "parameters", "and", "returns", "the", "strategy", "object"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\registry.py", "start_line": 84, "end_line": 100, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "function_214", "original_string": "def remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered strategy by name.\"\"\"\r\n        self.pop(name)", "language": "python", "code": "def remove(self, name: str) -> None:\r\n        \"\"\"Removes the registered strategy by name.\"\"\"\r\n        self.pop(name)", "code_tokens": ["def", "remove", "(", "self", ",", "name", ":", "str", ")", "-", ">", "None", ":", "STRING", "self", ".", "pop", "(", "name", ")"], "docstring": "Removes the registered strategy by name.", "docstring_tokens": ["removes", "the", "registered", "strategy", "by", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\registry.py", "start_line": 102, "end_line": 104, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "function_215", "original_string": "def available_strategies(self) -> list:\r\n        \"\"\"Returns a list of registered strategies.\"\"\"\r\n        return list(self.keys())", "language": "python", "code": "def available_strategies(self) -> list:\r\n        \"\"\"Returns a list of registered strategies.\"\"\"\r\n        return list(self.keys())", "code_tokens": ["def", "available_strategies", "(", "self", ")", "-", ">", "list", ":", "STRING", "return", "list", "(", "self", ".", "keys", "(", ")", ")"], "docstring": "Returns a list of registered strategies.", "docstring_tokens": ["returns", "a", "list", "of", "registered", "strategies"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\registry.py", "start_line": 106, "end_line": 108, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\single_device.py", "func_name": "function_216", "original_string": "def all_reduce(self, tensor: Any | torch.Tensor, *args: Any, **kwargs: Any) -> Any | torch.Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates\r\n        with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "language": "python", "code": "def all_reduce(self, tensor: Any | torch.Tensor, *args: Any, **kwargs: Any) -> Any | torch.Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates\r\n        with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Any", "|", "torch", ".", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", "|", "torch", ".", "Tensor", ":", "STRING", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "as", "this", "plugin", "only", "operates"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\single_device.py", "start_line": 61, "end_line": 74, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\single_device.py", "func_name": "function_217", "original_string": "def all_gather(self, tensor: torch.Tensor, group: Any | None = None, sync_grads: bool = False) -> torch.Tensor:\r\n        \"\"\"Perform a ``all_gather`` on all processes.\"\"\"\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: torch.Tensor, group: Any | None = None, sync_grads: bool = False) -> torch.Tensor:\r\n        \"\"\"Perform a ``all_gather`` on all processes.\"\"\"\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "group", ":", "Any", "|", "None", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "torch", ".", "Tensor", ":", "STRING", "return", "tensor"], "docstring": "Perform a ``all_gather`` on all processes.", "docstring_tokens": ["perform", "a", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\single_device.py", "start_line": 77, "end_line": 79, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_218", "original_string": "def root_device(self) -> torch.device:\r\n        \"\"\"Returns the root device.\"\"\"", "language": "python", "code": "def root_device(self) -> torch.device:\r\n        \"\"\"Returns the root device.\"\"\"", "code_tokens": ["def", "root_device", "(", "self", ")", "-", ">", "torch", ".", "device", ":", "STRING"], "docstring": "Returns the root device.", "docstring_tokens": ["returns", "the", "root", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 63, "end_line": 64, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_219", "original_string": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether the current process is the rank zero process not only on the local node, but for all nodes.\"\"\"", "language": "python", "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether the current process is the rank zero process not only on the local node, but for all nodes.\"\"\"", "code_tokens": ["def", "is_global_zero", "(", "self", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Whether the current process is the rank zero process not only on the local node, but for all nodes.", "docstring_tokens": ["whether", "the", "current", "process", "is", "the", "rank", "zero", "process", "not", "only", "on", "the", "local", "node", "but", "for", "all", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 68, "end_line": 69, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_220", "original_string": "def _configure_launcher(self) -> None:\r\n        \"\"\"Attach the launcher based on Strategy.\"\"\"", "language": "python", "code": "def _configure_launcher(self) -> None:\r\n        \"\"\"Attach the launcher based on Strategy.\"\"\"", "code_tokens": ["def", "_configure_launcher", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Attach the launcher based on Strategy.", "docstring_tokens": ["attach", "the", "launcher", "based", "on", "strategy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 101, "end_line": 102, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_221", "original_string": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This must be called by the framework at the beginning of every process, before any distributed communication\r\n        takes place.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "language": "python", "code": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This must be called by the framework at the beginning of every process, before any distributed communication\r\n        takes place.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "code_tokens": ["def", "setup_environment", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup_device", "(", "self", ".", "root_device", ")"], "docstring": "Setup any processes or distributed connections.", "docstring_tokens": ["setup", "any", "processes", "or", "distributed", "connections"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 104, "end_line": 112, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_222", "original_string": "def process_dataloader(self, dataloader: DataLoader) -> DataLoader:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "language": "python", "code": "def process_dataloader(self, dataloader: DataLoader) -> DataLoader:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "code_tokens": ["def", "process_dataloader", "(", "self", ",", "dataloader", ":", "DataLoader", ")", "-", ">", "DataLoader", ":", "STRING", "return", "dataloader"], "docstring": "Wraps the dataloader if necessary.", "docstring_tokens": ["wraps", "the", "dataloader", "if", "necessary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 114, "end_line": 121, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_223", "original_string": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        precision_init_ctx = self.precision.tensor_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(precision_init_ctx)\r\n        return stack", "language": "python", "code": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        precision_init_ctx = self.precision.tensor_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(precision_init_ctx)\r\n        return stack", "code_tokens": ["def", "tensor_init_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING", "precision_init_ctx", "=", "self", ".", "precision", ".", "tensor_init_context", "(", ")", "stack", "=", "ExitStack", "(", ")", "stack", ".", "enter_context", "(", "self", ".", "root_device", ")", "stack", ".", "enter_context", "(", "precision_init_ctx", ")", "return", "stack"], "docstring": "Controls how tensors get created (device, dtype).", "docstring_tokens": ["controls", "how", "tensors", "get", "created", "device", "dtype"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 123, "end_line": 129, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_224", "original_string": "def module_init_context(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"A context manager wrapping the model instantiation.\r\n\r\n        Here, the strategy can control how the parameters of the model get created (device, dtype) and or apply other\r\n        patches to the model.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        precision_module_ctx = self.precision.module_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(_EmptyInit(enabled=bool(empty_init)))\r\n        stack.enter_context(precision_module_ctx)\r\n        return stack", "language": "python", "code": "def module_init_context(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"A context manager wrapping the model instantiation.\r\n\r\n        Here, the strategy can control how the parameters of the model get created (device, dtype) and or apply other\r\n        patches to the model.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        precision_module_ctx = self.precision.module_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(_EmptyInit(enabled=bool(empty_init)))\r\n        stack.enter_context(precision_module_ctx)\r\n        return stack", "code_tokens": ["def", "module_init_context", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "AbstractContextManager", ":", "STRING", "precision_module_ctx", "=", "self", ".", "precision", ".", "module_init_context", "(", ")", "stack", "=", "ExitStack", "(", ")", "stack", ".", "enter_context", "(", "self", ".", "root_device", ")", "stack", ".", "enter_context", "(", "_EmptyInit", "(", "enabled", "=", "bool", "(", "empty_init", ")", ")", ")", "stack", ".", "enter_context", "(", "precision_module_ctx", ")", "return", "stack"], "docstring": "A context manager wrapping the model instantiation.", "docstring_tokens": ["a", "context", "manager", "wrapping", "the", "model", "instantiation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 131, "end_line": 147, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_225", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Set up a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`setup_module` and :meth:`setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        module = self.setup_module(module)\r\n        optimizers = [self.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return module, optimizers, scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Set up a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`setup_module` and :meth:`setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        module = self.setup_module(module)\r\n        optimizers = [self.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return module, optimizers, scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "STRING", "]", "]", ":", "STRING", "module", "=", "self", ".", "setup_module", "(", "module", ")", "optimizers", "=", "[", "self", ".", "setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "return", "module", ",", "optimizers", ",", "scheduler"], "docstring": "Set up a model and multiple optimizers together.", "docstring_tokens": ["set", "up", "a", "model", "and", "multiple", "optimizers", "together"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 149, "end_line": 160, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_226", "original_string": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        return module", "language": "python", "code": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        return module", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "return", "module"], "docstring": "Performs setup for the model, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "model", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 162, "end_line": 164, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_227", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        return optimizer", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        return optimizer", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "STRING", "return", "optimizer"], "docstring": "Performs setup for the optimizer, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "optimizer", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 166, "end_line": 168, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_228", "original_string": "def module_to_device(self, module: Module) -> None:\r\n        \"\"\"Moves the model to the correct device.\"\"\"", "language": "python", "code": "def module_to_device(self, module: Module) -> None:\r\n        \"\"\"Moves the model to the correct device.\"\"\"", "code_tokens": ["def", "module_to_device", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "None", ":", "STRING"], "docstring": "Moves the model to the correct device.", "docstring_tokens": ["moves", "the", "model", "to", "the", "correct", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 171, "end_line": 172, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_229", "original_string": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n\r\n        \"\"\"\r\n        device = device or self.root_device\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n\r\n        \"\"\"\r\n        device = device or self.root_device\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "Any", ":", "STRING", "device", "=", "device", "or", "self", ".", "root_device", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Moves the batch to the correct device.", "docstring_tokens": ["moves", "the", "batch", "to", "the", "correct", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 174, "end_line": 186, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_230", "original_string": "def backward(self, tensor: Tensor, module: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"\r\n        self.precision.pre_backward(tensor, module)\r\n        self.precision.backward(tensor, module, *args, **kwargs)\r\n        self.precision.post_backward(tensor, module)", "language": "python", "code": "def backward(self, tensor: Tensor, module: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"\r\n        self.precision.pre_backward(tensor, module)\r\n        self.precision.backward(tensor, module, *args, **kwargs)\r\n        self.precision.post_backward(tensor, module)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "self", ".", "precision", ".", "pre_backward", "(", "tensor", ",", "module", ")", "self", ".", "precision", ".", "backward", "(", "tensor", ",", "module", ",", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "precision", ".", "post_backward", "(", "tensor", ",", "module", ")"], "docstring": "r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"", "docstring_tokens": ["r", "forwards", "backward", "calls", "to", "the", "precision", "plugin"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 188, "end_line": 192, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_231", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        return self.precision.optimizer_step(optimizer, **kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        return self.precision.optimizer_step(optimizer, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "STRING", "return", "self", ".", "precision", ".", "optimizer_step", "(", "optimizer", ",", "*", "*", "kwargs", ")"], "docstring": "Performs the actual optimizer step.", "docstring_tokens": ["performs", "the", "actual", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 194, "end_line": 206, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_232", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING"], "docstring": "Perform an all_gather on all processes.", "docstring_tokens": ["perform", "an", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 209, "end_line": 217, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_233", "original_string": "def all_reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "language": "python", "code": "def all_reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "Any", "]", ":", "STRING"], "docstring": "Reduces the given tensor (e.g. across GPUs/processes).", "docstring_tokens": ["reduces", "the", "given", "tensor", "e", "g", "across", "gpus", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 220, "end_line": 234, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_234", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "STRING"], "docstring": "Synchronizes all processes which blocks processes until the whole group enters this function.", "docstring_tokens": ["synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 237, "end_line": 243, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_235", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "STRING"], "docstring": "Broadcasts an object to all processes.", "docstring_tokens": ["broadcasts", "an", "object", "to", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 246, "end_line": 253, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_236", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduce a boolean decision across all processes.\"\"\"\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduce a boolean decision across all processes.\"\"\"\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "STRING", "return", "decision"], "docstring": "Reduce a boolean decision across all processes.", "docstring_tokens": ["reduce", "a", "boolean", "decision", "across", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 255, "end_line": 257, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_237", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        \"\"\"\r\n        state = self._convert_stateful_objects_in_state(state, filter=(filter or {}))\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint=state, path=path, storage_options=storage_options)", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        \"\"\"\r\n        state = self._convert_stateful_objects_in_state(state, filter=(filter or {}))\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint=state, path=path, storage_options=storage_options)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "state", "=", "self", ".", "_convert_stateful_objects_in_state", "(", "state", ",", "filter", "=", "(", "filter", "or", "{", "}", ")", ")", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "checkpoint", "=", "state", ",", "path", "=", "path", ",", "storage_options", "=", "storage_options", ")"], "docstring": "Save model, optimizer, and other state as a checkpoint file.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "as", "a", "checkpoint", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 259, "end_line": 280, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_238", "original_string": "def get_module_state_dict(self, module: Module) -> dict[str, Union[Any, Tensor]]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        return module.state_dict()", "language": "python", "code": "def get_module_state_dict(self, module: Module) -> dict[str, Union[Any, Tensor]]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        return module.state_dict()", "code_tokens": ["def", "get_module_state_dict", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "dict", "[", "str", ",", "Union", "[", "Any", ",", "Tensor", "]", "]", ":", "STRING", "return", "module", ".", "state_dict", "(", ")"], "docstring": "Returns model state.", "docstring_tokens": ["returns", "model", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 282, "end_line": 284, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_239", "original_string": "def load_module_state_dict(\r\n        self, module: Module, state_dict: dict[str, Union[Any, Tensor]], strict: bool = True\r\n    ) -> None:\r\n        \"\"\"Loads the given state into the model.\"\"\"\r\n        module.load_state_dict(state_dict, strict=strict)", "language": "python", "code": "def load_module_state_dict(\r\n        self, module: Module, state_dict: dict[str, Union[Any, Tensor]], strict: bool = True\r\n    ) -> None:\r\n        \"\"\"Loads the given state into the model.\"\"\"\r\n        module.load_state_dict(state_dict, strict=strict)", "code_tokens": ["def", "load_module_state_dict", "(", "self", ",", "module", ":", "Module", ",", "state_dict", ":", "dict", "[", "str", ",", "Union", "[", "Any", ",", "Tensor", "]", "]", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")"], "docstring": "Loads the given state into the model.", "docstring_tokens": ["loads", "the", "given", "state", "into", "the", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 286, "end_line": 290, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_240", "original_string": "def get_optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom plugins.\r\n\r\n        \"\"\"\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        return optimizer.state_dict()", "language": "python", "code": "def get_optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom plugins.\r\n\r\n        \"\"\"\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        return optimizer.state_dict()", "code_tokens": ["def", "get_optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "STRING", "if", "hasattr", "(", "optimizer", ",", "STRING", ")", ":", "optimizer", ".", "consolidate_state_dict", "(", ")", "return", "optimizer", ".", "state_dict", "(", ")", "if", "self", ".", "is_global_zero", "else", "{", "}", "return", "optimizer", ".", "state_dict", "(", ")"], "docstring": "Returns state of an optimizer.", "docstring_tokens": ["returns", "state", "of", "an", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 292, "end_line": 305, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_241", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: Can be one of:\r\n\r\n                - A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                - ``None`` or the empty dict: The loaded checkpoint will be returned in full.\r\n                - A :class:`~torch.nn.Module` instance, if the checkpoint file contains a raw module state dict.\r\n                - A :class:`~torch.optim.Optimizer` instance, if the checkpoint file contains a raw optimizer state.\r\n\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        \"\"\"\r\n        torch.cuda.empty_cache()\r\n        checkpoint = self.checkpoint_io.load_checkpoint(path)\r\n        if not state:\r\n            return checkpoint\r\n\r\n        if isinstance(state, Module):\r\n            self.load_module_state_dict(module=state, state_dict=checkpoint, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            state.load_state_dict(checkpoint)\r\n            return {}\r\n\r\n        _validate_keys_for_strict_loading(state.keys(), checkpoint.keys(), strict=strict)\r\n        for name, obj in state.copy().items():\r\n            if name not in checkpoint:\r\n                continue\r\n            if isinstance(obj, _Stateful):\r\n                if isinstance(obj, Module):\r\n                    self.load_module_state_dict(module=obj, state_dict=checkpoint.pop(name), strict=strict)\r\n                else:\r\n                    obj.load_state_dict(checkpoint.pop(name))\r\n            else:\r\n                state[name] = checkpoint.pop(name)\r\n        return checkpoint", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: Can be one of:\r\n\r\n                - A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                - ``None`` or the empty dict: The loaded checkpoint will be returned in full.\r\n                - A :class:`~torch.nn.Module` instance, if the checkpoint file contains a raw module state dict.\r\n                - A :class:`~torch.optim.Optimizer` instance, if the checkpoint file contains a raw optimizer state.\r\n\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        \"\"\"\r\n        torch.cuda.empty_cache()\r\n        checkpoint = self.checkpoint_io.load_checkpoint(path)\r\n        if not state:\r\n            return checkpoint\r\n\r\n        if isinstance(state, Module):\r\n            self.load_module_state_dict(module=state, state_dict=checkpoint, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            state.load_state_dict(checkpoint)\r\n            return {}\r\n\r\n        _validate_keys_for_strict_loading(state.keys(), checkpoint.keys(), strict=strict)\r\n        for name, obj in state.copy().items():\r\n            if name not in checkpoint:\r\n                continue\r\n            if isinstance(obj, _Stateful):\r\n                if isinstance(obj, Module):\r\n                    self.load_module_state_dict(module=obj, state_dict=checkpoint.pop(name), strict=strict)\r\n                else:\r\n                    obj.load_state_dict(checkpoint.pop(name))\r\n            else:\r\n                state[name] = checkpoint.pop(name)\r\n        return checkpoint", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "checkpoint", "=", "self", ".", "checkpoint_io", ".", "load_checkpoint", "(", "path", ")", "if", "not", "state", ":", "return", "checkpoint", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "self", ".", "load_module_state_dict", "(", "module", "=", "state", ",", "state_dict", "=", "checkpoint", ",", "strict", "=", "strict", ")", "return", "{", "}", "if", "isinstance", "(", "state", ",", "Optimizer", ")", ":", "state", ".", "load_state_dict", "(", "checkpoint", ")", "return", "{", "}", "_validate_keys_for_strict_loading", "(", "state", ".", "keys", "(", ")", ",", "checkpoint", ".", "keys", "(", ")", ",", "strict", "=", "strict", ")", "for", "name", ",", "obj", "in", "state", ".", "copy", "(", ")", ".", "items", "(", ")", ":", "if", "name", "not", "in", "checkpoint", ":", "continue", "if", "isinstance", "(", "obj", ",", "_Stateful", ")", ":", "if", "isinstance", "(", "obj", ",", "Module", ")", ":", "self", ".", "load_module_state_dict", "(", "module", "=", "obj", ",", "state_dict", "=", "checkpoint", ".", "pop", "(", "name", ")", ",", "strict", "=", "strict", ")", "else", ":", "obj", ".", "load_state_dict", "(", "checkpoint", ".", "pop", "(", "name", ")", ")", "else", ":", "state", "[", "name", "]", "=", "checkpoint", ".", "pop", "(", "name", ")", "return", "checkpoint"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 307, "end_line": 355, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_242", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        self.precision.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        self.precision.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "precision", ".", "teardown", "(", ")", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "teardown", "(", ")", "self", ".", "checkpoint_io", ".", "teardown", "(", ")"], "docstring": "This method is called to teardown the training process.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 357, "end_line": 366, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_243", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: torch.nn.Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> torch.Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_norm_(\r\n            parameters, max_norm=max_norm, norm_type=norm_type, error_if_nonfinite=error_if_nonfinite\r\n        )", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: torch.nn.Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> torch.Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_norm_(\r\n            parameters, max_norm=max_norm, norm_type=norm_type, error_if_nonfinite=error_if_nonfinite\r\n        )", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "torch", ".", "Tensor", ":", "STRING", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "parameters", "=", "self", ".", "precision", ".", "main_params", "(", "optimizer", ")", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ",", "error_if_nonfinite", "=", "error_if_nonfinite", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 368, "end_line": 381, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_244", "original_string": "def clip_gradients_value(self, module: torch.nn.Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "language": "python", "code": "def clip_gradients_value(self, module: torch.nn.Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "code_tokens": ["def", "clip_gradients_value", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "float", ",", "int", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "parameters", "=", "self", ".", "precision", ".", "main_params", "(", "optimizer", ")", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "clip_value", "=", "clip_val", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 383, "end_line": 387, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_245", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks the synchronization of gradients during the backward pass.\r\n\r\n        This is a context manager. It is only effective if it wraps a call to `.backward()`.\r\n\r\n        \"\"\"", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks the synchronization of gradients during the backward pass.\r\n\r\n        This is a context manager. It is only effective if it wraps a call to `.backward()`.\r\n\r\n        \"\"\"", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "STRING"], "docstring": "Blocks the synchronization of gradients during the backward pass.", "docstring_tokens": ["blocks", "the", "synchronization", "of", "gradients", "during", "the", "backward", "pass"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 427, "end_line": 432, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "function_246", "original_string": "def module_sharded_context(self) -> AbstractContextManager:\r\n        \"\"\"A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of\r\n        parameters on creation.\r\n\r\n        By sharding layers directly on instantiation, one can reduce peak memory usage and initialization time.\r\n\r\n        \"\"\"", "language": "python", "code": "def module_sharded_context(self) -> AbstractContextManager:\r\n        \"\"\"A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of\r\n        parameters on creation.\r\n\r\n        By sharding layers directly on instantiation, one can reduce peak memory usage and initialization time.\r\n\r\n        \"\"\"", "code_tokens": ["def", "module_sharded_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "STRING"], "docstring": "A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of", "docstring_tokens": ["a", "context", "manager", "that", "goes", "over", "the", "instantiation", "of", "an", "class", "torch", "nn", "module", "and", "handles", "sharding", "of"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\strategy.py", "start_line": 439, "end_line": 445, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla.py", "func_name": "function_247", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "fSTRING", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla.py", "start_line": 176, "end_line": 203, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla.py", "func_name": "function_248", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary of the same format as ``state`` mapping keys to callables that return a\r\n                boolean indicating whether the given parameter should be saved (``True``) or filtered out (``False``).\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        super().save_checkpoint(path, state, storage_options=storage_options, filter=filter)", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary of the same format as ``state`` mapping keys to callables that return a\r\n                boolean indicating whether the given parameter should be saved (``True``) or filtered out (``False``).\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        super().save_checkpoint(path, state, storage_options=storage_options, filter=filter)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "super", "(", ")", ".", "save_checkpoint", "(", "path", ",", "state", ",", "storage_options", "=", "storage_options", ",", "filter", "=", "filter", ")"], "docstring": "Save model, optimizer, and other state as a checkpoint file.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "as", "a", "checkpoint", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla.py", "start_line": 275, "end_line": 298, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_249", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.\"\"\"\r\n        raise NotImplementedError(\r\n            f\"The `{type(self).__name__}` does not support the joint setup of module and optimizer(s).\"\r\n            \" Please do it in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n            \" call `setup_optimizer`.\"\r\n        )", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.\"\"\"\r\n        raise NotImplementedError(\r\n            f\"The `{type(self).__name__}` does not support the joint setup of module and optimizer(s).\"\r\n            \" Please do it in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n            \" call `setup_optimizer`.\"\r\n        )", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "STRING", "]", "]", ":", "STRING", "raise", "NotImplementedError", "(", "fSTRING", "STRING", "STRING", ")"], "docstring": "Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.", "docstring_tokens": ["returns", "notimplementederror", "since", "for", "xlafsdp", "optimizer", "setup", "must", "happen", "after", "module", "setup"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 198, "end_line": 206, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_250", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with XLAFSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if any(getattr(p, \"_is_sharded\", False) for group in optimizer.param_groups for p in group[\"params\"]):\r\n            return optimizer\r\n        raise ValueError(\r\n            \"The optimizer does not seem to reference any XLAFSDP parameters. HINT: Make sure to create the optimizer\"\r\n            \" after setting up the model.\"\r\n        )", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with XLAFSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if any(getattr(p, \"_is_sharded\", False) for group in optimizer.param_groups for p in group[\"params\"]):\r\n            return optimizer\r\n        raise ValueError(\r\n            \"The optimizer does not seem to reference any XLAFSDP parameters. HINT: Make sure to create the optimizer\"\r\n            \" after setting up the model.\"\r\n        )", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "STRING", "if", "any", "(", "getattr", "(", "p", ",", "STRING", ",", "False", ")", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "STRING", "]", ")", ":", "return", "optimizer", "raise", "ValueError", "(", "STRING", "STRING", ")"], "docstring": "Set up an optimizer for a model wrapped with XLAFSDP.", "docstring_tokens": ["set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "xlafsdp"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 256, "end_line": 269, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_251", "original_string": "def optimizer_step(self, optimizer: Optimizable, **kwargs: Any) -> Any:\r\n        \"\"\"Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.\r\n        Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        loss = optimizer.step(**kwargs)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        return loss", "language": "python", "code": "def optimizer_step(self, optimizer: Optimizable, **kwargs: Any) -> Any:\r\n        \"\"\"Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.\r\n        Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        loss = optimizer.step(**kwargs)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        return loss", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "loss", "=", "optimizer", ".", "step", "(", "*", "*", "kwargs", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "return", "loss"], "docstring": "Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.", "docstring_tokens": ["overrides", "default", "tpu", "optimizer_step", "since", "fsdp", "should", "not", "call", "torch_xla", "core", "xla_model", "optimizer_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 272, "end_line": 285, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_252", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        assert callable(module.clip_grad_norm_)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        assert callable(module.clip_grad_norm_)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Tensor", ":", "STRING", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "assert", "callable", "(", "module", ".", "clip_grad_norm_", ")", "return", "module", ".", "clip_grad_norm_", "(", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 288, "end_line": 299, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_253", "original_string": "def clip_gradients_value(self, module: Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        raise NotImplementedError(\r\n            \"XLA's FSDP strategy does not support to clip gradients by value.\"\r\n            \" Consider clipping by norm instead or choose another strategy!\"\r\n        )", "language": "python", "code": "def clip_gradients_value(self, module: Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        raise NotImplementedError(\r\n            \"XLA's FSDP strategy does not support to clip gradients by value.\"\r\n            \" Consider clipping by norm instead or choose another strategy!\"\r\n        )", "code_tokens": ["def", "clip_gradients_value", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "float", ",", "int", "]", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError", "(", "STRING", "STRING", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 302, "end_line": 307, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_254", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "fSTRING", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 310, "end_line": 337, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_255", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in the provided checkpoint directory.\r\n\r\n        If the user specifies sharded checkpointing, the directory will contain one file per process, with model- and\r\n        optimizer shards stored per file. If the user specifies full checkpointing, the directory will contain a\r\n        consolidated checkpoint combining all of the sharded checkpoints.\r\n\r\n        \"\"\"\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and any(path.iterdir()):\r\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = [module for module in state.values() if isinstance(module, XLAFSDP)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple XLAFSDP modules in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n\r\n        parallel_devices = self.parallel_devices\r\n        assert parallel_devices is not None\r\n        if self._sequential_save:\r\n            for rank in range(len(parallel_devices)):\r\n                if rank == self.local_rank:\r\n                    self._save_checkpoint_shard(path, state, storage_options, filter)\r\n                self.barrier(f\"wait-for-{rank}-save\")\r\n        else:\r\n            self._save_checkpoint_shard(path, state, storage_options, filter)\r\n\r\n        if self._state_dict_type == \"full\":\r\n            ckpt_prefix = str(path / \"checkpoint\")\r\n            ckpt_suffix = \"_rank-*-of-*.pth\"\r\n            if len(parallel_devices) != self.world_size:  # multihost\r\n                raise OSError(\r\n                    \"Multihost setups do not have a shared filesystem, so the checkpoint shards cannot be consolidated\"\r\n                    \" into a single checkpoint after saving them. Please switch to\"\r\n                    \" `XLAFSDPStrategy(state_dict_type='sharded')`. TIP: You can consolidate them manually by getting\"\r\n                    \" them together into a single directory and running `python -m\"\r\n                    f\" torch_xla.distributed.fsdp.consolidate_sharded_ckpts --ckpt_prefix {ckpt_prefix!r} --ckpt_suffix\"\r\n                    f\" {ckpt_suffix!r} --save_path 'path/to/consolidated.ckpt'`.\"\r\n                )\r\n\r\n            from torch_xla.distributed.fsdp import consolidate_sharded_model_checkpoints\r\n\r\n            self.barrier(\"before_ckpt_consolidation\")\r\n            if self.is_global_zero:\r\n                save_path = path.parent / \"consolidated.ckpt\"\r\n                consolidate_sharded_model_checkpoints(ckpt_prefix, ckpt_suffix, str(save_path))\r\n                self.checkpoint_io.remove_checkpoint(path)\r\n                get_filesystem(save_path).mv(str(save_path), str(path))\r\n            self.barrier(\"after_ckpt_consolidation\")", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in the provided checkpoint directory.\r\n\r\n        If the user specifies sharded checkpointing, the directory will contain one file per process, with model- and\r\n        optimizer shards stored per file. If the user specifies full checkpointing, the directory will contain a\r\n        consolidated checkpoint combining all of the sharded checkpoints.\r\n\r\n        \"\"\"\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and any(path.iterdir()):\r\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = [module for module in state.values() if isinstance(module, XLAFSDP)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple XLAFSDP modules in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n\r\n        parallel_devices = self.parallel_devices\r\n        assert parallel_devices is not None\r\n        if self._sequential_save:\r\n            for rank in range(len(parallel_devices)):\r\n                if rank == self.local_rank:\r\n                    self._save_checkpoint_shard(path, state, storage_options, filter)\r\n                self.barrier(f\"wait-for-{rank}-save\")\r\n        else:\r\n            self._save_checkpoint_shard(path, state, storage_options, filter)\r\n\r\n        if self._state_dict_type == \"full\":\r\n            ckpt_prefix = str(path / \"checkpoint\")\r\n            ckpt_suffix = \"_rank-*-of-*.pth\"\r\n            if len(parallel_devices) != self.world_size:  # multihost\r\n                raise OSError(\r\n                    \"Multihost setups do not have a shared filesystem, so the checkpoint shards cannot be consolidated\"\r\n                    \" into a single checkpoint after saving them. Please switch to\"\r\n                    \" `XLAFSDPStrategy(state_dict_type='sharded')`. TIP: You can consolidate them manually by getting\"\r\n                    \" them together into a single directory and running `python -m\"\r\n                    f\" torch_xla.distributed.fsdp.consolidate_sharded_ckpts --ckpt_prefix {ckpt_prefix!r} --ckpt_suffix\"\r\n                    f\" {ckpt_suffix!r} --save_path 'path/to/consolidated.ckpt'`.\"\r\n                )\r\n\r\n            from torch_xla.distributed.fsdp import consolidate_sharded_model_checkpoints\r\n\r\n            self.barrier(\"before_ckpt_consolidation\")\r\n            if self.is_global_zero:\r\n                save_path = path.parent / \"consolidated.ckpt\"\r\n                consolidate_sharded_model_checkpoints(ckpt_prefix, ckpt_suffix, str(save_path))\r\n                self.checkpoint_io.remove_checkpoint(path)\r\n                get_filesystem(save_path).mv(str(save_path), str(path))\r\n            self.barrier(\"after_ckpt_consolidation\")", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "path", ".", "is_dir", "(", ")", "and", "any", "(", "path", ".", "iterdir", "(", ")", ")", ":", "raise", "FileExistsError", "(", "fSTRING", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "XlaFullyShardedDataParallel", "as", "XLAFSDP", "modules", "=", "[", "module", "for", "module", "in", "state", ".", "values", "(", ")", "if", "isinstance", "(", "module", ",", "XLAFSDP", ")", "]", "if", "len", "(", "modules", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "if", "len", "(", "modules", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "parallel_devices", "=", "self", ".", "parallel_devices", "assert", "parallel_devices", "is", "not", "None", "if", "self", ".", "_sequential_save", ":", "for", "rank", "in", "range", "(", "len", "(", "parallel_devices", ")", ")", ":", "if", "rank", "=", "=", "self", ".", "local_rank", ":", "self", ".", "_save_checkpoint_shard", "(", "path", ",", "state", ",", "storage_options", ",", "filter", ")", "self", ".", "barrier", "(", "fSTRING", ")", "else", ":", "self", ".", "_save_checkpoint_shard", "(", "path", ",", "state", ",", "storage_options", ",", "filter", ")", "if", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "ckpt_prefix", "=", "str", "(", "path", "/", "STRING", ")", "ckpt_suffix", "=", "STRING", "if", "len", "(", "parallel_devices", ")", "!", "=", "self", ".", "world_size", ":", "#", "multihost", "raise", "OSError", "(", "STRING", "STRING", "STRING", "STRING", "fSTRING", "fSTRING", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "consolidate_sharded_model_checkpoints", "self", ".", "barrier", "(", "STRING", ")", "if", "self", ".", "is_global_zero", ":", "save_path", "=", "path", ".", "parent", "/", "STRING", "consolidate_sharded_model_checkpoints", "(", "ckpt_prefix", ",", "ckpt_suffix", ",", "str", "(", "save_path", ")", ")", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "path", ")", "get_filesystem", "(", "save_path", ")", ".", "mv", "(", "str", "(", "save_path", ")", ",", "str", "(", "path", ")", ")", "self", ".", "barrier", "(", "STRING", ")"], "docstring": "Save model, optimizer, and other state in the provided checkpoint directory.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "in", "the", "provided", "checkpoint", "directory"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 409, "end_line": 482, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_256", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Given a folder, load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        The strategy currently only supports saving and loading sharded checkpoints which are stored in form of a\r\n        directory of multiple files rather than a single file.\r\n\r\n        \"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got `XLAFSDPStrategy.load_checkpoint(..., state={state!r})` but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                \" `FSDPStrategy.load_checkpoint(..., state={'model': model, ...})`\"\r\n            )\r\n\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, (Module, Optimizer)):\r\n            raise NotImplementedError(\r\n                \"Loading a single module or optimizer object from a checkpoint\"\r\n                \" is not supported yet with the XLAFSDP strategy.\"\r\n            )\r\n\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = {key: module for key, module in state.items() if isinstance(module, XLAFSDP)}\r\n        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}\r\n        if self._state_dict_type == \"sharded\":\r\n            file = path / f\"checkpoint_rank-{self.global_rank:08d}-of-{self.world_size:08d}.pth\"\r\n            if not file.is_file():\r\n                raise ValueError(\r\n                    f\"The path {str(file)!r} does not point to valid sharded checkpoints. Make sure the path points to\"\r\n                    \" a directory with XLAFSDP checkpoint shards.\"\r\n                )\r\n            if len(modules) == 0:\r\n                raise ValueError(\r\n                    \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                    \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                    \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n                )\r\n            if len(modules) > 1:\r\n                raise ValueError(\r\n                    \"Found multiple XLAFSDP modules in the given state. Loading checkpoints with FSDP is\"\r\n                    \" currently limited to a single model per checkpoint. To load multiple models, call the\"\r\n                    \" load method for each model separately with a different path.\"\r\n                )\r\n\r\n            _, module = list(modules.items())[0]\r\n            sharded_ckpt = torch.load(file)\r\n\r\n            module.load_state_dict(sharded_ckpt[\"model\"], strict=strict)\r\n            for opt_key, opt in optimizers.items():\r\n                opt.load_state_dict(sharded_ckpt[opt_key])\r\n\r\n            loaded_metadata_keys = sharded_ckpt.keys() - modules.keys() - optimizers.keys()\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, loaded_metadata_keys, strict=strict)\r\n            for key in requested_metadata_keys:\r\n                if key in loaded_metadata_keys:\r\n                    state[key] = sharded_ckpt[key]\r\n                    loaded_metadata_keys.remove(key)\r\n\r\n            metadata = {}\r\n            if len(loaded_metadata_keys):\r\n                for key in loaded_metadata_keys:\r\n                    metadata[key] = sharded_ckpt[key]\r\n\r\n            if \"shard_metadata\" in metadata:\r\n                metadata.pop(\"shard_metadata\")\r\n\r\n            return metadata\r\n\r\n        if self._state_dict_type == \"full\":\r\n            if not path.is_file():\r\n                raise ValueError(\r\n                    f\"The path {str(path)!r} does not point to a valid full checkpoint. Make sure the path points to a\"\r\n                    \" directory with a full XLAFSDP checkpoint.\"\r\n                )\r\n            if len(optimizers) > 0 or len(state.keys() - modules.keys() - optimizers.keys()) > 0:\r\n                rank_zero_warn(\r\n                    \"Loading a full checkpoint will only load the full model.\"\r\n                    \" The optimizer and any additional metadata are not included.\"\r\n                )\r\n            if len(modules) > 0:\r\n                raise ValueError(\r\n                    \"Found a XLAFSDP model in the provided checkpoint state.\"\r\n                    \" Please provide the model without any XLAFSDP wrapper.\"\r\n                )\r\n            if \"model\" not in state or not isinstance(model := state[\"model\"], torch.nn.Module):\r\n                raise NotImplementedError(\"XLAFSDP only supports a single model instance with 'model' as the key.\")\r\n            full_ckpt = torch.load(path)\r\n            model.load_state_dict(full_ckpt.pop(\"model\"), strict=strict)\r\n            return full_ckpt\r\n\r\n        raise ValueError(f\"Unknown state_dict_type: {self._state_dict_type}\")", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Given a folder, load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        The strategy currently only supports saving and loading sharded checkpoints which are stored in form of a\r\n        directory of multiple files rather than a single file.\r\n\r\n        \"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got `XLAFSDPStrategy.load_checkpoint(..., state={state!r})` but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                \" `FSDPStrategy.load_checkpoint(..., state={'model': model, ...})`\"\r\n            )\r\n\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, (Module, Optimizer)):\r\n            raise NotImplementedError(\r\n                \"Loading a single module or optimizer object from a checkpoint\"\r\n                \" is not supported yet with the XLAFSDP strategy.\"\r\n            )\r\n\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = {key: module for key, module in state.items() if isinstance(module, XLAFSDP)}\r\n        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}\r\n        if self._state_dict_type == \"sharded\":\r\n            file = path / f\"checkpoint_rank-{self.global_rank:08d}-of-{self.world_size:08d}.pth\"\r\n            if not file.is_file():\r\n                raise ValueError(\r\n                    f\"The path {str(file)!r} does not point to valid sharded checkpoints. Make sure the path points to\"\r\n                    \" a directory with XLAFSDP checkpoint shards.\"\r\n                )\r\n            if len(modules) == 0:\r\n                raise ValueError(\r\n                    \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                    \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                    \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n                )\r\n            if len(modules) > 1:\r\n                raise ValueError(\r\n                    \"Found multiple XLAFSDP modules in the given state. Loading checkpoints with FSDP is\"\r\n                    \" currently limited to a single model per checkpoint. To load multiple models, call the\"\r\n                    \" load method for each model separately with a different path.\"\r\n                )\r\n\r\n            _, module = list(modules.items())[0]\r\n            sharded_ckpt = torch.load(file)\r\n\r\n            module.load_state_dict(sharded_ckpt[\"model\"], strict=strict)\r\n            for opt_key, opt in optimizers.items():\r\n                opt.load_state_dict(sharded_ckpt[opt_key])\r\n\r\n            loaded_metadata_keys = sharded_ckpt.keys() - modules.keys() - optimizers.keys()\r\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\r\n            _validate_keys_for_strict_loading(requested_metadata_keys, loaded_metadata_keys, strict=strict)\r\n            for key in requested_metadata_keys:\r\n                if key in loaded_metadata_keys:\r\n                    state[key] = sharded_ckpt[key]\r\n                    loaded_metadata_keys.remove(key)\r\n\r\n            metadata = {}\r\n            if len(loaded_metadata_keys):\r\n                for key in loaded_metadata_keys:\r\n                    metadata[key] = sharded_ckpt[key]\r\n\r\n            if \"shard_metadata\" in metadata:\r\n                metadata.pop(\"shard_metadata\")\r\n\r\n            return metadata\r\n\r\n        if self._state_dict_type == \"full\":\r\n            if not path.is_file():\r\n                raise ValueError(\r\n                    f\"The path {str(path)!r} does not point to a valid full checkpoint. Make sure the path points to a\"\r\n                    \" directory with a full XLAFSDP checkpoint.\"\r\n                )\r\n            if len(optimizers) > 0 or len(state.keys() - modules.keys() - optimizers.keys()) > 0:\r\n                rank_zero_warn(\r\n                    \"Loading a full checkpoint will only load the full model.\"\r\n                    \" The optimizer and any additional metadata are not included.\"\r\n                )\r\n            if len(modules) > 0:\r\n                raise ValueError(\r\n                    \"Found a XLAFSDP model in the provided checkpoint state.\"\r\n                    \" Please provide the model without any XLAFSDP wrapper.\"\r\n                )\r\n            if \"model\" not in state or not isinstance(model := state[\"model\"], torch.nn.Module):\r\n                raise NotImplementedError(\"XLAFSDP only supports a single model instance with 'model' as the key.\")\r\n            full_ckpt = torch.load(path)\r\n            model.load_state_dict(full_ckpt.pop(\"model\"), strict=strict)\r\n            return full_ckpt\r\n\r\n        raise ValueError(f\"Unknown state_dict_type: {self._state_dict_type}\")", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "state", ":", "raise", "ValueError", "(", "fSTRING", "STRING", "STRING", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "isinstance", "(", "state", ",", "(", "Module", ",", "Optimizer", ")", ")", ":", "raise", "NotImplementedError", "(", "STRING", "STRING", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "XlaFullyShardedDataParallel", "as", "XLAFSDP", "modules", "=", "{", "key", ":", "module", "for", "key", ",", "module", "in", "state", ".", "items", "(", ")", "if", "isinstance", "(", "module", ",", "XLAFSDP", ")", "}", "optimizers", "=", "{", "key", ":", "optim", "for", "key", ",", "optim", "in", "state", ".", "items", "(", ")", "if", "isinstance", "(", "optim", ",", "Optimizer", ")", "}", "if", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "file", "=", "path", "/", "fSTRING", "if", "not", "file", ".", "is_file", "(", ")", ":", "raise", "ValueError", "(", "fSTRING", "STRING", ")", "if", "len", "(", "modules", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "if", "len", "(", "modules", ")", ">", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", "STRING", ")", "_", ",", "module", "=", "list", "(", "modules", ".", "items", "(", ")", ")", "[", "0", "]", "sharded_ckpt", "=", "torch", ".", "load", "(", "file", ")", "module", ".", "load_state_dict", "(", "sharded_ckpt", "[", "STRING", "]", ",", "strict", "=", "strict", ")", "for", "opt_key", ",", "opt", "in", "optimizers", ".", "items", "(", ")", ":", "opt", ".", "load_state_dict", "(", "sharded_ckpt", "[", "opt_key", "]", ")", "loaded_metadata_keys", "=", "sharded_ckpt", ".", "keys", "(", ")", "-", "modules", ".", "keys", "(", ")", "-", "optimizers", ".", "keys", "(", ")", "requested_metadata_keys", "=", "state", ".", "keys", "(", ")", "-", "modules", ".", "keys", "(", ")", "-", "optimizers", ".", "keys", "(", ")", "_validate_keys_for_strict_loading", "(", "requested_metadata_keys", ",", "loaded_metadata_keys", ",", "strict", "=", "strict", ")", "for", "key", "in", "requested_metadata_keys", ":", "if", "key", "in", "loaded_metadata_keys", ":", "state", "[", "key", "]", "=", "sharded_ckpt", "[", "key", "]", "loaded_metadata_keys", ".", "remove", "(", "key", ")", "metadata", "=", "{", "}", "if", "len", "(", "loaded_metadata_keys", ")", ":", "for", "key", "in", "loaded_metadata_keys", ":", "metadata", "[", "key", "]", "=", "sharded_ckpt", "[", "key", "]", "if", "STRING", "in", "metadata", ":", "metadata", ".", "pop", "(", "STRING", ")", "return", "metadata", "if", "self", ".", "_state_dict_type", "=", "=", "STRING", ":", "if", "not", "path", ".", "is_file", "(", ")", ":", "raise", "ValueError", "(", "fSTRING", "STRING", ")", "if", "len", "(", "optimizers", ")", ">", "0", "or", "len", "(", "state", ".", "keys", "(", ")", "-", "modules", ".", "keys", "(", ")", "-", "optimizers", ".", "keys", "(", ")", ")", ">", "0", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "if", "len", "(", "modules", ")", ">", "0", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "if", "STRING", "not", "in", "state", "or", "not", "isinstance", "(", "model", ":", "=", "state", "[", "STRING", "]", ",", "torch", ".", "nn", ".", "Module", ")", ":", "raise", "NotImplementedError", "(", "STRING", ")", "full_ckpt", "=", "torch", ".", "load", "(", "path", ")", "model", ".", "load_state_dict", "(", "full_ckpt", ".", "pop", "(", "STRING", ")", ",", "strict", "=", "strict", ")", "return", "full_ckpt", "raise", "ValueError", "(", "fSTRING", ")"], "docstring": "Given a folder, load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["given", "a", "folder", "load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 513, "end_line": 614, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "function_257", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        if not isinstance(module, XLAFSDP):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `XlaFullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        if not isinstance(module, XLAFSDP):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `XlaFullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "STRING", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "XlaFullyShardedDataParallel", "as", "XLAFSDP", "if", "not", "isinstance", "(", "module", ",", "XLAFSDP", ")", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "fSTRING", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch_xla", "distributed", "fsdp", "xlafullyshardeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "start_line": 672, "end_line": 685, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\launcher.py", "func_name": "function_258", "original_string": "def is_interactive_compatible(self) -> bool:\r\n        \"\"\"Returns whether this launcher can work in interactive environments such as Jupyter notebooks.\"\"\"", "language": "python", "code": "def is_interactive_compatible(self) -> bool:\r\n        \"\"\"Returns whether this launcher can work in interactive environments such as Jupyter notebooks.\"\"\"", "code_tokens": ["def", "is_interactive_compatible", "(", "self", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Returns whether this launcher can work in interactive environments such as Jupyter notebooks.", "docstring_tokens": ["returns", "whether", "this", "launcher", "can", "work", "in", "interactive", "environments", "such", "as", "jupyter", "notebooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\launcher.py", "start_line": 30, "end_line": 31, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\launcher.py", "func_name": "function_259", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches the processes.\"\"\"", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches the processes.\"\"\"", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING"], "docstring": "Launches the processes.", "docstring_tokens": ["launches", "the", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\launcher.py", "start_line": 34, "end_line": 35, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_260", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [function, args, kwargs, return_queue]\r\n\r\n        mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n        )\r\n        return return_queue.get()", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [function, args, kwargs, return_queue]\r\n\r\n        mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n        )\r\n        return return_queue.get()", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "if", "self", ".", "_start_method", "in", "(", "STRING", ",", "STRING", ")", ":", "_check_bad_cuda_fork", "(", ")", "if", "self", ".", "_start_method", "=", "=", "STRING", ":", "_check_missing_main_guard", "(", ")", "assert", "self", ".", "_strategy", ".", "cluster_environment", "is", "not", "None", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "self", ".", "_strategy", ".", "cluster_environment", ".", "main_port", ")", "context", "=", "mp", ".", "get_context", "(", "self", ".", "_start_method", ")", "return_queue", "=", "context", ".", "SimpleQueue", "(", ")", "if", "self", ".", "_start_method", "=", "=", "STRING", ":", "global_states", "=", "_GlobalStateSnapshot", ".", "capture", "(", ")", "process_args", "=", "[", "function", ",", "args", ",", "kwargs", ",", "return_queue", ",", "global_states", "]", "else", ":", "process_args", "=", "[", "function", ",", "args", ",", "kwargs", ",", "return_queue", "]", "mp", ".", "start_processes", "(", "self", ".", "_wrapping_function", ",", "args", "=", "process_args", ",", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", ",", "start_method", "=", "self", ".", "_start_method", ",", ")", "return", "return_queue", ".", "get", "(", ")"], "docstring": "Launches processes that run the given function in parallel.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 84, "end_line": 122, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_261", "original_string": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "language": "python", "code": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "code_tokens": ["def", "capture", "(", "cls", ")", "-", ">", "STRING", ":", "STRING", "return", "cls", "(", "use_deterministic_algorithms", "=", "torch", ".", "are_deterministic_algorithms_enabled", "(", ")", ",", "use_deterministic_algorithms_warn_only", "=", "torch", ".", "is_deterministic_algorithms_warn_only_enabled", "(", ")", ",", "cudnn_benchmark", "=", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", ",", "rng_states", "=", "_collect_rng_states", "(", ")", ",", ")"], "docstring": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "docstring_tokens": ["capture", "a", "few", "global", "states", "from", "torch", "numpy", "etc", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 172, "end_line": 179, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_262", "original_string": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "language": "python", "code": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "code_tokens": ["def", "restore", "(", "self", ")", "-", ">", "None", ":", "STRING", "torch", ".", "use_deterministic_algorithms", "(", "self", ".", "use_deterministic_algorithms", ",", "warn_only", "=", "self", ".", "use_deterministic_algorithms_warn_only", ")", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "self", ".", "cudnn_benchmark", "_set_rng_states", "(", "self", ".", "rng_states", ")"], "docstring": "Restores all globals to the values captured in the :meth:`capture` method.", "docstring_tokens": ["restores", "all", "globals", "to", "the", "values", "captured", "in", "the", "meth", "capture", "method"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 181, "end_line": 187, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_263", "original_string": "def _check_bad_cuda_fork() -> None:\r\n    \"\"\"Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.\r\n\r\n    The error message replaces PyTorch's 'Cannot re-initialize CUDA in forked subprocess' with helpful advice for\r\n    Lightning users.\r\n\r\n    \"\"\"\r\n    if not torch.cuda.is_initialized():\r\n        return\r\n\r\n    message = (\r\n        \"Lightning can't create new processes if CUDA is already initialized. Did you manually call\"\r\n        \" `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any\"\r\n        \" other way? Please remove any such calls, or change the selected strategy.\"\r\n    )\r\n    if _IS_INTERACTIVE:\r\n        message += \" You will have to restart the Python kernel.\"\r\n    raise RuntimeError(message)", "language": "python", "code": "def _check_bad_cuda_fork() -> None:\r\n    \"\"\"Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.\r\n\r\n    The error message replaces PyTorch's 'Cannot re-initialize CUDA in forked subprocess' with helpful advice for\r\n    Lightning users.\r\n\r\n    \"\"\"\r\n    if not torch.cuda.is_initialized():\r\n        return\r\n\r\n    message = (\r\n        \"Lightning can't create new processes if CUDA is already initialized. Did you manually call\"\r\n        \" `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any\"\r\n        \" other way? Please remove any such calls, or change the selected strategy.\"\r\n    )\r\n    if _IS_INTERACTIVE:\r\n        message += \" You will have to restart the Python kernel.\"\r\n    raise RuntimeError(message)", "code_tokens": ["def", "_check_bad_cuda_fork", "(", ")", "-", ">", "None", ":", "STRING", "if", "not", "torch", ".", "cuda", ".", "is_initialized", "(", ")", ":", "return", "message", "=", "(", "STRING", "STRING", "STRING", ")", "if", "_IS_INTERACTIVE", ":", "message", "+", "=", "STRING", "raise", "RuntimeError", "(", "message", ")"], "docstring": "Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.", "docstring_tokens": ["checks", "whether", "it", "is", "safe", "to", "fork", "and", "initialize", "cuda", "in", "the", "new", "processes", "and", "raises", "an", "exception", "if", "not"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 190, "end_line": 207, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_264", "original_string": "def _disable_module_memory_sharing(data: Any) -> Any:\r\n    \"\"\"Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.\r\n\r\n    Note: This is only required when running on CPU.\r\n\r\n    \"\"\"\r\n\r\n    @torch.no_grad()\r\n    def unshare(module: Module) -> Module:\r\n        for tensor in itertools.chain(module.parameters(), module.buffers()):\r\n            tensor.data = tensor.data.clone()\r\n        return module\r\n\r\n    return apply_to_collection(data, function=unshare, dtype=Module)", "language": "python", "code": "def _disable_module_memory_sharing(data: Any) -> Any:\r\n    \"\"\"Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.\r\n\r\n    Note: This is only required when running on CPU.\r\n\r\n    \"\"\"\r\n\r\n    @torch.no_grad()\r\n    def unshare(module: Module) -> Module:\r\n        for tensor in itertools.chain(module.parameters(), module.buffers()):\r\n            tensor.data = tensor.data.clone()\r\n        return module\r\n\r\n    return apply_to_collection(data, function=unshare, dtype=Module)", "code_tokens": ["def", "_disable_module_memory_sharing", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "@", "torch", ".", "no_grad", "(", ")", "def", "unshare", "(", "module", ":", "Module", ")", "-", ">", "Module", ":", "for", "tensor", "in", "itertools", ".", "chain", "(", "module", ".", "parameters", "(", ")", ",", "module", ".", "buffers", "(", ")", ")", ":", "tensor", ".", "data", "=", "tensor", ".", "data", ".", "clone", "(", ")", "return", "module", "return", "apply_to_collection", "(", "data", ",", "function", "=", "unshare", ",", "dtype", "=", "Module", ")"], "docstring": "Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.", "docstring_tokens": ["disables", "memory", "sharing", "on", "parameters", "and", "buffers", "of", "nn", "module", "s", "contained", "in", "the", "given", "collection"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 210, "end_line": 226, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "function_265", "original_string": "def _check_missing_main_guard() -> None:\r\n    \"\"\"Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.\"\"\"\r\n    if not getattr(mp.current_process(), \"_inheriting\", False):\r\n        return\r\n    message = dedent(\r\n        \"\"\"\r\n        Launching multiple processes with the 'spawn' start method requires that your script guards the main\r\n        function with an `if __name__ == \\\"__main__\\\"` clause. For example:\r\n\r\n        def main():\r\n            ...\r\n\r\n        if __name__ == \"__main__\":\r\n            main()\r\n\r\n        Alternatively, you can run with `strategy=\"ddp\"` to avoid this error.\r\n        \"\"\"\r\n    )\r\n    raise RuntimeError(message)", "language": "python", "code": "def _check_missing_main_guard() -> None:\r\n    \"\"\"Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.\"\"\"\r\n    if not getattr(mp.current_process(), \"_inheriting\", False):\r\n        return\r\n    message = dedent(\r\n        \"\"\"\r\n        Launching multiple processes with the 'spawn' start method requires that your script guards the main\r\n        function with an `if __name__ == \\\"__main__\\\"` clause. For example:\r\n\r\n        def main():\r\n            ...\r\n\r\n        if __name__ == \"__main__\":\r\n            main()\r\n\r\n        Alternatively, you can run with `strategy=\"ddp\"` to avoid this error.\r\n        \"\"\"\r\n    )\r\n    raise RuntimeError(message)", "code_tokens": ["def", "_check_missing_main_guard", "(", ")", "-", ">", "None", ":", "STRING", "if", "not", "getattr", "(", "mp", ".", "current_process", "(", ")", ",", "STRING", ",", "False", ")", ":", "return", "message", "=", "dedent", "(", "STRING", ")", "raise", "RuntimeError", "(", "message", ")"], "docstring": "Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.", "docstring_tokens": ["raises", "an", "exception", "if", "the", "__name__", "__main__", "guard", "is", "missing"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "start_line": 229, "end_line": 248, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "function_266", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "self", ".", "cluster_environment", ".", "validate_settings", "(", "num_devices", "=", "self", ".", "num_processes", ",", "num_nodes", "=", "self", ".", "num_nodes", ")", "if", "not", "self", ".", "cluster_environment", ".", "creates_processes_externally", ":", "self", ".", "_call_children_scripts", "(", ")", "_launch_process_observer", "(", "self", ".", "procs", ")", "_set_num_threads_if_needed", "(", "num_processes", "=", "self", ".", "num_processes", ")", "return", "function", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Creates new processes, then calls the given function.", "docstring_tokens": ["creates", "new", "processes", "then", "calls", "the", "given", "function"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "start_line": 91, "end_line": 107, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "function_267", "original_string": "def _launch_process_observer(child_processes: list[subprocess.Popen]) -> None:\r\n    \"\"\"Launches a thread that runs along the main process and monitors the health of all processes.\"\"\"\r\n    _ChildProcessObserver(child_processes=child_processes, main_pid=os.getpid()).start()", "language": "python", "code": "def _launch_process_observer(child_processes: list[subprocess.Popen]) -> None:\r\n    \"\"\"Launches a thread that runs along the main process and monitors the health of all processes.\"\"\"\r\n    _ChildProcessObserver(child_processes=child_processes, main_pid=os.getpid()).start()", "code_tokens": ["def", "_launch_process_observer", "(", "child_processes", ":", "list", "[", "subprocess", ".", "Popen", "]", ")", "-", ">", "None", ":", "STRING", "_ChildProcessObserver", "(", "child_processes", "=", "child_processes", ",", "main_pid", "=", "os", ".", "getpid", "(", ")", ")", ".", "start", "(", ")"], "docstring": "Launches a thread that runs along the main process and monitors the health of all processes.", "docstring_tokens": ["launches", "a", "thread", "that", "runs", "along", "the", "main", "process", "and", "monitors", "the", "health", "of", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "start_line": 186, "end_line": 188, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "function_268", "original_string": "def _run(self) -> bool:\r\n        \"\"\"Runs once over all child processes to check whether they are still running.\"\"\"\r\n        for proc in self._child_processes:\r\n            proc.poll()\r\n\r\n        return_codes = [proc.returncode for proc in self._child_processes]\r\n        if all(return_code == 0 for return_code in return_codes):\r\n            return True\r\n\r\n        for i, proc in enumerate(self._child_processes):\r\n            if proc.returncode:\r\n                message = rank_prefixed_message(\r\n                    f\"Child process with PID {proc.pid} terminated with code {proc.returncode}.\"\r\n                    f\" Forcefully terminating all other processes to avoid zombies \ud83e\udddf\",\r\n                    rank=(i + 1),\r\n                )\r\n                _logger.info(message)\r\n                self._terminate_all()\r\n                return True\r\n\r\n        return False", "language": "python", "code": "def _run(self) -> bool:\r\n        \"\"\"Runs once over all child processes to check whether they are still running.\"\"\"\r\n        for proc in self._child_processes:\r\n            proc.poll()\r\n\r\n        return_codes = [proc.returncode for proc in self._child_processes]\r\n        if all(return_code == 0 for return_code in return_codes):\r\n            return True\r\n\r\n        for i, proc in enumerate(self._child_processes):\r\n            if proc.returncode:\r\n                message = rank_prefixed_message(\r\n                    f\"Child process with PID {proc.pid} terminated with code {proc.returncode}.\"\r\n                    f\" Forcefully terminating all other processes to avoid zombies \ud83e\udddf\",\r\n                    rank=(i + 1),\r\n                )\r\n                _logger.info(message)\r\n                self._terminate_all()\r\n                return True\r\n\r\n        return False", "code_tokens": ["def", "_run", "(", "self", ")", "-", ">", "bool", ":", "STRING", "for", "proc", "in", "self", ".", "_child_processes", ":", "proc", ".", "poll", "(", ")", "return_codes", "=", "[", "proc", ".", "returncode", "for", "proc", "in", "self", ".", "_child_processes", "]", "if", "all", "(", "return_code", "=", "=", "0", "for", "return_code", "in", "return_codes", ")", ":", "return", "True", "for", "i", ",", "proc", "in", "enumerate", "(", "self", ".", "_child_processes", ")", ":", "if", "proc", ".", "returncode", ":", "message", "=", "rank_prefixed_message", "(", "fSTRING", "fSTRING", ",", "rank", "=", "(", "i", "+", "1", ")", ",", ")", "_logger", ".", "info", "(", "message", ")", "self", ".", "_terminate_all", "(", ")", "return", "True", "return", "False"], "docstring": "Runs once over all child processes to check whether they are still running.", "docstring_tokens": ["runs", "once", "over", "all", "child", "processes", "to", "check", "whether", "they", "are", "still", "running"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "start_line": 207, "end_line": 227, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "function_269", "original_string": "def _terminate_all(self) -> None:\r\n        \"\"\"Terminates the main process and all its children.\"\"\"\r\n        for p in self._child_processes:\r\n            p.send_signal(self._termination_signal)\r\n        os.kill(self._main_pid, self._termination_signal)", "language": "python", "code": "def _terminate_all(self) -> None:\r\n        \"\"\"Terminates the main process and all its children.\"\"\"\r\n        for p in self._child_processes:\r\n            p.send_signal(self._termination_signal)\r\n        os.kill(self._main_pid, self._termination_signal)", "code_tokens": ["def", "_terminate_all", "(", "self", ")", "-", ">", "None", ":", "STRING", "for", "p", "in", "self", ".", "_child_processes", ":", "p", ".", "send_signal", "(", "self", ".", "_termination_signal", ")", "os", ".", "kill", "(", "self", ".", "_main_pid", ",", "self", ".", "_termination_signal", ")"], "docstring": "Terminates the main process and all its children.", "docstring_tokens": ["terminates", "the", "main", "process", "and", "all", "its", "children"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "start_line": 229, "end_line": 233, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\xla.py", "func_name": "function_270", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        return_queue: Union[queue.Queue, mp.SimpleQueue]\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            **spawn_kwargs,\r\n        )\r\n        return return_queue.get()", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        return_queue: Union[queue.Queue, mp.SimpleQueue]\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            **spawn_kwargs,\r\n        )\r\n        return return_queue.get()", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "return_queue", ":", "Union", "[", "queue", ".", "Queue", ",", "mp", ".", "SimpleQueue", "]", "return_queue", "=", "mp", ".", "Manager", "(", ")", ".", "Queue", "(", ")", "import", "torch_xla", ".", "distributed", ".", "xla_multiprocessing", "as", "xmp", "spawn_kwargs", "=", "{", "}", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", "if", "nprocs", "=", "=", "1", ":", "spawn_kwargs", "[", "STRING", "]", "=", "nprocs", "xmp", ".", "spawn", "(", "self", ".", "_wrapping_function", ",", "args", "=", "(", "function", ",", "args", ",", "kwargs", ",", "return_queue", ")", ",", "start_method", "=", "self", ".", "_start_method", ",", "*", "*", "spawn_kwargs", ",", ")", "return", "return_queue", ".", "get", "(", ")"], "docstring": "Launches processes that run the given function in parallel.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\strategies\\launchers\\xla.py", "start_line": 58, "end_line": 88, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\apply_func.py", "func_name": "function_271", "original_string": "def move_data_to_device(batch: Any, device: _DEVICE) -> Any:\r\n    \"\"\"Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be\r\n    moved and all other objects in the collection will be left untouched.\r\n\r\n    Args:\r\n        batch: A tensor or collection of tensors or anything that has a method ``.to(...)``.\r\n            See :func:`apply_to_collection` for a list of supported collection types.\r\n        device: The device to which the data should be moved\r\n\r\n    Return:\r\n        the same collection but with all contained tensors residing on the new device.\r\n\r\n    See Also:\r\n        - :meth:`torch.Tensor.to`\r\n        - :class:`torch.device`\r\n\r\n    \"\"\"\r\n    if isinstance(device, str):\r\n        device = torch.device(device)\r\n\r\n    def batch_to(data: Any) -> Any:\r\n        kwargs = {}\r\n        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:\r\n            kwargs[\"non_blocking\"] = True\r\n        data_output = data.to(device, **kwargs)\r\n        if data_output is not None:\r\n            return data_output\r\n        return data\r\n\r\n    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)", "language": "python", "code": "def move_data_to_device(batch: Any, device: _DEVICE) -> Any:\r\n    \"\"\"Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be\r\n    moved and all other objects in the collection will be left untouched.\r\n\r\n    Args:\r\n        batch: A tensor or collection of tensors or anything that has a method ``.to(...)``.\r\n            See :func:`apply_to_collection` for a list of supported collection types.\r\n        device: The device to which the data should be moved\r\n\r\n    Return:\r\n        the same collection but with all contained tensors residing on the new device.\r\n\r\n    See Also:\r\n        - :meth:`torch.Tensor.to`\r\n        - :class:`torch.device`\r\n\r\n    \"\"\"\r\n    if isinstance(device, str):\r\n        device = torch.device(device)\r\n\r\n    def batch_to(data: Any) -> Any:\r\n        kwargs = {}\r\n        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:\r\n            kwargs[\"non_blocking\"] = True\r\n        data_output = data.to(device, **kwargs)\r\n        if data_output is not None:\r\n            return data_output\r\n        return data\r\n\r\n    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)", "code_tokens": ["def", "move_data_to_device", "(", "batch", ":", "Any", ",", "device", ":", "_DEVICE", ")", "-", ">", "Any", ":", "STRING", "if", "isinstance", "(", "device", ",", "str", ")", ":", "device", "=", "torch", ".", "device", "(", "device", ")", "def", "batch_to", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "kwargs", "=", "{", "}", "if", "isinstance", "(", "data", ",", "Tensor", ")", "and", "isinstance", "(", "device", ",", "torch", ".", "device", ")", "and", "device", ".", "type", "not", "in", "_BLOCKING_DEVICE_TYPES", ":", "kwargs", "[", "STRING", "]", "=", "True", "data_output", "=", "data", ".", "to", "(", "device", ",", "*", "*", "kwargs", ")", "if", "data_output", "is", "not", "None", ":", "return", "data_output", "return", "data", "return", "apply_to_collection", "(", "batch", ",", "dtype", "=", "_TransferableDataType", ",", "function", "=", "batch_to", ")"], "docstring": "Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be", "docstring_tokens": ["transfers", "a", "collection", "of", "data", "to", "the", "given", "device", "any", "object", "that", "defines", "a", "method", "to", "device", "will", "be"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\apply_func.py", "start_line": 77, "end_line": 109, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\apply_func.py", "func_name": "function_272", "original_string": "def convert_tensors_to_scalars(data: Any) -> Any:\r\n    \"\"\"Recursively walk through a collection and convert single-item tensors to scalar values.\r\n\r\n    Raises:\r\n        ValueError:\r\n            If tensors inside ``metrics`` contains multiple elements, hence preventing conversion to a scalar.\r\n\r\n    \"\"\"\r\n\r\n    def to_item(value: Tensor) -> Union[int, float, bool]:\r\n        if value.numel() != 1:\r\n            raise ValueError(\r\n                f\"The metric `{value}` does not contain a single element, thus it cannot be converted to a scalar.\"\r\n            )\r\n        return value.item()\r\n\r\n    return apply_to_collection(data, Tensor, to_item)", "language": "python", "code": "def convert_tensors_to_scalars(data: Any) -> Any:\r\n    \"\"\"Recursively walk through a collection and convert single-item tensors to scalar values.\r\n\r\n    Raises:\r\n        ValueError:\r\n            If tensors inside ``metrics`` contains multiple elements, hence preventing conversion to a scalar.\r\n\r\n    \"\"\"\r\n\r\n    def to_item(value: Tensor) -> Union[int, float, bool]:\r\n        if value.numel() != 1:\r\n            raise ValueError(\r\n                f\"The metric `{value}` does not contain a single element, thus it cannot be converted to a scalar.\"\r\n            )\r\n        return value.item()\r\n\r\n    return apply_to_collection(data, Tensor, to_item)", "code_tokens": ["def", "convert_tensors_to_scalars", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "def", "to_item", "(", "value", ":", "Tensor", ")", "-", ">", "Union", "[", "int", ",", "float", ",", "bool", "]", ":", "if", "value", ".", "numel", "(", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "fSTRING", ")", "return", "value", ".", "item", "(", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "to_item", ")"], "docstring": "Recursively walk through a collection and convert single-item tensors to scalar values.", "docstring_tokens": ["recursively", "walk", "through", "a", "collection", "and", "convert", "single", "item", "tensors", "to", "scalar", "values"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\apply_func.py", "start_line": 119, "end_line": 135, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "function_273", "original_string": "def _load(\r\n    path_or_url: Union[IO, _PATH],\r\n    map_location: _MAP_LOCATION_TYPE = None,\r\n    weights_only: bool = False,\r\n) -> Any:\r\n    \"\"\"Loads a checkpoint.\r\n\r\n    Args:\r\n        path_or_url: Path or URL of the checkpoint.\r\n        map_location: a function, ``torch.device``, string or a dict specifying how to remap storage locations.\r\n\r\n    \"\"\"\r\n    if not isinstance(path_or_url, (str, Path)):\r\n        return torch.load(\r\n            path_or_url,\r\n            map_location=map_location,  # type: ignore[arg-type] # upstream annotation is not correct\r\n            weights_only=weights_only,\r\n        )\r\n    if str(path_or_url).startswith(\"http\"):\r\n        return torch.hub.load_state_dict_from_url(\r\n            str(path_or_url),\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )\r\n    fs = get_filesystem(path_or_url)\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n        return torch.load(\r\n            f,\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )", "language": "python", "code": "def _load(\r\n    path_or_url: Union[IO, _PATH],\r\n    map_location: _MAP_LOCATION_TYPE = None,\r\n    weights_only: bool = False,\r\n) -> Any:\r\n    \"\"\"Loads a checkpoint.\r\n\r\n    Args:\r\n        path_or_url: Path or URL of the checkpoint.\r\n        map_location: a function, ``torch.device``, string or a dict specifying how to remap storage locations.\r\n\r\n    \"\"\"\r\n    if not isinstance(path_or_url, (str, Path)):\r\n        return torch.load(\r\n            path_or_url,\r\n            map_location=map_location,  # type: ignore[arg-type] # upstream annotation is not correct\r\n            weights_only=weights_only,\r\n        )\r\n    if str(path_or_url).startswith(\"http\"):\r\n        return torch.hub.load_state_dict_from_url(\r\n            str(path_or_url),\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )\r\n    fs = get_filesystem(path_or_url)\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n        return torch.load(\r\n            f,\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )", "code_tokens": ["def", "_load", "(", "path_or_url", ":", "Union", "[", "IO", ",", "_PATH", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "weights_only", ":", "bool", "=", "False", ",", ")", "-", ">", "Any", ":", "STRING", "if", "not", "isinstance", "(", "path_or_url", ",", "(", "str", ",", "Path", ")", ")", ":", "return", "torch", ".", "load", "(", "path_or_url", ",", "map_location", "=", "map_location", ",", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", "#", "upstream", "annotation", "is", "not", "correct", "weights_only", "=", "weights_only", ",", ")", "if", "str", "(", "path_or_url", ")", ".", "startswith", "(", "STRING", ")", ":", "return", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "str", "(", "path_or_url", ")", ",", "map_location", "=", "map_location", ",", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", "weights_only", "=", "weights_only", ",", ")", "fs", "=", "get_filesystem", "(", "path_or_url", ")", "with", "fs", ".", "open", "(", "path_or_url", ",", "STRING", ")", "as", "f", ":", "return", "torch", ".", "load", "(", "f", ",", "map_location", "=", "map_location", ",", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", "weights_only", "=", "weights_only", ",", ")"], "docstring": "Loads a checkpoint.", "docstring_tokens": ["loads", "a", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "start_line": 33, "end_line": 64, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "function_274", "original_string": "def _atomic_save(checkpoint: dict[str, Any], filepath: Union[str, Path]) -> None:\r\n    \"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\r\n\r\n    Args:\r\n        checkpoint: The object to save.\r\n            Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\r\n            accepts.\r\n        filepath: The path to which the checkpoint will be saved.\r\n            This points to the file that the checkpoint will be stored in.\r\n\r\n    \"\"\"\r\n    bytesbuffer = io.BytesIO()\r\n    log.debug(f\"Saving checkpoint: {filepath}\")\r\n    torch.save(checkpoint, bytesbuffer)\r\n\r\n    try:\r\n        fs, urlpath = fsspec.core.url_to_fs(str(filepath))\r\n        with fs.transaction, fs.open(urlpath, \"wb\") as f:\r\n            f.write(bytesbuffer.getvalue())\r\n    except PermissionError as e:\r\n        if isinstance(e.__context__, OSError) and getattr(e.__context__, \"errno\", None) == errno.EXDEV:\r\n            raise RuntimeError(\r\n                'Upgrade fsspec to enable cross-device local checkpoints: pip install \"fsspec[http]>=2025.5.0\"',\r\n            ) from e", "language": "python", "code": "def _atomic_save(checkpoint: dict[str, Any], filepath: Union[str, Path]) -> None:\r\n    \"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\r\n\r\n    Args:\r\n        checkpoint: The object to save.\r\n            Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\r\n            accepts.\r\n        filepath: The path to which the checkpoint will be saved.\r\n            This points to the file that the checkpoint will be stored in.\r\n\r\n    \"\"\"\r\n    bytesbuffer = io.BytesIO()\r\n    log.debug(f\"Saving checkpoint: {filepath}\")\r\n    torch.save(checkpoint, bytesbuffer)\r\n\r\n    try:\r\n        fs, urlpath = fsspec.core.url_to_fs(str(filepath))\r\n        with fs.transaction, fs.open(urlpath, \"wb\") as f:\r\n            f.write(bytesbuffer.getvalue())\r\n    except PermissionError as e:\r\n        if isinstance(e.__context__, OSError) and getattr(e.__context__, \"errno\", None) == errno.EXDEV:\r\n            raise RuntimeError(\r\n                'Upgrade fsspec to enable cross-device local checkpoints: pip install \"fsspec[http]>=2025.5.0\"',\r\n            ) from e", "code_tokens": ["def", "_atomic_save", "(", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "filepath", ":", "Union", "[", "str", ",", "Path", "]", ")", "-", ">", "None", ":", "STRING", "bytesbuffer", "=", "io", ".", "BytesIO", "(", ")", "log", ".", "debug", "(", "fSTRING", ")", "torch", ".", "save", "(", "checkpoint", ",", "bytesbuffer", ")", "try", ":", "fs", ",", "urlpath", "=", "fsspec", ".", "core", ".", "url_to_fs", "(", "str", "(", "filepath", ")", ")", "with", "fs", ".", "transaction", ",", "fs", ".", "open", "(", "urlpath", ",", "STRING", ")", "as", "f", ":", "f", ".", "write", "(", "bytesbuffer", ".", "getvalue", "(", ")", ")", "except", "PermissionError", "as", "e", ":", "if", "isinstance", "(", "e", ".", "__context__", ",", "OSError", ")", "and", "getattr", "(", "e", ".", "__context__", ",", "STRING", ",", "None", ")", "=", "=", "errno", ".", "EXDEV", ":", "raise", "RuntimeError", "(", "STRING", ",", ")", "from", "e"], "docstring": "Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.", "docstring_tokens": ["saves", "a", "checkpoint", "atomically", "avoiding", "the", "creation", "of", "incomplete", "checkpoints"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "start_line": 72, "end_line": 96, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "function_275", "original_string": "def _is_dir(fs: AbstractFileSystem, path: Union[str, Path], strict: bool = False) -> bool:\r\n    \"\"\"Check if a path is directory-like.\r\n\r\n    This function determines if a given path is considered directory-like, taking into account the behavior\r\n    specific to object storage platforms. For other filesystems, it behaves similarly to the standard `fs.isdir`\r\n    method.\r\n\r\n    Args:\r\n        fs: The filesystem to check the path against.\r\n        path: The path or URL to be checked.\r\n        strict: A flag specific to Object Storage platforms. If set to ``False``, any non-existing path is considered\r\n            as a valid directory-like path. In such cases, the directory (and any non-existing parent directories)\r\n            will be created on the fly. Defaults to False.\r\n\r\n    \"\"\"\r\n    if _is_object_storage(fs):\r\n        if strict:\r\n            return fs.isdir(path)\r\n\r\n        return not fs.isfile(path)\r\n\r\n    return fs.isdir(path)", "language": "python", "code": "def _is_dir(fs: AbstractFileSystem, path: Union[str, Path], strict: bool = False) -> bool:\r\n    \"\"\"Check if a path is directory-like.\r\n\r\n    This function determines if a given path is considered directory-like, taking into account the behavior\r\n    specific to object storage platforms. For other filesystems, it behaves similarly to the standard `fs.isdir`\r\n    method.\r\n\r\n    Args:\r\n        fs: The filesystem to check the path against.\r\n        path: The path or URL to be checked.\r\n        strict: A flag specific to Object Storage platforms. If set to ``False``, any non-existing path is considered\r\n            as a valid directory-like path. In such cases, the directory (and any non-existing parent directories)\r\n            will be created on the fly. Defaults to False.\r\n\r\n    \"\"\"\r\n    if _is_object_storage(fs):\r\n        if strict:\r\n            return fs.isdir(path)\r\n\r\n        return not fs.isfile(path)\r\n\r\n    return fs.isdir(path)", "code_tokens": ["def", "_is_dir", "(", "fs", ":", "AbstractFileSystem", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "strict", ":", "bool", "=", "False", ")", "-", ">", "bool", ":", "STRING", "if", "_is_object_storage", "(", "fs", ")", ":", "if", "strict", ":", "return", "fs", ".", "isdir", "(", "path", ")", "return", "not", "fs", ".", "isfile", "(", "path", ")", "return", "fs", ".", "isdir", "(", "path", ")"], "docstring": "Check if a path is directory-like.", "docstring_tokens": ["check", "if", "a", "path", "is", "directory", "like"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "start_line": 121, "end_line": 148, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_276", "original_string": "def sized_len(dataloader: object) -> Optional[int]:\r\n    \"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\r\n    try:\r\n        length = len(dataloader)  # type: ignore [arg-type]\r\n    except (TypeError, NotImplementedError):\r\n        length = None\r\n    return length", "language": "python", "code": "def sized_len(dataloader: object) -> Optional[int]:\r\n    \"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\r\n    try:\r\n        length = len(dataloader)  # type: ignore [arg-type]\r\n    except (TypeError, NotImplementedError):\r\n        length = None\r\n    return length", "code_tokens": ["def", "sized_len", "(", "dataloader", ":", "object", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "try", ":", "length", "=", "len", "(", "dataloader", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", "except", "(", "TypeError", ",", "NotImplementedError", ")", ":", "length", "=", "None", "return", "length"], "docstring": "Try to get the length of an object, return ``None`` otherwise.", "docstring_tokens": ["try", "to", "get", "the", "length", "of", "an", "object", "return", "none", "otherwise"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 47, "end_line": 54, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_277", "original_string": "def has_len(dataloader: object) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented.\"\"\"\r\n    length = sized_len(dataloader)\r\n    if length == 0:\r\n        rank_zero_warn(\r\n            f\"`{dataloader.__class__.__name__}` returned 0 length. Please make sure this was your intention.\"\r\n        )\r\n    if length is not None and has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return length is not None", "language": "python", "code": "def has_len(dataloader: object) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented.\"\"\"\r\n    length = sized_len(dataloader)\r\n    if length == 0:\r\n        rank_zero_warn(\r\n            f\"`{dataloader.__class__.__name__}` returned 0 length. Please make sure this was your intention.\"\r\n        )\r\n    if length is not None and has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return length is not None", "code_tokens": ["def", "has_len", "(", "dataloader", ":", "object", ")", "-", ">", "TypeGuard", "[", "Sized", "]", ":", "STRING", "length", "=", "sized_len", "(", "dataloader", ")", "if", "length", "=", "=", "0", ":", "rank_zero_warn", "(", "fSTRING", ")", "if", "length", "is", "not", "None", "and", "has_iterable_dataset", "(", "dataloader", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "STRING", ")", "return", "length", "is", "not", "None"], "docstring": "Checks if a given object has ``__len__`` method implemented.", "docstring_tokens": ["checks", "if", "a", "given", "object", "has", "__len__", "method", "implemented"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 57, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_278", "original_string": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\"\"\"\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n\r\n    if batch_sampler is not None and type(batch_sampler) is not BatchSampler:\r\n        batch_sampler_cls = type(batch_sampler)\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=batch_sampler.drop_last,\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    raise\r\n\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                    \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                    \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        else:\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "language": "python", "code": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\"\"\"\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n\r\n    if batch_sampler is not None and type(batch_sampler) is not BatchSampler:\r\n        batch_sampler_cls = type(batch_sampler)\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=batch_sampler.drop_last,\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    raise\r\n\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                    \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                    \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        else:\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "code_tokens": ["def", "_dataloader_init_kwargs_resolve_sampler", "(", "dataloader", ":", "DataLoader", ",", "sampler", ":", "Union", "[", "Sampler", ",", "Iterable", "]", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "STRING", ")", "if", "batch_sampler", "is", "not", "None", "and", "type", "(", "batch_sampler", ")", "is", "not", "BatchSampler", ":", "batch_sampler_cls", "=", "type", "(", "batch_sampler", ")", "if", "hasattr", "(", "batch_sampler", ",", "STRING", ")", ":", "args", "=", "batch_sampler", ".", "__pl_saved_args", "kwargs", "=", "batch_sampler", ".", "__pl_saved_kwargs", "default_kwargs", "=", "batch_sampler", ".", "__pl_saved_default_kwargs", "arg_names", "=", "batch_sampler", ".", "__pl_saved_arg_names", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "STRING", ",", "sampler", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "STRING", ")", "batch_sampler", "=", "_reinstantiate_wrapped_cls", "(", "batch_sampler", ",", "*", "args", ",", "*", "*", "kwargs", ")", "elif", "hasattr", "(", "batch_sampler", ",", "STRING", ")", "and", "hasattr", "(", "batch_sampler", ",", "STRING", ")", ":", "try", ":", "batch_sampler", "=", "batch_sampler_cls", "(", "sampler", ",", "batch_size", "=", "batch_sampler", ".", "batch_size", ",", "drop_last", "=", "batch_sampler", ".", "drop_last", ",", ")", "except", "TypeError", "as", "ex", ":", "import", "re", "match", "=", "re", ".", "match", "(", "rSTRING", ",", "str", "(", "ex", ")", ")", "if", "not", "match", ":", "raise", "raise", "TypeError", "(", "STRING", "STRING", "STRING", "STRING", ")", "from", "ex", "else", ":", "raise", "TypeError", "(", "STRING", "STRING", "STRING", "STRING", ")", "return", "{", "STRING", ":", "None", ",", "STRING", ":", "False", ",", "STRING", ":", "batch_sampler", ",", "STRING", ":", "1", ",", "STRING", ":", "False", ",", "}", "return", "{", "STRING", ":", "sampler", ",", "STRING", ":", "False", ",", "STRING", ":", "None", "}"], "docstring": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-", "docstring_tokens": ["this", "function", "is", "used", "to", "handle", "the", "sampler", "batch_sampler", "arguments", "associated", "within", "a", "dataloader", "for", "its", "re"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 172, "end_line": 242, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_279", "original_string": "def _wrap_init_method(init: Callable, store_explicit_arg: Optional[str] = None) -> Callable:\r\n    \"\"\"Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and\r\n    :class:`~torch.utils.data.BatchSampler`) in order to enable re-instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(init)\r\n    def wrapper(obj: Any, *args: Any, **kwargs: Any) -> None:\r\n        old_inside_init = getattr(obj, \"__pl_inside_init\", False)\r\n        object.__setattr__(obj, \"__pl_inside_init\", True)\r\n        params = inspect.signature(init).parameters\r\n\r\n        parameters_defaults = OrderedDict(\r\n            (param.name, param.default)\r\n            for param in params.values()\r\n            if param.name != \"self\" and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)\r\n        )\r\n\r\n        param_names = tuple(parameters_defaults)[: len(args)]\r\n\r\n        default_kwargs = {\r\n            name: value\r\n            for name, value in parameters_defaults.items()\r\n            if name not in kwargs and name not in param_names and value != inspect.Parameter.empty\r\n        }\r\n\r\n        if not hasattr(obj, \"__pl_saved_args\"):\r\n            object.__setattr__(obj, \"__pl_saved_args\", args)\r\n            object.__setattr__(obj, \"__pl_saved_kwargs\", kwargs)\r\n            object.__setattr__(obj, \"__pl_saved_arg_names\", param_names)\r\n            object.__setattr__(obj, \"__pl_saved_default_kwargs\", default_kwargs)\r\n\r\n        if store_explicit_arg is not None:\r\n            if store_explicit_arg in param_names:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", args[param_names.index(store_explicit_arg)])\r\n            elif store_explicit_arg in kwargs:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", kwargs[store_explicit_arg])\r\n\r\n        init(obj, *args, **kwargs)\r\n        object.__setattr__(obj, \"__pl_inside_init\", old_inside_init)\r\n\r\n    return wrapper", "language": "python", "code": "def _wrap_init_method(init: Callable, store_explicit_arg: Optional[str] = None) -> Callable:\r\n    \"\"\"Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and\r\n    :class:`~torch.utils.data.BatchSampler`) in order to enable re-instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(init)\r\n    def wrapper(obj: Any, *args: Any, **kwargs: Any) -> None:\r\n        old_inside_init = getattr(obj, \"__pl_inside_init\", False)\r\n        object.__setattr__(obj, \"__pl_inside_init\", True)\r\n        params = inspect.signature(init).parameters\r\n\r\n        parameters_defaults = OrderedDict(\r\n            (param.name, param.default)\r\n            for param in params.values()\r\n            if param.name != \"self\" and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)\r\n        )\r\n\r\n        param_names = tuple(parameters_defaults)[: len(args)]\r\n\r\n        default_kwargs = {\r\n            name: value\r\n            for name, value in parameters_defaults.items()\r\n            if name not in kwargs and name not in param_names and value != inspect.Parameter.empty\r\n        }\r\n\r\n        if not hasattr(obj, \"__pl_saved_args\"):\r\n            object.__setattr__(obj, \"__pl_saved_args\", args)\r\n            object.__setattr__(obj, \"__pl_saved_kwargs\", kwargs)\r\n            object.__setattr__(obj, \"__pl_saved_arg_names\", param_names)\r\n            object.__setattr__(obj, \"__pl_saved_default_kwargs\", default_kwargs)\r\n\r\n        if store_explicit_arg is not None:\r\n            if store_explicit_arg in param_names:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", args[param_names.index(store_explicit_arg)])\r\n            elif store_explicit_arg in kwargs:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", kwargs[store_explicit_arg])\r\n\r\n        init(obj, *args, **kwargs)\r\n        object.__setattr__(obj, \"__pl_inside_init\", old_inside_init)\r\n\r\n    return wrapper", "code_tokens": ["def", "_wrap_init_method", "(", "init", ":", "Callable", ",", "store_explicit_arg", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "Callable", ":", "STRING", "@", "functools", ".", "wraps", "(", "init", ")", "def", "wrapper", "(", "obj", ":", "Any", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "old_inside_init", "=", "getattr", "(", "obj", ",", "STRING", ",", "False", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "True", ")", "params", "=", "inspect", ".", "signature", "(", "init", ")", ".", "parameters", "parameters_defaults", "=", "OrderedDict", "(", "(", "param", ".", "name", ",", "param", ".", "default", ")", "for", "param", "in", "params", ".", "values", "(", ")", "if", "param", ".", "name", "!", "=", "STRING", "and", "param", ".", "kind", "not", "in", "(", "param", ".", "VAR_POSITIONAL", ",", "param", ".", "VAR_KEYWORD", ")", ")", "param_names", "=", "tuple", "(", "parameters_defaults", ")", "[", ":", "len", "(", "args", ")", "]", "default_kwargs", "=", "{", "name", ":", "value", "for", "name", ",", "value", "in", "parameters_defaults", ".", "items", "(", ")", "if", "name", "not", "in", "kwargs", "and", "name", "not", "in", "param_names", "and", "value", "!", "=", "inspect", ".", "Parameter", ".", "empty", "}", "if", "not", "hasattr", "(", "obj", ",", "STRING", ")", ":", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "args", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "kwargs", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "param_names", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "default_kwargs", ")", "if", "store_explicit_arg", "is", "not", "None", ":", "if", "store_explicit_arg", "in", "param_names", ":", "object", ".", "__setattr__", "(", "obj", ",", "fSTRING", ",", "args", "[", "param_names", ".", "index", "(", "store_explicit_arg", ")", "]", ")", "elif", "store_explicit_arg", "in", "kwargs", ":", "object", ".", "__setattr__", "(", "obj", ",", "fSTRING", ",", "kwargs", "[", "store_explicit_arg", "]", ")", "init", "(", "obj", ",", "*", "args", ",", "*", "*", "kwargs", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "old_inside_init", ")", "return", "wrapper"], "docstring": "Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and", "docstring_tokens": ["wraps", "the", "__init__", "method", "of", "classes", "currently", "class", "torch", "utils", "data", "dataloader", "and"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 283, "end_line": 327, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_280", "original_string": "def _wrap_attr_method(method: Callable, tag: _WrapAttrTag) -> Callable:\r\n    \"\"\"Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`\r\n    and :class:`~torch.utils.data.BatchSampler`) in order to enable re- instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(method)\r\n    def wrapper(obj: Any, *args: Any) -> None:\r\n        name, *_ = args\r\n        prev_call_name, prev_call_method = getattr(obj, \"__pl_current_call\", (None, \"method\"))\r\n        first_call = not (prev_call_name == name and prev_call_method == tag)\r\n\r\n        object.__setattr__(obj, \"__pl_current_call\", (name, tag))\r\n\r\n        method(obj, *args)\r\n        if first_call and not getattr(obj, \"__pl_inside_init\", True):\r\n            attrs_record = getattr(obj, \"__pl_attrs_record\", [])\r\n            attrs_record.append((args, tag))\r\n            object.__setattr__(obj, \"__pl_attrs_record\", attrs_record)\r\n        object.__setattr__(obj, \"__pl_current_call\", (prev_call_name, prev_call_method))\r\n\r\n    return wrapper", "language": "python", "code": "def _wrap_attr_method(method: Callable, tag: _WrapAttrTag) -> Callable:\r\n    \"\"\"Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`\r\n    and :class:`~torch.utils.data.BatchSampler`) in order to enable re- instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(method)\r\n    def wrapper(obj: Any, *args: Any) -> None:\r\n        name, *_ = args\r\n        prev_call_name, prev_call_method = getattr(obj, \"__pl_current_call\", (None, \"method\"))\r\n        first_call = not (prev_call_name == name and prev_call_method == tag)\r\n\r\n        object.__setattr__(obj, \"__pl_current_call\", (name, tag))\r\n\r\n        method(obj, *args)\r\n        if first_call and not getattr(obj, \"__pl_inside_init\", True):\r\n            attrs_record = getattr(obj, \"__pl_attrs_record\", [])\r\n            attrs_record.append((args, tag))\r\n            object.__setattr__(obj, \"__pl_attrs_record\", attrs_record)\r\n        object.__setattr__(obj, \"__pl_current_call\", (prev_call_name, prev_call_method))\r\n\r\n    return wrapper", "code_tokens": ["def", "_wrap_attr_method", "(", "method", ":", "Callable", ",", "tag", ":", "_WrapAttrTag", ")", "-", ">", "Callable", ":", "STRING", "@", "functools", ".", "wraps", "(", "method", ")", "def", "wrapper", "(", "obj", ":", "Any", ",", "*", "args", ":", "Any", ")", "-", ">", "None", ":", "name", ",", "*", "_", "=", "args", "prev_call_name", ",", "prev_call_method", "=", "getattr", "(", "obj", ",", "STRING", ",", "(", "None", ",", "STRING", ")", ")", "first_call", "=", "not", "(", "prev_call_name", "=", "=", "name", "and", "prev_call_method", "=", "=", "tag", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "(", "name", ",", "tag", ")", ")", "method", "(", "obj", ",", "*", "args", ")", "if", "first_call", "and", "not", "getattr", "(", "obj", ",", "STRING", ",", "True", ")", ":", "attrs_record", "=", "getattr", "(", "obj", ",", "STRING", ",", "[", "]", ")", "attrs_record", ".", "append", "(", "(", "args", ",", "tag", ")", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "attrs_record", ")", "object", ".", "__setattr__", "(", "obj", ",", "STRING", ",", "(", "prev_call_name", ",", "prev_call_method", ")", ")", "return", "wrapper"], "docstring": "Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`", "docstring_tokens": ["wraps", "the", "__setattr__", "or", "__delattr__", "method", "of", "classes", "currently", "class", "torch", "utils", "data", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 330, "end_line": 354, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_281", "original_string": "def _replace_dunder_methods(base_cls: type, store_explicit_arg: Optional[str] = None) -> Generator[None, None, None]:\r\n    \"\"\"This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.\r\n\r\n    It patches the ``__init__``, ``__setattr__`` and ``__delattr__`` methods.\r\n\r\n    \"\"\"\r\n    classes = get_all_subclasses(base_cls) | {base_cls}\r\n    for cls in classes:\r\n        if \"__init__\" in cls.__dict__:\r\n            cls.__old__init__ = cls.__init__  # type: ignore[misc]\r\n            cls.__init__ = _wrap_init_method(cls.__init__, store_explicit_arg)  # type: ignore[misc]\r\n\r\n        for patch_fn_name, tag in ((\"__setattr__\", _WrapAttrTag.SET), (\"__delattr__\", _WrapAttrTag.DEL)):\r\n            if patch_fn_name in cls.__dict__ or cls is base_cls:\r\n                saved_name = f\"__old{patch_fn_name}\"\r\n                setattr(cls, saved_name, getattr(cls, patch_fn_name))\r\n                setattr(cls, patch_fn_name, _wrap_attr_method(getattr(cls, patch_fn_name), tag))\r\n    yield\r\n    for cls in classes:\r\n        for patched_name in (\"__setattr__\", \"__delattr__\", \"__init__\"):\r\n            if f\"__old{patched_name}\" in cls.__dict__:\r\n                setattr(cls, patched_name, getattr(cls, f\"__old{patched_name}\"))\r\n                delattr(cls, f\"__old{patched_name}\")", "language": "python", "code": "def _replace_dunder_methods(base_cls: type, store_explicit_arg: Optional[str] = None) -> Generator[None, None, None]:\r\n    \"\"\"This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.\r\n\r\n    It patches the ``__init__``, ``__setattr__`` and ``__delattr__`` methods.\r\n\r\n    \"\"\"\r\n    classes = get_all_subclasses(base_cls) | {base_cls}\r\n    for cls in classes:\r\n        if \"__init__\" in cls.__dict__:\r\n            cls.__old__init__ = cls.__init__  # type: ignore[misc]\r\n            cls.__init__ = _wrap_init_method(cls.__init__, store_explicit_arg)  # type: ignore[misc]\r\n\r\n        for patch_fn_name, tag in ((\"__setattr__\", _WrapAttrTag.SET), (\"__delattr__\", _WrapAttrTag.DEL)):\r\n            if patch_fn_name in cls.__dict__ or cls is base_cls:\r\n                saved_name = f\"__old{patch_fn_name}\"\r\n                setattr(cls, saved_name, getattr(cls, patch_fn_name))\r\n                setattr(cls, patch_fn_name, _wrap_attr_method(getattr(cls, patch_fn_name), tag))\r\n    yield\r\n    for cls in classes:\r\n        for patched_name in (\"__setattr__\", \"__delattr__\", \"__init__\"):\r\n            if f\"__old{patched_name}\" in cls.__dict__:\r\n                setattr(cls, patched_name, getattr(cls, f\"__old{patched_name}\"))\r\n                delattr(cls, f\"__old{patched_name}\")", "code_tokens": ["def", "_replace_dunder_methods", "(", "base_cls", ":", "type", ",", "store_explicit_arg", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "classes", "=", "get_all_subclasses", "(", "base_cls", ")", "|", "{", "base_cls", "}", "for", "cls", "in", "classes", ":", "if", "STRING", "in", "cls", ".", "__dict__", ":", "cls", ".", "__old__init__", "=", "cls", ".", "__init__", "#", "type", ":", "ignore", "[", "misc", "]", "cls", ".", "__init__", "=", "_wrap_init_method", "(", "cls", ".", "__init__", ",", "store_explicit_arg", ")", "#", "type", ":", "ignore", "[", "misc", "]", "for", "patch_fn_name", ",", "tag", "in", "(", "(", "STRING", ",", "_WrapAttrTag", ".", "SET", ")", ",", "(", "STRING", ",", "_WrapAttrTag", ".", "DEL", ")", ")", ":", "if", "patch_fn_name", "in", "cls", ".", "__dict__", "or", "cls", "is", "base_cls", ":", "saved_name", "=", "fSTRING", "setattr", "(", "cls", ",", "saved_name", ",", "getattr", "(", "cls", ",", "patch_fn_name", ")", ")", "setattr", "(", "cls", ",", "patch_fn_name", ",", "_wrap_attr_method", "(", "getattr", "(", "cls", ",", "patch_fn_name", ")", ",", "tag", ")", ")", "yield", "for", "cls", "in", "classes", ":", "for", "patched_name", "in", "(", "STRING", ",", "STRING", ",", "STRING", ")", ":", "if", "fSTRING", "in", "cls", ".", "__dict__", ":", "setattr", "(", "cls", ",", "patched_name", ",", "getattr", "(", "cls", ",", "fSTRING", ")", ")", "delattr", "(", "cls", ",", "fSTRING", ")"], "docstring": "This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.", "docstring_tokens": ["this", "context", "manager", "is", "used", "to", "add", "support", "for", "re", "instantiation", "of", "custom", "subclasses", "of", "base_cls"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 358, "end_line": 386, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_282", "original_string": "def _replace_value_in_saved_args(\r\n    replace_key: str,\r\n    replace_value: Any,\r\n    args: tuple[Any, ...],\r\n    kwargs: dict[str, Any],\r\n    default_kwargs: dict[str, Any],\r\n    arg_names: tuple[str, ...],\r\n) -> tuple[bool, tuple[Any, ...], dict[str, Any]]:\r\n    \"\"\"Tries to replace an argument value in a saved list of args and kwargs.\r\n\r\n    Returns a tuple indicating success of the operation and modified saved args and kwargs\r\n\r\n    \"\"\"\r\n\r\n    if replace_key in arg_names:\r\n        replace_index = arg_names.index(replace_key)\r\n        args = args[:replace_index] + (replace_value,) + args[replace_index + 1 :]\r\n        return True, args, kwargs\r\n    if replace_key in kwargs or replace_key in default_kwargs:\r\n        kwargs[replace_key] = replace_value\r\n        return True, args, kwargs\r\n\r\n    return False, args, kwargs", "language": "python", "code": "def _replace_value_in_saved_args(\r\n    replace_key: str,\r\n    replace_value: Any,\r\n    args: tuple[Any, ...],\r\n    kwargs: dict[str, Any],\r\n    default_kwargs: dict[str, Any],\r\n    arg_names: tuple[str, ...],\r\n) -> tuple[bool, tuple[Any, ...], dict[str, Any]]:\r\n    \"\"\"Tries to replace an argument value in a saved list of args and kwargs.\r\n\r\n    Returns a tuple indicating success of the operation and modified saved args and kwargs\r\n\r\n    \"\"\"\r\n\r\n    if replace_key in arg_names:\r\n        replace_index = arg_names.index(replace_key)\r\n        args = args[:replace_index] + (replace_value,) + args[replace_index + 1 :]\r\n        return True, args, kwargs\r\n    if replace_key in kwargs or replace_key in default_kwargs:\r\n        kwargs[replace_key] = replace_value\r\n        return True, args, kwargs\r\n\r\n    return False, args, kwargs", "code_tokens": ["def", "_replace_value_in_saved_args", "(", "replace_key", ":", "str", ",", "replace_value", ":", "Any", ",", "args", ":", "tuple", "[", "Any", ",", ".", ".", ".", "]", ",", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "default_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "arg_names", ":", "tuple", "[", "str", ",", ".", ".", ".", "]", ",", ")", "-", ">", "tuple", "[", "bool", ",", "tuple", "[", "Any", ",", ".", ".", ".", "]", ",", "dict", "[", "str", ",", "Any", "]", "]", ":", "STRING", "if", "replace_key", "in", "arg_names", ":", "replace_index", "=", "arg_names", ".", "index", "(", "replace_key", ")", "args", "=", "args", "[", ":", "replace_index", "]", "+", "(", "replace_value", ",", ")", "+", "args", "[", "replace_index", "+", "1", ":", "]", "return", "True", ",", "args", ",", "kwargs", "if", "replace_key", "in", "kwargs", "or", "replace_key", "in", "default_kwargs", ":", "kwargs", "[", "replace_key", "]", "=", "replace_value", "return", "True", ",", "args", ",", "kwargs", "return", "False", ",", "args", ",", "kwargs"], "docstring": "Tries to replace an argument value in a saved list of args and kwargs.", "docstring_tokens": ["tries", "to", "replace", "an", "argument", "value", "in", "a", "saved", "list", "of", "args", "and", "kwargs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 389, "end_line": 411, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_283", "original_string": "def _set_sampler_epoch(dataloader: object, epoch: int) -> None:\r\n    \"\"\"Calls the ``set_epoch`` method on either the sampler of the given dataloader.\r\n\r\n    Every PyTorch dataloader has either a sampler or a batch sampler. If the sampler is wrapped by a\r\n    :class:`~torch.utils.data.distributed.DistributedSampler`, ``set_epoch`` must be called at the beginning\r\n    of every epoch to ensure shuffling applies a new ordering. This has no effect if shuffling is off.\r\n\r\n    \"\"\"\r\n    objects: dict[int, Any] = {}\r\n    if (sampler := getattr(dataloader, \"sampler\", None)) is not None:\r\n        objects[id(sampler)] = sampler\r\n    if (batch_sampler := getattr(dataloader, \"batch_sampler\", None)) is not None and (\r\n        sampler := getattr(batch_sampler, \"sampler\", None)\r\n    ) is not None:\r\n        objects[id(sampler)] = sampler\r\n    for obj in objects.values():\r\n        set_epoch = getattr(obj, \"set_epoch\", None)\r\n        if callable(set_epoch):\r\n            set_epoch(epoch)", "language": "python", "code": "def _set_sampler_epoch(dataloader: object, epoch: int) -> None:\r\n    \"\"\"Calls the ``set_epoch`` method on either the sampler of the given dataloader.\r\n\r\n    Every PyTorch dataloader has either a sampler or a batch sampler. If the sampler is wrapped by a\r\n    :class:`~torch.utils.data.distributed.DistributedSampler`, ``set_epoch`` must be called at the beginning\r\n    of every epoch to ensure shuffling applies a new ordering. This has no effect if shuffling is off.\r\n\r\n    \"\"\"\r\n    objects: dict[int, Any] = {}\r\n    if (sampler := getattr(dataloader, \"sampler\", None)) is not None:\r\n        objects[id(sampler)] = sampler\r\n    if (batch_sampler := getattr(dataloader, \"batch_sampler\", None)) is not None and (\r\n        sampler := getattr(batch_sampler, \"sampler\", None)\r\n    ) is not None:\r\n        objects[id(sampler)] = sampler\r\n    for obj in objects.values():\r\n        set_epoch = getattr(obj, \"set_epoch\", None)\r\n        if callable(set_epoch):\r\n            set_epoch(epoch)", "code_tokens": ["def", "_set_sampler_epoch", "(", "dataloader", ":", "object", ",", "epoch", ":", "int", ")", "-", ">", "None", ":", "STRING", "objects", ":", "dict", "[", "int", ",", "Any", "]", "=", "{", "}", "if", "(", "sampler", ":", "=", "getattr", "(", "dataloader", ",", "STRING", ",", "None", ")", ")", "is", "not", "None", ":", "objects", "[", "id", "(", "sampler", ")", "]", "=", "sampler", "if", "(", "batch_sampler", ":", "=", "getattr", "(", "dataloader", ",", "STRING", ",", "None", ")", ")", "is", "not", "None", "and", "(", "sampler", ":", "=", "getattr", "(", "batch_sampler", ",", "STRING", ",", "None", ")", ")", "is", "not", "None", ":", "objects", "[", "id", "(", "sampler", ")", "]", "=", "sampler", "for", "obj", "in", "objects", ".", "values", "(", ")", ":", "set_epoch", "=", "getattr", "(", "obj", ",", "STRING", ",", "None", ")", "if", "callable", "(", "set_epoch", ")", ":", "set_epoch", "(", "epoch", ")"], "docstring": "Calls the ``set_epoch`` method on either the sampler of the given dataloader.", "docstring_tokens": ["calls", "the", "set_epoch", "method", "on", "either", "the", "sampler", "of", "the", "given", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 414, "end_line": 435, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "function_284", "original_string": "def suggested_max_num_workers(local_world_size: int) -> int:\r\n    \"\"\"Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on\r\n    the number of CPU cores available on the system and the number of distributed processes in the current machine.\r\n\r\n    Args:\r\n        local_world_size: The number of distributed processes running on the current machine. Set this to the number\r\n            of devices configured in Fabric/Trainer.\r\n\r\n    \"\"\"\r\n    if local_world_size < 1:\r\n        raise ValueError(f\"`local_world_size` should be >= 1, got {local_world_size}.\")\r\n    cpu_count = _num_cpus_available()\r\n    return max(1, cpu_count // local_world_size - 1)  # -1 to leave some resources for main process\r", "language": "python", "code": "def suggested_max_num_workers(local_world_size: int) -> int:\r\n    \"\"\"Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on\r\n    the number of CPU cores available on the system and the number of distributed processes in the current machine.\r\n\r\n    Args:\r\n        local_world_size: The number of distributed processes running on the current machine. Set this to the number\r\n            of devices configured in Fabric/Trainer.\r\n\r\n    \"\"\"\r\n    if local_world_size < 1:\r\n        raise ValueError(f\"`local_world_size` should be >= 1, got {local_world_size}.\")\r\n    cpu_count = _num_cpus_available()\r\n    return max(1, cpu_count // local_world_size - 1)  # -1 to leave some resources for main process\r", "code_tokens": ["def", "suggested_max_num_workers", "(", "local_world_size", ":", "int", ")", "-", ">", "int", ":", "STRING", "if", "local_world_size", "<", "1", ":", "raise", "ValueError", "(", "fSTRING", ")", "cpu_count", "=", "_num_cpus_available", "(", ")", "return", "max", "(", "1", ",", "cpu_count", "/", "/", "local_world_size", "-", "1", ")", "#", "-", "1", "to", "leave", "some", "resources", "for", "main", "process"], "docstring": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on", "docstring_tokens": ["suggests", "an", "upper", "bound", "of", "num_workers", "to", "use", "in", "a", "pytorch", "class", "torch", "utils", "data", "dataloader", "based", "on"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\data.py", "start_line": 438, "end_line": 450, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_285", "original_string": "def to(self, *args: Any, **kwargs: Any) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.to`.\"\"\"\r\n        device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\r\n        _update_properties(self, device=device, dtype=dtype)\r\n        return super().to(*args, **kwargs)", "language": "python", "code": "def to(self, *args: Any, **kwargs: Any) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.to`.\"\"\"\r\n        device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\r\n        _update_properties(self, device=device, dtype=dtype)\r\n        return super().to(*args, **kwargs)", "code_tokens": ["def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Self", ":", "STRING", "device", ",", "dtype", "=", "torch", ".", "_C", ".", "_nn", ".", "_parse_to", "(", "*", "args", ",", "*", "*", "kwargs", ")", "[", ":", "2", "]", "_update_properties", "(", "self", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "return", "super", "(", ")", ".", "to", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "See :meth:`torch.nn.Module.to`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 53, "end_line": 58, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_286", "original_string": "def cuda(self, device: Optional[Union[torch.device, int]] = None) -> Self:\r\n        \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\r\n        different objects. So it should be called before constructing optimizer if the module will live on GPU while\r\n        being optimized.\r\n\r\n        Arguments:\r\n            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\r\n                index will be used.\r\n\r\n        Returns:\r\n            Module: self\r\n\r\n        \"\"\"\r\n        if device is None:\r\n            device = torch.device(\"cuda\", torch.cuda.current_device())\r\n        elif isinstance(device, int):\r\n            device = torch.device(\"cuda\", index=device)\r\n        _update_properties(self, device=device)\r\n        return super().cuda(device=device)", "language": "python", "code": "def cuda(self, device: Optional[Union[torch.device, int]] = None) -> Self:\r\n        \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\r\n        different objects. So it should be called before constructing optimizer if the module will live on GPU while\r\n        being optimized.\r\n\r\n        Arguments:\r\n            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\r\n                index will be used.\r\n\r\n        Returns:\r\n            Module: self\r\n\r\n        \"\"\"\r\n        if device is None:\r\n            device = torch.device(\"cuda\", torch.cuda.current_device())\r\n        elif isinstance(device, int):\r\n            device = torch.device(\"cuda\", index=device)\r\n        _update_properties(self, device=device)\r\n        return super().cuda(device=device)", "code_tokens": ["def", "cuda", "(", "self", ",", "device", ":", "Optional", "[", "Union", "[", "torch", ".", "device", ",", "int", "]", "]", "=", "None", ")", "-", ">", "Self", ":", "STRING", "if", "device", "is", "None", ":", "device", "=", "torch", ".", "device", "(", "STRING", ",", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "elif", "isinstance", "(", "device", ",", "int", ")", ":", "device", "=", "torch", ".", "device", "(", "STRING", ",", "index", "=", "device", ")", "_update_properties", "(", "self", ",", "device", "=", "device", ")", "return", "super", "(", ")", ".", "cuda", "(", "device", "=", "device", ")"], "docstring": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers", "docstring_tokens": ["moves", "all", "model", "parameters", "and", "buffers", "to", "the", "gpu", "this", "also", "makes", "associated", "parameters", "and", "buffers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 61, "end_line": 79, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_287", "original_string": "def cpu(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\r\n        _update_properties(self, device=torch.device(\"cpu\"))\r\n        return super().cpu()", "language": "python", "code": "def cpu(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\r\n        _update_properties(self, device=torch.device(\"cpu\"))\r\n        return super().cpu()", "code_tokens": ["def", "cpu", "(", "self", ")", "-", ">", "Self", ":", "STRING", "_update_properties", "(", "self", ",", "device", "=", "torch", ".", "device", "(", "STRING", ")", ")", "return", "super", "(", ")", ".", "cpu", "(", ")"], "docstring": "See :meth:`torch.nn.Module.cpu`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "cpu"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 82, "end_line": 85, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_288", "original_string": "def type(self, dst_type: Union[str, torch.dtype]) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.type`.\"\"\"\r\n        _update_properties(self, dtype=dst_type)\r\n        return super().type(dst_type=dst_type)", "language": "python", "code": "def type(self, dst_type: Union[str, torch.dtype]) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.type`.\"\"\"\r\n        _update_properties(self, dtype=dst_type)\r\n        return super().type(dst_type=dst_type)", "code_tokens": ["def", "type", "(", "self", ",", "dst_type", ":", "Union", "[", "str", ",", "torch", ".", "dtype", "]", ")", "-", ">", "Self", ":", "STRING", "_update_properties", "(", "self", ",", "dtype", "=", "dst_type", ")", "return", "super", "(", ")", ".", "type", "(", "dst_type", "=", "dst_type", ")"], "docstring": "See :meth:`torch.nn.Module.type`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "type"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 88, "end_line": 91, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_289", "original_string": "def float(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.float`.\"\"\"\r\n        _update_properties(self, dtype=torch.float)\r\n        return super().float()", "language": "python", "code": "def float(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.float`.\"\"\"\r\n        _update_properties(self, dtype=torch.float)\r\n        return super().float()", "code_tokens": ["def", "float", "(", "self", ")", "-", ">", "Self", ":", "STRING", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "float", ")", "return", "super", "(", ")", ".", "float", "(", ")"], "docstring": "See :meth:`torch.nn.Module.float`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "float"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 94, "end_line": 97, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_290", "original_string": "def double(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.double`.\"\"\"\r\n        _update_properties(self, dtype=torch.double)\r\n        return super().double()", "language": "python", "code": "def double(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.double`.\"\"\"\r\n        _update_properties(self, dtype=torch.double)\r\n        return super().double()", "code_tokens": ["def", "double", "(", "self", ")", "-", ">", "Self", ":", "STRING", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "double", ")", "return", "super", "(", ")", ".", "double", "(", ")"], "docstring": "See :meth:`torch.nn.Module.double`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "double"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 100, "end_line": 103, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "function_291", "original_string": "def half(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.half`.\"\"\"\r\n        _update_properties(self, dtype=torch.half)\r\n        return super().half()", "language": "python", "code": "def half(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.half`.\"\"\"\r\n        _update_properties(self, dtype=torch.half)\r\n        return super().half()", "code_tokens": ["def", "half", "(", "self", ")", "-", ">", "Self", ":", "STRING", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "half", ")", "return", "super", "(", ")", ".", "half", "(", ")"], "docstring": "See :meth:`torch.nn.Module.half`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "half"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "start_line": 106, "end_line": 109, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_292", "original_string": "def _determine_root_gpu_device(gpus: list[_DEVICE]) -> Optional[_DEVICE]:\r\n    \"\"\"\r\n    Args:\r\n        gpus: Non-empty list of ints representing which GPUs to use\r\n\r\n    Returns:\r\n        Designated root GPU device id\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``gpus`` is not a list\r\n        AssertionError:\r\n            If GPU list is empty\r\n    \"\"\"\r\n    if gpus is None:\r\n        return None\r\n\r\n    if not isinstance(gpus, list):\r\n        raise TypeError(\"GPUs should be a list\")\r\n\r\n    assert len(gpus) > 0, \"GPUs should be a non-empty list\"\r\n\r\n    return gpus[0]", "language": "python", "code": "def _determine_root_gpu_device(gpus: list[_DEVICE]) -> Optional[_DEVICE]:\r\n    \"\"\"\r\n    Args:\r\n        gpus: Non-empty list of ints representing which GPUs to use\r\n\r\n    Returns:\r\n        Designated root GPU device id\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``gpus`` is not a list\r\n        AssertionError:\r\n            If GPU list is empty\r\n    \"\"\"\r\n    if gpus is None:\r\n        return None\r\n\r\n    if not isinstance(gpus, list):\r\n        raise TypeError(\"GPUs should be a list\")\r\n\r\n    assert len(gpus) > 0, \"GPUs should be a non-empty list\"\r\n\r\n    return gpus[0]", "code_tokens": ["def", "_determine_root_gpu_device", "(", "gpus", ":", "list", "[", "_DEVICE", "]", ")", "-", ">", "Optional", "[", "_DEVICE", "]", ":", "STRING", "if", "gpus", "is", "None", ":", "return", "None", "if", "not", "isinstance", "(", "gpus", ",", "list", ")", ":", "raise", "TypeError", "(", "STRING", ")", "assert", "len", "(", "gpus", ")", ">", "0", ",", "STRING", "return", "gpus", "[", "0", "]"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 22, "end_line": 45, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_293", "original_string": "def _parse_gpu_ids(\r\n    gpus: Optional[Union[int, str, list[int]]],\r\n    include_cuda: bool = False,\r\n    include_mps: bool = False,\r\n) -> Optional[list[int]]:\r\n    \"\"\"Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        gpus: An int -1 or string '-1' indicate that all available GPUs should be used.\r\n            A list of unique ints or a string containing a list of comma separated unique integers\r\n            indicates specific GPUs to use.\r\n            An int of 0 means that no GPUs should be used.\r\n            Any int N > 0 indicates that GPUs [0..N) should be used.\r\n        include_cuda: A boolean value indicating whether to include CUDA devices for GPU parsing.\r\n        include_mps: A boolean value indicating whether to include MPS devices for GPU parsing.\r\n\r\n    Returns:\r\n        A list of GPUs to be used or ``None`` if no GPUs were requested\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If no GPUs are available but the value of gpus variable indicates request for GPUs\r\n\r\n    .. note::\r\n        ``include_cuda`` and ``include_mps`` default to ``False`` so that you only\r\n        have to specify which device type to use and all other devices are not disabled.\r\n\r\n    \"\"\"\r\n    _check_data_type(gpus)\r\n\r\n    if gpus is None or (isinstance(gpus, int) and gpus == 0) or str(gpus).strip() in (\"0\", \"[]\"):\r\n        return None\r\n\r\n    gpus = _normalize_parse_gpu_string_input(gpus)\r\n    gpus = _normalize_parse_gpu_input_to_list(gpus, include_cuda=include_cuda, include_mps=include_mps)\r\n    if not gpus:\r\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\r\n\r\n    if (\r\n        torch.distributed.is_available()\r\n        and torch.distributed.is_torchelastic_launched()\r\n        and len(gpus) != 1\r\n        and len(_get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)) == 1\r\n    ):\r\n        return gpus\r\n\r\n    _check_unique(gpus)\r\n\r\n    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)", "language": "python", "code": "def _parse_gpu_ids(\r\n    gpus: Optional[Union[int, str, list[int]]],\r\n    include_cuda: bool = False,\r\n    include_mps: bool = False,\r\n) -> Optional[list[int]]:\r\n    \"\"\"Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        gpus: An int -1 or string '-1' indicate that all available GPUs should be used.\r\n            A list of unique ints or a string containing a list of comma separated unique integers\r\n            indicates specific GPUs to use.\r\n            An int of 0 means that no GPUs should be used.\r\n            Any int N > 0 indicates that GPUs [0..N) should be used.\r\n        include_cuda: A boolean value indicating whether to include CUDA devices for GPU parsing.\r\n        include_mps: A boolean value indicating whether to include MPS devices for GPU parsing.\r\n\r\n    Returns:\r\n        A list of GPUs to be used or ``None`` if no GPUs were requested\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If no GPUs are available but the value of gpus variable indicates request for GPUs\r\n\r\n    .. note::\r\n        ``include_cuda`` and ``include_mps`` default to ``False`` so that you only\r\n        have to specify which device type to use and all other devices are not disabled.\r\n\r\n    \"\"\"\r\n    _check_data_type(gpus)\r\n\r\n    if gpus is None or (isinstance(gpus, int) and gpus == 0) or str(gpus).strip() in (\"0\", \"[]\"):\r\n        return None\r\n\r\n    gpus = _normalize_parse_gpu_string_input(gpus)\r\n    gpus = _normalize_parse_gpu_input_to_list(gpus, include_cuda=include_cuda, include_mps=include_mps)\r\n    if not gpus:\r\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\r\n\r\n    if (\r\n        torch.distributed.is_available()\r\n        and torch.distributed.is_torchelastic_launched()\r\n        and len(gpus) != 1\r\n        and len(_get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)) == 1\r\n    ):\r\n        return gpus\r\n\r\n    _check_unique(gpus)\r\n\r\n    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)", "code_tokens": ["def", "_parse_gpu_ids", "(", "gpus", ":", "Optional", "[", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", "]", ",", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ",", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "STRING", "_check_data_type", "(", "gpus", ")", "if", "gpus", "is", "None", "or", "(", "isinstance", "(", "gpus", ",", "int", ")", "and", "gpus", "=", "=", "0", ")", "or", "str", "(", "gpus", ")", ".", "strip", "(", ")", "in", "(", "STRING", ",", "STRING", ")", ":", "return", "None", "gpus", "=", "_normalize_parse_gpu_string_input", "(", "gpus", ")", "gpus", "=", "_normalize_parse_gpu_input_to_list", "(", "gpus", ",", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", "if", "not", "gpus", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "(", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_torchelastic_launched", "(", ")", "and", "len", "(", "gpus", ")", "!", "=", "1", "and", "len", "(", "_get_all_available_gpus", "(", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", ")", "=", "=", "1", ")", ":", "return", "gpus", "_check_unique", "(", "gpus", ")", "return", "_sanitize_gpu_ids", "(", "gpus", ",", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")"], "docstring": "Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.", "docstring_tokens": ["parses", "the", "gpu", "ids", "given", "in", "the", "format", "as", "accepted", "by", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 48, "end_line": 102, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_294", "original_string": "def _sanitize_gpu_ids(gpus: list[int], include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the\r\n    GPUs is not available.\r\n\r\n    Args:\r\n        gpus: List of ints corresponding to GPU indices\r\n\r\n    Returns:\r\n        Unmodified gpus variable\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If machine has fewer available GPUs than requested.\r\n\r\n    \"\"\"\r\n    if sum((include_cuda, include_mps)) == 0:\r\n        raise ValueError(\"At least one gpu type should be specified!\")\r\n    all_available_gpus = _get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)\r\n    for gpu in gpus:\r\n        if gpu not in all_available_gpus:\r\n            raise MisconfigurationException(\r\n                f\"You requested gpu: {gpus}\\n But your machine only has: {all_available_gpus}\"\r\n            )\r\n    return gpus", "language": "python", "code": "def _sanitize_gpu_ids(gpus: list[int], include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the\r\n    GPUs is not available.\r\n\r\n    Args:\r\n        gpus: List of ints corresponding to GPU indices\r\n\r\n    Returns:\r\n        Unmodified gpus variable\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If machine has fewer available GPUs than requested.\r\n\r\n    \"\"\"\r\n    if sum((include_cuda, include_mps)) == 0:\r\n        raise ValueError(\"At least one gpu type should be specified!\")\r\n    all_available_gpus = _get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)\r\n    for gpu in gpus:\r\n        if gpu not in all_available_gpus:\r\n            raise MisconfigurationException(\r\n                f\"You requested gpu: {gpus}\\n But your machine only has: {all_available_gpus}\"\r\n            )\r\n    return gpus", "code_tokens": ["def", "_sanitize_gpu_ids", "(", "gpus", ":", "list", "[", "int", "]", ",", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "if", "sum", "(", "(", "include_cuda", ",", "include_mps", ")", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "STRING", ")", "all_available_gpus", "=", "_get_all_available_gpus", "(", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", "for", "gpu", "in", "gpus", ":", "if", "gpu", "not", "in", "all_available_gpus", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "return", "gpus"], "docstring": "Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the", "docstring_tokens": ["checks", "that", "each", "of", "the", "gpus", "in", "the", "list", "is", "actually", "available", "raises", "a", "misconfigurationexception", "if", "any", "of", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 115, "end_line": 138, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_295", "original_string": "def _get_all_available_gpus(include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available GPUs\r\n    \"\"\"\r\n    from lightning.fabric.accelerators.cuda import _get_all_visible_cuda_devices\r\n    from lightning.fabric.accelerators.mps import _get_all_available_mps_gpus\r\n\r\n    cuda_gpus = _get_all_visible_cuda_devices() if include_cuda else []\r\n    mps_gpus = _get_all_available_mps_gpus() if include_mps else []\r\n    return cuda_gpus + mps_gpus", "language": "python", "code": "def _get_all_available_gpus(include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available GPUs\r\n    \"\"\"\r\n    from lightning.fabric.accelerators.cuda import _get_all_visible_cuda_devices\r\n    from lightning.fabric.accelerators.mps import _get_all_available_mps_gpus\r\n\r\n    cuda_gpus = _get_all_visible_cuda_devices() if include_cuda else []\r\n    mps_gpus = _get_all_available_mps_gpus() if include_mps else []\r\n    return cuda_gpus + mps_gpus", "code_tokens": ["def", "_get_all_available_gpus", "(", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "cuda", "import", "_get_all_visible_cuda_devices", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "mps", "import", "_get_all_available_mps_gpus", "cuda_gpus", "=", "_get_all_visible_cuda_devices", "(", ")", "if", "include_cuda", "else", "[", "]", "mps_gpus", "=", "_get_all_available_mps_gpus", "(", ")", "if", "include_mps", "else", "[", "]", "return", "cuda_gpus", "+", "mps_gpus"], "docstring": "Returns:", "docstring_tokens": ["returns"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 157, "end_line": 167, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_296", "original_string": "def _check_unique(device_ids: list[int]) -> None:\r\n    \"\"\"Checks that the device_ids are unique.\r\n\r\n    Args:\r\n        device_ids: List of ints corresponding to GPUs indices\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If ``device_ids`` of GPUs aren't unique\r\n\r\n    \"\"\"\r\n    if len(device_ids) != len(set(device_ids)):\r\n        raise MisconfigurationException(\"Device ID's (GPU) must be unique.\")", "language": "python", "code": "def _check_unique(device_ids: list[int]) -> None:\r\n    \"\"\"Checks that the device_ids are unique.\r\n\r\n    Args:\r\n        device_ids: List of ints corresponding to GPUs indices\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If ``device_ids`` of GPUs aren't unique\r\n\r\n    \"\"\"\r\n    if len(device_ids) != len(set(device_ids)):\r\n        raise MisconfigurationException(\"Device ID's (GPU) must be unique.\")", "code_tokens": ["def", "_check_unique", "(", "device_ids", ":", "list", "[", "int", "]", ")", "-", ">", "None", ":", "STRING", "if", "len", "(", "device_ids", ")", "!", "=", "len", "(", "set", "(", "device_ids", ")", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "Checks that the device_ids are unique.", "docstring_tokens": ["checks", "that", "the", "device_ids", "are", "unique"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 170, "end_line": 182, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_297", "original_string": "def _check_data_type(device_ids: object) -> None:\r\n    \"\"\"Checks that the device_ids argument is one of the following: int, string, or sequence of integers.\r\n\r\n    Args:\r\n        device_ids: gpus/tpu_cores parameter as passed to the Trainer\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``device_ids`` of GPU/TPUs aren't ``int``, ``str`` or sequence of ``int```\r\n\r\n    \"\"\"\r\n    msg = \"Device IDs (GPU/TPU) must be an int, a string, a sequence of ints, but you passed\"\r\n    if device_ids is None:\r\n        raise TypeError(f\"{msg} None\")\r\n    if isinstance(device_ids, (MutableSequence, tuple)):\r\n        for id_ in device_ids:\r\n            id_type = type(id_)  # because `isinstance(False, int)` -> True\r\n            if id_type is not int:\r\n                raise TypeError(f\"{msg} a sequence of {type(id_).__name__}.\")\r\n    elif type(device_ids) not in (int, str):\r\n        raise TypeError(f\"{msg} {device_ids!r}.\")", "language": "python", "code": "def _check_data_type(device_ids: object) -> None:\r\n    \"\"\"Checks that the device_ids argument is one of the following: int, string, or sequence of integers.\r\n\r\n    Args:\r\n        device_ids: gpus/tpu_cores parameter as passed to the Trainer\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``device_ids`` of GPU/TPUs aren't ``int``, ``str`` or sequence of ``int```\r\n\r\n    \"\"\"\r\n    msg = \"Device IDs (GPU/TPU) must be an int, a string, a sequence of ints, but you passed\"\r\n    if device_ids is None:\r\n        raise TypeError(f\"{msg} None\")\r\n    if isinstance(device_ids, (MutableSequence, tuple)):\r\n        for id_ in device_ids:\r\n            id_type = type(id_)  # because `isinstance(False, int)` -> True\r\n            if id_type is not int:\r\n                raise TypeError(f\"{msg} a sequence of {type(id_).__name__}.\")\r\n    elif type(device_ids) not in (int, str):\r\n        raise TypeError(f\"{msg} {device_ids!r}.\")", "code_tokens": ["def", "_check_data_type", "(", "device_ids", ":", "object", ")", "-", ">", "None", ":", "STRING", "msg", "=", "STRING", "if", "device_ids", "is", "None", ":", "raise", "TypeError", "(", "fSTRING", ")", "if", "isinstance", "(", "device_ids", ",", "(", "MutableSequence", ",", "tuple", ")", ")", ":", "for", "id_", "in", "device_ids", ":", "id_type", "=", "type", "(", "id_", ")", "#", "because", "`", "isinstance", "(", "False", ",", "int", ")", "`", "-", ">", "True", "if", "id_type", "is", "not", "int", ":", "raise", "TypeError", "(", "fSTRING", ")", "elif", "type", "(", "device_ids", ")", "not", "in", "(", "int", ",", "str", ")", ":", "raise", "TypeError", "(", "fSTRING", ")"], "docstring": "Checks that the device_ids argument is one of the following: int, string, or sequence of integers.", "docstring_tokens": ["checks", "that", "the", "device_ids", "argument", "is", "one", "of", "the", "following", "int", "string", "or", "sequence", "of", "integers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 185, "end_line": 205, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "function_298", "original_string": "def _select_auto_accelerator() -> str:\r\n    \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n    from lightning.fabric.accelerators.cuda import CUDAAccelerator\r\n    from lightning.fabric.accelerators.mps import MPSAccelerator\r\n    from lightning.fabric.accelerators.xla import XLAAccelerator\r\n\r\n    if XLAAccelerator.is_available():\r\n        return \"tpu\"\r\n    if MPSAccelerator.is_available():\r\n        return \"mps\"\r\n    if CUDAAccelerator.is_available():\r\n        return \"cuda\"\r\n    return \"cpu\"", "language": "python", "code": "def _select_auto_accelerator() -> str:\r\n    \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n    from lightning.fabric.accelerators.cuda import CUDAAccelerator\r\n    from lightning.fabric.accelerators.mps import MPSAccelerator\r\n    from lightning.fabric.accelerators.xla import XLAAccelerator\r\n\r\n    if XLAAccelerator.is_available():\r\n        return \"tpu\"\r\n    if MPSAccelerator.is_available():\r\n        return \"mps\"\r\n    if CUDAAccelerator.is_available():\r\n        return \"cuda\"\r\n    return \"cpu\"", "code_tokens": ["def", "_select_auto_accelerator", "(", ")", "-", ">", "str", ":", "STRING", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "cuda", "import", "CUDAAccelerator", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "mps", "import", "MPSAccelerator", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "xla", "import", "XLAAccelerator", "if", "XLAAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "if", "MPSAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "if", "CUDAAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "return", "STRING"], "docstring": "Choose the accelerator type (str) based on availability.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\device_parser.py", "start_line": 208, "end_line": 220, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_299", "original_string": "def is_shared_filesystem(strategy: \"Strategy\", path: Optional[_PATH] = None, timeout: int = 3) -> bool:\r\n    \"\"\"Checks whether the filesystem under the given path is shared across all processes.\r\n\r\n    This function should only be used in a context where distributed is initialized.\r\n\r\n    Args:\r\n        strategy: The strategy being used, either from Fabric (``fabric.strategy``) or from Trainer\r\n            (``trainer.strategy``).\r\n        path: The path to check. Defaults to the current working directory. The user must have permissions to write\r\n            to this path or the parent folder, and the filesystem must be writable.\r\n        timeout: If any of the processes can't list the file created by rank 0 within this many seconds, the\r\n            filesystem is determined to be not shared.\r\n\r\n    \"\"\"\r\n    if path is not None and not _is_local_file_protocol(path):\r\n        return True\r\n\r\n    path = Path(Path.cwd() if path is None else path).resolve()\r\n\r\n    if not hasattr(strategy, \"world_size\") or strategy.world_size == 1:\r\n        return True\r\n\r\n    rank_zero_path = strategy.broadcast(path)\r\n    if not strategy.reduce_boolean_decision(rank_zero_path == path, all=True):\r\n        return False\r\n\r\n    if not strategy.reduce_boolean_decision(path.exists(), all=True):\r\n        raise FileNotFoundError(\r\n            f\"Unable to determine if the path belongs to a shared filesystem. The path does not exist: {path}\"\r\n        )\r\n\r\n    path = path.parent if path.is_file() else path\r\n    check_file = path / \".lightning_shared_fs_check\"\r\n    check_file.unlink(missing_ok=True)\r\n\r\n    strategy.barrier()\r\n    if strategy.is_global_zero:\r\n        check_file.touch()\r\n        found = True\r\n    else:\r\n        start = time.perf_counter()\r\n        found = False\r\n        while not found and (time.perf_counter() - start) < timeout:\r\n            found = check_file.exists()\r\n    strategy.barrier()\r\n\r\n    all_found = strategy.reduce_boolean_decision(found, all=True)\r\n\r\n    with contextlib.suppress(OSError):  # handle race condition on deletion\r\n        check_file.unlink()\r\n\r\n    return all_found", "language": "python", "code": "def is_shared_filesystem(strategy: \"Strategy\", path: Optional[_PATH] = None, timeout: int = 3) -> bool:\r\n    \"\"\"Checks whether the filesystem under the given path is shared across all processes.\r\n\r\n    This function should only be used in a context where distributed is initialized.\r\n\r\n    Args:\r\n        strategy: The strategy being used, either from Fabric (``fabric.strategy``) or from Trainer\r\n            (``trainer.strategy``).\r\n        path: The path to check. Defaults to the current working directory. The user must have permissions to write\r\n            to this path or the parent folder, and the filesystem must be writable.\r\n        timeout: If any of the processes can't list the file created by rank 0 within this many seconds, the\r\n            filesystem is determined to be not shared.\r\n\r\n    \"\"\"\r\n    if path is not None and not _is_local_file_protocol(path):\r\n        return True\r\n\r\n    path = Path(Path.cwd() if path is None else path).resolve()\r\n\r\n    if not hasattr(strategy, \"world_size\") or strategy.world_size == 1:\r\n        return True\r\n\r\n    rank_zero_path = strategy.broadcast(path)\r\n    if not strategy.reduce_boolean_decision(rank_zero_path == path, all=True):\r\n        return False\r\n\r\n    if not strategy.reduce_boolean_decision(path.exists(), all=True):\r\n        raise FileNotFoundError(\r\n            f\"Unable to determine if the path belongs to a shared filesystem. The path does not exist: {path}\"\r\n        )\r\n\r\n    path = path.parent if path.is_file() else path\r\n    check_file = path / \".lightning_shared_fs_check\"\r\n    check_file.unlink(missing_ok=True)\r\n\r\n    strategy.barrier()\r\n    if strategy.is_global_zero:\r\n        check_file.touch()\r\n        found = True\r\n    else:\r\n        start = time.perf_counter()\r\n        found = False\r\n        while not found and (time.perf_counter() - start) < timeout:\r\n            found = check_file.exists()\r\n    strategy.barrier()\r\n\r\n    all_found = strategy.reduce_boolean_decision(found, all=True)\r\n\r\n    with contextlib.suppress(OSError):  # handle race condition on deletion\r\n        check_file.unlink()\r\n\r\n    return all_found", "code_tokens": ["def", "is_shared_filesystem", "(", "strategy", ":", "STRING", ",", "path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "timeout", ":", "int", "=", "3", ")", "-", ">", "bool", ":", "STRING", "if", "path", "is", "not", "None", "and", "not", "_is_local_file_protocol", "(", "path", ")", ":", "return", "True", "path", "=", "Path", "(", "Path", ".", "cwd", "(", ")", "if", "path", "is", "None", "else", "path", ")", ".", "resolve", "(", ")", "if", "not", "hasattr", "(", "strategy", ",", "STRING", ")", "or", "strategy", ".", "world_size", "=", "=", "1", ":", "return", "True", "rank_zero_path", "=", "strategy", ".", "broadcast", "(", "path", ")", "if", "not", "strategy", ".", "reduce_boolean_decision", "(", "rank_zero_path", "=", "=", "path", ",", "all", "=", "True", ")", ":", "return", "False", "if", "not", "strategy", ".", "reduce_boolean_decision", "(", "path", ".", "exists", "(", ")", ",", "all", "=", "True", ")", ":", "raise", "FileNotFoundError", "(", "fSTRING", ")", "path", "=", "path", ".", "parent", "if", "path", ".", "is_file", "(", ")", "else", "path", "check_file", "=", "path", "/", "STRING", "check_file", ".", "unlink", "(", "missing_ok", "=", "True", ")", "strategy", ".", "barrier", "(", ")", "if", "strategy", ".", "is_global_zero", ":", "check_file", ".", "touch", "(", ")", "found", "=", "True", "else", ":", "start", "=", "time", ".", "perf_counter", "(", ")", "found", "=", "False", "while", "not", "found", "and", "(", "time", ".", "perf_counter", "(", ")", "-", "start", ")", "<", "timeout", ":", "found", "=", "check_file", ".", "exists", "(", ")", "strategy", ".", "barrier", "(", ")", "all_found", "=", "strategy", ".", "reduce_boolean_decision", "(", "found", ",", "all", "=", "True", ")", "with", "contextlib", ".", "suppress", "(", "OSError", ")", ":", "#", "handle", "race", "condition", "on", "deletion", "check_file", ".", "unlink", "(", ")", "return", "all_found"], "docstring": "Checks whether the filesystem under the given path is shared across all processes.", "docstring_tokens": ["checks", "whether", "the", "filesystem", "under", "the", "given", "path", "is", "shared", "across", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 43, "end_line": 99, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_300", "original_string": "def _gather_all_tensors(result: Tensor, group: Optional[Any] = None) -> list[Tensor]:\r\n    \"\"\"Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.\r\n\r\n    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case\r\n    tensors are padded, gathered and then trimmed to secure equal workload for all processes.\r\n\r\n    Args:\r\n        result: The value to sync\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n\r\n    Return:\r\n        gathered_result: List with size equal to the process group where\r\n            gathered_result[i] corresponds to result tensor from process i\r\n\r\n    \"\"\"\r\n    if group is None:\r\n        group = torch.distributed.group.WORLD\r\n\r\n    result = result.contiguous()\r\n\r\n    world_size = torch.distributed.get_world_size(group)\r\n    torch.distributed.barrier(group=group)\r\n\r\n    if result.ndim == 0:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    local_size = torch.tensor(result.shape, device=result.device)\r\n    local_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]\r\n    torch.distributed.all_gather(local_sizes, local_size, group=group)\r\n    max_size = torch.stack(local_sizes).max(dim=0).values\r\n    all_sizes_equal = all(all(ls == max_size) for ls in local_sizes)\r\n\r\n    if all_sizes_equal:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    pad_dims = []\r\n    pad_by = (max_size - local_size).detach().cpu()\r\n    for val in reversed(pad_by):\r\n        pad_dims.append(0)\r\n        pad_dims.append(val.item())\r\n    result_padded = F.pad(result, pad_dims)\r\n    gathered_result = [torch.zeros_like(result_padded) for _ in range(world_size)]\r\n    torch.distributed.all_gather(gathered_result, result_padded, group)\r\n    for idx, item_size in enumerate(local_sizes):\r\n        slice_param = [slice(dim_size) for dim_size in item_size]\r\n        gathered_result[idx] = gathered_result[idx][slice_param]\r\n    return gathered_result", "language": "python", "code": "def _gather_all_tensors(result: Tensor, group: Optional[Any] = None) -> list[Tensor]:\r\n    \"\"\"Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.\r\n\r\n    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case\r\n    tensors are padded, gathered and then trimmed to secure equal workload for all processes.\r\n\r\n    Args:\r\n        result: The value to sync\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n\r\n    Return:\r\n        gathered_result: List with size equal to the process group where\r\n            gathered_result[i] corresponds to result tensor from process i\r\n\r\n    \"\"\"\r\n    if group is None:\r\n        group = torch.distributed.group.WORLD\r\n\r\n    result = result.contiguous()\r\n\r\n    world_size = torch.distributed.get_world_size(group)\r\n    torch.distributed.barrier(group=group)\r\n\r\n    if result.ndim == 0:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    local_size = torch.tensor(result.shape, device=result.device)\r\n    local_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]\r\n    torch.distributed.all_gather(local_sizes, local_size, group=group)\r\n    max_size = torch.stack(local_sizes).max(dim=0).values\r\n    all_sizes_equal = all(all(ls == max_size) for ls in local_sizes)\r\n\r\n    if all_sizes_equal:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    pad_dims = []\r\n    pad_by = (max_size - local_size).detach().cpu()\r\n    for val in reversed(pad_by):\r\n        pad_dims.append(0)\r\n        pad_dims.append(val.item())\r\n    result_padded = F.pad(result, pad_dims)\r\n    gathered_result = [torch.zeros_like(result_padded) for _ in range(world_size)]\r\n    torch.distributed.all_gather(gathered_result, result_padded, group)\r\n    for idx, item_size in enumerate(local_sizes):\r\n        slice_param = [slice(dim_size) for dim_size in item_size]\r\n        gathered_result[idx] = gathered_result[idx][slice_param]\r\n    return gathered_result", "code_tokens": ["def", "_gather_all_tensors", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "list", "[", "Tensor", "]", ":", "STRING", "if", "group", "is", "None", ":", "group", "=", "torch", ".", "distributed", ".", "group", ".", "WORLD", "result", "=", "result", ".", "contiguous", "(", ")", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", ")", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "group", ")", "if", "result", ".", "ndim", "=", "=", "0", ":", "return", "_simple_gather_all_tensors", "(", "result", ",", "group", ",", "world_size", ")", "local_size", "=", "torch", ".", "tensor", "(", "result", ".", "shape", ",", "device", "=", "result", ".", "device", ")", "local_sizes", "=", "[", "torch", ".", "zeros_like", "(", "local_size", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "torch", ".", "distributed", ".", "all_gather", "(", "local_sizes", ",", "local_size", ",", "group", "=", "group", ")", "max_size", "=", "torch", ".", "stack", "(", "local_sizes", ")", ".", "max", "(", "dim", "=", "0", ")", ".", "values", "all_sizes_equal", "=", "all", "(", "all", "(", "ls", "=", "=", "max_size", ")", "for", "ls", "in", "local_sizes", ")", "if", "all_sizes_equal", ":", "return", "_simple_gather_all_tensors", "(", "result", ",", "group", ",", "world_size", ")", "pad_dims", "=", "[", "]", "pad_by", "=", "(", "max_size", "-", "local_size", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "for", "val", "in", "reversed", "(", "pad_by", ")", ":", "pad_dims", ".", "append", "(", "0", ")", "pad_dims", ".", "append", "(", "val", ".", "item", "(", ")", ")", "result_padded", "=", "F", ".", "pad", "(", "result", ",", "pad_dims", ")", "gathered_result", "=", "[", "torch", ".", "zeros_like", "(", "result_padded", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "torch", ".", "distributed", ".", "all_gather", "(", "gathered_result", ",", "result_padded", ",", "group", ")", "for", "idx", ",", "item_size", "in", "enumerate", "(", "local_sizes", ")", ":", "slice_param", "=", "[", "slice", "(", "dim_size", ")", "for", "dim_size", "in", "item_size", "]", "gathered_result", "[", "idx", "]", "=", "gathered_result", "[", "idx", "]", "[", "slice_param", "]", "return", "gathered_result"], "docstring": "Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.", "docstring_tokens": ["function", "to", "gather", "all", "tensors", "from", "several", "ddp", "processes", "onto", "a", "list", "that", "is", "broadcasted", "to", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 102, "end_line": 153, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_301", "original_string": "def _sync_ddp_if_available(\r\n    result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None\r\n) -> Tensor:\r\n    \"\"\"Function to reduce a tensor across worker processes during distributed training.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        reduced value\r\n\r\n    \"\"\"\r\n    if _distributed_is_initialized():\r\n        return _sync_ddp(result, group=group, reduce_op=reduce_op)\r\n    return result", "language": "python", "code": "def _sync_ddp_if_available(\r\n    result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None\r\n) -> Tensor:\r\n    \"\"\"Function to reduce a tensor across worker processes during distributed training.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        reduced value\r\n\r\n    \"\"\"\r\n    if _distributed_is_initialized():\r\n        return _sync_ddp(result, group=group, reduce_op=reduce_op)\r\n    return result", "code_tokens": ["def", "_sync_ddp_if_available", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "None", ")", "-", ">", "Tensor", ":", "STRING", "if", "_distributed_is_initialized", "(", ")", ":", "return", "_sync_ddp", "(", "result", ",", "group", "=", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "result"], "docstring": "Function to reduce a tensor across worker processes during distributed training.", "docstring_tokens": ["function", "to", "reduce", "a", "tensor", "across", "worker", "processes", "during", "distributed", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 162, "end_line": 179, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_302", "original_string": "def _sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None) -> Tensor:\r\n    \"\"\"Reduces a tensor across several distributed processes.\r\n\r\n    This operation is performed in-place, meaning the result will be placed back into the input tensor on all processes.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        The reduced value.\r\n\r\n    \"\"\"\r\n    divide_by_world_size = False\r\n    group = torch.distributed.group.WORLD if group is None else group\r\n\r\n    op: Optional[ReduceOp]\r\n    if isinstance(reduce_op, str):\r\n        reduce_op = \"avg\" if reduce_op == \"mean\" else reduce_op\r\n        if reduce_op.lower() == \"avg\" and torch.distributed.get_backend(group) == \"gloo\":\r\n            op = ReduceOp.SUM  # type: ignore[assignment]\r\n            divide_by_world_size = True\r\n        else:\r\n            op = getattr(ReduceOp, reduce_op.upper())\r\n    else:\r\n        op = reduce_op\r\n\r\n    if (\r\n        package_available(\"habana_frameworks\")\r\n        and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"\r\n        and result.type()\r\n        in (\r\n            \"torch.LongTensor\",\r\n            \"torch.hpu.LongTensor\",\r\n        )\r\n    ):\r\n        rank_zero_info(\"Long tensor unsupported on HPU, casting to float\")\r\n        result = result.float()\r\n\r\n    torch.distributed.barrier(group=group)\r\n    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)\r\n    world_size = torch.distributed.get_world_size(group)\r\n\r\n    if not divide_by_world_size:\r\n        return result\r\n    if not torch.is_floating_point(result):\r\n        return result.copy_(result / world_size)\r\n    return result.div_(world_size)", "language": "python", "code": "def _sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None) -> Tensor:\r\n    \"\"\"Reduces a tensor across several distributed processes.\r\n\r\n    This operation is performed in-place, meaning the result will be placed back into the input tensor on all processes.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        The reduced value.\r\n\r\n    \"\"\"\r\n    divide_by_world_size = False\r\n    group = torch.distributed.group.WORLD if group is None else group\r\n\r\n    op: Optional[ReduceOp]\r\n    if isinstance(reduce_op, str):\r\n        reduce_op = \"avg\" if reduce_op == \"mean\" else reduce_op\r\n        if reduce_op.lower() == \"avg\" and torch.distributed.get_backend(group) == \"gloo\":\r\n            op = ReduceOp.SUM  # type: ignore[assignment]\r\n            divide_by_world_size = True\r\n        else:\r\n            op = getattr(ReduceOp, reduce_op.upper())\r\n    else:\r\n        op = reduce_op\r\n\r\n    if (\r\n        package_available(\"habana_frameworks\")\r\n        and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"\r\n        and result.type()\r\n        in (\r\n            \"torch.LongTensor\",\r\n            \"torch.hpu.LongTensor\",\r\n        )\r\n    ):\r\n        rank_zero_info(\"Long tensor unsupported on HPU, casting to float\")\r\n        result = result.float()\r\n\r\n    torch.distributed.barrier(group=group)\r\n    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)\r\n    world_size = torch.distributed.get_world_size(group)\r\n\r\n    if not divide_by_world_size:\r\n        return result\r\n    if not torch.is_floating_point(result):\r\n        return result.copy_(result / world_size)\r\n    return result.div_(world_size)", "code_tokens": ["def", "_sync_ddp", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "None", ")", "-", ">", "Tensor", ":", "STRING", "divide_by_world_size", "=", "False", "group", "=", "torch", ".", "distributed", ".", "group", ".", "WORLD", "if", "group", "is", "None", "else", "group", "op", ":", "Optional", "[", "ReduceOp", "]", "if", "isinstance", "(", "reduce_op", ",", "str", ")", ":", "reduce_op", "=", "STRING", "if", "reduce_op", "=", "=", "STRING", "else", "reduce_op", "if", "reduce_op", ".", "lower", "(", ")", "=", "=", "STRING", "and", "torch", ".", "distributed", ".", "get_backend", "(", "group", ")", "=", "=", "STRING", ":", "op", "=", "ReduceOp", ".", "SUM", "#", "type", ":", "ignore", "[", "assignment", "]", "divide_by_world_size", "=", "True", "else", ":", "op", "=", "getattr", "(", "ReduceOp", ",", "reduce_op", ".", "upper", "(", ")", ")", "else", ":", "op", "=", "reduce_op", "if", "(", "package_available", "(", "STRING", ")", "and", "os", ".", "environ", ".", "get", "(", "STRING", ")", "=", "=", "STRING", "and", "result", ".", "type", "(", ")", "in", "(", "STRING", ",", "STRING", ",", ")", ")", ":", "rank_zero_info", "(", "STRING", ")", "result", "=", "result", ".", "float", "(", ")", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "group", ")", "torch", ".", "distributed", ".", "all_reduce", "(", "result", ",", "op", "=", "op", ",", "group", "=", "group", ",", "async_op", "=", "False", ")", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", ")", "if", "not", "divide_by_world_size", ":", "return", "result", "if", "not", "torch", ".", "is_floating_point", "(", "result", ")", ":", "return", "result", ".", "copy_", "(", "result", "/", "world_size", ")", "return", "result", ".", "div_", "(", "world_size", ")"], "docstring": "Reduces a tensor across several distributed processes.", "docstring_tokens": ["reduces", "a", "tensor", "across", "several", "distributed", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 182, "end_line": 237, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_303", "original_string": "def _all_gather_ddp_if_available(\r\n    tensor: Tensor, group: Optional[\"torch.distributed.ProcessGroup\"] = None, sync_grads: bool = False\r\n) -> Tensor:\r\n    \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n    Args:\r\n        tensor: Tensor of shape (batch, ...)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        sync_grads: Flag that allows users to synchronize gradients for all_gather op\r\n\r\n    Return:\r\n        A tensor of shape (world_size, batch, ...)\r\n\r\n    \"\"\"\r\n    if not _distributed_is_initialized():\r\n        return tensor\r\n\r\n    from torch.distributed.nn.functional import all_gather\r\n\r\n    tensor = tensor.contiguous()  # https://github.com/pytorch/pytorch/issues/73515\r\n    with nullcontext() if sync_grads else torch.no_grad():\r\n        gathered_tensors = all_gather(tensor, group)\r\n    return torch.stack(gathered_tensors)", "language": "python", "code": "def _all_gather_ddp_if_available(\r\n    tensor: Tensor, group: Optional[\"torch.distributed.ProcessGroup\"] = None, sync_grads: bool = False\r\n) -> Tensor:\r\n    \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n    Args:\r\n        tensor: Tensor of shape (batch, ...)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        sync_grads: Flag that allows users to synchronize gradients for all_gather op\r\n\r\n    Return:\r\n        A tensor of shape (world_size, batch, ...)\r\n\r\n    \"\"\"\r\n    if not _distributed_is_initialized():\r\n        return tensor\r\n\r\n    from torch.distributed.nn.functional import all_gather\r\n\r\n    tensor = tensor.contiguous()  # https://github.com/pytorch/pytorch/issues/73515\r\n    with nullcontext() if sync_grads else torch.no_grad():\r\n        gathered_tensors = all_gather(tensor, group)\r\n    return torch.stack(gathered_tensors)", "code_tokens": ["def", "_all_gather_ddp_if_available", "(", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "if", "not", "_distributed_is_initialized", "(", ")", ":", "return", "tensor", "from", "torch", ".", "distributed", ".", "nn", ".", "functional", "import", "all_gather", "tensor", "=", "tensor", ".", "contiguous", "(", ")", "#", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "issues", "/", "73515", "with", "nullcontext", "(", ")", "if", "sync_grads", "else", "torch", ".", "no_grad", "(", ")", ":", "gathered_tensors", "=", "all_gather", "(", "tensor", ",", "group", ")", "return", "torch", ".", "stack", "(", "gathered_tensors", ")"], "docstring": "Function to gather a tensor from several distributed processes.", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 240, "end_line": 262, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_304", "original_string": "def _init_dist_connection(\r\n    cluster_environment: \"ClusterEnvironment\",\r\n    torch_distributed_backend: str,\r\n    global_rank: Optional[int] = None,\r\n    world_size: Optional[int] = None,\r\n    **kwargs: Any,\r\n) -> None:\r\n    \"\"\"Utility function to initialize distributed connection by setting env variables and initializing the distributed\r\n    process group.\r\n\r\n    Args:\r\n        cluster_environment: ``ClusterEnvironment`` instance\r\n        torch_distributed_backend: Backend to use (includes `nccl` and `gloo`)\r\n        global_rank: Rank of the current process\r\n        world_size: Number of processes in the group\r\n        kwargs: Kwargs for ``init_process_group``\r\n\r\n    Raises:\r\n        RuntimeError:\r\n            If ``torch.distributed`` is not available\r\n\r\n    \"\"\"\r\n    if not torch.distributed.is_available():\r\n        raise RuntimeError(\"torch.distributed is not available. Cannot initialize distributed process group\")\r\n    if torch.distributed.is_initialized():\r\n        log.debug(\"torch.distributed is already initialized. Exiting early\")\r\n        return\r\n    global_rank = global_rank if global_rank is not None else cluster_environment.global_rank()\r\n    world_size = world_size if world_size is not None else cluster_environment.world_size()\r\n    os.environ[\"MASTER_ADDR\"] = cluster_environment.main_address\r\n    os.environ[\"MASTER_PORT\"] = str(cluster_environment.main_port)\r\n    log.info(f\"Initializing distributed: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank + 1}/{world_size}\")\r\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\n    if torch_distributed_backend == \"nccl\":\r\n        atexit.register(_destroy_dist_connection)\r\n\r\n    rank_zero_info(\r\n        f\"{'-' * 100}\\n\"\r\n        f\"distributed_backend={torch_distributed_backend}\\n\"\r\n        f\"All distributed processes registered. Starting with {world_size} processes\\n\"\r\n        f\"{'-' * 100}\\n\"\r\n    )", "language": "python", "code": "def _init_dist_connection(\r\n    cluster_environment: \"ClusterEnvironment\",\r\n    torch_distributed_backend: str,\r\n    global_rank: Optional[int] = None,\r\n    world_size: Optional[int] = None,\r\n    **kwargs: Any,\r\n) -> None:\r\n    \"\"\"Utility function to initialize distributed connection by setting env variables and initializing the distributed\r\n    process group.\r\n\r\n    Args:\r\n        cluster_environment: ``ClusterEnvironment`` instance\r\n        torch_distributed_backend: Backend to use (includes `nccl` and `gloo`)\r\n        global_rank: Rank of the current process\r\n        world_size: Number of processes in the group\r\n        kwargs: Kwargs for ``init_process_group``\r\n\r\n    Raises:\r\n        RuntimeError:\r\n            If ``torch.distributed`` is not available\r\n\r\n    \"\"\"\r\n    if not torch.distributed.is_available():\r\n        raise RuntimeError(\"torch.distributed is not available. Cannot initialize distributed process group\")\r\n    if torch.distributed.is_initialized():\r\n        log.debug(\"torch.distributed is already initialized. Exiting early\")\r\n        return\r\n    global_rank = global_rank if global_rank is not None else cluster_environment.global_rank()\r\n    world_size = world_size if world_size is not None else cluster_environment.world_size()\r\n    os.environ[\"MASTER_ADDR\"] = cluster_environment.main_address\r\n    os.environ[\"MASTER_PORT\"] = str(cluster_environment.main_port)\r\n    log.info(f\"Initializing distributed: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank + 1}/{world_size}\")\r\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\n    if torch_distributed_backend == \"nccl\":\r\n        atexit.register(_destroy_dist_connection)\r\n\r\n    rank_zero_info(\r\n        f\"{'-' * 100}\\n\"\r\n        f\"distributed_backend={torch_distributed_backend}\\n\"\r\n        f\"All distributed processes registered. Starting with {world_size} processes\\n\"\r\n        f\"{'-' * 100}\\n\"\r\n    )", "code_tokens": ["def", "_init_dist_connection", "(", "cluster_environment", ":", "STRING", ",", "torch_distributed_backend", ":", "str", ",", "global_rank", ":", "Optional", "[", "int", "]", "=", "None", ",", "world_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "STRING", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "raise", "RuntimeError", "(", "STRING", ")", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "log", ".", "debug", "(", "STRING", ")", "return", "global_rank", "=", "global_rank", "if", "global_rank", "is", "not", "None", "else", "cluster_environment", ".", "global_rank", "(", ")", "world_size", "=", "world_size", "if", "world_size", "is", "not", "None", "else", "cluster_environment", ".", "world_size", "(", ")", "os", ".", "environ", "[", "STRING", "]", "=", "cluster_environment", ".", "main_address", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "cluster_environment", ".", "main_port", ")", "log", ".", "info", "(", "fSTRING", ")", "torch", ".", "distributed", ".", "init_process_group", "(", "torch_distributed_backend", ",", "rank", "=", "global_rank", ",", "world_size", "=", "world_size", ",", "*", "*", "kwargs", ")", "if", "torch_distributed_backend", "=", "=", "STRING", ":", "atexit", ".", "register", "(", "_destroy_dist_connection", ")", "rank_zero_info", "(", "fSTRING", "fSTRING", "fSTRING", "fSTRING", ")"], "docstring": "Utility function to initialize distributed connection by setting env variables and initializing the distributed", "docstring_tokens": ["utility", "function", "to", "initialize", "distributed", "connection", "by", "setting", "env", "variables", "and", "initializing", "the", "distributed"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 265, "end_line": 309, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_305", "original_string": "def _get_default_process_group_backend_for_device(device: torch.device) -> str:\r\n    \"\"\"Return corresponding distributed backend for a given device.\"\"\"\r\n    device_backend_map = torch.distributed.Backend.default_device_backend_map\r\n    if device.type in device_backend_map:\r\n        return device_backend_map[device.type]\r\n    return \"gloo\"", "language": "python", "code": "def _get_default_process_group_backend_for_device(device: torch.device) -> str:\r\n    \"\"\"Return corresponding distributed backend for a given device.\"\"\"\r\n    device_backend_map = torch.distributed.Backend.default_device_backend_map\r\n    if device.type in device_backend_map:\r\n        return device_backend_map[device.type]\r\n    return \"gloo\"", "code_tokens": ["def", "_get_default_process_group_backend_for_device", "(", "device", ":", "torch", ".", "device", ")", "-", ">", "str", ":", "STRING", "device_backend_map", "=", "torch", ".", "distributed", ".", "Backend", ".", "default_device_backend_map", "if", "device", ".", "type", "in", "device_backend_map", ":", "return", "device_backend_map", "[", "device", ".", "type", "]", "return", "STRING"], "docstring": "Return corresponding distributed backend for a given device.", "docstring_tokens": ["return", "corresponding", "distributed", "backend", "for", "a", "given", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 320, "end_line": 325, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "function_306", "original_string": "def reset(self) -> None:\r\n        \"\"\"Reset the sampler list in order to get new sampling.\"\"\"\r\n        self._sampler_list = list(self._sampler)", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Reset the sampler list in order to get new sampling.\"\"\"\r\n        self._sampler_list = list(self._sampler)", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "_sampler_list", "=", "list", "(", "self", ".", "_sampler", ")"], "docstring": "Reset the sampler list in order to get new sampling.", "docstring_tokens": ["reset", "the", "sampler", "list", "in", "order", "to", "get", "new", "sampling"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\distributed.py", "start_line": 363, "end_line": 365, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\init.py", "func_name": "function_307", "original_string": "def _materialize(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize a module.\"\"\"\r\n    module.to_empty(device=device, recurse=False)\r\n    if not hasattr(module, \"reset_parameters\"):\r\n        raise TypeError(\r\n            f\"Materialization requires that the `{type(module).__name__}.reset_parameters` method is implemented.\"\r\n            \" This method is used to initialize any children parameters or buffers in this module.\"\r\n        )\r\n    if callable(module.reset_parameters):\r\n        module.reset_parameters()", "language": "python", "code": "def _materialize(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize a module.\"\"\"\r\n    module.to_empty(device=device, recurse=False)\r\n    if not hasattr(module, \"reset_parameters\"):\r\n        raise TypeError(\r\n            f\"Materialization requires that the `{type(module).__name__}.reset_parameters` method is implemented.\"\r\n            \" This method is used to initialize any children parameters or buffers in this module.\"\r\n        )\r\n    if callable(module.reset_parameters):\r\n        module.reset_parameters()", "code_tokens": ["def", "_materialize", "(", "module", ":", "Module", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "STRING", "module", ".", "to_empty", "(", "device", "=", "device", ",", "recurse", "=", "False", ")", "if", "not", "hasattr", "(", "module", ",", "STRING", ")", ":", "raise", "TypeError", "(", "fSTRING", "STRING", ")", "if", "callable", "(", "module", ".", "reset_parameters", ")", ":", "module", ".", "reset_parameters", "(", ")"], "docstring": "Materialize a module.", "docstring_tokens": ["materialize", "a", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\init.py", "start_line": 61, "end_line": 70, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\init.py", "func_name": "function_308", "original_string": "def _materialize_meta_tensors(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize all tensors in a given module.\"\"\"\r\n    for module in module.modules():\r\n        if _has_meta_device_parameters_or_buffers(module, recurse=False):\r\n            _materialize(module, device)", "language": "python", "code": "def _materialize_meta_tensors(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize all tensors in a given module.\"\"\"\r\n    for module in module.modules():\r\n        if _has_meta_device_parameters_or_buffers(module, recurse=False):\r\n            _materialize(module, device)", "code_tokens": ["def", "_materialize_meta_tensors", "(", "module", ":", "Module", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "STRING", "for", "module", "in", "module", ".", "modules", "(", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "module", ",", "recurse", "=", "False", ")", ":", "_materialize", "(", "module", ",", "device", ")"], "docstring": "Materialize all tensors in a given module.", "docstring_tokens": ["materialize", "all", "tensors", "in", "a", "given", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\init.py", "start_line": 73, "end_line": 77, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\load.py", "func_name": "function_309", "original_string": "def _move_state_into(\r\n    source: dict[str, Any], destination: dict[str, Union[Any, _Stateful]], keys: Optional[set[str]] = None\r\n) -> None:\r\n    \"\"\"Takes the state from the source destination and moves it into the destination dictionary.\r\n\r\n    If an object in the destination follows the stateful protocol, it loads the source state via ``load_state_dict``.\r\n\r\n    \"\"\"\r\n    keys = set(source) if keys is None else keys & set(source)\r\n    for key in keys:\r\n        state = source.pop(key)\r\n        if key in destination and isinstance(destination[key], _Stateful):\r\n            destination[key].load_state_dict(state)\r\n        else:\r\n            destination[key] = state", "language": "python", "code": "def _move_state_into(\r\n    source: dict[str, Any], destination: dict[str, Union[Any, _Stateful]], keys: Optional[set[str]] = None\r\n) -> None:\r\n    \"\"\"Takes the state from the source destination and moves it into the destination dictionary.\r\n\r\n    If an object in the destination follows the stateful protocol, it loads the source state via ``load_state_dict``.\r\n\r\n    \"\"\"\r\n    keys = set(source) if keys is None else keys & set(source)\r\n    for key in keys:\r\n        state = source.pop(key)\r\n        if key in destination and isinstance(destination[key], _Stateful):\r\n            destination[key].load_state_dict(state)\r\n        else:\r\n            destination[key] = state", "code_tokens": ["def", "_move_state_into", "(", "source", ":", "dict", "[", "str", ",", "Any", "]", ",", "destination", ":", "dict", "[", "str", ",", "Union", "[", "Any", ",", "_Stateful", "]", "]", ",", "keys", ":", "Optional", "[", "set", "[", "str", "]", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "keys", "=", "set", "(", "source", ")", "if", "keys", "is", "None", "else", "keys", "&", "set", "(", "source", ")", "for", "key", "in", "keys", ":", "state", "=", "source", ".", "pop", "(", "key", ")", "if", "key", "in", "destination", "and", "isinstance", "(", "destination", "[", "key", "]", ",", "_Stateful", ")", ":", "destination", "[", "key", "]", ".", "load_state_dict", "(", "state", ")", "else", ":", "destination", "[", "key", "]", "=", "state"], "docstring": "Takes the state from the source destination and moves it into the destination dictionary.", "docstring_tokens": ["takes", "the", "state", "from", "the", "source", "destination", "and", "moves", "it", "into", "the", "destination", "dictionary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\load.py", "start_line": 222, "end_line": 236, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\load.py", "func_name": "function_310", "original_string": "def _load_distributed_checkpoint(checkpoint_folder: Path) -> dict[str, Any]:\r\n    \"\"\"Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.\r\n\r\n    The current implementation assumes that the entire checkpoint fits in CPU memory.\r\n\r\n    \"\"\"\r\n    if not _TORCH_GREATER_EQUAL_2_3:\r\n        raise ImportError(\"Processing distributed checkpoints requires PyTorch >= 2.3.\")\r\n\r\n    from torch.distributed.checkpoint import FileSystemReader\r\n    from torch.distributed.checkpoint.format_utils import _EmptyStateDictLoadPlanner\r\n    from torch.distributed.checkpoint.state_dict_loader import _load_state_dict\r\n\r\n    checkpoint: dict[str, Any] = {}\r\n    _load_state_dict(\r\n        checkpoint,\r\n        storage_reader=FileSystemReader(checkpoint_folder),\r\n        planner=_EmptyStateDictLoadPlanner(),\r\n        no_dist=True,\r\n    )\r\n\r\n    extra_file = checkpoint_folder / _METADATA_FILENAME\r\n    extra = torch.load(extra_file, map_location=\"cpu\") if extra_file.is_file() else {}\r\n    checkpoint.update(extra)\r\n\r\n    return checkpoint", "language": "python", "code": "def _load_distributed_checkpoint(checkpoint_folder: Path) -> dict[str, Any]:\r\n    \"\"\"Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.\r\n\r\n    The current implementation assumes that the entire checkpoint fits in CPU memory.\r\n\r\n    \"\"\"\r\n    if not _TORCH_GREATER_EQUAL_2_3:\r\n        raise ImportError(\"Processing distributed checkpoints requires PyTorch >= 2.3.\")\r\n\r\n    from torch.distributed.checkpoint import FileSystemReader\r\n    from torch.distributed.checkpoint.format_utils import _EmptyStateDictLoadPlanner\r\n    from torch.distributed.checkpoint.state_dict_loader import _load_state_dict\r\n\r\n    checkpoint: dict[str, Any] = {}\r\n    _load_state_dict(\r\n        checkpoint,\r\n        storage_reader=FileSystemReader(checkpoint_folder),\r\n        planner=_EmptyStateDictLoadPlanner(),\r\n        no_dist=True,\r\n    )\r\n\r\n    extra_file = checkpoint_folder / _METADATA_FILENAME\r\n    extra = torch.load(extra_file, map_location=\"cpu\") if extra_file.is_file() else {}\r\n    checkpoint.update(extra)\r\n\r\n    return checkpoint", "code_tokens": ["def", "_load_distributed_checkpoint", "(", "checkpoint_folder", ":", "Path", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "_TORCH_GREATER_EQUAL_2_3", ":", "raise", "ImportError", "(", "STRING", ")", "from", "torch", ".", "distributed", ".", "checkpoint", "import", "FileSystemReader", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "format_utils", "import", "_EmptyStateDictLoadPlanner", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict_loader", "import", "_load_state_dict", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "_load_state_dict", "(", "checkpoint", ",", "storage_reader", "=", "FileSystemReader", "(", "checkpoint_folder", ")", ",", "planner", "=", "_EmptyStateDictLoadPlanner", "(", ")", ",", "no_dist", "=", "True", ",", ")", "extra_file", "=", "checkpoint_folder", "/", "_METADATA_FILENAME", "extra", "=", "torch", ".", "load", "(", "extra_file", ",", "map_location", "=", "STRING", ")", "if", "extra_file", ".", "is_file", "(", ")", "else", "{", "}", "checkpoint", ".", "update", "(", "extra", ")", "return", "checkpoint"], "docstring": "Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.", "docstring_tokens": ["loads", "a", "sharded", "checkpoint", "saved", "with", "the", "torch", "distributed", "checkpoint", "into", "a", "full", "state", "dict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\load.py", "start_line": 239, "end_line": 265, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_311", "original_string": "def _convert_params(params: Optional[Union[dict[str, Any], Namespace]]) -> dict[str, Any]:\r\n    \"\"\"Ensure parameters are a dict or convert to dict if necessary.\r\n\r\n    Args:\r\n        params: Target to be converted to a dictionary\r\n\r\n    Returns:\r\n        params as a dictionary\r\n\r\n    \"\"\"\r\n    if isinstance(params, Namespace):\r\n        params = vars(params)\r\n\r\n    if params is None:\r\n        params = {}\r\n\r\n    return params", "language": "python", "code": "def _convert_params(params: Optional[Union[dict[str, Any], Namespace]]) -> dict[str, Any]:\r\n    \"\"\"Ensure parameters are a dict or convert to dict if necessary.\r\n\r\n    Args:\r\n        params: Target to be converted to a dictionary\r\n\r\n    Returns:\r\n        params as a dictionary\r\n\r\n    \"\"\"\r\n    if isinstance(params, Namespace):\r\n        params = vars(params)\r\n\r\n    if params is None:\r\n        params = {}\r\n\r\n    return params", "code_tokens": ["def", "_convert_params", "(", "params", ":", "Optional", "[", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "isinstance", "(", "params", ",", "Namespace", ")", ":", "params", "=", "vars", "(", "params", ")", "if", "params", "is", "None", ":", "params", "=", "{", "}", "return", "params"], "docstring": "Ensure parameters are a dict or convert to dict if necessary.", "docstring_tokens": ["ensure", "parameters", "are", "a", "dict", "or", "convert", "to", "dict", "if", "necessary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 26, "end_line": 43, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_312", "original_string": "def _sanitize_callable_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n\r\n    Returns:\r\n        dictionary with all callables sanitized\r\n\r\n    \"\"\"\r\n\r\n    def _sanitize_callable(val: Any) -> Any:\r\n        if inspect.isclass(val):\r\n            return val.__name__\r\n        if callable(val):\r\n            try:\r\n                _val = val()\r\n                if callable(_val):\r\n                    return val.__name__\r\n                return _val\r\n            except Exception:\r\n                return getattr(val, \"__name__\", None)\r\n        return val\r\n\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}", "language": "python", "code": "def _sanitize_callable_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n\r\n    Returns:\r\n        dictionary with all callables sanitized\r\n\r\n    \"\"\"\r\n\r\n    def _sanitize_callable(val: Any) -> Any:\r\n        if inspect.isclass(val):\r\n            return val.__name__\r\n        if callable(val):\r\n            try:\r\n                _val = val()\r\n                if callable(_val):\r\n                    return val.__name__\r\n                return _val\r\n            except Exception:\r\n                return getattr(val, \"__name__\", None)\r\n        return val\r\n\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}", "code_tokens": ["def", "_sanitize_callable_params", "(", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "def", "_sanitize_callable", "(", "val", ":", "Any", ")", "-", ">", "Any", ":", "if", "inspect", ".", "isclass", "(", "val", ")", ":", "return", "val", ".", "__name__", "if", "callable", "(", "val", ")", ":", "try", ":", "_val", "=", "val", "(", ")", "if", "callable", "(", "_val", ")", ":", "return", "val", ".", "__name__", "return", "_val", "except", "Exception", ":", "return", "getattr", "(", "val", ",", "STRING", ",", "None", ")", "return", "val", "return", "{", "key", ":", "_sanitize_callable", "(", "val", ")", "for", "key", ",", "val", "in", "params", ".", "items", "(", ")", "}"], "docstring": "Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.", "docstring_tokens": ["sanitize", "callable", "params", "dict", "e", "g", "a", "function_", "at", "0x", "a", "function_"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 46, "end_line": 73, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_313", "original_string": "def _flatten_dict(params: MutableMapping[Any, Any], delimiter: str = \"/\", parent_key: str = \"\") -> dict[str, Any]:\r\n    \"\"\"Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n        delimiter: Delimiter to express the hierarchy. Defaults to ``'/'``.\r\n\r\n    Returns:\r\n        Flattened dict.\r\n\r\n    Examples:\r\n        >>> _flatten_dict({'a': {'b': 'c'}})\r\n        {'a/b': 'c'}\r\n        >>> _flatten_dict({'a': {'b': 123}})\r\n        {'a/b': 123}\r\n        >>> _flatten_dict({5: {'a': 123}})\r\n        {'5/a': 123}\r\n        >>> _flatten_dict({\"dl\": [{\"a\": 1, \"c\": 3}, {\"b\": 2, \"d\": 5}], \"l\": [1, 2, 3, 4]})\r\n        {'dl/0/a': 1, 'dl/0/c': 3, 'dl/1/b': 2, 'dl/1/d': 5, 'l': [1, 2, 3, 4]}\r\n\r\n    \"\"\"\r\n    result: dict[str, Any] = {}\r\n    for k, v in params.items():\r\n        new_key = parent_key + delimiter + str(k) if parent_key else str(k)\r\n        if is_dataclass(v) and not isinstance(v, type):\r\n            v = asdict(v)\r\n        elif isinstance(v, Namespace):\r\n            v = vars(v)\r\n\r\n        if isinstance(v, MutableMapping):\r\n            result = {**result, **_flatten_dict(v, parent_key=new_key, delimiter=delimiter)}\r\n        elif isinstance(v, list) and all(isinstance(item, MutableMapping) for item in v):\r\n            for i, item in enumerate(v):\r\n                result = {**result, **_flatten_dict(item, parent_key=f\"{new_key}/{i}\", delimiter=delimiter)}\r\n        else:\r\n            result[new_key] = v\r\n    return result", "language": "python", "code": "def _flatten_dict(params: MutableMapping[Any, Any], delimiter: str = \"/\", parent_key: str = \"\") -> dict[str, Any]:\r\n    \"\"\"Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n        delimiter: Delimiter to express the hierarchy. Defaults to ``'/'``.\r\n\r\n    Returns:\r\n        Flattened dict.\r\n\r\n    Examples:\r\n        >>> _flatten_dict({'a': {'b': 'c'}})\r\n        {'a/b': 'c'}\r\n        >>> _flatten_dict({'a': {'b': 123}})\r\n        {'a/b': 123}\r\n        >>> _flatten_dict({5: {'a': 123}})\r\n        {'5/a': 123}\r\n        >>> _flatten_dict({\"dl\": [{\"a\": 1, \"c\": 3}, {\"b\": 2, \"d\": 5}], \"l\": [1, 2, 3, 4]})\r\n        {'dl/0/a': 1, 'dl/0/c': 3, 'dl/1/b': 2, 'dl/1/d': 5, 'l': [1, 2, 3, 4]}\r\n\r\n    \"\"\"\r\n    result: dict[str, Any] = {}\r\n    for k, v in params.items():\r\n        new_key = parent_key + delimiter + str(k) if parent_key else str(k)\r\n        if is_dataclass(v) and not isinstance(v, type):\r\n            v = asdict(v)\r\n        elif isinstance(v, Namespace):\r\n            v = vars(v)\r\n\r\n        if isinstance(v, MutableMapping):\r\n            result = {**result, **_flatten_dict(v, parent_key=new_key, delimiter=delimiter)}\r\n        elif isinstance(v, list) and all(isinstance(item, MutableMapping) for item in v):\r\n            for i, item in enumerate(v):\r\n                result = {**result, **_flatten_dict(item, parent_key=f\"{new_key}/{i}\", delimiter=delimiter)}\r\n        else:\r\n            result[new_key] = v\r\n    return result", "code_tokens": ["def", "_flatten_dict", "(", "params", ":", "MutableMapping", "[", "Any", ",", "Any", "]", ",", "delimiter", ":", "str", "=", "STRING", ",", "parent_key", ":", "str", "=", "STRING", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "result", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "new_key", "=", "parent_key", "+", "delimiter", "+", "str", "(", "k", ")", "if", "parent_key", "else", "str", "(", "k", ")", "if", "is_dataclass", "(", "v", ")", "and", "not", "isinstance", "(", "v", ",", "type", ")", ":", "v", "=", "asdict", "(", "v", ")", "elif", "isinstance", "(", "v", ",", "Namespace", ")", ":", "v", "=", "vars", "(", "v", ")", "if", "isinstance", "(", "v", ",", "MutableMapping", ")", ":", "result", "=", "{", "*", "*", "result", ",", "*", "*", "_flatten_dict", "(", "v", ",", "parent_key", "=", "new_key", ",", "delimiter", "=", "delimiter", ")", "}", "elif", "isinstance", "(", "v", ",", "list", ")", "and", "all", "(", "isinstance", "(", "item", ",", "MutableMapping", ")", "for", "item", "in", "v", ")", ":", "for", "i", ",", "item", "in", "enumerate", "(", "v", ")", ":", "result", "=", "{", "*", "*", "result", ",", "*", "*", "_flatten_dict", "(", "item", ",", "parent_key", "=", "fSTRING", ",", "delimiter", "=", "delimiter", ")", "}", "else", ":", "result", "[", "new_key", "]", "=", "v", "return", "result"], "docstring": "Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.", "docstring_tokens": ["flatten", "hierarchical", "dict", "e", "g", "a", "b", "c", "a", "b", "c"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 76, "end_line": 113, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_314", "original_string": "def _sanitize_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Returns params with non-primitvies converted to strings for logging.\r\n\r\n    >>> import torch\r\n    >>> params = {\"float\": 0.3,\r\n    ...           \"int\": 1,\r\n    ...           \"string\": \"abc\",\r\n    ...           \"bool\": True,\r\n    ...           \"list\": [1, 2, 3],\r\n    ...           \"namespace\": Namespace(foo=3),\r\n    ...           \"layer\": torch.nn.BatchNorm1d}\r\n    >>> import pprint\r\n    >>> pprint.pprint(_sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\r\n    {'bool': True,\r\n        'float': 0.3,\r\n        'int': 1,\r\n        'layer': \"<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\",\r\n        'list': '[1, 2, 3]',\r\n        'namespace': 'Namespace(foo=3)',\r\n        'string': 'abc'}\r\n\r\n    \"\"\"\r\n    for k in params:\r\n        if _NUMPY_AVAILABLE:\r\n            import numpy as np\r\n\r\n            if isinstance(params[k], (np.bool_, np.integer, np.floating)):\r\n                params[k] = params[k].item()\r\n        if type(params[k]) not in [bool, int, float, str, Tensor]:\r\n            params[k] = str(params[k])\r\n    return params", "language": "python", "code": "def _sanitize_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Returns params with non-primitvies converted to strings for logging.\r\n\r\n    >>> import torch\r\n    >>> params = {\"float\": 0.3,\r\n    ...           \"int\": 1,\r\n    ...           \"string\": \"abc\",\r\n    ...           \"bool\": True,\r\n    ...           \"list\": [1, 2, 3],\r\n    ...           \"namespace\": Namespace(foo=3),\r\n    ...           \"layer\": torch.nn.BatchNorm1d}\r\n    >>> import pprint\r\n    >>> pprint.pprint(_sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\r\n    {'bool': True,\r\n        'float': 0.3,\r\n        'int': 1,\r\n        'layer': \"<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\",\r\n        'list': '[1, 2, 3]',\r\n        'namespace': 'Namespace(foo=3)',\r\n        'string': 'abc'}\r\n\r\n    \"\"\"\r\n    for k in params:\r\n        if _NUMPY_AVAILABLE:\r\n            import numpy as np\r\n\r\n            if isinstance(params[k], (np.bool_, np.integer, np.floating)):\r\n                params[k] = params[k].item()\r\n        if type(params[k]) not in [bool, int, float, str, Tensor]:\r\n            params[k] = str(params[k])\r\n    return params", "code_tokens": ["def", "_sanitize_params", "(", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "for", "k", "in", "params", ":", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "if", "isinstance", "(", "params", "[", "k", "]", ",", "(", "np", ".", "bool_", ",", "np", ".", "integer", ",", "np", ".", "floating", ")", ")", ":", "params", "[", "k", "]", "=", "params", "[", "k", "]", ".", "item", "(", ")", "if", "type", "(", "params", "[", "k", "]", ")", "not", "in", "[", "bool", ",", "int", ",", "float", ",", "str", ",", "Tensor", "]", ":", "params", "[", "k", "]", "=", "str", "(", "params", "[", "k", "]", ")", "return", "params"], "docstring": "Returns params with non-primitvies converted to strings for logging.", "docstring_tokens": ["returns", "params", "with", "non", "primitvies", "converted", "to", "strings", "for", "logging"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 116, "end_line": 146, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_315", "original_string": "def _convert_json_serializable(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Convert non-serializable objects in params to string.\"\"\"\r\n    return {k: str(v) if not _is_json_serializable(v) else v for k, v in params.items()}", "language": "python", "code": "def _convert_json_serializable(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Convert non-serializable objects in params to string.\"\"\"\r\n    return {k: str(v) if not _is_json_serializable(v) else v for k, v in params.items()}", "code_tokens": ["def", "_convert_json_serializable", "(", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "{", "k", ":", "str", "(", "v", ")", "if", "not", "_is_json_serializable", "(", "v", ")", "else", "v", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", "}"], "docstring": "Convert non-serializable objects in params to string.", "docstring_tokens": ["convert", "non", "serializable", "objects", "in", "params", "to", "string"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 149, "end_line": 151, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_316", "original_string": "def _is_json_serializable(value: Any) -> bool:\r\n    \"\"\"Test whether a variable can be encoded as json.\"\"\"\r\n    if value is None or isinstance(value, (bool, int, float, str, list, dict)):  # fast path\r\n        return True\r\n    try:\r\n        json.dumps(value)\r\n        return True\r\n    except (TypeError, OverflowError):\r\n        return False", "language": "python", "code": "def _is_json_serializable(value: Any) -> bool:\r\n    \"\"\"Test whether a variable can be encoded as json.\"\"\"\r\n    if value is None or isinstance(value, (bool, int, float, str, list, dict)):  # fast path\r\n        return True\r\n    try:\r\n        json.dumps(value)\r\n        return True\r\n    except (TypeError, OverflowError):\r\n        return False", "code_tokens": ["def", "_is_json_serializable", "(", "value", ":", "Any", ")", "-", ">", "bool", ":", "STRING", "if", "value", "is", "None", "or", "isinstance", "(", "value", ",", "(", "bool", ",", "int", ",", "float", ",", "str", ",", "list", ",", "dict", ")", ")", ":", "#", "fast", "path", "return", "True", "try", ":", "json", ".", "dumps", "(", "value", ")", "return", "True", "except", "(", "TypeError", ",", "OverflowError", ")", ":", "return", "False"], "docstring": "Test whether a variable can be encoded as json.", "docstring_tokens": ["test", "whether", "a", "variable", "can", "be", "encoded", "as", "json"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 154, "end_line": 163, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "function_317", "original_string": "def _add_prefix(\r\n    metrics: Mapping[str, Union[Tensor, float]], prefix: str, separator: str\r\n) -> Mapping[str, Union[Tensor, float]]:\r\n    \"\"\"Insert prefix before each key in a dict, separated by the separator.\r\n\r\n    Args:\r\n        metrics: Dictionary with metric names as keys and measured quantities as values\r\n        prefix: Prefix to insert before each key\r\n        separator: Separates prefix and original key name\r\n\r\n    Returns:\r\n        Dictionary with prefix and separator inserted before each key\r\n\r\n    \"\"\"\r\n    if not prefix:\r\n        return metrics\r\n    return {f\"{prefix}{separator}{k}\": v for k, v in metrics.items()}", "language": "python", "code": "def _add_prefix(\r\n    metrics: Mapping[str, Union[Tensor, float]], prefix: str, separator: str\r\n) -> Mapping[str, Union[Tensor, float]]:\r\n    \"\"\"Insert prefix before each key in a dict, separated by the separator.\r\n\r\n    Args:\r\n        metrics: Dictionary with metric names as keys and measured quantities as values\r\n        prefix: Prefix to insert before each key\r\n        separator: Separates prefix and original key name\r\n\r\n    Returns:\r\n        Dictionary with prefix and separator inserted before each key\r\n\r\n    \"\"\"\r\n    if not prefix:\r\n        return metrics\r\n    return {f\"{prefix}{separator}{k}\": v for k, v in metrics.items()}", "code_tokens": ["def", "_add_prefix", "(", "metrics", ":", "Mapping", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ",", "prefix", ":", "str", ",", "separator", ":", "str", ")", "-", ">", "Mapping", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ":", "STRING", "if", "not", "prefix", ":", "return", "metrics", "return", "{", "fSTRING", ":", "v", "for", "k", ",", "v", "in", "metrics", ".", "items", "(", ")", "}"], "docstring": "Insert prefix before each key in a dict, separated by the separator.", "docstring_tokens": ["insert", "prefix", "before", "each", "key", "in", "a", "dict", "separated", "by", "the", "separator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\logger.py", "start_line": 166, "end_line": 182, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\optimizer.py", "func_name": "function_318", "original_string": "def _optimizers_to_device(optimizers: Iterable[Optimizer], device: _DEVICE) -> None:\r\n    \"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\r\n    for opt in optimizers:\r\n        _optimizer_to_device(opt, device)", "language": "python", "code": "def _optimizers_to_device(optimizers: Iterable[Optimizer], device: _DEVICE) -> None:\r\n    \"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\r\n    for opt in optimizers:\r\n        _optimizer_to_device(opt, device)", "code_tokens": ["def", "_optimizers_to_device", "(", "optimizers", ":", "Iterable", "[", "Optimizer", "]", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "STRING", "for", "opt", "in", "optimizers", ":", "_optimizer_to_device", "(", "opt", ",", "device", ")"], "docstring": "Moves optimizer states for a sequence of optimizers to the device.", "docstring_tokens": ["moves", "optimizer", "states", "for", "a", "sequence", "of", "optimizers", "to", "the", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\optimizer.py", "start_line": 23, "end_line": 26, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\optimizer.py", "func_name": "function_319", "original_string": "def _optimizer_to_device(optimizer: Optimizer, device: _DEVICE) -> None:\r\n    \"\"\"Moves the state of a single optimizer to the device.\"\"\"\r\n    for p, v in optimizer.state.items():\r\n        if not isinstance(v, MutableMapping):\r\n            optimizer.state[p] = apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen=True)\r\n            continue\r\n        for key, val in v.items():\r\n            if key != \"step\":\r\n                v[key] = move_data_to_device(val, device)", "language": "python", "code": "def _optimizer_to_device(optimizer: Optimizer, device: _DEVICE) -> None:\r\n    \"\"\"Moves the state of a single optimizer to the device.\"\"\"\r\n    for p, v in optimizer.state.items():\r\n        if not isinstance(v, MutableMapping):\r\n            optimizer.state[p] = apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen=True)\r\n            continue\r\n        for key, val in v.items():\r\n            if key != \"step\":\r\n                v[key] = move_data_to_device(val, device)", "code_tokens": ["def", "_optimizer_to_device", "(", "optimizer", ":", "Optimizer", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "STRING", "for", "p", ",", "v", "in", "optimizer", ".", "state", ".", "items", "(", ")", ":", "if", "not", "isinstance", "(", "v", ",", "MutableMapping", ")", ":", "optimizer", ".", "state", "[", "p", "]", "=", "apply_to_collection", "(", "v", ",", "Tensor", ",", "move_data_to_device", ",", "device", ",", "allow_frozen", "=", "True", ")", "continue", "for", "key", ",", "val", "in", "v", ".", "items", "(", ")", ":", "if", "key", "!", "=", "STRING", ":", "v", "[", "key", "]", "=", "move_data_to_device", "(", "val", ",", "device", ")"], "docstring": "Moves the state of a single optimizer to the device.", "docstring_tokens": ["moves", "the", "state", "of", "a", "single", "optimizer", "to", "the", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\optimizer.py", "start_line": 29, "end_line": 40, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\registry.py", "func_name": "function_320", "original_string": "def _load_external_callbacks(group: str) -> list[Any]:\r\n    \"\"\"Collect external callbacks registered through entry points.\r\n\r\n    The entry points are expected to be functions returning a list of callbacks.\r\n\r\n    Args:\r\n        group: The entry point group name to load callbacks from.\r\n\r\n    Return:\r\n        A list of all callbacks collected from external factories.\r\n\r\n    \"\"\"\r\n    factories = (\r\n        entry_points(group=group) if _PYTHON_GREATER_EQUAL_3_10_0 else entry_points().get(group, {})  # type: ignore[arg-type]\r\n    )\r\n\r\n    external_callbacks: list[Any] = []\r\n    for factory in factories:\r\n        callback_factory = factory.load()\r\n        callbacks_list: Union[list[Any], Any] = callback_factory()\r\n        callbacks_list = [callbacks_list] if not isinstance(callbacks_list, list) else callbacks_list\r\n        if callbacks_list:\r\n            _log.info(\r\n                f\"Adding {len(callbacks_list)} callbacks from entry point '{factory.name}':\"\r\n                f\" {', '.join(type(cb).__name__ for cb in callbacks_list)}\"\r\n            )\r\n        external_callbacks.extend(callbacks_list)\r\n    return external_callbacks", "language": "python", "code": "def _load_external_callbacks(group: str) -> list[Any]:\r\n    \"\"\"Collect external callbacks registered through entry points.\r\n\r\n    The entry points are expected to be functions returning a list of callbacks.\r\n\r\n    Args:\r\n        group: The entry point group name to load callbacks from.\r\n\r\n    Return:\r\n        A list of all callbacks collected from external factories.\r\n\r\n    \"\"\"\r\n    factories = (\r\n        entry_points(group=group) if _PYTHON_GREATER_EQUAL_3_10_0 else entry_points().get(group, {})  # type: ignore[arg-type]\r\n    )\r\n\r\n    external_callbacks: list[Any] = []\r\n    for factory in factories:\r\n        callback_factory = factory.load()\r\n        callbacks_list: Union[list[Any], Any] = callback_factory()\r\n        callbacks_list = [callbacks_list] if not isinstance(callbacks_list, list) else callbacks_list\r\n        if callbacks_list:\r\n            _log.info(\r\n                f\"Adding {len(callbacks_list)} callbacks from entry point '{factory.name}':\"\r\n                f\" {', '.join(type(cb).__name__ for cb in callbacks_list)}\"\r\n            )\r\n        external_callbacks.extend(callbacks_list)\r\n    return external_callbacks", "code_tokens": ["def", "_load_external_callbacks", "(", "group", ":", "str", ")", "-", ">", "list", "[", "Any", "]", ":", "STRING", "factories", "=", "(", "entry_points", "(", "group", "=", "group", ")", "if", "_PYTHON_GREATER_EQUAL_3_10_0", "else", "entry_points", "(", ")", ".", "get", "(", "group", ",", "{", "}", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", ")", "external_callbacks", ":", "list", "[", "Any", "]", "=", "[", "]", "for", "factory", "in", "factories", ":", "callback_factory", "=", "factory", ".", "load", "(", ")", "callbacks_list", ":", "Union", "[", "list", "[", "Any", "]", ",", "Any", "]", "=", "callback_factory", "(", ")", "callbacks_list", "=", "[", "callbacks_list", "]", "if", "not", "isinstance", "(", "callbacks_list", ",", "list", ")", "else", "callbacks_list", "if", "callbacks_list", ":", "_log", ".", "info", "(", "fSTRING", "fSTRING", ")", "external_callbacks", ".", "extend", "(", "callbacks_list", ")", "return", "external_callbacks"], "docstring": "Collect external callbacks registered through entry points.", "docstring_tokens": ["collect", "external", "callbacks", "registered", "through", "entry", "points"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\registry.py", "start_line": 26, "end_line": 53, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_321", "original_string": "def seed_everything(seed: Optional[int] = None, workers: bool = False, verbose: bool = True) -> int:\r\n    r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\r\n    In addition, sets the following environment variables:\r\n\r\n    - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\r\n    - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\r\n\r\n    Args:\r\n        seed: the integer value seed for global random state in Lightning.\r\n            If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\r\n            ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0. If seed is\r\n            not in bounds or cannot be cast to int, a ValueError is raised.\r\n        workers: if set to ``True``, will properly configure all dataloaders passed to the\r\n            Trainer with a ``worker_init_fn``. If the user already provides such a function\r\n            for their dataloaders, setting this argument will have no influence. See also:\r\n            :func:`~lightning.fabric.utilities.seed.pl_worker_init_function`.\r\n        verbose: Whether to print a message on each rank with the seed being set.\r\n\r\n    \"\"\"\r\n    if seed is None:\r\n        env_seed = os.environ.get(\"PL_GLOBAL_SEED\")\r\n        if env_seed is None:\r\n            seed = 0\r\n            if verbose:\r\n                rank_zero_warn(f\"No seed found, seed set to {seed}\")\r\n        else:\r\n            try:\r\n                seed = int(env_seed)\r\n            except ValueError:\r\n                raise ValueError(f\"Invalid seed specified via PL_GLOBAL_SEED: {repr(env_seed)}\")\r\n    elif not isinstance(seed, int):\r\n        seed = int(seed)\r\n\r\n    if not (min_seed_value <= seed <= max_seed_value):\r\n        raise ValueError(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\r\n\r\n    if verbose:\r\n        log.info(rank_prefixed_message(f\"Seed set to {seed}\", _get_rank()))\r\n\r\n    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\r\n    random.seed(seed)\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n    os.environ[\"PL_SEED_WORKERS\"] = f\"{int(workers)}\"\r\n\r\n    return seed", "language": "python", "code": "def seed_everything(seed: Optional[int] = None, workers: bool = False, verbose: bool = True) -> int:\r\n    r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\r\n    In addition, sets the following environment variables:\r\n\r\n    - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\r\n    - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\r\n\r\n    Args:\r\n        seed: the integer value seed for global random state in Lightning.\r\n            If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\r\n            ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0. If seed is\r\n            not in bounds or cannot be cast to int, a ValueError is raised.\r\n        workers: if set to ``True``, will properly configure all dataloaders passed to the\r\n            Trainer with a ``worker_init_fn``. If the user already provides such a function\r\n            for their dataloaders, setting this argument will have no influence. See also:\r\n            :func:`~lightning.fabric.utilities.seed.pl_worker_init_function`.\r\n        verbose: Whether to print a message on each rank with the seed being set.\r\n\r\n    \"\"\"\r\n    if seed is None:\r\n        env_seed = os.environ.get(\"PL_GLOBAL_SEED\")\r\n        if env_seed is None:\r\n            seed = 0\r\n            if verbose:\r\n                rank_zero_warn(f\"No seed found, seed set to {seed}\")\r\n        else:\r\n            try:\r\n                seed = int(env_seed)\r\n            except ValueError:\r\n                raise ValueError(f\"Invalid seed specified via PL_GLOBAL_SEED: {repr(env_seed)}\")\r\n    elif not isinstance(seed, int):\r\n        seed = int(seed)\r\n\r\n    if not (min_seed_value <= seed <= max_seed_value):\r\n        raise ValueError(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\r\n\r\n    if verbose:\r\n        log.info(rank_prefixed_message(f\"Seed set to {seed}\", _get_rank()))\r\n\r\n    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\r\n    random.seed(seed)\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n    os.environ[\"PL_SEED_WORKERS\"] = f\"{int(workers)}\"\r\n\r\n    return seed", "code_tokens": ["def", "seed_everything", "(", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "workers", ":", "bool", "=", "False", ",", "verbose", ":", "bool", "=", "True", ")", "-", ">", "int", ":", "rSTRING", "if", "seed", "is", "None", ":", "env_seed", "=", "os", ".", "environ", ".", "get", "(", "STRING", ")", "if", "env_seed", "is", "None", ":", "seed", "=", "0", "if", "verbose", ":", "rank_zero_warn", "(", "fSTRING", ")", "else", ":", "try", ":", "seed", "=", "int", "(", "env_seed", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "fSTRING", ")", "elif", "not", "isinstance", "(", "seed", ",", "int", ")", ":", "seed", "=", "int", "(", "seed", ")", "if", "not", "(", "min_seed_value", "<", "=", "seed", "<", "=", "max_seed_value", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "if", "verbose", ":", "log", ".", "info", "(", "rank_prefixed_message", "(", "fSTRING", ",", "_get_rank", "(", ")", ")", ")", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "seed", ")", "random", ".", "seed", "(", "seed", ")", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "np", ".", "random", ".", "seed", "(", "seed", ")", "torch", ".", "manual_seed", "(", "seed", ")", "os", ".", "environ", "[", "STRING", "]", "=", "fSTRING", "return", "seed"], "docstring": "r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.", "docstring_tokens": ["r", "function", "that", "sets", "the", "seed", "for", "pseudo", "random", "number", "generators", "in", "torch", "numpy", "and", "python", "s", "random", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 19, "end_line": 68, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_322", "original_string": "def reset_seed() -> None:\r\n    r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.\r\n\r\n    If :func:`~lightning.fabric.utilities.seed.seed_everything` is unused, this function will do nothing.\r\n\r\n    \"\"\"\r\n    seed = os.environ.get(\"PL_GLOBAL_SEED\", None)\r\n    if seed is None:\r\n        return\r\n    workers = os.environ.get(\"PL_SEED_WORKERS\", \"0\")\r\n    seed_everything(int(seed), workers=bool(int(workers)), verbose=False)", "language": "python", "code": "def reset_seed() -> None:\r\n    r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.\r\n\r\n    If :func:`~lightning.fabric.utilities.seed.seed_everything` is unused, this function will do nothing.\r\n\r\n    \"\"\"\r\n    seed = os.environ.get(\"PL_GLOBAL_SEED\", None)\r\n    if seed is None:\r\n        return\r\n    workers = os.environ.get(\"PL_SEED_WORKERS\", \"0\")\r\n    seed_everything(int(seed), workers=bool(int(workers)), verbose=False)", "code_tokens": ["def", "reset_seed", "(", ")", "-", ">", "None", ":", "rSTRING", "seed", "=", "os", ".", "environ", ".", "get", "(", "STRING", ",", "None", ")", "if", "seed", "is", "None", ":", "return", "workers", "=", "os", ".", "environ", ".", "get", "(", "STRING", ",", "STRING", ")", "seed_everything", "(", "int", "(", "seed", ")", ",", "workers", "=", "bool", "(", "int", "(", "workers", ")", ")", ",", "verbose", "=", "False", ")"], "docstring": "r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.", "docstring_tokens": ["r", "reset", "the", "seed", "to", "the", "value", "that", "func", "lightning", "fabric", "utilities", "seed", "seed_everything", "previously", "set"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 71, "end_line": 81, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_323", "original_string": "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:  # pragma: no cover\r\n    r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with\r\n    ``seed_everything(seed, workers=True)``.\r\n\r\n    See also the PyTorch documentation on\r\n    `randomness in DataLoaders <https://pytorch.org/docs/stable/notes/randomness.html#dataloader>`_.\r\n\r\n    \"\"\"\r\n    global_rank = rank if rank is not None else rank_zero_only.rank\r\n    process_seed = torch.initial_seed()\r\n    base_seed = process_seed - worker_id\r\n    log.debug(\r\n        f\"Initializing random number generators of process {global_rank} worker {worker_id} with base seed {base_seed}\"\r\n    )\r\n    seed_sequence = _generate_seed_sequence(base_seed, worker_id, global_rank, count=4)\r\n    torch.manual_seed(seed_sequence[0])  # torch takes a 64-bit seed\r\n    random.seed((seed_sequence[1] << 32) | seed_sequence[2])  # combine two 64-bit seeds\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        ss = np.random.SeedSequence([base_seed, worker_id, global_rank])\r\n        np_rng_seed = ss.generate_state(4)\r\n\r\n        np.random.seed(np_rng_seed)", "language": "python", "code": "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:  # pragma: no cover\r\n    r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with\r\n    ``seed_everything(seed, workers=True)``.\r\n\r\n    See also the PyTorch documentation on\r\n    `randomness in DataLoaders <https://pytorch.org/docs/stable/notes/randomness.html#dataloader>`_.\r\n\r\n    \"\"\"\r\n    global_rank = rank if rank is not None else rank_zero_only.rank\r\n    process_seed = torch.initial_seed()\r\n    base_seed = process_seed - worker_id\r\n    log.debug(\r\n        f\"Initializing random number generators of process {global_rank} worker {worker_id} with base seed {base_seed}\"\r\n    )\r\n    seed_sequence = _generate_seed_sequence(base_seed, worker_id, global_rank, count=4)\r\n    torch.manual_seed(seed_sequence[0])  # torch takes a 64-bit seed\r\n    random.seed((seed_sequence[1] << 32) | seed_sequence[2])  # combine two 64-bit seeds\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        ss = np.random.SeedSequence([base_seed, worker_id, global_rank])\r\n        np_rng_seed = ss.generate_state(4)\r\n\r\n        np.random.seed(np_rng_seed)", "code_tokens": ["def", "pl_worker_init_function", "(", "worker_id", ":", "int", ",", "rank", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "#", "pragma", ":", "no", "cover", "rSTRING", "global_rank", "=", "rank", "if", "rank", "is", "not", "None", "else", "rank_zero_only", ".", "rank", "process_seed", "=", "torch", ".", "initial_seed", "(", ")", "base_seed", "=", "process_seed", "-", "worker_id", "log", ".", "debug", "(", "fSTRING", ")", "seed_sequence", "=", "_generate_seed_sequence", "(", "base_seed", ",", "worker_id", ",", "global_rank", ",", "count", "=", "4", ")", "torch", ".", "manual_seed", "(", "seed_sequence", "[", "0", "]", ")", "#", "torch", "takes", "a", "64", "-", "bit", "seed", "random", ".", "seed", "(", "(", "seed_sequence", "[", "1", "]", "<", "<", "32", ")", "|", "seed_sequence", "[", "2", "]", ")", "#", "combine", "two", "64", "-", "bit", "seeds", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "ss", "=", "np", ".", "random", ".", "SeedSequence", "(", "[", "base_seed", ",", "worker_id", ",", "global_rank", "]", ")", "np_rng_seed", "=", "ss", ".", "generate_state", "(", "4", ")", "np", ".", "random", ".", "seed", "(", "np_rng_seed", ")"], "docstring": "r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with", "docstring_tokens": ["r", "the", "worker_init_fn", "that", "lightning", "automatically", "adds", "to", "your", "dataloader", "if", "you", "previously", "set", "the", "seed", "with"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 84, "end_line": 109, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_324", "original_string": "def _generate_seed_sequence(base_seed: int, worker_id: int, global_rank: int, count: int) -> list[int]:\r\n    \"\"\"Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)\r\n    algorithm.\"\"\"\r\n    combined_seed = (base_seed << 32) | (worker_id << 16) | global_rank\r\n    seeds = []\r\n    for _ in range(count):\r\n        combined_seed = (combined_seed * 6364136223846793005 + 1) & ((1 << 64) - 1)\r\n        seeds.append(combined_seed)\r\n    return seeds", "language": "python", "code": "def _generate_seed_sequence(base_seed: int, worker_id: int, global_rank: int, count: int) -> list[int]:\r\n    \"\"\"Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)\r\n    algorithm.\"\"\"\r\n    combined_seed = (base_seed << 32) | (worker_id << 16) | global_rank\r\n    seeds = []\r\n    for _ in range(count):\r\n        combined_seed = (combined_seed * 6364136223846793005 + 1) & ((1 << 64) - 1)\r\n        seeds.append(combined_seed)\r\n    return seeds", "code_tokens": ["def", "_generate_seed_sequence", "(", "base_seed", ":", "int", ",", "worker_id", ":", "int", ",", "global_rank", ":", "int", ",", "count", ":", "int", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "combined_seed", "=", "(", "base_seed", "<", "<", "32", ")", "|", "(", "worker_id", "<", "<", "16", ")", "|", "global_rank", "seeds", "=", "[", "]", "for", "_", "in", "range", "(", "count", ")", ":", "combined_seed", "=", "(", "combined_seed", "*", "6364136223846793005", "+", "1", ")", "&", "(", "(", "1", "<", "<", "64", ")", "-", "1", ")", "seeds", ".", "append", "(", "combined_seed", ")", "return", "seeds"], "docstring": "Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)", "docstring_tokens": ["generates", "a", "sequence", "of", "seeds", "from", "a", "base", "seed", "worker", "id", "and", "rank", "using", "the", "linear", "congruential", "generator", "lcg"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 112, "end_line": 122, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_325", "original_string": "def _collect_rng_states(include_cuda: bool = True) -> dict[str, Any]:\r\n    r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"\r\n    states = {\r\n        \"torch\": torch.get_rng_state(),\r\n        \"python\": python_get_rng_state(),\r\n    }\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        states[\"numpy\"] = np.random.get_state()\r\n    if include_cuda:\r\n        states[\"torch.cuda\"] = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else []\r\n    return states", "language": "python", "code": "def _collect_rng_states(include_cuda: bool = True) -> dict[str, Any]:\r\n    r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"\r\n    states = {\r\n        \"torch\": torch.get_rng_state(),\r\n        \"python\": python_get_rng_state(),\r\n    }\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        states[\"numpy\"] = np.random.get_state()\r\n    if include_cuda:\r\n        states[\"torch.cuda\"] = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else []\r\n    return states", "code_tokens": ["def", "_collect_rng_states", "(", "include_cuda", ":", "bool", "=", "True", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "rSTRING", "states", "=", "{", "STRING", ":", "torch", ".", "get_rng_state", "(", ")", ",", "STRING", ":", "python_get_rng_state", "(", ")", ",", "}", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "states", "[", "STRING", "]", "=", "np", ".", "random", ".", "get_state", "(", ")", "if", "include_cuda", ":", "states", "[", "STRING", "]", "=", "torch", ".", "cuda", ".", "get_rng_state_all", "(", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", "return", "states"], "docstring": "r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"", "docstring_tokens": ["r", "collect", "the", "global", "random", "state", "of", "mod", "torch", "mod", "torch", "cuda", "mod", "numpy", "and", "python"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 125, "end_line": 137, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "function_326", "original_string": "def _set_rng_states(rng_state_dict: dict[str, Any]) -> None:\r\n    r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current\r\n    process.\"\"\"\r\n    torch.set_rng_state(rng_state_dict[\"torch\"])\r\n    if \"torch.cuda\" in rng_state_dict:\r\n        torch.cuda.set_rng_state_all(rng_state_dict[\"torch.cuda\"])\r\n    if _NUMPY_AVAILABLE and \"numpy\" in rng_state_dict:\r\n        import numpy as np\r\n\r\n        np.random.set_state(rng_state_dict[\"numpy\"])\r\n    version, state, gauss = rng_state_dict[\"python\"]\r\n    python_set_rng_state((version, tuple(state), gauss))", "language": "python", "code": "def _set_rng_states(rng_state_dict: dict[str, Any]) -> None:\r\n    r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current\r\n    process.\"\"\"\r\n    torch.set_rng_state(rng_state_dict[\"torch\"])\r\n    if \"torch.cuda\" in rng_state_dict:\r\n        torch.cuda.set_rng_state_all(rng_state_dict[\"torch.cuda\"])\r\n    if _NUMPY_AVAILABLE and \"numpy\" in rng_state_dict:\r\n        import numpy as np\r\n\r\n        np.random.set_state(rng_state_dict[\"numpy\"])\r\n    version, state, gauss = rng_state_dict[\"python\"]\r\n    python_set_rng_state((version, tuple(state), gauss))", "code_tokens": ["def", "_set_rng_states", "(", "rng_state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING", "torch", ".", "set_rng_state", "(", "rng_state_dict", "[", "STRING", "]", ")", "if", "STRING", "in", "rng_state_dict", ":", "torch", ".", "cuda", ".", "set_rng_state_all", "(", "rng_state_dict", "[", "STRING", "]", ")", "if", "_NUMPY_AVAILABLE", "and", "STRING", "in", "rng_state_dict", ":", "import", "numpy", "as", "np", "np", ".", "random", ".", "set_state", "(", "rng_state_dict", "[", "STRING", "]", ")", "version", ",", "state", ",", "gauss", "=", "rng_state_dict", "[", "STRING", "]", "python_set_rng_state", "(", "(", "version", ",", "tuple", "(", "state", ")", ",", "gauss", ")", ")"], "docstring": "r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current", "docstring_tokens": ["r", "set", "the", "global", "random", "state", "of", "mod", "torch", "mod", "torch", "cuda", "mod", "numpy", "and", "python", "in", "the", "current"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\seed.py", "start_line": 140, "end_line": 152, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\spike.py", "func_name": "function_327", "original_string": "def on_train_batch_end(self, fabric: \"Fabric\", loss: torch.Tensor, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Checks if we currently have a loss-spike.\"\"\"\r\n        if batch_idx == 0:\r\n            self.running_mean.to(fabric.strategy.root_device)\r\n\r\n        if self.exclude_batches_path is None:\r\n            self.exclude_batches_path = os.getcwd()\r\n\r\n        if not str(self.exclude_batches_path).endswith(\".json\"):\r\n            self.exclude_batches_path = os.path.join(self.exclude_batches_path, \"skip_batches.json\")\r\n\r\n        is_spike = bool(batch_idx >= self.warmup and self._is_spike(loss))\r\n        fabric.strategy.barrier()\r\n\r\n        is_spike_global = fabric.strategy.reduce_boolean_decision(is_spike, all=False)\r\n\r\n        if is_spike_global:\r\n            self._handle_spike(fabric, batch_idx)\r\n        else:\r\n            is_finite_all = self.finite_only or fabric.strategy.reduce_boolean_decision(\r\n                bool(torch.isfinite(loss).all()), all=True\r\n            )\r\n            if is_finite_all:\r\n                self._update_stats(loss)", "language": "python", "code": "def on_train_batch_end(self, fabric: \"Fabric\", loss: torch.Tensor, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Checks if we currently have a loss-spike.\"\"\"\r\n        if batch_idx == 0:\r\n            self.running_mean.to(fabric.strategy.root_device)\r\n\r\n        if self.exclude_batches_path is None:\r\n            self.exclude_batches_path = os.getcwd()\r\n\r\n        if not str(self.exclude_batches_path).endswith(\".json\"):\r\n            self.exclude_batches_path = os.path.join(self.exclude_batches_path, \"skip_batches.json\")\r\n\r\n        is_spike = bool(batch_idx >= self.warmup and self._is_spike(loss))\r\n        fabric.strategy.barrier()\r\n\r\n        is_spike_global = fabric.strategy.reduce_boolean_decision(is_spike, all=False)\r\n\r\n        if is_spike_global:\r\n            self._handle_spike(fabric, batch_idx)\r\n        else:\r\n            is_finite_all = self.finite_only or fabric.strategy.reduce_boolean_decision(\r\n                bool(torch.isfinite(loss).all()), all=True\r\n            )\r\n            if is_finite_all:\r\n                self._update_stats(loss)", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "fabric", ":", "STRING", ",", "loss", ":", "torch", ".", "Tensor", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING", "if", "batch_idx", "=", "=", "0", ":", "self", ".", "running_mean", ".", "to", "(", "fabric", ".", "strategy", ".", "root_device", ")", "if", "self", ".", "exclude_batches_path", "is", "None", ":", "self", ".", "exclude_batches_path", "=", "os", ".", "getcwd", "(", ")", "if", "not", "str", "(", "self", ".", "exclude_batches_path", ")", ".", "endswith", "(", "STRING", ")", ":", "self", ".", "exclude_batches_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "exclude_batches_path", ",", "STRING", ")", "is_spike", "=", "bool", "(", "batch_idx", ">", "=", "self", ".", "warmup", "and", "self", ".", "_is_spike", "(", "loss", ")", ")", "fabric", ".", "strategy", ".", "barrier", "(", ")", "is_spike_global", "=", "fabric", ".", "strategy", ".", "reduce_boolean_decision", "(", "is_spike", ",", "all", "=", "False", ")", "if", "is_spike_global", ":", "self", ".", "_handle_spike", "(", "fabric", ",", "batch_idx", ")", "else", ":", "is_finite_all", "=", "self", ".", "finite_only", "or", "fabric", ".", "strategy", ".", "reduce_boolean_decision", "(", "bool", "(", "torch", ".", "isfinite", "(", "loss", ")", ".", "all", "(", ")", ")", ",", "all", "=", "True", ")", "if", "is_finite_all", ":", "self", ".", "_update_stats", "(", "loss", ")"], "docstring": "Checks if we currently have a loss-spike.", "docstring_tokens": ["checks", "if", "we", "currently", "have", "a", "loss", "spike"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\spike.py", "start_line": 70, "end_line": 94, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "function_328", "original_string": "def update(\r\n        self,\r\n        *,\r\n        time: float,\r\n        batches: int,\r\n        samples: int,\r\n        lengths: Optional[int] = None,\r\n        flops: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Update throughput metrics.\r\n\r\n        Args:\r\n            time: Total elapsed time in seconds. It should monotonically increase by the iteration time with each\r\n                call.\r\n            batches: Total batches seen per device. It should monotonically increase with each call.\r\n            samples: Total samples seen per device. It should monotonically increase by the batch size with each call.\r\n            lengths: Total length of the samples seen. It should monotonically increase by the lengths of a batch with\r\n                each call.\r\n            flops: Flops elapased per device since last ``update()`` call. You can easily compute this by using\r\n                :func:`measure_flops` and multiplying it by the number of batches that have been processed.\r\n                The value might be different in each device if the batch size is not the same.\r\n\r\n        \"\"\"\r\n        self._time.append(time)\r\n        if samples < batches:\r\n            raise ValueError(f\"Expected samples ({samples}) to be greater or equal than batches ({batches})\")\r\n        self._batches.append(batches)\r\n        self._samples.append(samples)\r\n        if lengths is not None:\r\n            if lengths < samples:\r\n                raise ValueError(f\"Expected lengths ({lengths}) to be greater or equal than samples ({samples})\")\r\n            self._lengths.append(lengths)\r\n            if len(self._samples) != len(self._lengths):\r\n                raise RuntimeError(\r\n                    f\"If lengths are passed ({len(self._lengths)}), there needs to be the same number of samples\"\r\n                    f\" ({len(self._samples)})\"\r\n                )\r\n        if flops is not None:\r\n            self._flops.append(flops * self.world_size)", "language": "python", "code": "def update(\r\n        self,\r\n        *,\r\n        time: float,\r\n        batches: int,\r\n        samples: int,\r\n        lengths: Optional[int] = None,\r\n        flops: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Update throughput metrics.\r\n\r\n        Args:\r\n            time: Total elapsed time in seconds. It should monotonically increase by the iteration time with each\r\n                call.\r\n            batches: Total batches seen per device. It should monotonically increase with each call.\r\n            samples: Total samples seen per device. It should monotonically increase by the batch size with each call.\r\n            lengths: Total length of the samples seen. It should monotonically increase by the lengths of a batch with\r\n                each call.\r\n            flops: Flops elapased per device since last ``update()`` call. You can easily compute this by using\r\n                :func:`measure_flops` and multiplying it by the number of batches that have been processed.\r\n                The value might be different in each device if the batch size is not the same.\r\n\r\n        \"\"\"\r\n        self._time.append(time)\r\n        if samples < batches:\r\n            raise ValueError(f\"Expected samples ({samples}) to be greater or equal than batches ({batches})\")\r\n        self._batches.append(batches)\r\n        self._samples.append(samples)\r\n        if lengths is not None:\r\n            if lengths < samples:\r\n                raise ValueError(f\"Expected lengths ({lengths}) to be greater or equal than samples ({samples})\")\r\n            self._lengths.append(lengths)\r\n            if len(self._samples) != len(self._lengths):\r\n                raise RuntimeError(\r\n                    f\"If lengths are passed ({len(self._lengths)}), there needs to be the same number of samples\"\r\n                    f\" ({len(self._samples)})\"\r\n                )\r\n        if flops is not None:\r\n            self._flops.append(flops * self.world_size)", "code_tokens": ["def", "update", "(", "self", ",", "*", ",", "time", ":", "float", ",", "batches", ":", "int", ",", "samples", ":", "int", ",", "lengths", ":", "Optional", "[", "int", "]", "=", "None", ",", "flops", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "_time", ".", "append", "(", "time", ")", "if", "samples", "<", "batches", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "_batches", ".", "append", "(", "batches", ")", "self", ".", "_samples", ".", "append", "(", "samples", ")", "if", "lengths", "is", "not", "None", ":", "if", "lengths", "<", "samples", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "_lengths", ".", "append", "(", "lengths", ")", "if", "len", "(", "self", ".", "_samples", ")", "!", "=", "len", "(", "self", ".", "_lengths", ")", ":", "raise", "RuntimeError", "(", "fSTRING", "fSTRING", ")", "if", "flops", "is", "not", "None", ":", "self", ".", "_flops", ".", "append", "(", "flops", "*", "self", ".", "world_size", ")"], "docstring": "Update throughput metrics.", "docstring_tokens": ["update", "throughput", "metrics"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\throughput.py", "start_line": 112, "end_line": 151, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "function_329", "original_string": "def compute(self) -> _THROUGHPUT_METRICS:\r\n        \"\"\"Compute throughput metrics.\"\"\"\r\n        metrics = {\r\n            \"time\": self._time[-1],\r\n            \"batches\": self._batches[-1],\r\n            \"samples\": self._samples[-1],\r\n        }\r\n        if self._lengths:\r\n            metrics[\"lengths\"] = self._lengths[-1]\r\n\r\n        add_global_metrics = self.world_size > 1\r\n        if len(self._time) == self._time.maxlen:\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            elapsed_batches = self._batches[-1] - self._batches[0]\r\n            elapsed_samples = self._samples[-1] - self._samples[0]\r\n            dev_samples_per_sec = elapsed_samples / elapsed_time\r\n            dev_batches_per_sec = elapsed_batches / elapsed_time\r\n            metrics.update({\r\n                f\"device{self.separator}batches_per_sec\": elapsed_batches / elapsed_time,\r\n                f\"device{self.separator}samples_per_sec\": dev_samples_per_sec,\r\n            })\r\n            if add_global_metrics:\r\n                samples_per_sec = dev_batches_per_sec * self.world_size\r\n                metrics.update({\r\n                    \"batches_per_sec\": samples_per_sec,\r\n                    \"samples_per_sec\": dev_samples_per_sec * self.world_size,\r\n                })\r\n\r\n            if len(self._lengths) == self._lengths.maxlen:\r\n                elapsed_lengths = self._lengths[-1] - self._lengths[0]\r\n                dev_items_per_sec = elapsed_lengths / elapsed_time\r\n                metrics[f\"device{self.separator}items_per_sec\"] = dev_items_per_sec\r\n                if add_global_metrics:\r\n                    items_per_sec = dev_items_per_sec * self.world_size\r\n                    metrics[\"items_per_sec\"] = items_per_sec\r\n\r\n        if len(self._flops) == self._flops.maxlen:\r\n            elapsed_flops = sum(self._flops) - self._flops[0]\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            flops_per_sec = elapsed_flops / elapsed_time\r\n            dev_flops_per_sec = flops_per_sec / self.world_size\r\n            if add_global_metrics:\r\n                metrics[\"flops_per_sec\"] = flops_per_sec\r\n            metrics[f\"device{self.separator}flops_per_sec\"] = dev_flops_per_sec\r\n            if self.available_flops:\r\n                metrics[f\"device{self.separator}mfu\"] = dev_flops_per_sec / self.available_flops\r\n\r\n        return metrics", "language": "python", "code": "def compute(self) -> _THROUGHPUT_METRICS:\r\n        \"\"\"Compute throughput metrics.\"\"\"\r\n        metrics = {\r\n            \"time\": self._time[-1],\r\n            \"batches\": self._batches[-1],\r\n            \"samples\": self._samples[-1],\r\n        }\r\n        if self._lengths:\r\n            metrics[\"lengths\"] = self._lengths[-1]\r\n\r\n        add_global_metrics = self.world_size > 1\r\n        if len(self._time) == self._time.maxlen:\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            elapsed_batches = self._batches[-1] - self._batches[0]\r\n            elapsed_samples = self._samples[-1] - self._samples[0]\r\n            dev_samples_per_sec = elapsed_samples / elapsed_time\r\n            dev_batches_per_sec = elapsed_batches / elapsed_time\r\n            metrics.update({\r\n                f\"device{self.separator}batches_per_sec\": elapsed_batches / elapsed_time,\r\n                f\"device{self.separator}samples_per_sec\": dev_samples_per_sec,\r\n            })\r\n            if add_global_metrics:\r\n                samples_per_sec = dev_batches_per_sec * self.world_size\r\n                metrics.update({\r\n                    \"batches_per_sec\": samples_per_sec,\r\n                    \"samples_per_sec\": dev_samples_per_sec * self.world_size,\r\n                })\r\n\r\n            if len(self._lengths) == self._lengths.maxlen:\r\n                elapsed_lengths = self._lengths[-1] - self._lengths[0]\r\n                dev_items_per_sec = elapsed_lengths / elapsed_time\r\n                metrics[f\"device{self.separator}items_per_sec\"] = dev_items_per_sec\r\n                if add_global_metrics:\r\n                    items_per_sec = dev_items_per_sec * self.world_size\r\n                    metrics[\"items_per_sec\"] = items_per_sec\r\n\r\n        if len(self._flops) == self._flops.maxlen:\r\n            elapsed_flops = sum(self._flops) - self._flops[0]\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            flops_per_sec = elapsed_flops / elapsed_time\r\n            dev_flops_per_sec = flops_per_sec / self.world_size\r\n            if add_global_metrics:\r\n                metrics[\"flops_per_sec\"] = flops_per_sec\r\n            metrics[f\"device{self.separator}flops_per_sec\"] = dev_flops_per_sec\r\n            if self.available_flops:\r\n                metrics[f\"device{self.separator}mfu\"] = dev_flops_per_sec / self.available_flops\r\n\r\n        return metrics", "code_tokens": ["def", "compute", "(", "self", ")", "-", ">", "_THROUGHPUT_METRICS", ":", "STRING", "metrics", "=", "{", "STRING", ":", "self", ".", "_time", "[", "-", "1", "]", ",", "STRING", ":", "self", ".", "_batches", "[", "-", "1", "]", ",", "STRING", ":", "self", ".", "_samples", "[", "-", "1", "]", ",", "}", "if", "self", ".", "_lengths", ":", "metrics", "[", "STRING", "]", "=", "self", ".", "_lengths", "[", "-", "1", "]", "add_global_metrics", "=", "self", ".", "world_size", ">", "1", "if", "len", "(", "self", ".", "_time", ")", "=", "=", "self", ".", "_time", ".", "maxlen", ":", "elapsed_time", "=", "self", ".", "_time", "[", "-", "1", "]", "-", "self", ".", "_time", "[", "0", "]", "elapsed_batches", "=", "self", ".", "_batches", "[", "-", "1", "]", "-", "self", ".", "_batches", "[", "0", "]", "elapsed_samples", "=", "self", ".", "_samples", "[", "-", "1", "]", "-", "self", ".", "_samples", "[", "0", "]", "dev_samples_per_sec", "=", "elapsed_samples", "/", "elapsed_time", "dev_batches_per_sec", "=", "elapsed_batches", "/", "elapsed_time", "metrics", ".", "update", "(", "{", "fSTRING", ":", "elapsed_batches", "/", "elapsed_time", ",", "fSTRING", ":", "dev_samples_per_sec", ",", "}", ")", "if", "add_global_metrics", ":", "samples_per_sec", "=", "dev_batches_per_sec", "*", "self", ".", "world_size", "metrics", ".", "update", "(", "{", "STRING", ":", "samples_per_sec", ",", "STRING", ":", "dev_samples_per_sec", "*", "self", ".", "world_size", ",", "}", ")", "if", "len", "(", "self", ".", "_lengths", ")", "=", "=", "self", ".", "_lengths", ".", "maxlen", ":", "elapsed_lengths", "=", "self", ".", "_lengths", "[", "-", "1", "]", "-", "self", ".", "_lengths", "[", "0", "]", "dev_items_per_sec", "=", "elapsed_lengths", "/", "elapsed_time", "metrics", "[", "fSTRING", "]", "=", "dev_items_per_sec", "if", "add_global_metrics", ":", "items_per_sec", "=", "dev_items_per_sec", "*", "self", ".", "world_size", "metrics", "[", "STRING", "]", "=", "items_per_sec", "if", "len", "(", "self", ".", "_flops", ")", "=", "=", "self", ".", "_flops", ".", "maxlen", ":", "elapsed_flops", "=", "sum", "(", "self", ".", "_flops", ")", "-", "self", ".", "_flops", "[", "0", "]", "elapsed_time", "=", "self", ".", "_time", "[", "-", "1", "]", "-", "self", ".", "_time", "[", "0", "]", "flops_per_sec", "=", "elapsed_flops", "/", "elapsed_time", "dev_flops_per_sec", "=", "flops_per_sec", "/", "self", ".", "world_size", "if", "add_global_metrics", ":", "metrics", "[", "STRING", "]", "=", "flops_per_sec", "metrics", "[", "fSTRING", "]", "=", "dev_flops_per_sec", "if", "self", ".", "available_flops", ":", "metrics", "[", "fSTRING", "]", "=", "dev_flops_per_sec", "/", "self", ".", "available_flops", "return", "metrics"], "docstring": "Compute throughput metrics.", "docstring_tokens": ["compute", "throughput", "metrics"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\throughput.py", "start_line": 153, "end_line": 203, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "function_330", "original_string": "def compute_and_log(self, step: Optional[int] = None, **kwargs: Any) -> _THROUGHPUT_METRICS:\r\n        r\"\"\"See :meth:`Throughput.compute`\r\n\r\n        Args:\r\n            step: Can be used to override the logging step.\r\n            \\**kwargs: See available parameters in :meth:`Throughput.compute`\r\n\r\n        \"\"\"\r\n        self.step = (self.step + 1) if step is None else step\r\n        metrics = self.compute(**kwargs)\r\n        self._fabric.log_dict(metrics=metrics, step=self.step)\r\n        return metrics", "language": "python", "code": "def compute_and_log(self, step: Optional[int] = None, **kwargs: Any) -> _THROUGHPUT_METRICS:\r\n        r\"\"\"See :meth:`Throughput.compute`\r\n\r\n        Args:\r\n            step: Can be used to override the logging step.\r\n            \\**kwargs: See available parameters in :meth:`Throughput.compute`\r\n\r\n        \"\"\"\r\n        self.step = (self.step + 1) if step is None else step\r\n        metrics = self.compute(**kwargs)\r\n        self._fabric.log_dict(metrics=metrics, step=self.step)\r\n        return metrics", "code_tokens": ["def", "compute_and_log", "(", "self", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "_THROUGHPUT_METRICS", ":", "rSTRING", "self", ".", "step", "=", "(", "self", ".", "step", "+", "1", ")", "if", "step", "is", "None", "else", "step", "metrics", "=", "self", ".", "compute", "(", "*", "*", "kwargs", ")", "self", ".", "_fabric", ".", "log_dict", "(", "metrics", "=", "metrics", ",", "step", "=", "self", ".", "step", ")", "return", "metrics"], "docstring": "r\"\"\"See :meth:`Throughput.compute`", "docstring_tokens": ["r", "see", "meth", "throughput", "compute"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\throughput.py", "start_line": 251, "end_line": 262, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "function_331", "original_string": "def measure_flops(\r\n    model: torch.nn.Module,\r\n    forward_fn: Callable[[], torch.Tensor],\r\n    loss_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\r\n) -> int:\r\n    \"\"\"Utility to compute the total number of FLOPs used by a module during training or during inference.\r\n\r\n    It's recommended to create a meta-device model for this:\r\n\r\n    Example::\r\n\r\n        with torch.device(\"meta\"):\r\n            model = MyModel()\r\n            x = torch.randn(2, 32)\r\n\r\n        model_fwd = lambda: model(x)\r\n        fwd_flops = measure_flops(model, model_fwd)\r\n\r\n        model_loss = lambda y: y.sum()\r\n        fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n\r\n    Args:\r\n        model: The model whose FLOPs should be measured.\r\n        forward_fn: A function that runs ``forward`` on the model and returns the result.\r\n        loss_fn: A function that computes the loss given the ``forward_fn`` output. If provided, the loss and `backward`\r\n            FLOPs will be included in the result.\r\n\r\n    \"\"\"\r\n    from torch.utils.flop_counter import FlopCounterMode\r\n\r\n    flop_counter = FlopCounterMode(display=False)\r\n    with flop_counter:\r\n        if loss_fn is None:\r\n            forward_fn()\r\n        else:\r\n            loss_fn(forward_fn()).backward()\r\n    return flop_counter.get_total_flops()", "language": "python", "code": "def measure_flops(\r\n    model: torch.nn.Module,\r\n    forward_fn: Callable[[], torch.Tensor],\r\n    loss_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\r\n) -> int:\r\n    \"\"\"Utility to compute the total number of FLOPs used by a module during training or during inference.\r\n\r\n    It's recommended to create a meta-device model for this:\r\n\r\n    Example::\r\n\r\n        with torch.device(\"meta\"):\r\n            model = MyModel()\r\n            x = torch.randn(2, 32)\r\n\r\n        model_fwd = lambda: model(x)\r\n        fwd_flops = measure_flops(model, model_fwd)\r\n\r\n        model_loss = lambda y: y.sum()\r\n        fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n\r\n    Args:\r\n        model: The model whose FLOPs should be measured.\r\n        forward_fn: A function that runs ``forward`` on the model and returns the result.\r\n        loss_fn: A function that computes the loss given the ``forward_fn`` output. If provided, the loss and `backward`\r\n            FLOPs will be included in the result.\r\n\r\n    \"\"\"\r\n    from torch.utils.flop_counter import FlopCounterMode\r\n\r\n    flop_counter = FlopCounterMode(display=False)\r\n    with flop_counter:\r\n        if loss_fn is None:\r\n            forward_fn()\r\n        else:\r\n            loss_fn(forward_fn()).backward()\r\n    return flop_counter.get_total_flops()", "code_tokens": ["def", "measure_flops", "(", "model", ":", "torch", ".", "nn", ".", "Module", ",", "forward_fn", ":", "Callable", "[", "[", "]", ",", "torch", ".", "Tensor", "]", ",", "loss_fn", ":", "Optional", "[", "Callable", "[", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", ")", "-", ">", "int", ":", "STRING", "from", "torch", ".", "utils", ".", "flop_counter", "import", "FlopCounterMode", "flop_counter", "=", "FlopCounterMode", "(", "display", "=", "False", ")", "with", "flop_counter", ":", "if", "loss_fn", "is", "None", ":", "forward_fn", "(", ")", "else", ":", "loss_fn", "(", "forward_fn", "(", ")", ")", ".", "backward", "(", ")", "return", "flop_counter", ".", "get_total_flops", "(", ")"], "docstring": "Utility to compute the total number of FLOPs used by a module during training or during inference.", "docstring_tokens": ["utility", "to", "compute", "the", "total", "number", "of", "flops", "used", "by", "a", "module", "during", "training", "or", "during", "inference"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\throughput.py", "start_line": 265, "end_line": 301, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "function_332", "original_string": "def get_available_flops(device: torch.device, dtype: Union[torch.dtype, str]) -> Optional[int]:\r\n    \"\"\"Returns the available theoretical FLOPs.\r\n\r\n    This is an optimistic upper limit that could only be achievable if only thick matmuls were run in a benchmark\r\n    environment.\r\n\r\n    \"\"\"\r\n    if device.type == \"cuda\":\r\n        device_name = torch.cuda.get_device_name(device)\r\n        chip = device_name.lower()\r\n        if \"h200\" in chip:\r\n            if \"sxm1\" in chip:\r\n                chip = \"h200 sxm1\"\r\n            elif \"nvl1\" in chip:\r\n                chip = \"h200 nvl1\"\r\n        elif \"h100\" in chip:\r\n            if \"hbm3\" in chip:\r\n                chip = \"h100 sxm\"\r\n            elif \"nvl\" in chip:\r\n                chip = \"h100 nvl\"\r\n            elif \"pcie\" in chip or \"hbm2e\" in chip:\r\n                chip = \"h100 pcie\"\r\n        elif \"l4\" in chip:\r\n            chip = \"l40\" if \"tesla\" in chip else \"l4\"\r\n        elif \"geforce rtx\" in chip:\r\n            number = chip.split(\" \")[3]\r\n            extra = \"\"\r\n            if \"super\" in chip:\r\n                extra = \" super\"\r\n            elif \"ti\" in chip:\r\n                extra = \" ti\"\r\n            chip = f\"rtx {number}{extra}\"\r\n        elif \"a6000\" in chip:\r\n            chip = \"a6000\"\r\n        elif \"a100\" in chip:\r\n            chip = \"a100\"\r\n        elif \"a40\" in chip:\r\n            chip = \"a40\"\r\n        elif \"a10g\" in chip:\r\n            chip = \"a10g\"\r\n        elif \"t4\" in chip:\r\n            chip = \"t4\"\r\n        elif \"quadro rtx 5000\" in chip:\r\n            chip = \"quadro rtx 5000\"\r\n        elif \"titan rtx\" in chip:\r\n            chip = \"titan rtx\"\r\n        elif \"v100-sxm\" in chip:\r\n            chip = \"v100 sxm\"\r\n        elif \"v100-pcie\" in chip:\r\n            chip = \"v100 pcie\"\r\n        elif \"v100s-pcie\" in chip:\r\n            chip = \"v100s pcie\"\r\n        else:\r\n            rank_zero_warn(f\"FLOPs not found for {device_name!r}\")\r\n            return None\r\n        if chip not in _CUDA_FLOPS:\r\n            rank_zero_warn(f\"FLOPs not found for {device_name!r}, chip is {chip!r}\")\r\n            return None\r\n        dtype_to_flops = _CUDA_FLOPS[chip]\r\n        if dtype is torch.float32:\r\n            from lightning.fabric.accelerators.cuda import _is_ampere_or_later\r\n\r\n            if _is_ampere_or_later() and torch.get_float32_matmul_precision() != \"highest\":\r\n                dtype = \"tfloat32\"\r\n        if dtype not in dtype_to_flops:\r\n            rank_zero_warn(f\"{device_name!r} does not support {dtype}\")\r\n            return None\r\n        return int(dtype_to_flops[dtype])\r\n\r\n    if device.type == \"xla\":\r\n        from lightning.fabric.accelerators.xla import _XLA_GREATER_EQUAL_2_1\r\n\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n        else:\r\n            from torch_xla.experimental import tpu\r\n\r\n        tpu_env = tpu.get_tpu_env()\r\n        device_name = tpu_env.get(\"TYPE\") or tpu_env[\"ACCELERATOR_TYPE\"].split(\"-\")[0]\r\n        chip = device_name.lower()\r\n        assert isinstance(device_name, str)\r\n        if chip not in _TPU_FLOPS:\r\n            rank_zero_warn(f\"FLOPs not found for TPU {device_name!r} with {dtype}\")\r\n            return None\r\n        return int(_TPU_FLOPS[chip])", "language": "python", "code": "def get_available_flops(device: torch.device, dtype: Union[torch.dtype, str]) -> Optional[int]:\r\n    \"\"\"Returns the available theoretical FLOPs.\r\n\r\n    This is an optimistic upper limit that could only be achievable if only thick matmuls were run in a benchmark\r\n    environment.\r\n\r\n    \"\"\"\r\n    if device.type == \"cuda\":\r\n        device_name = torch.cuda.get_device_name(device)\r\n        chip = device_name.lower()\r\n        if \"h200\" in chip:\r\n            if \"sxm1\" in chip:\r\n                chip = \"h200 sxm1\"\r\n            elif \"nvl1\" in chip:\r\n                chip = \"h200 nvl1\"\r\n        elif \"h100\" in chip:\r\n            if \"hbm3\" in chip:\r\n                chip = \"h100 sxm\"\r\n            elif \"nvl\" in chip:\r\n                chip = \"h100 nvl\"\r\n            elif \"pcie\" in chip or \"hbm2e\" in chip:\r\n                chip = \"h100 pcie\"\r\n        elif \"l4\" in chip:\r\n            chip = \"l40\" if \"tesla\" in chip else \"l4\"\r\n        elif \"geforce rtx\" in chip:\r\n            number = chip.split(\" \")[3]\r\n            extra = \"\"\r\n            if \"super\" in chip:\r\n                extra = \" super\"\r\n            elif \"ti\" in chip:\r\n                extra = \" ti\"\r\n            chip = f\"rtx {number}{extra}\"\r\n        elif \"a6000\" in chip:\r\n            chip = \"a6000\"\r\n        elif \"a100\" in chip:\r\n            chip = \"a100\"\r\n        elif \"a40\" in chip:\r\n            chip = \"a40\"\r\n        elif \"a10g\" in chip:\r\n            chip = \"a10g\"\r\n        elif \"t4\" in chip:\r\n            chip = \"t4\"\r\n        elif \"quadro rtx 5000\" in chip:\r\n            chip = \"quadro rtx 5000\"\r\n        elif \"titan rtx\" in chip:\r\n            chip = \"titan rtx\"\r\n        elif \"v100-sxm\" in chip:\r\n            chip = \"v100 sxm\"\r\n        elif \"v100-pcie\" in chip:\r\n            chip = \"v100 pcie\"\r\n        elif \"v100s-pcie\" in chip:\r\n            chip = \"v100s pcie\"\r\n        else:\r\n            rank_zero_warn(f\"FLOPs not found for {device_name!r}\")\r\n            return None\r\n        if chip not in _CUDA_FLOPS:\r\n            rank_zero_warn(f\"FLOPs not found for {device_name!r}, chip is {chip!r}\")\r\n            return None\r\n        dtype_to_flops = _CUDA_FLOPS[chip]\r\n        if dtype is torch.float32:\r\n            from lightning.fabric.accelerators.cuda import _is_ampere_or_later\r\n\r\n            if _is_ampere_or_later() and torch.get_float32_matmul_precision() != \"highest\":\r\n                dtype = \"tfloat32\"\r\n        if dtype not in dtype_to_flops:\r\n            rank_zero_warn(f\"{device_name!r} does not support {dtype}\")\r\n            return None\r\n        return int(dtype_to_flops[dtype])\r\n\r\n    if device.type == \"xla\":\r\n        from lightning.fabric.accelerators.xla import _XLA_GREATER_EQUAL_2_1\r\n\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n        else:\r\n            from torch_xla.experimental import tpu\r\n\r\n        tpu_env = tpu.get_tpu_env()\r\n        device_name = tpu_env.get(\"TYPE\") or tpu_env[\"ACCELERATOR_TYPE\"].split(\"-\")[0]\r\n        chip = device_name.lower()\r\n        assert isinstance(device_name, str)\r\n        if chip not in _TPU_FLOPS:\r\n            rank_zero_warn(f\"FLOPs not found for TPU {device_name!r} with {dtype}\")\r\n            return None\r\n        return int(_TPU_FLOPS[chip])", "code_tokens": ["def", "get_available_flops", "(", "device", ":", "torch", ".", "device", ",", "dtype", ":", "Union", "[", "torch", ".", "dtype", ",", "str", "]", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "if", "device", ".", "type", "=", "=", "STRING", ":", "device_name", "=", "torch", ".", "cuda", ".", "get_device_name", "(", "device", ")", "chip", "=", "device_name", ".", "lower", "(", ")", "if", "STRING", "in", "chip", ":", "if", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "if", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", "or", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "if", "STRING", "in", "chip", "else", "STRING", "elif", "STRING", "in", "chip", ":", "number", "=", "chip", ".", "split", "(", "STRING", ")", "[", "3", "]", "extra", "=", "STRING", "if", "STRING", "in", "chip", ":", "extra", "=", "STRING", "elif", "STRING", "in", "chip", ":", "extra", "=", "STRING", "chip", "=", "fSTRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "elif", "STRING", "in", "chip", ":", "chip", "=", "STRING", "else", ":", "rank_zero_warn", "(", "fSTRING", ")", "return", "None", "if", "chip", "not", "in", "_CUDA_FLOPS", ":", "rank_zero_warn", "(", "fSTRING", ")", "return", "None", "dtype_to_flops", "=", "_CUDA_FLOPS", "[", "chip", "]", "if", "dtype", "is", "torch", ".", "float32", ":", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "cuda", "import", "_is_ampere_or_later", "if", "_is_ampere_or_later", "(", ")", "and", "torch", ".", "get_float32_matmul_precision", "(", ")", "!", "=", "STRING", ":", "dtype", "=", "STRING", "if", "dtype", "not", "in", "dtype_to_flops", ":", "rank_zero_warn", "(", "fSTRING", ")", "return", "None", "return", "int", "(", "dtype_to_flops", "[", "dtype", "]", ")", "if", "device", ".", "type", "=", "=", "STRING", ":", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "xla", "import", "_XLA_GREATER_EQUAL_2_1", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", ".", "_internal", "import", "tpu", "else", ":", "from", "torch_xla", ".", "experimental", "import", "tpu", "tpu_env", "=", "tpu", ".", "get_tpu_env", "(", ")", "device_name", "=", "tpu_env", ".", "get", "(", "STRING", ")", "or", "tpu_env", "[", "STRING", "]", ".", "split", "(", "STRING", ")", "[", "0", "]", "chip", "=", "device_name", ".", "lower", "(", ")", "assert", "isinstance", "(", "device_name", ",", "str", ")", "if", "chip", "not", "in", "_TPU_FLOPS", ":", "rank_zero_warn", "(", "fSTRING", ")", "return", "None", "return", "int", "(", "_TPU_FLOPS", "[", "chip", "]", ")"], "docstring": "Returns the available theoretical FLOPs.", "docstring_tokens": ["returns", "the", "available", "theoretical", "flops"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\throughput.py", "start_line": 545, "end_line": 633, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\warnings.py", "func_name": "function_333", "original_string": "def disable_possible_user_warnings(module: str = \"\") -> None:\r\n    \"\"\"Ignore warnings of the category ``PossibleUserWarning`` from Lightning.\r\n\r\n    For more granular control over which warnings to ignore, use :func:`warnings.filterwarnings` directly.\r\n\r\n    Args:\r\n        module: Name of the module for which the warnings should be ignored (e.g., ``'lightning.pytorch.strategies'``).\r\n            Default: Disables warnings from all modules.\r\n\r\n    \"\"\"\r\n    warnings.filterwarnings(\"ignore\", module=module, category=PossibleUserWarning)", "language": "python", "code": "def disable_possible_user_warnings(module: str = \"\") -> None:\r\n    \"\"\"Ignore warnings of the category ``PossibleUserWarning`` from Lightning.\r\n\r\n    For more granular control over which warnings to ignore, use :func:`warnings.filterwarnings` directly.\r\n\r\n    Args:\r\n        module: Name of the module for which the warnings should be ignored (e.g., ``'lightning.pytorch.strategies'``).\r\n            Default: Disables warnings from all modules.\r\n\r\n    \"\"\"\r\n    warnings.filterwarnings(\"ignore\", module=module, category=PossibleUserWarning)", "code_tokens": ["def", "disable_possible_user_warnings", "(", "module", ":", "str", "=", "STRING", ")", "-", ">", "None", ":", "STRING", "warnings", ".", "filterwarnings", "(", "STRING", ",", "module", "=", "module", ",", "category", "=", "PossibleUserWarning", ")"], "docstring": "Ignore warnings of the category ``PossibleUserWarning`` from Lightning.", "docstring_tokens": ["ignore", "warnings", "of", "the", "category", "possibleuserwarning", "from", "lightning"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\warnings.py", "start_line": 26, "end_line": 36, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\warnings.py", "func_name": "function_334", "original_string": "def _custom_format_warning(\r\n    message: Union[Warning, str], category: type[Warning], filename: str, lineno: int, line: Optional[str] = None\r\n) -> str:\r\n    \"\"\"Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.\"\"\"\r\n    if _is_path_in_lightning(Path(filename)):\r\n        return f\"{filename}:{lineno}: {message}\\n\"\r\n    return _default_format_warning(message, category, filename, lineno, line)", "language": "python", "code": "def _custom_format_warning(\r\n    message: Union[Warning, str], category: type[Warning], filename: str, lineno: int, line: Optional[str] = None\r\n) -> str:\r\n    \"\"\"Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.\"\"\"\r\n    if _is_path_in_lightning(Path(filename)):\r\n        return f\"{filename}:{lineno}: {message}\\n\"\r\n    return _default_format_warning(message, category, filename, lineno, line)", "code_tokens": ["def", "_custom_format_warning", "(", "message", ":", "Union", "[", "Warning", ",", "str", "]", ",", "category", ":", "type", "[", "Warning", "]", ",", "filename", ":", "str", ",", "lineno", ":", "int", ",", "line", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "str", ":", "STRING", "if", "_is_path_in_lightning", "(", "Path", "(", "filename", ")", ")", ":", "return", "fSTRING", "return", "_default_format_warning", "(", "message", ",", "category", ",", "filename", ",", "lineno", ",", "line", ")"], "docstring": "Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.", "docstring_tokens": ["custom", "formatting", "that", "avoids", "an", "extra", "line", "in", "case", "warnings", "are", "emitted", "from", "the", "rank_zero", "functions"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\warnings.py", "start_line": 39, "end_line": 46, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\warnings.py", "func_name": "function_335", "original_string": "def _is_path_in_lightning(path: Path) -> bool:\r\n    \"\"\"Naive check whether the path looks like a path from the lightning package.\"\"\"\r\n    return \"lightning\" in str(path.absolute())", "language": "python", "code": "def _is_path_in_lightning(path: Path) -> bool:\r\n    \"\"\"Naive check whether the path looks like a path from the lightning package.\"\"\"\r\n    return \"lightning\" in str(path.absolute())", "code_tokens": ["def", "_is_path_in_lightning", "(", "path", ":", "Path", ")", "-", ">", "bool", ":", "STRING", "return", "STRING", "in", "str", "(", "path", ".", "absolute", "(", ")", ")"], "docstring": "Naive check whether the path looks like a path from the lightning package.", "docstring_tokens": ["naive", "check", "whether", "the", "path", "looks", "like", "a", "path", "from", "the", "lightning", "package"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\fabric\\utilities\\warnings.py", "start_line": 56, "end_line": 58, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_336", "original_string": "def __init__(\r\n        self,\r\n        *args: Any,\r\n        description: str = \"Lightning Trainer command line tool\",\r\n        env_prefix: str = \"PL\",\r\n        default_env: bool = False,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        \"\"\"Initialize argument parser that supports configuration file input.\r\n\r\n        For full details of accepted arguments see `ArgumentParser.__init__\r\n        <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentParser.__init__>`_.\r\n\r\n        Args:\r\n            description: Description of the tool shown when running ``--help``.\r\n            env_prefix: Prefix for environment variables. Set ``default_env=True`` to enable env parsing.\r\n            default_env: Whether to parse environment variables.\r\n\r\n        \"\"\"\r\n        if not _JSONARGPARSE_SIGNATURES_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"{_JSONARGPARSE_SIGNATURES_AVAILABLE}\")\r\n        super().__init__(*args, description=description, env_prefix=env_prefix, default_env=default_env, **kwargs)\r\n        self.callback_keys: list[str] = []\r\n        self._optimizers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}\r\n        self._lr_schedulers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}", "language": "python", "code": "def __init__(\r\n        self,\r\n        *args: Any,\r\n        description: str = \"Lightning Trainer command line tool\",\r\n        env_prefix: str = \"PL\",\r\n        default_env: bool = False,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        \"\"\"Initialize argument parser that supports configuration file input.\r\n\r\n        For full details of accepted arguments see `ArgumentParser.__init__\r\n        <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentParser.__init__>`_.\r\n\r\n        Args:\r\n            description: Description of the tool shown when running ``--help``.\r\n            env_prefix: Prefix for environment variables. Set ``default_env=True`` to enable env parsing.\r\n            default_env: Whether to parse environment variables.\r\n\r\n        \"\"\"\r\n        if not _JSONARGPARSE_SIGNATURES_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"{_JSONARGPARSE_SIGNATURES_AVAILABLE}\")\r\n        super().__init__(*args, description=description, env_prefix=env_prefix, default_env=default_env, **kwargs)\r\n        self.callback_keys: list[str] = []\r\n        self._optimizers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}\r\n        self._lr_schedulers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}", "code_tokens": ["def", "__init__", "(", "self", ",", "*", "args", ":", "Any", ",", "description", ":", "str", "=", "STRING", ",", "env_prefix", ":", "str", "=", "STRING", ",", "default_env", ":", "bool", "=", "False", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "STRING", "if", "not", "_JSONARGPARSE_SIGNATURES_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "fSTRING", ")", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "description", "=", "description", ",", "env_prefix", "=", "env_prefix", ",", "default_env", "=", "default_env", ",", "*", "*", "kwargs", ")", "self", ".", "callback_keys", ":", "list", "[", "str", "]", "=", "[", "]", "self", ".", "_optimizers", ":", "dict", "[", "str", ",", "tuple", "[", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "str", "]", "]", "=", "{", "}", "self", ".", "_lr_schedulers", ":", "dict", "[", "str", ",", "tuple", "[", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "str", "]", "]", "=", "{", "}"], "docstring": "Initialize argument parser that supports configuration file input.", "docstring_tokens": ["initialize", "argument", "parser", "that", "supports", "configuration", "file", "input"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 88, "end_line": 113, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_337", "original_string": "def add_lightning_class_args(\r\n        self,\r\n        lightning_class: Union[\r\n            Callable[..., Union[Trainer, LightningModule, LightningDataModule, Callback]],\r\n            type[Trainer],\r\n            type[LightningModule],\r\n            type[LightningDataModule],\r\n            type[Callback],\r\n        ],\r\n        nested_key: str,\r\n        subclass_mode: bool = False,\r\n        required: bool = True,\r\n    ) -> list[str]:\r\n        \"\"\"Adds arguments from a lightning class to a nested key of the parser.\r\n\r\n        Args:\r\n            lightning_class: A callable or any subclass of {Trainer, LightningModule, LightningDataModule, Callback}.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            subclass_mode: Whether allow any subclass of the given class.\r\n            required: Whether the argument group is required.\r\n\r\n        Returns:\r\n            A list with the names of the class arguments added.\r\n\r\n        \"\"\"\r\n        if callable(lightning_class) and not isinstance(lightning_class, type):\r\n            lightning_class = class_from_function(lightning_class)\r\n\r\n        if isinstance(lightning_class, type) and issubclass(\r\n            lightning_class, (Trainer, LightningModule, LightningDataModule, Callback)\r\n        ):\r\n            if issubclass(lightning_class, Callback):\r\n                self.callback_keys.append(nested_key)\r\n            if subclass_mode:\r\n                return self.add_subclass_arguments(lightning_class, nested_key, fail_untyped=False, required=required)\r\n            return self.add_class_arguments(\r\n                lightning_class,\r\n                nested_key,\r\n                fail_untyped=False,\r\n                instantiate=not issubclass(lightning_class, Trainer),\r\n                sub_configs=True,\r\n            )\r\n        raise MisconfigurationException(\r\n            f\"Cannot add arguments from: {lightning_class}. You should provide either a callable or a subclass of: \"\r\n            \"Trainer, LightningModule, LightningDataModule, or Callback.\"\r\n        )", "language": "python", "code": "def add_lightning_class_args(\r\n        self,\r\n        lightning_class: Union[\r\n            Callable[..., Union[Trainer, LightningModule, LightningDataModule, Callback]],\r\n            type[Trainer],\r\n            type[LightningModule],\r\n            type[LightningDataModule],\r\n            type[Callback],\r\n        ],\r\n        nested_key: str,\r\n        subclass_mode: bool = False,\r\n        required: bool = True,\r\n    ) -> list[str]:\r\n        \"\"\"Adds arguments from a lightning class to a nested key of the parser.\r\n\r\n        Args:\r\n            lightning_class: A callable or any subclass of {Trainer, LightningModule, LightningDataModule, Callback}.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            subclass_mode: Whether allow any subclass of the given class.\r\n            required: Whether the argument group is required.\r\n\r\n        Returns:\r\n            A list with the names of the class arguments added.\r\n\r\n        \"\"\"\r\n        if callable(lightning_class) and not isinstance(lightning_class, type):\r\n            lightning_class = class_from_function(lightning_class)\r\n\r\n        if isinstance(lightning_class, type) and issubclass(\r\n            lightning_class, (Trainer, LightningModule, LightningDataModule, Callback)\r\n        ):\r\n            if issubclass(lightning_class, Callback):\r\n                self.callback_keys.append(nested_key)\r\n            if subclass_mode:\r\n                return self.add_subclass_arguments(lightning_class, nested_key, fail_untyped=False, required=required)\r\n            return self.add_class_arguments(\r\n                lightning_class,\r\n                nested_key,\r\n                fail_untyped=False,\r\n                instantiate=not issubclass(lightning_class, Trainer),\r\n                sub_configs=True,\r\n            )\r\n        raise MisconfigurationException(\r\n            f\"Cannot add arguments from: {lightning_class}. You should provide either a callable or a subclass of: \"\r\n            \"Trainer, LightningModule, LightningDataModule, or Callback.\"\r\n        )", "code_tokens": ["def", "add_lightning_class_args", "(", "self", ",", "lightning_class", ":", "Union", "[", "Callable", "[", ".", ".", ".", ",", "Union", "[", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "Callback", "]", "]", ",", "type", "[", "Trainer", "]", ",", "type", "[", "LightningModule", "]", ",", "type", "[", "LightningDataModule", "]", ",", "type", "[", "Callback", "]", ",", "]", ",", "nested_key", ":", "str", ",", "subclass_mode", ":", "bool", "=", "False", ",", "required", ":", "bool", "=", "True", ",", ")", "-", ">", "list", "[", "str", "]", ":", "STRING", "if", "callable", "(", "lightning_class", ")", "and", "not", "isinstance", "(", "lightning_class", ",", "type", ")", ":", "lightning_class", "=", "class_from_function", "(", "lightning_class", ")", "if", "isinstance", "(", "lightning_class", ",", "type", ")", "and", "issubclass", "(", "lightning_class", ",", "(", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "Callback", ")", ")", ":", "if", "issubclass", "(", "lightning_class", ",", "Callback", ")", ":", "self", ".", "callback_keys", ".", "append", "(", "nested_key", ")", "if", "subclass_mode", ":", "return", "self", ".", "add_subclass_arguments", "(", "lightning_class", ",", "nested_key", ",", "fail_untyped", "=", "False", ",", "required", "=", "required", ")", "return", "self", ".", "add_class_arguments", "(", "lightning_class", ",", "nested_key", ",", "fail_untyped", "=", "False", ",", "instantiate", "=", "not", "issubclass", "(", "lightning_class", ",", "Trainer", ")", ",", "sub_configs", "=", "True", ",", ")", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")"], "docstring": "Adds arguments from a lightning class to a nested key of the parser.", "docstring_tokens": ["adds", "arguments", "from", "a", "lightning", "class", "to", "a", "nested", "key", "of", "the", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 115, "end_line": 160, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_338", "original_string": "def add_optimizer_args(\r\n        self,\r\n        optimizer_class: Union[type[Optimizer], tuple[type[Optimizer], ...]] = (Optimizer,),\r\n        nested_key: str = \"optimizer\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from an optimizer class to a nested key of the parser.\r\n\r\n        Args:\r\n            optimizer_class: Any subclass of :class:`torch.optim.Optimizer`. Use tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer_class, tuple):\r\n            assert all(issubclass(o, Optimizer) for o in optimizer_class)\r\n        else:\r\n            assert issubclass(optimizer_class, Optimizer)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"params\"}}\r\n        if isinstance(optimizer_class, tuple):\r\n            self.add_subclass_arguments(optimizer_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(optimizer_class, nested_key, sub_configs=True, **kwargs)\r\n        self._optimizers[nested_key] = (optimizer_class, link_to)", "language": "python", "code": "def add_optimizer_args(\r\n        self,\r\n        optimizer_class: Union[type[Optimizer], tuple[type[Optimizer], ...]] = (Optimizer,),\r\n        nested_key: str = \"optimizer\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from an optimizer class to a nested key of the parser.\r\n\r\n        Args:\r\n            optimizer_class: Any subclass of :class:`torch.optim.Optimizer`. Use tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer_class, tuple):\r\n            assert all(issubclass(o, Optimizer) for o in optimizer_class)\r\n        else:\r\n            assert issubclass(optimizer_class, Optimizer)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"params\"}}\r\n        if isinstance(optimizer_class, tuple):\r\n            self.add_subclass_arguments(optimizer_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(optimizer_class, nested_key, sub_configs=True, **kwargs)\r\n        self._optimizers[nested_key] = (optimizer_class, link_to)", "code_tokens": ["def", "add_optimizer_args", "(", "self", ",", "optimizer_class", ":", "Union", "[", "type", "[", "Optimizer", "]", ",", "tuple", "[", "type", "[", "Optimizer", "]", ",", ".", ".", ".", "]", "]", "=", "(", "Optimizer", ",", ")", ",", "nested_key", ":", "str", "=", "STRING", ",", "link_to", ":", "str", "=", "STRING", ",", ")", "-", ">", "None", ":", "STRING", "if", "isinstance", "(", "optimizer_class", ",", "tuple", ")", ":", "assert", "all", "(", "issubclass", "(", "o", ",", "Optimizer", ")", "for", "o", "in", "optimizer_class", ")", "else", ":", "assert", "issubclass", "(", "optimizer_class", ",", "Optimizer", ")", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "STRING", ":", "False", ",", "STRING", ":", "False", ",", "STRING", ":", "{", "STRING", "}", "}", "if", "isinstance", "(", "optimizer_class", ",", "tuple", ")", ":", "self", ".", "add_subclass_arguments", "(", "optimizer_class", ",", "nested_key", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "add_class_arguments", "(", "optimizer_class", ",", "nested_key", ",", "sub_configs", "=", "True", ",", "*", "*", "kwargs", ")", "self", ".", "_optimizers", "[", "nested_key", "]", "=", "(", "optimizer_class", ",", "link_to", ")"], "docstring": "Adds arguments from an optimizer class to a nested key of the parser.", "docstring_tokens": ["adds", "arguments", "from", "an", "optimizer", "class", "to", "a", "nested", "key", "of", "the", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 162, "end_line": 185, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_339", "original_string": "def add_lr_scheduler_args(\r\n        self,\r\n        lr_scheduler_class: Union[LRSchedulerType, tuple[LRSchedulerType, ...]] = LRSchedulerTypeTuple,\r\n        nested_key: str = \"lr_scheduler\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from a learning rate scheduler class to a nested key of the parser.\r\n\r\n        Args:\r\n            lr_scheduler_class: Any subclass of ``torch.optim.lr_scheduler.{_LRScheduler, ReduceLROnPlateau}``. Use\r\n                tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            assert all(issubclass(o, LRSchedulerTypeTuple) for o in lr_scheduler_class)\r\n        else:\r\n            assert issubclass(lr_scheduler_class, LRSchedulerTypeTuple)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"optimizer\"}}\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            self.add_subclass_arguments(lr_scheduler_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(lr_scheduler_class, nested_key, sub_configs=True, **kwargs)\r\n        self._lr_schedulers[nested_key] = (lr_scheduler_class, link_to)", "language": "python", "code": "def add_lr_scheduler_args(\r\n        self,\r\n        lr_scheduler_class: Union[LRSchedulerType, tuple[LRSchedulerType, ...]] = LRSchedulerTypeTuple,\r\n        nested_key: str = \"lr_scheduler\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from a learning rate scheduler class to a nested key of the parser.\r\n\r\n        Args:\r\n            lr_scheduler_class: Any subclass of ``torch.optim.lr_scheduler.{_LRScheduler, ReduceLROnPlateau}``. Use\r\n                tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            assert all(issubclass(o, LRSchedulerTypeTuple) for o in lr_scheduler_class)\r\n        else:\r\n            assert issubclass(lr_scheduler_class, LRSchedulerTypeTuple)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"optimizer\"}}\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            self.add_subclass_arguments(lr_scheduler_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(lr_scheduler_class, nested_key, sub_configs=True, **kwargs)\r\n        self._lr_schedulers[nested_key] = (lr_scheduler_class, link_to)", "code_tokens": ["def", "add_lr_scheduler_args", "(", "self", ",", "lr_scheduler_class", ":", "Union", "[", "LRSchedulerType", ",", "tuple", "[", "LRSchedulerType", ",", ".", ".", ".", "]", "]", "=", "LRSchedulerTypeTuple", ",", "nested_key", ":", "str", "=", "STRING", ",", "link_to", ":", "str", "=", "STRING", ",", ")", "-", ">", "None", ":", "STRING", "if", "isinstance", "(", "lr_scheduler_class", ",", "tuple", ")", ":", "assert", "all", "(", "issubclass", "(", "o", ",", "LRSchedulerTypeTuple", ")", "for", "o", "in", "lr_scheduler_class", ")", "else", ":", "assert", "issubclass", "(", "lr_scheduler_class", ",", "LRSchedulerTypeTuple", ")", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "STRING", ":", "False", ",", "STRING", ":", "False", ",", "STRING", ":", "{", "STRING", "}", "}", "if", "isinstance", "(", "lr_scheduler_class", ",", "tuple", ")", ":", "self", ".", "add_subclass_arguments", "(", "lr_scheduler_class", ",", "nested_key", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "add_class_arguments", "(", "lr_scheduler_class", ",", "nested_key", ",", "sub_configs", "=", "True", ",", "*", "*", "kwargs", ")", "self", ".", "_lr_schedulers", "[", "nested_key", "]", "=", "(", "lr_scheduler_class", ",", "link_to", ")"], "docstring": "Adds arguments from a learning rate scheduler class to a nested key of the parser.", "docstring_tokens": ["adds", "arguments", "from", "a", "learning", "rate", "scheduler", "class", "to", "a", "nested", "key", "of", "the", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 187, "end_line": 211, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_340", "original_string": "def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\r\n        \"\"\"Implement to save the config in some other place additional to the standard log_dir.\r\n\r\n        Example:\r\n            def save_config(self, trainer, pl_module, stage):\r\n                if isinstance(trainer.logger, Logger):\r\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\r\n                    trainer.logger.log_hyperparams({\"config\": config})\r\n\r\n        Note:\r\n            This method is only called on rank zero. This allows to implement a custom save config without having to\r\n            worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the\r\n            process hang waiting for a broadcast. If you need to make collective calls, implement the setup method\r\n            instead.\r\n\r\n        \"\"\"", "language": "python", "code": "def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\r\n        \"\"\"Implement to save the config in some other place additional to the standard log_dir.\r\n\r\n        Example:\r\n            def save_config(self, trainer, pl_module, stage):\r\n                if isinstance(trainer.logger, Logger):\r\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\r\n                    trainer.logger.log_hyperparams({\"config\": config})\r\n\r\n        Note:\r\n            This method is only called on rank zero. This allows to implement a custom save config without having to\r\n            worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the\r\n            process hang waiting for a broadcast. If you need to make collective calls, implement the setup method\r\n            instead.\r\n\r\n        \"\"\"", "code_tokens": ["def", "save_config", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Implement to save the config in some other place additional to the standard log_dir.", "docstring_tokens": ["implement", "to", "save", "the", "config", "in", "some", "other", "place", "additional", "to", "the", "standard", "log_dir"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 293, "end_line": 308, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_341", "original_string": "def __init__(\r\n        self,\r\n        model_class: Optional[Union[type[LightningModule], Callable[..., LightningModule]]] = None,\r\n        datamodule_class: Optional[Union[type[LightningDataModule], Callable[..., LightningDataModule]]] = None,\r\n        save_config_callback: Optional[type[SaveConfigCallback]] = SaveConfigCallback,\r\n        save_config_kwargs: Optional[dict[str, Any]] = None,\r\n        trainer_class: Union[type[Trainer], Callable[..., Trainer]] = Trainer,\r\n        trainer_defaults: Optional[dict[str, Any]] = None,\r\n        seed_everything_default: Union[bool, int] = True,\r\n        parser_kwargs: Optional[Union[dict[str, Any], dict[str, dict[str, Any]]]] = None,\r\n        parser_class: type[LightningArgumentParser] = LightningArgumentParser,\r\n        subclass_mode_model: bool = False,\r\n        subclass_mode_data: bool = False,\r\n        args: ArgsType = None,\r\n        run: bool = True,\r\n        auto_configure_optimizers: bool = True,\r\n        load_from_checkpoint_support: bool = True,\r\n    ) -> None:\r\n        \"\"\"Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are\r\n        called / instantiated using a parsed configuration file and / or command line args.\r\n\r\n        Parsing of configuration from environment variables can be enabled by setting ``parser_kwargs={\"default_env\":\r\n        True}``. A full configuration yaml would be parsed from ``PL_CONFIG`` if set. Individual settings are so parsed\r\n        from variables named for example ``PL_TRAINER__MAX_EPOCHS``.\r\n\r\n        For more info, read :ref:`the CLI docs <lightning-cli>`.\r\n\r\n        Args:\r\n            model_class: An optional :class:`~lightning.pytorch.core.LightningModule` class to train on or a\r\n                callable which returns a :class:`~lightning.pytorch.core.LightningModule` instance when\r\n                called. If ``None``, you can pass a registered model with ``--model=MyModel``.\r\n            datamodule_class: An optional :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class or a\r\n                callable which returns a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` instance when\r\n                called. If ``None``, you can pass a registered datamodule with ``--data=MyDataModule``.\r\n            save_config_callback: A callback class to save the config.\r\n            save_config_kwargs: Parameters that will be used to instantiate the save_config_callback.\r\n            trainer_class: An optional subclass of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class or a\r\n                callable which returns a :class:`~lightning.pytorch.trainer.trainer.Trainer` instance when called.\r\n            trainer_defaults: Set to override Trainer defaults or add persistent callbacks. The callbacks added through\r\n                this argument will not be configurable from a configuration file and will always be present for\r\n                this particular CLI. Alternatively, configurable callbacks can be added as explained in\r\n                :ref:`the CLI docs <lightning-cli>`.\r\n            seed_everything_default: Number for the :func:`~lightning.fabric.utilities.seed.seed_everything`\r\n                seed value. Set to True to automatically choose a seed value.\r\n                Setting it to False will avoid calling ``seed_everything``.\r\n            parser_kwargs: Additional arguments to instantiate each ``LightningArgumentParser``.\r\n            subclass_mode_model: Whether model can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            subclass_mode_data: Whether datamodule can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            args: Arguments to parse. If ``None`` the arguments are taken from ``sys.argv``. Command line style\r\n                arguments can be given in a ``list``. Alternatively, structured config options can be given in a\r\n                ``dict`` or ``jsonargparse.Namespace``.\r\n            run: Whether subcommands should be added to run a :class:`~lightning.pytorch.trainer.trainer.Trainer`\r\n                method. If set to ``False``, the trainer and model classes will be instantiated only.\r\n            auto_configure_optimizers: Whether to automatically add default optimizer and lr_scheduler arguments.\r\n            load_from_checkpoint_support: Whether ``save_hyperparameters`` should save the original parsed\r\n                hyperparameters (instead of what ``__init__`` receives), such that it is possible for\r\n                ``load_from_checkpoint`` to correctly instantiate classes even when using complex nesting and\r\n                dependency injection.\r\n\r\n        \"\"\"\r\n        self.save_config_callback = save_config_callback\r\n        self.save_config_kwargs = save_config_kwargs or {}\r\n        self.trainer_class = trainer_class\r\n        self.trainer_defaults = trainer_defaults or {}\r\n        self.seed_everything_default = seed_everything_default\r\n        self.parser_kwargs = parser_kwargs or {}\r\n        self.parser_class = parser_class\r\n        self.auto_configure_optimizers = auto_configure_optimizers\r\n\r\n        self.model_class = model_class\r\n        self._model_class = model_class or LightningModule\r\n        self.subclass_mode_model = (model_class is None) or subclass_mode_model\r\n\r\n        self.datamodule_class = datamodule_class\r\n        self._datamodule_class = datamodule_class or LightningDataModule\r\n        self.subclass_mode_data = (datamodule_class is None) or subclass_mode_data\r\n\r\n        main_kwargs, subparser_kwargs = self._setup_parser_kwargs(self.parser_kwargs)\r\n        self.setup_parser(run, main_kwargs, subparser_kwargs)\r\n        self.parse_arguments(self.parser, args)\r\n        self._parse_ckpt_path()\r\n\r\n        self.subcommand = self.config[\"subcommand\"] if run else None\r\n\r\n        self._set_seed()\r\n\r\n        if load_from_checkpoint_support:\r\n            self._add_instantiators()\r\n        self.before_instantiate_classes()\r\n        self.instantiate_classes()\r\n        self.after_instantiate_classes()\r\n\r\n        if self.subcommand is not None:\r\n            self._run_subcommand(self.subcommand)", "language": "python", "code": "def __init__(\r\n        self,\r\n        model_class: Optional[Union[type[LightningModule], Callable[..., LightningModule]]] = None,\r\n        datamodule_class: Optional[Union[type[LightningDataModule], Callable[..., LightningDataModule]]] = None,\r\n        save_config_callback: Optional[type[SaveConfigCallback]] = SaveConfigCallback,\r\n        save_config_kwargs: Optional[dict[str, Any]] = None,\r\n        trainer_class: Union[type[Trainer], Callable[..., Trainer]] = Trainer,\r\n        trainer_defaults: Optional[dict[str, Any]] = None,\r\n        seed_everything_default: Union[bool, int] = True,\r\n        parser_kwargs: Optional[Union[dict[str, Any], dict[str, dict[str, Any]]]] = None,\r\n        parser_class: type[LightningArgumentParser] = LightningArgumentParser,\r\n        subclass_mode_model: bool = False,\r\n        subclass_mode_data: bool = False,\r\n        args: ArgsType = None,\r\n        run: bool = True,\r\n        auto_configure_optimizers: bool = True,\r\n        load_from_checkpoint_support: bool = True,\r\n    ) -> None:\r\n        \"\"\"Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are\r\n        called / instantiated using a parsed configuration file and / or command line args.\r\n\r\n        Parsing of configuration from environment variables can be enabled by setting ``parser_kwargs={\"default_env\":\r\n        True}``. A full configuration yaml would be parsed from ``PL_CONFIG`` if set. Individual settings are so parsed\r\n        from variables named for example ``PL_TRAINER__MAX_EPOCHS``.\r\n\r\n        For more info, read :ref:`the CLI docs <lightning-cli>`.\r\n\r\n        Args:\r\n            model_class: An optional :class:`~lightning.pytorch.core.LightningModule` class to train on or a\r\n                callable which returns a :class:`~lightning.pytorch.core.LightningModule` instance when\r\n                called. If ``None``, you can pass a registered model with ``--model=MyModel``.\r\n            datamodule_class: An optional :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class or a\r\n                callable which returns a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` instance when\r\n                called. If ``None``, you can pass a registered datamodule with ``--data=MyDataModule``.\r\n            save_config_callback: A callback class to save the config.\r\n            save_config_kwargs: Parameters that will be used to instantiate the save_config_callback.\r\n            trainer_class: An optional subclass of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class or a\r\n                callable which returns a :class:`~lightning.pytorch.trainer.trainer.Trainer` instance when called.\r\n            trainer_defaults: Set to override Trainer defaults or add persistent callbacks. The callbacks added through\r\n                this argument will not be configurable from a configuration file and will always be present for\r\n                this particular CLI. Alternatively, configurable callbacks can be added as explained in\r\n                :ref:`the CLI docs <lightning-cli>`.\r\n            seed_everything_default: Number for the :func:`~lightning.fabric.utilities.seed.seed_everything`\r\n                seed value. Set to True to automatically choose a seed value.\r\n                Setting it to False will avoid calling ``seed_everything``.\r\n            parser_kwargs: Additional arguments to instantiate each ``LightningArgumentParser``.\r\n            subclass_mode_model: Whether model can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            subclass_mode_data: Whether datamodule can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            args: Arguments to parse. If ``None`` the arguments are taken from ``sys.argv``. Command line style\r\n                arguments can be given in a ``list``. Alternatively, structured config options can be given in a\r\n                ``dict`` or ``jsonargparse.Namespace``.\r\n            run: Whether subcommands should be added to run a :class:`~lightning.pytorch.trainer.trainer.Trainer`\r\n                method. If set to ``False``, the trainer and model classes will be instantiated only.\r\n            auto_configure_optimizers: Whether to automatically add default optimizer and lr_scheduler arguments.\r\n            load_from_checkpoint_support: Whether ``save_hyperparameters`` should save the original parsed\r\n                hyperparameters (instead of what ``__init__`` receives), such that it is possible for\r\n                ``load_from_checkpoint`` to correctly instantiate classes even when using complex nesting and\r\n                dependency injection.\r\n\r\n        \"\"\"\r\n        self.save_config_callback = save_config_callback\r\n        self.save_config_kwargs = save_config_kwargs or {}\r\n        self.trainer_class = trainer_class\r\n        self.trainer_defaults = trainer_defaults or {}\r\n        self.seed_everything_default = seed_everything_default\r\n        self.parser_kwargs = parser_kwargs or {}\r\n        self.parser_class = parser_class\r\n        self.auto_configure_optimizers = auto_configure_optimizers\r\n\r\n        self.model_class = model_class\r\n        self._model_class = model_class or LightningModule\r\n        self.subclass_mode_model = (model_class is None) or subclass_mode_model\r\n\r\n        self.datamodule_class = datamodule_class\r\n        self._datamodule_class = datamodule_class or LightningDataModule\r\n        self.subclass_mode_data = (datamodule_class is None) or subclass_mode_data\r\n\r\n        main_kwargs, subparser_kwargs = self._setup_parser_kwargs(self.parser_kwargs)\r\n        self.setup_parser(run, main_kwargs, subparser_kwargs)\r\n        self.parse_arguments(self.parser, args)\r\n        self._parse_ckpt_path()\r\n\r\n        self.subcommand = self.config[\"subcommand\"] if run else None\r\n\r\n        self._set_seed()\r\n\r\n        if load_from_checkpoint_support:\r\n            self._add_instantiators()\r\n        self.before_instantiate_classes()\r\n        self.instantiate_classes()\r\n        self.after_instantiate_classes()\r\n\r\n        if self.subcommand is not None:\r\n            self._run_subcommand(self.subcommand)", "code_tokens": ["def", "__init__", "(", "self", ",", "model_class", ":", "Optional", "[", "Union", "[", "type", "[", "LightningModule", "]", ",", "Callable", "[", ".", ".", ".", ",", "LightningModule", "]", "]", "]", "=", "None", ",", "datamodule_class", ":", "Optional", "[", "Union", "[", "type", "[", "LightningDataModule", "]", ",", "Callable", "[", ".", ".", ".", ",", "LightningDataModule", "]", "]", "]", "=", "None", ",", "save_config_callback", ":", "Optional", "[", "type", "[", "SaveConfigCallback", "]", "]", "=", "SaveConfigCallback", ",", "save_config_kwargs", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "trainer_class", ":", "Union", "[", "type", "[", "Trainer", "]", ",", "Callable", "[", ".", ".", ".", ",", "Trainer", "]", "]", "=", "Trainer", ",", "trainer_defaults", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "seed_everything_default", ":", "Union", "[", "bool", ",", "int", "]", "=", "True", ",", "parser_kwargs", ":", "Optional", "[", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "dict", "[", "str", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "parser_class", ":", "type", "[", "LightningArgumentParser", "]", "=", "LightningArgumentParser", ",", "subclass_mode_model", ":", "bool", "=", "False", ",", "subclass_mode_data", ":", "bool", "=", "False", ",", "args", ":", "ArgsType", "=", "None", ",", "run", ":", "bool", "=", "True", ",", "auto_configure_optimizers", ":", "bool", "=", "True", ",", "load_from_checkpoint_support", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "save_config_callback", "=", "save_config_callback", "self", ".", "save_config_kwargs", "=", "save_config_kwargs", "or", "{", "}", "self", ".", "trainer_class", "=", "trainer_class", "self", ".", "trainer_defaults", "=", "trainer_defaults", "or", "{", "}", "self", ".", "seed_everything_default", "=", "seed_everything_default", "self", ".", "parser_kwargs", "=", "parser_kwargs", "or", "{", "}", "self", ".", "parser_class", "=", "parser_class", "self", ".", "auto_configure_optimizers", "=", "auto_configure_optimizers", "self", ".", "model_class", "=", "model_class", "self", ".", "_model_class", "=", "model_class", "or", "LightningModule", "self", ".", "subclass_mode_model", "=", "(", "model_class", "is", "None", ")", "or", "subclass_mode_model", "self", ".", "datamodule_class", "=", "datamodule_class", "self", ".", "_datamodule_class", "=", "datamodule_class", "or", "LightningDataModule", "self", ".", "subclass_mode_data", "=", "(", "datamodule_class", "is", "None", ")", "or", "subclass_mode_data", "main_kwargs", ",", "subparser_kwargs", "=", "self", ".", "_setup_parser_kwargs", "(", "self", ".", "parser_kwargs", ")", "self", ".", "setup_parser", "(", "run", ",", "main_kwargs", ",", "subparser_kwargs", ")", "self", ".", "parse_arguments", "(", "self", ".", "parser", ",", "args", ")", "self", ".", "_parse_ckpt_path", "(", ")", "self", ".", "subcommand", "=", "self", ".", "config", "[", "STRING", "]", "if", "run", "else", "None", "self", ".", "_set_seed", "(", ")", "if", "load_from_checkpoint_support", ":", "self", ".", "_add_instantiators", "(", ")", "self", ".", "before_instantiate_classes", "(", ")", "self", ".", "instantiate_classes", "(", ")", "self", ".", "after_instantiate_classes", "(", ")", "if", "self", ".", "subcommand", "is", "not", "None", ":", "self", ".", "_run_subcommand", "(", "self", ".", "subcommand", ")"], "docstring": "Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are", "docstring_tokens": ["receives", "as", "input", "pytorch", "lightning", "classes", "or", "callables", "which", "return", "pytorch", "lightning", "classes", "which", "are"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 314, "end_line": 413, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_342", "original_string": "def init_parser(self, **kwargs: Any) -> LightningArgumentParser:\r\n        \"\"\"Method that instantiates the argument parser.\"\"\"\r\n        kwargs.setdefault(\"dump_header\", [f\"lightning.pytorch=={pl.__version__}\"])\r\n        parser = self.parser_class(**kwargs)\r\n        parser.add_argument(\r\n            \"-c\", \"--config\", action=ActionConfigFile, help=\"Path to a configuration file in json or yaml format.\"\r\n        )\r\n        return parser", "language": "python", "code": "def init_parser(self, **kwargs: Any) -> LightningArgumentParser:\r\n        \"\"\"Method that instantiates the argument parser.\"\"\"\r\n        kwargs.setdefault(\"dump_header\", [f\"lightning.pytorch=={pl.__version__}\"])\r\n        parser = self.parser_class(**kwargs)\r\n        parser.add_argument(\r\n            \"-c\", \"--config\", action=ActionConfigFile, help=\"Path to a configuration file in json or yaml format.\"\r\n        )\r\n        return parser", "code_tokens": ["def", "init_parser", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "LightningArgumentParser", ":", "STRING", "kwargs", ".", "setdefault", "(", "STRING", ",", "[", "fSTRING", "]", ")", "parser", "=", "self", ".", "parser_class", "(", "*", "*", "kwargs", ")", "parser", ".", "add_argument", "(", "STRING", ",", "STRING", ",", "action", "=", "ActionConfigFile", ",", "help", "=", "STRING", ")", "return", "parser"], "docstring": "Method that instantiates the argument parser.", "docstring_tokens": ["method", "that", "instantiates", "the", "argument", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 421, "end_line": 428, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_343", "original_string": "def setup_parser(\r\n        self, add_subcommands: bool, main_kwargs: dict[str, Any], subparser_kwargs: dict[str, Any]\r\n    ) -> None:\r\n        \"\"\"Initialize and setup the parser, subcommands, and arguments.\"\"\"\r\n        self.parser = self.init_parser(**main_kwargs)\r\n        if add_subcommands:\r\n            self._subcommand_method_arguments: dict[str, list[str]] = {}\r\n            self._add_subcommands(self.parser, **subparser_kwargs)\r\n        else:\r\n            self._add_arguments(self.parser)", "language": "python", "code": "def setup_parser(\r\n        self, add_subcommands: bool, main_kwargs: dict[str, Any], subparser_kwargs: dict[str, Any]\r\n    ) -> None:\r\n        \"\"\"Initialize and setup the parser, subcommands, and arguments.\"\"\"\r\n        self.parser = self.init_parser(**main_kwargs)\r\n        if add_subcommands:\r\n            self._subcommand_method_arguments: dict[str, list[str]] = {}\r\n            self._add_subcommands(self.parser, **subparser_kwargs)\r\n        else:\r\n            self._add_arguments(self.parser)", "code_tokens": ["def", "setup_parser", "(", "self", ",", "add_subcommands", ":", "bool", ",", "main_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "subparser_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "parser", "=", "self", ".", "init_parser", "(", "*", "*", "main_kwargs", ")", "if", "add_subcommands", ":", "self", ".", "_subcommand_method_arguments", ":", "dict", "[", "str", ",", "list", "[", "str", "]", "]", "=", "{", "}", "self", ".", "_add_subcommands", "(", "self", ".", "parser", ",", "*", "*", "subparser_kwargs", ")", "else", ":", "self", ".", "_add_arguments", "(", "self", ".", "parser", ")"], "docstring": "Initialize and setup the parser, subcommands, and arguments.", "docstring_tokens": ["initialize", "and", "setup", "the", "parser", "subcommands", "and", "arguments"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 430, "end_line": 439, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_344", "original_string": "def add_default_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds default arguments to the parser.\"\"\"\r\n        parser.add_argument(\r\n            \"--seed_everything\",\r\n            type=Union[bool, int],\r\n            default=self.seed_everything_default,\r\n            help=(\r\n                \"Set to an int to run seed_everything with this value before classes instantiation.\"\r\n                \"Set to True to use a random seed.\"\r\n            ),\r\n        )", "language": "python", "code": "def add_default_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds default arguments to the parser.\"\"\"\r\n        parser.add_argument(\r\n            \"--seed_everything\",\r\n            type=Union[bool, int],\r\n            default=self.seed_everything_default,\r\n            help=(\r\n                \"Set to an int to run seed_everything with this value before classes instantiation.\"\r\n                \"Set to True to use a random seed.\"\r\n            ),\r\n        )", "code_tokens": ["def", "add_default_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "STRING", "parser", ".", "add_argument", "(", "STRING", ",", "type", "=", "Union", "[", "bool", ",", "int", "]", ",", "default", "=", "self", ".", "seed_everything_default", ",", "help", "=", "(", "STRING", "STRING", ")", ",", ")"], "docstring": "Adds default arguments to the parser.", "docstring_tokens": ["adds", "default", "arguments", "to", "the", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 441, "end_line": 451, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_345", "original_string": "def add_core_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds arguments from the core classes to the parser.\"\"\"\r\n        parser.add_lightning_class_args(self.trainer_class, \"trainer\")\r\n        trainer_defaults = {\"trainer.\" + k: v for k, v in self.trainer_defaults.items() if k != \"callbacks\"}\r\n        parser.set_defaults(trainer_defaults)\r\n\r\n        parser.add_lightning_class_args(self._model_class, \"model\", subclass_mode=self.subclass_mode_model)\r\n\r\n        if self.datamodule_class is not None:\r\n            parser.add_lightning_class_args(self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data)\r\n        else:\r\n            parser.add_lightning_class_args(\r\n                self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data, required=False\r\n            )", "language": "python", "code": "def add_core_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds arguments from the core classes to the parser.\"\"\"\r\n        parser.add_lightning_class_args(self.trainer_class, \"trainer\")\r\n        trainer_defaults = {\"trainer.\" + k: v for k, v in self.trainer_defaults.items() if k != \"callbacks\"}\r\n        parser.set_defaults(trainer_defaults)\r\n\r\n        parser.add_lightning_class_args(self._model_class, \"model\", subclass_mode=self.subclass_mode_model)\r\n\r\n        if self.datamodule_class is not None:\r\n            parser.add_lightning_class_args(self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data)\r\n        else:\r\n            parser.add_lightning_class_args(\r\n                self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data, required=False\r\n            )", "code_tokens": ["def", "add_core_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "STRING", "parser", ".", "add_lightning_class_args", "(", "self", ".", "trainer_class", ",", "STRING", ")", "trainer_defaults", "=", "{", "STRING", "+", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "trainer_defaults", ".", "items", "(", ")", "if", "k", "!", "=", "STRING", "}", "parser", ".", "set_defaults", "(", "trainer_defaults", ")", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_model_class", ",", "STRING", ",", "subclass_mode", "=", "self", ".", "subclass_mode_model", ")", "if", "self", ".", "datamodule_class", "is", "not", "None", ":", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_datamodule_class", ",", "STRING", ",", "subclass_mode", "=", "self", ".", "subclass_mode_data", ")", "else", ":", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_datamodule_class", ",", "STRING", ",", "subclass_mode", "=", "self", ".", "subclass_mode_data", ",", "required", "=", "False", ")"], "docstring": "Adds arguments from the core classes to the parser.", "docstring_tokens": ["adds", "arguments", "from", "the", "core", "classes", "to", "the", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 453, "end_line": 467, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_346", "original_string": "def add_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Implement to add extra arguments to the parser or link arguments.\r\n\r\n        Args:\r\n            parser: The parser object to which arguments can be added\r\n\r\n        \"\"\"", "language": "python", "code": "def add_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Implement to add extra arguments to the parser or link arguments.\r\n\r\n        Args:\r\n            parser: The parser object to which arguments can be added\r\n\r\n        \"\"\"", "code_tokens": ["def", "add_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "STRING"], "docstring": "Implement to add extra arguments to the parser or link arguments.", "docstring_tokens": ["implement", "to", "add", "extra", "arguments", "to", "the", "parser", "or", "link", "arguments"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 482, "end_line": 488, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_347", "original_string": "def subcommands() -> dict[str, set[str]]:\r\n        \"\"\"Defines the list of available subcommands and the arguments to skip.\"\"\"\r\n        return {\r\n            \"fit\": {\"model\", \"train_dataloaders\", \"val_dataloaders\", \"datamodule\"},\r\n            \"validate\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"test\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"predict\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n        }", "language": "python", "code": "def subcommands() -> dict[str, set[str]]:\r\n        \"\"\"Defines the list of available subcommands and the arguments to skip.\"\"\"\r\n        return {\r\n            \"fit\": {\"model\", \"train_dataloaders\", \"val_dataloaders\", \"datamodule\"},\r\n            \"validate\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"test\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"predict\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n        }", "code_tokens": ["def", "subcommands", "(", ")", "-", ">", "dict", "[", "str", ",", "set", "[", "str", "]", "]", ":", "STRING", "return", "{", "STRING", ":", "{", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", "}", ",", "STRING", ":", "{", "STRING", ",", "STRING", ",", "STRING", "}", ",", "STRING", ":", "{", "STRING", ",", "STRING", ",", "STRING", "}", ",", "STRING", ":", "{", "STRING", ",", "STRING", ",", "STRING", "}", ",", "}"], "docstring": "Defines the list of available subcommands and the arguments to skip.", "docstring_tokens": ["defines", "the", "list", "of", "available", "subcommands", "and", "the", "arguments", "to", "skip"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 491, "end_line": 498, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_348", "original_string": "def _add_subcommands(self, parser: LightningArgumentParser, **kwargs: Any) -> None:\r\n        \"\"\"Adds subcommands to the input parser.\"\"\"\r\n        self._subcommand_parsers: dict[str, LightningArgumentParser] = {}\r\n        parser_subcommands = parser.add_subcommands()\r\n        trainer_class = (\r\n            self.trainer_class if isinstance(self.trainer_class, type) else class_from_function(self.trainer_class)\r\n        )\r\n        for subcommand in self.subcommands():\r\n            fn = getattr(trainer_class, subcommand)\r\n            description = _get_short_description(fn)\r\n            subparser_kwargs = kwargs.get(subcommand, {})\r\n            subparser_kwargs.setdefault(\"description\", description)\r\n            subcommand_parser = self._prepare_subcommand_parser(trainer_class, subcommand, **subparser_kwargs)\r\n            self._subcommand_parsers[subcommand] = subcommand_parser\r\n            parser_subcommands.add_subcommand(subcommand, subcommand_parser, help=description)", "language": "python", "code": "def _add_subcommands(self, parser: LightningArgumentParser, **kwargs: Any) -> None:\r\n        \"\"\"Adds subcommands to the input parser.\"\"\"\r\n        self._subcommand_parsers: dict[str, LightningArgumentParser] = {}\r\n        parser_subcommands = parser.add_subcommands()\r\n        trainer_class = (\r\n            self.trainer_class if isinstance(self.trainer_class, type) else class_from_function(self.trainer_class)\r\n        )\r\n        for subcommand in self.subcommands():\r\n            fn = getattr(trainer_class, subcommand)\r\n            description = _get_short_description(fn)\r\n            subparser_kwargs = kwargs.get(subcommand, {})\r\n            subparser_kwargs.setdefault(\"description\", description)\r\n            subcommand_parser = self._prepare_subcommand_parser(trainer_class, subcommand, **subparser_kwargs)\r\n            self._subcommand_parsers[subcommand] = subcommand_parser\r\n            parser_subcommands.add_subcommand(subcommand, subcommand_parser, help=description)", "code_tokens": ["def", "_add_subcommands", "(", "self", ",", "parser", ":", "LightningArgumentParser", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "self", ".", "_subcommand_parsers", ":", "dict", "[", "str", ",", "LightningArgumentParser", "]", "=", "{", "}", "parser_subcommands", "=", "parser", ".", "add_subcommands", "(", ")", "trainer_class", "=", "(", "self", ".", "trainer_class", "if", "isinstance", "(", "self", ".", "trainer_class", ",", "type", ")", "else", "class_from_function", "(", "self", ".", "trainer_class", ")", ")", "for", "subcommand", "in", "self", ".", "subcommands", "(", ")", ":", "fn", "=", "getattr", "(", "trainer_class", ",", "subcommand", ")", "description", "=", "_get_short_description", "(", "fn", ")", "subparser_kwargs", "=", "kwargs", ".", "get", "(", "subcommand", ",", "{", "}", ")", "subparser_kwargs", ".", "setdefault", "(", "STRING", ",", "description", ")", "subcommand_parser", "=", "self", ".", "_prepare_subcommand_parser", "(", "trainer_class", ",", "subcommand", ",", "*", "*", "subparser_kwargs", ")", "self", ".", "_subcommand_parsers", "[", "subcommand", "]", "=", "subcommand_parser", "parser_subcommands", ".", "add_subcommand", "(", "subcommand", ",", "subcommand_parser", ",", "help", "=", "description", ")"], "docstring": "Adds subcommands to the input parser.", "docstring_tokens": ["adds", "subcommands", "to", "the", "input", "parser"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 500, "end_line": 517, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_349", "original_string": "def link_optimizers_and_lr_schedulers(parser: LightningArgumentParser) -> None:\r\n        \"\"\"Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.\"\"\"\r\n        optimizers_and_lr_schedulers = {**parser._optimizers, **parser._lr_schedulers}\r\n        for key, (class_type, link_to) in optimizers_and_lr_schedulers.items():\r\n            if link_to == \"AUTOMATIC\":\r\n                continue\r\n            if isinstance(class_type, tuple):\r\n                parser.link_arguments(key, link_to)\r\n            else:\r\n                add_class_path = _add_class_path_generator(class_type)\r\n                parser.link_arguments(key, link_to, compute_fn=add_class_path)", "language": "python", "code": "def link_optimizers_and_lr_schedulers(parser: LightningArgumentParser) -> None:\r\n        \"\"\"Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.\"\"\"\r\n        optimizers_and_lr_schedulers = {**parser._optimizers, **parser._lr_schedulers}\r\n        for key, (class_type, link_to) in optimizers_and_lr_schedulers.items():\r\n            if link_to == \"AUTOMATIC\":\r\n                continue\r\n            if isinstance(class_type, tuple):\r\n                parser.link_arguments(key, link_to)\r\n            else:\r\n                add_class_path = _add_class_path_generator(class_type)\r\n                parser.link_arguments(key, link_to, compute_fn=add_class_path)", "code_tokens": ["def", "link_optimizers_and_lr_schedulers", "(", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "STRING", "optimizers_and_lr_schedulers", "=", "{", "*", "*", "parser", ".", "_optimizers", ",", "*", "*", "parser", ".", "_lr_schedulers", "}", "for", "key", ",", "(", "class_type", ",", "link_to", ")", "in", "optimizers_and_lr_schedulers", ".", "items", "(", ")", ":", "if", "link_to", "=", "=", "STRING", ":", "continue", "if", "isinstance", "(", "class_type", ",", "tuple", ")", ":", "parser", ".", "link_arguments", "(", "key", ",", "link_to", ")", "else", ":", "add_class_path", "=", "_add_class_path_generator", "(", "class_type", ")", "parser", ".", "link_arguments", "(", "key", ",", "link_to", ",", "compute_fn", "=", "add_class_path", ")"], "docstring": "Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.", "docstring_tokens": ["creates", "argument", "links", "for", "optimizers", "and", "learning", "rate", "schedulers", "that", "specified", "a", "link_to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 530, "end_line": 540, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_350", "original_string": "def parse_arguments(self, parser: LightningArgumentParser, args: ArgsType) -> None:\r\n        \"\"\"Parses command line arguments and stores it in ``self.config``.\"\"\"\r\n        if args is not None and len(sys.argv) > 1:\r\n            rank_zero_warn(\r\n                \"LightningCLI's args parameter is intended to run from within Python like if it were from the command \"\r\n                \"line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: \"\r\n                f\"sys.argv[1:]={sys.argv[1:]}, args={args}.\"\r\n            )\r\n        if isinstance(args, (dict, Namespace)):\r\n            self.config = parser.parse_object(args)\r\n        else:\r\n            self.config = parser.parse_args(args)", "language": "python", "code": "def parse_arguments(self, parser: LightningArgumentParser, args: ArgsType) -> None:\r\n        \"\"\"Parses command line arguments and stores it in ``self.config``.\"\"\"\r\n        if args is not None and len(sys.argv) > 1:\r\n            rank_zero_warn(\r\n                \"LightningCLI's args parameter is intended to run from within Python like if it were from the command \"\r\n                \"line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: \"\r\n                f\"sys.argv[1:]={sys.argv[1:]}, args={args}.\"\r\n            )\r\n        if isinstance(args, (dict, Namespace)):\r\n            self.config = parser.parse_object(args)\r\n        else:\r\n            self.config = parser.parse_args(args)", "code_tokens": ["def", "parse_arguments", "(", "self", ",", "parser", ":", "LightningArgumentParser", ",", "args", ":", "ArgsType", ")", "-", ">", "None", ":", "STRING", "if", "args", "is", "not", "None", "and", "len", "(", "sys", ".", "argv", ")", ">", "1", ":", "rank_zero_warn", "(", "STRING", "STRING", "fSTRING", ")", "if", "isinstance", "(", "args", ",", "(", "dict", ",", "Namespace", ")", ")", ":", "self", ".", "config", "=", "parser", ".", "parse_object", "(", "args", ")", "else", ":", "self", ".", "config", "=", "parser", ".", "parse_args", "(", "args", ")"], "docstring": "Parses command line arguments and stores it in ``self.config``.", "docstring_tokens": ["parses", "command", "line", "arguments", "and", "stores", "it", "in", "self", "config"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 542, "end_line": 553, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_351", "original_string": "def _parse_ckpt_path(self) -> None:\r\n        \"\"\"If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.\"\"\"\r\n        if not self.config.get(\"subcommand\"):\r\n            return\r\n        ckpt_path = self.config[self.config.subcommand].get(\"ckpt_path\")\r\n        if ckpt_path and Path(ckpt_path).is_file():\r\n            ckpt = torch.load(ckpt_path, weights_only=True, map_location=\"cpu\")\r\n            hparams = ckpt.get(\"hyper_parameters\", {})\r\n            hparams.pop(\"_instantiator\", None)\r\n            if not hparams:\r\n                return\r\n            if \"_class_path\" in hparams:\r\n                hparams = {\r\n                    \"class_path\": hparams.pop(\"_class_path\"),\r\n                    \"dict_kwargs\": hparams,\r\n                }\r\n            hparams = {self.config.subcommand: {\"model\": hparams}}\r\n            try:\r\n                self.config = self.parser.parse_object(hparams, self.config)\r\n            except SystemExit:\r\n                sys.stderr.write(\"Parsing of ckpt_path hyperparameters failed!\\n\")\r\n                raise", "language": "python", "code": "def _parse_ckpt_path(self) -> None:\r\n        \"\"\"If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.\"\"\"\r\n        if not self.config.get(\"subcommand\"):\r\n            return\r\n        ckpt_path = self.config[self.config.subcommand].get(\"ckpt_path\")\r\n        if ckpt_path and Path(ckpt_path).is_file():\r\n            ckpt = torch.load(ckpt_path, weights_only=True, map_location=\"cpu\")\r\n            hparams = ckpt.get(\"hyper_parameters\", {})\r\n            hparams.pop(\"_instantiator\", None)\r\n            if not hparams:\r\n                return\r\n            if \"_class_path\" in hparams:\r\n                hparams = {\r\n                    \"class_path\": hparams.pop(\"_class_path\"),\r\n                    \"dict_kwargs\": hparams,\r\n                }\r\n            hparams = {self.config.subcommand: {\"model\": hparams}}\r\n            try:\r\n                self.config = self.parser.parse_object(hparams, self.config)\r\n            except SystemExit:\r\n                sys.stderr.write(\"Parsing of ckpt_path hyperparameters failed!\\n\")\r\n                raise", "code_tokens": ["def", "_parse_ckpt_path", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "config", ".", "get", "(", "STRING", ")", ":", "return", "ckpt_path", "=", "self", ".", "config", "[", "self", ".", "config", ".", "subcommand", "]", ".", "get", "(", "STRING", ")", "if", "ckpt_path", "and", "Path", "(", "ckpt_path", ")", ".", "is_file", "(", ")", ":", "ckpt", "=", "torch", ".", "load", "(", "ckpt_path", ",", "weights_only", "=", "True", ",", "map_location", "=", "STRING", ")", "hparams", "=", "ckpt", ".", "get", "(", "STRING", ",", "{", "}", ")", "hparams", ".", "pop", "(", "STRING", ",", "None", ")", "if", "not", "hparams", ":", "return", "if", "STRING", "in", "hparams", ":", "hparams", "=", "{", "STRING", ":", "hparams", ".", "pop", "(", "STRING", ")", ",", "STRING", ":", "hparams", ",", "}", "hparams", "=", "{", "self", ".", "config", ".", "subcommand", ":", "{", "STRING", ":", "hparams", "}", "}", "try", ":", "self", ".", "config", "=", "self", ".", "parser", ".", "parse_object", "(", "hparams", ",", "self", ".", "config", ")", "except", "SystemExit", ":", "sys", ".", "stderr", ".", "write", "(", "STRING", ")", "raise"], "docstring": "If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.", "docstring_tokens": ["if", "a", "checkpoint", "path", "is", "given", "parse", "the", "hyperparameters", "from", "the", "checkpoint", "and", "update", "the", "config"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 555, "end_line": 576, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_352", "original_string": "def before_instantiate_classes(self) -> None:\r\n        \"\"\"Implement to run some code before instantiating the classes.\"\"\"", "language": "python", "code": "def before_instantiate_classes(self) -> None:\r\n        \"\"\"Implement to run some code before instantiating the classes.\"\"\"", "code_tokens": ["def", "before_instantiate_classes", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Implement to run some code before instantiating the classes.", "docstring_tokens": ["implement", "to", "run", "some", "code", "before", "instantiating", "the", "classes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 599, "end_line": 600, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_353", "original_string": "def instantiate_classes(self) -> None:\r\n        \"\"\"Instantiates the classes and sets their attributes.\"\"\"\r\n        self.config_init = self.parser.instantiate_classes(self.config)\r\n        self.datamodule = self._get(self.config_init, \"data\")\r\n        self.model = self._get(self.config_init, \"model\")\r\n        self._add_configure_optimizers_method_to_model(self.subcommand)\r\n        self.trainer = self.instantiate_trainer()", "language": "python", "code": "def instantiate_classes(self) -> None:\r\n        \"\"\"Instantiates the classes and sets their attributes.\"\"\"\r\n        self.config_init = self.parser.instantiate_classes(self.config)\r\n        self.datamodule = self._get(self.config_init, \"data\")\r\n        self.model = self._get(self.config_init, \"model\")\r\n        self._add_configure_optimizers_method_to_model(self.subcommand)\r\n        self.trainer = self.instantiate_trainer()", "code_tokens": ["def", "instantiate_classes", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "config_init", "=", "self", ".", "parser", ".", "instantiate_classes", "(", "self", ".", "config", ")", "self", ".", "datamodule", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "STRING", ")", "self", ".", "model", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "STRING", ")", "self", ".", "_add_configure_optimizers_method_to_model", "(", "self", ".", "subcommand", ")", "self", ".", "trainer", "=", "self", ".", "instantiate_trainer", "(", ")"], "docstring": "Instantiates the classes and sets their attributes.", "docstring_tokens": ["instantiates", "the", "classes", "and", "sets", "their", "attributes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 602, "end_line": 608, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_354", "original_string": "def after_instantiate_classes(self) -> None:\r\n        \"\"\"Implement to run some code after instantiating the classes.\"\"\"", "language": "python", "code": "def after_instantiate_classes(self) -> None:\r\n        \"\"\"Implement to run some code after instantiating the classes.\"\"\"", "code_tokens": ["def", "after_instantiate_classes", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Implement to run some code after instantiating the classes.", "docstring_tokens": ["implement", "to", "run", "some", "code", "after", "instantiating", "the", "classes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 610, "end_line": 611, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_355", "original_string": "def instantiate_trainer(self, **kwargs: Any) -> Trainer:\r\n        \"\"\"Instantiates the trainer.\r\n\r\n        Args:\r\n            kwargs: Any custom trainer arguments.\r\n\r\n        \"\"\"\r\n        extra_callbacks = [self._get(self.config_init, c) for c in self._parser(self.subcommand).callback_keys]\r\n        trainer_config = {**self._get(self.config_init, \"trainer\", default={}), **kwargs}\r\n        return self._instantiate_trainer(trainer_config, extra_callbacks)", "language": "python", "code": "def instantiate_trainer(self, **kwargs: Any) -> Trainer:\r\n        \"\"\"Instantiates the trainer.\r\n\r\n        Args:\r\n            kwargs: Any custom trainer arguments.\r\n\r\n        \"\"\"\r\n        extra_callbacks = [self._get(self.config_init, c) for c in self._parser(self.subcommand).callback_keys]\r\n        trainer_config = {**self._get(self.config_init, \"trainer\", default={}), **kwargs}\r\n        return self._instantiate_trainer(trainer_config, extra_callbacks)", "code_tokens": ["def", "instantiate_trainer", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Trainer", ":", "STRING", "extra_callbacks", "=", "[", "self", ".", "_get", "(", "self", ".", "config_init", ",", "c", ")", "for", "c", "in", "self", ".", "_parser", "(", "self", ".", "subcommand", ")", ".", "callback_keys", "]", "trainer_config", "=", "{", "*", "*", "self", ".", "_get", "(", "self", ".", "config_init", ",", "STRING", ",", "default", "=", "{", "}", ")", ",", "*", "*", "kwargs", "}", "return", "self", ".", "_instantiate_trainer", "(", "trainer_config", ",", "extra_callbacks", ")"], "docstring": "Instantiates the trainer.", "docstring_tokens": ["instantiates", "the", "trainer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 613, "end_line": 622, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_356", "original_string": "def configure_optimizers(\r\n        lightning_module: LightningModule, optimizer: Optimizer, lr_scheduler: Optional[LRSchedulerTypeUnion] = None\r\n    ) -> Any:\r\n        \"\"\"Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.\r\n\r\n        Args:\r\n            lightning_module: A reference to the model.\r\n            optimizer: The optimizer.\r\n            lr_scheduler: The learning rate scheduler (if used).\r\n\r\n        \"\"\"\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        if isinstance(lr_scheduler, ReduceLROnPlateau):\r\n            return {\r\n                \"optimizer\": optimizer,\r\n                \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"monitor\": lr_scheduler.monitor},\r\n            }\r\n        return [optimizer], [lr_scheduler]", "language": "python", "code": "def configure_optimizers(\r\n        lightning_module: LightningModule, optimizer: Optimizer, lr_scheduler: Optional[LRSchedulerTypeUnion] = None\r\n    ) -> Any:\r\n        \"\"\"Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.\r\n\r\n        Args:\r\n            lightning_module: A reference to the model.\r\n            optimizer: The optimizer.\r\n            lr_scheduler: The learning rate scheduler (if used).\r\n\r\n        \"\"\"\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        if isinstance(lr_scheduler, ReduceLROnPlateau):\r\n            return {\r\n                \"optimizer\": optimizer,\r\n                \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"monitor\": lr_scheduler.monitor},\r\n            }\r\n        return [optimizer], [lr_scheduler]", "code_tokens": ["def", "configure_optimizers", "(", "lightning_module", ":", "LightningModule", ",", "optimizer", ":", "Optimizer", ",", "lr_scheduler", ":", "Optional", "[", "LRSchedulerTypeUnion", "]", "=", "None", ")", "-", ">", "Any", ":", "STRING", "if", "lr_scheduler", "is", "None", ":", "return", "optimizer", "if", "isinstance", "(", "lr_scheduler", ",", "ReduceLROnPlateau", ")", ":", "return", "{", "STRING", ":", "optimizer", ",", "STRING", ":", "{", "STRING", ":", "lr_scheduler", ",", "STRING", ":", "lr_scheduler", ".", "monitor", "}", ",", "}", "return", "[", "optimizer", "]", ",", "[", "lr_scheduler", "]"], "docstring": "Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.", "docstring_tokens": ["override", "to", "customize", "the", "meth", "lightning", "pytorch", "core", "lightningmodule", "configure_optimizers", "method"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 656, "end_line": 674, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_357", "original_string": "def _add_configure_optimizers_method_to_model(self, subcommand: Optional[str]) -> None:\r\n        \"\"\"Overrides the model's :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method if a\r\n        single optimizer and optionally a scheduler argument groups are added to the parser as 'AUTOMATIC'.\"\"\"\r\n        if not self.auto_configure_optimizers:\r\n            return\r\n\r\n        parser = self._parser(subcommand)\r\n\r\n        def get_automatic(\r\n            class_type: Union[type, tuple[type, ...]], register: dict[str, tuple[Union[type, tuple[type, ...]], str]]\r\n        ) -> list[str]:\r\n            automatic = []\r\n            for key, (base_class, link_to) in register.items():\r\n                if not isinstance(base_class, tuple):\r\n                    base_class = (base_class,)\r\n                if link_to == \"AUTOMATIC\" and any(issubclass(c, class_type) for c in base_class):\r\n                    automatic.append(key)\r\n            return automatic\r\n\r\n        optimizers = get_automatic(Optimizer, parser._optimizers)\r\n        lr_schedulers = get_automatic(LRSchedulerTypeTuple, parser._lr_schedulers)\r\n\r\n        if len(optimizers) == 0:\r\n            return\r\n\r\n        if len(optimizers) > 1 or len(lr_schedulers) > 1:\r\n            raise MisconfigurationException(\r\n                f\"`{self.__class__.__name__}.add_configure_optimizers_method_to_model` expects at most one optimizer \"\r\n                f\"and one lr_scheduler to be 'AUTOMATIC', but found {optimizers + lr_schedulers}. In this case the \"\r\n                \"user is expected to link the argument groups and implement `configure_optimizers`, see \"\r\n                \"https://lightning.ai/docs/pytorch/stable/common/lightning_cli.html\"\r\n                \"#optimizers-and-learning-rate-schedulers\"\r\n            )\r\n\r\n        optimizer_class = parser._optimizers[optimizers[0]][0]\r\n        optimizer_init = self._get(self.config_init, optimizers[0])\r\n        if not isinstance(optimizer_class, tuple):\r\n            optimizer_init = _global_add_class_path(optimizer_class, optimizer_init)\r\n        if not optimizer_init:\r\n            return\r\n\r\n        lr_scheduler_init = None\r\n        if lr_schedulers:\r\n            lr_scheduler_class = parser._lr_schedulers[lr_schedulers[0]][0]\r\n            lr_scheduler_init = self._get(self.config_init, lr_schedulers[0])\r\n            if not isinstance(lr_scheduler_class, tuple):\r\n                lr_scheduler_init = _global_add_class_path(lr_scheduler_class, lr_scheduler_init)\r\n\r\n        if is_overridden(\"configure_optimizers\", self.model):\r\n            _warn(\r\n                f\"`{self.model.__class__.__name__}.configure_optimizers` will be overridden by \"\r\n                f\"`{self.__class__.__name__}.configure_optimizers`.\"\r\n            )\r\n\r\n        optimizer = instantiate_class(self.model.parameters(), optimizer_init)\r\n        lr_scheduler = instantiate_class(optimizer, lr_scheduler_init) if lr_scheduler_init else None\r\n        fn = partial(self.configure_optimizers, optimizer=optimizer, lr_scheduler=lr_scheduler)\r\n        update_wrapper(fn, self.configure_optimizers)  # necessary for `is_overridden`\r\n        self.model.configure_optimizers = MethodType(fn, self.model)", "language": "python", "code": "def _add_configure_optimizers_method_to_model(self, subcommand: Optional[str]) -> None:\r\n        \"\"\"Overrides the model's :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method if a\r\n        single optimizer and optionally a scheduler argument groups are added to the parser as 'AUTOMATIC'.\"\"\"\r\n        if not self.auto_configure_optimizers:\r\n            return\r\n\r\n        parser = self._parser(subcommand)\r\n\r\n        def get_automatic(\r\n            class_type: Union[type, tuple[type, ...]], register: dict[str, tuple[Union[type, tuple[type, ...]], str]]\r\n        ) -> list[str]:\r\n            automatic = []\r\n            for key, (base_class, link_to) in register.items():\r\n                if not isinstance(base_class, tuple):\r\n                    base_class = (base_class,)\r\n                if link_to == \"AUTOMATIC\" and any(issubclass(c, class_type) for c in base_class):\r\n                    automatic.append(key)\r\n            return automatic\r\n\r\n        optimizers = get_automatic(Optimizer, parser._optimizers)\r\n        lr_schedulers = get_automatic(LRSchedulerTypeTuple, parser._lr_schedulers)\r\n\r\n        if len(optimizers) == 0:\r\n            return\r\n\r\n        if len(optimizers) > 1 or len(lr_schedulers) > 1:\r\n            raise MisconfigurationException(\r\n                f\"`{self.__class__.__name__}.add_configure_optimizers_method_to_model` expects at most one optimizer \"\r\n                f\"and one lr_scheduler to be 'AUTOMATIC', but found {optimizers + lr_schedulers}. In this case the \"\r\n                \"user is expected to link the argument groups and implement `configure_optimizers`, see \"\r\n                \"https://lightning.ai/docs/pytorch/stable/common/lightning_cli.html\"\r\n                \"#optimizers-and-learning-rate-schedulers\"\r\n            )\r\n\r\n        optimizer_class = parser._optimizers[optimizers[0]][0]\r\n        optimizer_init = self._get(self.config_init, optimizers[0])\r\n        if not isinstance(optimizer_class, tuple):\r\n            optimizer_init = _global_add_class_path(optimizer_class, optimizer_init)\r\n        if not optimizer_init:\r\n            return\r\n\r\n        lr_scheduler_init = None\r\n        if lr_schedulers:\r\n            lr_scheduler_class = parser._lr_schedulers[lr_schedulers[0]][0]\r\n            lr_scheduler_init = self._get(self.config_init, lr_schedulers[0])\r\n            if not isinstance(lr_scheduler_class, tuple):\r\n                lr_scheduler_init = _global_add_class_path(lr_scheduler_class, lr_scheduler_init)\r\n\r\n        if is_overridden(\"configure_optimizers\", self.model):\r\n            _warn(\r\n                f\"`{self.model.__class__.__name__}.configure_optimizers` will be overridden by \"\r\n                f\"`{self.__class__.__name__}.configure_optimizers`.\"\r\n            )\r\n\r\n        optimizer = instantiate_class(self.model.parameters(), optimizer_init)\r\n        lr_scheduler = instantiate_class(optimizer, lr_scheduler_init) if lr_scheduler_init else None\r\n        fn = partial(self.configure_optimizers, optimizer=optimizer, lr_scheduler=lr_scheduler)\r\n        update_wrapper(fn, self.configure_optimizers)  # necessary for `is_overridden`\r\n        self.model.configure_optimizers = MethodType(fn, self.model)", "code_tokens": ["def", "_add_configure_optimizers_method_to_model", "(", "self", ",", "subcommand", ":", "Optional", "[", "str", "]", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "auto_configure_optimizers", ":", "return", "parser", "=", "self", ".", "_parser", "(", "subcommand", ")", "def", "get_automatic", "(", "class_type", ":", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "register", ":", "dict", "[", "str", ",", "tuple", "[", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "str", "]", "]", ")", "-", ">", "list", "[", "str", "]", ":", "automatic", "=", "[", "]", "for", "key", ",", "(", "base_class", ",", "link_to", ")", "in", "register", ".", "items", "(", ")", ":", "if", "not", "isinstance", "(", "base_class", ",", "tuple", ")", ":", "base_class", "=", "(", "base_class", ",", ")", "if", "link_to", "=", "=", "STRING", "and", "any", "(", "issubclass", "(", "c", ",", "class_type", ")", "for", "c", "in", "base_class", ")", ":", "automatic", ".", "append", "(", "key", ")", "return", "automatic", "optimizers", "=", "get_automatic", "(", "Optimizer", ",", "parser", ".", "_optimizers", ")", "lr_schedulers", "=", "get_automatic", "(", "LRSchedulerTypeTuple", ",", "parser", ".", "_lr_schedulers", ")", "if", "len", "(", "optimizers", ")", "=", "=", "0", ":", "return", "if", "len", "(", "optimizers", ")", ">", "1", "or", "len", "(", "lr_schedulers", ")", ">", "1", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", "STRING", "STRING", "STRING", ")", "optimizer_class", "=", "parser", ".", "_optimizers", "[", "optimizers", "[", "0", "]", "]", "[", "0", "]", "optimizer_init", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "optimizers", "[", "0", "]", ")", "if", "not", "isinstance", "(", "optimizer_class", ",", "tuple", ")", ":", "optimizer_init", "=", "_global_add_class_path", "(", "optimizer_class", ",", "optimizer_init", ")", "if", "not", "optimizer_init", ":", "return", "lr_scheduler_init", "=", "None", "if", "lr_schedulers", ":", "lr_scheduler_class", "=", "parser", ".", "_lr_schedulers", "[", "lr_schedulers", "[", "0", "]", "]", "[", "0", "]", "lr_scheduler_init", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "lr_schedulers", "[", "0", "]", ")", "if", "not", "isinstance", "(", "lr_scheduler_class", ",", "tuple", ")", ":", "lr_scheduler_init", "=", "_global_add_class_path", "(", "lr_scheduler_class", ",", "lr_scheduler_init", ")", "if", "is_overridden", "(", "STRING", ",", "self", ".", "model", ")", ":", "_warn", "(", "fSTRING", "fSTRING", ")", "optimizer", "=", "instantiate_class", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "optimizer_init", ")", "lr_scheduler", "=", "instantiate_class", "(", "optimizer", ",", "lr_scheduler_init", ")", "if", "lr_scheduler_init", "else", "None", "fn", "=", "partial", "(", "self", ".", "configure_optimizers", ",", "optimizer", "=", "optimizer", ",", "lr_scheduler", "=", "lr_scheduler", ")", "update_wrapper", "(", "fn", ",", "self", ".", "configure_optimizers", ")", "#", "necessary", "for", "`", "is_overridden", "`", "self", ".", "model", ".", "configure_optimizers", "=", "MethodType", "(", "fn", ",", "self", ".", "model", ")"], "docstring": "Overrides the model's :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method if a", "docstring_tokens": ["overrides", "the", "model", "s", "meth", "lightning", "pytorch", "core", "lightningmodule", "configure_optimizers", "method", "if", "a"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 676, "end_line": 736, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_358", "original_string": "def _get(self, config: Namespace, key: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Utility to get a config value which might be inside a subcommand.\"\"\"\r\n        return config.get(str(self.subcommand), config).get(key, default)", "language": "python", "code": "def _get(self, config: Namespace, key: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Utility to get a config value which might be inside a subcommand.\"\"\"\r\n        return config.get(str(self.subcommand), config).get(key, default)", "code_tokens": ["def", "_get", "(", "self", ",", "config", ":", "Namespace", ",", "key", ":", "str", ",", "default", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "Any", ":", "STRING", "return", "config", ".", "get", "(", "str", "(", "self", ".", "subcommand", ")", ",", "config", ")", ".", "get", "(", "key", ",", "default", ")"], "docstring": "Utility to get a config value which might be inside a subcommand.", "docstring_tokens": ["utility", "to", "get", "a", "config", "value", "which", "might", "be", "inside", "a", "subcommand"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 738, "end_line": 740, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_359", "original_string": "def _run_subcommand(self, subcommand: str) -> None:\r\n        \"\"\"Run the chosen subcommand.\"\"\"\r\n        before_fn = getattr(self, f\"before_{subcommand}\", None)\r\n        if callable(before_fn):\r\n            before_fn()\r\n\r\n        default = getattr(self.trainer, subcommand)\r\n        fn = getattr(self, subcommand, default)\r\n        fn_kwargs = self._prepare_subcommand_kwargs(subcommand)\r\n        fn(**fn_kwargs)\r\n\r\n        after_fn = getattr(self, f\"after_{subcommand}\", None)\r\n        if callable(after_fn):\r\n            after_fn()", "language": "python", "code": "def _run_subcommand(self, subcommand: str) -> None:\r\n        \"\"\"Run the chosen subcommand.\"\"\"\r\n        before_fn = getattr(self, f\"before_{subcommand}\", None)\r\n        if callable(before_fn):\r\n            before_fn()\r\n\r\n        default = getattr(self.trainer, subcommand)\r\n        fn = getattr(self, subcommand, default)\r\n        fn_kwargs = self._prepare_subcommand_kwargs(subcommand)\r\n        fn(**fn_kwargs)\r\n\r\n        after_fn = getattr(self, f\"after_{subcommand}\", None)\r\n        if callable(after_fn):\r\n            after_fn()", "code_tokens": ["def", "_run_subcommand", "(", "self", ",", "subcommand", ":", "str", ")", "-", ">", "None", ":", "STRING", "before_fn", "=", "getattr", "(", "self", ",", "fSTRING", ",", "None", ")", "if", "callable", "(", "before_fn", ")", ":", "before_fn", "(", ")", "default", "=", "getattr", "(", "self", ".", "trainer", ",", "subcommand", ")", "fn", "=", "getattr", "(", "self", ",", "subcommand", ",", "default", ")", "fn_kwargs", "=", "self", ".", "_prepare_subcommand_kwargs", "(", "subcommand", ")", "fn", "(", "*", "*", "fn_kwargs", ")", "after_fn", "=", "getattr", "(", "self", ",", "fSTRING", ",", "None", ")", "if", "callable", "(", "after_fn", ")", ":", "after_fn", "(", ")"], "docstring": "Run the chosen subcommand.", "docstring_tokens": ["run", "the", "chosen", "subcommand"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 742, "end_line": 755, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_360", "original_string": "def _prepare_subcommand_kwargs(self, subcommand: str) -> dict[str, Any]:\r\n        \"\"\"Prepares the keyword arguments to pass to the subcommand to run.\"\"\"\r\n        fn_kwargs = {\r\n            k: v for k, v in self.config_init[subcommand].items() if k in self._subcommand_method_arguments[subcommand]\r\n        }\r\n        fn_kwargs[\"model\"] = self.model\r\n        if self.datamodule is not None:\r\n            fn_kwargs[\"datamodule\"] = self.datamodule\r\n        return fn_kwargs", "language": "python", "code": "def _prepare_subcommand_kwargs(self, subcommand: str) -> dict[str, Any]:\r\n        \"\"\"Prepares the keyword arguments to pass to the subcommand to run.\"\"\"\r\n        fn_kwargs = {\r\n            k: v for k, v in self.config_init[subcommand].items() if k in self._subcommand_method_arguments[subcommand]\r\n        }\r\n        fn_kwargs[\"model\"] = self.model\r\n        if self.datamodule is not None:\r\n            fn_kwargs[\"datamodule\"] = self.datamodule\r\n        return fn_kwargs", "code_tokens": ["def", "_prepare_subcommand_kwargs", "(", "self", ",", "subcommand", ":", "str", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "fn_kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "config_init", "[", "subcommand", "]", ".", "items", "(", ")", "if", "k", "in", "self", ".", "_subcommand_method_arguments", "[", "subcommand", "]", "}", "fn_kwargs", "[", "STRING", "]", "=", "self", ".", "model", "if", "self", ".", "datamodule", "is", "not", "None", ":", "fn_kwargs", "[", "STRING", "]", "=", "self", ".", "datamodule", "return", "fn_kwargs"], "docstring": "Prepares the keyword arguments to pass to the subcommand to run.", "docstring_tokens": ["prepares", "the", "keyword", "arguments", "to", "pass", "to", "the", "subcommand", "to", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 757, "end_line": 765, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_361", "original_string": "def _set_seed(self) -> None:\r\n        \"\"\"Sets the seed.\"\"\"\r\n        config_seed = self._get(self.config, \"seed_everything\")\r\n        if config_seed is False:\r\n            return\r\n        if config_seed is True:\r\n            config_seed = seed_everything(workers=True)\r\n        else:\r\n            config_seed = seed_everything(config_seed, workers=True)\r\n        if self.subcommand:\r\n            self.config[self.subcommand][\"seed_everything\"] = config_seed\r\n        else:\r\n            self.config[\"seed_everything\"] = config_seed", "language": "python", "code": "def _set_seed(self) -> None:\r\n        \"\"\"Sets the seed.\"\"\"\r\n        config_seed = self._get(self.config, \"seed_everything\")\r\n        if config_seed is False:\r\n            return\r\n        if config_seed is True:\r\n            config_seed = seed_everything(workers=True)\r\n        else:\r\n            config_seed = seed_everything(config_seed, workers=True)\r\n        if self.subcommand:\r\n            self.config[self.subcommand][\"seed_everything\"] = config_seed\r\n        else:\r\n            self.config[\"seed_everything\"] = config_seed", "code_tokens": ["def", "_set_seed", "(", "self", ")", "-", ">", "None", ":", "STRING", "config_seed", "=", "self", ".", "_get", "(", "self", ".", "config", ",", "STRING", ")", "if", "config_seed", "is", "False", ":", "return", "if", "config_seed", "is", "True", ":", "config_seed", "=", "seed_everything", "(", "workers", "=", "True", ")", "else", ":", "config_seed", "=", "seed_everything", "(", "config_seed", ",", "workers", "=", "True", ")", "if", "self", ".", "subcommand", ":", "self", ".", "config", "[", "self", ".", "subcommand", "]", "[", "STRING", "]", "=", "config_seed", "else", ":", "self", ".", "config", "[", "STRING", "]", "=", "config_seed"], "docstring": "Sets the seed.", "docstring_tokens": ["sets", "the", "seed"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 767, "end_line": 780, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "function_362", "original_string": "def instantiate_class(args: Union[Any, tuple[Any, ...]], init: dict[str, Any]) -> Any:\r\n    \"\"\"Instantiates a class with the given args and init.\r\n\r\n    Args:\r\n        args: Positional arguments required for instantiation.\r\n        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\r\n\r\n    Returns:\r\n        The instantiated class object.\r\n\r\n    \"\"\"\r\n    kwargs = init.get(\"init_args\", {})\r\n    if not isinstance(args, tuple):\r\n        args = (args,)\r\n    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\r\n    module = __import__(class_module, fromlist=[class_name])\r\n    args_class = getattr(module, class_name)\r\n    return args_class(*args, **kwargs)", "language": "python", "code": "def instantiate_class(args: Union[Any, tuple[Any, ...]], init: dict[str, Any]) -> Any:\r\n    \"\"\"Instantiates a class with the given args and init.\r\n\r\n    Args:\r\n        args: Positional arguments required for instantiation.\r\n        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\r\n\r\n    Returns:\r\n        The instantiated class object.\r\n\r\n    \"\"\"\r\n    kwargs = init.get(\"init_args\", {})\r\n    if not isinstance(args, tuple):\r\n        args = (args,)\r\n    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\r\n    module = __import__(class_module, fromlist=[class_name])\r\n    args_class = getattr(module, class_name)\r\n    return args_class(*args, **kwargs)", "code_tokens": ["def", "instantiate_class", "(", "args", ":", "Union", "[", "Any", ",", "tuple", "[", "Any", ",", ".", ".", ".", "]", "]", ",", "init", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "Any", ":", "STRING", "kwargs", "=", "init", ".", "get", "(", "STRING", ",", "{", "}", ")", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "args", "=", "(", "args", ",", ")", "class_module", ",", "class_name", "=", "init", "[", "STRING", "]", ".", "rsplit", "(", "STRING", ",", "1", ")", "module", "=", "__import__", "(", "class_module", ",", "fromlist", "=", "[", "class_name", "]", ")", "args_class", "=", "getattr", "(", "module", ",", "class_name", ")", "return", "args_class", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Instantiates a class with the given args and init.", "docstring_tokens": ["instantiates", "a", "class", "with", "the", "given", "args", "and", "init"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\cli.py", "start_line": 802, "end_line": 819, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "func_name": "function_363", "original_string": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Called by the Trainer to set up the accelerator before the model starts running on the device.\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Called by the Trainer to set up the accelerator before the model starts running on the device.\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called by the Trainer to set up the accelerator before the model starts running on the device.", "docstring_tokens": ["called", "by", "the", "trainer", "to", "set", "up", "the", "accelerator", "before", "the", "model", "starts", "running", "on", "the", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "start_line": 28, "end_line": 34, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "func_name": "function_364", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get stats for a given device.\r\n\r\n        Args:\r\n            device: device for which to get stats\r\n\r\n        Returns:\r\n            Dictionary of device stats\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get stats for a given device.\r\n\r\n        Args:\r\n            device: device for which to get stats\r\n\r\n        Returns:\r\n            Dictionary of device stats\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "raise", "NotImplementedError"], "docstring": "Get stats for a given device.", "docstring_tokens": ["get", "stats", "for", "a", "given", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "start_line": 36, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_365", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise MisconfigurationException(f\"Device should be CPU, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise MisconfigurationException(f\"Device should be CPU, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 30, "end_line": 37, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_366", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get CPU stats from ``psutil`` package.\"\"\"\r\n        return get_cpu_stats()", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get CPU stats from ``psutil`` package.\"\"\"\r\n        return get_cpu_stats()", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "get_cpu_stats", "(", ")"], "docstring": "Get CPU stats from ``psutil`` package.", "docstring_tokens": ["get", "cpu", "stats", "from", "psutil", "package"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 40, "end_line": 42, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_367", "original_string": "def parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)", "language": "python", "code": "def parse_devices(devices: Union[int, str]) -> int:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_cpu_cores(devices)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "int", ":", "STRING", "return", "_parse_cpu_cores", "(", "devices", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 50, "end_line": 52, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_368", "original_string": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "devices", "=", "_parse_cpu_cores", "(", "devices", ")", "return", "[", "torch", ".", "device", "(", "STRING", ")", "]", "*", "devices"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 56, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_369", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "1"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 63, "end_line": 65, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "function_370", "original_string": "def is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"CPU is always available for execution.\"\"\"\r\n        return True", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "STRING", "return", "True"], "docstring": "CPU is always available for execution.", "docstring_tokens": ["cpu", "is", "always", "available", "for", "execution"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "start_line": 69, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_371", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not GPU.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise MisconfigurationException(f\"Device should be GPU, got {device} instead\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not GPU.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise MisconfigurationException(f\"Device should be GPU, got {device} instead\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "_check_cuda_matmul_precision", "(", "device", ")", "torch", ".", "cuda", ".", "set_device", "(", "device", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 37, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_372", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given GPU device.\r\n\r\n        Args:\r\n            device: GPU device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics to their values.\r\n\r\n        Raises:\r\n            FileNotFoundError:\r\n                If nvidia-smi installation not found\r\n\r\n        \"\"\"\r\n        return torch.cuda.memory_stats(device)", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given GPU device.\r\n\r\n        Args:\r\n            device: GPU device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics to their values.\r\n\r\n        Raises:\r\n            FileNotFoundError:\r\n                If nvidia-smi installation not found\r\n\r\n        \"\"\"\r\n        return torch.cuda.memory_stats(device)", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "torch", ".", "cuda", ".", "memory_stats", "(", "device", ")"], "docstring": "Gets stats for the given GPU device.", "docstring_tokens": ["gets", "stats", "for", "the", "given", "gpu", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 63, "end_line": 77, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_373", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "STRING", "return", "_parse_gpu_ids", "(", "devices", ",", "include_cuda", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 85, "end_line": 87, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_374", "original_string": "def get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]", "language": "python", "code": "def get_parallel_devices(devices: list[int]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        return [torch.device(\"cuda\", i) for i in devices]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "list", "[", "int", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "return", "[", "torch", ".", "device", "(", "STRING", ",", "i", ")", "for", "i", "in", "devices", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 91, "end_line": 93, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_375", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return num_cuda_devices()", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "num_cuda_devices", "(", ")"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 97, "end_line": 99, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_376", "original_string": "def get_nvidia_gpu_stats(device: _DEVICE) -> dict[str, float]:  # pragma: no-cover\r\n    \"\"\"Get GPU stats including memory, fan speed, and temperature from nvidia-smi.\r\n\r\n    Args:\r\n        device: GPU device for which to get stats\r\n\r\n    Returns:\r\n        A dictionary mapping the metrics to their values.\r\n\r\n    Raises:\r\n        FileNotFoundError:\r\n            If nvidia-smi installation not found\r\n\r\n    \"\"\"\r\n    nvidia_smi_path = shutil.which(\"nvidia-smi\")\r\n    if nvidia_smi_path is None:\r\n        raise FileNotFoundError(\"nvidia-smi: command not found\")\r\n\r\n    gpu_stat_metrics = [\r\n        (\"utilization.gpu\", \"%\"),\r\n        (\"memory.used\", \"MB\"),\r\n        (\"memory.free\", \"MB\"),\r\n        (\"utilization.memory\", \"%\"),\r\n        (\"fan.speed\", \"%\"),\r\n        (\"temperature.gpu\", \"\u00b0C\"),\r\n        (\"temperature.memory\", \"\u00b0C\"),\r\n    ]\r\n    gpu_stat_keys = [k for k, _ in gpu_stat_metrics]\r\n    gpu_query = \",\".join(gpu_stat_keys)\r\n\r\n    index = torch._utils._get_device_index(device)\r\n    gpu_id = _get_gpu_id(index)\r\n    result = subprocess.run(\r\n        [nvidia_smi_path, f\"--query-gpu={gpu_query}\", \"--format=csv,nounits,noheader\", f\"--id={gpu_id}\"],\r\n        encoding=\"utf-8\",\r\n        capture_output=True,\r\n        check=True,\r\n    )\r\n\r\n    def _to_float(x: str) -> float:\r\n        try:\r\n            return float(x)\r\n        except ValueError:\r\n            return 0.0\r\n\r\n    s = result.stdout.strip()\r\n    stats = [_to_float(x) for x in s.split(\", \")]\r\n    return {f\"{x} ({unit})\": stat for (x, unit), stat in zip(gpu_stat_metrics, stats)}", "language": "python", "code": "def get_nvidia_gpu_stats(device: _DEVICE) -> dict[str, float]:  # pragma: no-cover\r\n    \"\"\"Get GPU stats including memory, fan speed, and temperature from nvidia-smi.\r\n\r\n    Args:\r\n        device: GPU device for which to get stats\r\n\r\n    Returns:\r\n        A dictionary mapping the metrics to their values.\r\n\r\n    Raises:\r\n        FileNotFoundError:\r\n            If nvidia-smi installation not found\r\n\r\n    \"\"\"\r\n    nvidia_smi_path = shutil.which(\"nvidia-smi\")\r\n    if nvidia_smi_path is None:\r\n        raise FileNotFoundError(\"nvidia-smi: command not found\")\r\n\r\n    gpu_stat_metrics = [\r\n        (\"utilization.gpu\", \"%\"),\r\n        (\"memory.used\", \"MB\"),\r\n        (\"memory.free\", \"MB\"),\r\n        (\"utilization.memory\", \"%\"),\r\n        (\"fan.speed\", \"%\"),\r\n        (\"temperature.gpu\", \"\u00b0C\"),\r\n        (\"temperature.memory\", \"\u00b0C\"),\r\n    ]\r\n    gpu_stat_keys = [k for k, _ in gpu_stat_metrics]\r\n    gpu_query = \",\".join(gpu_stat_keys)\r\n\r\n    index = torch._utils._get_device_index(device)\r\n    gpu_id = _get_gpu_id(index)\r\n    result = subprocess.run(\r\n        [nvidia_smi_path, f\"--query-gpu={gpu_query}\", \"--format=csv,nounits,noheader\", f\"--id={gpu_id}\"],\r\n        encoding=\"utf-8\",\r\n        capture_output=True,\r\n        check=True,\r\n    )\r\n\r\n    def _to_float(x: str) -> float:\r\n        try:\r\n            return float(x)\r\n        except ValueError:\r\n            return 0.0\r\n\r\n    s = result.stdout.strip()\r\n    stats = [_to_float(x) for x in s.split(\", \")]\r\n    return {f\"{x} ({unit})\": stat for (x, unit), stat in zip(gpu_stat_metrics, stats)}", "code_tokens": ["def", "get_nvidia_gpu_stats", "(", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "float", "]", ":", "#", "pragma", ":", "no", "-", "cover", "STRING", "nvidia_smi_path", "=", "shutil", ".", "which", "(", "STRING", ")", "if", "nvidia_smi_path", "is", "None", ":", "raise", "FileNotFoundError", "(", "STRING", ")", "gpu_stat_metrics", "=", "[", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "]", "gpu_stat_keys", "=", "[", "k", "for", "k", ",", "_", "in", "gpu_stat_metrics", "]", "gpu_query", "=", "STRING", ".", "join", "(", "gpu_stat_keys", ")", "index", "=", "torch", ".", "_utils", ".", "_get_device_index", "(", "device", ")", "gpu_id", "=", "_get_gpu_id", "(", "index", ")", "result", "=", "subprocess", ".", "run", "(", "[", "nvidia_smi_path", ",", "fSTRING", ",", "STRING", ",", "fSTRING", "]", ",", "encoding", "=", "STRING", ",", "capture_output", "=", "True", ",", "check", "=", "True", ",", ")", "def", "_to_float", "(", "x", ":", "str", ")", "-", ">", "float", ":", "try", ":", "return", "float", "(", "x", ")", "except", "ValueError", ":", "return", "0", ".", "0", "s", "=", "result", ".", "stdout", ".", "strip", "(", ")", "stats", "=", "[", "_to_float", "(", "x", ")", "for", "x", "in", "s", ".", "split", "(", "STRING", ")", "]", "return", "{", "fSTRING", ":", "stat", "for", "(", "x", ",", "unit", ")", ",", "stat", "in", "zip", "(", "gpu_stat_metrics", ",", "stats", ")", "}"], "docstring": "Get GPU stats including memory, fan speed, and temperature from nvidia-smi.", "docstring_tokens": ["get", "gpu", "stats", "including", "memory", "fan", "speed", "and", "temperature", "from", "nvidia", "smi"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 116, "end_line": 163, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "function_377", "original_string": "def _get_gpu_id(device_id: int) -> str:\r\n    \"\"\"Get the unmasked real GPU IDs.\"\"\"\r\n    default = \",\".join(str(i) for i in range(num_cuda_devices()))\r\n    cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", default=default).split(\",\")\r\n    return cuda_visible_devices[device_id].strip()", "language": "python", "code": "def _get_gpu_id(device_id: int) -> str:\r\n    \"\"\"Get the unmasked real GPU IDs.\"\"\"\r\n    default = \",\".join(str(i) for i in range(num_cuda_devices()))\r\n    cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", default=default).split(\",\")\r\n    return cuda_visible_devices[device_id].strip()", "code_tokens": ["def", "_get_gpu_id", "(", "device_id", ":", "int", ")", "-", ">", "str", ":", "STRING", "default", "=", "STRING", ".", "join", "(", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_cuda_devices", "(", ")", ")", ")", "cuda_visible_devices", "=", "os", ".", "getenv", "(", "STRING", ",", "default", "=", "default", ")", ".", "split", "(", "STRING", ")", "return", "cuda_visible_devices", "[", "device_id", "]", ".", "strip", "(", ")"], "docstring": "Get the unmasked real GPU IDs.", "docstring_tokens": ["get", "the", "unmasked", "real", "gpu", "ids"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "start_line": 166, "end_line": 171, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_378", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise MisconfigurationException(f\"Device should be MPS, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise MisconfigurationException(f\"Device should be MPS, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "STRING", "if", "device", ".", "type", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 35, "end_line": 42, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_379", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get M1 (cpu + gpu) stats from ``psutil`` package.\"\"\"\r\n        return get_device_stats()", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get M1 (cpu + gpu) stats from ``psutil`` package.\"\"\"\r\n        return get_device_stats()", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "get_device_stats", "(", ")"], "docstring": "Get M1 (cpu + gpu) stats from ``psutil`` package.", "docstring_tokens": ["get", "m1", "cpu", "gpu", "stats", "from", "psutil", "package"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 45, "end_line": 47, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_380", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_gpu_ids(devices, include_mps=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        return _parse_gpu_ids(devices, include_mps=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "STRING", "return", "_parse_gpu_ids", "(", "devices", ",", "include_mps", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 55, "end_line": 57, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_381", "original_string": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "STRING", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "assert", "parsed_devices", "is", "not", "None", "return", "[", "torch", ".", "device", "(", "STRING", ",", "i", ")", "for", "i", "in", "range", "(", "len", "(", "parsed_devices", ")", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 61, "end_line": 66, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_382", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        return 1", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "STRING", "return", "1"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 70, "end_line": 72, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "function_383", "original_string": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        return _MPSAccelerator.is_available()", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        return _MPSAccelerator.is_available()", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "STRING", "return", "_MPSAccelerator", ".", "is_available", "(", ")"], "docstring": "MPS is only available on a machine with the ARM-based Apple Silicon processors.", "docstring_tokens": ["mps", "is", "only", "available", "on", "a", "machine", "with", "the", "arm", "based", "apple", "silicon", "processors"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\mps.py", "start_line": 76, "end_line": 78, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\xla.py", "func_name": "function_384", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given XLA device.\r\n\r\n        Args:\r\n            device: XLA device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics (free memory and peak memory) to their values.\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        memory_info = xm.get_memory_info(device)\r\n        free_memory = memory_info[\"kb_free\"]\r\n        peak_memory = memory_info[\"kb_total\"] - free_memory\r\n        return {\r\n            \"avg. free memory (MB)\": free_memory,\r\n            \"avg. peak memory (MB)\": peak_memory,\r\n        }", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given XLA device.\r\n\r\n        Args:\r\n            device: XLA device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics (free memory and peak memory) to their values.\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        memory_info = xm.get_memory_info(device)\r\n        free_memory = memory_info[\"kb_free\"]\r\n        peak_memory = memory_info[\"kb_total\"] - free_memory\r\n        return {\r\n            \"avg. free memory (MB)\": free_memory,\r\n            \"avg. peak memory (MB)\": peak_memory,\r\n        }", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "memory_info", "=", "xm", ".", "get_memory_info", "(", "device", ")", "free_memory", "=", "memory_info", "[", "STRING", "]", "peak_memory", "=", "memory_info", "[", "STRING", "]", "-", "free_memory", "return", "{", "STRING", ":", "free_memory", ",", "STRING", ":", "peak_memory", ",", "}"], "docstring": "Gets stats for the given XLA device.", "docstring_tokens": ["gets", "stats", "for", "the", "given", "xla", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\accelerators\\xla.py", "start_line": 31, "end_line": 49, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_385", "original_string": "def state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__", "language": "python", "code": "def state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__", "code_tokens": ["def", "state_key", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "__class__", ".", "__qualname__"], "docstring": "Identifier for the state of the callback.", "docstring_tokens": ["identifier", "for", "the", "state", "of", "the", "callback"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 32, "end_line": 40, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_386", "original_string": "def _legacy_state_key(self) -> type[\"Callback\"]:\r\n        \"\"\"State key for checkpoints saved prior to version 1.5.0.\"\"\"\r\n        return type(self)", "language": "python", "code": "def _legacy_state_key(self) -> type[\"Callback\"]:\r\n        \"\"\"State key for checkpoints saved prior to version 1.5.0.\"\"\"\r\n        return type(self)", "code_tokens": ["def", "_legacy_state_key", "(", "self", ")", "-", ">", "type", "[", "STRING", "]", ":", "STRING", "return", "type", "(", "self", ")"], "docstring": "State key for checkpoints saved prior to version 1.5.0.", "docstring_tokens": ["state", "key", "for", "checkpoints", "saved", "prior", "to", "version", "1", "5", "0"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 43, "end_line": 45, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_387", "original_string": "def _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"", "language": "python", "code": "def _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"", "code_tokens": ["def", "_generate_state_key", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "str", ":", "STRING", "return", "fSTRING"], "docstring": "Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for", "docstring_tokens": ["formats", "a", "set", "of", "key", "value", "pairs", "into", "a", "state", "key", "string", "with", "the", "callback", "class", "name", "prefixed", "useful", "for"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 47, "end_line": 55, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_388", "original_string": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\"\"\"", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\"\"\"", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when fit, validate, test, predict, or tune begins.", "docstring_tokens": ["called", "when", "fit", "validate", "test", "predict", "or", "tune", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 57, "end_line": 58, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_389", "original_string": "def teardown(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune ends.\"\"\"", "language": "python", "code": "def teardown(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune ends.\"\"\"", "code_tokens": ["def", "teardown", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when fit, validate, test, predict, or tune ends.", "docstring_tokens": ["called", "when", "fit", "validate", "test", "predict", "or", "tune", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 60, "end_line": 61, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_390", "original_string": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit begins.\"\"\"", "language": "python", "code": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit begins.\"\"\"", "code_tokens": ["def", "on_fit_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when fit begins.", "docstring_tokens": ["called", "when", "fit", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 63, "end_line": 64, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_391", "original_string": "def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit ends.\"\"\"", "language": "python", "code": "def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when fit ends.\"\"\"", "code_tokens": ["def", "on_fit_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when fit ends.", "docstring_tokens": ["called", "when", "fit", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 66, "end_line": 67, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_392", "original_string": "def on_sanity_check_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check starts.\"\"\"", "language": "python", "code": "def on_sanity_check_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check starts.\"\"\"", "code_tokens": ["def", "on_sanity_check_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation sanity check starts.", "docstring_tokens": ["called", "when", "the", "validation", "sanity", "check", "starts"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 69, "end_line": 70, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_393", "original_string": "def on_sanity_check_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check ends.\"\"\"", "language": "python", "code": "def on_sanity_check_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation sanity check ends.\"\"\"", "code_tokens": ["def", "on_sanity_check_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation sanity check ends.", "docstring_tokens": ["called", "when", "the", "validation", "sanity", "check", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 72, "end_line": 73, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_394", "original_string": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"", "language": "python", "code": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train batch begins.", "docstring_tokens": ["called", "when", "the", "train", "batch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 75, "end_line": 78, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_395", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train batch ends.", "docstring_tokens": ["called", "when", "the", "train", "batch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 80, "end_line": 89, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_396", "original_string": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch begins.\"\"\"", "language": "python", "code": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch begins.\"\"\"", "code_tokens": ["def", "on_train_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train epoch begins.", "docstring_tokens": ["called", "when", "the", "train", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 91, "end_line": 92, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_397", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train epoch ends.", "docstring_tokens": ["called", "when", "the", "train", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 94, "end_line": 121, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_398", "original_string": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch begins.\"\"\"", "language": "python", "code": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch begins.\"\"\"", "code_tokens": ["def", "on_validation_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the val epoch begins.", "docstring_tokens": ["called", "when", "the", "val", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 123, "end_line": 124, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_399", "original_string": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch ends.\"\"\"", "language": "python", "code": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the val epoch ends.\"\"\"", "code_tokens": ["def", "on_validation_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the val epoch ends.", "docstring_tokens": ["called", "when", "the", "val", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 126, "end_line": 127, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_400", "original_string": "def on_test_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch begins.\"\"\"", "language": "python", "code": "def on_test_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch begins.\"\"\"", "code_tokens": ["def", "on_test_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test epoch begins.", "docstring_tokens": ["called", "when", "the", "test", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 129, "end_line": 130, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_401", "original_string": "def on_test_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch ends.\"\"\"", "language": "python", "code": "def on_test_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test epoch ends.\"\"\"", "code_tokens": ["def", "on_test_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test epoch ends.", "docstring_tokens": ["called", "when", "the", "test", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 132, "end_line": 133, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_402", "original_string": "def on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch begins.\"\"\"", "language": "python", "code": "def on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch begins.\"\"\"", "code_tokens": ["def", "on_predict_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the predict epoch begins.", "docstring_tokens": ["called", "when", "the", "predict", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 135, "end_line": 136, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_403", "original_string": "def on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch ends.\"\"\"", "language": "python", "code": "def on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict epoch ends.\"\"\"", "code_tokens": ["def", "on_predict_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the predict epoch ends.", "docstring_tokens": ["called", "when", "the", "predict", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 138, "end_line": 139, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_404", "original_string": "def on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"", "language": "python", "code": "def on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"", "code_tokens": ["def", "on_validation_batch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation batch begins.", "docstring_tokens": ["called", "when", "the", "validation", "batch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 141, "end_line": 149, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_405", "original_string": "def on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"", "language": "python", "code": "def on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"", "code_tokens": ["def", "on_validation_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation batch ends.", "docstring_tokens": ["called", "when", "the", "validation", "batch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 151, "end_line": 160, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_406", "original_string": "def on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"", "language": "python", "code": "def on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"", "code_tokens": ["def", "on_test_batch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test batch begins.", "docstring_tokens": ["called", "when", "the", "test", "batch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 162, "end_line": 170, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_407", "original_string": "def on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"", "language": "python", "code": "def on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"", "code_tokens": ["def", "on_test_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test batch ends.", "docstring_tokens": ["called", "when", "the", "test", "batch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 172, "end_line": 181, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_408", "original_string": "def on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"", "language": "python", "code": "def on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"", "code_tokens": ["def", "on_predict_batch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the predict batch begins.", "docstring_tokens": ["called", "when", "the", "predict", "batch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 183, "end_line": 191, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_409", "original_string": "def on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"", "language": "python", "code": "def on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"", "code_tokens": ["def", "on_predict_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "Any", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the predict batch ends.", "docstring_tokens": ["called", "when", "the", "predict", "batch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 193, "end_line": 202, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_410", "original_string": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train begins.\"\"\"", "language": "python", "code": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train begins.\"\"\"", "code_tokens": ["def", "on_train_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train begins.", "docstring_tokens": ["called", "when", "the", "train", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 204, "end_line": 205, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_411", "original_string": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train ends.\"\"\"", "language": "python", "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train ends.\"\"\"", "code_tokens": ["def", "on_train_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the train ends.", "docstring_tokens": ["called", "when", "the", "train", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 207, "end_line": 208, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_412", "original_string": "def on_validation_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop begins.\"\"\"", "language": "python", "code": "def on_validation_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop begins.\"\"\"", "code_tokens": ["def", "on_validation_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation loop begins.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 210, "end_line": 211, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_413", "original_string": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop ends.\"\"\"", "language": "python", "code": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the validation loop ends.\"\"\"", "code_tokens": ["def", "on_validation_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the validation loop ends.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 213, "end_line": 214, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_414", "original_string": "def on_test_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test begins.\"\"\"", "language": "python", "code": "def on_test_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test begins.\"\"\"", "code_tokens": ["def", "on_test_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test begins.", "docstring_tokens": ["called", "when", "the", "test", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 216, "end_line": 217, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_415", "original_string": "def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test ends.\"\"\"", "language": "python", "code": "def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the test ends.\"\"\"", "code_tokens": ["def", "on_test_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the test ends.", "docstring_tokens": ["called", "when", "the", "test", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 219, "end_line": 220, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_416", "original_string": "def on_predict_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict begins.\"\"\"", "language": "python", "code": "def on_predict_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the predict begins.\"\"\"", "code_tokens": ["def", "on_predict_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when the predict begins.", "docstring_tokens": ["called", "when", "the", "predict", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 222, "end_line": 223, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_417", "original_string": "def on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when predict ends.\"\"\"", "language": "python", "code": "def on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when predict ends.\"\"\"", "code_tokens": ["def", "on_predict_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when predict ends.", "docstring_tokens": ["called", "when", "predict", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 225, "end_line": 226, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_418", "original_string": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Called when any trainer execution is interrupted by an exception.\"\"\"", "language": "python", "code": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Called when any trainer execution is interrupted by an exception.\"\"\"", "code_tokens": ["def", "on_exception", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "exception", ":", "BaseException", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when any trainer execution is interrupted by an exception.", "docstring_tokens": ["called", "when", "any", "trainer", "execution", "is", "interrupted", "by", "an", "exception"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 228, "end_line": 229, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_419", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate callback's ``state_dict``.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "callback", "s", "state_dict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 231, "end_line": 238, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_420", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "callback", "state", "given", "callback", "s", "state_dict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 240, "end_line": 247, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_421", "original_string": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING"], "docstring": "r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.", "docstring_tokens": ["r", "called", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to", "save"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 249, "end_line": 259, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_422", "original_string": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING"], "docstring": "r\"\"\"Called when loading a model checkpoint, use to reload state.", "docstring_tokens": ["r", "called", "when", "loading", "a", "model", "checkpoint", "use", "to", "reload", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 261, "end_line": 271, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_423", "original_string": "def on_before_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\"\"\"", "language": "python", "code": "def on_before_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\"\"\"", "code_tokens": ["def", "on_before_backward", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "loss", ":", "Tensor", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called before ``loss.backward()``.", "docstring_tokens": ["called", "before", "loss", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 273, "end_line": 274, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_424", "original_string": "def on_after_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\"", "language": "python", "code": "def on_after_backward(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\"", "code_tokens": ["def", "on_after_backward", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called after ``loss.backward()`` and before optimizers are stepped.", "docstring_tokens": ["called", "after", "loss", "backward", "and", "before", "optimizers", "are", "stepped"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 276, "end_line": 277, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_425", "original_string": "def on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"", "language": "python", "code": "def on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"", "code_tokens": ["def", "on_before_optimizer_step", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called before ``optimizer.step()``.", "docstring_tokens": ["called", "before", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 279, "end_line": 282, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "function_426", "original_string": "def on_before_zero_grad(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.zero_grad()``.\"\"\"", "language": "python", "code": "def on_before_zero_grad(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.zero_grad()``.\"\"\"", "code_tokens": ["def", "on_before_zero_grad", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called before ``optimizer.zero_grad()``.", "docstring_tokens": ["called", "before", "optimizer", "zero_grad"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\callback.py", "start_line": 284, "end_line": 285, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "func_name": "function_427", "original_string": "def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)", "language": "python", "code": "def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)", "code_tokens": ["def", "_run_early_stopping_check", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "logs", "=", "trainer", ".", "callback_metrics", "if", "trainer", ".", "fast_dev_run", "or", "not", "self", ".", "_validate_condition_metric", "(", "#", "disable", "early_stopping", "with", "fast_dev_run", "logs", ")", ":", "#", "short", "circuit", "if", "metric", "not", "present", "return", "current", "=", "logs", "[", "self", ".", "monitor", "]", ".", "squeeze", "(", ")", "should_stop", ",", "reason", "=", "self", ".", "_evaluate_stopping_criteria", "(", "current", ")", "should_stop", "=", "trainer", ".", "strategy", ".", "reduce_boolean_decision", "(", "should_stop", ",", "all", "=", "False", ")", "trainer", ".", "should_stop", "=", "trainer", ".", "should_stop", "or", "should_stop", "if", "should_stop", ":", "self", ".", "stopped_epoch", "=", "trainer", ".", "current_epoch", "self", ".", "stopping_reason_message", "=", "reason", "if", "reason", "and", "self", ".", "verbose", ":", "self", ".", "_log_info", "(", "trainer", ",", "reason", ",", "self", ".", "log_rank_zero_only", ")"], "docstring": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training.", "docstring_tokens": ["checks", "whether", "the", "early", "stopping", "condition", "is", "met", "and", "if", "so", "tells", "the", "trainer", "to", "stop", "the", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "start_line": 224, "end_line": 243, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "func_name": "function_428", "original_string": "def _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg", "language": "python", "code": "def _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg", "code_tokens": ["def", "_improvement_message", "(", "self", ",", "current", ":", "Tensor", ")", "-", ">", "str", ":", "STRING", "if", "torch", ".", "isfinite", "(", "self", ".", "best_score", ")", ":", "msg", "=", "(", "fSTRING", "fSTRING", ")", "else", ":", "msg", "=", "fSTRING", "return", "msg"], "docstring": "Formats a log message that informs the user about an improvement in the monitored score.", "docstring_tokens": ["formats", "a", "log", "message", "that", "informs", "the", "user", "about", "an", "improvement", "in", "the", "monitored", "score"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "start_line": 288, "end_line": 297, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_429", "original_string": "def flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]", "language": "python", "code": "def flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]", "code_tokens": ["def", "flatten_modules", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ")", "-", ">", "list", "[", "Module", "]", ":", "STRING", "if", "isinstance", "(", "modules", ",", "ModuleDict", ")", ":", "modules", "=", "modules", ".", "values", "(", ")", "if", "isinstance", "(", "modules", ",", "Iterable", ")", ":", "_flatten_modules", "=", "[", "]", "for", "m", "in", "modules", ":", "_flatten_modules", ".", "extend", "(", "BaseFinetuning", ".", "flatten_modules", "(", "m", ")", ")", "_modules", "=", "iter", "(", "_flatten_modules", ")", "else", ":", "_modules", "=", "modules", ".", "modules", "(", ")", "return", "[", "m", "for", "m", "in", "_modules", "if", "not", "list", "(", "m", ".", "children", "(", ")", ")", "or", "m", ".", "_parameters", "]"], "docstring": "This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules", "docstring_tokens": ["this", "function", "is", "used", "to", "flatten", "a", "module", "or", "an", "iterable", "of", "modules", "into", "a", "list", "of", "its", "leaf", "modules", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 119, "end_line": 143, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_430", "original_string": "def filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param", "language": "python", "code": "def filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param", "code_tokens": ["def", "filter_params", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "train_bn", ":", "bool", "=", "True", ",", "requires_grad", ":", "bool", "=", "True", ")", "-", ">", "Generator", ":", "STRING", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "mod", "in", "modules", ":", "if", "isinstance", "(", "mod", ",", "_BatchNorm", ")", "and", "not", "train_bn", ":", "continue", "for", "param", "in", "mod", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "if", "param", ".", "requires_grad", "=", "=", "requires_grad", ":", "yield", "param"], "docstring": "Yields the `requires_grad` parameters of a given module or list of modules.", "docstring_tokens": ["yields", "the", "requires_grad", "parameters", "of", "a", "given", "module", "or", "list", "of", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 146, "end_line": 166, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_431", "original_string": "def make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True", "language": "python", "code": "def make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True", "code_tokens": ["def", "make_trainable", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ")", "-", ">", "None", ":", "STRING", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "module", "in", "modules", ":", "if", "isinstance", "(", "module", ",", "_BatchNorm", ")", ":", "module", ".", "track_running_stats", "=", "True", "for", "param", "in", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "param", ".", "requires_grad", "=", "True"], "docstring": "Unfreezes the parameters of the provided modules.", "docstring_tokens": ["unfreezes", "the", "parameters", "of", "the", "provided", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 169, "end_line": 182, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_432", "original_string": "def freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False", "language": "python", "code": "def freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False", "code_tokens": ["def", "freeze_module", "(", "module", ":", "Module", ")", "-", ">", "None", ":", "STRING", "if", "isinstance", "(", "module", ",", "_BatchNorm", ")", ":", "module", ".", "track_running_stats", "=", "False", "for", "param", "in", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "param", ".", "requires_grad", "=", "False"], "docstring": "Freezes the parameters of the provided module.", "docstring_tokens": ["freezes", "the", "parameters", "of", "the", "provided", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 185, "end_line": 196, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_433", "original_string": "def freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)", "language": "python", "code": "def freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)", "code_tokens": ["def", "freeze", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "train_bn", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "mod", "in", "modules", ":", "if", "isinstance", "(", "mod", ",", "_BatchNorm", ")", "and", "train_bn", ":", "BaseFinetuning", ".", "make_trainable", "(", "mod", ")", "else", ":", "BaseFinetuning", ".", "freeze_module", "(", "mod", ")"], "docstring": "Freezes the parameters of the provided modules.", "docstring_tokens": ["freezes", "the", "parameters", "of", "the", "provided", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 199, "end_line": 215, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_434", "original_string": "def filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params", "language": "python", "code": "def filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params", "code_tokens": ["def", "filter_on_optimizer", "(", "optimizer", ":", "Optimizer", ",", "params", ":", "Iterable", ")", "-", ">", "list", ":", "STRING", "out_params", "=", "[", "]", "removed_params", "=", "[", "]", "for", "param", "in", "params", ":", "if", "not", "any", "(", "torch", ".", "equal", "(", "p", ",", "param", ")", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "STRING", "]", ")", ":", "out_params", ".", "append", "(", "param", ")", "else", ":", "removed_params", ".", "append", "(", "param", ")", "if", "removed_params", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "fSTRING", ",", ")", "return", "out_params"], "docstring": "This function is used to exclude any parameter which already exists in this optimizer.", "docstring_tokens": ["this", "function", "is", "used", "to", "exclude", "any", "parameter", "which", "already", "exists", "in", "this", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 218, "end_line": 244, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_435", "original_string": "def unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})", "language": "python", "code": "def unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})", "code_tokens": ["def", "unfreeze_and_add_param_group", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "optimizer", ":", "Optimizer", ",", "lr", ":", "Optional", "[", "float", "]", "=", "None", ",", "initial_denom_lr", ":", "float", "=", "10", ".", "0", ",", "train_bn", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "STRING", "BaseFinetuning", ".", "make_trainable", "(", "modules", ")", "params_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "STRING", "]", "if", "lr", "is", "None", "else", "float", "(", "lr", ")", "denom_lr", "=", "initial_denom_lr", "if", "lr", "is", "None", "else", "1", ".", "0", "params", "=", "BaseFinetuning", ".", "filter_params", "(", "modules", ",", "train_bn", "=", "train_bn", ",", "requires_grad", "=", "True", ")", "params", "=", "BaseFinetuning", ".", "filter_on_optimizer", "(", "optimizer", ",", "params", ")", "if", "params", ":", "optimizer", ".", "add_param_group", "(", "{", "STRING", ":", "params", ",", "STRING", ":", "params_lr", "/", "denom_lr", "}", ")"], "docstring": "Unfreezes a module and adds its parameters to an optimizer.", "docstring_tokens": ["unfreezes", "a", "module", "and", "adds", "its", "parameters", "to", "an", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 247, "end_line": 273, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_436", "original_string": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)", "language": "python", "code": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)", "code_tokens": ["def", "on_train_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "for", "opt_idx", ",", "optimizer", "in", "enumerate", "(", "trainer", ".", "optimizers", ")", ":", "num_param_groups", "=", "len", "(", "optimizer", ".", "param_groups", ")", "self", ".", "finetune_function", "(", "pl_module", ",", "trainer", ".", "current_epoch", ",", "optimizer", ")", "current_param_groups", "=", "optimizer", ".", "param_groups", "self", ".", "_store", "(", "pl_module", ",", "opt_idx", ",", "num_param_groups", ",", "current_param_groups", ")"], "docstring": "Called when the epoch begins.", "docstring_tokens": ["called", "when", "the", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 316, "end_line": 322, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_437", "original_string": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override to add your unfreeze logic.\"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override to add your unfreeze logic.\"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "finetune_function", "(", "self", ",", "pl_module", ":", "STRING", ",", "epoch", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError"], "docstring": "Override to add your unfreeze logic.", "docstring_tokens": ["override", "to", "add", "your", "unfreeze", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 324, "end_line": 326, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_438", "original_string": "def freeze_before_training(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Override to add your freeze logic.\"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def freeze_before_training(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Override to add your freeze logic.\"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "freeze_before_training", "(", "self", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError"], "docstring": "Override to add your freeze logic.", "docstring_tokens": ["override", "to", "add", "your", "freeze", "logic"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 328, "end_line": 330, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_439", "original_string": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")", "language": "python", "code": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")", "code_tokens": ["def", "on_fit_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "hasattr", "(", "pl_module", ",", "STRING", ")", "and", "isinstance", "(", "pl_module", ".", "backbone", ",", "Module", ")", ":", "return", "super", "(", ")", ".", "on_fit_start", "(", "trainer", ",", "pl_module", ")", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "Raises:", "docstring_tokens": ["raises"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 402, "end_line": 410, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "function_440", "original_string": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )", "language": "python", "code": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )", "code_tokens": ["def", "finetune_function", "(", "self", ",", "pl_module", ":", "STRING", ",", "epoch", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING", "if", "epoch", "=", "=", "self", ".", "unfreeze_backbone_at_epoch", ":", "current_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "STRING", "]", "initial_backbone_lr", "=", "(", "self", ".", "backbone_initial_lr", "if", "self", ".", "backbone_initial_lr", "is", "not", "None", "else", "current_lr", "*", "self", ".", "backbone_initial_ratio_lr", ")", "self", ".", "previous_backbone_lr", "=", "initial_backbone_lr", "self", ".", "unfreeze_and_add_param_group", "(", "pl_module", ".", "backbone", ",", "optimizer", ",", "initial_backbone_lr", ",", "train_bn", "=", "self", ".", "train_bn", ",", "initial_denom_lr", "=", "self", ".", "initial_denom_lr", ",", ")", "if", "self", ".", "verbose", ":", "log", ".", "info", "(", "fSTRING", "fSTRING", ")", "elif", "epoch", ">", "self", ".", "unfreeze_backbone_at_epoch", ":", "current_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "STRING", "]", "next_current_backbone_lr", "=", "self", ".", "lambda_func", "(", "epoch", "+", "1", ")", "*", "self", ".", "previous_backbone_lr", "next_current_backbone_lr", "=", "(", "current_lr", "if", "(", "self", ".", "should_align", "and", "next_current_backbone_lr", ">", "current_lr", ")", "else", "next_current_backbone_lr", ")", "optimizer", ".", "param_groups", "[", "-", "1", "]", "[", "STRING", "]", "=", "next_current_backbone_lr", "self", ".", "previous_backbone_lr", "=", "next_current_backbone_lr", "if", "self", ".", "verbose", ":", "log", ".", "info", "(", "fSTRING", "fSTRING", ")"], "docstring": "Called when the epoch begins.", "docstring_tokens": ["called", "when", "the", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "start_line": 417, "end_line": 454, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py", "func_name": "function_441", "original_string": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )", "language": "python", "code": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )", "code_tokens": ["def", "on_train_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "not", "pl_module", ".", "automatic_optimization", ":", "raise", "RuntimeError", "(", "STRING", ")", "overridden_optimizer_step", "=", "is_overridden", "(", "STRING", ",", "pl_module", ")", "overridden_optimizer_zero_grad", "=", "is_overridden", "(", "STRING", ",", "pl_module", ")", "going_to_accumulate_grad_batches", "=", "self", ".", "going_to_accumulate_grad_batches", "(", ")", "has_overridden_optimization_functions", "=", "overridden_optimizer_step", "or", "overridden_optimizer_zero_grad", "if", "has_overridden_optimization_functions", "and", "going_to_accumulate_grad_batches", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", ")", "from", "lightning", ".", "pytorch", ".", "strategies", "import", "DeepSpeedStrategy", "if", "isinstance", "(", "trainer", ".", "strategy", ",", "DeepSpeedStrategy", ")", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", ")", "if", "trainer", ".", "accumulate_grad_batches", "!", "=", "1", ":", "raise", "ValueError", "(", "STRING", "STRING", ")"], "docstring": "Performns a configuration validation before training starts and raises errors for incompatible settings.", "docstring_tokens": ["performns", "a", "configuration", "validation", "before", "training", "starts", "and", "raises", "errors", "for", "incompatible", "settings"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py", "start_line": 103, "end_line": 135, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "function_442", "original_string": "def on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}", "language": "python", "code": "def on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}", "code_tokens": ["def", "on_train_start", "(", "self", ",", "trainer", ":", "STRING", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "not", "trainer", ".", "loggers", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "self", ".", "log_momentum", ":", "def", "_check_no_key", "(", "key", ":", "str", ")", "-", ">", "bool", ":", "if", "trainer", ".", "lr_scheduler_configs", ":", "return", "any", "(", "key", "not", "in", "config", ".", "scheduler", ".", "optimizer", ".", "defaults", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ")", "return", "any", "(", "key", "not", "in", "optimizer", ".", "defaults", "for", "optimizer", "in", "trainer", ".", "optimizers", ")", "if", "_check_no_key", "(", "STRING", ")", "and", "_check_no_key", "(", "STRING", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", ",", "category", "=", "RuntimeWarning", ",", ")", "names", ":", "list", "[", "list", "[", "str", "]", "]", "=", "[", "]", "(", "sched_hparam_keys", ",", "optimizers_with_scheduler", ",", "optimizers_with_scheduler_types", ",", ")", "=", "self", ".", "_find_names_from_schedulers", "(", "trainer", ".", "lr_scheduler_configs", ")", "names", ".", "extend", "(", "sched_hparam_keys", ")", "optimizer_hparam_keys", ",", "_", "=", "self", ".", "_find_names_from_optimizers", "(", "trainer", ".", "optimizers", ",", "seen_optimizers", "=", "optimizers_with_scheduler", ",", "seen_optimizer_types", "=", "optimizers_with_scheduler_types", ",", ")", "names", ".", "extend", "(", "optimizer_hparam_keys", ")", "names_flatten", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "names", ")", ")", "self", ".", "lrs", "=", "{", "name", ":", "[", "]", "for", "name", "in", "names_flatten", "}", "self", ".", "last_momentum_values", "=", "{", "name", "+", "STRING", ":", "None", "for", "name", "in", "names_flatten", "}", "self", ".", "last_weight_decay_values", "=", "{", "name", "+", "STRING", ":", "None", "for", "name", "in", "names_flatten", "}"], "docstring": "Called before training, determines unique names for all lr schedulers in the case of multiple of the same", "docstring_tokens": ["called", "before", "training", "determines", "unique", "names", "for", "all", "lr", "schedulers", "in", "the", "case", "of", "multiple", "of", "the", "same"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "start_line": 111, "end_line": 163, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "function_443", "original_string": "def _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []", "language": "python", "code": "def _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []", "code_tokens": ["def", "_remap_keys", "(", "self", ",", "names", ":", "list", "[", "list", "[", "str", "]", "]", ",", "token", ":", "str", "=", "STRING", ")", "-", ">", "None", ":", "STRING", "for", "group_new_names", "in", "names", ":", "for", "new_name", "in", "group_new_names", ":", "old_name", "=", "new_name", ".", "replace", "(", "token", ",", "STRING", ")", "if", "token", "in", "new_name", "and", "old_name", "in", "self", ".", "lrs", ":", "self", ".", "lrs", "[", "new_name", "]", "=", "self", ".", "lrs", ".", "pop", "(", "old_name", ")", "elif", "new_name", "not", "in", "self", ".", "lrs", ":", "self", ".", "lrs", "[", "new_name", "]", "=", "[", "]"], "docstring": "This function is used the remap the keys if param groups for a given optimizer increased.", "docstring_tokens": ["this", "function", "is", "used", "the", "remap", "the", "keys", "if", "param", "groups", "for", "a", "given", "optimizer", "increased"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "start_line": 243, "end_line": 251, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "function_444", "original_string": "def _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}", "language": "python", "code": "def _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}", "code_tokens": ["def", "_extract_weight_decay", "(", "self", ",", "param_group", ":", "dict", "[", "str", ",", "Any", "]", ",", "name", ":", "str", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "self", ".", "log_weight_decay", ":", "return", "{", "}", "weight_decay", "=", "param_group", "[", "STRING", "]", "self", ".", "last_weight_decay_values", "[", "name", "]", "=", "weight_decay", "return", "{", "name", ":", "weight_decay", "}"], "docstring": "Extracts the weight decay statistics from a parameter group.", "docstring_tokens": ["extracts", "the", "weight", "decay", "statistics", "from", "a", "parameter", "group"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "start_line": 261, "end_line": 268, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_445", "original_string": "def on_train_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n    ) -> None:\r\n        \"\"\"Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\"\"\"\r\n        skip_due_to_state = self._should_skip_saving_checkpoint(trainer)\r\n        skip_batch = self._every_n_train_steps < 1 or (trainer.global_step % self._every_n_train_steps != 0)\r\n\r\n        train_time_interval = self._train_time_interval\r\n        skip_time = True\r\n        now = time.monotonic()\r\n        if train_time_interval is not None:\r\n            prev_time_check = self._last_time_checked\r\n            skip_time = prev_time_check is None or (now - prev_time_check) < train_time_interval.total_seconds()\r\n            skip_time = trainer.strategy.broadcast(skip_time)\r\n\r\n        if skip_batch and skip_time:\r\n            return\r\n        if not skip_time:\r\n            self._last_time_checked = now\r\n\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        if self.monitor is not None and self.monitor not in monitor_candidates:\r\n            self._defer_save_until_validation = True\r\n            return\r\n\r\n        if (\r\n            self.monitor is not None\r\n            and not self._should_save_on_train_epoch_end(trainer)\r\n            and getattr(trainer.fit_loop.epoch_loop.batch_progress, \"is_last_batch\", False)\r\n        ):\r\n            will_run_val = False\r\n            if getattr(trainer, \"enable_validation\", False):\r\n                num_val_batches = (\r\n                    sum(trainer.num_val_batches)\r\n                    if isinstance(trainer.num_val_batches, list)\r\n                    else trainer.num_val_batches\r\n                )\r\n                if num_val_batches and num_val_batches > 0:\r\n                    cve = trainer.check_val_every_n_epoch\r\n                    if cve is None or ((trainer.current_epoch + 1) % cve == 0):\r\n                        will_run_val = True\r\n\r\n            if will_run_val:\r\n                self._defer_save_until_validation = True\r\n                return\r\n\r\n        if skip_due_to_state:\r\n            return\r\n\r\n        self._save_topk_checkpoint(trainer, monitor_candidates)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_train_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n    ) -> None:\r\n        \"\"\"Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\"\"\"\r\n        skip_due_to_state = self._should_skip_saving_checkpoint(trainer)\r\n        skip_batch = self._every_n_train_steps < 1 or (trainer.global_step % self._every_n_train_steps != 0)\r\n\r\n        train_time_interval = self._train_time_interval\r\n        skip_time = True\r\n        now = time.monotonic()\r\n        if train_time_interval is not None:\r\n            prev_time_check = self._last_time_checked\r\n            skip_time = prev_time_check is None or (now - prev_time_check) < train_time_interval.total_seconds()\r\n            skip_time = trainer.strategy.broadcast(skip_time)\r\n\r\n        if skip_batch and skip_time:\r\n            return\r\n        if not skip_time:\r\n            self._last_time_checked = now\r\n\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        if self.monitor is not None and self.monitor not in monitor_candidates:\r\n            self._defer_save_until_validation = True\r\n            return\r\n\r\n        if (\r\n            self.monitor is not None\r\n            and not self._should_save_on_train_epoch_end(trainer)\r\n            and getattr(trainer.fit_loop.epoch_loop.batch_progress, \"is_last_batch\", False)\r\n        ):\r\n            will_run_val = False\r\n            if getattr(trainer, \"enable_validation\", False):\r\n                num_val_batches = (\r\n                    sum(trainer.num_val_batches)\r\n                    if isinstance(trainer.num_val_batches, list)\r\n                    else trainer.num_val_batches\r\n                )\r\n                if num_val_batches and num_val_batches > 0:\r\n                    cve = trainer.check_val_every_n_epoch\r\n                    if cve is None or ((trainer.current_epoch + 1) % cve == 0):\r\n                        will_run_val = True\r\n\r\n            if will_run_val:\r\n                self._defer_save_until_validation = True\r\n                return\r\n\r\n        if skip_due_to_state:\r\n            return\r\n\r\n        self._save_topk_checkpoint(trainer, monitor_candidates)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", ")", "-", ">", "None", ":", "STRING", "skip_due_to_state", "=", "self", ".", "_should_skip_saving_checkpoint", "(", "trainer", ")", "skip_batch", "=", "self", ".", "_every_n_train_steps", "<", "1", "or", "(", "trainer", ".", "global_step", "%", "self", ".", "_every_n_train_steps", "!", "=", "0", ")", "train_time_interval", "=", "self", ".", "_train_time_interval", "skip_time", "=", "True", "now", "=", "time", ".", "monotonic", "(", ")", "if", "train_time_interval", "is", "not", "None", ":", "prev_time_check", "=", "self", ".", "_last_time_checked", "skip_time", "=", "prev_time_check", "is", "None", "or", "(", "now", "-", "prev_time_check", ")", "<", "train_time_interval", ".", "total_seconds", "(", ")", "skip_time", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "skip_time", ")", "if", "skip_batch", "and", "skip_time", ":", "return", "if", "not", "skip_time", ":", "self", ".", "_last_time_checked", "=", "now", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "if", "self", ".", "monitor", "is", "not", "None", "and", "self", ".", "monitor", "not", "in", "monitor_candidates", ":", "self", ".", "_defer_save_until_validation", "=", "True", "return", "if", "(", "self", ".", "monitor", "is", "not", "None", "and", "not", "self", ".", "_should_save_on_train_epoch_end", "(", "trainer", ")", "and", "getattr", "(", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "batch_progress", ",", "STRING", ",", "False", ")", ")", ":", "will_run_val", "=", "False", "if", "getattr", "(", "trainer", ",", "STRING", ",", "False", ")", ":", "num_val_batches", "=", "(", "sum", "(", "trainer", ".", "num_val_batches", ")", "if", "isinstance", "(", "trainer", ".", "num_val_batches", ",", "list", ")", "else", "trainer", ".", "num_val_batches", ")", "if", "num_val_batches", "and", "num_val_batches", ">", "0", ":", "cve", "=", "trainer", ".", "check_val_every_n_epoch", "if", "cve", "is", "None", "or", "(", "(", "trainer", ".", "current_epoch", "+", "1", ")", "%", "cve", "=", "=", "0", ")", ":", "will_run_val", "=", "True", "if", "will_run_val", ":", "self", ".", "_defer_save_until_validation", "=", "True", "return", "if", "skip_due_to_state", ":", "return", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`", "docstring_tokens": ["save", "checkpoint", "on", "train", "batch", "end", "if", "we", "meet", "the", "criteria", "for", "every_n_train_steps"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 304, "end_line": 373, "has_examples": false, "num_comments": 8, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_446", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_should_skip_saving_checkpoint", "(", "trainer", ")", "and", "self", ".", "_should_save_on_train_epoch_end", "(", "trainer", ")", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "if", "self", ".", "_every_n_epochs", ">", "=", "1", "and", "(", "trainer", ".", "current_epoch", "+", "1", ")", "%", "self", ".", "_every_n_epochs", "=", "=", "0", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Save a checkpoint at the end of the training epoch.", "docstring_tokens": ["save", "a", "checkpoint", "at", "the", "end", "of", "the", "training", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 376, "end_line": 382, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_447", "original_string": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_validation_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_should_skip_saving_checkpoint", "(", "trainer", ")", "and", "not", "self", ".", "_should_save_on_train_epoch_end", "(", "trainer", ")", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "if", "self", ".", "_defer_save_until_validation", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_defer_save_until_validation", "=", "False", "return", "if", "self", ".", "_every_n_epochs", ">", "=", "1", "and", "(", "trainer", ".", "current_epoch", "+", "1", ")", "%", "self", ".", "_every_n_epochs", "=", "=", "0", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Save a checkpoint at the end of the validation stage.", "docstring_tokens": ["save", "a", "checkpoint", "at", "the", "end", "of", "the", "validation", "stage"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 385, "end_line": 399, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_448", "original_string": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )", "language": "python", "code": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )", "code_tokens": ["def", "on_exception", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "exception", ":", "BaseException", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_should_save_on_exception", "(", "trainer", ")", ":", "return", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "filepath", "=", "self", ".", "format_checkpoint_name", "(", "metrics", "=", "monitor_candidates", ")", "self", ".", "_save_checkpoint", "(", "trainer", ",", "filepath", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "rank_zero_info", "(", "f", "\"", "An", "{", "type", "(", "exception", ")", ".", "__name__", "}", "was", "raised", "with", "message", ":", "\\", "{", "str", "(", "exception", ")", "}", ",", "saved", "checkpoint", "to", "{", "filepath", "}", "\"", ")"], "docstring": "Save a checkpoint when an exception is raised.", "docstring_tokens": ["save", "a", "checkpoint", "when", "an", "exception", "is", "raised"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 402, "end_line": 413, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_449", "original_string": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_train_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "save_last", "and", "not", "self", ".", "_last_checkpoint_saved", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Ensure save_last=True is applied when training ends.", "docstring_tokens": ["ensure", "save_last", "true", "is", "applied", "when", "training", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 415, "end_line": 419, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_450", "original_string": "def format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name", "language": "python", "code": "def format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name", "code_tokens": ["def", "format_checkpoint_name", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "Tensor", "]", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "ver", ":", "Optional", "[", "int", "]", "=", "None", ",", "prefix", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "str", ":", "STRING", "filename", "=", "filename", "or", "self", ".", "filename", "filename", "=", "self", ".", "_format_checkpoint_name", "(", "filename", ",", "metrics", ",", "prefix", "=", "prefix", ",", "auto_insert_metric_name", "=", "self", ".", "auto_insert_metric_name", ")", "if", "ver", "is", "not", "None", ":", "filename", "=", "self", ".", "CHECKPOINT_JOIN_CHAR", ".", "join", "(", "(", "filename", ",", "fSTRING", ")", ")", "ckpt_name", "=", "fSTRING", "return", "os", ".", "path", ".", "join", "(", "self", ".", "dirpath", ",", "ckpt_name", ")", "if", "self", ".", "dirpath", "else", "ckpt_name"], "docstring": "Generate a filename according to the defined template.", "docstring_tokens": ["generate", "a", "filename", "according", "to", "the", "defined", "template"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 665, "end_line": 710, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_451", "original_string": "def __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path", "language": "python", "code": "def __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path", "code_tokens": ["def", "__resolve_ckpt_dir", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "_PATH", ":", "STRING", "if", "self", ".", "dirpath", "is", "not", "None", ":", "return", "self", ".", "dirpath", "if", "len", "(", "trainer", ".", "loggers", ")", ">", "0", ":", "if", "trainer", ".", "loggers", "[", "0", "]", ".", "save_dir", "is", "not", "None", ":", "save_dir", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "save_dir", "else", ":", "save_dir", "=", "trainer", ".", "default_root_dir", "name", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "name", "version", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "version", "version", "=", "version", "if", "isinstance", "(", "version", ",", "str", ")", "else", "fSTRING", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "str", "(", "name", ")", ",", "version", ",", "STRING", ")", "else", ":", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "STRING", ")", "return", "ckpt_path"], "docstring": "Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to", "docstring_tokens": ["determines", "model", "checkpoint", "save", "directory", "at", "runtime", "reference", "attributes", "from", "the", "trainer", "s", "logger", "to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 712, "end_line": 740, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_452", "original_string": "def to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)", "language": "python", "code": "def to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)", "code_tokens": ["def", "to_yaml", "(", "self", ",", "filepath", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "best_k", "=", "{", "k", ":", "v", ".", "item", "(", ")", "for", "k", ",", "v", "in", "self", ".", "best_k_models", ".", "items", "(", ")", "}", "if", "filepath", "is", "None", ":", "assert", "self", ".", "dirpath", "filepath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dirpath", ",", "STRING", ")", "with", "self", ".", "_fs", ".", "open", "(", "filepath", ",", "STRING", ")", "as", "fp", ":", "yaml", ".", "dump", "(", "best_k", ",", "fp", ")"], "docstring": "Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML", "docstring_tokens": ["saves", "the", "best_k_models", "dict", "containing", "the", "checkpoint", "paths", "with", "the", "corresponding", "scores", "to", "a", "yaml"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 864, "end_line": 872, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_453", "original_string": "def file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)", "language": "python", "code": "def file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)", "code_tokens": ["def", "file_exists", "(", "self", ",", "filepath", ":", "_PATH", ",", "trainer", ":", "STRING", ")", "-", ">", "bool", ":", "STRING", "exists", "=", "self", ".", "_fs", ".", "exists", "(", "filepath", ")", "return", "trainer", ".", "strategy", ".", "broadcast", "(", "exists", ")"], "docstring": "Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal", "docstring_tokens": ["checks", "if", "a", "file", "exists", "on", "rank", "0", "and", "broadcasts", "the", "result", "to", "all", "other", "ranks", "preventing", "the", "internal"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 874, "end_line": 878, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_454", "original_string": "def _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents", "language": "python", "code": "def _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents", "code_tokens": ["def", "_should_remove_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "previous", ":", "str", ",", "current", ":", "str", ")", "-", ">", "bool", ":", "STRING", "if", "previous", "=", "=", "current", ":", "return", "False", "if", "not", "_is_local_file_protocol", "(", "previous", ")", ":", "return", "True", "previous", "=", "Path", "(", "previous", ")", ".", "absolute", "(", ")", "resume_path", "=", "Path", "(", "trainer", ".", "ckpt_path", ")", ".", "absolute", "(", ")", "if", "trainer", ".", "ckpt_path", "is", "not", "None", "else", "None", "if", "resume_path", "is", "not", "None", "and", "previous", "=", "=", "resume_path", ":", "return", "False", "assert", "self", ".", "dirpath", "is", "not", "None", "dirpath", "=", "Path", "(", "self", ".", "dirpath", ")", ".", "absolute", "(", ")", "return", "dirpath", "in", "previous", ".", "parents"], "docstring": "Checks if the previous checkpoint should be deleted.", "docstring_tokens": ["checks", "if", "the", "previous", "checkpoint", "should", "be", "deleted"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 880, "end_line": 899, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "function_455", "original_string": "def _remove_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\r\n        \"\"\"Calls the strategy to remove the checkpoint file.\"\"\"\r\n        trainer.strategy.remove_checkpoint(filepath)", "language": "python", "code": "def _remove_checkpoint(self, trainer: \"pl.Trainer\", filepath: str) -> None:\r\n        \"\"\"Calls the strategy to remove the checkpoint file.\"\"\"\r\n        trainer.strategy.remove_checkpoint(filepath)", "code_tokens": ["def", "_remove_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "filepath", ":", "str", ")", "-", ">", "None", ":", "STRING", "trainer", ".", "strategy", ".", "remove_checkpoint", "(", "filepath", ")"], "docstring": "Calls the strategy to remove the checkpoint file.", "docstring_tokens": ["calls", "the", "strategy", "to", "remove", "the", "checkpoint", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "start_line": 901, "end_line": 903, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "func_name": "function_456", "original_string": "def write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()", "language": "python", "code": "def write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()", "code_tokens": ["def", "write_on_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "prediction", ":", "Any", ",", "batch_indices", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", ",", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError", "(", ")"], "docstring": "Override with the logic to write a single batch.", "docstring_tokens": ["override", "with", "the", "logic", "to", "write", "a", "single", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "start_line": 119, "end_line": 130, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "func_name": "function_457", "original_string": "def write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()", "language": "python", "code": "def write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()", "code_tokens": ["def", "write_on_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "predictions", ":", "Sequence", "[", "Any", "]", ",", "batch_indices", ":", "Sequence", "[", "Any", "]", ",", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError", "(", ")"], "docstring": "Override with the logic to write all batches.", "docstring_tokens": ["override", "with", "the", "logic", "to", "write", "all", "batches"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "start_line": 132, "end_line": 140, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_458", "original_string": "def __init__(\r\n        self,\r\n        pruning_fn: Union[Callable, str],\r\n        parameters_to_prune: _PARAM_LIST = (),\r\n        parameter_names: Optional[list[str]] = None,\r\n        use_global_unstructured: bool = True,\r\n        amount: Union[int, float, Callable[[int], Union[int, float]]] = 0.5,\r\n        apply_pruning: Union[bool, Callable[[int], bool]] = True,\r\n        make_pruning_permanent: bool = True,\r\n        use_lottery_ticket_hypothesis: Union[bool, Callable[[int], bool]] = True,\r\n        resample_parameters: bool = False,\r\n        pruning_dim: Optional[int] = None,\r\n        pruning_norm: Optional[int] = None,\r\n        verbose: int = 0,\r\n        prune_on_train_epoch_end: bool = True,\r\n    ) -> None:\r\n        \"\"\"Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\r\n        parameters during training.\r\n\r\n        To learn more about pruning with PyTorch, please take a look at\r\n        `this tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. code-block:: python\r\n\r\n            parameters_to_prune = [(model.mlp_1, \"weight\"), (model.mlp_2, \"weight\")]\r\n\r\n            trainer = Trainer(\r\n                callbacks=[\r\n                    ModelPruning(\r\n                        pruning_fn=\"l1_unstructured\",\r\n                        parameters_to_prune=parameters_to_prune,\r\n                        amount=0.01,\r\n                        use_global_unstructured=True,\r\n                    )\r\n                ]\r\n            )\r\n\r\n        When ``parameters_to_prune`` is ``None``, ``parameters_to_prune`` will contain all parameters from the model.\r\n        The user can override ``filter_parameters_to_prune`` to filter any ``nn.Module`` to be pruned.\r\n\r\n        Args:\r\n\r\n            pruning_fn: Function from torch.nn.utils.prune module or your own PyTorch ``BasePruningMethod`` subclass.\r\n                Can also be string e.g. `\"l1_unstructured\"`. See pytorch docs for more details.\r\n\r\n            parameters_to_prune: List of tuples ``(nn.Module, \"parameter_name_string\")``.\r\n\r\n            parameter_names: List of parameter names to be pruned from the nn.Module.\r\n                Can either be ``\"weight\"`` or ``\"bias\"``.\r\n\r\n            use_global_unstructured: Whether to apply pruning globally on the model.\r\n                If ``parameters_to_prune`` is provided, global unstructured will be restricted on them.\r\n\r\n            amount: Quantity of parameters to prune:\r\n\r\n                - ``float``. Between 0.0 and 1.0. Represents the fraction of parameters to prune.\r\n                - ``int``. Represents the absolute number of parameters to prune.\r\n                - ``Callable``. For dynamic values. Will be called every epoch. Should return a value.\r\n\r\n            apply_pruning: Whether to apply pruning.\r\n\r\n                - ``bool``. Always apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            make_pruning_permanent: Whether to remove all reparameterization pre-hooks and apply masks\r\n                when training ends or the model is saved.\r\n\r\n            use_lottery_ticket_hypothesis: See `The lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`_:\r\n\r\n                - ``bool``. Whether to apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            resample_parameters: Used with ``use_lottery_ticket_hypothesis``. If True, the model parameters will\r\n                be resampled, otherwise, the exact original parameters will be used.\r\n\r\n            pruning_dim: If you are using a structured pruning method you need to specify the dimension.\r\n\r\n            pruning_norm: If you are using ``ln_structured`` you need to specify the norm.\r\n\r\n            verbose: Verbosity level. 0 to disable, 1 to log overall sparsity, 2 to log per-layer sparsity\r\n\r\n            prune_on_train_epoch_end: whether to apply pruning at the end of the training epoch.\r\n                If this is ``False``, then the check runs at the end of the validation epoch.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameter_names`` is neither ``\"weight\"`` nor ``\"bias\"``,\r\n                if the provided ``pruning_fn`` is not supported,\r\n                if ``pruning_dim`` is not provided when ``\"unstructured\"``,\r\n                if ``pruning_norm`` is not provided when ``\"ln_structured\"``,\r\n                if ``pruning_fn`` is neither ``str`` nor :class:`torch.nn.utils.prune.BasePruningMethod`, or\r\n                if ``amount`` is none of ``int``, ``float`` and ``Callable``.\r\n\r\n        \"\"\"\r\n\r\n        self._use_global_unstructured = use_global_unstructured\r\n        self._parameters_to_prune = parameters_to_prune\r\n        self._use_lottery_ticket_hypothesis = use_lottery_ticket_hypothesis\r\n        self._resample_parameters = resample_parameters\r\n        self._prune_on_train_epoch_end = prune_on_train_epoch_end\r\n        self._parameter_names = parameter_names or self.PARAMETER_NAMES\r\n        self._global_kwargs: dict[str, Any] = {}\r\n        self._original_layers: Optional[dict[int, _LayerRef]] = None\r\n        self._pruning_method_name: Optional[str] = None\r\n\r\n        for name in self._parameter_names:\r\n            if name not in self.PARAMETER_NAMES:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `parameter_names` name: {name} isn't in {self.PARAMETER_NAMES}\"\r\n                )\r\n\r\n        if isinstance(pruning_fn, str):\r\n            pruning_kwargs = {}\r\n            pruning_fn = pruning_fn.lower()\r\n            if pruning_fn not in _PYTORCH_PRUNING_FUNCTIONS:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `pruning_fn` {pruning_fn} isn't available in PyTorch's\"\r\n                    f\" built-in functions: {list(_PYTORCH_PRUNING_FUNCTIONS.keys())} \"\r\n                )\r\n            if pruning_fn.endswith(\"_structured\"):\r\n                if pruning_dim is None:\r\n                    raise MisconfigurationException(\r\n                        \"When requesting `structured` pruning, the `pruning_dim` should be provided.\"\r\n                    )\r\n                if pruning_fn == \"ln_structured\":\r\n                    if pruning_norm is None:\r\n                        raise MisconfigurationException(\r\n                            \"When requesting `ln_structured` pruning, the `pruning_norm` should be provided.\"\r\n                        )\r\n                    pruning_kwargs[\"n\"] = pruning_norm\r\n                pruning_kwargs[\"dim\"] = pruning_dim\r\n            pruning_fn = self._create_pruning_fn(pruning_fn, **pruning_kwargs)\r\n        elif self._is_pruning_method(pruning_fn):\r\n            if not use_global_unstructured:\r\n                raise MisconfigurationException(\r\n                    \"PyTorch `BasePruningMethod` is currently only supported with `use_global_unstructured=True`.\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                f\"`pruning_fn` is expected to be a str in {list(_PYTORCH_PRUNING_FUNCTIONS.keys())}\"\r\n                f\" or a PyTorch `BasePruningMethod`. Found: {pruning_fn}.\"\r\n                \" HINT: if passing a `BasePruningMethod`, pass the class, not an instance\"\r\n            )\r\n\r\n        if use_global_unstructured and pruning_fn.PRUNING_TYPE != \"unstructured\":  # type: ignore\r\n            raise MisconfigurationException(\r\n                'Only the \"unstructured\" PRUNING_TYPE is supported with `use_global_unstructured=True`.'\r\n                f\" Found method {pruning_fn} of type {pruning_fn.PRUNING_TYPE}. \"  # type: ignore[union-attr]\r\n            )\r\n\r\n        self.pruning_fn = pruning_fn\r\n        self._apply_pruning = apply_pruning\r\n        self._make_pruning_permanent = make_pruning_permanent\r\n\r\n        if not (isinstance(amount, (int, float)) or callable(amount)):\r\n            raise MisconfigurationException(\r\n                \"`amount` should be provided and be either an int, a float or Callable function.\"\r\n            )\r\n\r\n        self.amount = amount\r\n\r\n        if verbose not in (0, 1, 2):\r\n            raise MisconfigurationException(\"`verbose` must be any of (0, 1, 2)\")\r\n\r\n        self._verbose = verbose", "language": "python", "code": "def __init__(\r\n        self,\r\n        pruning_fn: Union[Callable, str],\r\n        parameters_to_prune: _PARAM_LIST = (),\r\n        parameter_names: Optional[list[str]] = None,\r\n        use_global_unstructured: bool = True,\r\n        amount: Union[int, float, Callable[[int], Union[int, float]]] = 0.5,\r\n        apply_pruning: Union[bool, Callable[[int], bool]] = True,\r\n        make_pruning_permanent: bool = True,\r\n        use_lottery_ticket_hypothesis: Union[bool, Callable[[int], bool]] = True,\r\n        resample_parameters: bool = False,\r\n        pruning_dim: Optional[int] = None,\r\n        pruning_norm: Optional[int] = None,\r\n        verbose: int = 0,\r\n        prune_on_train_epoch_end: bool = True,\r\n    ) -> None:\r\n        \"\"\"Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks\r\n        parameters during training.\r\n\r\n        To learn more about pruning with PyTorch, please take a look at\r\n        `this tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. code-block:: python\r\n\r\n            parameters_to_prune = [(model.mlp_1, \"weight\"), (model.mlp_2, \"weight\")]\r\n\r\n            trainer = Trainer(\r\n                callbacks=[\r\n                    ModelPruning(\r\n                        pruning_fn=\"l1_unstructured\",\r\n                        parameters_to_prune=parameters_to_prune,\r\n                        amount=0.01,\r\n                        use_global_unstructured=True,\r\n                    )\r\n                ]\r\n            )\r\n\r\n        When ``parameters_to_prune`` is ``None``, ``parameters_to_prune`` will contain all parameters from the model.\r\n        The user can override ``filter_parameters_to_prune`` to filter any ``nn.Module`` to be pruned.\r\n\r\n        Args:\r\n\r\n            pruning_fn: Function from torch.nn.utils.prune module or your own PyTorch ``BasePruningMethod`` subclass.\r\n                Can also be string e.g. `\"l1_unstructured\"`. See pytorch docs for more details.\r\n\r\n            parameters_to_prune: List of tuples ``(nn.Module, \"parameter_name_string\")``.\r\n\r\n            parameter_names: List of parameter names to be pruned from the nn.Module.\r\n                Can either be ``\"weight\"`` or ``\"bias\"``.\r\n\r\n            use_global_unstructured: Whether to apply pruning globally on the model.\r\n                If ``parameters_to_prune`` is provided, global unstructured will be restricted on them.\r\n\r\n            amount: Quantity of parameters to prune:\r\n\r\n                - ``float``. Between 0.0 and 1.0. Represents the fraction of parameters to prune.\r\n                - ``int``. Represents the absolute number of parameters to prune.\r\n                - ``Callable``. For dynamic values. Will be called every epoch. Should return a value.\r\n\r\n            apply_pruning: Whether to apply pruning.\r\n\r\n                - ``bool``. Always apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            make_pruning_permanent: Whether to remove all reparameterization pre-hooks and apply masks\r\n                when training ends or the model is saved.\r\n\r\n            use_lottery_ticket_hypothesis: See `The lottery ticket hypothesis <https://arxiv.org/abs/1803.03635>`_:\r\n\r\n                - ``bool``. Whether to apply it or not.\r\n                - ``Callable[[epoch], bool]``. For dynamic values. Will be called every epoch.\r\n\r\n            resample_parameters: Used with ``use_lottery_ticket_hypothesis``. If True, the model parameters will\r\n                be resampled, otherwise, the exact original parameters will be used.\r\n\r\n            pruning_dim: If you are using a structured pruning method you need to specify the dimension.\r\n\r\n            pruning_norm: If you are using ``ln_structured`` you need to specify the norm.\r\n\r\n            verbose: Verbosity level. 0 to disable, 1 to log overall sparsity, 2 to log per-layer sparsity\r\n\r\n            prune_on_train_epoch_end: whether to apply pruning at the end of the training epoch.\r\n                If this is ``False``, then the check runs at the end of the validation epoch.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameter_names`` is neither ``\"weight\"`` nor ``\"bias\"``,\r\n                if the provided ``pruning_fn`` is not supported,\r\n                if ``pruning_dim`` is not provided when ``\"unstructured\"``,\r\n                if ``pruning_norm`` is not provided when ``\"ln_structured\"``,\r\n                if ``pruning_fn`` is neither ``str`` nor :class:`torch.nn.utils.prune.BasePruningMethod`, or\r\n                if ``amount`` is none of ``int``, ``float`` and ``Callable``.\r\n\r\n        \"\"\"\r\n\r\n        self._use_global_unstructured = use_global_unstructured\r\n        self._parameters_to_prune = parameters_to_prune\r\n        self._use_lottery_ticket_hypothesis = use_lottery_ticket_hypothesis\r\n        self._resample_parameters = resample_parameters\r\n        self._prune_on_train_epoch_end = prune_on_train_epoch_end\r\n        self._parameter_names = parameter_names or self.PARAMETER_NAMES\r\n        self._global_kwargs: dict[str, Any] = {}\r\n        self._original_layers: Optional[dict[int, _LayerRef]] = None\r\n        self._pruning_method_name: Optional[str] = None\r\n\r\n        for name in self._parameter_names:\r\n            if name not in self.PARAMETER_NAMES:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `parameter_names` name: {name} isn't in {self.PARAMETER_NAMES}\"\r\n                )\r\n\r\n        if isinstance(pruning_fn, str):\r\n            pruning_kwargs = {}\r\n            pruning_fn = pruning_fn.lower()\r\n            if pruning_fn not in _PYTORCH_PRUNING_FUNCTIONS:\r\n                raise MisconfigurationException(\r\n                    f\"The provided `pruning_fn` {pruning_fn} isn't available in PyTorch's\"\r\n                    f\" built-in functions: {list(_PYTORCH_PRUNING_FUNCTIONS.keys())} \"\r\n                )\r\n            if pruning_fn.endswith(\"_structured\"):\r\n                if pruning_dim is None:\r\n                    raise MisconfigurationException(\r\n                        \"When requesting `structured` pruning, the `pruning_dim` should be provided.\"\r\n                    )\r\n                if pruning_fn == \"ln_structured\":\r\n                    if pruning_norm is None:\r\n                        raise MisconfigurationException(\r\n                            \"When requesting `ln_structured` pruning, the `pruning_norm` should be provided.\"\r\n                        )\r\n                    pruning_kwargs[\"n\"] = pruning_norm\r\n                pruning_kwargs[\"dim\"] = pruning_dim\r\n            pruning_fn = self._create_pruning_fn(pruning_fn, **pruning_kwargs)\r\n        elif self._is_pruning_method(pruning_fn):\r\n            if not use_global_unstructured:\r\n                raise MisconfigurationException(\r\n                    \"PyTorch `BasePruningMethod` is currently only supported with `use_global_unstructured=True`.\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                f\"`pruning_fn` is expected to be a str in {list(_PYTORCH_PRUNING_FUNCTIONS.keys())}\"\r\n                f\" or a PyTorch `BasePruningMethod`. Found: {pruning_fn}.\"\r\n                \" HINT: if passing a `BasePruningMethod`, pass the class, not an instance\"\r\n            )\r\n\r\n        if use_global_unstructured and pruning_fn.PRUNING_TYPE != \"unstructured\":  # type: ignore\r\n            raise MisconfigurationException(\r\n                'Only the \"unstructured\" PRUNING_TYPE is supported with `use_global_unstructured=True`.'\r\n                f\" Found method {pruning_fn} of type {pruning_fn.PRUNING_TYPE}. \"  # type: ignore[union-attr]\r\n            )\r\n\r\n        self.pruning_fn = pruning_fn\r\n        self._apply_pruning = apply_pruning\r\n        self._make_pruning_permanent = make_pruning_permanent\r\n\r\n        if not (isinstance(amount, (int, float)) or callable(amount)):\r\n            raise MisconfigurationException(\r\n                \"`amount` should be provided and be either an int, a float or Callable function.\"\r\n            )\r\n\r\n        self.amount = amount\r\n\r\n        if verbose not in (0, 1, 2):\r\n            raise MisconfigurationException(\"`verbose` must be any of (0, 1, 2)\")\r\n\r\n        self._verbose = verbose", "code_tokens": ["def", "__init__", "(", "self", ",", "pruning_fn", ":", "Union", "[", "Callable", ",", "str", "]", ",", "parameters_to_prune", ":", "_PARAM_LIST", "=", "(", ")", ",", "parameter_names", ":", "Optional", "[", "list", "[", "str", "]", "]", "=", "None", ",", "use_global_unstructured", ":", "bool", "=", "True", ",", "amount", ":", "Union", "[", "int", ",", "float", ",", "Callable", "[", "[", "int", "]", ",", "Union", "[", "int", ",", "float", "]", "]", "]", "=", "0", ".", "5", ",", "apply_pruning", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "int", "]", ",", "bool", "]", "]", "=", "True", ",", "make_pruning_permanent", ":", "bool", "=", "True", ",", "use_lottery_ticket_hypothesis", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "int", "]", ",", "bool", "]", "]", "=", "True", ",", "resample_parameters", ":", "bool", "=", "False", ",", "pruning_dim", ":", "Optional", "[", "int", "]", "=", "None", ",", "pruning_norm", ":", "Optional", "[", "int", "]", "=", "None", ",", "verbose", ":", "int", "=", "0", ",", "prune_on_train_epoch_end", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "_use_global_unstructured", "=", "use_global_unstructured", "self", ".", "_parameters_to_prune", "=", "parameters_to_prune", "self", ".", "_use_lottery_ticket_hypothesis", "=", "use_lottery_ticket_hypothesis", "self", ".", "_resample_parameters", "=", "resample_parameters", "self", ".", "_prune_on_train_epoch_end", "=", "prune_on_train_epoch_end", "self", ".", "_parameter_names", "=", "parameter_names", "or", "self", ".", "PARAMETER_NAMES", "self", ".", "_global_kwargs", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "self", ".", "_original_layers", ":", "Optional", "[", "dict", "[", "int", ",", "_LayerRef", "]", "]", "=", "None", "self", ".", "_pruning_method_name", ":", "Optional", "[", "str", "]", "=", "None", "for", "name", "in", "self", ".", "_parameter_names", ":", "if", "name", "not", "in", "self", ".", "PARAMETER_NAMES", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "if", "isinstance", "(", "pruning_fn", ",", "str", ")", ":", "pruning_kwargs", "=", "{", "}", "pruning_fn", "=", "pruning_fn", ".", "lower", "(", ")", "if", "pruning_fn", "not", "in", "_PYTORCH_PRUNING_FUNCTIONS", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", ")", "if", "pruning_fn", ".", "endswith", "(", "STRING", ")", ":", "if", "pruning_dim", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "pruning_fn", "=", "=", "STRING", ":", "if", "pruning_norm", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "pruning_kwargs", "[", "STRING", "]", "=", "pruning_norm", "pruning_kwargs", "[", "STRING", "]", "=", "pruning_dim", "pruning_fn", "=", "self", ".", "_create_pruning_fn", "(", "pruning_fn", ",", "*", "*", "pruning_kwargs", ")", "elif", "self", ".", "_is_pruning_method", "(", "pruning_fn", ")", ":", "if", "not", "use_global_unstructured", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "else", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", "STRING", ")", "if", "use_global_unstructured", "and", "pruning_fn", ".", "PRUNING_TYPE", "!", "=", "STRING", ":", "#", "type", ":", "ignore", "raise", "MisconfigurationException", "(", "STRING", "fSTRING", "#", "type", ":", "ignore", "[", "union", "-", "attr", "]", ")", "self", ".", "pruning_fn", "=", "pruning_fn", "self", ".", "_apply_pruning", "=", "apply_pruning", "self", ".", "_make_pruning_permanent", "=", "make_pruning_permanent", "if", "not", "(", "isinstance", "(", "amount", ",", "(", "int", ",", "float", ")", ")", "or", "callable", "(", "amount", ")", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "self", ".", "amount", "=", "amount", "if", "verbose", "not", "in", "(", "0", ",", "1", ",", "2", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "self", ".", "_verbose", "=", "verbose"], "docstring": "Model pruning Callback, using PyTorch's prune utilities. This callback is responsible of pruning networks", "docstring_tokens": ["model", "pruning", "callback", "using", "pytorch", "s", "prune", "utilities", "this", "callback", "is", "responsible", "of", "pruning", "networks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 65, "end_line": 232, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_459", "original_string": "def filter_parameters_to_prune(self, parameters_to_prune: _PARAM_LIST = ()) -> _PARAM_LIST:\r\n        \"\"\"This function can be overridden to control which module to prune.\"\"\"\r\n        return parameters_to_prune", "language": "python", "code": "def filter_parameters_to_prune(self, parameters_to_prune: _PARAM_LIST = ()) -> _PARAM_LIST:\r\n        \"\"\"This function can be overridden to control which module to prune.\"\"\"\r\n        return parameters_to_prune", "code_tokens": ["def", "filter_parameters_to_prune", "(", "self", ",", "parameters_to_prune", ":", "_PARAM_LIST", "=", "(", ")", ")", "-", ">", "_PARAM_LIST", ":", "STRING", "return", "parameters_to_prune"], "docstring": "This function can be overridden to control which module to prune.", "docstring_tokens": ["this", "function", "can", "be", "overridden", "to", "control", "which", "module", "to", "prune"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 234, "end_line": 236, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_460", "original_string": "def _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)", "language": "python", "code": "def _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)", "code_tokens": ["def", "_create_pruning_fn", "(", "self", ",", "pruning_fn", ":", "str", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Union", "[", "Callable", ",", "pytorch_prune", ".", "BasePruningMethod", "]", ":", "STRING", "pruning_meth", "=", "(", "_PYTORCH_PRUNING_METHOD", "[", "pruning_fn", "]", "if", "self", ".", "_use_global_unstructured", "else", "_PYTORCH_PRUNING_FUNCTIONS", "[", "pruning_fn", "]", ")", "assert", "callable", "(", "pruning_meth", ")", ",", "STRING", "if", "self", ".", "_use_global_unstructured", ":", "self", ".", "_global_kwargs", "=", "kwargs", "self", ".", "_pruning_method_name", "=", "pruning_meth", ".", "__name__", "if", "self", ".", "_use_global_unstructured", ":", "return", "pruning_meth", "return", "ModelPruning", ".", "_wrap_pruning_fn", "(", "pruning_meth", ",", "*", "*", "kwargs", ")"], "docstring": "This function takes `pruning_fn`, a function name.", "docstring_tokens": ["this", "function", "takes", "pruning_fn", "a", "function", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 238, "end_line": 258, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_461", "original_string": "def make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]", "language": "python", "code": "def make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]", "code_tokens": ["def", "make_pruning_permanent", "(", "self", ",", "module", ":", "nn", ".", "Module", ")", "-", ">", "None", ":", "STRING", "for", "_", ",", "module", "in", "module", ".", "named_modules", "(", ")", ":", "for", "k", "in", "list", "(", "module", ".", "_forward_pre_hooks", ")", ":", "hook", "=", "module", ".", "_forward_pre_hooks", "[", "k", "]", "if", "isinstance", "(", "hook", ",", "pytorch_prune", ".", "BasePruningMethod", ")", ":", "hook", ".", "remove", "(", "module", ")", "del", "module", ".", "_forward_pre_hooks", "[", "k", "]"], "docstring": "Removes pruning buffers from any pruned modules.", "docstring_tokens": ["removes", "pruning", "buffers", "from", "any", "pruned", "modules"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 264, "end_line": 275, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_462", "original_string": "def apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)", "language": "python", "code": "def apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)", "code_tokens": ["def", "apply_lottery_ticket_hypothesis", "(", "self", ")", "-", ">", "None", ":", "rSTRING", "#", "noqa", ":", "E501", "assert", "self", ".", "_original_layers", "is", "not", "None", "for", "d", "in", "self", ".", "_original_layers", ".", "values", "(", ")", ":", "copy", "=", "d", "[", "STRING", "]", "names", "=", "d", "[", "STRING", "]", "if", "self", ".", "_resample_parameters", "and", "hasattr", "(", "copy", ",", "STRING", ")", "and", "callable", "(", "copy", ".", "reset_parameters", ")", ":", "copy", "=", "deepcopy", "(", "copy", ")", "#", "keep", "the", "original", "parameters", "copy", ".", "reset_parameters", "(", ")", "for", "i", ",", "name", "in", "names", ":", "new", ",", "_", "=", "self", ".", "_parameters_to_prune", "[", "i", "]", "self", ".", "_copy_param", "(", "new", ",", "copy", ",", "name", ")"], "docstring": "r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):", "docstring_tokens": ["r", "lottery", "ticket", "hypothesis", "algorithm", "see", "page", "2", "of", "the", "paper"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 286, "end_line": 308, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_463", "original_string": "def apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)", "language": "python", "code": "def apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)", "code_tokens": ["def", "apply_pruning", "(", "self", ",", "amount", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_verbose", ":", "prev_stats", "=", "[", "self", ".", "_get_pruned_stats", "(", "m", ",", "n", ")", "for", "m", ",", "n", "in", "self", ".", "_parameters_to_prune", "]", "if", "self", ".", "_use_global_unstructured", ":", "self", ".", "_apply_global_pruning", "(", "amount", ")", "else", ":", "self", ".", "_apply_local_pruning", "(", "amount", ")", "if", "self", ".", "_verbose", ":", "curr_stats", "=", "[", "self", ".", "_get_pruned_stats", "(", "m", ",", "n", ")", "for", "m", ",", "n", "in", "self", ".", "_parameters_to_prune", "]", "self", ".", "_log_sparsity_stats", "(", "prev_stats", ",", "curr_stats", ",", "amount", "=", "amount", ")"], "docstring": "Applies pruning to ``parameters_to_prune``.", "docstring_tokens": ["applies", "pruning", "to", "parameters_to_prune"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 333, "end_line": 345, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "function_464", "original_string": "def sanitize_parameters_to_prune(\r\n        pl_module: LightningModule, parameters_to_prune: _PARAM_LIST = (), parameter_names: Sequence[str] = ()\r\n    ) -> _PARAM_LIST:\r\n        \"\"\"This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\r\n        ``parameters_to_prune is None``, it will be generated with all parameters of the model.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameters_to_prune`` doesn't exist in the model, or\r\n                if ``parameters_to_prune`` is neither a list nor a tuple.\r\n\r\n        \"\"\"\r\n        parameters = parameter_names or ModelPruning.PARAMETER_NAMES\r\n\r\n        current_modules = [m for m in pl_module.modules() if not isinstance(m, _MODULE_CONTAINERS)]\r\n\r\n        if not parameters_to_prune:\r\n            parameters_to_prune = [\r\n                (m, p)\r\n                for p in parameters\r\n                for m in current_modules\r\n                if getattr(m, p, None) is not None and isinstance(getattr(m, p, None), nn.Parameter)\r\n            ]\r\n        elif (\r\n            isinstance(parameters_to_prune, (list, tuple))\r\n            and len(parameters_to_prune) > 0\r\n            and all(len(p) == 2 for p in parameters_to_prune)\r\n            and all(isinstance(a, nn.Module) and isinstance(b, str) for a, b in parameters_to_prune)\r\n        ):\r\n            missing_modules, missing_parameters = [], []\r\n            for module, name in parameters_to_prune:\r\n                if module not in current_modules:\r\n                    missing_modules.append(module)\r\n                    continue\r\n                if not hasattr(module, name):\r\n                    missing_parameters.append(name)\r\n\r\n            if missing_modules or missing_parameters:\r\n                raise MisconfigurationException(\r\n                    \"Some provided `parameters_to_prune` don't exist in the model.\"\r\n                    f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                \"The provided `parameters_to_prune` should either be list of tuple\"\r\n                \" with 2 elements: (nn.Module, parameter_name_to_prune) or None\"\r\n            )\r\n\r\n        return parameters_to_prune", "language": "python", "code": "def sanitize_parameters_to_prune(\r\n        pl_module: LightningModule, parameters_to_prune: _PARAM_LIST = (), parameter_names: Sequence[str] = ()\r\n    ) -> _PARAM_LIST:\r\n        \"\"\"This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If\r\n        ``parameters_to_prune is None``, it will be generated with all parameters of the model.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``parameters_to_prune`` doesn't exist in the model, or\r\n                if ``parameters_to_prune`` is neither a list nor a tuple.\r\n\r\n        \"\"\"\r\n        parameters = parameter_names or ModelPruning.PARAMETER_NAMES\r\n\r\n        current_modules = [m for m in pl_module.modules() if not isinstance(m, _MODULE_CONTAINERS)]\r\n\r\n        if not parameters_to_prune:\r\n            parameters_to_prune = [\r\n                (m, p)\r\n                for p in parameters\r\n                for m in current_modules\r\n                if getattr(m, p, None) is not None and isinstance(getattr(m, p, None), nn.Parameter)\r\n            ]\r\n        elif (\r\n            isinstance(parameters_to_prune, (list, tuple))\r\n            and len(parameters_to_prune) > 0\r\n            and all(len(p) == 2 for p in parameters_to_prune)\r\n            and all(isinstance(a, nn.Module) and isinstance(b, str) for a, b in parameters_to_prune)\r\n        ):\r\n            missing_modules, missing_parameters = [], []\r\n            for module, name in parameters_to_prune:\r\n                if module not in current_modules:\r\n                    missing_modules.append(module)\r\n                    continue\r\n                if not hasattr(module, name):\r\n                    missing_parameters.append(name)\r\n\r\n            if missing_modules or missing_parameters:\r\n                raise MisconfigurationException(\r\n                    \"Some provided `parameters_to_prune` don't exist in the model.\"\r\n                    f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n                )\r\n        else:\r\n            raise MisconfigurationException(\r\n                \"The provided `parameters_to_prune` should either be list of tuple\"\r\n                \" with 2 elements: (nn.Module, parameter_name_to_prune) or None\"\r\n            )\r\n\r\n        return parameters_to_prune", "code_tokens": ["def", "sanitize_parameters_to_prune", "(", "pl_module", ":", "LightningModule", ",", "parameters_to_prune", ":", "_PARAM_LIST", "=", "(", ")", ",", "parameter_names", ":", "Sequence", "[", "str", "]", "=", "(", ")", ")", "-", ">", "_PARAM_LIST", ":", "STRING", "parameters", "=", "parameter_names", "or", "ModelPruning", ".", "PARAMETER_NAMES", "current_modules", "=", "[", "m", "for", "m", "in", "pl_module", ".", "modules", "(", ")", "if", "not", "isinstance", "(", "m", ",", "_MODULE_CONTAINERS", ")", "]", "if", "not", "parameters_to_prune", ":", "parameters_to_prune", "=", "[", "(", "m", ",", "p", ")", "for", "p", "in", "parameters", "for", "m", "in", "current_modules", "if", "getattr", "(", "m", ",", "p", ",", "None", ")", "is", "not", "None", "and", "isinstance", "(", "getattr", "(", "m", ",", "p", ",", "None", ")", ",", "nn", ".", "Parameter", ")", "]", "elif", "(", "isinstance", "(", "parameters_to_prune", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "parameters_to_prune", ")", ">", "0", "and", "all", "(", "len", "(", "p", ")", "=", "=", "2", "for", "p", "in", "parameters_to_prune", ")", "and", "all", "(", "isinstance", "(", "a", ",", "nn", ".", "Module", ")", "and", "isinstance", "(", "b", ",", "str", ")", "for", "a", ",", "b", "in", "parameters_to_prune", ")", ")", ":", "missing_modules", ",", "missing_parameters", "=", "[", "]", ",", "[", "]", "for", "module", ",", "name", "in", "parameters_to_prune", ":", "if", "module", "not", "in", "current_modules", ":", "missing_modules", ".", "append", "(", "module", ")", "continue", "if", "not", "hasattr", "(", "module", ",", "name", ")", ":", "missing_parameters", ".", "append", "(", "name", ")", "if", "missing_modules", "or", "missing_parameters", ":", "raise", "MisconfigurationException", "(", "STRING", "fSTRING", ")", "else", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", ")", "return", "parameters_to_prune"], "docstring": "This function is responsible of sanitizing ``parameters_to_prune`` and ``parameter_names``. If", "docstring_tokens": ["this", "function", "is", "responsible", "of", "sanitizing", "parameters_to_prune", "and", "parameter_names", "if"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "start_line": 443, "end_line": 491, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "function_465", "original_string": "def __init__(\r\n        self,\r\n        swa_lrs: Union[float, list[float]],\r\n        swa_epoch_start: Union[int, float] = 0.8,\r\n        annealing_epochs: int = 10,\r\n        annealing_strategy: Literal[\"cos\", \"linear\"] = \"cos\",\r\n        avg_fn: Optional[_AVG_FN] = None,\r\n        device: Optional[Union[torch.device, str]] = torch.device(\"cpu\"),\r\n    ):\r\n        r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\r\n\r\n        Stochastic Weight Averaging was proposed in ``Averaging Weights Leads to\r\n        Wider Optima and Better Generalization`` by Pavel Izmailov, Dmitrii\r\n        Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\r\n        (UAI 2018).\r\n\r\n        This documentation is highly inspired by PyTorch's work on SWA.\r\n        The callback arguments follow the scheme defined in PyTorch's ``swa_utils`` package.\r\n\r\n        For a SWA explanation, please take a look\r\n        `here <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently not supported for multiple optimizers/schedulers.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently only supported on every epoch.\r\n\r\n        See also how to :ref:`enable it directly on the Trainer <advanced/training_tricks:Weight Averaging>`.\r\n\r\n        Arguments:\r\n\r\n            swa_lrs: The SWA learning rate to use:\r\n\r\n                - ``float``. Use this value for all parameter groups of the optimizer.\r\n                - ``List[float]``. A list values for each parameter group of the optimizer.\r\n\r\n            swa_epoch_start: If provided as int, the procedure will start from\r\n                the ``swa_epoch_start``-th epoch. If provided as float between 0 and 1,\r\n                the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch\r\n\r\n            annealing_epochs: number of epochs in the annealing phase (default: 10)\r\n\r\n            annealing_strategy: Specifies the annealing strategy (default: \"cos\"):\r\n\r\n                - ``\"cos\"``. For cosine annealing.\r\n                - ``\"linear\"`` For linear annealing\r\n\r\n            avg_fn: the averaging function used to update the parameters;\r\n                the function must take in the current value of the\r\n                :class:`AveragedModel` parameter, the current value of :attr:`model`\r\n                parameter and the number of models already averaged; if None,\r\n                equally weighted average is used (default: ``None``)\r\n\r\n            device: if provided, the averaged model will be stored on the ``device``.\r\n                When None is provided, it will infer the `device` from ``pl_module``.\r\n                (default: ``\"cpu\"``)\r\n\r\n        \"\"\"\r\n\r\n        err_msg = \"swa_epoch_start should be a >0 integer or a float between 0 and 1.\"\r\n        if isinstance(swa_epoch_start, int) and swa_epoch_start < 1:\r\n            raise MisconfigurationException(err_msg)\r\n        if isinstance(swa_epoch_start, float) and not (0 <= swa_epoch_start <= 1):\r\n            raise MisconfigurationException(err_msg)\r\n\r\n        wrong_type = not isinstance(swa_lrs, (float, list))\r\n        wrong_float = isinstance(swa_lrs, float) and swa_lrs <= 0\r\n        wrong_list = isinstance(swa_lrs, list) and not all(lr > 0 and isinstance(lr, float) for lr in swa_lrs)\r\n        if wrong_type or wrong_float or wrong_list:\r\n            raise MisconfigurationException(\"The `swa_lrs` should a positive float, or a list of positive floats\")\r\n\r\n        if avg_fn is not None and not callable(avg_fn):\r\n            raise MisconfigurationException(\"The `avg_fn` should be callable.\")\r\n\r\n        if device is not None and not isinstance(device, (torch.device, str)):\r\n            raise MisconfigurationException(f\"device is expected to be a torch.device or a str. Found {device}\")\r\n\r\n        self.n_averaged: Optional[Tensor] = None\r\n        self._swa_epoch_start = swa_epoch_start\r\n        self._swa_lrs = swa_lrs\r\n        self._annealing_epochs = annealing_epochs\r\n        self._annealing_strategy = annealing_strategy\r\n        self._avg_fn = avg_fn or self.avg_fn\r\n        self._device = device\r\n        self._model_contains_batch_norm: Optional[bool] = None\r\n        self._average_model: Optional[pl.LightningModule] = None\r\n        self._initialized = False\r\n        self._swa_scheduler: Optional[LRScheduler] = None\r\n        self._scheduler_state: Optional[dict] = None\r\n        self._init_n_averaged = 0\r\n        self._latest_update_epoch = -1\r\n        self.momenta: dict[nn.modules.batchnorm._BatchNorm, Optional[float]] = {}\r\n        self._max_epochs: int", "language": "python", "code": "def __init__(\r\n        self,\r\n        swa_lrs: Union[float, list[float]],\r\n        swa_epoch_start: Union[int, float] = 0.8,\r\n        annealing_epochs: int = 10,\r\n        annealing_strategy: Literal[\"cos\", \"linear\"] = \"cos\",\r\n        avg_fn: Optional[_AVG_FN] = None,\r\n        device: Optional[Union[torch.device, str]] = torch.device(\"cpu\"),\r\n    ):\r\n        r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.\r\n\r\n        Stochastic Weight Averaging was proposed in ``Averaging Weights Leads to\r\n        Wider Optima and Better Generalization`` by Pavel Izmailov, Dmitrii\r\n        Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\r\n        (UAI 2018).\r\n\r\n        This documentation is highly inspired by PyTorch's work on SWA.\r\n        The callback arguments follow the scheme defined in PyTorch's ``swa_utils`` package.\r\n\r\n        For a SWA explanation, please take a look\r\n        `here <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging>`_.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently not supported for multiple optimizers/schedulers.\r\n\r\n        .. warning:: ``StochasticWeightAveraging`` is currently only supported on every epoch.\r\n\r\n        See also how to :ref:`enable it directly on the Trainer <advanced/training_tricks:Weight Averaging>`.\r\n\r\n        Arguments:\r\n\r\n            swa_lrs: The SWA learning rate to use:\r\n\r\n                - ``float``. Use this value for all parameter groups of the optimizer.\r\n                - ``List[float]``. A list values for each parameter group of the optimizer.\r\n\r\n            swa_epoch_start: If provided as int, the procedure will start from\r\n                the ``swa_epoch_start``-th epoch. If provided as float between 0 and 1,\r\n                the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch\r\n\r\n            annealing_epochs: number of epochs in the annealing phase (default: 10)\r\n\r\n            annealing_strategy: Specifies the annealing strategy (default: \"cos\"):\r\n\r\n                - ``\"cos\"``. For cosine annealing.\r\n                - ``\"linear\"`` For linear annealing\r\n\r\n            avg_fn: the averaging function used to update the parameters;\r\n                the function must take in the current value of the\r\n                :class:`AveragedModel` parameter, the current value of :attr:`model`\r\n                parameter and the number of models already averaged; if None,\r\n                equally weighted average is used (default: ``None``)\r\n\r\n            device: if provided, the averaged model will be stored on the ``device``.\r\n                When None is provided, it will infer the `device` from ``pl_module``.\r\n                (default: ``\"cpu\"``)\r\n\r\n        \"\"\"\r\n\r\n        err_msg = \"swa_epoch_start should be a >0 integer or a float between 0 and 1.\"\r\n        if isinstance(swa_epoch_start, int) and swa_epoch_start < 1:\r\n            raise MisconfigurationException(err_msg)\r\n        if isinstance(swa_epoch_start, float) and not (0 <= swa_epoch_start <= 1):\r\n            raise MisconfigurationException(err_msg)\r\n\r\n        wrong_type = not isinstance(swa_lrs, (float, list))\r\n        wrong_float = isinstance(swa_lrs, float) and swa_lrs <= 0\r\n        wrong_list = isinstance(swa_lrs, list) and not all(lr > 0 and isinstance(lr, float) for lr in swa_lrs)\r\n        if wrong_type or wrong_float or wrong_list:\r\n            raise MisconfigurationException(\"The `swa_lrs` should a positive float, or a list of positive floats\")\r\n\r\n        if avg_fn is not None and not callable(avg_fn):\r\n            raise MisconfigurationException(\"The `avg_fn` should be callable.\")\r\n\r\n        if device is not None and not isinstance(device, (torch.device, str)):\r\n            raise MisconfigurationException(f\"device is expected to be a torch.device or a str. Found {device}\")\r\n\r\n        self.n_averaged: Optional[Tensor] = None\r\n        self._swa_epoch_start = swa_epoch_start\r\n        self._swa_lrs = swa_lrs\r\n        self._annealing_epochs = annealing_epochs\r\n        self._annealing_strategy = annealing_strategy\r\n        self._avg_fn = avg_fn or self.avg_fn\r\n        self._device = device\r\n        self._model_contains_batch_norm: Optional[bool] = None\r\n        self._average_model: Optional[pl.LightningModule] = None\r\n        self._initialized = False\r\n        self._swa_scheduler: Optional[LRScheduler] = None\r\n        self._scheduler_state: Optional[dict] = None\r\n        self._init_n_averaged = 0\r\n        self._latest_update_epoch = -1\r\n        self.momenta: dict[nn.modules.batchnorm._BatchNorm, Optional[float]] = {}\r\n        self._max_epochs: int", "code_tokens": ["def", "__init__", "(", "self", ",", "swa_lrs", ":", "Union", "[", "float", ",", "list", "[", "float", "]", "]", ",", "swa_epoch_start", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "8", ",", "annealing_epochs", ":", "int", "=", "10", ",", "annealing_strategy", ":", "Literal", "[", "STRING", ",", "STRING", "]", "=", "STRING", ",", "avg_fn", ":", "Optional", "[", "_AVG_FN", "]", "=", "None", ",", "device", ":", "Optional", "[", "Union", "[", "torch", ".", "device", ",", "str", "]", "]", "=", "torch", ".", "device", "(", "STRING", ")", ",", ")", ":", "rSTRING", "err_msg", "=", "STRING", "if", "isinstance", "(", "swa_epoch_start", ",", "int", ")", "and", "swa_epoch_start", "<", "1", ":", "raise", "MisconfigurationException", "(", "err_msg", ")", "if", "isinstance", "(", "swa_epoch_start", ",", "float", ")", "and", "not", "(", "0", "<", "=", "swa_epoch_start", "<", "=", "1", ")", ":", "raise", "MisconfigurationException", "(", "err_msg", ")", "wrong_type", "=", "not", "isinstance", "(", "swa_lrs", ",", "(", "float", ",", "list", ")", ")", "wrong_float", "=", "isinstance", "(", "swa_lrs", ",", "float", ")", "and", "swa_lrs", "<", "=", "0", "wrong_list", "=", "isinstance", "(", "swa_lrs", ",", "list", ")", "and", "not", "all", "(", "lr", ">", "0", "and", "isinstance", "(", "lr", ",", "float", ")", "for", "lr", "in", "swa_lrs", ")", "if", "wrong_type", "or", "wrong_float", "or", "wrong_list", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "avg_fn", "is", "not", "None", "and", "not", "callable", "(", "avg_fn", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "device", "is", "not", "None", "and", "not", "isinstance", "(", "device", ",", "(", "torch", ".", "device", ",", "str", ")", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "self", ".", "n_averaged", ":", "Optional", "[", "Tensor", "]", "=", "None", "self", ".", "_swa_epoch_start", "=", "swa_epoch_start", "self", ".", "_swa_lrs", "=", "swa_lrs", "self", ".", "_annealing_epochs", "=", "annealing_epochs", "self", ".", "_annealing_strategy", "=", "annealing_strategy", "self", ".", "_avg_fn", "=", "avg_fn", "or", "self", ".", "avg_fn", "self", ".", "_device", "=", "device", "self", ".", "_model_contains_batch_norm", ":", "Optional", "[", "bool", "]", "=", "None", "self", ".", "_average_model", ":", "Optional", "[", "pl", ".", "LightningModule", "]", "=", "None", "self", ".", "_initialized", "=", "False", "self", ".", "_swa_scheduler", ":", "Optional", "[", "LRScheduler", "]", "=", "None", "self", ".", "_scheduler_state", ":", "Optional", "[", "dict", "]", "=", "None", "self", ".", "_init_n_averaged", "=", "0", "self", ".", "_latest_update_epoch", "=", "-", "1", "self", ".", "momenta", ":", "dict", "[", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ",", "Optional", "[", "float", "]", "]", "=", "{", "}", "self", ".", "_max_epochs", ":", "int"], "docstring": "r\"\"\"Implements the Stochastic Weight Averaging (SWA) Callback to average a model.", "docstring_tokens": ["r", "implements", "the", "stochastic", "weight", "averaging", "swa", "callback", "to", "average", "a", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "start_line": 39, "end_line": 132, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "function_466", "original_string": "def reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0", "language": "python", "code": "def reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0", "code_tokens": ["def", "reset_batch_norm_and_save_state", "(", "self", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "self", ".", "momenta", "=", "{", "}", "for", "module", "in", "pl_module", ".", "modules", "(", ")", ":", "if", "not", "isinstance", "(", "module", ",", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", ":", "continue", "assert", "module", ".", "running_mean", "is", "not", "None", "module", ".", "running_mean", "=", "torch", ".", "zeros_like", "(", "module", ".", "running_mean", ",", "device", "=", "pl_module", ".", "device", ",", "dtype", "=", "module", ".", "running_mean", ".", "dtype", ",", ")", "assert", "module", ".", "running_var", "is", "not", "None", "module", ".", "running_var", "=", "torch", ".", "ones_like", "(", "module", ".", "running_var", ",", "device", "=", "pl_module", ".", "device", ",", "dtype", "=", "module", ".", "running_var", ".", "dtype", ",", ")", "self", ".", "momenta", "[", "module", "]", "=", "module", ".", "momentum", "module", ".", "momentum", "=", "None", "assert", "module", ".", "num_batches_tracked", "is", "not", "None", "module", ".", "num_batches_tracked", "*", "=", "0"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l140", "l154"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "start_line": 286, "end_line": 307, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "function_467", "original_string": "def reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]", "language": "python", "code": "def reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]", "code_tokens": ["def", "reset_momenta", "(", "self", ")", "-", ">", "None", ":", "STRING", "for", "bn_module", "in", "self", ".", "momenta", ":", "bn_module", ".", "momentum", "=", "self", ".", "momenta", "[", "bn_module", "]"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l164", "l165"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "start_line": 309, "end_line": 312, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "function_468", "original_string": "def update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1", "language": "python", "code": "def update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1", "code_tokens": ["def", "update_parameters", "(", "average_model", ":", "STRING", ",", "model", ":", "STRING", ",", "n_averaged", ":", "Tensor", ",", "avg_fn", ":", "_AVG_FN", ")", "-", ">", "None", ":", "STRING", "for", "p_swa", ",", "p_model", "in", "zip", "(", "average_model", ".", "parameters", "(", ")", ",", "model", ".", "parameters", "(", ")", ")", ":", "device", "=", "p_swa", ".", "device", "p_swa_", "=", "p_swa", ".", "detach", "(", ")", "p_model_", "=", "p_model", ".", "detach", "(", ")", ".", "to", "(", "device", ")", "src", "=", "p_model_", "if", "n_averaged", "=", "=", "0", "else", "avg_fn", "(", "p_swa_", ",", "p_model_", ",", "n_averaged", ".", "to", "(", "device", ")", ")", "p_swa_", ".", "copy_", "(", "src", ")", "n_averaged", "+", "=", "1"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l104", "l112"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "start_line": 315, "end_line": 325, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "function_469", "original_string": "def avg_fn(averaged_model_parameter: Tensor, model_parameter: Tensor, num_averaged: Tensor) -> Tensor:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\"\"\"\r\n        return averaged_model_parameter + (model_parameter - averaged_model_parameter) / (num_averaged + 1)", "language": "python", "code": "def avg_fn(averaged_model_parameter: Tensor, model_parameter: Tensor, num_averaged: Tensor) -> Tensor:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.\"\"\"\r\n        return averaged_model_parameter + (model_parameter - averaged_model_parameter) / (num_averaged + 1)", "code_tokens": ["def", "avg_fn", "(", "averaged_model_parameter", ":", "Tensor", ",", "model_parameter", ":", "Tensor", ",", "num_averaged", ":", "Tensor", ")", "-", ">", "Tensor", ":", "STRING", "return", "averaged_model_parameter", "+", "(", "model_parameter", "-", "averaged_model_parameter", ")", "/", "(", "num_averaged", "+", "1", ")"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L95-L97.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l95", "l97"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "start_line": 328, "end_line": 330, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "function_470", "original_string": "def start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]", "language": "python", "code": "def start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]", "code_tokens": ["def", "start_time", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "STRING", "stage", "=", "RunningStage", "(", "stage", ")", "return", "self", ".", "_start_time", "[", "stage", "]"], "docstring": "Return the start time of a particular stage (in seconds)", "docstring_tokens": ["return", "the", "start", "time", "of", "a", "particular", "stage", "in", "seconds"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\timer.py", "start_line": 117, "end_line": 120, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "function_471", "original_string": "def end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]", "language": "python", "code": "def end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]", "code_tokens": ["def", "end_time", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "STRING", "stage", "=", "RunningStage", "(", "stage", ")", "return", "self", ".", "_end_time", "[", "stage", "]"], "docstring": "Return the end time of a particular stage (in seconds)", "docstring_tokens": ["return", "the", "end", "time", "of", "a", "particular", "stage", "in", "seconds"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\timer.py", "start_line": 122, "end_line": 125, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "function_472", "original_string": "def time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset", "language": "python", "code": "def time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset", "code_tokens": ["def", "time_elapsed", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "float", ":", "STRING", "start", "=", "self", ".", "start_time", "(", "stage", ")", "end", "=", "self", ".", "end_time", "(", "stage", ")", "offset", "=", "self", ".", "_offset", "if", "stage", "=", "=", "RunningStage", ".", "TRAINING", "else", "0", "if", "start", "is", "None", ":", "return", "offset", "if", "end", "is", "None", ":", "return", "time", ".", "monotonic", "(", ")", "-", "start", "+", "offset", "return", "end", "-", "start", "+", "offset"], "docstring": "Return the time elapsed for a particular stage (in seconds)", "docstring_tokens": ["return", "the", "time", "elapsed", "for", "a", "particular", "stage", "in", "seconds"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\timer.py", "start_line": 127, "end_line": 136, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "function_473", "original_string": "def time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None", "language": "python", "code": "def time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None", "code_tokens": ["def", "time_remaining", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "STRING", "if", "self", ".", "_duration", "is", "not", "None", ":", "return", "self", ".", "_duration", "-", "self", ".", "time_elapsed", "(", "stage", ")", "return", "None"], "docstring": "Return the time remaining for a particular stage (in seconds)", "docstring_tokens": ["return", "the", "time", "remaining", "for", "a", "particular", "stage", "in", "seconds"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\timer.py", "start_line": 138, "end_line": 142, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_474", "original_string": "def should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None", "language": "python", "code": "def should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None", "code_tokens": ["def", "should_update", "(", "self", ",", "step_idx", ":", "Optional", "[", "int", "]", "=", "None", ",", "epoch_idx", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "bool", ":", "STRING", "return", "step_idx", "is", "not", "None"], "docstring": "Called after every optimizer step and after every training epoch to check whether the average model should", "docstring_tokens": ["called", "after", "every", "optimizer", "step", "and", "after", "every", "training", "epoch", "to", "check", "whether", "the", "average", "model", "should"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 114, "end_line": 130, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_475", "original_string": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING", "if", "stage", "=", "=", "STRING", ":", "device", "=", "self", ".", "_device", "or", "pl_module", ".", "device", "if", "is_overridden", "(", "STRING", ",", "pl_module", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "pl_module", ".", "configure_model", "(", ")", "self", ".", "_average_model", "=", "AveragedModel", "(", "model", "=", "pl_module", ",", "device", "=", "device", ",", "use_buffers", "=", "self", ".", "_use_buffers", ",", "*", "*", "self", ".", "_kwargs", ")"], "docstring": "Called when fit, validate, test, predict, or tune begins.", "docstring_tokens": ["called", "when", "fit", "validate", "test", "predict", "or", "tune", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 133, "end_line": 158, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_476", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING", "step_idx", "=", "trainer", ".", "global_step", "-", "1", "if", "(", "trainer", ".", "global_step", ">", "self", ".", "_latest_update_step", ")", "and", "self", ".", "should_update", "(", "step_idx", "=", "step_idx", ")", ":", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_average_model", ".", "update_parameters", "(", "pl_module", ")", "self", ".", "_latest_update_step", "=", "trainer", ".", "global_step"], "docstring": "Called when a training batch ends.", "docstring_tokens": ["called", "when", "a", "training", "batch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 161, "end_line": 182, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_477", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "(", "trainer", ".", "current_epoch", ">", "self", ".", "_latest_update_epoch", ")", "and", "self", ".", "should_update", "(", "epoch_idx", "=", "trainer", ".", "current_epoch", ")", ":", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_average_model", ".", "update_parameters", "(", "pl_module", ")", "self", ".", "_latest_update_epoch", "=", "trainer", ".", "current_epoch"], "docstring": "Called when a training epoch ends.", "docstring_tokens": ["called", "when", "a", "training", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 185, "end_line": 198, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_478", "original_string": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)", "language": "python", "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)", "code_tokens": ["def", "on_train_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_copy_average_to_current", "(", "pl_module", ")"], "docstring": "Called when training ends.", "docstring_tokens": ["called", "when", "training", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 201, "end_line": 212, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_479", "original_string": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "language": "python", "code": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "code_tokens": ["def", "on_validation_epoch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_average_model", "is", "not", "None", ":", "self", ".", "_swap_models", "(", "pl_module", ")"], "docstring": "Called when a validation epoch begins.", "docstring_tokens": ["called", "when", "a", "validation", "epoch", "begins"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 215, "end_line": 226, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_480", "original_string": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "language": "python", "code": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "code_tokens": ["def", "on_validation_epoch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_average_model", "is", "not", "None", ":", "self", ".", "_swap_models", "(", "pl_module", ")"], "docstring": "Called when a validation epoch ends.", "docstring_tokens": ["called", "when", "a", "validation", "epoch", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 229, "end_line": 240, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_481", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "{", "STRING", ":", "self", ".", "_latest_update_step", "}"], "docstring": "Called when saving a checkpoint.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 243, "end_line": 252, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_482", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "_latest_update_step", "=", "state_dict", "[", "STRING", "]"], "docstring": "Called when loading a checkpoint.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 255, "end_line": 264, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_483", "original_string": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }", "language": "python", "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING", "if", "self", ".", "_average_model", "is", "None", ":", "rank_zero_info", "(", "STRING", "STRING", "STRING", ")", "else", ":", "average_model_state", "=", "self", ".", "_average_model", ".", "state_dict", "(", ")", "checkpoint", "[", "STRING", "]", "=", "checkpoint", "[", "STRING", "]", "checkpoint", "[", "STRING", "]", "=", "{", "name", "[", "7", ":", "]", ":", "value", "for", "name", ",", "value", "in", "average_model_state", ".", "items", "(", ")", "if", "name", ".", "startswith", "(", "STRING", ")", "}", "checkpoint", "[", "STRING", "]", "=", "{", "name", ":", "value", "for", "name", ",", "value", "in", "average_model_state", ".", "items", "(", ")", "if", "not", "name", ".", "startswith", "(", "STRING", ")", "}"], "docstring": "r\"\"\"Called when saving a checkpoint.", "docstring_tokens": ["r", "called", "when", "saving", "a", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 267, "end_line": 298, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_484", "original_string": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)", "language": "python", "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING", "if", "self", ".", "_average_model", "is", "None", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", ")", "elif", "(", "STRING", "in", "checkpoint", ")", "and", "(", "STRING", "in", "checkpoint", ")", ":", "rank_zero_info", "(", "STRING", ")", "average_model_state", "=", "{", "STRING", "+", "name", ":", "value", "for", "name", ",", "value", "in", "checkpoint", "[", "STRING", "]", ".", "items", "(", ")", "}", "average_model_state", "|", "=", "checkpoint", "[", "STRING", "]", "self", ".", "_average_model", ".", "load_state_dict", "(", "average_model_state", ")", "pl_module", ".", "load_state_dict", "(", "checkpoint", "[", "STRING", "]", ")", "else", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "self", ".", "_average_model", ".", "module", ".", "load_state_dict", "(", "deepcopy", "(", "checkpoint", "[", "STRING", "]", ")", ",", "strict", "=", "False", ")"], "docstring": "r\"\"\"Called when loading a model checkpoint.", "docstring_tokens": ["r", "called", "when", "loading", "a", "model", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 301, "end_line": 334, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_485", "original_string": "def _swap_models(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Swaps the parameter values of the current model and the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            tmp = average_param.data.clone()\r\n            average_param.data.copy_(current_param.data)\r\n            current_param.data.copy_(tmp)", "language": "python", "code": "def _swap_models(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Swaps the parameter values of the current model and the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            tmp = average_param.data.clone()\r\n            average_param.data.copy_(current_param.data)\r\n            current_param.data.copy_(tmp)", "code_tokens": ["def", "_swap_models", "(", "self", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "_average_model", "is", "not", "None", "average_params", "=", "itertools", ".", "chain", "(", "self", ".", "_average_model", ".", "module", ".", "parameters", "(", ")", ",", "self", ".", "_average_model", ".", "module", ".", "buffers", "(", ")", ")", "current_params", "=", "itertools", ".", "chain", "(", "pl_module", ".", "parameters", "(", ")", ",", "pl_module", ".", "buffers", "(", ")", ")", "for", "average_param", ",", "current_param", "in", "zip", "(", "average_params", ",", "current_params", ")", ":", "tmp", "=", "average_param", ".", "data", ".", "clone", "(", ")", "average_param", ".", "data", ".", "copy_", "(", "current_param", ".", "data", ")", "current_param", ".", "data", ".", "copy_", "(", "tmp", ")"], "docstring": "Swaps the parameter values of the current model and the :class:`AveragedModel`.", "docstring_tokens": ["swaps", "the", "parameter", "values", "of", "the", "current", "model", "and", "the", "class", "averagedmodel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 336, "end_line": 349, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "function_486", "original_string": "def _copy_average_to_current(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Copies the parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            current_param.data.copy_(average_param.data)", "language": "python", "code": "def _copy_average_to_current(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Copies the parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            current_param.data.copy_(average_param.data)", "code_tokens": ["def", "_copy_average_to_current", "(", "self", ",", "pl_module", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "_average_model", "is", "not", "None", "average_params", "=", "itertools", ".", "chain", "(", "self", ".", "_average_model", ".", "module", ".", "parameters", "(", ")", ",", "self", ".", "_average_model", ".", "module", ".", "buffers", "(", ")", ")", "current_params", "=", "itertools", ".", "chain", "(", "pl_module", ".", "parameters", "(", ")", ",", "pl_module", ".", "buffers", "(", ")", ")", "for", "average_param", ",", "current_param", "in", "zip", "(", "average_params", ",", "current_params", ")", ":", "current_param", ".", "data", ".", "copy_", "(", "average_param", ".", "data", ")"], "docstring": "Copies the parameter values from the :class:`AveragedModel` to the current model.", "docstring_tokens": ["copies", "the", "parameter", "values", "from", "the", "class", "averagedmodel", "to", "the", "current", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "start_line": 351, "end_line": 362, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_487", "original_string": "def total_train_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of training batches, which may change from epoch to epoch.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the training\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        if self.trainer.max_epochs == -1 and self.trainer.max_steps is not None and self.trainer.max_steps > 0:\r\n            remaining_steps = self.trainer.max_steps - self.trainer.global_step\r\n            return min(self.trainer.num_training_batches, remaining_steps)\r\n        return self.trainer.num_training_batches", "language": "python", "code": "def total_train_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of training batches, which may change from epoch to epoch.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the training\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        if self.trainer.max_epochs == -1 and self.trainer.max_steps is not None and self.trainer.max_steps > 0:\r\n            remaining_steps = self.trainer.max_steps - self.trainer.global_step\r\n            return min(self.trainer.num_training_batches, remaining_steps)\r\n        return self.trainer.num_training_batches", "code_tokens": ["def", "total_train_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "if", "self", ".", "trainer", ".", "max_epochs", "=", "=", "-", "1", "and", "self", ".", "trainer", ".", "max_steps", "is", "not", "None", "and", "self", ".", "trainer", ".", "max_steps", ">", "0", ":", "remaining_steps", "=", "self", ".", "trainer", ".", "max_steps", "-", "self", ".", "trainer", ".", "global_step", "return", "min", "(", "self", ".", "trainer", ".", "num_training_batches", ",", "remaining_steps", ")", "return", "self", ".", "trainer", ".", "num_training_batches"], "docstring": "The total number of training batches, which may change from epoch to epoch.", "docstring_tokens": ["the", "total", "number", "of", "training", "batches", "which", "may", "change", "from", "epoch", "to", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 80, "end_line": 90, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_488", "original_string": "def total_val_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the validation\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_sanity_val_batches if self.trainer.sanity_checking else self.trainer.num_val_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "language": "python", "code": "def total_val_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the validation\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_sanity_val_batches if self.trainer.sanity_checking else self.trainer.num_val_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "code_tokens": ["def", "total_val_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "batches", "=", "self", ".", "trainer", ".", "num_sanity_val_batches", "if", "self", ".", "trainer", ".", "sanity_checking", "else", "self", ".", "trainer", ".", "num_val_batches", "if", "isinstance", "(", "batches", ",", "list", ")", ":", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "batches", "[", "self", ".", "_current_eval_dataloader_idx", "]", "return", "batches"], "docstring": "The total number of validation batches, which may change from epoch to epoch for current dataloader.", "docstring_tokens": ["the", "total", "number", "of", "validation", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 93, "end_line": 104, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_489", "original_string": "def total_test_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of testing batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the test dataloader is\r\n        of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_test_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "language": "python", "code": "def total_test_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of testing batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the test dataloader is\r\n        of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_test_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "code_tokens": ["def", "total_test_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "batches", "=", "self", ".", "trainer", ".", "num_test_batches", "if", "isinstance", "(", "batches", ",", "list", ")", ":", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "batches", "[", "self", ".", "_current_eval_dataloader_idx", "]", "return", "batches"], "docstring": "The total number of testing batches, which may change from epoch to epoch for current dataloader.", "docstring_tokens": ["the", "total", "number", "of", "testing", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 107, "end_line": 118, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_490", "original_string": "def total_predict_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of prediction batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        assert self._current_eval_dataloader_idx is not None\r\n        return self.trainer.num_predict_batches[self._current_eval_dataloader_idx]", "language": "python", "code": "def total_predict_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of prediction batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        assert self._current_eval_dataloader_idx is not None\r\n        return self.trainer.num_predict_batches[self._current_eval_dataloader_idx]", "code_tokens": ["def", "total_predict_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "self", ".", "trainer", ".", "num_predict_batches", "[", "self", ".", "_current_eval_dataloader_idx", "]"], "docstring": "The total number of prediction batches, which may change from epoch to epoch for current dataloader.", "docstring_tokens": ["the", "total", "number", "of", "prediction", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 121, "end_line": 129, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_491", "original_string": "def total_val_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for all val dataloaders.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        if not self.trainer.fit_loop.epoch_loop._should_check_val_epoch():\r\n            return 0\r\n        return (\r\n            sum(self.trainer.num_val_batches)\r\n            if isinstance(self.trainer.num_val_batches, list)\r\n            else self.trainer.num_val_batches\r\n        )", "language": "python", "code": "def total_val_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for all val dataloaders.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        if not self.trainer.fit_loop.epoch_loop._should_check_val_epoch():\r\n            return 0\r\n        return (\r\n            sum(self.trainer.num_val_batches)\r\n            if isinstance(self.trainer.num_val_batches, list)\r\n            else self.trainer.num_val_batches\r\n        )", "code_tokens": ["def", "total_val_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "if", "not", "self", ".", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "_should_check_val_epoch", "(", ")", ":", "return", "0", "return", "(", "sum", "(", "self", ".", "trainer", ".", "num_val_batches", ")", "if", "isinstance", "(", "self", ".", "trainer", ".", "num_val_batches", ",", "list", ")", "else", "self", ".", "trainer", ".", "num_val_batches", ")"], "docstring": "The total number of validation batches, which may change from epoch to epoch for all val dataloaders.", "docstring_tokens": ["the", "total", "number", "of", "validation", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "all", "val", "dataloaders"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 132, "end_line": 145, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_492", "original_string": "def disable(self) -> None:\r\n        \"\"\"You should provide a way to disable the progress bar.\"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def disable(self) -> None:\r\n        \"\"\"You should provide a way to disable the progress bar.\"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "disable", "(", "self", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError"], "docstring": "You should provide a way to disable the progress bar.", "docstring_tokens": ["you", "should", "provide", "a", "way", "to", "disable", "the", "progress", "bar"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 155, "end_line": 157, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_493", "original_string": "def enable(self) -> None:\r\n        \"\"\"You should provide a way to enable the progress bar.\r\n\r\n        The :class:`~lightning.pytorch.trainer.trainer.Trainer` will call this in e.g. pre-training\r\n        routines like the :ref:`learning rate finder <advanced/training_tricks:Learning Rate Finder>`.\r\n        to temporarily enable and disable the training progress bar.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def enable(self) -> None:\r\n        \"\"\"You should provide a way to enable the progress bar.\r\n\r\n        The :class:`~lightning.pytorch.trainer.trainer.Trainer` will call this in e.g. pre-training\r\n        routines like the :ref:`learning rate finder <advanced/training_tricks:Learning Rate Finder>`.\r\n        to temporarily enable and disable the training progress bar.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "enable", "(", "self", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError"], "docstring": "You should provide a way to enable the progress bar.", "docstring_tokens": ["you", "should", "provide", "a", "way", "to", "enable", "the", "progress", "bar"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 159, "end_line": 167, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_494", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"You should provide a way to print without breaking the progress bar.\"\"\"\r\n        print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"You should provide a way to print without breaking the progress bar.\"\"\"\r\n        print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "You should provide a way to print without breaking the progress bar.", "docstring_tokens": ["you", "should", "provide", "a", "way", "to", "print", "without", "breaking", "the", "progress", "bar"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 169, "end_line": 171, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_495", "original_string": "def get_metrics(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\r\n    ) -> dict[str, Union[int, str, float, dict[str, float]]]:\r\n        r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.\r\n        Implement this to override the items displayed in the progress bar.\r\n\r\n        Here is an example of how to override the defaults:\r\n\r\n        .. code-block:: python\r\n\r\n            def get_metrics(self, trainer, model):\r\n                items = super().get_metrics(trainer, model)\r\n                items.pop(\"v_num\", None)\r\n                return items\r\n\r\n        Return:\r\n            Dictionary with the items to be displayed in the progress bar.\r\n\r\n        \"\"\"\r\n        standard_metrics = get_standard_metrics(trainer)\r\n        pbar_metrics = trainer.progress_bar_metrics\r\n        duplicates = list(standard_metrics.keys() & pbar_metrics.keys())\r\n        if duplicates:\r\n            rank_zero_warn(\r\n                f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\r\n                f\" `self.log('{duplicates[0]}', ..., prog_bar=True)` will overwrite this value. \"\r\n                \" If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\",\r\n            )\r\n\r\n        return {**standard_metrics, **pbar_metrics}", "language": "python", "code": "def get_metrics(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\r\n    ) -> dict[str, Union[int, str, float, dict[str, float]]]:\r\n        r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.\r\n        Implement this to override the items displayed in the progress bar.\r\n\r\n        Here is an example of how to override the defaults:\r\n\r\n        .. code-block:: python\r\n\r\n            def get_metrics(self, trainer, model):\r\n                items = super().get_metrics(trainer, model)\r\n                items.pop(\"v_num\", None)\r\n                return items\r\n\r\n        Return:\r\n            Dictionary with the items to be displayed in the progress bar.\r\n\r\n        \"\"\"\r\n        standard_metrics = get_standard_metrics(trainer)\r\n        pbar_metrics = trainer.progress_bar_metrics\r\n        duplicates = list(standard_metrics.keys() & pbar_metrics.keys())\r\n        if duplicates:\r\n            rank_zero_warn(\r\n                f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\r\n                f\" `self.log('{duplicates[0]}', ..., prog_bar=True)` will overwrite this value. \"\r\n                \" If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\",\r\n            )\r\n\r\n        return {**standard_metrics, **pbar_metrics}", "code_tokens": ["def", "get_metrics", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ")", "-", ">", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", ",", "float", ",", "dict", "[", "str", ",", "float", "]", "]", "]", ":", "rSTRING", "standard_metrics", "=", "get_standard_metrics", "(", "trainer", ")", "pbar_metrics", "=", "trainer", ".", "progress_bar_metrics", "duplicates", "=", "list", "(", "standard_metrics", ".", "keys", "(", ")", "&", "pbar_metrics", ".", "keys", "(", ")", ")", "if", "duplicates", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", "STRING", ",", ")", "return", "{", "*", "*", "standard_metrics", ",", "*", "*", "pbar_metrics", "}"], "docstring": "r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.", "docstring_tokens": ["r", "combines", "progress", "bar", "metrics", "collected", "from", "the", "trainer", "with", "standard", "metrics", "from", "get_standard_metrics"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 179, "end_line": 209, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "function_496", "original_string": "def get_standard_metrics(trainer: \"pl.Trainer\") -> dict[str, Union[int, str]]:\r\n    r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the\r\n    experiment when using a logger.\r\n\r\n    .. code-block::\r\n\r\n        Epoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, v_num=10]\r\n\r\n    Return:\r\n        Dictionary with the standard metrics to be displayed in the progress bar.\r\n\r\n    \"\"\"\r\n    items_dict: dict[str, Union[int, str]] = {}\r\n    if trainer.loggers:\r\n        from lightning.pytorch.loggers.utilities import _version\r\n\r\n        if (version := _version(trainer.loggers)) not in (\"\", None):\r\n            if isinstance(version, str):\r\n                version = version[-4:]\r\n            items_dict[\"v_num\"] = version\r\n\r\n    return items_dict", "language": "python", "code": "def get_standard_metrics(trainer: \"pl.Trainer\") -> dict[str, Union[int, str]]:\r\n    r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the\r\n    experiment when using a logger.\r\n\r\n    .. code-block::\r\n\r\n        Epoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, v_num=10]\r\n\r\n    Return:\r\n        Dictionary with the standard metrics to be displayed in the progress bar.\r\n\r\n    \"\"\"\r\n    items_dict: dict[str, Union[int, str]] = {}\r\n    if trainer.loggers:\r\n        from lightning.pytorch.loggers.utilities import _version\r\n\r\n        if (version := _version(trainer.loggers)) not in (\"\", None):\r\n            if isinstance(version, str):\r\n                version = version[-4:]\r\n            items_dict[\"v_num\"] = version\r\n\r\n    return items_dict", "code_tokens": ["def", "get_standard_metrics", "(", "trainer", ":", "STRING", ")", "-", ">", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", "]", "]", ":", "rSTRING", "items_dict", ":", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", "]", "]", "=", "{", "}", "if", "trainer", ".", "loggers", ":", "from", "lightning", ".", "pytorch", ".", "loggers", ".", "utilities", "import", "_version", "if", "(", "version", ":", "=", "_version", "(", "trainer", ".", "loggers", ")", ")", "not", "in", "(", "STRING", ",", "None", ")", ":", "if", "isinstance", "(", "version", ",", "str", ")", ":", "version", "=", "version", "[", "-", "4", ":", "]", "items_dict", "[", "STRING", "]", "=", "version", "return", "items_dict"], "docstring": "r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the", "docstring_tokens": ["r", "returns", "the", "standard", "metrics", "displayed", "in", "the", "progress", "bar", "currently", "it", "only", "includes", "the", "version", "of", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "start_line": 212, "end_line": 234, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\rich_progress.py", "func_name": "function_497", "original_string": "def render(self, task: \"Task\") -> _RichProgressBar:\r\n            \"\"\"Gets a progress bar widget for a task.\"\"\"\r\n            assert task.total is not None\r\n            assert task.remaining is not None\r\n            return _RichProgressBar(\r\n                total=max(0, task.total),\r\n                completed=max(0, task.completed),\r\n                width=None if self.bar_width is None else max(1, self.bar_width),\r\n                pulse=not task.started or not math.isfinite(task.remaining),\r\n                animation_time=task.get_time(),\r\n                style=self.style,\r\n                complete_style=self.complete_style,\r\n                finished_style=self.finished_style,\r\n                pulse_style=self.pulse_style,\r\n            )", "language": "python", "code": "def render(self, task: \"Task\") -> _RichProgressBar:\r\n            \"\"\"Gets a progress bar widget for a task.\"\"\"\r\n            assert task.total is not None\r\n            assert task.remaining is not None\r\n            return _RichProgressBar(\r\n                total=max(0, task.total),\r\n                completed=max(0, task.completed),\r\n                width=None if self.bar_width is None else max(1, self.bar_width),\r\n                pulse=not task.started or not math.isfinite(task.remaining),\r\n                animation_time=task.get_time(),\r\n                style=self.style,\r\n                complete_style=self.complete_style,\r\n                finished_style=self.finished_style,\r\n                pulse_style=self.pulse_style,\r\n            )", "code_tokens": ["def", "render", "(", "self", ",", "task", ":", "STRING", ")", "-", ">", "_RichProgressBar", ":", "STRING", "assert", "task", ".", "total", "is", "not", "None", "assert", "task", ".", "remaining", "is", "not", "None", "return", "_RichProgressBar", "(", "total", "=", "max", "(", "0", ",", "task", ".", "total", ")", ",", "completed", "=", "max", "(", "0", ",", "task", ".", "completed", ")", ",", "width", "=", "None", "if", "self", ".", "bar_width", "is", "None", "else", "max", "(", "1", ",", "self", ".", "bar_width", ")", ",", "pulse", "=", "not", "task", ".", "started", "or", "not", "math", ".", "isfinite", "(", "task", ".", "remaining", ")", ",", "animation_time", "=", "task", ".", "get_time", "(", ")", ",", "style", "=", "self", ".", "style", ",", "complete_style", "=", "self", ".", "complete_style", ",", "finished_style", "=", "self", ".", "finished_style", ",", "pulse_style", "=", "self", ".", "pulse_style", ",", ")"], "docstring": "Gets a progress bar widget for a task.", "docstring_tokens": ["gets", "a", "progress", "bar", "widget", "for", "a", "task"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\rich_progress.py", "start_line": 40, "end_line": 54, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_498", "original_string": "def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from\r\n        flickering.\"\"\"\r\n        super().__init__(*args, **kwargs)", "language": "python", "code": "def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from\r\n        flickering.\"\"\"\r\n        super().__init__(*args, **kwargs)", "code_tokens": ["def", "__init__", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from", "docstring_tokens": ["custom", "tqdm", "progressbar", "where", "we", "append", "0", "to", "floating", "points", "strings", "to", "prevent", "the", "progress", "bar", "from"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 39, "end_line": 43, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_499", "original_string": "def format_num(n: Union[int, float, str]) -> str:\r\n        \"\"\"Add additional padding to the formatted numbers.\"\"\"\r\n        should_be_padded = isinstance(n, (float, str))\r\n        if not isinstance(n, str):\r\n            n = _tqdm.format_num(n)\r\n            assert isinstance(n, str)\r\n        if should_be_padded and \"e\" not in n:\r\n            if \".\" not in n and len(n) < _PAD_SIZE:\r\n                try:\r\n                    _ = float(n)\r\n                except ValueError:\r\n                    return n\r\n                n += \".\"\r\n            n += \"0\" * (_PAD_SIZE - len(n))\r\n        return n", "language": "python", "code": "def format_num(n: Union[int, float, str]) -> str:\r\n        \"\"\"Add additional padding to the formatted numbers.\"\"\"\r\n        should_be_padded = isinstance(n, (float, str))\r\n        if not isinstance(n, str):\r\n            n = _tqdm.format_num(n)\r\n            assert isinstance(n, str)\r\n        if should_be_padded and \"e\" not in n:\r\n            if \".\" not in n and len(n) < _PAD_SIZE:\r\n                try:\r\n                    _ = float(n)\r\n                except ValueError:\r\n                    return n\r\n                n += \".\"\r\n            n += \"0\" * (_PAD_SIZE - len(n))\r\n        return n", "code_tokens": ["def", "format_num", "(", "n", ":", "Union", "[", "int", ",", "float", ",", "str", "]", ")", "-", ">", "str", ":", "STRING", "should_be_padded", "=", "isinstance", "(", "n", ",", "(", "float", ",", "str", ")", ")", "if", "not", "isinstance", "(", "n", ",", "str", ")", ":", "n", "=", "_tqdm", ".", "format_num", "(", "n", ")", "assert", "isinstance", "(", "n", ",", "str", ")", "if", "should_be_padded", "and", "STRING", "not", "in", "n", ":", "if", "STRING", "not", "in", "n", "and", "len", "(", "n", ")", "<", "_PAD_SIZE", ":", "try", ":", "_", "=", "float", "(", "n", ")", "except", "ValueError", ":", "return", "n", "n", "+", "=", "STRING", "n", "+", "=", "STRING", "*", "(", "_PAD_SIZE", "-", "len", "(", "n", ")", ")", "return", "n"], "docstring": "Add additional padding to the formatted numbers.", "docstring_tokens": ["add", "additional", "padding", "to", "the", "formatted", "numbers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 46, "end_line": 60, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_500", "original_string": "def init_sanity_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for the validation sanity run.\"\"\"\r\n        return Tqdm(\r\n            desc=self.sanity_check_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=False,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_sanity_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for the validation sanity run.\"\"\"\r\n        return Tqdm(\r\n            desc=self.sanity_check_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=False,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_sanity_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "STRING", "return", "Tqdm", "(", "desc", "=", "self", ".", "sanity_check_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "False", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for the validation sanity run.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "the", "validation", "sanity", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 185, "end_line": 195, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_501", "original_string": "def init_train_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for training.\"\"\"\r\n        return Tqdm(\r\n            desc=self.train_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_train_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for training.\"\"\"\r\n        return Tqdm(\r\n            desc=self.train_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_train_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "STRING", "return", "Tqdm", "(", "desc", "=", "self", ".", "train_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "smoothing", "=", "0", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for training.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 197, "end_line": 208, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_502", "original_string": "def init_predict_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for predicting.\"\"\"\r\n        return Tqdm(\r\n            desc=self.predict_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_predict_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for predicting.\"\"\"\r\n        return Tqdm(\r\n            desc=self.predict_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_predict_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "STRING", "return", "Tqdm", "(", "desc", "=", "self", ".", "predict_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "smoothing", "=", "0", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for predicting.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "predicting"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 210, "end_line": 221, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_503", "original_string": "def init_validation_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for validation.\"\"\"\r\n        has_main_bar = self.trainer.state.fn != \"validate\"\r\n        return Tqdm(\r\n            desc=self.validation_description,\r\n            position=(2 * self.process_position + has_main_bar),\r\n            disable=self.is_disabled,\r\n            leave=not has_main_bar,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_validation_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for validation.\"\"\"\r\n        has_main_bar = self.trainer.state.fn != \"validate\"\r\n        return Tqdm(\r\n            desc=self.validation_description,\r\n            position=(2 * self.process_position + has_main_bar),\r\n            disable=self.is_disabled,\r\n            leave=not has_main_bar,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_validation_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "STRING", "has_main_bar", "=", "self", ".", "trainer", ".", "state", ".", "fn", "!", "=", "STRING", "return", "Tqdm", "(", "desc", "=", "self", ".", "validation_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", "+", "has_main_bar", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "not", "has_main_bar", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for validation.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 223, "end_line": 235, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_504", "original_string": "def init_test_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for testing.\"\"\"\r\n        return Tqdm(\r\n            desc=\"Testing\",\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_test_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for testing.\"\"\"\r\n        return Tqdm(\r\n            desc=\"Testing\",\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_test_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "STRING", "return", "Tqdm", "(", "desc", "=", "STRING", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for testing.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "testing"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 237, "end_line": 247, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "function_505", "original_string": "def convert_inf(x: Optional[Union[int, float]]) -> Optional[Union[int, float]]:\r\n    \"\"\"The tqdm doesn't support inf/nan values.\r\n\r\n    We have to convert it to None.\r\n\r\n    \"\"\"\r\n    if x is None or math.isinf(x) or math.isnan(x):\r\n        return None\r\n    return x", "language": "python", "code": "def convert_inf(x: Optional[Union[int, float]]) -> Optional[Union[int, float]]:\r\n    \"\"\"The tqdm doesn't support inf/nan values.\r\n\r\n    We have to convert it to None.\r\n\r\n    \"\"\"\r\n    if x is None or math.isinf(x) or math.isnan(x):\r\n        return None\r\n    return x", "code_tokens": ["def", "convert_inf", "(", "x", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", ")", "-", ">", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "if", "x", "is", "None", "or", "math", ".", "isinf", "(", "x", ")", "or", "math", ".", "isnan", "(", "x", ")", ":", "return", "None", "return", "x"], "docstring": "The tqdm doesn't support inf/nan values.", "docstring_tokens": ["the", "tqdm", "doesn", "t", "support", "inf", "nan", "values"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "start_line": 452, "end_line": 460, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_506", "original_string": "def from_datasets(\r\n        cls,\r\n        train_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        val_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        test_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        predict_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        batch_size: int = 1,\r\n        num_workers: int = 0,\r\n        **datamodule_kwargs: Any,\r\n    ) -> \"LightningDataModule\":\r\n        r\"\"\"Create an instance from torch.utils.data.Dataset.\r\n\r\n        Args:\r\n            train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()\r\n            val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()\r\n            test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()\r\n            predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()\r\n            batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            num_workers: Number of subprocesses to use for data loading. 0 means that the\r\n                data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.\r\n\r\n        \"\"\"\r\n\r\n        def dataloader(ds: Dataset, shuffle: bool = False) -> DataLoader:\r\n            shuffle &= not isinstance(ds, IterableDataset)\r\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\r\n\r\n        def train_dataloader() -> TRAIN_DATALOADERS:\r\n            return apply_to_collection(train_dataset, Dataset, dataloader, shuffle=True)\r\n\r\n        def val_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(val_dataset, Dataset, dataloader)\r\n\r\n        def test_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(test_dataset, Dataset, dataloader)\r\n\r\n        def predict_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(predict_dataset, Dataset, dataloader)\r\n\r\n        candidate_kwargs = {\"batch_size\": batch_size, \"num_workers\": num_workers}\r\n        accepted_params = inspect.signature(cls.__init__).parameters\r\n        accepts_kwargs = any(param.kind == param.VAR_KEYWORD for param in accepted_params.values())\r\n        if accepts_kwargs:\r\n            special_kwargs = candidate_kwargs\r\n        else:\r\n            accepted_param_names = set(accepted_params)\r\n            accepted_param_names.discard(\"self\")\r\n            special_kwargs = {k: v for k, v in candidate_kwargs.items() if k in accepted_param_names}\r\n\r\n        datamodule = cls(**datamodule_kwargs, **special_kwargs)\r\n        if train_dataset is not None:\r\n            datamodule.train_dataloader = train_dataloader  # type: ignore[method-assign]\r\n        if val_dataset is not None:\r\n            datamodule.val_dataloader = val_dataloader  # type: ignore[method-assign]\r\n        if test_dataset is not None:\r\n            datamodule.test_dataloader = test_dataloader  # type: ignore[method-assign]\r\n        if predict_dataset is not None:\r\n            datamodule.predict_dataloader = predict_dataloader  # type: ignore[method-assign]\r\n        return datamodule", "language": "python", "code": "def from_datasets(\r\n        cls,\r\n        train_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        val_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        test_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        predict_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        batch_size: int = 1,\r\n        num_workers: int = 0,\r\n        **datamodule_kwargs: Any,\r\n    ) -> \"LightningDataModule\":\r\n        r\"\"\"Create an instance from torch.utils.data.Dataset.\r\n\r\n        Args:\r\n            train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()\r\n            val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()\r\n            test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()\r\n            predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()\r\n            batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            num_workers: Number of subprocesses to use for data loading. 0 means that the\r\n                data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.\r\n\r\n        \"\"\"\r\n\r\n        def dataloader(ds: Dataset, shuffle: bool = False) -> DataLoader:\r\n            shuffle &= not isinstance(ds, IterableDataset)\r\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\r\n\r\n        def train_dataloader() -> TRAIN_DATALOADERS:\r\n            return apply_to_collection(train_dataset, Dataset, dataloader, shuffle=True)\r\n\r\n        def val_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(val_dataset, Dataset, dataloader)\r\n\r\n        def test_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(test_dataset, Dataset, dataloader)\r\n\r\n        def predict_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(predict_dataset, Dataset, dataloader)\r\n\r\n        candidate_kwargs = {\"batch_size\": batch_size, \"num_workers\": num_workers}\r\n        accepted_params = inspect.signature(cls.__init__).parameters\r\n        accepts_kwargs = any(param.kind == param.VAR_KEYWORD for param in accepted_params.values())\r\n        if accepts_kwargs:\r\n            special_kwargs = candidate_kwargs\r\n        else:\r\n            accepted_param_names = set(accepted_params)\r\n            accepted_param_names.discard(\"self\")\r\n            special_kwargs = {k: v for k, v in candidate_kwargs.items() if k in accepted_param_names}\r\n\r\n        datamodule = cls(**datamodule_kwargs, **special_kwargs)\r\n        if train_dataset is not None:\r\n            datamodule.train_dataloader = train_dataloader  # type: ignore[method-assign]\r\n        if val_dataset is not None:\r\n            datamodule.val_dataloader = val_dataloader  # type: ignore[method-assign]\r\n        if test_dataset is not None:\r\n            datamodule.test_dataloader = test_dataloader  # type: ignore[method-assign]\r\n        if predict_dataset is not None:\r\n            datamodule.predict_dataloader = predict_dataloader  # type: ignore[method-assign]\r\n        return datamodule", "code_tokens": ["def", "from_datasets", "(", "cls", ",", "train_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "val_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "test_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "predict_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "batch_size", ":", "int", "=", "1", ",", "num_workers", ":", "int", "=", "0", ",", "*", "*", "datamodule_kwargs", ":", "Any", ",", ")", "-", ">", "STRING", ":", "rSTRING", "def", "dataloader", "(", "ds", ":", "Dataset", ",", "shuffle", ":", "bool", "=", "False", ")", "-", ">", "DataLoader", ":", "shuffle", "&", "=", "not", "isinstance", "(", "ds", ",", "IterableDataset", ")", "return", "DataLoader", "(", "ds", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ")", "def", "train_dataloader", "(", ")", "-", ">", "TRAIN_DATALOADERS", ":", "return", "apply_to_collection", "(", "train_dataset", ",", "Dataset", ",", "dataloader", ",", "shuffle", "=", "True", ")", "def", "val_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "val_dataset", ",", "Dataset", ",", "dataloader", ")", "def", "test_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "test_dataset", ",", "Dataset", ",", "dataloader", ")", "def", "predict_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "predict_dataset", ",", "Dataset", ",", "dataloader", ")", "candidate_kwargs", "=", "{", "STRING", ":", "batch_size", ",", "STRING", ":", "num_workers", "}", "accepted_params", "=", "inspect", ".", "signature", "(", "cls", ".", "__init__", ")", ".", "parameters", "accepts_kwargs", "=", "any", "(", "param", ".", "kind", "=", "=", "param", ".", "VAR_KEYWORD", "for", "param", "in", "accepted_params", ".", "values", "(", ")", ")", "if", "accepts_kwargs", ":", "special_kwargs", "=", "candidate_kwargs", "else", ":", "accepted_param_names", "=", "set", "(", "accepted_params", ")", "accepted_param_names", ".", "discard", "(", "STRING", ")", "special_kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "candidate_kwargs", ".", "items", "(", ")", "if", "k", "in", "accepted_param_names", "}", "datamodule", "=", "cls", "(", "*", "*", "datamodule_kwargs", ",", "*", "*", "special_kwargs", ")", "if", "train_dataset", "is", "not", "None", ":", "datamodule", ".", "train_dataloader", "=", "train_dataloader", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "if", "val_dataset", "is", "not", "None", ":", "datamodule", ".", "val_dataloader", "=", "val_dataloader", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "if", "test_dataset", "is", "not", "None", ":", "datamodule", ".", "test_dataloader", "=", "test_dataloader", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "if", "predict_dataset", "is", "not", "None", ":", "datamodule", ".", "predict_dataloader", "=", "predict_dataloader", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "return", "datamodule"], "docstring": "r\"\"\"Create an instance from torch.utils.data.Dataset.", "docstring_tokens": ["r", "create", "an", "instance", "from", "torch", "utils", "data", "dataset"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 88, "end_line": 149, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_507", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\r\n\r\n        Returns:\r\n            A dictionary containing datamodule state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\r\n\r\n        Returns:\r\n            A dictionary containing datamodule state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate and save datamodule state.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "and", "save", "datamodule", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 151, "end_line": 158, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_508", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.\r\n\r\n        Args:\r\n            state_dict: the datamodule state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.\r\n\r\n        Args:\r\n            state_dict: the datamodule state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "datamodule", "state", "given", "datamodule", "state_dict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 160, "end_line": 167, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_509", "original_string": "def on_exception(self, exception: BaseException) -> None:\r\n        \"\"\"Called when the trainer execution is interrupted by an exception.\"\"\"\r\n        pass", "language": "python", "code": "def on_exception(self, exception: BaseException) -> None:\r\n        \"\"\"Called when the trainer execution is interrupted by an exception.\"\"\"\r\n        pass", "code_tokens": ["def", "on_exception", "(", "self", ",", "exception", ":", "BaseException", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when the trainer execution is interrupted by an exception.", "docstring_tokens": ["called", "when", "the", "trainer", "execution", "is", "interrupted", "by", "an", "exception"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 169, "end_line": 171, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_510", "original_string": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the\r\n        arguments passed to ``__init__``  in the checkpoint under ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningDataModule` for use.\r\n\r\n                If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            \\**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`\r\n            **class** to call it instead of the :class:`LightningDataModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Example::\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                PATH,\r\n                batch_size=32,\r\n                num_workers=10,\r\n            )\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location=map_location,\r\n            hparams_file=hparams_file,\r\n            strict=None,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "language": "python", "code": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the\r\n        arguments passed to ``__init__``  in the checkpoint under ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningDataModule` for use.\r\n\r\n                If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            \\**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`\r\n            **class** to call it instead of the :class:`LightningDataModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Example::\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                PATH,\r\n                batch_size=32,\r\n                num_workers=10,\r\n            )\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location=map_location,\r\n            hparams_file=hparams_file,\r\n            strict=None,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "code_tokens": ["def", "load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ":", "Union", "[", "_PATH", ",", "IO", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "hparams_file", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Self", ":", "rSTRING", "loaded", "=", "_load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ",", "map_location", "=", "map_location", ",", "hparams_file", "=", "hparams_file", ",", "strict", "=", "None", ",", "*", "*", "kwargs", ",", ")", "return", "cast", "(", "Self", ",", "loaded", ")"], "docstring": "r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the", "docstring_tokens": ["r", "primary", "way", "of", "loading", "a", "datamodule", "from", "a", "checkpoint", "when", "lightning", "saves", "a", "checkpoint", "it", "stores", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 174, "end_line": 246, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_511", "original_string": "def __str__(self) -> str:\r\n        \"\"\"Return a string representation of the datasets that are set up.\r\n\r\n        Returns:\r\n            A string representation of the datasets that are setup.\r\n\r\n        \"\"\"\r\n\r\n        class dataset_info:\r\n            def __init__(self, available: bool, length: str) -> None:\r\n                self.available = available\r\n                self.length = length\r\n\r\n        def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)\r\n\r\n        def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)\r\n\r\n        def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info\r\n\r\n        def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)\r\n\r\n        datamodule_loader_methods: list[tuple[str, str]] = [\r\n            (\"Train dataloader\", \"train_dataloader\"),\r\n            (\"Validation dataloader\", \"val_dataloader\"),\r\n            (\"Test dataloader\", \"test_dataloader\"),\r\n            (\"Predict dataloader\", \"predict_dataloader\"),\r\n        ]\r\n\r\n        dataloader_info = extract_loader_info(datamodule_loader_methods)\r\n        dataloader_str = format_loader_info(dataloader_info)\r\n        return dataloader_str", "language": "python", "code": "def __str__(self) -> str:\r\n        \"\"\"Return a string representation of the datasets that are set up.\r\n\r\n        Returns:\r\n            A string representation of the datasets that are setup.\r\n\r\n        \"\"\"\r\n\r\n        class dataset_info:\r\n            def __init__(self, available: bool, length: str) -> None:\r\n                self.available = available\r\n                self.length = length\r\n\r\n        def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)\r\n\r\n        def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)\r\n\r\n        def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info\r\n\r\n        def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)\r\n\r\n        datamodule_loader_methods: list[tuple[str, str]] = [\r\n            (\"Train dataloader\", \"train_dataloader\"),\r\n            (\"Validation dataloader\", \"val_dataloader\"),\r\n            (\"Test dataloader\", \"test_dataloader\"),\r\n            (\"Predict dataloader\", \"predict_dataloader\"),\r\n        ]\r\n\r\n        dataloader_info = extract_loader_info(datamodule_loader_methods)\r\n        dataloader_str = format_loader_info(dataloader_info)\r\n        return dataloader_str", "code_tokens": ["def", "__str__", "(", "self", ")", "-", ">", "str", ":", "STRING", "class", "dataset_info", ":", "def", "__init__", "(", "self", ",", "available", ":", "bool", ",", "length", ":", "str", ")", "-", ">", "None", ":", "self", ".", "available", "=", "available", "self", ".", "length", "=", "length", "def", "retrieve_dataset_info", "(", "loader", ":", "DataLoader", ")", "-", ">", "dataset_info", ":", "STRING", "dataset", "=", "loader", ".", "dataset", "size", ":", "str", "=", "str", "(", "len", "(", "dataset", ")", ")", "if", "isinstance", "(", "dataset", ",", "Sized", ")", "else", "STRING", "return", "dataset_info", "(", "True", ",", "size", ")", "def", "loader_info", "(", "loader", ":", "Union", "[", "DataLoader", ",", "Iterable", "[", "DataLoader", "]", "]", ",", ")", "-", ">", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", ":", "STRING", "return", "apply_to_collection", "(", "loader", ",", "DataLoader", ",", "retrieve_dataset_info", ")", "def", "extract_loader_info", "(", "methods", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", ")", "-", ">", "dict", ":", "STRING", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", "=", "{", "}", "for", "loader_name", ",", "func_name", "in", "methods", ":", "loader_method", "=", "getattr", "(", "self", ",", "func_name", ",", "None", ")", "try", ":", "loader", "=", "loader_method", "(", ")", "#", "type", ":", "ignore", "info", "[", "loader_name", "]", "=", "loader_info", "(", "loader", ")", "except", "Exception", ":", "info", "[", "loader_name", "]", "=", "dataset_info", "(", "False", ",", "STRING", ")", "return", "info", "def", "format_loader_info", "(", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", ")", "-", ">", "str", ":", "STRING", "output", "=", "[", "]", "for", "loader_name", ",", "loader_info", "in", "info", ".", "items", "(", ")", ":", "if", "isinstance", "(", "loader_info", ",", "dataset_info", ")", ":", "loader_info_formatted", "=", "STRING", "if", "not", "loader_info", ".", "available", "else", "fSTRING", "else", ":", "loader_info_formatted", "=", "STRING", ".", "join", "(", "STRING", "if", "not", "loader_info_i", ".", "available", "else", "fSTRING", "for", "i", ",", "loader_info_i", "in", "enumerate", "(", "loader_info", ",", "start", "=", "1", ")", ")", "output", ".", "append", "(", "fSTRING", ")", "return", "os", ".", "linesep", ".", "join", "(", "output", ")", "datamodule_loader_methods", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", "=", "[", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "(", "STRING", ",", "STRING", ")", ",", "]", "dataloader_info", "=", "extract_loader_info", "(", "datamodule_loader_methods", ")", "dataloader_str", "=", "format_loader_info", "(", "dataloader_info", ")", "return", "dataloader_str"], "docstring": "Return a string representation of the datasets that are set up.", "docstring_tokens": ["return", "a", "string", "representation", "of", "the", "datasets", "that", "are", "set", "up"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 248, "end_line": 318, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_512", "original_string": "def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)", "language": "python", "code": "def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)", "code_tokens": ["def", "retrieve_dataset_info", "(", "loader", ":", "DataLoader", ")", "-", ">", "dataset_info", ":", "STRING", "dataset", "=", "loader", ".", "dataset", "size", ":", "str", "=", "str", "(", "len", "(", "dataset", ")", ")", "if", "isinstance", "(", "dataset", ",", "Sized", ")", "else", "STRING", "return", "dataset_info", "(", "True", ",", "size", ")"], "docstring": "Helper function to compute dataset information.", "docstring_tokens": ["helper", "function", "to", "compute", "dataset", "information"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 261, "end_line": 266, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_513", "original_string": "def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)", "language": "python", "code": "def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)", "code_tokens": ["def", "loader_info", "(", "loader", ":", "Union", "[", "DataLoader", ",", "Iterable", "[", "DataLoader", "]", "]", ",", ")", "-", ">", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", ":", "STRING", "return", "apply_to_collection", "(", "loader", ",", "DataLoader", ",", "retrieve_dataset_info", ")"], "docstring": "Helper function to compute dataset information.", "docstring_tokens": ["helper", "function", "to", "compute", "dataset", "information"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 268, "end_line": 272, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_514", "original_string": "def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info", "language": "python", "code": "def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info", "code_tokens": ["def", "extract_loader_info", "(", "methods", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", ")", "-", ">", "dict", ":", "STRING", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", "=", "{", "}", "for", "loader_name", ",", "func_name", "in", "methods", ":", "loader_method", "=", "getattr", "(", "self", ",", "func_name", ",", "None", ")", "try", ":", "loader", "=", "loader_method", "(", ")", "#", "type", ":", "ignore", "info", "[", "loader_name", "]", "=", "loader_info", "(", "loader", ")", "except", "Exception", ":", "info", "[", "loader_name", "]", "=", "dataset_info", "(", "False", ",", "STRING", ")", "return", "info"], "docstring": "Helper function to extract information for each dataloader method.", "docstring_tokens": ["helper", "function", "to", "extract", "information", "for", "each", "dataloader", "method"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 274, "end_line": 286, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "function_515", "original_string": "def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)", "language": "python", "code": "def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)", "code_tokens": ["def", "format_loader_info", "(", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", ")", "-", ">", "str", ":", "STRING", "output", "=", "[", "]", "for", "loader_name", ",", "loader_info", "in", "info", ".", "items", "(", ")", ":", "if", "isinstance", "(", "loader_info", ",", "dataset_info", ")", ":", "loader_info_formatted", "=", "STRING", "if", "not", "loader_info", ".", "available", "else", "fSTRING", "else", ":", "loader_info_formatted", "=", "STRING", ".", "join", "(", "STRING", "if", "not", "loader_info_i", ".", "available", "else", "fSTRING", "for", "i", ",", "loader_info_i", "in", "enumerate", "(", "loader_info", ",", "start", "=", "1", ")", ")", "output", ".", "append", "(", "fSTRING", ")", "return", "os", ".", "linesep", ".", "join", "(", "output", ")"], "docstring": "Helper function to format loader information.", "docstring_tokens": ["helper", "function", "to", "format", "loader", "information"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\datamodule.py", "start_line": 288, "end_line": 304, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_516", "original_string": "def on_fit_start(self) -> None:\r\n        \"\"\"Called at the very beginning of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "language": "python", "code": "def on_fit_start(self) -> None:\r\n        \"\"\"Called at the very beginning of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_fit_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the very beginning of fit.", "docstring_tokens": ["called", "at", "the", "very", "beginning", "of", "fit"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 29, "end_line": 34, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_517", "original_string": "def on_fit_end(self) -> None:\r\n        \"\"\"Called at the very end of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "language": "python", "code": "def on_fit_end(self) -> None:\r\n        \"\"\"Called at the very end of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_fit_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the very end of fit.", "docstring_tokens": ["called", "at", "the", "very", "end", "of", "fit"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 36, "end_line": 41, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_518", "original_string": "def on_train_start(self) -> None:\r\n        \"\"\"Called at the beginning of training after sanity check.\"\"\"", "language": "python", "code": "def on_train_start(self) -> None:\r\n        \"\"\"Called at the beginning of training after sanity check.\"\"\"", "code_tokens": ["def", "on_train_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of training after sanity check.", "docstring_tokens": ["called", "at", "the", "beginning", "of", "training", "after", "sanity", "check"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 43, "end_line": 44, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_519", "original_string": "def on_train_end(self) -> None:\r\n        \"\"\"Called at the end of training before logger experiment is closed.\"\"\"", "language": "python", "code": "def on_train_end(self) -> None:\r\n        \"\"\"Called at the end of training before logger experiment is closed.\"\"\"", "code_tokens": ["def", "on_train_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of training before logger experiment is closed.", "docstring_tokens": ["called", "at", "the", "end", "of", "training", "before", "logger", "experiment", "is", "closed"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 46, "end_line": 47, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_520", "original_string": "def on_validation_start(self) -> None:\r\n        \"\"\"Called at the beginning of validation.\"\"\"", "language": "python", "code": "def on_validation_start(self) -> None:\r\n        \"\"\"Called at the beginning of validation.\"\"\"", "code_tokens": ["def", "on_validation_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of validation.", "docstring_tokens": ["called", "at", "the", "beginning", "of", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 49, "end_line": 50, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_521", "original_string": "def on_validation_end(self) -> None:\r\n        \"\"\"Called at the end of validation.\"\"\"", "language": "python", "code": "def on_validation_end(self) -> None:\r\n        \"\"\"Called at the end of validation.\"\"\"", "code_tokens": ["def", "on_validation_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of validation.", "docstring_tokens": ["called", "at", "the", "end", "of", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 52, "end_line": 53, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_522", "original_string": "def on_test_start(self) -> None:\r\n        \"\"\"Called at the beginning of testing.\"\"\"", "language": "python", "code": "def on_test_start(self) -> None:\r\n        \"\"\"Called at the beginning of testing.\"\"\"", "code_tokens": ["def", "on_test_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of testing.", "docstring_tokens": ["called", "at", "the", "beginning", "of", "testing"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 55, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_523", "original_string": "def on_test_end(self) -> None:\r\n        \"\"\"Called at the end of testing.\"\"\"", "language": "python", "code": "def on_test_end(self) -> None:\r\n        \"\"\"Called at the end of testing.\"\"\"", "code_tokens": ["def", "on_test_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of testing.", "docstring_tokens": ["called", "at", "the", "end", "of", "testing"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 58, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_524", "original_string": "def on_predict_start(self) -> None:\r\n        \"\"\"Called at the beginning of predicting.\"\"\"", "language": "python", "code": "def on_predict_start(self) -> None:\r\n        \"\"\"Called at the beginning of predicting.\"\"\"", "code_tokens": ["def", "on_predict_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of predicting.", "docstring_tokens": ["called", "at", "the", "beginning", "of", "predicting"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 61, "end_line": 62, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_525", "original_string": "def on_predict_end(self) -> None:\r\n        \"\"\"Called at the end of predicting.\"\"\"", "language": "python", "code": "def on_predict_end(self) -> None:\r\n        \"\"\"Called at the end of predicting.\"\"\"", "code_tokens": ["def", "on_predict_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of predicting.", "docstring_tokens": ["called", "at", "the", "end", "of", "predicting"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 64, "end_line": 65, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_526", "original_string": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\r\n\r\n        If you return -1 here, you will skip training for the rest of the current epoch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\r\n\r\n        If you return -1 here, you will skip training for the rest of the current epoch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING"], "docstring": "Called in the training loop before anything happens for that batch.", "docstring_tokens": ["called", "in", "the", "training", "loop", "before", "anything", "happens", "for", "that", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 67, "end_line": 76, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_527", "original_string": "def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of training_step(x)\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of training_step(x)\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the training loop after the batch.", "docstring_tokens": ["called", "in", "the", "training", "loop", "after", "the", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 78, "end_line": 90, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_528", "original_string": "def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the validation loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the validation loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_validation_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the validation loop before anything happens for that batch.", "docstring_tokens": ["called", "in", "the", "validation", "loop", "before", "anything", "happens", "for", "that", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 92, "end_line": 100, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_529", "original_string": "def on_validation_batch_end(\r\n        self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0\r\n    ) -> None:\r\n        \"\"\"Called in the validation loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of validation_step(x)\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_validation_batch_end(\r\n        self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0\r\n    ) -> None:\r\n        \"\"\"Called in the validation loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of validation_step(x)\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_validation_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the validation loop after the batch.", "docstring_tokens": ["called", "in", "the", "validation", "loop", "after", "the", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 102, "end_line": 113, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_530", "original_string": "def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_test_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the test loop before anything happens for that batch.", "docstring_tokens": ["called", "in", "the", "test", "loop", "before", "anything", "happens", "for", "that", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 115, "end_line": 123, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_531", "original_string": "def on_test_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of test_step(x)\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_test_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of test_step(x)\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_test_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the test loop after the batch.", "docstring_tokens": ["called", "in", "the", "test", "loop", "after", "the", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 125, "end_line": 134, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_532", "original_string": "def on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_predict_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the predict loop before anything happens for that batch.", "docstring_tokens": ["called", "in", "the", "predict", "loop", "before", "anything", "happens", "for", "that", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 136, "end_line": 144, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_533", "original_string": "def on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of predict_step(x)\r\n            batch: The batched data as it is returned by the prediction DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of predict_step(x)\r\n            batch: The batched data as it is returned by the prediction DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_predict_batch_end", "(", "self", ",", "outputs", ":", "Optional", "[", "Any", "]", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the predict loop after the batch.", "docstring_tokens": ["called", "in", "the", "predict", "loop", "after", "the", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 146, "end_line": 155, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_534", "original_string": "def on_validation_model_zero_grad(self) -> None:\r\n        \"\"\"Called by the training loop to release gradients before entering the validation loop.\"\"\"\r\n        self.zero_grad()", "language": "python", "code": "def on_validation_model_zero_grad(self) -> None:\r\n        \"\"\"Called by the training loop to release gradients before entering the validation loop.\"\"\"\r\n        self.zero_grad()", "code_tokens": ["def", "on_validation_model_zero_grad", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "zero_grad", "(", ")"], "docstring": "Called by the training loop to release gradients before entering the validation loop.", "docstring_tokens": ["called", "by", "the", "training", "loop", "to", "release", "gradients", "before", "entering", "the", "validation", "loop"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 157, "end_line": 159, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_535", "original_string": "def on_validation_model_eval(self) -> None:\r\n        \"\"\"Called when the validation loop starts.\r\n\r\n        The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_validation_model_eval(self) -> None:\r\n        \"\"\"Called when the validation loop starts.\r\n\r\n        The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_validation_model_eval", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the validation loop starts.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "starts"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 161, "end_line": 168, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_536", "original_string": "def on_validation_model_train(self) -> None:\r\n        \"\"\"Called when the validation loop ends.\r\n\r\n        The validation loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting validation. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.train()", "language": "python", "code": "def on_validation_model_train(self) -> None:\r\n        \"\"\"Called when the validation loop ends.\r\n\r\n        The validation loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting validation. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.train()", "code_tokens": ["def", "on_validation_model_train", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "trainer", ".", "model", ".", "train", "(", ")"], "docstring": "Called when the validation loop ends.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 170, "end_line": 179, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_537", "original_string": "def on_test_model_eval(self) -> None:\r\n        \"\"\"Called when the test loop starts.\r\n\r\n        The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_test_model_eval(self) -> None:\r\n        \"\"\"Called when the test loop starts.\r\n\r\n        The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_test_model_eval", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the test loop starts.", "docstring_tokens": ["called", "when", "the", "test", "loop", "starts"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 181, "end_line": 188, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_538", "original_string": "def on_test_model_train(self) -> None:\r\n        \"\"\"Called when the test loop ends.\r\n\r\n        The test loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting testing. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.train()", "language": "python", "code": "def on_test_model_train(self) -> None:\r\n        \"\"\"Called when the test loop ends.\r\n\r\n        The test loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting testing. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.train()", "code_tokens": ["def", "on_test_model_train", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "trainer", ".", "model", ".", "train", "(", ")"], "docstring": "Called when the test loop ends.", "docstring_tokens": ["called", "when", "the", "test", "loop", "ends"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 190, "end_line": 199, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_539", "original_string": "def on_predict_model_eval(self) -> None:\r\n        \"\"\"Called when the predict loop starts.\r\n\r\n        The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_predict_model_eval(self) -> None:\r\n        \"\"\"Called when the predict loop starts.\r\n\r\n        The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_predict_model_eval", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the predict loop starts.", "docstring_tokens": ["called", "when", "the", "predict", "loop", "starts"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 201, "end_line": 208, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_540", "original_string": "def on_train_epoch_start(self) -> None:\r\n        \"\"\"Called in the training loop at the very beginning of the epoch.\"\"\"", "language": "python", "code": "def on_train_epoch_start(self) -> None:\r\n        \"\"\"Called in the training loop at the very beginning of the epoch.\"\"\"", "code_tokens": ["def", "on_train_epoch_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the training loop at the very beginning of the epoch.", "docstring_tokens": ["called", "in", "the", "training", "loop", "at", "the", "very", "beginning", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 210, "end_line": 211, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_541", "original_string": "def on_train_epoch_end(self) -> None:\r\n        \"\"\"Called in the training loop at the very end of the epoch.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`~lightning.pytorch.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n                def on_train_epoch_end(self):\r\n                    epoch_mean = torch.stack(self.training_step_outputs).mean()\r\n                    self.log(\"training_epoch_mean\", epoch_mean)\r\n                    self.training_step_outputs.clear()\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_epoch_end(self) -> None:\r\n        \"\"\"Called in the training loop at the very end of the epoch.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`~lightning.pytorch.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n                def on_train_epoch_end(self):\r\n                    epoch_mean = torch.stack(self.training_step_outputs).mean()\r\n                    self.log(\"training_epoch_mean\", epoch_mean)\r\n                    self.training_step_outputs.clear()\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the training loop at the very end of the epoch.", "docstring_tokens": ["called", "in", "the", "training", "loop", "at", "the", "very", "end", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 213, "end_line": 238, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_542", "original_string": "def on_validation_epoch_start(self) -> None:\r\n        \"\"\"Called in the validation loop at the very beginning of the epoch.\"\"\"", "language": "python", "code": "def on_validation_epoch_start(self) -> None:\r\n        \"\"\"Called in the validation loop at the very beginning of the epoch.\"\"\"", "code_tokens": ["def", "on_validation_epoch_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the validation loop at the very beginning of the epoch.", "docstring_tokens": ["called", "in", "the", "validation", "loop", "at", "the", "very", "beginning", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 240, "end_line": 241, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_543", "original_string": "def on_validation_epoch_end(self) -> None:\r\n        \"\"\"Called in the validation loop at the very end of the epoch.\"\"\"", "language": "python", "code": "def on_validation_epoch_end(self) -> None:\r\n        \"\"\"Called in the validation loop at the very end of the epoch.\"\"\"", "code_tokens": ["def", "on_validation_epoch_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the validation loop at the very end of the epoch.", "docstring_tokens": ["called", "in", "the", "validation", "loop", "at", "the", "very", "end", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 243, "end_line": 244, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_544", "original_string": "def on_test_epoch_start(self) -> None:\r\n        \"\"\"Called in the test loop at the very beginning of the epoch.\"\"\"", "language": "python", "code": "def on_test_epoch_start(self) -> None:\r\n        \"\"\"Called in the test loop at the very beginning of the epoch.\"\"\"", "code_tokens": ["def", "on_test_epoch_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the test loop at the very beginning of the epoch.", "docstring_tokens": ["called", "in", "the", "test", "loop", "at", "the", "very", "beginning", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 246, "end_line": 247, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_545", "original_string": "def on_test_epoch_end(self) -> None:\r\n        \"\"\"Called in the test loop at the very end of the epoch.\"\"\"", "language": "python", "code": "def on_test_epoch_end(self) -> None:\r\n        \"\"\"Called in the test loop at the very end of the epoch.\"\"\"", "code_tokens": ["def", "on_test_epoch_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called in the test loop at the very end of the epoch.", "docstring_tokens": ["called", "in", "the", "test", "loop", "at", "the", "very", "end", "of", "the", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 249, "end_line": 250, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_546", "original_string": "def on_predict_epoch_start(self) -> None:\r\n        \"\"\"Called at the beginning of predicting.\"\"\"", "language": "python", "code": "def on_predict_epoch_start(self) -> None:\r\n        \"\"\"Called at the beginning of predicting.\"\"\"", "code_tokens": ["def", "on_predict_epoch_start", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of predicting.", "docstring_tokens": ["called", "at", "the", "beginning", "of", "predicting"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 252, "end_line": 253, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_547", "original_string": "def on_predict_epoch_end(self) -> None:\r\n        \"\"\"Called at the end of predicting.\"\"\"", "language": "python", "code": "def on_predict_epoch_end(self) -> None:\r\n        \"\"\"Called at the end of predicting.\"\"\"", "code_tokens": ["def", "on_predict_epoch_end", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of predicting.", "docstring_tokens": ["called", "at", "the", "end", "of", "predicting"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 255, "end_line": 256, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_548", "original_string": "def on_before_zero_grad(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called after ``training_step()`` and before ``optimizer.zero_grad()``.\r\n\r\n        Called in the training loop after taking an optimizer step and before zeroing grads.\r\n        Good place to inspect weight information with weights updated.\r\n\r\n        This is where it is called::\r\n\r\n            for optimizer in optimizers:\r\n                out = training_step(...)\r\n\r\n                model.on_before_zero_grad(optimizer) # < ---- called here\r\n                optimizer.zero_grad()\r\n\r\n                backward()\r\n\r\n        Args:\r\n            optimizer: The optimizer for which grads should be zeroed.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_before_zero_grad(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called after ``training_step()`` and before ``optimizer.zero_grad()``.\r\n\r\n        Called in the training loop after taking an optimizer step and before zeroing grads.\r\n        Good place to inspect weight information with weights updated.\r\n\r\n        This is where it is called::\r\n\r\n            for optimizer in optimizers:\r\n                out = training_step(...)\r\n\r\n                model.on_before_zero_grad(optimizer) # < ---- called here\r\n                optimizer.zero_grad()\r\n\r\n                backward()\r\n\r\n        Args:\r\n            optimizer: The optimizer for which grads should be zeroed.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_before_zero_grad", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called after ``training_step()`` and before ``optimizer.zero_grad()``.", "docstring_tokens": ["called", "after", "training_step", "and", "before", "optimizer", "zero_grad"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 258, "end_line": 277, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_549", "original_string": "def on_before_backward(self, loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\r\n\r\n        Args:\r\n            loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def on_before_backward(self, loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\r\n\r\n        Args:\r\n            loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "on_before_backward", "(", "self", ",", "loss", ":", "Tensor", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called before ``loss.backward()``.", "docstring_tokens": ["called", "before", "loss", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 279, "end_line": 286, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_550", "original_string": "def on_after_backward(self) -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\r\n\r\n        Note:\r\n            If using native AMP, the gradients will not be unscaled at this point.\r\n            Use the ``on_before_optimizer_step`` if you need the unscaled gradients.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_after_backward(self) -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\r\n\r\n        Note:\r\n            If using native AMP, the gradients will not be unscaled at this point.\r\n            Use the ``on_before_optimizer_step`` if you need the unscaled gradients.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_after_backward", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called after ``loss.backward()`` and before optimizers are stepped.", "docstring_tokens": ["called", "after", "loss", "backward", "and", "before", "optimizers", "are", "stepped"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 288, "end_line": 295, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_551", "original_string": "def on_before_optimizer_step(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\r\n\r\n        If using gradient accumulation, the hook is called once the gradients have been accumulated.\r\n        See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\r\n\r\n        If using AMP, the loss will be unscaled before calling this hook.\r\n        See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\r\n        for more information on the scaling of gradients.\r\n\r\n        If clipping gradients, the gradients will not have been clipped yet.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n\r\n        Example::\r\n\r\n            def on_before_optimizer_step(self, optimizer):\r\n                if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\r\n                    for k, v in self.named_parameters():\r\n                        self.logger.experiment.add_histogram(\r\n                            tag=k, values=v.grad, global_step=self.trainer.global_step\r\n                        )\r\n\r\n        \"\"\"", "language": "python", "code": "def on_before_optimizer_step(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\r\n\r\n        If using gradient accumulation, the hook is called once the gradients have been accumulated.\r\n        See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\r\n\r\n        If using AMP, the loss will be unscaled before calling this hook.\r\n        See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\r\n        for more information on the scaling of gradients.\r\n\r\n        If clipping gradients, the gradients will not have been clipped yet.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n\r\n        Example::\r\n\r\n            def on_before_optimizer_step(self, optimizer):\r\n                if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\r\n                    for k, v in self.named_parameters():\r\n                        self.logger.experiment.add_histogram(\r\n                            tag=k, values=v.grad, global_step=self.trainer.global_step\r\n                        )\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_before_optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called before ``optimizer.step()``.", "docstring_tokens": ["called", "before", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 297, "end_line": 322, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_552", "original_string": "def configure_sharded_model(self) -> None:\r\n        \"\"\"Deprecated.\r\n\r\n        Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_sharded_model(self) -> None:\r\n        \"\"\"Deprecated.\r\n\r\n        Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_sharded_model", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Deprecated.", "docstring_tokens": ["deprecated"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 324, "end_line": 329, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_553", "original_string": "def configure_model(self) -> None:\r\n        \"\"\"Hook to create modules in a strategy and precision aware context.\r\n\r\n        This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\r\n        the model instantly to save memory and initialization time.\r\n        For non-sharded strategies, you can choose to override this hook or to initialize your model under the\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\r\n\r\n        This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\r\n        implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\r\n        to it should be a no-op.\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_model(self) -> None:\r\n        \"\"\"Hook to create modules in a strategy and precision aware context.\r\n\r\n        This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\r\n        the model instantly to save memory and initialization time.\r\n        For non-sharded strategies, you can choose to override this hook or to initialize your model under the\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\r\n\r\n        This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\r\n        implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\r\n        to it should be a no-op.\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_model", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Hook to create modules in a strategy and precision aware context.", "docstring_tokens": ["hook", "to", "create", "modules", "in", "a", "strategy", "and", "precision", "aware", "context"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 331, "end_line": 343, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_554", "original_string": "def __init__(self) -> None:\r\n        \"\"\"\r\n        Attributes:\r\n            prepare_data_per_node:\r\n                If True, each LOCAL_RANK=0 will call prepare data.\r\n                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\r\n            allow_zero_length_dataloader_with_multiple_devices:\r\n                If True, dataloader with zero length within local rank is allowed.\r\n                Default value is False.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.prepare_data_per_node: bool = True\r\n        self.allow_zero_length_dataloader_with_multiple_devices: bool = False", "language": "python", "code": "def __init__(self) -> None:\r\n        \"\"\"\r\n        Attributes:\r\n            prepare_data_per_node:\r\n                If True, each LOCAL_RANK=0 will call prepare data.\r\n                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\r\n            allow_zero_length_dataloader_with_multiple_devices:\r\n                If True, dataloader with zero length within local rank is allowed.\r\n                Default value is False.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.prepare_data_per_node: bool = True\r\n        self.allow_zero_length_dataloader_with_multiple_devices: bool = False", "code_tokens": ["def", "__init__", "(", "self", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "prepare_data_per_node", ":", "bool", "=", "True", "self", ".", "allow_zero_length_dataloader_with_multiple_devices", ":", "bool", "=", "False"], "docstring": "Attributes:", "docstring_tokens": ["attributes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 349, "end_line": 361, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_555", "original_string": "def prepare_data(self) -> None:\r\n        \"\"\"Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\r\n        settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\r\n        so you can safely add your downloading logic within.\r\n\r\n        .. warning:: DO NOT set state to the model (use ``setup`` instead)\r\n            since this is NOT called on every device\r\n\r\n        Example::\r\n\r\n            def prepare_data(self):\r\n                download_data()\r\n                tokenize()\r\n                etc()\r\n\r\n                self.split = data_split\r\n                self.some_state = some_other_state()\r\n\r\n        In a distributed environment, ``prepare_data`` can be called in two ways\r\n        (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\r\n\r\n        1. Once per node. This is the default and is only called on LOCAL_RANK=0.\r\n        2. Once in total. Only called on GLOBAL_RANK=0.\r\n\r\n        Example::\r\n\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = True\r\n\r\n\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = False\r\n\r\n        This is called before requesting the dataloaders:\r\n\r\n        .. code-block:: python\r\n\r\n            model.prepare_data()\r\n            initialize_distributed()\r\n            model.setup(stage)\r\n            model.train_dataloader()\r\n            model.val_dataloader()\r\n            model.test_dataloader()\r\n            model.predict_dataloader()\r\n\r\n        \"\"\"", "language": "python", "code": "def prepare_data(self) -> None:\r\n        \"\"\"Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\r\n        settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\r\n        so you can safely add your downloading logic within.\r\n\r\n        .. warning:: DO NOT set state to the model (use ``setup`` instead)\r\n            since this is NOT called on every device\r\n\r\n        Example::\r\n\r\n            def prepare_data(self):\r\n                download_data()\r\n                tokenize()\r\n                etc()\r\n\r\n                self.split = data_split\r\n                self.some_state = some_other_state()\r\n\r\n        In a distributed environment, ``prepare_data`` can be called in two ways\r\n        (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\r\n\r\n        1. Once per node. This is the default and is only called on LOCAL_RANK=0.\r\n        2. Once in total. Only called on GLOBAL_RANK=0.\r\n\r\n        Example::\r\n\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = True\r\n\r\n\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = False\r\n\r\n        This is called before requesting the dataloaders:\r\n\r\n        .. code-block:: python\r\n\r\n            model.prepare_data()\r\n            initialize_distributed()\r\n            model.setup(stage)\r\n            model.train_dataloader()\r\n            model.val_dataloader()\r\n            model.test_dataloader()\r\n            model.predict_dataloader()\r\n\r\n        \"\"\"", "code_tokens": ["def", "prepare_data", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Use this to download and prepare data. Downloading and saving data with multiple processes (distributed", "docstring_tokens": ["use", "this", "to", "download", "and", "prepare", "data", "downloading", "and", "saving", "data", "with", "multiple", "processes", "distributed"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 363, "end_line": 417, "has_examples": true, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_556", "original_string": "def setup(self, stage: str) -> None:\r\n        \"\"\"Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\r\n        need to build models dynamically or adjust something about them. This hook is called on every process when\r\n        using DDP.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        Example::\r\n\r\n            class LitModel(...):\r\n                def __init__(self):\r\n                    self.l1 = None\r\n\r\n                def prepare_data(self):\r\n                    download_data()\r\n                    tokenize()\r\n\r\n                    self.something = else\r\n\r\n                def setup(self, stage):\r\n                    data = load_data(...)\r\n                    self.l1 = nn.Linear(28, data.num_classes)\r\n\r\n        \"\"\"", "language": "python", "code": "def setup(self, stage: str) -> None:\r\n        \"\"\"Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\r\n        need to build models dynamically or adjust something about them. This hook is called on every process when\r\n        using DDP.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        Example::\r\n\r\n            class LitModel(...):\r\n                def __init__(self):\r\n                    self.l1 = None\r\n\r\n                def prepare_data(self):\r\n                    download_data()\r\n                    tokenize()\r\n\r\n                    self.something = else\r\n\r\n                def setup(self, stage):\r\n                    data = load_data(...)\r\n                    self.l1 = nn.Linear(28, data.num_classes)\r\n\r\n        \"\"\"", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you", "docstring_tokens": ["called", "at", "the", "beginning", "of", "fit", "train", "validate", "validate", "test", "or", "predict", "this", "is", "a", "good", "hook", "when", "you"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 419, "end_line": 444, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_557", "original_string": "def teardown(self, stage: str) -> None:\r\n        \"\"\"Called at the end of fit (train + validate), validate, test, or predict.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        \"\"\"", "language": "python", "code": "def teardown(self, stage: str) -> None:\r\n        \"\"\"Called at the end of fit (train + validate), validate, test, or predict.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        \"\"\"", "code_tokens": ["def", "teardown", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called at the end of fit (train + validate), validate, test, or predict.", "docstring_tokens": ["called", "at", "the", "end", "of", "fit", "train", "validate", "validate", "test", "or", "predict"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 446, "end_line": 452, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_558", "original_string": "def train_dataloader(self) -> TRAIN_DATALOADERS:\r\n        \"\"\"An iterable or collection of iterables specifying training samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def train_dataloader(self) -> TRAIN_DATALOADERS:\r\n        \"\"\"An iterable or collection of iterables specifying training samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "TRAIN_DATALOADERS", ":", "STRING", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "An iterable or collection of iterables specifying training samples.", "docstring_tokens": ["an", "iterable", "or", "collection", "of", "iterables", "specifying", "training", "samples"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 454, "end_line": 481, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_559", "original_string": "def test_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying test samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\r\n            this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`test_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def test_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying test samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\r\n            this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`test_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "test_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "rSTRING", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying test samples.", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "test", "samples"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 483, "end_line": 511, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_560", "original_string": "def val_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying validation samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\r\n            implement this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def val_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying validation samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\r\n            implement this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "val_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "rSTRING", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying validation samples.", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 513, "end_line": 538, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_561", "original_string": "def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying prediction samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Return:\r\n            A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\r\n            \"`predict_dataloader` must be implemented to be used with the Lightning Trainer\"\r\n        )", "language": "python", "code": "def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying prediction samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Return:\r\n            A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\r\n            \"`predict_dataloader` must be implemented to be used with the Lightning Trainer\"\r\n        )", "code_tokens": ["def", "predict_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "rSTRING", "raise", "MisconfigurationException", "(", "STRING", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying prediction samples.", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "prediction", "samples"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 540, "end_line": 561, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_562", "original_string": "def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\r\n        \"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\r\n        structure.\r\n\r\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\r\n\r\n        - :class:`torch.Tensor` or anything that implements `.to(...)`\r\n        - :class:`list`\r\n        - :class:`dict`\r\n        - :class:`tuple`\r\n\r\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\r\n\r\n        Note:\r\n            This hook should only transfer the data and not modify it, nor should it move the data to\r\n            any other device than the one passed in as argument (unless you know what you are doing).\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be transferred to a new device.\r\n            device: The target device as defined in PyTorch.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A reference to the data on the new device.\r\n\r\n        Example::\r\n\r\n            def transfer_batch_to_device(self, batch, device, dataloader_idx):\r\n                if isinstance(batch, CustomBatch):\r\n                    batch.samples = batch.samples.to(device)\r\n                    batch.targets = batch.targets.to(device)\r\n                elif dataloader_idx == 0:\r\n                    pass\r\n                else:\r\n                    batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`move_data_to_device`\r\n            - :meth:`apply_to_collection`\r\n\r\n        \"\"\"\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\r\n        \"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\r\n        structure.\r\n\r\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\r\n\r\n        - :class:`torch.Tensor` or anything that implements `.to(...)`\r\n        - :class:`list`\r\n        - :class:`dict`\r\n        - :class:`tuple`\r\n\r\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\r\n\r\n        Note:\r\n            This hook should only transfer the data and not modify it, nor should it move the data to\r\n            any other device than the one passed in as argument (unless you know what you are doing).\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be transferred to a new device.\r\n            device: The target device as defined in PyTorch.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A reference to the data on the new device.\r\n\r\n        Example::\r\n\r\n            def transfer_batch_to_device(self, batch, device, dataloader_idx):\r\n                if isinstance(batch, CustomBatch):\r\n                    batch.samples = batch.samples.to(device)\r\n                    batch.targets = batch.targets.to(device)\r\n                elif dataloader_idx == 0:\r\n                    pass\r\n                else:\r\n                    batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`move_data_to_device`\r\n            - :meth:`apply_to_collection`\r\n\r\n        \"\"\"\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "transfer_batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "torch", ".", "device", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "STRING", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data", "docstring_tokens": ["override", "this", "hook", "if", "your", "class", "torch", "utils", "data", "dataloader", "returns", "tensors", "wrapped", "in", "a", "custom", "data"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 563, "end_line": 610, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_563", "original_string": "def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch before it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_before_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_after_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "language": "python", "code": "def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch before it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_before_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_after_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "code_tokens": ["def", "on_before_batch_transfer", "(", "self", ",", "batch", ":", "Any", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "STRING", "return", "batch"], "docstring": "Override to alter or apply batch augmentations to your batch before it is transferred to the device.", "docstring_tokens": ["override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "before", "it", "is", "transferred", "to", "the", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 612, "end_line": 638, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_564", "original_string": "def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch after it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_after_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = gpu_transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_before_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "language": "python", "code": "def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch after it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_after_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = gpu_transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_before_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "code_tokens": ["def", "on_after_batch_transfer", "(", "self", ",", "batch", ":", "Any", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "STRING", "return", "batch"], "docstring": "Override to alter or apply batch augmentations to your batch after it is transferred to the device.", "docstring_tokens": ["override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "after", "it", "is", "transferred", "to", "the", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 640, "end_line": 666, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_565", "original_string": "def on_load_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\r\n        your chance to restore this.\r\n\r\n        Args:\r\n            checkpoint: Loaded checkpoint\r\n\r\n        Example::\r\n\r\n            def on_load_checkpoint(self, checkpoint):\r\n                self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\r\n\r\n        Note:\r\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\r\n            There is no need for you to restore anything regarding training.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_load_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\r\n        your chance to restore this.\r\n\r\n        Args:\r\n            checkpoint: Loaded checkpoint\r\n\r\n        Example::\r\n\r\n            def on_load_checkpoint(self, checkpoint):\r\n                self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\r\n\r\n        Note:\r\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\r\n            There is no need for you to restore anything regarding training.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING"], "docstring": "r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is", "docstring_tokens": ["r", "called", "by", "lightning", "to", "restore", "your", "model", "if", "you", "saved", "something", "with", "meth", "on_save_checkpoint", "this", "is"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 672, "end_line": 689, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "function_566", "original_string": "def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\r\n        save.\r\n\r\n        Args:\r\n            checkpoint: The full checkpoint dictionary before it gets dumped to a file.\r\n                Implementations of this hook can insert additional data into this dictionary.\r\n\r\n        Example::\r\n\r\n            def on_save_checkpoint(self, checkpoint):\r\n                checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\r\n\r\n        Note:\r\n            Lightning saves all aspects of training (epoch, global step, etc...)\r\n            including amp scaling.\r\n            There is no need for you to store anything about training.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\r\n        save.\r\n\r\n        Args:\r\n            checkpoint: The full checkpoint dictionary before it gets dumped to a file.\r\n                Implementations of this hook can insert additional data into this dictionary.\r\n\r\n        Example::\r\n\r\n            def on_save_checkpoint(self, checkpoint):\r\n                checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\r\n\r\n        Note:\r\n            Lightning saves all aspects of training (epoch, global step, etc...)\r\n            including amp scaling.\r\n            There is no need for you to store anything about training.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "rSTRING"], "docstring": "r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to", "docstring_tokens": ["r", "called", "by", "lightning", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\hooks.py", "start_line": 691, "end_line": 710, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_567", "original_string": "def optimizers(self, use_pl_optimizer: bool = True) -> MODULE_OPTIMIZERS:\r\n        \"\"\"Returns the optimizer(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Args:\r\n            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\r\n                :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\r\n                profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\r\n                ``step`` method and custom optimizers that don't have this method are not supported.\r\n\r\n        Returns:\r\n            A single optimizer, or a list of optimizers in case multiple ones are present.\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            opts: MODULE_OPTIMIZERS = self._fabric_optimizers\r\n        elif use_pl_optimizer:\r\n            opts = self.trainer.strategy._lightning_optimizers\r\n        else:\r\n            opts = self.trainer.optimizers\r\n\r\n        if (\r\n            isinstance(opts, list)\r\n            and len(opts) == 1\r\n            and isinstance(opts[0], (Optimizer, LightningOptimizer, _FabricOptimizer))\r\n        ):\r\n            return opts[0]\r\n        return opts", "language": "python", "code": "def optimizers(self, use_pl_optimizer: bool = True) -> MODULE_OPTIMIZERS:\r\n        \"\"\"Returns the optimizer(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Args:\r\n            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\r\n                :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\r\n                profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\r\n                ``step`` method and custom optimizers that don't have this method are not supported.\r\n\r\n        Returns:\r\n            A single optimizer, or a list of optimizers in case multiple ones are present.\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            opts: MODULE_OPTIMIZERS = self._fabric_optimizers\r\n        elif use_pl_optimizer:\r\n            opts = self.trainer.strategy._lightning_optimizers\r\n        else:\r\n            opts = self.trainer.optimizers\r\n\r\n        if (\r\n            isinstance(opts, list)\r\n            and len(opts) == 1\r\n            and isinstance(opts[0], (Optimizer, LightningOptimizer, _FabricOptimizer))\r\n        ):\r\n            return opts[0]\r\n        return opts", "code_tokens": ["def", "optimizers", "(", "self", ",", "use_pl_optimizer", ":", "bool", "=", "True", ")", "-", ">", "MODULE_OPTIMIZERS", ":", "STRING", "if", "self", ".", "_fabric", ":", "opts", ":", "MODULE_OPTIMIZERS", "=", "self", ".", "_fabric_optimizers", "elif", "use_pl_optimizer", ":", "opts", "=", "self", ".", "trainer", ".", "strategy", ".", "_lightning_optimizers", "else", ":", "opts", "=", "self", ".", "trainer", ".", "optimizers", "if", "(", "isinstance", "(", "opts", ",", "list", ")", "and", "len", "(", "opts", ")", "=", "=", "1", "and", "isinstance", "(", "opts", "[", "0", "]", ",", "(", "Optimizer", ",", "LightningOptimizer", ",", "_FabricOptimizer", ")", ")", ")", ":", "return", "opts", "[", "0", "]", "return", "opts"], "docstring": "Returns the optimizer(s) that are being used during training. Useful for manual optimization.", "docstring_tokens": ["returns", "the", "optimizer", "s", "that", "are", "being", "used", "during", "training", "useful", "for", "manual", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 167, "end_line": 195, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_568", "original_string": "def lr_schedulers(self) -> Union[None, list[LRSchedulerPLType], LRSchedulerPLType]:\r\n        \"\"\"Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Returns:\r\n            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\r\n            schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        if not self.trainer.lr_scheduler_configs:\r\n            return None\r\n\r\n        lr_schedulers: list[LRSchedulerPLType] = [config.scheduler for config in self.trainer.lr_scheduler_configs]\r\n\r\n        if len(lr_schedulers) == 1:\r\n            return lr_schedulers[0]\r\n\r\n        return lr_schedulers", "language": "python", "code": "def lr_schedulers(self) -> Union[None, list[LRSchedulerPLType], LRSchedulerPLType]:\r\n        \"\"\"Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Returns:\r\n            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\r\n            schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        if not self.trainer.lr_scheduler_configs:\r\n            return None\r\n\r\n        lr_schedulers: list[LRSchedulerPLType] = [config.scheduler for config in self.trainer.lr_scheduler_configs]\r\n\r\n        if len(lr_schedulers) == 1:\r\n            return lr_schedulers[0]\r\n\r\n        return lr_schedulers", "code_tokens": ["def", "lr_schedulers", "(", "self", ")", "-", ">", "Union", "[", "None", ",", "list", "[", "LRSchedulerPLType", "]", ",", "LRSchedulerPLType", "]", ":", "STRING", "if", "not", "self", ".", "trainer", ".", "lr_scheduler_configs", ":", "return", "None", "lr_schedulers", ":", "list", "[", "LRSchedulerPLType", "]", "=", "[", "config", ".", "scheduler", "for", "config", "in", "self", ".", "trainer", ".", "lr_scheduler_configs", "]", "if", "len", "(", "lr_schedulers", ")", "=", "=", "1", ":", "return", "lr_schedulers", "[", "0", "]", "return", "lr_schedulers"], "docstring": "Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.", "docstring_tokens": ["returns", "the", "learning", "rate", "scheduler", "s", "that", "are", "being", "used", "during", "training", "useful", "for", "manual", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 197, "end_line": 216, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_569", "original_string": "def example_input_array(self) -> Optional[Union[Tensor, tuple, dict]]:\r\n        \"\"\"The example input array is a specification of what the module can consume in the :meth:`forward` method. The\r\n        return type is interpreted as follows:\r\n\r\n        -   Single tensor: It is assumed the model takes a single argument, i.e.,\r\n            ``model.forward(model.example_input_array)``\r\n        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\r\n            ``model.forward(*model.example_input_array)``\r\n        -   Dict: The input array represents named keyword arguments, i.e.,\r\n            ``model.forward(**model.example_input_array)``\r\n\r\n        \"\"\"\r\n        return self._example_input_array", "language": "python", "code": "def example_input_array(self) -> Optional[Union[Tensor, tuple, dict]]:\r\n        \"\"\"The example input array is a specification of what the module can consume in the :meth:`forward` method. The\r\n        return type is interpreted as follows:\r\n\r\n        -   Single tensor: It is assumed the model takes a single argument, i.e.,\r\n            ``model.forward(model.example_input_array)``\r\n        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\r\n            ``model.forward(*model.example_input_array)``\r\n        -   Dict: The input array represents named keyword arguments, i.e.,\r\n            ``model.forward(**model.example_input_array)``\r\n\r\n        \"\"\"\r\n        return self._example_input_array", "code_tokens": ["def", "example_input_array", "(", "self", ")", "-", ">", "Optional", "[", "Union", "[", "Tensor", ",", "tuple", ",", "dict", "]", "]", ":", "STRING", "return", "self", ".", "_example_input_array"], "docstring": "The example input array is a specification of what the module can consume in the :meth:`forward` method. The", "docstring_tokens": ["the", "example", "input", "array", "is", "a", "specification", "of", "what", "the", "module", "can", "consume", "in", "the", "meth", "forward", "method", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 247, "end_line": 259, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_570", "original_string": "def current_epoch(self) -> int:\r\n        \"\"\"The current epoch in the ``Trainer``, or 0 if not attached.\"\"\"\r\n        return self.trainer.current_epoch if self._trainer else 0", "language": "python", "code": "def current_epoch(self) -> int:\r\n        \"\"\"The current epoch in the ``Trainer``, or 0 if not attached.\"\"\"\r\n        return self.trainer.current_epoch if self._trainer else 0", "code_tokens": ["def", "current_epoch", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "trainer", ".", "current_epoch", "if", "self", ".", "_trainer", "else", "0"], "docstring": "The current epoch in the ``Trainer``, or 0 if not attached.", "docstring_tokens": ["the", "current", "epoch", "in", "the", "trainer", "or", "0", "if", "not", "attached"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 266, "end_line": 268, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_571", "original_string": "def global_step(self) -> int:\r\n        \"\"\"Total training batches seen across all epochs.\r\n\r\n        If no Trainer is attached, this property is 0.\r\n\r\n        \"\"\"\r\n        return self.trainer.global_step if self._trainer else 0", "language": "python", "code": "def global_step(self) -> int:\r\n        \"\"\"Total training batches seen across all epochs.\r\n\r\n        If no Trainer is attached, this property is 0.\r\n\r\n        \"\"\"\r\n        return self.trainer.global_step if self._trainer else 0", "code_tokens": ["def", "global_step", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "trainer", ".", "global_step", "if", "self", ".", "_trainer", "else", "0"], "docstring": "Total training batches seen across all epochs.", "docstring_tokens": ["total", "training", "batches", "seen", "across", "all", "epochs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 271, "end_line": 277, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_572", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The index of the current process across all nodes and devices.\"\"\"\r\n        return self.trainer.global_rank if self._trainer else 0", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The index of the current process across all nodes and devices.\"\"\"\r\n        return self.trainer.global_rank if self._trainer else 0", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "trainer", ".", "global_rank", "if", "self", ".", "_trainer", "else", "0"], "docstring": "The index of the current process across all nodes and devices.", "docstring_tokens": ["the", "index", "of", "the", "current", "process", "across", "all", "nodes", "and", "devices"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 280, "end_line": 282, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_573", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The index of the current process within a single node.\"\"\"\r\n        return self.trainer.local_rank if self._trainer else 0", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The index of the current process within a single node.\"\"\"\r\n        return self.trainer.local_rank if self._trainer else 0", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "trainer", ".", "local_rank", "if", "self", ".", "_trainer", "else", "0"], "docstring": "The index of the current process within a single node.", "docstring_tokens": ["the", "index", "of", "the", "current", "process", "within", "a", "single", "node"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 285, "end_line": 287, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_574", "original_string": "def on_gpu(self) -> bool:\r\n        \"\"\"Returns ``True`` if this model is currently located on a GPU.\r\n\r\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.\r\n\r\n        \"\"\"\r\n        return self.device.type == \"cuda\"", "language": "python", "code": "def on_gpu(self) -> bool:\r\n        \"\"\"Returns ``True`` if this model is currently located on a GPU.\r\n\r\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.\r\n\r\n        \"\"\"\r\n        return self.device.type == \"cuda\"", "code_tokens": ["def", "on_gpu", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "device", ".", "type", "=", "=", "STRING"], "docstring": "Returns ``True`` if this model is currently located on a GPU.", "docstring_tokens": ["returns", "true", "if", "this", "model", "is", "currently", "located", "on", "a", "gpu"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 290, "end_line": 296, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_575", "original_string": "def automatic_optimization(self) -> bool:\r\n        \"\"\"If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.\"\"\"\r\n        return self._automatic_optimization", "language": "python", "code": "def automatic_optimization(self) -> bool:\r\n        \"\"\"If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.\"\"\"\r\n        return self._automatic_optimization", "code_tokens": ["def", "automatic_optimization", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_automatic_optimization"], "docstring": "If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.", "docstring_tokens": ["if", "set", "to", "false", "you", "are", "responsible", "for", "calling", "backward", "step", "zero_grad"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 299, "end_line": 301, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_576", "original_string": "def strict_loading(self) -> bool:\r\n        \"\"\"Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.\"\"\"\r\n        return self._strict_loading in (None, True)", "language": "python", "code": "def strict_loading(self) -> bool:\r\n        \"\"\"Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.\"\"\"\r\n        return self._strict_loading in (None, True)", "code_tokens": ["def", "strict_loading", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_strict_loading", "in", "(", "None", ",", "True", ")"], "docstring": "Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.", "docstring_tokens": ["determines", "how", "lightning", "loads", "this", "model", "using", "load_state_dict", "strict", "model", "strict_loading"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 308, "end_line": 311, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_577", "original_string": "def logger(self) -> Optional[Union[Logger, FabricLogger]]:\r\n        \"\"\"Reference to the logger object in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.logger\r\n        return self._trainer.logger if self._trainer is not None else None", "language": "python", "code": "def logger(self) -> Optional[Union[Logger, FabricLogger]]:\r\n        \"\"\"Reference to the logger object in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.logger\r\n        return self._trainer.logger if self._trainer is not None else None", "code_tokens": ["def", "logger", "(", "self", ")", "-", ">", "Optional", "[", "Union", "[", "Logger", ",", "FabricLogger", "]", "]", ":", "STRING", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_fabric", ".", "logger", "return", "self", ".", "_trainer", ".", "logger", "if", "self", ".", "_trainer", "is", "not", "None", "else", "None"], "docstring": "Reference to the logger object in the Trainer.", "docstring_tokens": ["reference", "to", "the", "logger", "object", "in", "the", "trainer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 318, "end_line": 322, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_578", "original_string": "def loggers(self) -> Union[list[Logger], list[FabricLogger]]:\r\n        \"\"\"Reference to the list of loggers in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.loggers\r\n        if self._trainer is not None:\r\n            return self._trainer.loggers\r\n        return []", "language": "python", "code": "def loggers(self) -> Union[list[Logger], list[FabricLogger]]:\r\n        \"\"\"Reference to the list of loggers in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.loggers\r\n        if self._trainer is not None:\r\n            return self._trainer.loggers\r\n        return []", "code_tokens": ["def", "loggers", "(", "self", ")", "-", ">", "Union", "[", "list", "[", "Logger", "]", ",", "list", "[", "FabricLogger", "]", "]", ":", "STRING", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_fabric", ".", "loggers", "if", "self", ".", "_trainer", "is", "not", "None", ":", "return", "self", ".", "_trainer", ".", "loggers", "return", "[", "]"], "docstring": "Reference to the list of loggers in the Trainer.", "docstring_tokens": ["reference", "to", "the", "list", "of", "loggers", "in", "the", "trainer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 325, "end_line": 331, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_579", "original_string": "def device_mesh(self) -> Optional[\"DeviceMesh\"]:\r\n        \"\"\"Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule.\"\"\"\r\n        return self._device_mesh", "language": "python", "code": "def device_mesh(self) -> Optional[\"DeviceMesh\"]:\r\n        \"\"\"Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule.\"\"\"\r\n        return self._device_mesh", "code_tokens": ["def", "device_mesh", "(", "self", ")", "-", ">", "Optional", "[", "STRING", "]", ":", "STRING", "return", "self", ".", "_device_mesh"], "docstring": "Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the", "docstring_tokens": ["strategies", "like", "modelparallelstrategy", "will", "create", "a", "device", "mesh", "that", "can", "be", "accessed", "in", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 334, "end_line": 337, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_580", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.\r\n\r\n        Args:\r\n            *args: The thing to print. The same as for Python's built-in print function.\r\n            **kwargs: The same as for Python's built-in print function.\r\n\r\n        Example::\r\n\r\n            def forward(self, x):\r\n                self.print(x, 'in forward')\r\n\r\n        \"\"\"\r\n        if self.trainer.is_global_zero:\r\n            progress_bar = self.trainer.progress_bar_callback\r\n            if progress_bar is not None and progress_bar.is_enabled:\r\n                progress_bar.print(*args, **kwargs)\r\n            else:\r\n                print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.\r\n\r\n        Args:\r\n            *args: The thing to print. The same as for Python's built-in print function.\r\n            **kwargs: The same as for Python's built-in print function.\r\n\r\n        Example::\r\n\r\n            def forward(self, x):\r\n                self.print(x, 'in forward')\r\n\r\n        \"\"\"\r\n        if self.trainer.is_global_zero:\r\n            progress_bar = self.trainer.progress_bar_callback\r\n            if progress_bar is not None and progress_bar.is_enabled:\r\n                progress_bar.print(*args, **kwargs)\r\n            else:\r\n                print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "if", "self", ".", "trainer", ".", "is_global_zero", ":", "progress_bar", "=", "self", ".", "trainer", ".", "progress_bar_callback", "if", "progress_bar", "is", "not", "None", "and", "progress_bar", ".", "is_enabled", ":", "progress_bar", ".", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.", "docstring_tokens": ["r", "prints", "only", "from", "process", "0", "use", "this", "in", "any", "distributed", "mode", "to", "log", "only", "once"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 365, "end_line": 383, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_581", "original_string": "def log(\r\n        self,\r\n        name: str,\r\n        value: _METRIC,\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a key, value pair.\r\n\r\n        Example::\r\n\r\n            self.log('train_loss', loss)\r\n\r\n        The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.\r\n\r\n        Args:\r\n            name: key to log. Must be identical across all processes if using DDP or any other distributed strategy.\r\n            value: value to log. Can be a ``float``, ``Tensor``, or a ``Metric``.\r\n            prog_bar: if ``True`` logs to the progress bar.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step. The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto detach the graph.\r\n            sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the DDP group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple dataloaders). If False, user needs to give unique names for\r\n                each dataloader to not mix the values.\r\n            batch_size: Current batch_size. This will be directly inferred from the loaded batch,\r\n                but for some data structures you might need to explicitly provide it.\r\n            metric_attribute: To restore the metric state, Lightning requires the reference of the\r\n                :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            self._log_dict_through_fabric(dictionary={name: value}, logger=logger)\r\n            return\r\n\r\n        apply_to_collection(value, dict, self.__check_not_nested, name)\r\n        apply_to_collection(\r\n            value, object, self.__check_allowed, name, value, wrong_dtype=(numbers.Number, Metric, Tensor)\r\n        )\r\n\r\n        trainer = self._trainer\r\n        if trainer is None:\r\n            rank_zero_warn(\r\n                \"You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.\"\r\n                \" This is most likely because the model hasn't been passed to the `Trainer`\"\r\n            )\r\n            return\r\n        if trainer.barebones:\r\n            rank_zero_warn(\r\n                \"You are trying to `self.log()` but `Trainer(barebones=True)` is configured.\"\r\n                \" Logging can impact raw speed so it is disabled under this setting.\"\r\n            )\r\n            return\r\n        results = trainer._results\r\n        if results is None:\r\n            raise MisconfigurationException(\r\n                \"You are trying to `self.log()` but the loop's result collection is not registered\"\r\n                \" yet. This is most likely because you are trying to log in a `predict` hook,\"\r\n                \" but it doesn't support logging\"\r\n            )\r\n        if self._current_fx_name is None:\r\n            raise MisconfigurationException(\r\n                \"You are trying to `self.log()` but it is not managed by the `Trainer` control flow\"\r\n            )\r\n\r\n        on_step, on_epoch = _FxValidator.check_logging_and_get_default_levels(\r\n            self._current_fx_name, on_step=on_step, on_epoch=on_epoch\r\n        )\r\n\r\n        if add_dataloader_idx and \"/dataloader_idx_\" in name:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log` with the key `{name}`\"\r\n                \" but it should not contain information about `dataloader_idx` when `add_dataloader_idx=True`\"\r\n            )\r\n\r\n        value = apply_to_collection(value, (Tensor, numbers.Number), self.__to_tensor, name)\r\n\r\n        if trainer._logger_connector.should_reset_tensors(self._current_fx_name):\r\n            results.reset(metrics=False, fx=self._current_fx_name)\r\n\r\n        if metric_attribute is None and isinstance(value, Metric):\r\n            if self._metric_attributes is None:\r\n                self._metric_attributes = {\r\n                    id(module): name for name, module in self.named_modules() if isinstance(module, Metric)\r\n                }\r\n                if not self._metric_attributes:\r\n                    raise MisconfigurationException(\r\n                        \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n                        \" You can fix this by setting an attribute for the metric in your `LightningModule`.\"\r\n                    )\r\n            metric_attribute = self._metric_attributes.get(id(value), None)\r\n            if metric_attribute is None:\r\n                raise MisconfigurationException(\r\n                    \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n                    f\" You can fix this by calling `self.log({name}, ..., metric_attribute=name)` where `name` is one\"\r\n                    f\" of {list(self._metric_attributes.values())}\"\r\n                )\r\n\r\n        if (\r\n            trainer.training\r\n            and is_param_in_hook_signature(self.training_step, \"dataloader_iter\", explicit=True)\r\n            and batch_size is None\r\n        ):\r\n            raise MisconfigurationException(\r\n                \"With `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.\"\r\n            )\r\n\r\n        if logger and trainer.logger is None:\r\n            rank_zero_warn(\r\n                f\"You called `self.log({name!r}, ..., logger=True)` but have no logger configured. You can enable one\"\r\n                \" by doing `Trainer(logger=ALogger(...))`\"\r\n            )\r\n        if logger is None:\r\n            logger = True\r\n\r\n        results.log(\r\n            self._current_fx_name,\r\n            name,\r\n            value,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            batch_size=batch_size,\r\n            sync_dist=sync_dist and trainer._accelerator_connector.is_distributed,\r\n            sync_dist_fn=trainer.strategy.reduce,\r\n            sync_dist_group=sync_dist_group,\r\n            metric_attribute=metric_attribute,\r\n            rank_zero_only=rank_zero_only,\r\n        )\r\n\r\n        trainer._logger_connector._current_fx = self._current_fx_name", "language": "python", "code": "def log(\r\n        self,\r\n        name: str,\r\n        value: _METRIC,\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a key, value pair.\r\n\r\n        Example::\r\n\r\n            self.log('train_loss', loss)\r\n\r\n        The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.\r\n\r\n        Args:\r\n            name: key to log. Must be identical across all processes if using DDP or any other distributed strategy.\r\n            value: value to log. Can be a ``float``, ``Tensor``, or a ``Metric``.\r\n            prog_bar: if ``True`` logs to the progress bar.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step. The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto detach the graph.\r\n            sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the DDP group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple dataloaders). If False, user needs to give unique names for\r\n                each dataloader to not mix the values.\r\n            batch_size: Current batch_size. This will be directly inferred from the loaded batch,\r\n                but for some data structures you might need to explicitly provide it.\r\n            metric_attribute: To restore the metric state, Lightning requires the reference of the\r\n                :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            self._log_dict_through_fabric(dictionary={name: value}, logger=logger)\r\n            return\r\n\r\n        apply_to_collection(value, dict, self.__check_not_nested, name)\r\n        apply_to_collection(\r\n            value, object, self.__check_allowed, name, value, wrong_dtype=(numbers.Number, Metric, Tensor)\r\n        )\r\n\r\n        trainer = self._trainer\r\n        if trainer is None:\r\n            rank_zero_warn(\r\n                \"You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.\"\r\n                \" This is most likely because the model hasn't been passed to the `Trainer`\"\r\n            )\r\n            return\r\n        if trainer.barebones:\r\n            rank_zero_warn(\r\n                \"You are trying to `self.log()` but `Trainer(barebones=True)` is configured.\"\r\n                \" Logging can impact raw speed so it is disabled under this setting.\"\r\n            )\r\n            return\r\n        results = trainer._results\r\n        if results is None:\r\n            raise MisconfigurationException(\r\n                \"You are trying to `self.log()` but the loop's result collection is not registered\"\r\n                \" yet. This is most likely because you are trying to log in a `predict` hook,\"\r\n                \" but it doesn't support logging\"\r\n            )\r\n        if self._current_fx_name is None:\r\n            raise MisconfigurationException(\r\n                \"You are trying to `self.log()` but it is not managed by the `Trainer` control flow\"\r\n            )\r\n\r\n        on_step, on_epoch = _FxValidator.check_logging_and_get_default_levels(\r\n            self._current_fx_name, on_step=on_step, on_epoch=on_epoch\r\n        )\r\n\r\n        if add_dataloader_idx and \"/dataloader_idx_\" in name:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log` with the key `{name}`\"\r\n                \" but it should not contain information about `dataloader_idx` when `add_dataloader_idx=True`\"\r\n            )\r\n\r\n        value = apply_to_collection(value, (Tensor, numbers.Number), self.__to_tensor, name)\r\n\r\n        if trainer._logger_connector.should_reset_tensors(self._current_fx_name):\r\n            results.reset(metrics=False, fx=self._current_fx_name)\r\n\r\n        if metric_attribute is None and isinstance(value, Metric):\r\n            if self._metric_attributes is None:\r\n                self._metric_attributes = {\r\n                    id(module): name for name, module in self.named_modules() if isinstance(module, Metric)\r\n                }\r\n                if not self._metric_attributes:\r\n                    raise MisconfigurationException(\r\n                        \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n                        \" You can fix this by setting an attribute for the metric in your `LightningModule`.\"\r\n                    )\r\n            metric_attribute = self._metric_attributes.get(id(value), None)\r\n            if metric_attribute is None:\r\n                raise MisconfigurationException(\r\n                    \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n                    f\" You can fix this by calling `self.log({name}, ..., metric_attribute=name)` where `name` is one\"\r\n                    f\" of {list(self._metric_attributes.values())}\"\r\n                )\r\n\r\n        if (\r\n            trainer.training\r\n            and is_param_in_hook_signature(self.training_step, \"dataloader_iter\", explicit=True)\r\n            and batch_size is None\r\n        ):\r\n            raise MisconfigurationException(\r\n                \"With `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.\"\r\n            )\r\n\r\n        if logger and trainer.logger is None:\r\n            rank_zero_warn(\r\n                f\"You called `self.log({name!r}, ..., logger=True)` but have no logger configured. You can enable one\"\r\n                \" by doing `Trainer(logger=ALogger(...))`\"\r\n            )\r\n        if logger is None:\r\n            logger = True\r\n\r\n        results.log(\r\n            self._current_fx_name,\r\n            name,\r\n            value,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            batch_size=batch_size,\r\n            sync_dist=sync_dist and trainer._accelerator_connector.is_distributed,\r\n            sync_dist_fn=trainer.strategy.reduce,\r\n            sync_dist_group=sync_dist_group,\r\n            metric_attribute=metric_attribute,\r\n            rank_zero_only=rank_zero_only,\r\n        )\r\n\r\n        trainer._logger_connector._current_fx = self._current_fx_name", "code_tokens": ["def", "log", "(", "self", ",", "name", ":", "str", ",", "value", ":", "_METRIC", ",", "prog_bar", ":", "bool", "=", "False", ",", "logger", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_step", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_epoch", ":", "Optional", "[", "bool", "]", "=", "None", ",", "reduce_fx", ":", "Union", "[", "str", ",", "Callable", "[", "[", "Any", "]", ",", "Any", "]", "]", "=", "STRING", ",", "enable_graph", ":", "bool", "=", "False", ",", "sync_dist", ":", "bool", "=", "False", ",", "sync_dist_group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "add_dataloader_idx", ":", "bool", "=", "True", ",", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "metric_attribute", ":", "Optional", "[", "str", "]", "=", "None", ",", "rank_zero_only", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_fabric", "is", "not", "None", ":", "self", ".", "_log_dict_through_fabric", "(", "dictionary", "=", "{", "name", ":", "value", "}", ",", "logger", "=", "logger", ")", "return", "apply_to_collection", "(", "value", ",", "dict", ",", "self", ".", "__check_not_nested", ",", "name", ")", "apply_to_collection", "(", "value", ",", "object", ",", "self", ".", "__check_allowed", ",", "name", ",", "value", ",", "wrong_dtype", "=", "(", "numbers", ".", "Number", ",", "Metric", ",", "Tensor", ")", ")", "trainer", "=", "self", ".", "_trainer", "if", "trainer", "is", "None", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "return", "if", "trainer", ".", "barebones", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "return", "results", "=", "trainer", ".", "_results", "if", "results", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", "STRING", ")", "if", "self", ".", "_current_fx_name", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "on_step", ",", "on_epoch", "=", "_FxValidator", ".", "check_logging_and_get_default_levels", "(", "self", ".", "_current_fx_name", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ")", "if", "add_dataloader_idx", "and", "STRING", "in", "name", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")", "value", "=", "apply_to_collection", "(", "value", ",", "(", "Tensor", ",", "numbers", ".", "Number", ")", ",", "self", ".", "__to_tensor", ",", "name", ")", "if", "trainer", ".", "_logger_connector", ".", "should_reset_tensors", "(", "self", ".", "_current_fx_name", ")", ":", "results", ".", "reset", "(", "metrics", "=", "False", ",", "fx", "=", "self", ".", "_current_fx_name", ")", "if", "metric_attribute", "is", "None", "and", "isinstance", "(", "value", ",", "Metric", ")", ":", "if", "self", ".", "_metric_attributes", "is", "None", ":", "self", ".", "_metric_attributes", "=", "{", "id", "(", "module", ")", ":", "name", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", "if", "isinstance", "(", "module", ",", "Metric", ")", "}", "if", "not", "self", ".", "_metric_attributes", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", ")", "metric_attribute", "=", "self", ".", "_metric_attributes", ".", "get", "(", "id", "(", "value", ")", ",", "None", ")", "if", "metric_attribute", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", "fSTRING", "fSTRING", ")", "if", "(", "trainer", ".", "training", "and", "is_param_in_hook_signature", "(", "self", ".", "training_step", ",", "STRING", ",", "explicit", "=", "True", ")", "and", "batch_size", "is", "None", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "logger", "and", "trainer", ".", "logger", "is", "None", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ")", "if", "logger", "is", "None", ":", "logger", "=", "True", "results", ".", "log", "(", "self", ".", "_current_fx_name", ",", "name", ",", "value", ",", "prog_bar", "=", "prog_bar", ",", "logger", "=", "logger", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ",", "reduce_fx", "=", "reduce_fx", ",", "enable_graph", "=", "enable_graph", ",", "add_dataloader_idx", "=", "add_dataloader_idx", ",", "batch_size", "=", "batch_size", ",", "sync_dist", "=", "sync_dist", "and", "trainer", ".", "_accelerator_connector", ".", "is_distributed", ",", "sync_dist_fn"], "docstring": "Log a key, value pair.", "docstring_tokens": ["log", "a", "key", "value", "pair"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 385, "end_line": 549, "has_examples": true, "num_comments": 7, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_582", "original_string": "def log_dict(\r\n        self,\r\n        dictionary: Union[Mapping[str, _METRIC], MetricCollection],\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a dictionary of values at once.\r\n\r\n        Example::\r\n\r\n            values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\r\n            self.log_dict(values)\r\n\r\n        Args:\r\n            dictionary: key value pairs.\r\n                Keys must be identical across all processes if using DDP or any other distributed strategy.\r\n                The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\r\n            prog_bar: if ``True`` logs to the progress base.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step.\r\n                ``None`` auto-logs for training_step but not validation/test_step.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics.\r\n                ``None`` auto-logs for val/test step but not ``training_step``.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto-detach the graph\r\n            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the ddp group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple). If ``False``, user needs to give unique names for\r\n                each dataloader to not mix values.\r\n            batch_size: Current batch size. This will be directly inferred from the loaded batch,\r\n                but some data structures might need to explicitly provide it.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            return self._log_dict_through_fabric(dictionary=dictionary, logger=logger)\r\n\r\n        kwargs: dict[str, bool] = {}\r\n\r\n        if isinstance(dictionary, MetricCollection):\r\n            kwargs[\"keep_base\"] = False\r\n            if _TORCHMETRICS_GREATER_EQUAL_0_9_1 and dictionary._enable_compute_groups:\r\n                kwargs[\"copy_state\"] = False\r\n\r\n        for k, v in dictionary.items(**kwargs):\r\n            self.log(\r\n                name=k,\r\n                value=v,\r\n                prog_bar=prog_bar,\r\n                logger=logger,\r\n                on_step=on_step,\r\n                on_epoch=on_epoch,\r\n                reduce_fx=reduce_fx,\r\n                enable_graph=enable_graph,\r\n                sync_dist=sync_dist,\r\n                sync_dist_group=sync_dist_group,\r\n                add_dataloader_idx=add_dataloader_idx,\r\n                batch_size=batch_size,\r\n                rank_zero_only=rank_zero_only,\r\n            )\r\n        return None", "language": "python", "code": "def log_dict(\r\n        self,\r\n        dictionary: Union[Mapping[str, _METRIC], MetricCollection],\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a dictionary of values at once.\r\n\r\n        Example::\r\n\r\n            values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\r\n            self.log_dict(values)\r\n\r\n        Args:\r\n            dictionary: key value pairs.\r\n                Keys must be identical across all processes if using DDP or any other distributed strategy.\r\n                The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\r\n            prog_bar: if ``True`` logs to the progress base.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step.\r\n                ``None`` auto-logs for training_step but not validation/test_step.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics.\r\n                ``None`` auto-logs for val/test step but not ``training_step``.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto-detach the graph\r\n            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the ddp group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple). If ``False``, user needs to give unique names for\r\n                each dataloader to not mix values.\r\n            batch_size: Current batch size. This will be directly inferred from the loaded batch,\r\n                but some data structures might need to explicitly provide it.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            return self._log_dict_through_fabric(dictionary=dictionary, logger=logger)\r\n\r\n        kwargs: dict[str, bool] = {}\r\n\r\n        if isinstance(dictionary, MetricCollection):\r\n            kwargs[\"keep_base\"] = False\r\n            if _TORCHMETRICS_GREATER_EQUAL_0_9_1 and dictionary._enable_compute_groups:\r\n                kwargs[\"copy_state\"] = False\r\n\r\n        for k, v in dictionary.items(**kwargs):\r\n            self.log(\r\n                name=k,\r\n                value=v,\r\n                prog_bar=prog_bar,\r\n                logger=logger,\r\n                on_step=on_step,\r\n                on_epoch=on_epoch,\r\n                reduce_fx=reduce_fx,\r\n                enable_graph=enable_graph,\r\n                sync_dist=sync_dist,\r\n                sync_dist_group=sync_dist_group,\r\n                add_dataloader_idx=add_dataloader_idx,\r\n                batch_size=batch_size,\r\n                rank_zero_only=rank_zero_only,\r\n            )\r\n        return None", "code_tokens": ["def", "log_dict", "(", "self", ",", "dictionary", ":", "Union", "[", "Mapping", "[", "str", ",", "_METRIC", "]", ",", "MetricCollection", "]", ",", "prog_bar", ":", "bool", "=", "False", ",", "logger", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_step", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_epoch", ":", "Optional", "[", "bool", "]", "=", "None", ",", "reduce_fx", ":", "Union", "[", "str", ",", "Callable", "[", "[", "Any", "]", ",", "Any", "]", "]", "=", "STRING", ",", "enable_graph", ":", "bool", "=", "False", ",", "sync_dist", ":", "bool", "=", "False", ",", "sync_dist_group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "add_dataloader_idx", ":", "bool", "=", "True", ",", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "rank_zero_only", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_log_dict_through_fabric", "(", "dictionary", "=", "dictionary", ",", "logger", "=", "logger", ")", "kwargs", ":", "dict", "[", "str", ",", "bool", "]", "=", "{", "}", "if", "isinstance", "(", "dictionary", ",", "MetricCollection", ")", ":", "kwargs", "[", "STRING", "]", "=", "False", "if", "_TORCHMETRICS_GREATER_EQUAL_0_9_1", "and", "dictionary", ".", "_enable_compute_groups", ":", "kwargs", "[", "STRING", "]", "=", "False", "for", "k", ",", "v", "in", "dictionary", ".", "items", "(", "*", "*", "kwargs", ")", ":", "self", ".", "log", "(", "name", "=", "k", ",", "value", "=", "v", ",", "prog_bar", "=", "prog_bar", ",", "logger", "=", "logger", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ",", "reduce_fx", "=", "reduce_fx", ",", "enable_graph", "=", "enable_graph", ",", "sync_dist", "=", "sync_dist", ",", "sync_dist_group", "=", "sync_dist_group", ",", "add_dataloader_idx", "=", "add_dataloader_idx", ",", "batch_size", "=", "batch_size", ",", "rank_zero_only", "=", "rank_zero_only", ",", ")", "return", "None"], "docstring": "Log a dictionary of values at once.", "docstring_tokens": ["log", "a", "dictionary", "of", "values", "at", "once"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 551, "end_line": 629, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_583", "original_string": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        r\"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        all_gather = self.trainer.strategy.all_gather\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, all_gather, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        r\"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        all_gather = self.trainer.strategy.all_gather\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, all_gather, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "rSTRING", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "all_gather", "=", "self", ".", "trainer", ".", "strategy", ".", "all_gather", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "all_gather", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "r\"\"\"Gather tensors or collections of tensors from multiple processes.", "docstring_tokens": ["r", "gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 671, "end_line": 693, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_584", "original_string": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        r\"\"\"Same as :meth:`torch.nn.Module.forward`.\r\n\r\n        Args:\r\n            *args: Whatever you decide to pass into the forward method.\r\n            **kwargs: Keyword arguments are also possible.\r\n\r\n        Return:\r\n            Your model's output\r\n\r\n        \"\"\"\r\n        return super().forward(*args, **kwargs)", "language": "python", "code": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        r\"\"\"Same as :meth:`torch.nn.Module.forward`.\r\n\r\n        Args:\r\n            *args: Whatever you decide to pass into the forward method.\r\n            **kwargs: Keyword arguments are also possible.\r\n\r\n        Return:\r\n            Your model's output\r\n\r\n        \"\"\"\r\n        return super().forward(*args, **kwargs)", "code_tokens": ["def", "forward", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "rSTRING", "return", "super", "(", ")", ".", "forward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Same as :meth:`torch.nn.Module.forward`.", "docstring_tokens": ["r", "same", "as", "meth", "torch", "nn", "module", "forward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 696, "end_line": 707, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_585", "original_string": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\r\n        logger.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\r\n              automatic optimization.\r\n            - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\r\n              multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\r\n              the loss is not required.\r\n\r\n        In this step you'd normally do the forward pass and calculate the loss for a batch.\r\n        You can also do fancier things like multiple forward passes or something model specific.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                x, y, z = batch\r\n                out = self.encoder(x)\r\n                loss = self.loss(out, x)\r\n                return loss\r\n\r\n        To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\r\n\r\n        .. code-block:: python\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.automatic_optimization = False\r\n\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt1, opt2 = self.optimizers()\r\n\r\n                ...\r\n                opt1.step()\r\n                ...\r\n                opt2.step()\r\n\r\n        Note:\r\n            When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\r\n            normalized by ``accumulate_grad_batches`` internally.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`training_step` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\r\n        logger.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\r\n              automatic optimization.\r\n            - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\r\n              multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\r\n              the loss is not required.\r\n\r\n        In this step you'd normally do the forward pass and calculate the loss for a batch.\r\n        You can also do fancier things like multiple forward passes or something model specific.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                x, y, z = batch\r\n                out = self.encoder(x)\r\n                loss = self.loss(out, x)\r\n                return loss\r\n\r\n        To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\r\n\r\n        .. code-block:: python\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.automatic_optimization = False\r\n\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt1, opt2 = self.optimizers()\r\n\r\n                ...\r\n                opt1.step()\r\n                ...\r\n                opt2.step()\r\n\r\n        Note:\r\n            When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\r\n            normalized by ``accumulate_grad_batches`` internally.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`training_step` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "training_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "rSTRING", "rank_zero_warn", "(", "STRING", ")"], "docstring": "r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or", "docstring_tokens": ["r", "here", "you", "compute", "and", "return", "the", "training", "loss", "and", "some", "additional", "metrics", "for", "e", "g", "the", "progress", "bar", "or"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 709, "end_line": 763, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_586", "original_string": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or\r\n        calculate anything of interest like accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            def validation_step(self, batch, batch_idx): ...\r\n\r\n\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            def validation_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({'val_loss': loss, 'val_acc': val_acc})\r\n\r\n        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({f\"val_loss_{dataloader_idx}\": loss, f\"val_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to validate you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`validation_step` is called, the model has been put in eval mode\r\n            and PyTorch gradients have been disabled. At the end of validation,\r\n            the model goes back to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "language": "python", "code": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or\r\n        calculate anything of interest like accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            def validation_step(self, batch, batch_idx): ...\r\n\r\n\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            def validation_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({'val_loss': loss, 'val_acc': val_acc})\r\n\r\n        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({f\"val_loss_{dataloader_idx}\": loss, f\"val_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to validate you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`validation_step` is called, the model has been put in eval mode\r\n            and PyTorch gradients have been disabled. At the end of validation,\r\n            the model goes back to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "code_tokens": ["def", "validation_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "rSTRING"], "docstring": "r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or", "docstring_tokens": ["r", "operates", "on", "a", "single", "batch", "of", "data", "from", "the", "validation", "set", "in", "this", "step", "you", "d", "might", "generate", "examples", "or"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 765, "end_line": 845, "has_examples": true, "num_comments": 12, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_587", "original_string": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or\r\n        calculate anything of interest such as accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            def test_step(self, batch, batch_idx): ...\r\n\r\n\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            def test_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n\r\n        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({f\"test_loss_{dataloader_idx}\": loss, f\"test_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to test you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`test_step` is called, the model has been put in eval mode and\r\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\r\n            to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "language": "python", "code": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or\r\n        calculate anything of interest such as accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            def test_step(self, batch, batch_idx): ...\r\n\r\n\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            def test_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n\r\n        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n                x, y = batch\r\n\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                self.log_dict({f\"test_loss_{dataloader_idx}\": loss, f\"test_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to test you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`test_step` is called, the model has been put in eval mode and\r\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\r\n            to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "code_tokens": ["def", "test_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "rSTRING"], "docstring": "r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or", "docstring_tokens": ["r", "operates", "on", "a", "single", "batch", "of", "data", "from", "the", "test", "set", "in", "this", "step", "you", "d", "normally", "generate", "examples", "or"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 847, "end_line": 927, "has_examples": true, "num_comments": 12, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_588", "original_string": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls\r\n        :meth:`~lightning.pytorch.core.LightningModule.forward`. Override to add any processing logic.\r\n\r\n        The :meth:`~lightning.pytorch.core.LightningModule.predict_step` is used\r\n        to scale inference on multi-devices.\r\n\r\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\r\n        callback to write the predictions to disk or database after each batch or on epoch end.\r\n\r\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\r\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\r\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            Predicted output (optional).\r\n\r\n        Example ::\r\n\r\n            class MyModel(LightningModule):\r\n\r\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n                    return self(batch)\r\n\r\n            dm = ...\r\n            model = MyModel()\r\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\r\n            predictions = trainer.predict(model, dm)\r\n\r\n        \"\"\"\r\n        batch = kwargs.get(\"batch\", args[0])\r\n        return self(batch)", "language": "python", "code": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls\r\n        :meth:`~lightning.pytorch.core.LightningModule.forward`. Override to add any processing logic.\r\n\r\n        The :meth:`~lightning.pytorch.core.LightningModule.predict_step` is used\r\n        to scale inference on multi-devices.\r\n\r\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\r\n        callback to write the predictions to disk or database after each batch or on epoch end.\r\n\r\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\r\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\r\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            Predicted output (optional).\r\n\r\n        Example ::\r\n\r\n            class MyModel(LightningModule):\r\n\r\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n                    return self(batch)\r\n\r\n            dm = ...\r\n            model = MyModel()\r\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\r\n            predictions = trainer.predict(model, dm)\r\n\r\n        \"\"\"\r\n        batch = kwargs.get(\"batch\", args[0])\r\n        return self(batch)", "code_tokens": ["def", "predict_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "batch", "=", "kwargs", ".", "get", "(", "STRING", ",", "args", "[", "0", "]", ")", "return", "self", "(", "batch", ")"], "docstring": "Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls", "docstring_tokens": ["step", "function", "called", "during", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "predict", "by", "default", "it", "calls"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 929, "end_line": 967, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_589", "original_string": "def configure_callbacks(self) -> Union[Sequence[Callback], Callback]:\r\n        \"\"\"Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\r\n        called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\r\n        ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\r\n        present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\r\n        make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\r\n\r\n        Return:\r\n            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\r\n\r\n        Example::\r\n\r\n            def configure_callbacks(self):\r\n                early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\r\n                checkpoint = ModelCheckpoint(monitor=\"val_loss\")\r\n                return [early_stop, checkpoint]\r\n\r\n        \"\"\"\r\n        return []", "language": "python", "code": "def configure_callbacks(self) -> Union[Sequence[Callback], Callback]:\r\n        \"\"\"Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\r\n        called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\r\n        ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\r\n        present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\r\n        make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\r\n\r\n        Return:\r\n            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\r\n\r\n        Example::\r\n\r\n            def configure_callbacks(self):\r\n                early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\r\n                checkpoint = ModelCheckpoint(monitor=\"val_loss\")\r\n                return [early_stop, checkpoint]\r\n\r\n        \"\"\"\r\n        return []", "code_tokens": ["def", "configure_callbacks", "(", "self", ")", "-", ">", "Union", "[", "Sequence", "[", "Callback", "]", ",", "Callback", "]", ":", "STRING", "return", "[", "]"], "docstring": "Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets", "docstring_tokens": ["configure", "model", "specific", "callbacks", "when", "the", "model", "gets", "attached", "e", "g", "when", "fit", "or", "test", "gets"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 969, "end_line": 987, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_590", "original_string": "def configure_optimizers(self) -> OptimizerLRScheduler:\r\n        r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\r\n        But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\r\n        the manual optimization mode.\r\n\r\n        Return:\r\n            Any of these 6 options.\r\n\r\n            - **Single optimizer**.\r\n            - **List or Tuple** of optimizers.\r\n            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\r\n              (or multiple ``lr_scheduler_config``).\r\n            - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\r\n              key whose value is a single LR scheduler or ``lr_scheduler_config``.\r\n            - **None** - Fit will run without any optimizer.\r\n\r\n        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\r\n        The default configuration is shown below.\r\n\r\n        .. code-block:: python\r\n\r\n            lr_scheduler_config = {\r\n                \"scheduler\": lr_scheduler,\r\n                \"interval\": \"epoch\",\r\n                \"frequency\": 1,\r\n                \"monitor\": \"val_loss\",\r\n                \"strict\": True,\r\n                \"name\": None,\r\n            }\r\n\r\n        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\r\n        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\r\n        ``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\r\n        should be conditioned on.\r\n\r\n        .. testcode::\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = Adam(...)\r\n                return {\r\n                    \"optimizer\": optimizer,\r\n                    \"lr_scheduler\": {\r\n                        \"scheduler\": ReduceLROnPlateau(optimizer, ...),\r\n                        \"monitor\": \"metric_to_track\",\r\n                        \"frequency\": \"indicates how often the metric is updated\",\r\n                    },\r\n                }\r\n\r\n\r\n            def configure_optimizers(self):\r\n                optimizer1 = Adam(...)\r\n                optimizer2 = SGD(...)\r\n                scheduler1 = ReduceLROnPlateau(optimizer1, ...)\r\n                scheduler2 = LambdaLR(optimizer2, ...)\r\n                return (\r\n                    {\r\n                        \"optimizer\": optimizer1,\r\n                        \"lr_scheduler\": {\r\n                            \"scheduler\": scheduler1,\r\n                            \"monitor\": \"metric_to_track\",\r\n                        },\r\n                    },\r\n                    {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\r\n                )\r\n\r\n        Metrics can be made available to monitor by simply logging it using\r\n        ``self.log('metric_to_track', metric_val)`` in your :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Note:\r\n            Some things to know:\r\n\r\n            - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\r\n            - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\r\n              ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\r\n              the scheduler's ``.step()`` method automatically in case of automatic optimization.\r\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\r\n            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\r\n            - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\r\n              yourself.\r\n            - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`configure_optimizers` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def configure_optimizers(self) -> OptimizerLRScheduler:\r\n        r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\r\n        But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\r\n        the manual optimization mode.\r\n\r\n        Return:\r\n            Any of these 6 options.\r\n\r\n            - **Single optimizer**.\r\n            - **List or Tuple** of optimizers.\r\n            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\r\n              (or multiple ``lr_scheduler_config``).\r\n            - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\r\n              key whose value is a single LR scheduler or ``lr_scheduler_config``.\r\n            - **None** - Fit will run without any optimizer.\r\n\r\n        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\r\n        The default configuration is shown below.\r\n\r\n        .. code-block:: python\r\n\r\n            lr_scheduler_config = {\r\n                \"scheduler\": lr_scheduler,\r\n                \"interval\": \"epoch\",\r\n                \"frequency\": 1,\r\n                \"monitor\": \"val_loss\",\r\n                \"strict\": True,\r\n                \"name\": None,\r\n            }\r\n\r\n        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\r\n        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\r\n        ``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\r\n        should be conditioned on.\r\n\r\n        .. testcode::\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = Adam(...)\r\n                return {\r\n                    \"optimizer\": optimizer,\r\n                    \"lr_scheduler\": {\r\n                        \"scheduler\": ReduceLROnPlateau(optimizer, ...),\r\n                        \"monitor\": \"metric_to_track\",\r\n                        \"frequency\": \"indicates how often the metric is updated\",\r\n                    },\r\n                }\r\n\r\n\r\n            def configure_optimizers(self):\r\n                optimizer1 = Adam(...)\r\n                optimizer2 = SGD(...)\r\n                scheduler1 = ReduceLROnPlateau(optimizer1, ...)\r\n                scheduler2 = LambdaLR(optimizer2, ...)\r\n                return (\r\n                    {\r\n                        \"optimizer\": optimizer1,\r\n                        \"lr_scheduler\": {\r\n                            \"scheduler\": scheduler1,\r\n                            \"monitor\": \"metric_to_track\",\r\n                        },\r\n                    },\r\n                    {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\r\n                )\r\n\r\n        Metrics can be made available to monitor by simply logging it using\r\n        ``self.log('metric_to_track', metric_val)`` in your :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Note:\r\n            Some things to know:\r\n\r\n            - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\r\n            - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\r\n              ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\r\n              the scheduler's ``.step()`` method automatically in case of automatic optimization.\r\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\r\n            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\r\n            - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\r\n              yourself.\r\n            - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`configure_optimizers` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "configure_optimizers", "(", "self", ")", "-", ">", "OptimizerLRScheduler", ":", "rSTRING", "rank_zero_warn", "(", "STRING", ")"], "docstring": "r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.", "docstring_tokens": ["r", "choose", "what", "optimizers", "and", "learning", "rate", "schedulers", "to", "use", "in", "your", "optimization", "normally", "you", "d", "need", "one"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 989, "end_line": 1089, "has_examples": false, "num_comments": 9, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_591", "original_string": "def manual_backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\r\n        Lightning can ensure that all the proper scaling gets applied when using mixed precision.\r\n\r\n        See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                loss = ...\r\n                opt.zero_grad()\r\n                self.manual_backward(loss)\r\n                opt.step()\r\n\r\n        Args:\r\n            loss: The tensor on which to compute gradients. Must have a graph attached.\r\n            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            self._verify_is_manual_optimization(\"manual_backward\")\r\n            self.trainer.strategy.backward(loss, None, *args, **kwargs)", "language": "python", "code": "def manual_backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\r\n        Lightning can ensure that all the proper scaling gets applied when using mixed precision.\r\n\r\n        See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                loss = ...\r\n                opt.zero_grad()\r\n                self.manual_backward(loss)\r\n                opt.step()\r\n\r\n        Args:\r\n            loss: The tensor on which to compute gradients. Must have a graph attached.\r\n            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            self._verify_is_manual_optimization(\"manual_backward\")\r\n            self.trainer.strategy.backward(loss, None, *args, **kwargs)", "code_tokens": ["def", "manual_backward", "(", "self", ",", "loss", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_fabric", ":", "self", ".", "_fabric", ".", "backward", "(", "loss", ",", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "_verify_is_manual_optimization", "(", "STRING", ")", "self", ".", "trainer", ".", "strategy", ".", "backward", "(", "loss", ",", "None", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,", "docstring_tokens": ["call", "this", "directly", "from", "your", "meth", "training_step", "when", "doing", "optimizations", "manually", "by", "using", "this"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1091, "end_line": 1117, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_592", "original_string": "def backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)", "language": "python", "code": "def backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "loss", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_fabric", ":", "self", ".", "_fabric", ".", "backward", "(", "loss", ",", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "loss", ".", "backward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own", "docstring_tokens": ["called", "to", "perform", "backward", "on", "the", "loss", "returned", "in", "meth", "training_step", "override", "this", "hook", "with", "your", "own"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1119, "end_line": 1136, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_593", "original_string": "def toggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup.\r\n\r\n        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        \"\"\"\r\n        param_requires_grad_state = {}\r\n        for opt in self.trainer.optimizers:\r\n            for group in opt.param_groups:\r\n                for param in group[\"params\"]:\r\n                    if param in param_requires_grad_state:\r\n                        continue\r\n                    param_requires_grad_state[param] = param.requires_grad\r\n                    param.requires_grad = False\r\n\r\n        for group in optimizer.param_groups:\r\n            for param in group[\"params\"]:\r\n                param.requires_grad = param_requires_grad_state[param]\r\n        self._param_requires_grad_state = param_requires_grad_state", "language": "python", "code": "def toggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup.\r\n\r\n        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        \"\"\"\r\n        param_requires_grad_state = {}\r\n        for opt in self.trainer.optimizers:\r\n            for group in opt.param_groups:\r\n                for param in group[\"params\"]:\r\n                    if param in param_requires_grad_state:\r\n                        continue\r\n                    param_requires_grad_state[param] = param.requires_grad\r\n                    param.requires_grad = False\r\n\r\n        for group in optimizer.param_groups:\r\n            for param in group[\"params\"]:\r\n                param.requires_grad = param_requires_grad_state[param]\r\n        self._param_requires_grad_state = param_requires_grad_state", "code_tokens": ["def", "toggle_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "None", ":", "STRING", "param_requires_grad_state", "=", "{", "}", "for", "opt", "in", "self", ".", "trainer", ".", "optimizers", ":", "for", "group", "in", "opt", ".", "param_groups", ":", "for", "param", "in", "group", "[", "STRING", "]", ":", "if", "param", "in", "param_requires_grad_state", ":", "continue", "param_requires_grad_state", "[", "param", "]", "=", "param", ".", "requires_grad", "param", ".", "requires_grad", "=", "False", "for", "group", "in", "optimizer", ".", "param_groups", ":", "for", "param", "in", "group", "[", "STRING", "]", ":", "param", ".", "requires_grad", "=", "param_requires_grad_state", "[", "param", "]", "self", ".", "_param_requires_grad_state", "=", "param_requires_grad_state"], "docstring": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to", "docstring_tokens": ["makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1138, "end_line": 1165, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_594", "original_string": "def untoggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to untoggle.\r\n\r\n        \"\"\"\r\n        for opt in self.trainer.optimizers:\r\n            if not (opt is optimizer or (isinstance(optimizer, LightningOptimizer) and opt is optimizer.optimizer)):\r\n                for group in opt.param_groups:\r\n                    for param in group[\"params\"]:\r\n                        if param in self._param_requires_grad_state:\r\n                            param.requires_grad = self._param_requires_grad_state[param]\r\n        self._param_requires_grad_state = {}", "language": "python", "code": "def untoggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to untoggle.\r\n\r\n        \"\"\"\r\n        for opt in self.trainer.optimizers:\r\n            if not (opt is optimizer or (isinstance(optimizer, LightningOptimizer) and opt is optimizer.optimizer)):\r\n                for group in opt.param_groups:\r\n                    for param in group[\"params\"]:\r\n                        if param in self._param_requires_grad_state:\r\n                            param.requires_grad = self._param_requires_grad_state[param]\r\n        self._param_requires_grad_state = {}", "code_tokens": ["def", "untoggle_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "None", ":", "STRING", "for", "opt", "in", "self", ".", "trainer", ".", "optimizers", ":", "if", "not", "(", "opt", "is", "optimizer", "or", "(", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", "and", "opt", "is", "optimizer", ".", "optimizer", ")", ")", ":", "for", "group", "in", "opt", ".", "param_groups", ":", "for", "param", "in", "group", "[", "STRING", "]", ":", "if", "param", "in", "self", ".", "_param_requires_grad_state", ":", "param", ".", "requires_grad", "=", "self", ".", "_param_requires_grad_state", "[", "param", "]", "self", ".", "_param_requires_grad_state", "=", "{", "}"], "docstring": "Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.", "docstring_tokens": ["resets", "the", "state", "of", "required", "gradients", "that", "were", "toggled", "with", "meth", "toggle_optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1167, "end_line": 1181, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_595", "original_string": "def toggled_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> Generator:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup. Combines :meth:`toggle_optimizer` and\r\n        :meth:`untoggle_optimizer` into context manager.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                with self.toggled_optimizer(opt):\r\n                    loss = ...\r\n                    opt.zero_grad()\r\n                    self.manual_backward(loss)\r\n                    opt.step()\r\n\r\n        \"\"\"\r\n        self.toggle_optimizer(optimizer)\r\n        try:\r\n            yield\r\n        finally:\r\n            self.untoggle_optimizer(optimizer)", "language": "python", "code": "def toggled_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> Generator:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup. Combines :meth:`toggle_optimizer` and\r\n        :meth:`untoggle_optimizer` into context manager.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                with self.toggled_optimizer(opt):\r\n                    loss = ...\r\n                    opt.zero_grad()\r\n                    self.manual_backward(loss)\r\n                    opt.step()\r\n\r\n        \"\"\"\r\n        self.toggle_optimizer(optimizer)\r\n        try:\r\n            yield\r\n        finally:\r\n            self.untoggle_optimizer(optimizer)", "code_tokens": ["def", "toggled_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "Generator", ":", "STRING", "self", ".", "toggle_optimizer", "(", "optimizer", ")", "try", ":", "yield", "finally", ":", "self", ".", "untoggle_optimizer", "(", "optimizer", ")"], "docstring": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to", "docstring_tokens": ["makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1184, "end_line": 1207, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_596", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Handles gradient clipping internally.\r\n\r\n        Note:\r\n            - Do not override this method. If you want to customize gradient clipping, consider using\r\n              :meth:`configure_gradient_clipping` method.\r\n            - For manual optimization (``self.automatic_optimization = False``), if you want to use\r\n              gradient clipping, consider calling\r\n              ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\r\n              manually in the training step.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.\r\n\r\n        \"\"\"\r\n\r\n        if self.fabric is not None:\r\n            self.fabric.clip_gradients(\r\n                self,\r\n                optimizer,\r\n                clip_val=gradient_clip_val if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else None,\r\n                max_norm=None if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else gradient_clip_val,\r\n            )\r\n            return\r\n\r\n        if gradient_clip_val is None:\r\n            gradient_clip_val = self.trainer.gradient_clip_val or 0.0\r\n        elif self.trainer.gradient_clip_val is not None and self.trainer.gradient_clip_val != gradient_clip_val:\r\n            raise MisconfigurationException(\r\n                f\"You have set `Trainer(gradient_clip_val={self.trainer.gradient_clip_val!r})`\"\r\n                f\" and have passed `clip_gradients(gradient_clip_val={gradient_clip_val!r})`.\"\r\n                \" Please use only one of them.\"\r\n            )\r\n\r\n        if gradient_clip_algorithm is None:\r\n            gradient_clip_algorithm = self.trainer.gradient_clip_algorithm or \"norm\"\r\n        else:\r\n            gradient_clip_algorithm = gradient_clip_algorithm.lower()\r\n            if (\r\n                self.trainer.gradient_clip_algorithm is not None\r\n                and self.trainer.gradient_clip_algorithm != gradient_clip_algorithm\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"You have set `Trainer(gradient_clip_algorithm={self.trainer.gradient_clip_algorithm.value!r})`\"\r\n                    f\" and have passed `clip_gradients(gradient_clip_algorithm={gradient_clip_algorithm!r})\"\r\n                    \" Please use only one of them.\"\r\n                )\r\n\r\n        if not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if not GradClipAlgorithmType.supported_type(gradient_clip_algorithm.lower()):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid.\"\r\n                f\" Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        gradient_clip_algorithm = GradClipAlgorithmType(gradient_clip_algorithm)\r\n        self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Handles gradient clipping internally.\r\n\r\n        Note:\r\n            - Do not override this method. If you want to customize gradient clipping, consider using\r\n              :meth:`configure_gradient_clipping` method.\r\n            - For manual optimization (``self.automatic_optimization = False``), if you want to use\r\n              gradient clipping, consider calling\r\n              ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\r\n              manually in the training step.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.\r\n\r\n        \"\"\"\r\n\r\n        if self.fabric is not None:\r\n            self.fabric.clip_gradients(\r\n                self,\r\n                optimizer,\r\n                clip_val=gradient_clip_val if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else None,\r\n                max_norm=None if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else gradient_clip_val,\r\n            )\r\n            return\r\n\r\n        if gradient_clip_val is None:\r\n            gradient_clip_val = self.trainer.gradient_clip_val or 0.0\r\n        elif self.trainer.gradient_clip_val is not None and self.trainer.gradient_clip_val != gradient_clip_val:\r\n            raise MisconfigurationException(\r\n                f\"You have set `Trainer(gradient_clip_val={self.trainer.gradient_clip_val!r})`\"\r\n                f\" and have passed `clip_gradients(gradient_clip_val={gradient_clip_val!r})`.\"\r\n                \" Please use only one of them.\"\r\n            )\r\n\r\n        if gradient_clip_algorithm is None:\r\n            gradient_clip_algorithm = self.trainer.gradient_clip_algorithm or \"norm\"\r\n        else:\r\n            gradient_clip_algorithm = gradient_clip_algorithm.lower()\r\n            if (\r\n                self.trainer.gradient_clip_algorithm is not None\r\n                and self.trainer.gradient_clip_algorithm != gradient_clip_algorithm\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"You have set `Trainer(gradient_clip_algorithm={self.trainer.gradient_clip_algorithm.value!r})`\"\r\n                    f\" and have passed `clip_gradients(gradient_clip_algorithm={gradient_clip_algorithm!r})\"\r\n                    \" Please use only one of them.\"\r\n                )\r\n\r\n        if not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if not GradClipAlgorithmType.supported_type(gradient_clip_algorithm.lower()):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid.\"\r\n                f\" Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        gradient_clip_algorithm = GradClipAlgorithmType(gradient_clip_algorithm)\r\n        self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "gradient_clip_val", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "gradient_clip_algorithm", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "fabric", "is", "not", "None", ":", "self", ".", "fabric", ".", "clip_gradients", "(", "self", ",", "optimizer", ",", "clip_val", "=", "gradient_clip_val", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", "else", "None", ",", "max_norm", "=", "None", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", "else", "gradient_clip_val", ",", ")", "return", "if", "gradient_clip_val", "is", "None", ":", "gradient_clip_val", "=", "self", ".", "trainer", ".", "gradient_clip_val", "or", "0", ".", "0", "elif", "self", ".", "trainer", ".", "gradient_clip_val", "is", "not", "None", "and", "self", ".", "trainer", ".", "gradient_clip_val", "!", "=", "gradient_clip_val", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", "STRING", ")", "if", "gradient_clip_algorithm", "is", "None", ":", "gradient_clip_algorithm", "=", "self", ".", "trainer", ".", "gradient_clip_algorithm", "or", "STRING", "else", ":", "gradient_clip_algorithm", "=", "gradient_clip_algorithm", ".", "lower", "(", ")", "if", "(", "self", ".", "trainer", ".", "gradient_clip_algorithm", "is", "not", "None", "and", "self", ".", "trainer", ".", "gradient_clip_algorithm", "!", "=", "gradient_clip_algorithm", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", "STRING", ")", "if", "not", "isinstance", "(", "gradient_clip_val", ",", "(", "int", ",", "float", ")", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "if", "not", "GradClipAlgorithmType", ".", "supported_type", "(", "gradient_clip_algorithm", ".", "lower", "(", ")", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", ")", "gradient_clip_algorithm", "=", "GradClipAlgorithmType", "(", "gradient_clip_algorithm", ")", "self", ".", "trainer", ".", "precision_plugin", ".", "clip_gradients", "(", "optimizer", ",", "gradient_clip_val", ",", "gradient_clip_algorithm", ")"], "docstring": "Handles gradient clipping internally.", "docstring_tokens": ["handles", "gradient", "clipping", "internally"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1209, "end_line": 1275, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_597", "original_string": "def configure_gradient_clipping(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\r\n                will be available here.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\r\n                passed in Trainer will be available here.\r\n\r\n        Example::\r\n\r\n            def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\r\n                self.clip_gradients(\r\n                    optimizer,\r\n                    gradient_clip_val=gradient_clip_val,\r\n                    gradient_clip_algorithm=gradient_clip_algorithm\r\n                )\r\n\r\n        \"\"\"\r\n        self.clip_gradients(\r\n            optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm\r\n        )", "language": "python", "code": "def configure_gradient_clipping(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\r\n                will be available here.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\r\n                passed in Trainer will be available here.\r\n\r\n        Example::\r\n\r\n            def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\r\n                self.clip_gradients(\r\n                    optimizer,\r\n                    gradient_clip_val=gradient_clip_val,\r\n                    gradient_clip_algorithm=gradient_clip_algorithm\r\n                )\r\n\r\n        \"\"\"\r\n        self.clip_gradients(\r\n            optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm\r\n        )", "code_tokens": ["def", "configure_gradient_clipping", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "gradient_clip_val", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "gradient_clip_algorithm", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "clip_gradients", "(", "optimizer", ",", "gradient_clip_val", "=", "gradient_clip_val", ",", "gradient_clip_algorithm", "=", "gradient_clip_algorithm", ")"], "docstring": "Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.", "docstring_tokens": ["perform", "gradient", "clipping", "for", "the", "optimizer", "parameters", "called", "before", "meth", "optimizer_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1277, "end_line": 1306, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_598", "original_string": "def lr_scheduler_step(self, scheduler: LRSchedulerTypeUnion, metric: Optional[Any]) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\r\n        its ``interval``.\r\n\r\n        Args:\r\n            scheduler: Learning rate scheduler.\r\n            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\r\n\r\n        Examples::\r\n\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                if metric is None:\r\n                    scheduler.step()\r\n                else:\r\n                    scheduler.step(metric)\r\n\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                scheduler.step(epoch=self.current_epoch)\r\n\r\n        \"\"\"\r\n        if metric is None:\r\n            scheduler.step()  # type: ignore[call-arg]\r\n        else:\r\n            scheduler.step(metric)", "language": "python", "code": "def lr_scheduler_step(self, scheduler: LRSchedulerTypeUnion, metric: Optional[Any]) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\r\n        its ``interval``.\r\n\r\n        Args:\r\n            scheduler: Learning rate scheduler.\r\n            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\r\n\r\n        Examples::\r\n\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                if metric is None:\r\n                    scheduler.step()\r\n                else:\r\n                    scheduler.step(metric)\r\n\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                scheduler.step(epoch=self.current_epoch)\r\n\r\n        \"\"\"\r\n        if metric is None:\r\n            scheduler.step()  # type: ignore[call-arg]\r\n        else:\r\n            scheduler.step(metric)", "code_tokens": ["def", "lr_scheduler_step", "(", "self", ",", "scheduler", ":", "LRSchedulerTypeUnion", ",", "metric", ":", "Optional", "[", "Any", "]", ")", "-", ">", "None", ":", "rSTRING", "if", "metric", "is", "None", ":", "scheduler", ".", "step", "(", ")", "#", "type", ":", "ignore", "[", "call", "-", "arg", "]", "else", ":", "scheduler", ".", "step", "(", "metric", ")"], "docstring": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls", "docstring_tokens": ["r", "override", "this", "method", "to", "adjust", "the", "default", "way", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "calls"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1308, "end_line": 1334, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_599", "original_string": "def optimizer_step(\r\n        self,\r\n        epoch: int,\r\n        batch_idx: int,\r\n        optimizer: Union[Optimizer, LightningOptimizer],\r\n        optimizer_closure: Optional[Callable[[], Any]] = None,\r\n    ) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        the optimizer.\r\n\r\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\r\n        This method (and ``zero_grad()``) won't be called during the accumulation phase when\r\n        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n            optimizer_closure: The optimizer closure. This closure must be executed as it includes the\r\n                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\r\n\r\n        Examples::\r\n\r\n            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\r\n\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n\r\n        \"\"\"\r\n        optimizer.step(closure=optimizer_closure)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        epoch: int,\r\n        batch_idx: int,\r\n        optimizer: Union[Optimizer, LightningOptimizer],\r\n        optimizer_closure: Optional[Callable[[], Any]] = None,\r\n    ) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        the optimizer.\r\n\r\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\r\n        This method (and ``zero_grad()``) won't be called during the accumulation phase when\r\n        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n            optimizer_closure: The optimizer closure. This closure must be executed as it includes the\r\n                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\r\n\r\n        Examples::\r\n\r\n            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\r\n\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n\r\n        \"\"\"\r\n        optimizer.step(closure=optimizer_closure)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "epoch", ":", "int", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ",", "optimizer_closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "Any", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "rSTRING", "optimizer", ".", "step", "(", "closure", "=", "optimizer_closure", ")"], "docstring": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls", "docstring_tokens": ["r", "override", "this", "method", "to", "adjust", "the", "default", "way", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "calls"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1336, "end_line": 1367, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_600", "original_string": "def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n\r\n        Examples::\r\n\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad()\r\n\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad(set_to_none=True)\r\n\r\n        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.\r\n\r\n        \"\"\"\r\n        optimizer.zero_grad()", "language": "python", "code": "def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n\r\n        Examples::\r\n\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad()\r\n\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad(set_to_none=True)\r\n\r\n        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.\r\n\r\n        \"\"\"\r\n        optimizer.zero_grad()", "code_tokens": ["def", "optimizer_zero_grad", "(", "self", ",", "epoch", ":", "int", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "STRING", "optimizer", ".", "zero_grad", "(", ")"], "docstring": "Override this method to change the default behaviour of ``optimizer.zero_grad()``.", "docstring_tokens": ["override", "this", "method", "to", "change", "the", "default", "behaviour", "of", "optimizer", "zero_grad"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1369, "end_line": 1390, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_601", "original_string": "def freeze(self) -> None:\r\n        r\"\"\"Freeze all params for inference.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule(...)\r\n            model.freeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n        self.eval()", "language": "python", "code": "def freeze(self) -> None:\r\n        r\"\"\"Freeze all params for inference.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule(...)\r\n            model.freeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n        self.eval()", "code_tokens": ["def", "freeze", "(", "self", ")", "-", ">", "None", ":", "rSTRING", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "param", ".", "requires_grad", "=", "False", "self", ".", "eval", "(", ")"], "docstring": "r\"\"\"Freeze all params for inference.", "docstring_tokens": ["r", "freeze", "all", "params", "for", "inference"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1392, "end_line": 1404, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_602", "original_string": "def unfreeze(self) -> None:\r\n        \"\"\"Unfreeze all parameters for training.\r\n\r\n        .. code-block:: python\r\n\r\n            model = MyLightningModule(...)\r\n            model.unfreeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = True\r\n\r\n        self.train()", "language": "python", "code": "def unfreeze(self) -> None:\r\n        \"\"\"Unfreeze all parameters for training.\r\n\r\n        .. code-block:: python\r\n\r\n            model = MyLightningModule(...)\r\n            model.unfreeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = True\r\n\r\n        self.train()", "code_tokens": ["def", "unfreeze", "(", "self", ")", "-", ">", "None", ":", "STRING", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "param", ".", "requires_grad", "=", "True", "self", ".", "train", "(", ")"], "docstring": "Unfreeze all parameters for training.", "docstring_tokens": ["unfreeze", "all", "parameters", "for", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1406, "end_line": 1418, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_603", "original_string": "def to_onnx(\r\n        self,\r\n        file_path: Union[str, Path, BytesIO, None] = None,\r\n        input_sample: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Optional[\"ONNXProgram\"]:\r\n        \"\"\"Saves the model in ONNX format.\r\n\r\n        Args:\r\n            file_path: The path of the file the onnx model should be saved to. Default: None (no file saved).\r\n            input_sample: An input for tracing. Default: None (Use self.example_input_array)\r\n\r\n            **kwargs: Will be passed to torch.onnx.export function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            model.to_onnx(\"export.onnx\", input_sample, export_params=True)\r\n\r\n        \"\"\"\r\n        if not _ONNX_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"`{type(self).__name__}.to_onnx()` requires `onnx` to be installed.\")\r\n\r\n        if kwargs.get(\"dynamo\", False) and not (_ONNXSCRIPT_AVAILABLE and _TORCH_GREATER_EQUAL_2_5):\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_onnx(dynamo=True)` \"\r\n                \"requires `onnxscript` and `torch>=2.5.0` to be installed.\"\r\n            )\r\n\r\n        mode = self.training\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to ONNX since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        file_path = str(file_path) if isinstance(file_path, Path) else file_path\r\n        ret = torch.onnx.export(self, input_sample, file_path, **kwargs)  # type: ignore\r\n        self.train(mode)\r\n        return ret", "language": "python", "code": "def to_onnx(\r\n        self,\r\n        file_path: Union[str, Path, BytesIO, None] = None,\r\n        input_sample: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Optional[\"ONNXProgram\"]:\r\n        \"\"\"Saves the model in ONNX format.\r\n\r\n        Args:\r\n            file_path: The path of the file the onnx model should be saved to. Default: None (no file saved).\r\n            input_sample: An input for tracing. Default: None (Use self.example_input_array)\r\n\r\n            **kwargs: Will be passed to torch.onnx.export function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            model.to_onnx(\"export.onnx\", input_sample, export_params=True)\r\n\r\n        \"\"\"\r\n        if not _ONNX_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"`{type(self).__name__}.to_onnx()` requires `onnx` to be installed.\")\r\n\r\n        if kwargs.get(\"dynamo\", False) and not (_ONNXSCRIPT_AVAILABLE and _TORCH_GREATER_EQUAL_2_5):\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_onnx(dynamo=True)` \"\r\n                \"requires `onnxscript` and `torch>=2.5.0` to be installed.\"\r\n            )\r\n\r\n        mode = self.training\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to ONNX since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        file_path = str(file_path) if isinstance(file_path, Path) else file_path\r\n        ret = torch.onnx.export(self, input_sample, file_path, **kwargs)  # type: ignore\r\n        self.train(mode)\r\n        return ret", "code_tokens": ["def", "to_onnx", "(", "self", ",", "file_path", ":", "Union", "[", "str", ",", "Path", ",", "BytesIO", ",", "None", "]", "=", "None", ",", "input_sample", ":", "Optional", "[", "Any", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Optional", "[", "STRING", "]", ":", "STRING", "if", "not", "_ONNX_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "fSTRING", ")", "if", "kwargs", ".", "get", "(", "STRING", ",", "False", ")", "and", "not", "(", "_ONNXSCRIPT_AVAILABLE", "and", "_TORCH_GREATER_EQUAL_2_5", ")", ":", "raise", "ModuleNotFoundError", "(", "fSTRING", "STRING", ")", "mode", "=", "self", ".", "training", "if", "input_sample", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "input_sample", "=", "self", ".", "example_input_array", "input_sample", "=", "self", ".", "_on_before_batch_transfer", "(", "input_sample", ")", "input_sample", "=", "self", ".", "_apply_batch_transfer_handler", "(", "input_sample", ")", "file_path", "=", "str", "(", "file_path", ")", "if", "isinstance", "(", "file_path", ",", "Path", ")", "else", "file_path", "ret", "=", "torch", ".", "onnx", ".", "export", "(", "self", ",", "input_sample", ",", "file_path", ",", "*", "*", "kwargs", ")", "#", "type", ":", "ignore", "self", ".", "train", "(", "mode", ")", "return", "ret"], "docstring": "Saves the model in ONNX format.", "docstring_tokens": ["saves", "the", "model", "in", "onnx", "format"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1428, "end_line": 1484, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_604", "original_string": "def to_torchscript(\r\n        self,\r\n        file_path: Optional[Union[str, Path]] = None,\r\n        method: Optional[str] = \"script\",\r\n        example_inputs: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Union[ScriptModule, dict[str, ScriptModule]]:\r\n        \"\"\"By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\r\n        please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\r\n        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\r\n        scripted you should override this method. In case you want to return multiple modules, we recommend using a\r\n        dictionary.\r\n\r\n        Args:\r\n            file_path: Path where to save the torchscript. Default: None (no file saved).\r\n            method: Whether to use TorchScript's script or trace method. Default: 'script'\r\n            example_inputs: An input to be used to do tracing when method is set to 'trace'.\r\n              Default: None (uses :attr:`example_input_array`)\r\n            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\r\n              :func:`torch.jit.trace` function.\r\n\r\n        Note:\r\n            - Requires the implementation of the\r\n              :meth:`~lightning.pytorch.core.LightningModule.forward` method.\r\n            - The exported script will be set to evaluation mode.\r\n            - It is recommended that you install the latest supported version of PyTorch\r\n              to use this feature without limitations. See also the :mod:`torch.jit`\r\n              documentation for supported features.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n            model = SimpleModel()\r\n            model.to_torchscript(file_path=\"model.pt\")\r\n\r\n            torch.jit.save(model.to_torchscript(\r\n                file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\r\n            )\r\n\r\n        Return:\r\n            This LightningModule as a torchscript, regardless of whether `file_path` is\r\n            defined or not.\r\n\r\n        \"\"\"\r\n        mode = self.training\r\n\r\n        if method == \"script\":\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.script(self.eval(), **kwargs)\r\n        elif method == \"trace\":\r\n            if example_inputs is None:\r\n                if self.example_input_array is None:\r\n                    raise ValueError(\r\n                        \"Choosing method=`trace` requires either `example_inputs`\"\r\n                        \" or `model.example_input_array` to be defined.\"\r\n                    )\r\n                example_inputs = self.example_input_array\r\n\r\n            if kwargs.get(\"check_inputs\") is not None:\r\n                kwargs[\"check_inputs\"] = self._on_before_batch_transfer(kwargs[\"check_inputs\"])\r\n                kwargs[\"check_inputs\"] = self._apply_batch_transfer_handler(kwargs[\"check_inputs\"])\r\n\r\n            example_inputs = self._on_before_batch_transfer(example_inputs)\r\n            example_inputs = self._apply_batch_transfer_handler(example_inputs)\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.trace(func=self.eval(), example_inputs=example_inputs, **kwargs)\r\n        else:\r\n            raise ValueError(f\"The 'method' parameter only supports 'script' or 'trace', but value given was: {method}\")\r\n\r\n        self.train(mode)\r\n\r\n        if file_path is not None:\r\n            fs = get_filesystem(file_path)\r\n            with fs.open(file_path, \"wb\") as f:\r\n                torch.jit.save(torchscript_module, f)\r\n\r\n        return torchscript_module", "language": "python", "code": "def to_torchscript(\r\n        self,\r\n        file_path: Optional[Union[str, Path]] = None,\r\n        method: Optional[str] = \"script\",\r\n        example_inputs: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Union[ScriptModule, dict[str, ScriptModule]]:\r\n        \"\"\"By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\r\n        please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\r\n        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\r\n        scripted you should override this method. In case you want to return multiple modules, we recommend using a\r\n        dictionary.\r\n\r\n        Args:\r\n            file_path: Path where to save the torchscript. Default: None (no file saved).\r\n            method: Whether to use TorchScript's script or trace method. Default: 'script'\r\n            example_inputs: An input to be used to do tracing when method is set to 'trace'.\r\n              Default: None (uses :attr:`example_input_array`)\r\n            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\r\n              :func:`torch.jit.trace` function.\r\n\r\n        Note:\r\n            - Requires the implementation of the\r\n              :meth:`~lightning.pytorch.core.LightningModule.forward` method.\r\n            - The exported script will be set to evaluation mode.\r\n            - It is recommended that you install the latest supported version of PyTorch\r\n              to use this feature without limitations. See also the :mod:`torch.jit`\r\n              documentation for supported features.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n            model = SimpleModel()\r\n            model.to_torchscript(file_path=\"model.pt\")\r\n\r\n            torch.jit.save(model.to_torchscript(\r\n                file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\r\n            )\r\n\r\n        Return:\r\n            This LightningModule as a torchscript, regardless of whether `file_path` is\r\n            defined or not.\r\n\r\n        \"\"\"\r\n        mode = self.training\r\n\r\n        if method == \"script\":\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.script(self.eval(), **kwargs)\r\n        elif method == \"trace\":\r\n            if example_inputs is None:\r\n                if self.example_input_array is None:\r\n                    raise ValueError(\r\n                        \"Choosing method=`trace` requires either `example_inputs`\"\r\n                        \" or `model.example_input_array` to be defined.\"\r\n                    )\r\n                example_inputs = self.example_input_array\r\n\r\n            if kwargs.get(\"check_inputs\") is not None:\r\n                kwargs[\"check_inputs\"] = self._on_before_batch_transfer(kwargs[\"check_inputs\"])\r\n                kwargs[\"check_inputs\"] = self._apply_batch_transfer_handler(kwargs[\"check_inputs\"])\r\n\r\n            example_inputs = self._on_before_batch_transfer(example_inputs)\r\n            example_inputs = self._apply_batch_transfer_handler(example_inputs)\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.trace(func=self.eval(), example_inputs=example_inputs, **kwargs)\r\n        else:\r\n            raise ValueError(f\"The 'method' parameter only supports 'script' or 'trace', but value given was: {method}\")\r\n\r\n        self.train(mode)\r\n\r\n        if file_path is not None:\r\n            fs = get_filesystem(file_path)\r\n            with fs.open(file_path, \"wb\") as f:\r\n                torch.jit.save(torchscript_module, f)\r\n\r\n        return torchscript_module", "code_tokens": ["def", "to_torchscript", "(", "self", ",", "file_path", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "method", ":", "Optional", "[", "str", "]", "=", "STRING", ",", "example_inputs", ":", "Optional", "[", "Any", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Union", "[", "ScriptModule", ",", "dict", "[", "str", ",", "ScriptModule", "]", "]", ":", "STRING", "mode", "=", "self", ".", "training", "if", "method", "=", "=", "STRING", ":", "with", "_jit_is_scripting", "(", ")", ":", "torchscript_module", "=", "torch", ".", "jit", ".", "script", "(", "self", ".", "eval", "(", ")", ",", "*", "*", "kwargs", ")", "elif", "method", "=", "=", "STRING", ":", "if", "example_inputs", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "example_inputs", "=", "self", ".", "example_input_array", "if", "kwargs", ".", "get", "(", "STRING", ")", "is", "not", "None", ":", "kwargs", "[", "STRING", "]", "=", "self", ".", "_on_before_batch_transfer", "(", "kwargs", "[", "STRING", "]", ")", "kwargs", "[", "STRING", "]", "=", "self", ".", "_apply_batch_transfer_handler", "(", "kwargs", "[", "STRING", "]", ")", "example_inputs", "=", "self", ".", "_on_before_batch_transfer", "(", "example_inputs", ")", "example_inputs", "=", "self", ".", "_apply_batch_transfer_handler", "(", "example_inputs", ")", "with", "_jit_is_scripting", "(", ")", ":", "torchscript_module", "=", "torch", ".", "jit", ".", "trace", "(", "func", "=", "self", ".", "eval", "(", ")", ",", "example_inputs", "=", "example_inputs", ",", "*", "*", "kwargs", ")", "else", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "train", "(", "mode", ")", "if", "file_path", "is", "not", "None", ":", "fs", "=", "get_filesystem", "(", "file_path", ")", "with", "fs", ".", "open", "(", "file_path", ",", "STRING", ")", "as", "f", ":", "torch", ".", "jit", ".", "save", "(", "torchscript_module", ",", "f", ")", "return", "torchscript_module"], "docstring": "By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,", "docstring_tokens": ["by", "default", "compiles", "the", "whole", "model", "to", "a", "class", "torch", "jit", "scriptmodule", "if", "you", "want", "to", "use", "tracing"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1487, "end_line": 1572, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_605", "original_string": "def to_tensorrt(\r\n        self,\r\n        file_path: Optional[Union[str, Path, BytesIO]] = None,\r\n        input_sample: Optional[Any] = None,\r\n        ir: Literal[\"default\", \"dynamo\", \"ts\"] = \"default\",\r\n        output_format: Literal[\"exported_program\", \"torchscript\"] = \"exported_program\",\r\n        retrace: bool = False,\r\n        default_device: Union[str, torch.device] = \"cuda\",\r\n        **compile_kwargs: Any,\r\n    ) -> Union[ScriptModule, torch.fx.GraphModule]:\r\n        \"\"\"Export the model to ScriptModule or GraphModule using TensorRT compile backend.\r\n\r\n        Args:\r\n            file_path: Path where to save the tensorrt model. Default: None (no file saved).\r\n            input_sample: inputs to be used during `torch_tensorrt.compile`.\r\n                Default: None (Use :attr:`example_input_array`).\r\n            ir: The IR mode to use for TensorRT compilation. Default: \"default\".\r\n            output_format: The format of the output model. Default: \"exported_program\".\r\n            retrace: Whether to retrace the model. Default: False.\r\n            default_device: The device to use for the model when the current model is not in CUDA. Default: \"cuda\".\r\n            **compile_kwargs: Additional arguments that will be passed to the TensorRT compile function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            exported_program = model.to_tensorrt(\r\n                file_path=\"export.ep\",\r\n                inputs=input_sample,\r\n            )\r\n\r\n        \"\"\"\r\n        if not _TORCH_GREATER_EQUAL_2_2:\r\n            raise MisconfigurationException(\r\n                f\"TensorRT export requires PyTorch 2.2 or higher. Current version is {torch.__version__}.\"\r\n            )\r\n\r\n        if not _TORCH_TRT_AVAILABLE:\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_tensorrt` requires `torch_tensorrt` to be installed. \"\r\n            )\r\n\r\n        mode = self.training\r\n        device = self.device\r\n        if self.device.type != \"cuda\":\r\n            default_device = torch.device(default_device) if isinstance(default_device, str) else default_device\r\n\r\n            if not torch.cuda.is_available() or default_device.type != \"cuda\":\r\n                raise MisconfigurationException(\r\n                    f\"TensorRT only supports CUDA devices. The current device is {self.device}.\"\r\n                    f\" Please set the `default_device` argument to a CUDA device.\"\r\n                )\r\n\r\n            self.to(default_device)\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to TensorRT since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        import torch_tensorrt\r\n\r\n        input_sample = copy.deepcopy((input_sample,) if isinstance(input_sample, torch.Tensor) else input_sample)\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        with _jit_is_scripting() if ir == \"ts\" else nullcontext():\r\n            trt_obj = torch_tensorrt.compile(\r\n                module=self.eval(),\r\n                ir=ir,\r\n                inputs=input_sample,\r\n                **compile_kwargs,\r\n            )\r\n        self.train(mode)\r\n        self.to(device)\r\n\r\n        if file_path is not None:\r\n            if ir == \"ts\":\r\n                if output_format != \"torchscript\":\r\n                    raise ValueError(\r\n                        \"TensorRT with IR mode 'ts' only supports output format 'torchscript'.\"\r\n                        f\" The current output format is {output_format}.\"\r\n                    )\r\n                assert isinstance(trt_obj, (torch.jit.ScriptModule, torch.jit.ScriptFunction)), (\r\n                    f\"Expected TensorRT object to be a ScriptModule, but got {type(trt_obj)}.\"\r\n                )\r\n                torch.jit.save(trt_obj, file_path)\r\n            else:\r\n                torch_tensorrt.save(\r\n                    trt_obj,\r\n                    file_path,\r\n                    inputs=input_sample,\r\n                    output_format=output_format,\r\n                    retrace=retrace,\r\n                )\r\n        return trt_obj", "language": "python", "code": "def to_tensorrt(\r\n        self,\r\n        file_path: Optional[Union[str, Path, BytesIO]] = None,\r\n        input_sample: Optional[Any] = None,\r\n        ir: Literal[\"default\", \"dynamo\", \"ts\"] = \"default\",\r\n        output_format: Literal[\"exported_program\", \"torchscript\"] = \"exported_program\",\r\n        retrace: bool = False,\r\n        default_device: Union[str, torch.device] = \"cuda\",\r\n        **compile_kwargs: Any,\r\n    ) -> Union[ScriptModule, torch.fx.GraphModule]:\r\n        \"\"\"Export the model to ScriptModule or GraphModule using TensorRT compile backend.\r\n\r\n        Args:\r\n            file_path: Path where to save the tensorrt model. Default: None (no file saved).\r\n            input_sample: inputs to be used during `torch_tensorrt.compile`.\r\n                Default: None (Use :attr:`example_input_array`).\r\n            ir: The IR mode to use for TensorRT compilation. Default: \"default\".\r\n            output_format: The format of the output model. Default: \"exported_program\".\r\n            retrace: Whether to retrace the model. Default: False.\r\n            default_device: The device to use for the model when the current model is not in CUDA. Default: \"cuda\".\r\n            **compile_kwargs: Additional arguments that will be passed to the TensorRT compile function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            exported_program = model.to_tensorrt(\r\n                file_path=\"export.ep\",\r\n                inputs=input_sample,\r\n            )\r\n\r\n        \"\"\"\r\n        if not _TORCH_GREATER_EQUAL_2_2:\r\n            raise MisconfigurationException(\r\n                f\"TensorRT export requires PyTorch 2.2 or higher. Current version is {torch.__version__}.\"\r\n            )\r\n\r\n        if not _TORCH_TRT_AVAILABLE:\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_tensorrt` requires `torch_tensorrt` to be installed. \"\r\n            )\r\n\r\n        mode = self.training\r\n        device = self.device\r\n        if self.device.type != \"cuda\":\r\n            default_device = torch.device(default_device) if isinstance(default_device, str) else default_device\r\n\r\n            if not torch.cuda.is_available() or default_device.type != \"cuda\":\r\n                raise MisconfigurationException(\r\n                    f\"TensorRT only supports CUDA devices. The current device is {self.device}.\"\r\n                    f\" Please set the `default_device` argument to a CUDA device.\"\r\n                )\r\n\r\n            self.to(default_device)\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to TensorRT since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        import torch_tensorrt\r\n\r\n        input_sample = copy.deepcopy((input_sample,) if isinstance(input_sample, torch.Tensor) else input_sample)\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        with _jit_is_scripting() if ir == \"ts\" else nullcontext():\r\n            trt_obj = torch_tensorrt.compile(\r\n                module=self.eval(),\r\n                ir=ir,\r\n                inputs=input_sample,\r\n                **compile_kwargs,\r\n            )\r\n        self.train(mode)\r\n        self.to(device)\r\n\r\n        if file_path is not None:\r\n            if ir == \"ts\":\r\n                if output_format != \"torchscript\":\r\n                    raise ValueError(\r\n                        \"TensorRT with IR mode 'ts' only supports output format 'torchscript'.\"\r\n                        f\" The current output format is {output_format}.\"\r\n                    )\r\n                assert isinstance(trt_obj, (torch.jit.ScriptModule, torch.jit.ScriptFunction)), (\r\n                    f\"Expected TensorRT object to be a ScriptModule, but got {type(trt_obj)}.\"\r\n                )\r\n                torch.jit.save(trt_obj, file_path)\r\n            else:\r\n                torch_tensorrt.save(\r\n                    trt_obj,\r\n                    file_path,\r\n                    inputs=input_sample,\r\n                    output_format=output_format,\r\n                    retrace=retrace,\r\n                )\r\n        return trt_obj", "code_tokens": ["def", "to_tensorrt", "(", "self", ",", "file_path", ":", "Optional", "[", "Union", "[", "str", ",", "Path", ",", "BytesIO", "]", "]", "=", "None", ",", "input_sample", ":", "Optional", "[", "Any", "]", "=", "None", ",", "ir", ":", "Literal", "[", "STRING", ",", "STRING", ",", "STRING", "]", "=", "STRING", ",", "output_format", ":", "Literal", "[", "STRING", ",", "STRING", "]", "=", "STRING", ",", "retrace", ":", "bool", "=", "False", ",", "default_device", ":", "Union", "[", "str", ",", "torch", ".", "device", "]", "=", "STRING", ",", "*", "*", "compile_kwargs", ":", "Any", ",", ")", "-", ">", "Union", "[", "ScriptModule", ",", "torch", ".", "fx", ".", "GraphModule", "]", ":", "STRING", "if", "not", "_TORCH_GREATER_EQUAL_2_2", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "if", "not", "_TORCH_TRT_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "fSTRING", ")", "mode", "=", "self", ".", "training", "device", "=", "self", ".", "device", "if", "self", ".", "device", ".", "type", "!", "=", "STRING", ":", "default_device", "=", "torch", ".", "device", "(", "default_device", ")", "if", "isinstance", "(", "default_device", ",", "str", ")", "else", "default_device", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "or", "default_device", ".", "type", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", ")", "self", ".", "to", "(", "default_device", ")", "if", "input_sample", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "STRING", "STRING", ")", "input_sample", "=", "self", ".", "example_input_array", "import", "torch_tensorrt", "input_sample", "=", "copy", ".", "deepcopy", "(", "(", "input_sample", ",", ")", "if", "isinstance", "(", "input_sample", ",", "torch", ".", "Tensor", ")", "else", "input_sample", ")", "input_sample", "=", "self", ".", "_on_before_batch_transfer", "(", "input_sample", ")", "input_sample", "=", "self", ".", "_apply_batch_transfer_handler", "(", "input_sample", ")", "with", "_jit_is_scripting", "(", ")", "if", "ir", "=", "=", "STRING", "else", "nullcontext", "(", ")", ":", "trt_obj", "=", "torch_tensorrt", ".", "compile", "(", "module", "=", "self", ".", "eval", "(", ")", ",", "ir", "=", "ir", ",", "inputs", "=", "input_sample", ",", "*", "*", "compile_kwargs", ",", ")", "self", ".", "train", "(", "mode", ")", "self", ".", "to", "(", "device", ")", "if", "file_path", "is", "not", "None", ":", "if", "ir", "=", "=", "STRING", ":", "if", "output_format", "!", "=", "STRING", ":", "raise", "ValueError", "(", "STRING", "fSTRING", ")", "assert", "isinstance", "(", "trt_obj", ",", "(", "torch", ".", "jit", ".", "ScriptModule", ",", "torch", ".", "jit", ".", "ScriptFunction", ")", ")", ",", "(", "fSTRING", ")", "torch", ".", "jit", ".", "save", "(", "trt_obj", ",", "file_path", ")", "else", ":", "torch_tensorrt", ".", "save", "(", "trt_obj", ",", "file_path", ",", "inputs", "=", "input_sample", ",", "output_format", "=", "output_format", ",", "retrace", "=", "retrace", ",", ")", "return", "trt_obj"], "docstring": "Export the model to ScriptModule or GraphModule using TensorRT compile backend.", "docstring_tokens": ["export", "the", "model", "to", "scriptmodule", "or", "graphmodule", "using", "tensorrt", "compile", "backend"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1575, "end_line": 1683, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_606", "original_string": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        strict: Optional[bool] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\r\n        passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    drop_prob: 0.2\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningModule` for use.\r\n\r\n                If your model's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\r\n                set, in which case it defaults to the value of ``LightningModule.strict_loading``.\r\n            \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\r\n            **class** to call it instead of the :class:`LightningModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Note:\r\n            To ensure all layers can be loaded from the checkpoint, this function will call\r\n            :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\r\n            model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\r\n            not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\r\n            case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            map_location = {'cuda:1':'cuda:0'}\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                map_location=map_location\r\n            )\r\n\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                PATH,\r\n                num_layers=128,\r\n                pretrained_ckpt_path=NEW_PATH,\r\n            )\r\n\r\n            pretrained_model.eval()\r\n            pretrained_model.freeze()\r\n            y_hat = pretrained_model(x)\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location,\r\n            hparams_file,\r\n            strict,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "language": "python", "code": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        strict: Optional[bool] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\r\n        passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    drop_prob: 0.2\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningModule` for use.\r\n\r\n                If your model's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\r\n                set, in which case it defaults to the value of ``LightningModule.strict_loading``.\r\n            \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\r\n            **class** to call it instead of the :class:`LightningModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Note:\r\n            To ensure all layers can be loaded from the checkpoint, this function will call\r\n            :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\r\n            model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\r\n            not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\r\n            case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            map_location = {'cuda:1':'cuda:0'}\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                map_location=map_location\r\n            )\r\n\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                PATH,\r\n                num_layers=128,\r\n                pretrained_ckpt_path=NEW_PATH,\r\n            )\r\n\r\n            pretrained_model.eval()\r\n            pretrained_model.freeze()\r\n            y_hat = pretrained_model(x)\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location,\r\n            hparams_file,\r\n            strict,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "code_tokens": ["def", "load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ":", "Union", "[", "_PATH", ",", "IO", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "hparams_file", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "strict", ":", "Optional", "[", "bool", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Self", ":", "rSTRING", "loaded", "=", "_load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ",", "map_location", ",", "hparams_file", ",", "strict", ",", "*", "*", "kwargs", ",", ")", "return", "cast", "(", "Self", ",", "loaded", ")"], "docstring": "r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments", "docstring_tokens": ["r", "primary", "way", "of", "loading", "a", "model", "from", "a", "checkpoint", "when", "lightning", "saves", "a", "checkpoint", "it", "stores", "the", "arguments"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1686, "end_line": 1782, "has_examples": true, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "function_607", "original_string": "def _jit_is_scripting() -> Generator:\r\n    \"\"\"Workaround for https://github.com/pytorch/pytorch/issues/67146.\"\"\"\r\n    LightningModule._jit_is_scripting = True\r\n    try:\r\n        yield\r\n    finally:\r\n        LightningModule._jit_is_scripting = False", "language": "python", "code": "def _jit_is_scripting() -> Generator:\r\n    \"\"\"Workaround for https://github.com/pytorch/pytorch/issues/67146.\"\"\"\r\n    LightningModule._jit_is_scripting = True\r\n    try:\r\n        yield\r\n    finally:\r\n        LightningModule._jit_is_scripting = False", "code_tokens": ["def", "_jit_is_scripting", "(", ")", "-", ">", "Generator", ":", "STRING", "LightningModule", ".", "_jit_is_scripting", "=", "True", "try", ":", "yield", "finally", ":", "LightningModule", ".", "_jit_is_scripting", "=", "False"], "docstring": "Workaround for https://github.com/pytorch/pytorch/issues/67146.", "docstring_tokens": ["workaround", "for", "https", "github", "com", "pytorch", "pytorch", "issues", "67146"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\module.py", "start_line": 1792, "end_line": 1798, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "function_608", "original_string": "def toggle_model(self, sync_grad: bool = True) -> Generator[None, None, None]:\r\n        \"\"\"This function is just a helper for advanced users.\r\n\r\n        Considering the current optimizer as A and all other optimizers as B.\r\n        Toggling means all parameters from B exclusive to A will have ``requires_grad`` set to False.\r\n\r\n        When performing gradient accumulation, there is no need to perform grad synchronization\r\n        during the accumulation phase.\r\n        Setting `sync_grad` to False will block this synchronization and improve performance.\r\n\r\n        \"\"\"\r\n        from lightning.pytorch.loops.utilities import _block_parallel_sync_behavior\r\n\r\n        assert self._strategy is not None\r\n        lightning_module = self._strategy.lightning_module\r\n        assert lightning_module is not None\r\n        with _block_parallel_sync_behavior(self._strategy, block=(not sync_grad)):\r\n            lightning_module.toggle_optimizer(self)\r\n            yield\r\n            lightning_module.untoggle_optimizer(self)", "language": "python", "code": "def toggle_model(self, sync_grad: bool = True) -> Generator[None, None, None]:\r\n        \"\"\"This function is just a helper for advanced users.\r\n\r\n        Considering the current optimizer as A and all other optimizers as B.\r\n        Toggling means all parameters from B exclusive to A will have ``requires_grad`` set to False.\r\n\r\n        When performing gradient accumulation, there is no need to perform grad synchronization\r\n        during the accumulation phase.\r\n        Setting `sync_grad` to False will block this synchronization and improve performance.\r\n\r\n        \"\"\"\r\n        from lightning.pytorch.loops.utilities import _block_parallel_sync_behavior\r\n\r\n        assert self._strategy is not None\r\n        lightning_module = self._strategy.lightning_module\r\n        assert lightning_module is not None\r\n        with _block_parallel_sync_behavior(self._strategy, block=(not sync_grad)):\r\n            lightning_module.toggle_optimizer(self)\r\n            yield\r\n            lightning_module.untoggle_optimizer(self)", "code_tokens": ["def", "toggle_model", "(", "self", ",", "sync_grad", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "from", "lightning", ".", "pytorch", ".", "loops", ".", "utilities", "import", "_block_parallel_sync_behavior", "assert", "self", ".", "_strategy", "is", "not", "None", "lightning_module", "=", "self", ".", "_strategy", ".", "lightning_module", "assert", "lightning_module", "is", "not", "None", "with", "_block_parallel_sync_behavior", "(", "self", ".", "_strategy", ",", "block", "=", "(", "not", "sync_grad", ")", ")", ":", "lightning_module", ".", "toggle_optimizer", "(", "self", ")", "yield", "lightning_module", ".", "untoggle_optimizer", "(", "self", ")"], "docstring": "This function is just a helper for advanced users.", "docstring_tokens": ["this", "function", "is", "just", "a", "helper", "for", "advanced", "users"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\optimizer.py", "start_line": 62, "end_line": 82, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "function_609", "original_string": "def step(self, closure: Optional[Callable[[], Any]] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Performs a single optimization step (parameter update).\r\n\r\n        Args:\r\n            closure: An optional optimizer closure.\r\n            kwargs: Any additional arguments to the ``optimizer.step()`` call.\r\n\r\n        Returns:\r\n            The output from the step call, which is generally the output of the closure execution.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n\r\n                loss_gen = self.compute_generator_loss(...)\r\n                opt_gen.zero_grad()\r\n                self.manual_backward(loss_gen)\r\n                opt_gen.step()\r\n\r\n                loss_dis = self.compute_discriminator_loss(...)\r\n\r\n                opt_dis.zero_grad()\r\n                self.manual_backward(loss_dis)\r\n                opt_dis.step()\r\n\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n                accumulated_grad_batches = batch_idx % 2 == 0\r\n\r\n                def closure_gen():\r\n                    loss_gen = self.compute_generator_loss(...)\r\n                    self.manual_backward(loss_gen)\r\n                    if accumulated_grad_batches:\r\n                        opt_gen.zero_grad()\r\n\r\n                with opt_gen.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_gen.step(closure=closure_gen)\r\n\r\n                def closure_dis():\r\n                    loss_dis = self.compute_discriminator_loss(...)\r\n                    self.manual_backward(loss_dis)\r\n                    if accumulated_grad_batches:\r\n                        opt_dis.zero_grad()\r\n\r\n                with opt_dis.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_dis.step(closure=closure_dis)\r\n\r\n        \"\"\"\r\n        self._on_before_step()\r\n\r\n        if closure is None:\r\n            closure = do_nothing_closure\r\n        elif not callable(closure):\r\n            raise MisconfigurationException(\"When `optimizer.step(closure)` is called, the closure should be callable\")\r\n\r\n        assert self._strategy is not None\r\n        step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\r\n\r\n        self._on_after_step()\r\n\r\n        return step_output", "language": "python", "code": "def step(self, closure: Optional[Callable[[], Any]] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Performs a single optimization step (parameter update).\r\n\r\n        Args:\r\n            closure: An optional optimizer closure.\r\n            kwargs: Any additional arguments to the ``optimizer.step()`` call.\r\n\r\n        Returns:\r\n            The output from the step call, which is generally the output of the closure execution.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n\r\n                loss_gen = self.compute_generator_loss(...)\r\n                opt_gen.zero_grad()\r\n                self.manual_backward(loss_gen)\r\n                opt_gen.step()\r\n\r\n                loss_dis = self.compute_discriminator_loss(...)\r\n\r\n                opt_dis.zero_grad()\r\n                self.manual_backward(loss_dis)\r\n                opt_dis.step()\r\n\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n                accumulated_grad_batches = batch_idx % 2 == 0\r\n\r\n                def closure_gen():\r\n                    loss_gen = self.compute_generator_loss(...)\r\n                    self.manual_backward(loss_gen)\r\n                    if accumulated_grad_batches:\r\n                        opt_gen.zero_grad()\r\n\r\n                with opt_gen.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_gen.step(closure=closure_gen)\r\n\r\n                def closure_dis():\r\n                    loss_dis = self.compute_discriminator_loss(...)\r\n                    self.manual_backward(loss_dis)\r\n                    if accumulated_grad_batches:\r\n                        opt_dis.zero_grad()\r\n\r\n                with opt_dis.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_dis.step(closure=closure_dis)\r\n\r\n        \"\"\"\r\n        self._on_before_step()\r\n\r\n        if closure is None:\r\n            closure = do_nothing_closure\r\n        elif not callable(closure):\r\n            raise MisconfigurationException(\"When `optimizer.step(closure)` is called, the closure should be callable\")\r\n\r\n        assert self._strategy is not None\r\n        step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\r\n\r\n        self._on_after_step()\r\n\r\n        return step_output", "code_tokens": ["def", "step", "(", "self", ",", "closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "Any", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "self", ".", "_on_before_step", "(", ")", "if", "closure", "is", "None", ":", "closure", "=", "do_nothing_closure", "elif", "not", "callable", "(", "closure", ")", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "assert", "self", ".", "_strategy", "is", "not", "None", "step_output", "=", "self", ".", "_strategy", ".", "optimizer_step", "(", "self", ".", "_optimizer", ",", "closure", ",", "*", "*", "kwargs", ")", "self", ".", "_on_after_step", "(", ")", "return", "step_output"], "docstring": "Performs a single optimization step (parameter update).", "docstring_tokens": ["performs", "a", "single", "optimization", "step", "parameter", "update"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\optimizer.py", "start_line": 84, "end_line": 157, "has_examples": true, "num_comments": 7, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "function_610", "original_string": "def _init_optimizers_and_lr_schedulers(\r\n    model: \"pl.LightningModule\",\r\n) -> tuple[list[Optimizer], list[LRSchedulerConfig]]:\r\n    \"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\r\n    from lightning.pytorch.trainer import call\r\n\r\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\r\n\r\n    if optim_conf is None:\r\n        rank_zero_warn(\r\n            \"`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\",\r\n        )\r\n        optim_conf = _MockOptimizer()\r\n\r\n    optimizers, lr_schedulers, monitor = _configure_optimizers(optim_conf)\r\n    lr_scheduler_configs = (\r\n        _configure_schedulers_automatic_opt(lr_schedulers, monitor)\r\n        if model.automatic_optimization\r\n        else _configure_schedulers_manual_opt(lr_schedulers)\r\n    )\r\n    _validate_multiple_optimizers_support(optimizers, model)\r\n    _validate_optimizers_attached(optimizers, lr_scheduler_configs)\r\n    _validate_scheduler_api(lr_scheduler_configs, model)\r\n    return optimizers, lr_scheduler_configs", "language": "python", "code": "def _init_optimizers_and_lr_schedulers(\r\n    model: \"pl.LightningModule\",\r\n) -> tuple[list[Optimizer], list[LRSchedulerConfig]]:\r\n    \"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\r\n    from lightning.pytorch.trainer import call\r\n\r\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\r\n\r\n    if optim_conf is None:\r\n        rank_zero_warn(\r\n            \"`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\",\r\n        )\r\n        optim_conf = _MockOptimizer()\r\n\r\n    optimizers, lr_schedulers, monitor = _configure_optimizers(optim_conf)\r\n    lr_scheduler_configs = (\r\n        _configure_schedulers_automatic_opt(lr_schedulers, monitor)\r\n        if model.automatic_optimization\r\n        else _configure_schedulers_manual_opt(lr_schedulers)\r\n    )\r\n    _validate_multiple_optimizers_support(optimizers, model)\r\n    _validate_optimizers_attached(optimizers, lr_scheduler_configs)\r\n    _validate_scheduler_api(lr_scheduler_configs, model)\r\n    return optimizers, lr_scheduler_configs", "code_tokens": ["def", "_init_optimizers_and_lr_schedulers", "(", "model", ":", "STRING", ",", ")", "-", ">", "tuple", "[", "list", "[", "Optimizer", "]", ",", "list", "[", "LRSchedulerConfig", "]", "]", ":", "STRING", "from", "lightning", ".", "pytorch", ".", "trainer", "import", "call", "optim_conf", "=", "call", ".", "_call_lightning_module_hook", "(", "model", ".", "trainer", ",", "STRING", ",", "pl_module", "=", "model", ")", "if", "optim_conf", "is", "None", ":", "rank_zero_warn", "(", "STRING", ",", ")", "optim_conf", "=", "_MockOptimizer", "(", ")", "optimizers", ",", "lr_schedulers", ",", "monitor", "=", "_configure_optimizers", "(", "optim_conf", ")", "lr_scheduler_configs", "=", "(", "_configure_schedulers_automatic_opt", "(", "lr_schedulers", ",", "monitor", ")", "if", "model", ".", "automatic_optimization", "else", "_configure_schedulers_manual_opt", "(", "lr_schedulers", ")", ")", "_validate_multiple_optimizers_support", "(", "optimizers", ",", "model", ")", "_validate_optimizers_attached", "(", "optimizers", ",", "lr_scheduler_configs", ")", "_validate_scheduler_api", "(", "lr_scheduler_configs", ",", "model", ")", "return", "optimizers", ",", "lr_scheduler_configs"], "docstring": "Calls `LightningModule.configure_optimizers` and parses and validates the output.", "docstring_tokens": ["calls", "lightningmodule", "configure_optimizers", "and", "parses", "and", "validates", "the", "output"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\optimizer.py", "start_line": 173, "end_line": 196, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "function_611", "original_string": "def _configure_schedulers_automatic_opt(schedulers: list, monitor: Optional[str]) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` with relevant information, when using automatic optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            supported_keys = {field.name for field in fields(LRSchedulerConfig)}\r\n            extra_keys = scheduler.keys() - supported_keys\r\n            if extra_keys:\r\n                rank_zero_warn(\r\n                    f\"Found unsupported keys in the lr scheduler dict: {extra_keys}.\"\r\n                    \" HINT: remove them from the output of `configure_optimizers`.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n                scheduler = {k: v for k, v in scheduler.items() if k in supported_keys}\r\n            if \"scheduler\" not in scheduler:\r\n                raise MisconfigurationException(\r\n                    'The lr scheduler dict must have the key \"scheduler\" with its item being an lr scheduler'\r\n                )\r\n            if \"interval\" in scheduler and scheduler[\"interval\"] not in (\"step\", \"epoch\"):\r\n                raise MisconfigurationException(\r\n                    'The \"interval\" key in lr scheduler dict must be \"step\" or \"epoch\"'\r\n                    f' but is \"{scheduler[\"interval\"]}\"'\r\n                )\r\n            scheduler[\"reduce_on_plateau\"] = scheduler.get(\r\n                \"reduce_on_plateau\", isinstance(scheduler[\"scheduler\"], optim.lr_scheduler.ReduceLROnPlateau)\r\n            )\r\n            if scheduler[\"reduce_on_plateau\"] and scheduler.get(\"monitor\") is None:\r\n                raise MisconfigurationException(\r\n                    \"The lr scheduler dict must include a monitor when a `ReduceLROnPlateau` scheduler is used.\"\r\n                    ' For example: {\"optimizer\": optimizer, \"lr_scheduler\":'\r\n                    ' {\"scheduler\": scheduler, \"monitor\": \"your_loss\"}}'\r\n                )\r\n            is_one_cycle = isinstance(scheduler[\"scheduler\"], optim.lr_scheduler.OneCycleLR)\r\n            if is_one_cycle and scheduler.get(\"interval\", \"epoch\") == \"epoch\":\r\n                rank_zero_warn(\r\n                    \"A `OneCycleLR` scheduler is using 'interval': 'epoch'.\"\r\n                    \" Are you sure you didn't mean 'interval': 'step'?\",\r\n                    category=RuntimeWarning,\r\n                )\r\n            config = LRSchedulerConfig(**scheduler)\r\n        elif isinstance(scheduler, ReduceLROnPlateau):\r\n            if monitor is None:\r\n                raise MisconfigurationException(\r\n                    \"`configure_optimizers` must include a monitor when a `ReduceLROnPlateau`\"\r\n                    \" scheduler is used. For example:\"\r\n                    ' {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"metric_to_track\"}'\r\n                )\r\n            config = LRSchedulerConfig(scheduler, reduce_on_plateau=True, monitor=monitor)\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "language": "python", "code": "def _configure_schedulers_automatic_opt(schedulers: list, monitor: Optional[str]) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` with relevant information, when using automatic optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            supported_keys = {field.name for field in fields(LRSchedulerConfig)}\r\n            extra_keys = scheduler.keys() - supported_keys\r\n            if extra_keys:\r\n                rank_zero_warn(\r\n                    f\"Found unsupported keys in the lr scheduler dict: {extra_keys}.\"\r\n                    \" HINT: remove them from the output of `configure_optimizers`.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n                scheduler = {k: v for k, v in scheduler.items() if k in supported_keys}\r\n            if \"scheduler\" not in scheduler:\r\n                raise MisconfigurationException(\r\n                    'The lr scheduler dict must have the key \"scheduler\" with its item being an lr scheduler'\r\n                )\r\n            if \"interval\" in scheduler and scheduler[\"interval\"] not in (\"step\", \"epoch\"):\r\n                raise MisconfigurationException(\r\n                    'The \"interval\" key in lr scheduler dict must be \"step\" or \"epoch\"'\r\n                    f' but is \"{scheduler[\"interval\"]}\"'\r\n                )\r\n            scheduler[\"reduce_on_plateau\"] = scheduler.get(\r\n                \"reduce_on_plateau\", isinstance(scheduler[\"scheduler\"], optim.lr_scheduler.ReduceLROnPlateau)\r\n            )\r\n            if scheduler[\"reduce_on_plateau\"] and scheduler.get(\"monitor\") is None:\r\n                raise MisconfigurationException(\r\n                    \"The lr scheduler dict must include a monitor when a `ReduceLROnPlateau` scheduler is used.\"\r\n                    ' For example: {\"optimizer\": optimizer, \"lr_scheduler\":'\r\n                    ' {\"scheduler\": scheduler, \"monitor\": \"your_loss\"}}'\r\n                )\r\n            is_one_cycle = isinstance(scheduler[\"scheduler\"], optim.lr_scheduler.OneCycleLR)\r\n            if is_one_cycle and scheduler.get(\"interval\", \"epoch\") == \"epoch\":\r\n                rank_zero_warn(\r\n                    \"A `OneCycleLR` scheduler is using 'interval': 'epoch'.\"\r\n                    \" Are you sure you didn't mean 'interval': 'step'?\",\r\n                    category=RuntimeWarning,\r\n                )\r\n            config = LRSchedulerConfig(**scheduler)\r\n        elif isinstance(scheduler, ReduceLROnPlateau):\r\n            if monitor is None:\r\n                raise MisconfigurationException(\r\n                    \"`configure_optimizers` must include a monitor when a `ReduceLROnPlateau`\"\r\n                    \" scheduler is used. For example:\"\r\n                    ' {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"metric_to_track\"}'\r\n                )\r\n            config = LRSchedulerConfig(scheduler, reduce_on_plateau=True, monitor=monitor)\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "code_tokens": ["def", "_configure_schedulers_automatic_opt", "(", "schedulers", ":", "list", ",", "monitor", ":", "Optional", "[", "str", "]", ")", "-", ">", "list", "[", "LRSchedulerConfig", "]", ":", "STRING", "lr_scheduler_configs", "=", "[", "]", "for", "scheduler", "in", "schedulers", ":", "if", "isinstance", "(", "scheduler", ",", "dict", ")", ":", "supported_keys", "=", "{", "field", ".", "name", "for", "field", "in", "fields", "(", "LRSchedulerConfig", ")", "}", "extra_keys", "=", "scheduler", ".", "keys", "(", ")", "-", "supported_keys", "if", "extra_keys", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ",", "category", "=", "RuntimeWarning", ",", ")", "scheduler", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "scheduler", ".", "items", "(", ")", "if", "k", "in", "supported_keys", "}", "if", "STRING", "not", "in", "scheduler", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "if", "STRING", "in", "scheduler", "and", "scheduler", "[", "STRING", "]", "not", "in", "(", "STRING", ",", "STRING", ")", ":", "raise", "MisconfigurationException", "(", "STRING", "fSTRING", ")", "scheduler", "[", "STRING", "]", "=", "scheduler", ".", "get", "(", "STRING", ",", "isinstance", "(", "scheduler", "[", "STRING", "]", ",", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ")", "if", "scheduler", "[", "STRING", "]", "and", "scheduler", ".", "get", "(", "STRING", ")", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", "STRING", ")", "is_one_cycle", "=", "isinstance", "(", "scheduler", "[", "STRING", "]", ",", "optim", ".", "lr_scheduler", ".", "OneCycleLR", ")", "if", "is_one_cycle", "and", "scheduler", ".", "get", "(", "STRING", ",", "STRING", ")", "=", "=", "STRING", ":", "rank_zero_warn", "(", "STRING", "STRING", ",", "category", "=", "RuntimeWarning", ",", ")", "config", "=", "LRSchedulerConfig", "(", "*", "*", "scheduler", ")", "elif", "isinstance", "(", "scheduler", ",", "ReduceLROnPlateau", ")", ":", "if", "monitor", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", "STRING", ")", "config", "=", "LRSchedulerConfig", "(", "scheduler", ",", "reduce_on_plateau", "=", "True", ",", "monitor", "=", "monitor", ")", "else", ":", "config", "=", "LRSchedulerConfig", "(", "scheduler", ")", "lr_scheduler_configs", ".", "append", "(", "config", ")", "return", "lr_scheduler_configs"], "docstring": "Convert each scheduler into `LRSchedulerConfig` with relevant information, when using automatic optimization.", "docstring_tokens": ["convert", "each", "scheduler", "into", "lrschedulerconfig", "with", "relevant", "information", "when", "using", "automatic", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\optimizer.py", "start_line": 249, "end_line": 301, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "function_612", "original_string": "def _configure_schedulers_manual_opt(schedulers: list) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual\r\n    optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            invalid_keys = {\"reduce_on_plateau\", \"monitor\", \"strict\"}\r\n            keys_to_warn = [k for k in scheduler if k in invalid_keys]\r\n\r\n            if keys_to_warn:\r\n                rank_zero_warn(\r\n                    f\"The lr scheduler dict contains the key(s) {keys_to_warn}, but the keys will be ignored.\"\r\n                    \" You need to call `lr_scheduler.step()` manually in manual optimization.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n            config = LRSchedulerConfig(**{key: scheduler[key] for key in scheduler if key not in invalid_keys})\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "language": "python", "code": "def _configure_schedulers_manual_opt(schedulers: list) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual\r\n    optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            invalid_keys = {\"reduce_on_plateau\", \"monitor\", \"strict\"}\r\n            keys_to_warn = [k for k in scheduler if k in invalid_keys]\r\n\r\n            if keys_to_warn:\r\n                rank_zero_warn(\r\n                    f\"The lr scheduler dict contains the key(s) {keys_to_warn}, but the keys will be ignored.\"\r\n                    \" You need to call `lr_scheduler.step()` manually in manual optimization.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n            config = LRSchedulerConfig(**{key: scheduler[key] for key in scheduler if key not in invalid_keys})\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "code_tokens": ["def", "_configure_schedulers_manual_opt", "(", "schedulers", ":", "list", ")", "-", ">", "list", "[", "LRSchedulerConfig", "]", ":", "STRING", "lr_scheduler_configs", "=", "[", "]", "for", "scheduler", "in", "schedulers", ":", "if", "isinstance", "(", "scheduler", ",", "dict", ")", ":", "invalid_keys", "=", "{", "STRING", ",", "STRING", ",", "STRING", "}", "keys_to_warn", "=", "[", "k", "for", "k", "in", "scheduler", "if", "k", "in", "invalid_keys", "]", "if", "keys_to_warn", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ",", "category", "=", "RuntimeWarning", ",", ")", "config", "=", "LRSchedulerConfig", "(", "*", "*", "{", "key", ":", "scheduler", "[", "key", "]", "for", "key", "in", "scheduler", "if", "key", "not", "in", "invalid_keys", "}", ")", "else", ":", "config", "=", "LRSchedulerConfig", "(", "scheduler", ")", "lr_scheduler_configs", ".", "append", "(", "config", ")", "return", "lr_scheduler_configs"], "docstring": "Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual", "docstring_tokens": ["convert", "each", "scheduler", "into", "lrschedulerconfig", "structure", "with", "relevant", "information", "when", "using", "manual"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\optimizer.py", "start_line": 304, "end_line": 326, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "function_613", "original_string": "def _convert_loaded_hparams(\r\n    model_args: dict[str, Any], hparams_type: Optional[Union[Callable, str]] = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert hparams according given type in callable or string (past) format.\"\"\"\r\n    if not hparams_type:\r\n        return model_args\r\n    if isinstance(hparams_type, str):\r\n        hparams_type = AttributeDict\r\n    return hparams_type(model_args)", "language": "python", "code": "def _convert_loaded_hparams(\r\n    model_args: dict[str, Any], hparams_type: Optional[Union[Callable, str]] = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert hparams according given type in callable or string (past) format.\"\"\"\r\n    if not hparams_type:\r\n        return model_args\r\n    if isinstance(hparams_type, str):\r\n        hparams_type = AttributeDict\r\n    return hparams_type(model_args)", "code_tokens": ["def", "_convert_loaded_hparams", "(", "model_args", ":", "dict", "[", "str", ",", "Any", "]", ",", "hparams_type", ":", "Optional", "[", "Union", "[", "Callable", ",", "str", "]", "]", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "hparams_type", ":", "return", "model_args", "if", "isinstance", "(", "hparams_type", ",", "str", ")", ":", "hparams_type", "=", "AttributeDict", "return", "hparams_type", "(", "model_args", ")"], "docstring": "Convert hparams according given type in callable or string (past) format.", "docstring_tokens": ["convert", "hparams", "according", "given", "type", "in", "callable", "or", "string", "past", "format"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\saving.py", "start_line": 201, "end_line": 212, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "function_614", "original_string": "def update_hparams(hparams: dict, updates: dict) -> None:\r\n    \"\"\"Overrides hparams with new values.\r\n\r\n    >>> hparams = {'c': 4}\r\n    >>> update_hparams(hparams, {'a': {'b': 2}, 'c': 1})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (2, 1)\r\n    >>> update_hparams(hparams, {'a': {'b': 4}, 'c': 7})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (4, 7)\r\n\r\n    Args:\r\n        hparams: the original params and also target object\r\n        updates: new params to be used as update\r\n\r\n    \"\"\"\r\n    for k, v in updates.items():\r\n        if k not in hparams:\r\n            hparams[k] = v\r\n            continue\r\n\r\n        if isinstance(v, dict):\r\n            update_hparams(hparams[k], updates[k])\r\n        else:\r\n            hparams.update({k: v})", "language": "python", "code": "def update_hparams(hparams: dict, updates: dict) -> None:\r\n    \"\"\"Overrides hparams with new values.\r\n\r\n    >>> hparams = {'c': 4}\r\n    >>> update_hparams(hparams, {'a': {'b': 2}, 'c': 1})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (2, 1)\r\n    >>> update_hparams(hparams, {'a': {'b': 4}, 'c': 7})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (4, 7)\r\n\r\n    Args:\r\n        hparams: the original params and also target object\r\n        updates: new params to be used as update\r\n\r\n    \"\"\"\r\n    for k, v in updates.items():\r\n        if k not in hparams:\r\n            hparams[k] = v\r\n            continue\r\n\r\n        if isinstance(v, dict):\r\n            update_hparams(hparams[k], updates[k])\r\n        else:\r\n            hparams.update({k: v})", "code_tokens": ["def", "update_hparams", "(", "hparams", ":", "dict", ",", "updates", ":", "dict", ")", "-", ">", "None", ":", "STRING", "for", "k", ",", "v", "in", "updates", ".", "items", "(", ")", ":", "if", "k", "not", "in", "hparams", ":", "hparams", "[", "k", "]", "=", "v", "continue", "if", "isinstance", "(", "v", ",", "dict", ")", ":", "update_hparams", "(", "hparams", "[", "k", "]", ",", "updates", "[", "k", "]", ")", "else", ":", "hparams", ".", "update", "(", "{", "k", ":", "v", "}", ")"], "docstring": "Overrides hparams with new values.", "docstring_tokens": ["overrides", "hparams", "with", "new", "values"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\saving.py", "start_line": 215, "end_line": 242, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "function_615", "original_string": "def load_hparams_from_tags_csv(tags_csv: _PATH) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_csv = os.path.join('.', 'testing-hparams.csv')\r\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\r\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_csv)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(tags_csv)\r\n    if not fs.exists(tags_csv):\r\n        rank_zero_warn(f\"Missing Tags: {tags_csv}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(tags_csv, \"r\", newline=\"\") as fp:\r\n        csv_reader = csv.reader(fp, delimiter=\",\")\r\n        return {row[0]: convert(row[1]) for row in list(csv_reader)[1:]}", "language": "python", "code": "def load_hparams_from_tags_csv(tags_csv: _PATH) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_csv = os.path.join('.', 'testing-hparams.csv')\r\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\r\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_csv)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(tags_csv)\r\n    if not fs.exists(tags_csv):\r\n        rank_zero_warn(f\"Missing Tags: {tags_csv}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(tags_csv, \"r\", newline=\"\") as fp:\r\n        csv_reader = csv.reader(fp, delimiter=\",\")\r\n        return {row[0]: convert(row[1]) for row in list(csv_reader)[1:]}", "code_tokens": ["def", "load_hparams_from_tags_csv", "(", "tags_csv", ":", "_PATH", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "fs", "=", "get_filesystem", "(", "tags_csv", ")", "if", "not", "fs", ".", "exists", "(", "tags_csv", ")", ":", "rank_zero_warn", "(", "fSTRING", ",", "category", "=", "RuntimeWarning", ")", "return", "{", "}", "with", "fs", ".", "open", "(", "tags_csv", ",", "STRING", ",", "newline", "=", "STRING", ")", "as", "fp", ":", "csv_reader", "=", "csv", ".", "reader", "(", "fp", ",", "delimiter", "=", "STRING", ")", "return", "{", "row", "[", "0", "]", ":", "convert", "(", "row", "[", "1", "]", ")", "for", "row", "in", "list", "(", "csv_reader", ")", "[", "1", ":", "]", "}"], "docstring": "Load hparams from a file.", "docstring_tokens": ["load", "hparams", "from", "a", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\saving.py", "start_line": 245, "end_line": 264, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "function_616", "original_string": "def load_hparams_from_yaml(config_yaml: _PATH, use_omegaconf: bool = True) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n        Args:\r\n            config_yaml: Path to config yaml file\r\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n                the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_yaml = './testing-hparams.yaml'\r\n    >>> save_hparams_to_yaml(path_yaml, hparams)\r\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_yaml)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not fs.exists(config_yaml):\r\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(config_yaml, \"r\") as fp:\r\n        hparams = yaml.full_load(fp)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        with contextlib.suppress(UnsupportedValueType, ValidationError):\r\n            return OmegaConf.create(hparams)\r\n    return hparams", "language": "python", "code": "def load_hparams_from_yaml(config_yaml: _PATH, use_omegaconf: bool = True) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n        Args:\r\n            config_yaml: Path to config yaml file\r\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n                the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_yaml = './testing-hparams.yaml'\r\n    >>> save_hparams_to_yaml(path_yaml, hparams)\r\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_yaml)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not fs.exists(config_yaml):\r\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(config_yaml, \"r\") as fp:\r\n        hparams = yaml.full_load(fp)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        with contextlib.suppress(UnsupportedValueType, ValidationError):\r\n            return OmegaConf.create(hparams)\r\n    return hparams", "code_tokens": ["def", "load_hparams_from_yaml", "(", "config_yaml", ":", "_PATH", ",", "use_omegaconf", ":", "bool", "=", "True", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "fs", "=", "get_filesystem", "(", "config_yaml", ")", "if", "not", "fs", ".", "exists", "(", "config_yaml", ")", ":", "rank_zero_warn", "(", "fSTRING", ",", "category", "=", "RuntimeWarning", ")", "return", "{", "}", "with", "fs", ".", "open", "(", "config_yaml", ",", "STRING", ")", "as", "fp", ":", "hparams", "=", "yaml", ".", "full_load", "(", "fp", ")", "if", "_OMEGACONF_AVAILABLE", "and", "use_omegaconf", ":", "from", "omegaconf", "import", "OmegaConf", "from", "omegaconf", ".", "errors", "import", "UnsupportedValueType", ",", "ValidationError", "with", "contextlib", ".", "suppress", "(", "UnsupportedValueType", ",", "ValidationError", ")", ":", "return", "OmegaConf", ".", "create", "(", "hparams", ")", "return", "hparams"], "docstring": "Load hparams from a file.", "docstring_tokens": ["load", "hparams", "from", "a", "file"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\saving.py", "start_line": 283, "end_line": 314, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "function_617", "original_string": "def save_hparams_to_yaml(config_yaml: _PATH, hparams: Union[dict, Namespace], use_omegaconf: bool = True) -> None:\r\n    \"\"\"\r\n    Args:\r\n        config_yaml: path to new YAML file\r\n        hparams: parameters to be saved\r\n        use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n            the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not _is_dir(fs, os.path.dirname(config_yaml)):\r\n        raise RuntimeError(f\"Missing folder: {os.path.dirname(config_yaml)}.\")\r\n\r\n    if isinstance(hparams, Namespace):\r\n        hparams = vars(hparams)\r\n    elif isinstance(hparams, AttributeDict):\r\n        hparams = dict(hparams)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.dictconfig import DictConfig\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        hparams = deepcopy(hparams)\r\n        hparams = apply_to_collection(hparams, DictConfig, OmegaConf.to_container, resolve=True)\r\n        with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n            try:\r\n                OmegaConf.save(hparams, fp)\r\n                return\r\n            except (UnsupportedValueType, ValidationError):\r\n                pass\r\n\r\n    if not isinstance(hparams, dict):\r\n        raise TypeError(\"hparams must be dictionary\")\r\n\r\n    hparams_allowed = {}\r\n    for k, v in hparams.items():\r\n        try:\r\n            v = v.name if isinstance(v, Enum) else v\r\n            yaml.dump(v)\r\n        except (TypeError, ValueError):\r\n            warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\r\n            hparams[k] = type(v).__name__\r\n        else:\r\n            hparams_allowed[k] = v\r\n\r\n    with fs.open(config_yaml, \"w\", newline=\"\") as fp:\r\n        yaml.dump(hparams_allowed, fp)", "language": "python", "code": "def save_hparams_to_yaml(config_yaml: _PATH, hparams: Union[dict, Namespace], use_omegaconf: bool = True) -> None:\r\n    \"\"\"\r\n    Args:\r\n        config_yaml: path to new YAML file\r\n        hparams: parameters to be saved\r\n        use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n            the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not _is_dir(fs, os.path.dirname(config_yaml)):\r\n        raise RuntimeError(f\"Missing folder: {os.path.dirname(config_yaml)}.\")\r\n\r\n    if isinstance(hparams, Namespace):\r\n        hparams = vars(hparams)\r\n    elif isinstance(hparams, AttributeDict):\r\n        hparams = dict(hparams)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.dictconfig import DictConfig\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        hparams = deepcopy(hparams)\r\n        hparams = apply_to_collection(hparams, DictConfig, OmegaConf.to_container, resolve=True)\r\n        with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n            try:\r\n                OmegaConf.save(hparams, fp)\r\n                return\r\n            except (UnsupportedValueType, ValidationError):\r\n                pass\r\n\r\n    if not isinstance(hparams, dict):\r\n        raise TypeError(\"hparams must be dictionary\")\r\n\r\n    hparams_allowed = {}\r\n    for k, v in hparams.items():\r\n        try:\r\n            v = v.name if isinstance(v, Enum) else v\r\n            yaml.dump(v)\r\n        except (TypeError, ValueError):\r\n            warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\r\n            hparams[k] = type(v).__name__\r\n        else:\r\n            hparams_allowed[k] = v\r\n\r\n    with fs.open(config_yaml, \"w\", newline=\"\") as fp:\r\n        yaml.dump(hparams_allowed, fp)", "code_tokens": ["def", "save_hparams_to_yaml", "(", "config_yaml", ":", "_PATH", ",", "hparams", ":", "Union", "[", "dict", ",", "Namespace", "]", ",", "use_omegaconf", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "STRING", "fs", "=", "get_filesystem", "(", "config_yaml", ")", "if", "not", "_is_dir", "(", "fs", ",", "os", ".", "path", ".", "dirname", "(", "config_yaml", ")", ")", ":", "raise", "RuntimeError", "(", "fSTRING", ")", "if", "isinstance", "(", "hparams", ",", "Namespace", ")", ":", "hparams", "=", "vars", "(", "hparams", ")", "elif", "isinstance", "(", "hparams", ",", "AttributeDict", ")", ":", "hparams", "=", "dict", "(", "hparams", ")", "if", "_OMEGACONF_AVAILABLE", "and", "use_omegaconf", ":", "from", "omegaconf", "import", "OmegaConf", "from", "omegaconf", ".", "dictconfig", "import", "DictConfig", "from", "omegaconf", ".", "errors", "import", "UnsupportedValueType", ",", "ValidationError", "hparams", "=", "deepcopy", "(", "hparams", ")", "hparams", "=", "apply_to_collection", "(", "hparams", ",", "DictConfig", ",", "OmegaConf", ".", "to_container", ",", "resolve", "=", "True", ")", "with", "fs", ".", "open", "(", "config_yaml", ",", "STRING", ",", "encoding", "=", "STRING", ")", "as", "fp", ":", "try", ":", "OmegaConf", ".", "save", "(", "hparams", ",", "fp", ")", "return", "except", "(", "UnsupportedValueType", ",", "ValidationError", ")", ":", "pass", "if", "not", "isinstance", "(", "hparams", ",", "dict", ")", ":", "raise", "TypeError", "(", "STRING", ")", "hparams_allowed", "=", "{", "}", "for", "k", ",", "v", "in", "hparams", ".", "items", "(", ")", ":", "try", ":", "v", "=", "v", ".", "name", "if", "isinstance", "(", "v", ",", "Enum", ")", "else", "v", "yaml", ".", "dump", "(", "v", ")", "except", "(", "TypeError", ",", "ValueError", ")", ":", "warn", "(", "fSTRING", ")", "hparams", "[", "k", "]", "=", "type", "(", "v", ")", ".", "__name__", "else", ":", "hparams_allowed", "[", "k", "]", "=", "v", "with", "fs", ".", "open", "(", "config_yaml", ",", "STRING", ",", "newline", "=", "STRING", ")", "as", "fp", ":", "yaml", ".", "dump", "(", "hparams_allowed", ",", "fp", ")"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\saving.py", "start_line": 317, "end_line": 369, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "function_618", "original_string": "def save_hyperparameters(\r\n        self,\r\n        *args: Any,\r\n        ignore: Optional[Union[Sequence[str], str]] = None,\r\n        frame: Optional[types.FrameType] = None,\r\n        logger: bool = True,\r\n    ) -> None:\r\n        \"\"\"Save arguments to ``hparams`` attribute.\r\n\r\n        Args:\r\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\r\n                or string names or arguments from class ``__init__``\r\n            ignore: an argument name or a list of argument names from\r\n                class ``__init__`` to be ignored\r\n            frame: a frame object. Default is None\r\n            logger: Whether to send the hyperparameters to the logger. Default: True\r\n\r\n        Example::\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # manually assign arguments\r\n            ...         self.save_hyperparameters('arg1', 'arg3')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class AutomaticArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # equivalent automatic\r\n            ...         self.save_hyperparameters()\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = AutomaticArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg2\": abc\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class SingleArgModel(HyperparametersMixin):\r\n            ...     def __init__(self, params):\r\n            ...         super().__init__()\r\n            ...         # manually assign single argument\r\n            ...         self.save_hyperparameters(params)\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\r\n            >>> model.hparams\r\n            \"p1\": 1\r\n            \"p2\": abc\r\n            \"p3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # pass argument(s) to ignore as a string or in a list\r\n            ...         self.save_hyperparameters(ignore='arg2')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n        \"\"\"\r\n        self._log_hyperparams = logger\r\n        given_hparams = _given_hyperparameters.get()\r\n        if given_hparams is None and not frame:\r\n            current_frame = inspect.currentframe()\r\n            if current_frame:\r\n                frame = current_frame.f_back\r\n        save_hyperparameters(self, *args, ignore=ignore, frame=frame, given_hparams=given_hparams)", "language": "python", "code": "def save_hyperparameters(\r\n        self,\r\n        *args: Any,\r\n        ignore: Optional[Union[Sequence[str], str]] = None,\r\n        frame: Optional[types.FrameType] = None,\r\n        logger: bool = True,\r\n    ) -> None:\r\n        \"\"\"Save arguments to ``hparams`` attribute.\r\n\r\n        Args:\r\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\r\n                or string names or arguments from class ``__init__``\r\n            ignore: an argument name or a list of argument names from\r\n                class ``__init__`` to be ignored\r\n            frame: a frame object. Default is None\r\n            logger: Whether to send the hyperparameters to the logger. Default: True\r\n\r\n        Example::\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # manually assign arguments\r\n            ...         self.save_hyperparameters('arg1', 'arg3')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class AutomaticArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # equivalent automatic\r\n            ...         self.save_hyperparameters()\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = AutomaticArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg2\": abc\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class SingleArgModel(HyperparametersMixin):\r\n            ...     def __init__(self, params):\r\n            ...         super().__init__()\r\n            ...         # manually assign single argument\r\n            ...         self.save_hyperparameters(params)\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\r\n            >>> model.hparams\r\n            \"p1\": 1\r\n            \"p2\": abc\r\n            \"p3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # pass argument(s) to ignore as a string or in a list\r\n            ...         self.save_hyperparameters(ignore='arg2')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n        \"\"\"\r\n        self._log_hyperparams = logger\r\n        given_hparams = _given_hyperparameters.get()\r\n        if given_hparams is None and not frame:\r\n            current_frame = inspect.currentframe()\r\n            if current_frame:\r\n                frame = current_frame.f_back\r\n        save_hyperparameters(self, *args, ignore=ignore, frame=frame, given_hparams=given_hparams)", "code_tokens": ["def", "save_hyperparameters", "(", "self", ",", "*", "args", ":", "Any", ",", "ignore", ":", "Optional", "[", "Union", "[", "Sequence", "[", "str", "]", ",", "str", "]", "]", "=", "None", ",", "frame", ":", "Optional", "[", "types", ".", "FrameType", "]", "=", "None", ",", "logger", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "_log_hyperparams", "=", "logger", "given_hparams", "=", "_given_hyperparameters", ".", "get", "(", ")", "if", "given_hparams", "is", "None", "and", "not", "frame", ":", "current_frame", "=", "inspect", ".", "currentframe", "(", ")", "if", "current_frame", ":", "frame", "=", "current_frame", ".", "f_back", "save_hyperparameters", "(", "self", ",", "*", "args", ",", "ignore", "=", "ignore", ",", "frame", "=", "frame", ",", "given_hparams", "=", "given_hparams", ")"], "docstring": "Save arguments to ``hparams`` attribute.", "docstring_tokens": ["save", "arguments", "to", "hparams", "attribute"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "start_line": 50, "end_line": 130, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "function_619", "original_string": "def hparams(self) -> Union[AttributeDict, MutableMapping]:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\r\n        the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\r\n\r\n        Returns:\r\n            Mutable hyperparameters dictionary\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams\"):\r\n            self._hparams = AttributeDict()\r\n        return self._hparams", "language": "python", "code": "def hparams(self) -> Union[AttributeDict, MutableMapping]:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\r\n        the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\r\n\r\n        Returns:\r\n            Mutable hyperparameters dictionary\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams\"):\r\n            self._hparams = AttributeDict()\r\n        return self._hparams", "code_tokens": ["def", "hparams", "(", "self", ")", "-", ">", "Union", "[", "AttributeDict", ",", "MutableMapping", "]", ":", "STRING", "if", "not", "hasattr", "(", "self", ",", "STRING", ")", ":", "self", ".", "_hparams", "=", "AttributeDict", "(", ")", "return", "self", ".", "_hparams"], "docstring": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For", "docstring_tokens": ["the", "collection", "of", "hyperparameters", "saved", "with", "meth", "save_hyperparameters", "it", "is", "mutable", "by", "the", "user", "for"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "start_line": 153, "end_line": 163, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "function_620", "original_string": "def hparams_initial(self) -> AttributeDict:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\r\n        Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\r\n\r\n        Returns:\r\n            AttributeDict: immutable initial hyperparameters\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams_initial\"):\r\n            return AttributeDict()\r\n        return copy.deepcopy(self._hparams_initial)", "language": "python", "code": "def hparams_initial(self) -> AttributeDict:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\r\n        Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\r\n\r\n        Returns:\r\n            AttributeDict: immutable initial hyperparameters\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams_initial\"):\r\n            return AttributeDict()\r\n        return copy.deepcopy(self._hparams_initial)", "code_tokens": ["def", "hparams_initial", "(", "self", ")", "-", ">", "AttributeDict", ":", "STRING", "if", "not", "hasattr", "(", "self", ",", "STRING", ")", ":", "return", "AttributeDict", "(", ")", "return", "copy", ".", "deepcopy", "(", "self", ".", "_hparams_initial", ")"], "docstring": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.", "docstring_tokens": ["the", "collection", "of", "hyperparameters", "saved", "with", "meth", "save_hyperparameters", "these", "contents", "are", "read", "only"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "start_line": 166, "end_line": 177, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_621", "original_string": "def _try_load(path_data: str, trials: int = 30, delta: float = 1.0) -> tuple[Tensor, Tensor]:\r\n        \"\"\"Resolving loading from the same time from multiple concurrent processes.\"\"\"\r\n        res, exception = None, None\r\n        assert trials, \"at least some trial has to be set\"\r\n        assert os.path.isfile(path_data), f\"missing file: {path_data}\"\r\n        for _ in range(trials):\r\n            try:\r\n                res = torch.load(path_data)\r\n            except Exception as ex:\r\n                exception = ex\r\n                time.sleep(delta * random.random())  # noqa: S311\r\n            else:\r\n                break\r\n        assert res is not None\r\n        if exception is not None:\r\n            raise exception\r\n        return res", "language": "python", "code": "def _try_load(path_data: str, trials: int = 30, delta: float = 1.0) -> tuple[Tensor, Tensor]:\r\n        \"\"\"Resolving loading from the same time from multiple concurrent processes.\"\"\"\r\n        res, exception = None, None\r\n        assert trials, \"at least some trial has to be set\"\r\n        assert os.path.isfile(path_data), f\"missing file: {path_data}\"\r\n        for _ in range(trials):\r\n            try:\r\n                res = torch.load(path_data)\r\n            except Exception as ex:\r\n                exception = ex\r\n                time.sleep(delta * random.random())  # noqa: S311\r\n            else:\r\n                break\r\n        assert res is not None\r\n        if exception is not None:\r\n            raise exception\r\n        return res", "code_tokens": ["def", "_try_load", "(", "path_data", ":", "str", ",", "trials", ":", "int", "=", "30", ",", "delta", ":", "float", "=", "1", ".", "0", ")", "-", ">", "tuple", "[", "Tensor", ",", "Tensor", "]", ":", "STRING", "res", ",", "exception", "=", "None", ",", "None", "assert", "trials", ",", "STRING", "assert", "os", ".", "path", ".", "isfile", "(", "path_data", ")", ",", "fSTRING", "for", "_", "in", "range", "(", "trials", ")", ":", "try", ":", "res", "=", "torch", ".", "load", "(", "path_data", ")", "except", "Exception", "as", "ex", ":", "exception", "=", "ex", "time", ".", "sleep", "(", "delta", "*", "random", ".", "random", "(", ")", ")", "#", "noqa", ":", "S311", "else", ":", "break", "assert", "res", "is", "not", "None", "if", "exception", "is", "not", "None", ":", "raise", "exception", "return", "res"], "docstring": "Resolving loading from the same time from multiple concurrent processes.", "docstring_tokens": ["resolving", "loading", "from", "the", "same", "time", "from", "multiple", "concurrent", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 102, "end_line": 120, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_622", "original_string": "def __init__(\r\n        self,\r\n        data_dir: str = _DATASETS_PATH,\r\n        val_split: int = 5000,\r\n        num_workers: int = 16,\r\n        normalize: bool = False,\r\n        seed: int = 42,\r\n        batch_size: int = 32,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            data_dir: where to save/load the data\r\n            val_split: how many of the training images to use for the validation split\r\n            num_workers: how many workers to use for loading data\r\n            normalize: If true applies image normalize\r\n            seed: starting seed for RNG.\r\n            batch_size: desired batch size.\r\n        \"\"\"\r\n        super().__init__()\r\n        if num_workers and _IS_WINDOWS:\r\n            warn(\r\n                f\"You have requested num_workers={num_workers} on Windows,\"\r\n                \" but currently recommended is 0, so we set it for you\"\r\n            )\r\n            num_workers = 0\r\n\r\n        self.data_dir = data_dir\r\n        self.val_split = val_split\r\n        self.num_workers = num_workers\r\n        self.normalize = normalize\r\n        self.seed = seed\r\n        self.batch_size = batch_size", "language": "python", "code": "def __init__(\r\n        self,\r\n        data_dir: str = _DATASETS_PATH,\r\n        val_split: int = 5000,\r\n        num_workers: int = 16,\r\n        normalize: bool = False,\r\n        seed: int = 42,\r\n        batch_size: int = 32,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            data_dir: where to save/load the data\r\n            val_split: how many of the training images to use for the validation split\r\n            num_workers: how many workers to use for loading data\r\n            normalize: If true applies image normalize\r\n            seed: starting seed for RNG.\r\n            batch_size: desired batch size.\r\n        \"\"\"\r\n        super().__init__()\r\n        if num_workers and _IS_WINDOWS:\r\n            warn(\r\n                f\"You have requested num_workers={num_workers} on Windows,\"\r\n                \" but currently recommended is 0, so we set it for you\"\r\n            )\r\n            num_workers = 0\r\n\r\n        self.data_dir = data_dir\r\n        self.val_split = val_split\r\n        self.num_workers = num_workers\r\n        self.normalize = normalize\r\n        self.seed = seed\r\n        self.batch_size = batch_size", "code_tokens": ["def", "__init__", "(", "self", ",", "data_dir", ":", "str", "=", "_DATASETS_PATH", ",", "val_split", ":", "int", "=", "5000", ",", "num_workers", ":", "int", "=", "16", ",", "normalize", ":", "bool", "=", "False", ",", "seed", ":", "int", "=", "42", ",", "batch_size", ":", "int", "=", "32", ",", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", ")", "if", "num_workers", "and", "_IS_WINDOWS", ":", "warn", "(", "fSTRING", "STRING", ")", "num_workers", "=", "0", "self", ".", "data_dir", "=", "data_dir", "self", ".", "val_split", "=", "val_split", "self", ".", "num_workers", "=", "num_workers", "self", ".", "normalize", "=", "normalize", "self", ".", "seed", "=", "seed", "self", ".", "batch_size", "=", "batch_size"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 155, "end_line": 187, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_623", "original_string": "def prepare_data(self) -> None:\r\n        \"\"\"Saves MNIST files to `data_dir`\"\"\"\r\n        MNIST(self.data_dir, train=True, download=True)\r\n        MNIST(self.data_dir, train=False, download=True)", "language": "python", "code": "def prepare_data(self) -> None:\r\n        \"\"\"Saves MNIST files to `data_dir`\"\"\"\r\n        MNIST(self.data_dir, train=True, download=True)\r\n        MNIST(self.data_dir, train=False, download=True)", "code_tokens": ["def", "prepare_data", "(", "self", ")", "-", ">", "None", ":", "STRING", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "True", ",", "download", "=", "True", ")", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "False", ",", "download", "=", "True", ")"], "docstring": "Saves MNIST files to `data_dir`", "docstring_tokens": ["saves", "mnist", "files", "to", "data_dir"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 193, "end_line": 196, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_624", "original_string": "def setup(self, stage: str) -> None:\r\n        \"\"\"Split the train and valid dataset.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset: Dataset = MNIST(self.data_dir, train=True, download=False, **extra)\r\n        assert isinstance(dataset, Sized)\r\n        train_length = len(dataset)\r\n        self.dataset_train, self.dataset_val = random_split(\r\n            dataset, [train_length - self.val_split, self.val_split], generator=torch.Generator().manual_seed(42)\r\n        )", "language": "python", "code": "def setup(self, stage: str) -> None:\r\n        \"\"\"Split the train and valid dataset.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset: Dataset = MNIST(self.data_dir, train=True, download=False, **extra)\r\n        assert isinstance(dataset, Sized)\r\n        train_length = len(dataset)\r\n        self.dataset_train, self.dataset_val = random_split(\r\n            dataset, [train_length - self.val_split, self.val_split], generator=torch.Generator().manual_seed(42)\r\n        )", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "STRING", "extra", "=", "{", "STRING", ":", "self", ".", "default_transforms", "}", "if", "self", ".", "default_transforms", "else", "{", "}", "dataset", ":", "Dataset", "=", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "True", ",", "download", "=", "False", ",", "*", "*", "extra", ")", "assert", "isinstance", "(", "dataset", ",", "Sized", ")", "train_length", "=", "len", "(", "dataset", ")", "self", ".", "dataset_train", ",", "self", ".", "dataset_val", "=", "random_split", "(", "dataset", ",", "[", "train_length", "-", "self", ".", "val_split", ",", "self", ".", "val_split", "]", ",", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "42", ")", ")"], "docstring": "Split the train and valid dataset.", "docstring_tokens": ["split", "the", "train", "and", "valid", "dataset"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 198, "end_line": 206, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_625", "original_string": "def train_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST train set removes a subset to use for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_train,\r\n            batch_size=self.batch_size,\r\n            shuffle=True,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def train_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST train set removes a subset to use for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_train,\r\n            batch_size=self.batch_size,\r\n            shuffle=True,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "STRING", "return", "DataLoader", "(", "self", ".", "dataset_train", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST train set removes a subset to use for validation.", "docstring_tokens": ["mnist", "train", "set", "removes", "a", "subset", "to", "use", "for", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 208, "end_line": 217, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_626", "original_string": "def val_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST val set uses a subset of the training set for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_val,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def val_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST val set uses a subset of the training set for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_val,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "val_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "STRING", "return", "DataLoader", "(", "self", ".", "dataset_val", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST val set uses a subset of the training set for validation.", "docstring_tokens": ["mnist", "val", "set", "uses", "a", "subset", "of", "the", "training", "set", "for", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 219, "end_line": 228, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "function_627", "original_string": "def test_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST test set uses the test split.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset = MNIST(self.data_dir, train=False, download=False, **extra)\r\n        return DataLoader(\r\n            dataset,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def test_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST test set uses the test split.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset = MNIST(self.data_dir, train=False, download=False, **extra)\r\n        return DataLoader(\r\n            dataset,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "test_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "STRING", "extra", "=", "{", "STRING", ":", "self", ".", "default_transforms", "}", "if", "self", ".", "default_transforms", "else", "{", "}", "dataset", "=", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "False", ",", "download", "=", "False", ",", "*", "*", "extra", ")", "return", "DataLoader", "(", "dataset", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST test set uses the test split.", "docstring_tokens": ["mnist", "test", "set", "uses", "the", "test", "split"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "start_line": 230, "end_line": 241, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\transformer.py", "func_name": "function_628", "original_string": "def generate_square_subsequent_mask(self, size: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence to prevent future tokens from being seen.\"\"\"\r\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\r\n        mask = mask.float().masked_fill(mask == 1, float(\"-inf\")).masked_fill(mask == 0, 0.0)\r\n        return mask", "language": "python", "code": "def generate_square_subsequent_mask(self, size: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence to prevent future tokens from being seen.\"\"\"\r\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\r\n        mask = mask.float().masked_fill(mask == 1, float(\"-inf\")).masked_fill(mask == 0, 0.0)\r\n        return mask", "code_tokens": ["def", "generate_square_subsequent_mask", "(", "self", ",", "size", ":", "int", ")", "-", ">", "Tensor", ":", "STRING", "mask", "=", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "size", ",", "size", ")", ",", "diagonal", "=", "1", ")", "mask", "=", "mask", ".", "float", "(", ")", ".", "masked_fill", "(", "mask", "=", "=", "1", ",", "float", "(", "STRING", ")", ")", ".", "masked_fill", "(", "mask", "=", "=", "0", ",", "0", ".", "0", ")", "return", "mask"], "docstring": "Generate a square mask for the sequence to prevent future tokens from being seen.", "docstring_tokens": ["generate", "a", "square", "mask", "for", "the", "sequence", "to", "prevent", "future", "tokens", "from", "being", "seen"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\demos\\transformer.py", "start_line": 58, "end_line": 62, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "function_629", "original_string": "def experiment(self) -> comet_experiment:\r\n        r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_comet_function()\r\n\r\n        \"\"\"\r\n\r\n        if not self._experiment:\r\n            self._create_experiment()\r\n\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> comet_experiment:\r\n        r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_comet_function()\r\n\r\n        \"\"\"\r\n\r\n        if not self._experiment:\r\n            self._create_experiment()\r\n\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "comet_experiment", ":", "rSTRING", "if", "not", "self", ".", "_experiment", ":", "self", ".", "_create_experiment", "(", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the", "docstring_tokens": ["r", "actual", "comet", "object", "to", "use", "comet", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\comet.py", "start_line": 305, "end_line": 320, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "function_630", "original_string": "def finalize(self, status: str) -> None:\r\n        \"\"\"We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using\r\n        it after training is complete but instead of ending we will upload/save all the data.\"\"\"\r\n        if self._experiment is None:\r\n            return\r\n\r\n        self.experiment.flush()", "language": "python", "code": "def finalize(self, status: str) -> None:\r\n        \"\"\"We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using\r\n        it after training is complete but instead of ending we will upload/save all the data.\"\"\"\r\n        if self._experiment is None:\r\n            return\r\n\r\n        self.experiment.flush()", "code_tokens": ["def", "finalize", "(", "self", ",", "status", ":", "str", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_experiment", "is", "None", ":", "return", "self", ".", "experiment", ".", "flush", "(", ")"], "docstring": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using", "docstring_tokens": ["we", "will", "not", "end", "experiment", "will", "not", "call", "self", "_experiment", "end", "here", "to", "have", "an", "ability", "to", "continue", "using"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\comet.py", "start_line": 354, "end_line": 363, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "function_631", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._comet_config.offline_directory", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._comet_config.offline_directory", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_comet_config", ".", "offline_directory"], "docstring": "Gets the save directory.", "docstring_tokens": ["gets", "the", "save", "directory"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\comet.py", "start_line": 367, "end_line": 374, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "function_632", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Gets the project name.\r\n\r\n        Returns:\r\n            The project name if it is specified.\r\n\r\n        \"\"\"\r\n        return self._project_name", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Gets the project name.\r\n\r\n        Returns:\r\n            The project name if it is specified.\r\n\r\n        \"\"\"\r\n        return self._project_name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_project_name"], "docstring": "Gets the project name.", "docstring_tokens": ["gets", "the", "project", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\comet.py", "start_line": 378, "end_line": 385, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "function_633", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the version.\r\n\r\n        Returns:\r\n            The experiment key if present\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment.get_key()", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the version.\r\n\r\n        Returns:\r\n            The experiment key if present\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment.get_key()", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", ".", "get_key", "(", ")"], "docstring": "Gets the version.", "docstring_tokens": ["gets", "the", "version"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\comet.py", "start_line": 389, "end_line": 398, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "function_634", "original_string": "def log_hparams(self, params: dict[str, Any]) -> None:\r\n        \"\"\"Record hparams and save into files.\"\"\"\r\n        self.hparams.update(params)\r\n        hparams_file = os.path.join(self.log_dir, self.NAME_HPARAMS_FILE)\r\n        save_hparams_to_yaml(hparams_file, self.hparams)", "language": "python", "code": "def log_hparams(self, params: dict[str, Any]) -> None:\r\n        \"\"\"Record hparams and save into files.\"\"\"\r\n        self.hparams.update(params)\r\n        hparams_file = os.path.join(self.log_dir, self.NAME_HPARAMS_FILE)\r\n        save_hparams_to_yaml(hparams_file, self.hparams)", "code_tokens": ["def", "log_hparams", "(", "self", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "hparams", ".", "update", "(", "params", ")", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "log_dir", ",", "self", ".", "NAME_HPARAMS_FILE", ")", "save_hparams_to_yaml", "(", "hparams_file", ",", "self", ".", "hparams", ")"], "docstring": "Record hparams and save into files.", "docstring_tokens": ["record", "hparams", "and", "save", "into", "files"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "start_line": 56, "end_line": 60, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "function_635", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(self.save_dir, self.name)", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(self.save_dir, self.name)", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "self", ".", "name", ")"], "docstring": "Parent directory for all checkpoint subdirectories.", "docstring_tokens": ["parent", "directory", "for", "all", "checkpoint", "subdirectories"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "start_line": 106, "end_line": 113, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "function_636", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self.root_dir, version)", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self.root_dir, version)", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "fSTRING", "return", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "version", ")"], "docstring": "The log directory for this run.", "docstring_tokens": ["the", "log", "directory", "for", "this", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "start_line": 117, "end_line": 126, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "function_637", "original_string": "def save_dir(self) -> str:\r\n        \"\"\"The current directory where logs are saved.\r\n\r\n        Returns:\r\n            The path to current directory where logs are saved.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "language": "python", "code": "def save_dir(self) -> str:\r\n        \"\"\"The current directory where logs are saved.\r\n\r\n        Returns:\r\n            The path to current directory where logs are saved.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_save_dir"], "docstring": "The current directory where logs are saved.", "docstring_tokens": ["the", "current", "directory", "where", "logs", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "start_line": 130, "end_line": 137, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "function_638", "original_string": "def experiment(self) -> _FabricExperimentWriter:\r\n        r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your\r\n        :class:`~lightning.pytorch.core.LightningModule` do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        self._fs.makedirs(self.root_dir, exist_ok=True)\r\n        self._experiment = ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> _FabricExperimentWriter:\r\n        r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your\r\n        :class:`~lightning.pytorch.core.LightningModule` do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        self._fs.makedirs(self.root_dir, exist_ok=True)\r\n        self._experiment = ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "_FabricExperimentWriter", ":", "rSTRING", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "self", ".", "_fs", ".", "makedirs", "(", "self", ".", "root_dir", ",", "exist_ok", "=", "True", ")", "self", ".", "_experiment", "=", "ExperimentWriter", "(", "log_dir", "=", "self", ".", "log_dir", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your", "docstring_tokens": ["r", "actual", "_experimentwriter", "object", "to", "use", "_experimentwriter", "features", "in", "your"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "start_line": 148, "end_line": 162, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_639", "original_string": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "ModelCheckpoint", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called after model checkpoint callback saves a new checkpoint.", "docstring_tokens": ["called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 34, "end_line": 41, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_640", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where experiment logs get saved, or `None` if the logger does not save data\r\n        locally.\"\"\"\r\n        return None", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where experiment logs get saved, or `None` if the logger does not save data\r\n        locally.\"\"\"\r\n        return None", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "None"], "docstring": "Return the root directory where experiment logs get saved, or `None` if the logger does not save data", "docstring_tokens": ["return", "the", "root", "directory", "where", "experiment", "logs", "get", "saved", "or", "none", "if", "the", "logger", "does", "not", "save", "data"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 44, "end_line": 47, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_641", "original_string": "def experiment(self) -> DummyExperiment:\r\n        \"\"\"Return the experiment object associated with this logger.\"\"\"\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> DummyExperiment:\r\n        \"\"\"Return the experiment object associated with this logger.\"\"\"\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "DummyExperiment", ":", "STRING", "return", "self", ".", "_experiment"], "docstring": "Return the experiment object associated with this logger.", "docstring_tokens": ["return", "the", "experiment", "object", "associated", "with", "this", "logger"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 62, "end_line": 64, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_642", "original_string": "def name(self) -> str:\r\n        \"\"\"Return the experiment name.\"\"\"\r\n        return \"\"", "language": "python", "code": "def name(self) -> str:\r\n        \"\"\"Return the experiment name.\"\"\"\r\n        return \"\"", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "STRING"], "docstring": "Return the experiment name.", "docstring_tokens": ["return", "the", "experiment", "name"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 76, "end_line": 78, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_643", "original_string": "def version(self) -> str:\r\n        \"\"\"Return the experiment version.\"\"\"\r\n        return \"\"", "language": "python", "code": "def version(self) -> str:\r\n        \"\"\"Return the experiment version.\"\"\"\r\n        return \"\"", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "STRING"], "docstring": "Return the experiment version.", "docstring_tokens": ["return", "the", "experiment", "version"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 82, "end_line": 84, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_644", "original_string": "def __getattr__(self, name: str) -> Callable:\r\n        \"\"\"Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.\"\"\"\r\n\r\n        def method(*args: Any, **kwargs: Any) -> None:\r\n            return None\r\n\r\n        return method", "language": "python", "code": "def __getattr__(self, name: str) -> Callable:\r\n        \"\"\"Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.\"\"\"\r\n\r\n        def method(*args: Any, **kwargs: Any) -> None:\r\n            return None\r\n\r\n        return method", "code_tokens": ["def", "__getattr__", "(", "self", ",", "name", ":", "str", ")", "-", ">", "Callable", ":", "STRING", "def", "method", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "return", "None", "return", "method"], "docstring": "Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.", "docstring_tokens": ["allows", "the", "dummylogger", "to", "be", "called", "with", "arbitrary", "methods", "to", "avoid", "attributeerrors"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 90, "end_line": 96, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "function_645", "original_string": "def merge_dicts(  # pragma: no cover\r\n    dicts: Sequence[Mapping],\r\n    agg_key_funcs: Optional[Mapping] = None,\r\n    default_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n) -> dict:\r\n    \"\"\"Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.\r\n\r\n    Args:\r\n        dicts:\r\n            Sequence of dictionaries to be merged.\r\n        agg_key_funcs:\r\n            Mapping from key name to function. This function will aggregate a\r\n            list of values, obtained from the same key of all dictionaries.\r\n            If some key has no specified aggregation function, the default one\r\n            will be used. Default is: ``None`` (all keys will be aggregated by the\r\n            default function).\r\n        default_func:\r\n            Default function to aggregate keys, which are not presented in the\r\n            `agg_key_funcs` map.\r\n\r\n    Returns:\r\n        Dictionary with merged values.\r\n\r\n    Examples:\r\n        >>> import pprint\r\n        >>> d1 = {'a': 1.7, 'b': 2.0, 'c': 1, 'd': {'d1': 1, 'd3': 3}}\r\n        >>> d2 = {'a': 1.1, 'b': 2.2, 'v': 1, 'd': {'d1': 2, 'd2': 3}}\r\n        >>> d3 = {'a': 1.1, 'v': 2.3, 'd': {'d3': 3, 'd4': {'d5': 1}}}\r\n        >>> dflt_func = min\r\n        >>> agg_funcs = {'a': statistics.mean, 'v': max, 'd': {'d1': sum}}\r\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\r\n        {'a': 1.3,\r\n         'b': 2.0,\r\n         'c': 1,\r\n         'd': {'d1': 3, 'd2': 3, 'd3': 3, 'd4': {'d5': 1}},\r\n         'v': 2.3}\r\n\r\n    \"\"\"\r\n    agg_key_funcs = agg_key_funcs or {}\r\n    keys = list(functools.reduce(operator.or_, [set(d.keys()) for d in dicts]))\r\n    d_out: dict = defaultdict(dict)\r\n    for k in keys:\r\n        fn = agg_key_funcs.get(k)\r\n        values_to_agg = [v for v in [d_in.get(k) for d_in in dicts] if v is not None]\r\n\r\n        if isinstance(values_to_agg[0], dict):\r\n            d_out[k] = merge_dicts(values_to_agg, fn, default_func)\r\n        else:\r\n            d_out[k] = (fn or default_func)(values_to_agg)\r\n\r\n    return dict(d_out)", "language": "python", "code": "def merge_dicts(  # pragma: no cover\r\n    dicts: Sequence[Mapping],\r\n    agg_key_funcs: Optional[Mapping] = None,\r\n    default_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n) -> dict:\r\n    \"\"\"Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.\r\n\r\n    Args:\r\n        dicts:\r\n            Sequence of dictionaries to be merged.\r\n        agg_key_funcs:\r\n            Mapping from key name to function. This function will aggregate a\r\n            list of values, obtained from the same key of all dictionaries.\r\n            If some key has no specified aggregation function, the default one\r\n            will be used. Default is: ``None`` (all keys will be aggregated by the\r\n            default function).\r\n        default_func:\r\n            Default function to aggregate keys, which are not presented in the\r\n            `agg_key_funcs` map.\r\n\r\n    Returns:\r\n        Dictionary with merged values.\r\n\r\n    Examples:\r\n        >>> import pprint\r\n        >>> d1 = {'a': 1.7, 'b': 2.0, 'c': 1, 'd': {'d1': 1, 'd3': 3}}\r\n        >>> d2 = {'a': 1.1, 'b': 2.2, 'v': 1, 'd': {'d1': 2, 'd2': 3}}\r\n        >>> d3 = {'a': 1.1, 'v': 2.3, 'd': {'d3': 3, 'd4': {'d5': 1}}}\r\n        >>> dflt_func = min\r\n        >>> agg_funcs = {'a': statistics.mean, 'v': max, 'd': {'d1': sum}}\r\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\r\n        {'a': 1.3,\r\n         'b': 2.0,\r\n         'c': 1,\r\n         'd': {'d1': 3, 'd2': 3, 'd3': 3, 'd4': {'d5': 1}},\r\n         'v': 2.3}\r\n\r\n    \"\"\"\r\n    agg_key_funcs = agg_key_funcs or {}\r\n    keys = list(functools.reduce(operator.or_, [set(d.keys()) for d in dicts]))\r\n    d_out: dict = defaultdict(dict)\r\n    for k in keys:\r\n        fn = agg_key_funcs.get(k)\r\n        values_to_agg = [v for v in [d_in.get(k) for d_in in dicts] if v is not None]\r\n\r\n        if isinstance(values_to_agg[0], dict):\r\n            d_out[k] = merge_dicts(values_to_agg, fn, default_func)\r\n        else:\r\n            d_out[k] = (fn or default_func)(values_to_agg)\r\n\r\n    return dict(d_out)", "code_tokens": ["def", "merge_dicts", "(", "#", "pragma", ":", "no", "cover", "dicts", ":", "Sequence", "[", "Mapping", "]", ",", "agg_key_funcs", ":", "Optional", "[", "Mapping", "]", "=", "None", ",", "default_func", ":", "Callable", "[", "[", "Sequence", "[", "float", "]", "]", ",", "float", "]", "=", "statistics", ".", "mean", ",", ")", "-", ">", "dict", ":", "STRING", "agg_key_funcs", "=", "agg_key_funcs", "or", "{", "}", "keys", "=", "list", "(", "functools", ".", "reduce", "(", "operator", ".", "or_", ",", "[", "set", "(", "d", ".", "keys", "(", ")", ")", "for", "d", "in", "dicts", "]", ")", ")", "d_out", ":", "dict", "=", "defaultdict", "(", "dict", ")", "for", "k", "in", "keys", ":", "fn", "=", "agg_key_funcs", ".", "get", "(", "k", ")", "values_to_agg", "=", "[", "v", "for", "v", "in", "[", "d_in", ".", "get", "(", "k", ")", "for", "d_in", "in", "dicts", "]", "if", "v", "is", "not", "None", "]", "if", "isinstance", "(", "values_to_agg", "[", "0", "]", ",", "dict", ")", ":", "d_out", "[", "k", "]", "=", "merge_dicts", "(", "values_to_agg", ",", "fn", ",", "default_func", ")", "else", ":", "d_out", "[", "k", "]", "=", "(", "fn", "or", "default_func", ")", "(", "values_to_agg", ")", "return", "dict", "(", "d_out", ")"], "docstring": "Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.", "docstring_tokens": ["merge", "a", "sequence", "with", "dictionaries", "into", "one", "dictionary", "by", "aggregating", "the", "same", "keys", "with", "some", "given", "function"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\logger.py", "start_line": 100, "end_line": 150, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_646", "original_string": "def experiment(self) -> \"MlflowClient\":\r\n        r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_mlflow_function()\r\n\r\n        \"\"\"\r\n        import mlflow\r\n\r\n        if self._initialized:\r\n            return self._mlflow_client\r\n\r\n        mlflow.set_tracking_uri(self._tracking_uri)\r\n\r\n        if self._run_id is not None:\r\n            run = self._mlflow_client.get_run(self._run_id)\r\n            self._experiment_id = run.info.experiment_id\r\n            self._initialized = True\r\n            return self._mlflow_client\r\n\r\n        if self._experiment_id is None:\r\n            expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n            if expt is not None and expt.lifecycle_stage != \"deleted\":\r\n                self._experiment_id = expt.experiment_id\r\n            else:\r\n                log.warning(f\"Experiment with name {self._experiment_name} not found. Creating it.\")\r\n                self._experiment_id = self._mlflow_client.create_experiment(\r\n                    name=self._experiment_name, artifact_location=self._artifact_location\r\n                )\r\n\r\n        if self._run_id is None:\r\n            if self._run_name is not None:\r\n                self.tags = self.tags or {}\r\n\r\n                from mlflow.utils.mlflow_tags import MLFLOW_RUN_NAME\r\n\r\n                if MLFLOW_RUN_NAME in self.tags:\r\n                    log.warning(\r\n                        f\"The tag {MLFLOW_RUN_NAME} is found in tags. The value will be overridden by {self._run_name}.\"\r\n                    )\r\n                self.tags[MLFLOW_RUN_NAME] = self._run_name\r\n\r\n            resolve_tags = _get_resolve_tags()\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n            self._run_id = run.info.run_id\r\n        self._initialized = True\r\n        return self._mlflow_client", "language": "python", "code": "def experiment(self) -> \"MlflowClient\":\r\n        r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_mlflow_function()\r\n\r\n        \"\"\"\r\n        import mlflow\r\n\r\n        if self._initialized:\r\n            return self._mlflow_client\r\n\r\n        mlflow.set_tracking_uri(self._tracking_uri)\r\n\r\n        if self._run_id is not None:\r\n            run = self._mlflow_client.get_run(self._run_id)\r\n            self._experiment_id = run.info.experiment_id\r\n            self._initialized = True\r\n            return self._mlflow_client\r\n\r\n        if self._experiment_id is None:\r\n            expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n            if expt is not None and expt.lifecycle_stage != \"deleted\":\r\n                self._experiment_id = expt.experiment_id\r\n            else:\r\n                log.warning(f\"Experiment with name {self._experiment_name} not found. Creating it.\")\r\n                self._experiment_id = self._mlflow_client.create_experiment(\r\n                    name=self._experiment_name, artifact_location=self._artifact_location\r\n                )\r\n\r\n        if self._run_id is None:\r\n            if self._run_name is not None:\r\n                self.tags = self.tags or {}\r\n\r\n                from mlflow.utils.mlflow_tags import MLFLOW_RUN_NAME\r\n\r\n                if MLFLOW_RUN_NAME in self.tags:\r\n                    log.warning(\r\n                        f\"The tag {MLFLOW_RUN_NAME} is found in tags. The value will be overridden by {self._run_name}.\"\r\n                    )\r\n                self.tags[MLFLOW_RUN_NAME] = self._run_name\r\n\r\n            resolve_tags = _get_resolve_tags()\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n            self._run_id = run.info.run_id\r\n        self._initialized = True\r\n        return self._mlflow_client", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "STRING", ":", "rSTRING", "import", "mlflow", "if", "self", ".", "_initialized", ":", "return", "self", ".", "_mlflow_client", "mlflow", ".", "set_tracking_uri", "(", "self", ".", "_tracking_uri", ")", "if", "self", ".", "_run_id", "is", "not", "None", ":", "run", "=", "self", ".", "_mlflow_client", ".", "get_run", "(", "self", ".", "_run_id", ")", "self", ".", "_experiment_id", "=", "run", ".", "info", ".", "experiment_id", "self", ".", "_initialized", "=", "True", "return", "self", ".", "_mlflow_client", "if", "self", ".", "_experiment_id", "is", "None", ":", "expt", "=", "self", ".", "_mlflow_client", ".", "get_experiment_by_name", "(", "self", ".", "_experiment_name", ")", "if", "expt", "is", "not", "None", "and", "expt", ".", "lifecycle_stage", "!", "=", "STRING", ":", "self", ".", "_experiment_id", "=", "expt", ".", "experiment_id", "else", ":", "log", ".", "warning", "(", "fSTRING", ")", "self", ".", "_experiment_id", "=", "self", ".", "_mlflow_client", ".", "create_experiment", "(", "name", "=", "self", ".", "_experiment_name", ",", "artifact_location", "=", "self", ".", "_artifact_location", ")", "if", "self", ".", "_run_id", "is", "None", ":", "if", "self", ".", "_run_name", "is", "not", "None", ":", "self", ".", "tags", "=", "self", ".", "tags", "or", "{", "}", "from", "mlflow", ".", "utils", ".", "mlflow_tags", "import", "MLFLOW_RUN_NAME", "if", "MLFLOW_RUN_NAME", "in", "self", ".", "tags", ":", "log", ".", "warning", "(", "fSTRING", ")", "self", ".", "tags", "[", "MLFLOW_RUN_NAME", "]", "=", "self", ".", "_run_name", "resolve_tags", "=", "_get_resolve_tags", "(", ")", "run", "=", "self", ".", "_mlflow_client", ".", "create_run", "(", "experiment_id", "=", "self", ".", "_experiment_id", ",", "tags", "=", "resolve_tags", "(", "self", ".", "tags", ")", ")", "self", ".", "_run_id", "=", "run", ".", "info", ".", "run_id", "self", ".", "_initialized", "=", "True", "return", "self", ".", "_mlflow_client"], "docstring": "r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the", "docstring_tokens": ["r", "actual", "mlflow", "object", "to", "use", "mlflow", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 156, "end_line": 204, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_647", "original_string": "def run_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._run_id", "language": "python", "code": "def run_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._run_id", "code_tokens": ["def", "run_id", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "_", "=", "self", ".", "experiment", "return", "self", ".", "_run_id"], "docstring": "Create the experiment if it does not exist to get the run id.", "docstring_tokens": ["create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "run", "id"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 207, "end_line": 215, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_648", "original_string": "def experiment_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._experiment_id", "language": "python", "code": "def experiment_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._experiment_id", "code_tokens": ["def", "experiment_id", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "_", "=", "self", ".", "experiment", "return", "self", ".", "_experiment_id"], "docstring": "Create the experiment if it does not exist to get the experiment id.", "docstring_tokens": ["create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "experiment", "id"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 218, "end_line": 226, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_649", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"The root file directory in which MLflow experiments are saved.\r\n\r\n        Return:\r\n            Local path to the root experiment directory if the tracking uri is local.\r\n            Otherwise returns `None`.\r\n\r\n        \"\"\"\r\n        if self._tracking_uri.startswith(LOCAL_FILE_URI_PREFIX):\r\n            return self._tracking_uri[len(LOCAL_FILE_URI_PREFIX) :]\r\n        return None", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"The root file directory in which MLflow experiments are saved.\r\n\r\n        Return:\r\n            Local path to the root experiment directory if the tracking uri is local.\r\n            Otherwise returns `None`.\r\n\r\n        \"\"\"\r\n        if self._tracking_uri.startswith(LOCAL_FILE_URI_PREFIX):\r\n            return self._tracking_uri[len(LOCAL_FILE_URI_PREFIX) :]\r\n        return None", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "if", "self", ".", "_tracking_uri", ".", "startswith", "(", "LOCAL_FILE_URI_PREFIX", ")", ":", "return", "self", ".", "_tracking_uri", "[", "len", "(", "LOCAL_FILE_URI_PREFIX", ")", ":", "]", "return", "None"], "docstring": "The root file directory in which MLflow experiments are saved.", "docstring_tokens": ["the", "root", "file", "directory", "in", "which", "mlflow", "experiments", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 293, "end_line": 303, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_650", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        return self.experiment_id", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        return self.experiment_id", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "experiment_id"], "docstring": "Get the experiment id.", "docstring_tokens": ["get", "the", "experiment", "id"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 307, "end_line": 314, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "function_651", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        return self.run_id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        return self.run_id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "run_id"], "docstring": "Get the run id.", "docstring_tokens": ["get", "the", "run", "id"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "start_line": 318, "end_line": 325, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_652", "original_string": "def _construct_path_with_prefix(self, *keys: str) -> str:\r\n        \"\"\"Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.\"\"\"\r\n        if self._prefix:\r\n            return self.LOGGER_JOIN_CHAR.join([self._prefix, *keys])\r\n        return self.LOGGER_JOIN_CHAR.join(keys)", "language": "python", "code": "def _construct_path_with_prefix(self, *keys: str) -> str:\r\n        \"\"\"Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.\"\"\"\r\n        if self._prefix:\r\n            return self.LOGGER_JOIN_CHAR.join([self._prefix, *keys])\r\n        return self.LOGGER_JOIN_CHAR.join(keys)", "code_tokens": ["def", "_construct_path_with_prefix", "(", "self", ",", "*", "keys", ":", "str", ")", "-", ">", "str", ":", "STRING", "if", "self", ".", "_prefix", ":", "return", "self", ".", "LOGGER_JOIN_CHAR", ".", "join", "(", "[", "self", ".", "_prefix", ",", "*", "keys", "]", ")", "return", "self", ".", "LOGGER_JOIN_CHAR", ".", "join", "(", "keys", ")"], "docstring": "Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.", "docstring_tokens": ["return", "sequence", "of", "keys", "joined", "by", "logger_join_char", "started", "with", "_prefix", "if", "defined"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 311, "end_line": 315, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_653", "original_string": "def experiment(self) -> \"Run\":\r\n        r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your\r\n        :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Example::\r\n\r\n            class LitModel(LightningModule):\r\n                def training_step(self, batch, batch_idx):\r\n                    acc = ...\r\n                    self.logger.experiment[\"train/acc\"].append(acc)\r\n\r\n                    img = ...\r\n                    self.logger.experiment[\"train/misclassified_images\"].append(File.as_image(img))\r\n\r\n        Note that the syntax ``self.logger.experiment[\"your/metadata/structure\"].append(metadata)``\r\n        is specific to Neptune and extends the logger capabilities.\r\n        It lets you log various types of metadata, such as scores, files,\r\n        images, interactive visuals, and CSVs. Refer to the\r\n        `Neptune docs <https://docs.neptune.ai/logging/methods>`_\r\n        for more detailed explanations.\r\n        You can also use the regular logger methods ``log_metrics()``, and ``log_hyperparams()``\r\n        with NeptuneLogger.\r\n\r\n        \"\"\"\r\n        return self.run", "language": "python", "code": "def experiment(self) -> \"Run\":\r\n        r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your\r\n        :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Example::\r\n\r\n            class LitModel(LightningModule):\r\n                def training_step(self, batch, batch_idx):\r\n                    acc = ...\r\n                    self.logger.experiment[\"train/acc\"].append(acc)\r\n\r\n                    img = ...\r\n                    self.logger.experiment[\"train/misclassified_images\"].append(File.as_image(img))\r\n\r\n        Note that the syntax ``self.logger.experiment[\"your/metadata/structure\"].append(metadata)``\r\n        is specific to Neptune and extends the logger capabilities.\r\n        It lets you log various types of metadata, such as scores, files,\r\n        images, interactive visuals, and CSVs. Refer to the\r\n        `Neptune docs <https://docs.neptune.ai/logging/methods>`_\r\n        for more detailed explanations.\r\n        You can also use the regular logger methods ``log_metrics()``, and ``log_hyperparams()``\r\n        with NeptuneLogger.\r\n\r\n        \"\"\"\r\n        return self.run", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "STRING", ":", "rSTRING", "return", "self", ".", "run"], "docstring": "r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your", "docstring_tokens": ["r", "actual", "neptune", "run", "object", "allows", "you", "to", "use", "neptune", "logging", "features", "in", "your"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 354, "end_line": 380, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_654", "original_string": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace]) -> None:\r\n        r\"\"\"Log hyperparameters to the run.\r\n\r\n        Hyperparameters will be logged under the \"<prefix>/hyperparams\" namespace.\r\n\r\n        Note:\r\n\r\n            You can also log parameters by directly using the logger instance:\r\n            ``neptune_logger.experiment[\"model/hyper-parameters\"] = params_dict``.\r\n\r\n            In this way you can keep hierarchical structure of the parameters.\r\n\r\n        Args:\r\n            params: `dict`.\r\n                Python dictionary structure with parameters.\r\n\r\n        Example::\r\n\r\n            from lightning.pytorch.loggers import NeptuneLogger\r\n            import neptune\r\n\r\n            PARAMS = {\r\n                \"batch_size\": 64,\r\n                \"lr\": 0.07,\r\n                \"decay_factor\": 0.97,\r\n            }\r\n\r\n            neptune_logger = NeptuneLogger(\r\n                api_key=neptune.ANONYMOUS_API_TOKEN,\r\n                project=\"common/pytorch-lightning-integration\"\r\n            )\r\n\r\n            neptune_logger.log_hyperparams(PARAMS)\r\n\r\n        \"\"\"\r\n        from neptune.utils import stringify_unsupported\r\n\r\n        params = _convert_params(params)\r\n        params = _sanitize_callable_params(params)\r\n\r\n        parameters_key = self.PARAMETERS_KEY\r\n        parameters_key = self._construct_path_with_prefix(parameters_key)\r\n\r\n        self.run[parameters_key] = stringify_unsupported(params)", "language": "python", "code": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace]) -> None:\r\n        r\"\"\"Log hyperparameters to the run.\r\n\r\n        Hyperparameters will be logged under the \"<prefix>/hyperparams\" namespace.\r\n\r\n        Note:\r\n\r\n            You can also log parameters by directly using the logger instance:\r\n            ``neptune_logger.experiment[\"model/hyper-parameters\"] = params_dict``.\r\n\r\n            In this way you can keep hierarchical structure of the parameters.\r\n\r\n        Args:\r\n            params: `dict`.\r\n                Python dictionary structure with parameters.\r\n\r\n        Example::\r\n\r\n            from lightning.pytorch.loggers import NeptuneLogger\r\n            import neptune\r\n\r\n            PARAMS = {\r\n                \"batch_size\": 64,\r\n                \"lr\": 0.07,\r\n                \"decay_factor\": 0.97,\r\n            }\r\n\r\n            neptune_logger = NeptuneLogger(\r\n                api_key=neptune.ANONYMOUS_API_TOKEN,\r\n                project=\"common/pytorch-lightning-integration\"\r\n            )\r\n\r\n            neptune_logger.log_hyperparams(PARAMS)\r\n\r\n        \"\"\"\r\n        from neptune.utils import stringify_unsupported\r\n\r\n        params = _convert_params(params)\r\n        params = _sanitize_callable_params(params)\r\n\r\n        parameters_key = self.PARAMETERS_KEY\r\n        parameters_key = self._construct_path_with_prefix(parameters_key)\r\n\r\n        self.run[parameters_key] = stringify_unsupported(params)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ")", "-", ">", "None", ":", "rSTRING", "from", "neptune", ".", "utils", "import", "stringify_unsupported", "params", "=", "_convert_params", "(", "params", ")", "params", "=", "_sanitize_callable_params", "(", "params", ")", "parameters_key", "=", "self", ".", "PARAMETERS_KEY", "parameters_key", "=", "self", ".", "_construct_path_with_prefix", "(", "parameters_key", ")", "self", ".", "run", "[", "parameters_key", "]", "=", "stringify_unsupported", "(", "params", ")"], "docstring": "r\"\"\"Log hyperparameters to the run.", "docstring_tokens": ["r", "log", "hyperparameters", "to", "the", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 398, "end_line": 441, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_655", "original_string": "def log_metrics(self, metrics: dict[str, Union[Tensor, float]], step: Optional[int] = None) -> None:\r\n        \"\"\"Log metrics (numeric values) in Neptune runs.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values.\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        if rank_zero_only.rank != 0:\r\n            raise ValueError(\"run tried to log from global_rank != 0\")\r\n\r\n        metrics = _add_prefix(metrics, self._prefix, self.LOGGER_JOIN_CHAR)\r\n\r\n        for key, val in metrics.items():\r\n            self.run[key].append(val, step=step)", "language": "python", "code": "def log_metrics(self, metrics: dict[str, Union[Tensor, float]], step: Optional[int] = None) -> None:\r\n        \"\"\"Log metrics (numeric values) in Neptune runs.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values.\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        if rank_zero_only.rank != 0:\r\n            raise ValueError(\"run tried to log from global_rank != 0\")\r\n\r\n        metrics = _add_prefix(metrics, self._prefix, self.LOGGER_JOIN_CHAR)\r\n\r\n        for key, val in metrics.items():\r\n            self.run[key].append(val, step=step)", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "rank_zero_only", ".", "rank", "!", "=", "0", ":", "raise", "ValueError", "(", "STRING", ")", "metrics", "=", "_add_prefix", "(", "metrics", ",", "self", ".", "_prefix", ",", "self", ".", "LOGGER_JOIN_CHAR", ")", "for", "key", ",", "val", "in", "metrics", ".", "items", "(", ")", ":", "self", ".", "run", "[", "key", "]", ".", "append", "(", "val", ",", "step", "=", "step", ")"], "docstring": "Log metrics (numeric values) in Neptune runs.", "docstring_tokens": ["log", "metrics", "numeric", "values", "in", "neptune", "runs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 446, "end_line": 460, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_656", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save\r\n        locally.\r\n\r\n        Returns:\r\n            the root directory where experiment logs get saved\r\n\r\n        \"\"\"\r\n        return os.path.join(os.getcwd(), \".neptune\")", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save\r\n        locally.\r\n\r\n        Returns:\r\n            the root directory where experiment logs get saved\r\n\r\n        \"\"\"\r\n        return os.path.join(os.getcwd(), \".neptune\")", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "STRING", ")"], "docstring": "Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save", "docstring_tokens": ["gets", "the", "save", "directory", "of", "the", "experiment", "which", "in", "this", "case", "is", "none", "because", "neptune", "does", "not", "save"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 477, "end_line": 485, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_657", "original_string": "def after_save_checkpoint(self, checkpoint_callback: Checkpoint) -> None:\r\n        \"\"\"Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        if not self._log_model_checkpoints:\r\n            return\r\n\r\n        file_names = set()\r\n        checkpoints_namespace = self._construct_path_with_prefix(\"model/checkpoints\")\r\n\r\n        if hasattr(checkpoint_callback, \"last_model_path\") and checkpoint_callback.last_model_path:\r\n            model_last_name = self._get_full_model_name(checkpoint_callback.last_model_path, checkpoint_callback)\r\n            file_names.add(model_last_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_last_name}\"].upload(checkpoint_callback.last_model_path)\r\n\r\n        if hasattr(checkpoint_callback, \"best_k_models\"):\r\n            for key in checkpoint_callback.best_k_models:\r\n                model_name = self._get_full_model_name(key, checkpoint_callback)\r\n                file_names.add(model_name)\r\n                self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(key)\r\n\r\n        if hasattr(checkpoint_callback, \"best_model_path\") and checkpoint_callback.best_model_path:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_path\")] = checkpoint_callback.best_model_path\r\n\r\n            model_name = self._get_full_model_name(checkpoint_callback.best_model_path, checkpoint_callback)\r\n            file_names.add(model_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(checkpoint_callback.best_model_path)\r\n\r\n        if self.run.exists(checkpoints_namespace):\r\n            exp_structure = self.run.get_structure()\r\n            uploaded_model_names = self._get_full_model_names_from_exp_structure(exp_structure, checkpoints_namespace)\r\n\r\n            for file_to_drop in list(uploaded_model_names - file_names):\r\n                del self.run[f\"{checkpoints_namespace}/{file_to_drop}\"]\r\n\r\n        if hasattr(checkpoint_callback, \"best_model_score\") and checkpoint_callback.best_model_score:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_score\")] = (\r\n                checkpoint_callback.best_model_score.cpu().detach().numpy()\r\n            )", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: Checkpoint) -> None:\r\n        \"\"\"Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        if not self._log_model_checkpoints:\r\n            return\r\n\r\n        file_names = set()\r\n        checkpoints_namespace = self._construct_path_with_prefix(\"model/checkpoints\")\r\n\r\n        if hasattr(checkpoint_callback, \"last_model_path\") and checkpoint_callback.last_model_path:\r\n            model_last_name = self._get_full_model_name(checkpoint_callback.last_model_path, checkpoint_callback)\r\n            file_names.add(model_last_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_last_name}\"].upload(checkpoint_callback.last_model_path)\r\n\r\n        if hasattr(checkpoint_callback, \"best_k_models\"):\r\n            for key in checkpoint_callback.best_k_models:\r\n                model_name = self._get_full_model_name(key, checkpoint_callback)\r\n                file_names.add(model_name)\r\n                self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(key)\r\n\r\n        if hasattr(checkpoint_callback, \"best_model_path\") and checkpoint_callback.best_model_path:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_path\")] = checkpoint_callback.best_model_path\r\n\r\n            model_name = self._get_full_model_name(checkpoint_callback.best_model_path, checkpoint_callback)\r\n            file_names.add(model_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(checkpoint_callback.best_model_path)\r\n\r\n        if self.run.exists(checkpoints_namespace):\r\n            exp_structure = self.run.get_structure()\r\n            uploaded_model_names = self._get_full_model_names_from_exp_structure(exp_structure, checkpoints_namespace)\r\n\r\n            for file_to_drop in list(uploaded_model_names - file_names):\r\n                del self.run[f\"{checkpoints_namespace}/{file_to_drop}\"]\r\n\r\n        if hasattr(checkpoint_callback, \"best_model_score\") and checkpoint_callback.best_model_score:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_score\")] = (\r\n                checkpoint_callback.best_model_score.cpu().detach().numpy()\r\n            )", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "Checkpoint", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_log_model_checkpoints", ":", "return", "file_names", "=", "set", "(", ")", "checkpoints_namespace", "=", "self", ".", "_construct_path_with_prefix", "(", "STRING", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", "and", "checkpoint_callback", ".", "last_model_path", ":", "model_last_name", "=", "self", ".", "_get_full_model_name", "(", "checkpoint_callback", ".", "last_model_path", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_last_name", ")", "self", ".", "run", "[", "fSTRING", "]", ".", "upload", "(", "checkpoint_callback", ".", "last_model_path", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", ":", "for", "key", "in", "checkpoint_callback", ".", "best_k_models", ":", "model_name", "=", "self", ".", "_get_full_model_name", "(", "key", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_name", ")", "self", ".", "run", "[", "fSTRING", "]", ".", "upload", "(", "key", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", "and", "checkpoint_callback", ".", "best_model_path", ":", "self", ".", "run", "[", "self", ".", "_construct_path_with_prefix", "(", "STRING", ")", "]", "=", "checkpoint_callback", ".", "best_model_path", "model_name", "=", "self", ".", "_get_full_model_name", "(", "checkpoint_callback", ".", "best_model_path", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_name", ")", "self", ".", "run", "[", "fSTRING", "]", ".", "upload", "(", "checkpoint_callback", ".", "best_model_path", ")", "if", "self", ".", "run", ".", "exists", "(", "checkpoints_namespace", ")", ":", "exp_structure", "=", "self", ".", "run", ".", "get_structure", "(", ")", "uploaded_model_names", "=", "self", ".", "_get_full_model_names_from_exp_structure", "(", "exp_structure", ",", "checkpoints_namespace", ")", "for", "file_to_drop", "in", "list", "(", "uploaded_model_names", "-", "file_names", ")", ":", "del", "self", ".", "run", "[", "fSTRING", "]", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", "and", "checkpoint_callback", ".", "best_model_score", ":", "self", ".", "run", "[", "self", ".", "_construct_path_with_prefix", "(", "STRING", ")", "]", "=", "(", "checkpoint_callback", ".", "best_model_score", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")"], "docstring": "Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.", "docstring_tokens": ["automatically", "log", "checkpointed", "model", "called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 500, "end_line": 546, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_658", "original_string": "def _get_full_model_name(model_path: str, checkpoint_callback: Checkpoint) -> str:\r\n        \"\"\"Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.\"\"\"\r\n        if hasattr(checkpoint_callback, \"dirpath\"):\r\n            model_path = os.path.normpath(model_path)\r\n            expected_model_path = os.path.normpath(checkpoint_callback.dirpath)\r\n            if not model_path.startswith(expected_model_path):\r\n                raise ValueError(f\"{model_path} was expected to start with {expected_model_path}.\")\r\n            filepath, _ = os.path.splitext(model_path[len(expected_model_path) + 1 :])\r\n            return filepath.replace(os.sep, \"/\")\r\n        return model_path.replace(os.sep, \"/\")", "language": "python", "code": "def _get_full_model_name(model_path: str, checkpoint_callback: Checkpoint) -> str:\r\n        \"\"\"Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.\"\"\"\r\n        if hasattr(checkpoint_callback, \"dirpath\"):\r\n            model_path = os.path.normpath(model_path)\r\n            expected_model_path = os.path.normpath(checkpoint_callback.dirpath)\r\n            if not model_path.startswith(expected_model_path):\r\n                raise ValueError(f\"{model_path} was expected to start with {expected_model_path}.\")\r\n            filepath, _ = os.path.splitext(model_path[len(expected_model_path) + 1 :])\r\n            return filepath.replace(os.sep, \"/\")\r\n        return model_path.replace(os.sep, \"/\")", "code_tokens": ["def", "_get_full_model_name", "(", "model_path", ":", "str", ",", "checkpoint_callback", ":", "Checkpoint", ")", "-", ">", "str", ":", "STRING", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", ":", "model_path", "=", "os", ".", "path", ".", "normpath", "(", "model_path", ")", "expected_model_path", "=", "os", ".", "path", ".", "normpath", "(", "checkpoint_callback", ".", "dirpath", ")", "if", "not", "model_path", ".", "startswith", "(", "expected_model_path", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "filepath", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "model_path", "[", "len", "(", "expected_model_path", ")", "+", "1", ":", "]", ")", "return", "filepath", ".", "replace", "(", "os", ".", "sep", ",", "STRING", ")", "return", "model_path", ".", "replace", "(", "os", ".", "sep", ",", "STRING", ")"], "docstring": "Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.", "docstring_tokens": ["returns", "model", "name", "which", "is", "string", "model_path", "appended", "to", "checkpoint_callback", "dirpath"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 549, "end_line": 559, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_659", "original_string": "def _get_full_model_names_from_exp_structure(cls, exp_structure: dict[str, Any], namespace: str) -> set[str]:\r\n        \"\"\"Returns all paths to properties which were already logged in `namespace`\"\"\"\r\n        structure_keys: list[str] = namespace.split(cls.LOGGER_JOIN_CHAR)\r\n        for key in structure_keys:\r\n            exp_structure = exp_structure[key]\r\n        uploaded_models_dict = exp_structure\r\n        return set(cls._dict_paths(uploaded_models_dict))", "language": "python", "code": "def _get_full_model_names_from_exp_structure(cls, exp_structure: dict[str, Any], namespace: str) -> set[str]:\r\n        \"\"\"Returns all paths to properties which were already logged in `namespace`\"\"\"\r\n        structure_keys: list[str] = namespace.split(cls.LOGGER_JOIN_CHAR)\r\n        for key in structure_keys:\r\n            exp_structure = exp_structure[key]\r\n        uploaded_models_dict = exp_structure\r\n        return set(cls._dict_paths(uploaded_models_dict))", "code_tokens": ["def", "_get_full_model_names_from_exp_structure", "(", "cls", ",", "exp_structure", ":", "dict", "[", "str", ",", "Any", "]", ",", "namespace", ":", "str", ")", "-", ">", "set", "[", "str", "]", ":", "STRING", "structure_keys", ":", "list", "[", "str", "]", "=", "namespace", ".", "split", "(", "cls", ".", "LOGGER_JOIN_CHAR", ")", "for", "key", "in", "structure_keys", ":", "exp_structure", "=", "exp_structure", "[", "key", "]", "uploaded_models_dict", "=", "exp_structure", "return", "set", "(", "cls", ".", "_dict_paths", "(", "uploaded_models_dict", ")", ")"], "docstring": "Returns all paths to properties which were already logged in `namespace`", "docstring_tokens": ["returns", "all", "paths", "to", "properties", "which", "were", "already", "logged", "in", "namespace"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 562, "end_line": 568, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_660", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name or 'offline-name' when exp is run in offline mode.\"\"\"\r\n        return self._run_name", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Return the experiment name or 'offline-name' when exp is run in offline mode.\"\"\"\r\n        return self._run_name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_run_name"], "docstring": "Return the experiment name or 'offline-name' when exp is run in offline mode.", "docstring_tokens": ["return", "the", "experiment", "name", "or", "offline", "name", "when", "exp", "is", "run", "in", "offline", "mode"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 581, "end_line": 583, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "function_661", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Return the experiment version.\r\n\r\n        It's Neptune Run's short_id\r\n\r\n        \"\"\"\r\n        return self._run_short_id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Return the experiment version.\r\n\r\n        It's Neptune Run's short_id\r\n\r\n        \"\"\"\r\n        return self._run_short_id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_run_short_id"], "docstring": "Return the experiment version.", "docstring_tokens": ["return", "the", "experiment", "version"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\neptune.py", "start_line": 587, "end_line": 593, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "function_662", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all tensorboard checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(super().root_dir, self.name)", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all tensorboard checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(super().root_dir, self.name)", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "os", ".", "path", ".", "join", "(", "super", "(", ")", ".", "root_dir", ",", "self", ".", "name", ")"], "docstring": "Parent directory for all tensorboard checkpoint subdirectories.", "docstring_tokens": ["parent", "directory", "for", "all", "tensorboard", "checkpoint", "subdirectories"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "start_line": 114, "end_line": 121, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "function_663", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "fSTRING", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "version", ")", "if", "isinstance", "(", "self", ".", "sub_dir", ",", "str", ")", ":", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "self", ".", "sub_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expandvars", "(", "log_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "log_dir", ")", "return", "log_dir"], "docstring": "The directory for this run's tensorboard checkpoint.", "docstring_tokens": ["the", "directory", "for", "this", "run", "s", "tensorboard", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "start_line": 125, "end_line": 139, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "function_664", "original_string": "def save_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "language": "python", "code": "def save_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "self", ".", "_root_dir"], "docstring": "Gets the save directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "start_line": 143, "end_line": 150, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "function_665", "original_string": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container, OmegaConf\r\n\r\n        params = _convert_params(params)\r\n\r\n        if _OMEGACONF_AVAILABLE and isinstance(params, Container):\r\n            self.hparams = OmegaConf.merge(self.hparams, params)\r\n        else:\r\n            self.hparams.update(params)\r\n\r\n        return super().log_hyperparams(params=params, metrics=metrics, step=step)", "language": "python", "code": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container, OmegaConf\r\n\r\n        params = _convert_params(params)\r\n\r\n        if _OMEGACONF_AVAILABLE and isinstance(params, Container):\r\n            self.hparams = OmegaConf.merge(self.hparams, params)\r\n        else:\r\n            self.hparams.update(params)\r\n\r\n        return super().log_hyperparams(params=params, metrics=metrics, step=step)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "metrics", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "_OMEGACONF_AVAILABLE", ":", "from", "omegaconf", "import", "Container", ",", "OmegaConf", "params", "=", "_convert_params", "(", "params", ")", "if", "_OMEGACONF_AVAILABLE", "and", "isinstance", "(", "params", ",", "Container", ")", ":", "self", ".", "hparams", "=", "OmegaConf", ".", "merge", "(", "self", ".", "hparams", ",", "params", ")", "else", ":", "self", ".", "hparams", ".", "update", "(", "params", ")", "return", "super", "(", ")", ".", "log_hyperparams", "(", "params", "=", "params", ",", "metrics", "=", "metrics", ",", "step", "=", "step", ")"], "docstring": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the", "docstring_tokens": ["record", "hyperparameters", "tensorboard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "start_line": 154, "end_line": 181, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "function_666", "original_string": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "ModelCheckpoint", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called after model checkpoint callback saves a new checkpoint.", "docstring_tokens": ["called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "start_line": 232, "end_line": 239, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\utilities.py", "func_name": "function_667", "original_string": "def _scan_checkpoints(checkpoint_callback: Checkpoint, logged_model_time: dict) -> list[tuple[float, str, float, str]]:\r\n    \"\"\"Return the checkpoints to be logged.\r\n\r\n    Args:\r\n        checkpoint_callback: Checkpoint callback reference.\r\n        logged_model_time: dictionary containing the logged model times.\r\n\r\n    \"\"\"\r\n    checkpoints = {}\r\n    if hasattr(checkpoint_callback, \"last_model_path\") and hasattr(checkpoint_callback, \"current_score\"):\r\n        checkpoints[checkpoint_callback.last_model_path] = (checkpoint_callback.current_score, \"latest\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_model_path\") and hasattr(checkpoint_callback, \"best_model_score\"):\r\n        checkpoints[checkpoint_callback.best_model_path] = (checkpoint_callback.best_model_score, \"best\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_k_models\"):\r\n        for key, value in checkpoint_callback.best_k_models.items():\r\n            checkpoints[key] = (value, \"best_k\")\r\n\r\n    checkpoints = sorted(\r\n        (Path(p).stat().st_mtime, p, s, tag) for p, (s, tag) in checkpoints.items() if Path(p).is_file()\r\n    )\r\n    checkpoints = [c for c in checkpoints if c[1] not in logged_model_time or logged_model_time[c[1]] < c[0]]\r\n    return checkpoints", "language": "python", "code": "def _scan_checkpoints(checkpoint_callback: Checkpoint, logged_model_time: dict) -> list[tuple[float, str, float, str]]:\r\n    \"\"\"Return the checkpoints to be logged.\r\n\r\n    Args:\r\n        checkpoint_callback: Checkpoint callback reference.\r\n        logged_model_time: dictionary containing the logged model times.\r\n\r\n    \"\"\"\r\n    checkpoints = {}\r\n    if hasattr(checkpoint_callback, \"last_model_path\") and hasattr(checkpoint_callback, \"current_score\"):\r\n        checkpoints[checkpoint_callback.last_model_path] = (checkpoint_callback.current_score, \"latest\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_model_path\") and hasattr(checkpoint_callback, \"best_model_score\"):\r\n        checkpoints[checkpoint_callback.best_model_path] = (checkpoint_callback.best_model_score, \"best\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_k_models\"):\r\n        for key, value in checkpoint_callback.best_k_models.items():\r\n            checkpoints[key] = (value, \"best_k\")\r\n\r\n    checkpoints = sorted(\r\n        (Path(p).stat().st_mtime, p, s, tag) for p, (s, tag) in checkpoints.items() if Path(p).is_file()\r\n    )\r\n    checkpoints = [c for c in checkpoints if c[1] not in logged_model_time or logged_model_time[c[1]] < c[0]]\r\n    return checkpoints", "code_tokens": ["def", "_scan_checkpoints", "(", "checkpoint_callback", ":", "Checkpoint", ",", "logged_model_time", ":", "dict", ")", "-", ">", "list", "[", "tuple", "[", "float", ",", "str", ",", "float", ",", "str", "]", "]", ":", "STRING", "checkpoints", "=", "{", "}", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", "and", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", ":", "checkpoints", "[", "checkpoint_callback", ".", "last_model_path", "]", "=", "(", "checkpoint_callback", ".", "current_score", ",", "STRING", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", "and", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", ":", "checkpoints", "[", "checkpoint_callback", ".", "best_model_path", "]", "=", "(", "checkpoint_callback", ".", "best_model_score", ",", "STRING", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "STRING", ")", ":", "for", "key", ",", "value", "in", "checkpoint_callback", ".", "best_k_models", ".", "items", "(", ")", ":", "checkpoints", "[", "key", "]", "=", "(", "value", ",", "STRING", ")", "checkpoints", "=", "sorted", "(", "(", "Path", "(", "p", ")", ".", "stat", "(", ")", ".", "st_mtime", ",", "p", ",", "s", ",", "tag", ")", "for", "p", ",", "(", "s", ",", "tag", ")", "in", "checkpoints", ".", "items", "(", ")", "if", "Path", "(", "p", ")", ".", "is_file", "(", ")", ")", "checkpoints", "=", "[", "c", "for", "c", "in", "checkpoints", "if", "c", "[", "1", "]", "not", "in", "logged_model_time", "or", "logged_model_time", "[", "c", "[", "1", "]", "]", "<", "c", "[", "0", "]", "]", "return", "checkpoints"], "docstring": "Return the checkpoints to be logged.", "docstring_tokens": ["return", "the", "checkpoints", "to", "be", "logged"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\utilities.py", "start_line": 31, "end_line": 55, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_668", "original_string": "def experiment(self) -> Union[\"Run\", \"RunDisabled\"]:\r\n        r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n        .. code-block:: python\r\n\r\n            self.logger.experiment.some_wandb_function()\r\n\r\n        \"\"\"\r\n        import wandb\r\n        from wandb.sdk.lib import RunDisabled\r\n        from wandb.wandb_run import Run\r\n\r\n        if self._experiment is None:\r\n            if self._offline:\r\n                os.environ[\"WANDB_MODE\"] = \"dryrun\"\r\n\r\n            attach_id = getattr(self, \"_attach_id\", None)\r\n            if wandb.run is not None:\r\n                rank_zero_warn(\r\n                    \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\r\n                    \" this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\"\r\n                )\r\n                self._experiment = wandb.run\r\n            elif attach_id is not None and hasattr(wandb, \"_attach\"):\r\n                self._experiment = wandb._attach(attach_id)\r\n            else:\r\n                self._experiment = wandb.init(**self._wandb_init)\r\n\r\n                if isinstance(self._experiment, (Run, RunDisabled)) and getattr(\r\n                    self._experiment, \"define_metric\", None\r\n                ):\r\n                    if self._wandb_init.get(\"sync_tensorboard\"):\r\n                        self._experiment.define_metric(\"*\", step_metric=\"global_step\")\r\n                    else:\r\n                        self._experiment.define_metric(\"trainer/global_step\")\r\n                        self._experiment.define_metric(\"*\", step_metric=\"trainer/global_step\", step_sync=True)\r\n\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> Union[\"Run\", \"RunDisabled\"]:\r\n        r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n        .. code-block:: python\r\n\r\n            self.logger.experiment.some_wandb_function()\r\n\r\n        \"\"\"\r\n        import wandb\r\n        from wandb.sdk.lib import RunDisabled\r\n        from wandb.wandb_run import Run\r\n\r\n        if self._experiment is None:\r\n            if self._offline:\r\n                os.environ[\"WANDB_MODE\"] = \"dryrun\"\r\n\r\n            attach_id = getattr(self, \"_attach_id\", None)\r\n            if wandb.run is not None:\r\n                rank_zero_warn(\r\n                    \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\r\n                    \" this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\"\r\n                )\r\n                self._experiment = wandb.run\r\n            elif attach_id is not None and hasattr(wandb, \"_attach\"):\r\n                self._experiment = wandb._attach(attach_id)\r\n            else:\r\n                self._experiment = wandb.init(**self._wandb_init)\r\n\r\n                if isinstance(self._experiment, (Run, RunDisabled)) and getattr(\r\n                    self._experiment, \"define_metric\", None\r\n                ):\r\n                    if self._wandb_init.get(\"sync_tensorboard\"):\r\n                        self._experiment.define_metric(\"*\", step_metric=\"global_step\")\r\n                    else:\r\n                        self._experiment.define_metric(\"trainer/global_step\")\r\n                        self._experiment.define_metric(\"*\", step_metric=\"trainer/global_step\", step_sync=True)\r\n\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "Union", "[", "STRING", ",", "STRING", "]", ":", "rSTRING", "import", "wandb", "from", "wandb", ".", "sdk", ".", "lib", "import", "RunDisabled", "from", "wandb", ".", "wandb_run", "import", "Run", "if", "self", ".", "_experiment", "is", "None", ":", "if", "self", ".", "_offline", ":", "os", ".", "environ", "[", "STRING", "]", "=", "STRING", "attach_id", "=", "getattr", "(", "self", ",", "STRING", ",", "None", ")", "if", "wandb", ".", "run", "is", "not", "None", ":", "rank_zero_warn", "(", "STRING", "STRING", ")", "self", ".", "_experiment", "=", "wandb", ".", "run", "elif", "attach_id", "is", "not", "None", "and", "hasattr", "(", "wandb", ",", "STRING", ")", ":", "self", ".", "_experiment", "=", "wandb", ".", "_attach", "(", "attach_id", ")", "else", ":", "self", ".", "_experiment", "=", "wandb", ".", "init", "(", "*", "*", "self", ".", "_wandb_init", ")", "if", "isinstance", "(", "self", ".", "_experiment", ",", "(", "Run", ",", "RunDisabled", ")", ")", "and", "getattr", "(", "self", ".", "_experiment", ",", "STRING", ",", "None", ")", ":", "if", "self", ".", "_wandb_init", ".", "get", "(", "STRING", ")", ":", "self", ".", "_experiment", ".", "define_metric", "(", "STRING", ",", "step_metric", "=", "STRING", ")", "else", ":", "self", ".", "_experiment", ".", "define_metric", "(", "STRING", ")", "self", ".", "_experiment", ".", "define_metric", "(", "STRING", ",", "step_metric", "=", "STRING", ",", "step_sync", "=", "True", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the", "docstring_tokens": ["r", "actual", "wandb", "object", "to", "use", "wandb", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 377, "end_line": 421, "has_examples": true, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_669", "original_string": "def log_table(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[Any]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log a Table containing any object type (text, image, audio, video, molecule, html, etc).\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        metrics = {key: wandb.Table(columns=columns, data=data, dataframe=dataframe)}\r\n        self.log_metrics(metrics, step)", "language": "python", "code": "def log_table(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[Any]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log a Table containing any object type (text, image, audio, video, molecule, html, etc).\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        metrics = {key: wandb.Table(columns=columns, data=data, dataframe=dataframe)}\r\n        self.log_metrics(metrics, step)", "code_tokens": ["def", "log_table", "(", "self", ",", "key", ":", "str", ",", "columns", ":", "Optional", "[", "list", "[", "str", "]", "]", "=", "None", ",", "data", ":", "Optional", "[", "list", "[", "list", "[", "Any", "]", "]", "]", "=", "None", ",", "dataframe", ":", "Any", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "import", "wandb", "metrics", "=", "{", "key", ":", "wandb", ".", "Table", "(", "columns", "=", "columns", ",", "data", "=", "data", ",", "dataframe", "=", "dataframe", ")", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")"], "docstring": "Log a Table containing any object type (text, image, audio, video, molecule, html, etc).", "docstring_tokens": ["log", "a", "table", "containing", "any", "object", "type", "text", "image", "audio", "video", "molecule", "html", "etc"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 448, "end_line": 464, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_670", "original_string": "def log_text(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[str]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log text as a Table.\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n\r\n        self.log_table(key, columns, data, dataframe, step)", "language": "python", "code": "def log_text(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[str]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log text as a Table.\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n\r\n        self.log_table(key, columns, data, dataframe, step)", "code_tokens": ["def", "log_text", "(", "self", ",", "key", ":", "str", ",", "columns", ":", "Optional", "[", "list", "[", "str", "]", "]", "=", "None", ",", "data", ":", "Optional", "[", "list", "[", "list", "[", "str", "]", "]", "]", "=", "None", ",", "dataframe", ":", "Any", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "log_table", "(", "key", ",", "columns", ",", "data", ",", "dataframe", ",", "step", ")"], "docstring": "Log text as a Table.", "docstring_tokens": ["log", "text", "as", "a", "table"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 467, "end_line": 481, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_671", "original_string": "def log_image(self, key: str, images: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log images (tensors, numpy arrays, PIL Images or file paths).\r\n\r\n        Optional kwargs are lists passed to each image (ex: caption, masks, boxes).\r\n\r\n        \"\"\"\r\n        if not isinstance(images, list):\r\n            raise TypeError(f'Expected a list as \"images\", found {type(images)}')\r\n        n = len(images)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Image(img, **kwarg) for img, kwarg in zip(images, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_image(self, key: str, images: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log images (tensors, numpy arrays, PIL Images or file paths).\r\n\r\n        Optional kwargs are lists passed to each image (ex: caption, masks, boxes).\r\n\r\n        \"\"\"\r\n        if not isinstance(images, list):\r\n            raise TypeError(f'Expected a list as \"images\", found {type(images)}')\r\n        n = len(images)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Image(img, **kwarg) for img, kwarg in zip(images, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_image", "(", "self", ",", "key", ":", "str", ",", "images", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "not", "isinstance", "(", "images", ",", "list", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "n", "=", "len", "(", "images", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "fSTRING", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Image", "(", "img", ",", "*", "*", "kwarg", ")", "for", "img", ",", "kwarg", "in", "zip", "(", "images", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]"], "docstring": "Log images (tensors, numpy arrays, PIL Images or file paths).", "docstring_tokens": ["log", "images", "tensors", "numpy", "arrays", "pil", "images", "or", "file", "paths"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 484, "end_line": 501, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_672", "original_string": "def log_audio(self, key: str, audios: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Log audios (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the audio files\r\n            audios: The list of audio file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the audio files\r\n            \\**kwargs: Optional kwargs are lists passed to each ``Wandb.Audio`` instance (ex: caption, sample_rate).\r\n\r\n        Optional kwargs are lists passed to each audio (ex: caption, sample_rate).\r\n\r\n        \"\"\"\r\n        if not isinstance(audios, list):\r\n            raise TypeError(f'Expected a list as \"audios\", found {type(audios)}')\r\n        n = len(audios)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Audio(audio, **kwarg) for audio, kwarg in zip(audios, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_audio(self, key: str, audios: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Log audios (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the audio files\r\n            audios: The list of audio file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the audio files\r\n            \\**kwargs: Optional kwargs are lists passed to each ``Wandb.Audio`` instance (ex: caption, sample_rate).\r\n\r\n        Optional kwargs are lists passed to each audio (ex: caption, sample_rate).\r\n\r\n        \"\"\"\r\n        if not isinstance(audios, list):\r\n            raise TypeError(f'Expected a list as \"audios\", found {type(audios)}')\r\n        n = len(audios)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Audio(audio, **kwarg) for audio, kwarg in zip(audios, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_audio", "(", "self", ",", "key", ":", "str", ",", "audios", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "rSTRING", "if", "not", "isinstance", "(", "audios", ",", "list", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "n", "=", "len", "(", "audios", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "fSTRING", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Audio", "(", "audio", ",", "*", "*", "kwarg", ")", "for", "audio", ",", "kwarg", "in", "zip", "(", "audios", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]"], "docstring": "r\"\"\"Log audios (numpy arrays, or file paths).", "docstring_tokens": ["r", "log", "audios", "numpy", "arrays", "or", "file", "paths"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 504, "end_line": 527, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_673", "original_string": "def log_video(self, key: str, videos: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log videos (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the video files\r\n            videos: The list of video file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the video files\r\n            **kwargs: Optional kwargs are lists passed to each Wandb.Video instance (ex: caption, fps, format).\r\n\r\n        Optional kwargs are lists passed to each video (ex: caption, fps, format).\r\n\r\n        \"\"\"\r\n        if not isinstance(videos, list):\r\n            raise TypeError(f'Expected a list as \"videos\", found {type(videos)}')\r\n        n = len(videos)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Video(video, **kwarg) for video, kwarg in zip(videos, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_video(self, key: str, videos: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log videos (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the video files\r\n            videos: The list of video file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the video files\r\n            **kwargs: Optional kwargs are lists passed to each Wandb.Video instance (ex: caption, fps, format).\r\n\r\n        Optional kwargs are lists passed to each video (ex: caption, fps, format).\r\n\r\n        \"\"\"\r\n        if not isinstance(videos, list):\r\n            raise TypeError(f'Expected a list as \"videos\", found {type(videos)}')\r\n        n = len(videos)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Video(video, **kwarg) for video, kwarg in zip(videos, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_video", "(", "self", ",", "key", ":", "str", ",", "videos", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "not", "isinstance", "(", "videos", ",", "list", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "n", "=", "len", "(", "videos", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "fSTRING", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Video", "(", "video", ",", "*", "*", "kwarg", ")", "for", "video", ",", "kwarg", "in", "zip", "(", "videos", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]"], "docstring": "Log videos (numpy arrays, or file paths).", "docstring_tokens": ["log", "videos", "numpy", "arrays", "or", "file", "paths"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 530, "end_line": 553, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_674", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_save_dir"], "docstring": "Gets the save directory.", "docstring_tokens": ["gets", "the", "save", "directory"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 557, "end_line": 564, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_675", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"The project name of this experiment.\r\n\r\n        Returns:\r\n            The name of the project the current experiment belongs to. This name is not the same as `wandb.Run`'s\r\n            name. To access wandb's internal experiment name, use ``logger.experiment.name`` instead.\r\n\r\n        \"\"\"\r\n        return self._project", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"The project name of this experiment.\r\n\r\n        Returns:\r\n            The name of the project the current experiment belongs to. This name is not the same as `wandb.Run`'s\r\n            name. To access wandb's internal experiment name, use ``logger.experiment.name`` instead.\r\n\r\n        \"\"\"\r\n        return self._project", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_project"], "docstring": "The project name of this experiment.", "docstring_tokens": ["the", "project", "name", "of", "this", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 568, "end_line": 576, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_676", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the id of the experiment.\r\n\r\n        Returns:\r\n            The id of the experiment if the experiment exists else the id given to the constructor.\r\n\r\n        \"\"\"\r\n        return self._experiment.id if self._experiment else self._id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the id of the experiment.\r\n\r\n        Returns:\r\n            The id of the experiment if the experiment exists else the id given to the constructor.\r\n\r\n        \"\"\"\r\n        return self._experiment.id if self._experiment else self._id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "return", "self", ".", "_experiment", ".", "id", "if", "self", ".", "_experiment", "else", "self", ".", "_id"], "docstring": "Gets the id of the experiment.", "docstring_tokens": ["gets", "the", "id", "of", "the", "experiment"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 580, "end_line": 588, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_677", "original_string": "def download_artifact(\r\n        artifact: str,\r\n        save_dir: Optional[_PATH] = None,\r\n        artifact_type: Optional[str] = None,\r\n        use_artifact: Optional[bool] = True,\r\n    ) -> str:\r\n        \"\"\"Downloads an artifact from the wandb server.\r\n\r\n        Args:\r\n            artifact: The path of the artifact to download.\r\n            save_dir: The directory to save the artifact to.\r\n            artifact_type: The type of artifact to download.\r\n            use_artifact: Whether to add an edge between the artifact graph.\r\n\r\n        Returns:\r\n            The path to the downloaded artifact.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        if wandb.run is not None and use_artifact:\r\n            artifact = wandb.run.use_artifact(artifact)\r\n        else:\r\n            api = wandb.Api()\r\n            artifact = api.artifact(artifact, type=artifact_type)\r\n\r\n        save_dir = None if save_dir is None else os.fspath(save_dir)\r\n        return artifact.download(root=save_dir)", "language": "python", "code": "def download_artifact(\r\n        artifact: str,\r\n        save_dir: Optional[_PATH] = None,\r\n        artifact_type: Optional[str] = None,\r\n        use_artifact: Optional[bool] = True,\r\n    ) -> str:\r\n        \"\"\"Downloads an artifact from the wandb server.\r\n\r\n        Args:\r\n            artifact: The path of the artifact to download.\r\n            save_dir: The directory to save the artifact to.\r\n            artifact_type: The type of artifact to download.\r\n            use_artifact: Whether to add an edge between the artifact graph.\r\n\r\n        Returns:\r\n            The path to the downloaded artifact.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        if wandb.run is not None and use_artifact:\r\n            artifact = wandb.run.use_artifact(artifact)\r\n        else:\r\n            api = wandb.Api()\r\n            artifact = api.artifact(artifact, type=artifact_type)\r\n\r\n        save_dir = None if save_dir is None else os.fspath(save_dir)\r\n        return artifact.download(root=save_dir)", "code_tokens": ["def", "download_artifact", "(", "artifact", ":", "str", ",", "save_dir", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "artifact_type", ":", "Optional", "[", "str", "]", "=", "None", ",", "use_artifact", ":", "Optional", "[", "bool", "]", "=", "True", ",", ")", "-", ">", "str", ":", "STRING", "import", "wandb", "if", "wandb", ".", "run", "is", "not", "None", "and", "use_artifact", ":", "artifact", "=", "wandb", ".", "run", ".", "use_artifact", "(", "artifact", ")", "else", ":", "api", "=", "wandb", ".", "Api", "(", ")", "artifact", "=", "api", ".", "artifact", "(", "artifact", ",", "type", "=", "artifact_type", ")", "save_dir", "=", "None", "if", "save_dir", "is", "None", "else", "os", ".", "fspath", "(", "save_dir", ")", "return", "artifact", ".", "download", "(", "root", "=", "save_dir", ")"], "docstring": "Downloads an artifact from the wandb server.", "docstring_tokens": ["downloads", "an", "artifact", "from", "the", "wandb", "server"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 600, "end_line": 627, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "function_678", "original_string": "def use_artifact(self, artifact: str, artifact_type: Optional[str] = None) -> \"Artifact\":\r\n        \"\"\"Logs to the wandb dashboard that the mentioned artifact is used by the run.\r\n\r\n        Args:\r\n            artifact: The path of the artifact.\r\n            artifact_type: The type of artifact being used.\r\n\r\n        Returns:\r\n            wandb Artifact object for the artifact.\r\n\r\n        \"\"\"\r\n        return self.experiment.use_artifact(artifact, type=artifact_type)", "language": "python", "code": "def use_artifact(self, artifact: str, artifact_type: Optional[str] = None) -> \"Artifact\":\r\n        \"\"\"Logs to the wandb dashboard that the mentioned artifact is used by the run.\r\n\r\n        Args:\r\n            artifact: The path of the artifact.\r\n            artifact_type: The type of artifact being used.\r\n\r\n        Returns:\r\n            wandb Artifact object for the artifact.\r\n\r\n        \"\"\"\r\n        return self.experiment.use_artifact(artifact, type=artifact_type)", "code_tokens": ["def", "use_artifact", "(", "self", ",", "artifact", ":", "str", ",", "artifact_type", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "STRING", ":", "STRING", "return", "self", ".", "experiment", ".", "use_artifact", "(", "artifact", ",", "type", "=", "artifact_type", ")"], "docstring": "Logs to the wandb dashboard that the mentioned artifact is used by the run.", "docstring_tokens": ["logs", "to", "the", "wandb", "dashboard", "that", "the", "mentioned", "artifact", "is", "used", "by", "the", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loggers\\wandb.py", "start_line": 629, "end_line": 640, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_679", "original_string": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "language": "python", "code": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "code_tokens": ["def", "num_dataloaders", "(", "self", ")", "-", ">", "int", ":", "STRING", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "return", "len", "(", "combined_loader", ".", "flattened", ")"], "docstring": "Returns the number of prediction dataloaders.", "docstring_tokens": ["returns", "the", "number", "of", "prediction", "dataloaders"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 88, "end_line": 92, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_680", "original_string": "def max_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The max number of batches to run per dataloader.\"\"\"\r\n        max_batches = self._max_batches\r\n        if not self.trainer.sanity_checking:\r\n            return max_batches\r\n        return [min(self.trainer.num_sanity_val_steps, batches) for batches in max_batches]", "language": "python", "code": "def max_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The max number of batches to run per dataloader.\"\"\"\r\n        max_batches = self._max_batches\r\n        if not self.trainer.sanity_checking:\r\n            return max_batches\r\n        return [min(self.trainer.num_sanity_val_steps, batches) for batches in max_batches]", "code_tokens": ["def", "max_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "max_batches", "=", "self", ".", "_max_batches", "if", "not", "self", ".", "trainer", ".", "sanity_checking", ":", "return", "max_batches", "return", "[", "min", "(", "self", ".", "trainer", ".", "num_sanity_val_steps", ",", "batches", ")", "for", "batches", "in", "max_batches", "]"], "docstring": "The max number of batches to run per dataloader.", "docstring_tokens": ["the", "max", "number", "of", "batches", "to", "run", "per", "dataloader"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 95, "end_line": 100, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_681", "original_string": "def skip(self) -> bool:\r\n        \"\"\"Returns whether the evaluation should be skipped.\"\"\"\r\n        return sum(self.max_batches) == 0", "language": "python", "code": "def skip(self) -> bool:\r\n        \"\"\"Returns whether the evaluation should be skipped.\"\"\"\r\n        return sum(self.max_batches) == 0", "code_tokens": ["def", "skip", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "sum", "(", "self", ".", "max_batches", ")", "=", "=", "0"], "docstring": "Returns whether the evaluation should be skipped.", "docstring_tokens": ["returns", "whether", "the", "evaluation", "should", "be", "skipped"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 103, "end_line": 105, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_682", "original_string": "def _should_reload_val_dl(self) -> bool:\r\n        \"\"\"Check if validation dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return bool(n_epochs and self.trainer.current_epoch - self._last_val_dl_reload_epoch >= n_epochs)", "language": "python", "code": "def _should_reload_val_dl(self) -> bool:\r\n        \"\"\"Check if validation dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return bool(n_epochs and self.trainer.current_epoch - self._last_val_dl_reload_epoch >= n_epochs)", "code_tokens": ["def", "_should_reload_val_dl", "(", "self", ")", "-", ">", "bool", ":", "STRING", "n_epochs", "=", "self", ".", "trainer", ".", "reload_dataloaders_every_n_epochs", "return", "bool", "(", "n_epochs", "and", "self", ".", "trainer", ".", "current_epoch", "-", "self", ".", "_last_val_dl_reload_epoch", ">", "=", "n_epochs", ")"], "docstring": "Check if validation dataloader should be reloaded.", "docstring_tokens": ["check", "if", "validation", "dataloader", "should", "be", "reloaded"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 108, "end_line": 111, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_683", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self._has_run = False\r\n        self._logged_outputs = []\r\n\r\n        if not self.restarting:\r\n            self.batch_progress.reset_on_run()\r\n        else:\r\n            self.batch_progress.reset_on_restart()\r\n        fn = trainer.state.fn\r\n        assert fn is not None\r\n        if fn != TrainerFn.FITTING:\r\n            self.batch_progress.reset_on_run()\r\n\r\n        assert trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(trainer, trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n\r\n        if fn == TrainerFn.FITTING:\r\n            for i, dl in enumerate(combined_loader.flattened):\r\n                _set_sampler_epoch(dl, trainer.fit_loop.epoch_progress.current.processed)\r\n\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self._has_run = False\r\n        self._logged_outputs = []\r\n\r\n        if not self.restarting:\r\n            self.batch_progress.reset_on_run()\r\n        else:\r\n            self.batch_progress.reset_on_restart()\r\n        fn = trainer.state.fn\r\n        assert fn is not None\r\n        if fn != TrainerFn.FITTING:\r\n            self.batch_progress.reset_on_run()\r\n\r\n        assert trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(trainer, trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n\r\n        if fn == TrainerFn.FITTING:\r\n            for i, dl in enumerate(combined_loader.flattened):\r\n                _set_sampler_epoch(dl, trainer.fit_loop.epoch_progress.current.processed)\r\n\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "self", ".", "_has_run", "=", "False", "self", ".", "_logged_outputs", "=", "[", "]", "if", "not", "self", ".", "restarting", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "else", ":", "self", ".", "batch_progress", ".", "reset_on_restart", "(", ")", "fn", "=", "trainer", ".", "state", ".", "fn", "assert", "fn", "is", "not", "None", "if", "fn", "!", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "assert", "trainer", ".", "state", ".", "stage", "is", "not", "None", "data_fetcher", "=", "_select_data_fetcher", "(", "trainer", ",", "trainer", ".", "state", ".", "stage", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "for", "i", ",", "dl", "in", "enumerate", "(", "combined_loader", ".", "flattened", ")", ":", "_set_sampler_epoch", "(", "dl", ",", "trainer", ".", "fit_loop", ".", "epoch_progress", ".", "current", ".", "processed", ")", "combined_loader", ".", "limits", "=", "self", ".", "max_batches", "data_fetcher", ".", "setup", "(", "combined_loader", ")", "iter", "(", "data_fetcher", ")", "#", "creates", "the", "iterator", "inside", "the", "fetcher", "data_fetcher", ".", "fetched", "+", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "data_fetcher", ".", "_start_profiler", "=", "self", ".", "_on_before_fetch", "data_fetcher", ".", "_stop_profiler", "=", "self", ".", "_on_after_fetch", "self", ".", "_data_fetcher", "=", "data_fetcher"], "docstring": "Resets the internal state of the loop.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 228, "end_line": 265, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_684", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``\r\n        hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_evaluation_model_eval()\r\n        self._on_evaluation_start()\r\n        self._on_evaluation_epoch_start()", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``\r\n        hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_evaluation_model_eval()\r\n        self._on_evaluation_start()\r\n        self._on_evaluation_epoch_start()", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "_verify_dataloader_idx_requirement", "(", ")", "self", ".", "_on_evaluation_model_eval", "(", ")", "self", ".", "_on_evaluation_start", "(", ")", "self", ".", "_on_evaluation_epoch_start", "(", ")"], "docstring": "Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``", "docstring_tokens": ["runs", "the", "_on_evaluation_model_eval", "_on_evaluation_start", "and", "_on_evaluation_epoch_start"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 280, "end_line": 286, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_685", "original_string": "def on_run_end(self) -> list[_OUT_DICT]:\r\n        \"\"\"Runs the ``_on_evaluation_epoch_end`` hook.\"\"\"\r\n        self.trainer._logger_connector.epoch_end_reached()\r\n        self.trainer._logger_connector._evaluation_epoch_end()\r\n\r\n        self._on_evaluation_epoch_end()\r\n\r\n        logged_outputs, self._logged_outputs = self._logged_outputs, []  # free memory\r\n        epoch_end_logged_outputs = self.trainer._logger_connector.update_eval_epoch_metrics()\r\n        all_logged_outputs = dict(ChainMap(*logged_outputs))  # list[dict] -> dict\r\n        all_logged_outputs.update(epoch_end_logged_outputs)\r\n        for dl_outputs in logged_outputs:\r\n            dl_outputs.update(epoch_end_logged_outputs)\r\n\r\n        self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n\r\n        self._on_evaluation_end()\r\n\r\n        self._on_evaluation_model_train()\r\n\r\n        if self.verbose and self.trainer.is_global_zero:\r\n            self._print_results(logged_outputs, self._stage.value)\r\n\r\n        now = time.monotonic()\r\n        self.trainer._last_val_time = now\r\n\r\n        return logged_outputs", "language": "python", "code": "def on_run_end(self) -> list[_OUT_DICT]:\r\n        \"\"\"Runs the ``_on_evaluation_epoch_end`` hook.\"\"\"\r\n        self.trainer._logger_connector.epoch_end_reached()\r\n        self.trainer._logger_connector._evaluation_epoch_end()\r\n\r\n        self._on_evaluation_epoch_end()\r\n\r\n        logged_outputs, self._logged_outputs = self._logged_outputs, []  # free memory\r\n        epoch_end_logged_outputs = self.trainer._logger_connector.update_eval_epoch_metrics()\r\n        all_logged_outputs = dict(ChainMap(*logged_outputs))  # list[dict] -> dict\r\n        all_logged_outputs.update(epoch_end_logged_outputs)\r\n        for dl_outputs in logged_outputs:\r\n            dl_outputs.update(epoch_end_logged_outputs)\r\n\r\n        self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n\r\n        self._on_evaluation_end()\r\n\r\n        self._on_evaluation_model_train()\r\n\r\n        if self.verbose and self.trainer.is_global_zero:\r\n            self._print_results(logged_outputs, self._stage.value)\r\n\r\n        now = time.monotonic()\r\n        self.trainer._last_val_time = now\r\n\r\n        return logged_outputs", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "list", "[", "_OUT_DICT", "]", ":", "STRING", "self", ".", "trainer", ".", "_logger_connector", ".", "epoch_end_reached", "(", ")", "self", ".", "trainer", ".", "_logger_connector", ".", "_evaluation_epoch_end", "(", ")", "self", ".", "_on_evaluation_epoch_end", "(", ")", "logged_outputs", ",", "self", ".", "_logged_outputs", "=", "self", ".", "_logged_outputs", ",", "[", "]", "#", "free", "memory", "epoch_end_logged_outputs", "=", "self", ".", "trainer", ".", "_logger_connector", ".", "update_eval_epoch_metrics", "(", ")", "all_logged_outputs", "=", "dict", "(", "ChainMap", "(", "*", "logged_outputs", ")", ")", "#", "list", "[", "dict", "]", "-", ">", "dict", "all_logged_outputs", ".", "update", "(", "epoch_end_logged_outputs", ")", "for", "dl_outputs", "in", "logged_outputs", ":", "dl_outputs", ".", "update", "(", "epoch_end_logged_outputs", ")", "self", ".", "trainer", ".", "_logger_connector", ".", "log_eval_end_metrics", "(", "all_logged_outputs", ")", "self", ".", "_on_evaluation_end", "(", ")", "self", ".", "_on_evaluation_model_train", "(", ")", "if", "self", ".", "verbose", "and", "self", ".", "trainer", ".", "is_global_zero", ":", "self", ".", "_print_results", "(", "logged_outputs", ",", "self", ".", "_stage", ".", "value", ")", "now", "=", "time", ".", "monotonic", "(", ")", "self", ".", "trainer", ".", "_last_val_time", "=", "now", "return", "logged_outputs"], "docstring": "Runs the ``_on_evaluation_epoch_end`` hook.", "docstring_tokens": ["runs", "the", "_on_evaluation_epoch_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 288, "end_line": 320, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_686", "original_string": "def _on_evaluation_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_start\" if trainer.testing else \"on_validation_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)", "language": "python", "code": "def _on_evaluation_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_start\" if trainer.testing else \"on_validation_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)", "code_tokens": ["def", "_on_evaluation_start", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Runs ``on_{validation/test}_start`` hooks.", "docstring_tokens": ["runs", "on_", "validation", "test", "_start", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 328, "end_line": 335, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_687", "original_string": "def _on_evaluation_model_eval(self) -> None:\r\n        \"\"\"Sets model to eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_eval\" if trainer.testing else \"on_validation_model_eval\"\r\n        self._module_mode.capture(trainer.lightning_module)\r\n        call._call_lightning_module_hook(trainer, hook_name)", "language": "python", "code": "def _on_evaluation_model_eval(self) -> None:\r\n        \"\"\"Sets model to eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_eval\" if trainer.testing else \"on_validation_model_eval\"\r\n        self._module_mode.capture(trainer.lightning_module)\r\n        call._call_lightning_module_hook(trainer, hook_name)", "code_tokens": ["def", "_on_evaluation_model_eval", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "self", ".", "_module_mode", ".", "capture", "(", "trainer", ".", "lightning_module", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")"], "docstring": "Sets model to eval mode.", "docstring_tokens": ["sets", "model", "to", "eval", "mode"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 337, "end_line": 342, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_688", "original_string": "def _on_evaluation_model_train(self) -> None:\r\n        \"\"\"Undoes the eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_train\" if trainer.testing else \"on_validation_model_train\"\r\n        if is_overridden(hook_name, trainer.lightning_module):\r\n            call._call_lightning_module_hook(trainer, hook_name)\r\n        else:\r\n            self._module_mode.restore(trainer.lightning_module)", "language": "python", "code": "def _on_evaluation_model_train(self) -> None:\r\n        \"\"\"Undoes the eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_train\" if trainer.testing else \"on_validation_model_train\"\r\n        if is_overridden(hook_name, trainer.lightning_module):\r\n            call._call_lightning_module_hook(trainer, hook_name)\r\n        else:\r\n            self._module_mode.restore(trainer.lightning_module)", "code_tokens": ["def", "_on_evaluation_model_train", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "if", "is_overridden", "(", "hook_name", ",", "trainer", ".", "lightning_module", ")", ":", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")", "else", ":", "self", ".", "_module_mode", ".", "restore", "(", "trainer", ".", "lightning_module", ")"], "docstring": "Undoes the eval mode.", "docstring_tokens": ["undoes", "the", "eval", "mode"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 344, "end_line": 351, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_689", "original_string": "def _on_evaluation_end(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_end\" if trainer.testing else \"on_validation_end\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)\r\n\r\n        trainer._logger_connector.reset_results()", "language": "python", "code": "def _on_evaluation_end(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_end\" if trainer.testing else \"on_validation_end\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)\r\n\r\n        trainer._logger_connector.reset_results()", "code_tokens": ["def", "_on_evaluation_end", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "trainer", ".", "_logger_connector", ".", "reset_results", "(", ")"], "docstring": "Runs ``on_{validation/test}_end`` hook.", "docstring_tokens": ["runs", "on_", "validation", "test", "_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 353, "end_line": 362, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_690", "original_string": "def _on_evaluation_epoch_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs the ``on_{validation/test}_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_start\" if trainer.testing else \"on_validation_epoch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)", "language": "python", "code": "def _on_evaluation_epoch_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs the ``on_{validation/test}_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_start\" if trainer.testing else \"on_validation_epoch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)", "code_tokens": ["def", "_on_evaluation_epoch_start", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Runs the ``on_{validation/test}_epoch_start`` hooks.", "docstring_tokens": ["runs", "the", "on_", "validation", "test", "_epoch_start", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 364, "end_line": 370, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_691", "original_string": "def _on_evaluation_epoch_end(self) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_epoch_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_end\" if trainer.testing else \"on_validation_epoch_end\"\r\n        call._call_callback_hooks(trainer, hook_name)\r\n        call._call_lightning_module_hook(trainer, hook_name)\r\n\r\n        trainer._logger_connector.on_epoch_end()", "language": "python", "code": "def _on_evaluation_epoch_end(self) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_epoch_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_end\" if trainer.testing else \"on_validation_epoch_end\"\r\n        call._call_callback_hooks(trainer, hook_name)\r\n        call._call_lightning_module_hook(trainer, hook_name)\r\n\r\n        trainer._logger_connector.on_epoch_end()", "code_tokens": ["def", "_on_evaluation_epoch_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")", "trainer", ".", "_logger_connector", ".", "on_epoch_end", "(", ")"], "docstring": "Runs ``on_{validation/test}_epoch_end`` hook.", "docstring_tokens": ["runs", "on_", "validation", "test", "_epoch_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 372, "end_line": 380, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_692", "original_string": "def _evaluation_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual evaluation step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: The current batch to run through the step.\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        hook_kwargs = self._build_kwargs(\r\n            batch, batch_idx, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n        )\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        trainer._logger_connector.on_batch_start(\r\n            batch, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n        )\r\n\r\n        hook_name = \"on_test_batch_start\" if trainer.testing else \"on_validation_batch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, hook_name, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        hook_name = \"test_step\" if trainer.testing else \"validation_step\"\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        output = call._call_strategy_hook(trainer, hook_name, *step_args)\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(\r\n                batch, batch_idx, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n            )\r\n\r\n        hook_name = \"on_test_batch_end\" if trainer.testing else \"on_validation_batch_end\"\r\n        call._call_callback_hooks(trainer, hook_name, output, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, hook_name, output, *hook_kwargs.values())\r\n\r\n        trainer._logger_connector.on_batch_end()\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if not trainer.sanity_checking:\r\n            self._has_run = True\r\n\r\n            trainer._logger_connector.update_eval_step_metrics(self._seen_batches_per_dataloader[dataloader_idx])\r\n            self._seen_batches_per_dataloader[dataloader_idx] += 1\r\n\r\n        if not self.batch_progress.is_last_batch and trainer.received_sigterm:\r\n            raise SIGTERMException", "language": "python", "code": "def _evaluation_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual evaluation step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: The current batch to run through the step.\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        hook_kwargs = self._build_kwargs(\r\n            batch, batch_idx, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n        )\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        trainer._logger_connector.on_batch_start(\r\n            batch, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n        )\r\n\r\n        hook_name = \"on_test_batch_start\" if trainer.testing else \"on_validation_batch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, hook_name, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        hook_name = \"test_step\" if trainer.testing else \"validation_step\"\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        output = call._call_strategy_hook(trainer, hook_name, *step_args)\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(\r\n                batch, batch_idx, dataloader_idx if self._is_sequential and self.num_dataloaders > 1 else None\r\n            )\r\n\r\n        hook_name = \"on_test_batch_end\" if trainer.testing else \"on_validation_batch_end\"\r\n        call._call_callback_hooks(trainer, hook_name, output, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, hook_name, output, *hook_kwargs.values())\r\n\r\n        trainer._logger_connector.on_batch_end()\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if not trainer.sanity_checking:\r\n            self._has_run = True\r\n\r\n            trainer._logger_connector.update_eval_step_metrics(self._seen_batches_per_dataloader[dataloader_idx])\r\n            self._seen_batches_per_dataloader[dataloader_idx] += 1\r\n\r\n        if not self.batch_progress.is_last_batch and trainer.received_sigterm:\r\n            raise SIGTERMException", "code_tokens": ["def", "_evaluation_step", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", ",", "dataloader_iter", ":", "Optional", "[", "Iterator", "]", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "data_fetcher", "=", "self", ".", "_data_fetcher", "assert", "data_fetcher", "is", "not", "None", "if", "not", "(", "using_dataloader_iter", ":", "=", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ")", ":", "batch", "=", "trainer", ".", "precision_plugin", ".", "convert_input", "(", "batch", ")", "batch", "=", "trainer", ".", "lightning_module", ".", "_on_before_batch_transfer", "(", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "batch", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "_is_sequential", "and", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "self", ".", "batch_progress", ".", "increment_ready", "(", ")", "trainer", ".", "_logger_connector", ".", "on_batch_start", "(", "batch", ",", "dataloader_idx", "if", "self", ".", "_is_sequential", "and", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "self", ".", "batch_progress", ".", "increment_started", "(", ")", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "step_args", "=", "(", "self", ".", "_build_step_args_from_hook_kwargs", "(", "hook_kwargs", ",", "hook_name", ")", "if", "not", "using_dataloader_iter", "else", "(", "dataloader_iter", ",", ")", ")", "output", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "hook_name", ",", "*", "step_args", ")", "self", ".", "batch_progress", ".", "increment_processed", "(", ")", "if", "using_dataloader_iter", ":", "batch", "=", "data_fetcher", ".", "_batch", "batch_idx", "=", "data_fetcher", ".", "_batch_idx", "dataloader_idx", "=", "data_fetcher", ".", "_dataloader_idx", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "_is_sequential", "and", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "hook_name", "=", "STRING", "if", "trainer", ".", "testing", "else", "STRING", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "output", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "output", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "trainer", ".", "_logger_connector", ".", "on_batch_end", "(", ")", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "if", "not", "trainer", ".", "sanity_checking", ":", "self", ".", "_has_run", "=", "True", "trainer", ".", "_logger_connector", ".", "update_eval_step_metrics", "(", "self", ".", "_seen_batches_per_dataloader", "[", "dataloader_idx", "]", ")", "self", ".", "_seen_batches_per_dataloader", "[", "dataloader_idx", "]", "+", "=", "1", "if", "not", "self", ".", "batch_progress", ".", "is_last_batch", "and", "trainer", ".", "received_sigterm", ":", "raise", "SIGTERMException"], "docstring": "Runs the actual evaluation step together with all the necessary bookkeeping and the hooks tied to it.", "docstring_tokens": ["runs", "the", "actual", "evaluation", "step", "together", "with", "all", "the", "necessary", "bookkeeping", "and", "the", "hooks", "tied", "to", "it"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 395, "end_line": 470, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_693", "original_string": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            batch: the current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "language": "python", "code": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            batch: the current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "Optional", "[", "int", "]", ")", "-", ">", "OrderedDict", ":", "STRING", "step_kwargs", "=", "OrderedDict", "(", "[", "(", "STRING", ",", "batch", ")", ",", "(", "STRING", ",", "batch_idx", ")", "]", ")", "if", "dataloader_idx", "is", "not", "None", ":", "step_kwargs", "[", "STRING", "]", "=", "dataloader_idx", "return", "step_kwargs"], "docstring": "Helper method to build the arguments for the current step.", "docstring_tokens": ["helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 472, "end_line": 488, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "function_694", "original_string": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `test_step` or `validation_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "language": "python", "code": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `test_step` or `validation_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "code_tokens": ["def", "_build_step_args_from_hook_kwargs", "(", "self", ",", "hook_kwargs", ":", "OrderedDict", ",", "step_hook_name", ":", "str", ")", "-", ">", "tuple", ":", "STRING", "kwargs", "=", "hook_kwargs", ".", "copy", "(", ")", "step_hook_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "step_hook_name", ")", "if", "not", "is_param_in_hook_signature", "(", "step_hook_fx", ",", "STRING", ",", "min_args", "=", "2", ")", ":", "kwargs", ".", "pop", "(", "STRING", ",", "None", ")", "return", "tuple", "(", "kwargs", ".", "values", "(", ")", ")"], "docstring": "Helper method to build args for `test_step` or `validation_step`.", "docstring_tokens": ["helper", "method", "to", "build", "args", "for", "test_step", "or", "validation_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "start_line": 490, "end_line": 496, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_695", "original_string": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        return self.epoch_loop.total_batch_idx", "language": "python", "code": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        return self.epoch_loop.total_batch_idx", "code_tokens": ["def", "total_batch_idx", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "total_batch_idx"], "docstring": "Returns the current batch index (across epochs)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "across", "epochs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 113, "end_line": 115, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_696", "original_string": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        return self.epoch_loop.batch_idx", "language": "python", "code": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        return self.epoch_loop.batch_idx", "code_tokens": ["def", "batch_idx", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "batch_idx"], "docstring": "Returns the current batch index (within this epoch)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "within", "this", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 118, "end_line": 120, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_697", "original_string": "def min_steps(self) -> Optional[int]:\r\n        \"\"\"Returns the minimum number of steps to run.\"\"\"\r\n        return self.epoch_loop.min_steps", "language": "python", "code": "def min_steps(self) -> Optional[int]:\r\n        \"\"\"Returns the minimum number of steps to run.\"\"\"\r\n        return self.epoch_loop.min_steps", "code_tokens": ["def", "min_steps", "(", "self", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "min_steps"], "docstring": "Returns the minimum number of steps to run.", "docstring_tokens": ["returns", "the", "minimum", "number", "of", "steps", "to", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 123, "end_line": 125, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_698", "original_string": "def max_steps(self) -> int:\r\n        \"\"\"Returns the maximum number of steps to run.\"\"\"\r\n        return self.epoch_loop.max_steps", "language": "python", "code": "def max_steps(self) -> int:\r\n        \"\"\"Returns the maximum number of steps to run.\"\"\"\r\n        return self.epoch_loop.max_steps", "code_tokens": ["def", "max_steps", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "max_steps"], "docstring": "Returns the maximum number of steps to run.", "docstring_tokens": ["returns", "the", "maximum", "number", "of", "steps", "to", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 128, "end_line": 130, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_699", "original_string": "def _skip_backward(self) -> bool:\r\n        \"\"\"Determines whether the loop will skip backward during automatic optimization.\"\"\"\r\n        return self.epoch_loop.automatic_optimization._skip_backward", "language": "python", "code": "def _skip_backward(self) -> bool:\r\n        \"\"\"Determines whether the loop will skip backward during automatic optimization.\"\"\"\r\n        return self.epoch_loop.automatic_optimization._skip_backward", "code_tokens": ["def", "_skip_backward", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "automatic_optimization", ".", "_skip_backward"], "docstring": "Determines whether the loop will skip backward during automatic optimization.", "docstring_tokens": ["determines", "whether", "the", "loop", "will", "skip", "backward", "during", "automatic", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 142, "end_line": 144, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_700", "original_string": "def _skip_backward(self, value: bool) -> None:\r\n        \"\"\"Determines whether the loop will skip backward during automatic optimization.\"\"\"\r\n        self.epoch_loop.automatic_optimization._skip_backward = value", "language": "python", "code": "def _skip_backward(self, value: bool) -> None:\r\n        \"\"\"Determines whether the loop will skip backward during automatic optimization.\"\"\"\r\n        self.epoch_loop.automatic_optimization._skip_backward = value", "code_tokens": ["def", "_skip_backward", "(", "self", ",", "value", ":", "bool", ")", "-", ">", "None", ":", "STRING", "self", ".", "epoch_loop", ".", "automatic_optimization", ".", "_skip_backward", "=", "value"], "docstring": "Determines whether the loop will skip backward during automatic optimization.", "docstring_tokens": ["determines", "whether", "the", "loop", "will", "skip", "backward", "during", "automatic", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 147, "end_line": 149, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_701", "original_string": "def _should_reload_train_dl(self) -> bool:\r\n        \"\"\"Check if train dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return n_epochs and self.trainer.current_epoch - self._last_train_dl_reload_epoch >= n_epochs", "language": "python", "code": "def _should_reload_train_dl(self) -> bool:\r\n        \"\"\"Check if train dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return n_epochs and self.trainer.current_epoch - self._last_train_dl_reload_epoch >= n_epochs", "code_tokens": ["def", "_should_reload_train_dl", "(", "self", ")", "-", ">", "bool", ":", "STRING", "n_epochs", "=", "self", ".", "trainer", ".", "reload_dataloaders_every_n_epochs", "return", "n_epochs", "and", "self", ".", "trainer", ".", "current_epoch", "-", "self", ".", "_last_train_dl_reload_epoch", ">", "=", "n_epochs"], "docstring": "Check if train dataloader should be reloaded.", "docstring_tokens": ["check", "if", "train", "dataloader", "should", "be", "reloaded"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 166, "end_line": 169, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_702", "original_string": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self.max_batches == 0:\r\n            rank_zero_info(\"`Trainer.fit` stopped: No training batches.\")\r\n            return True\r\n\r\n        stop_steps = _is_max_limit_reached(self.epoch_loop.global_step, self.max_steps)\r\n        if stop_steps:\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_steps={self.max_steps!r}` reached.\")\r\n            return True\r\n\r\n        assert isinstance(self.max_epochs, int)\r\n        stop_epochs = _is_max_limit_reached(self.epoch_progress.current.processed, self.max_epochs)\r\n        if stop_epochs:\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_epochs={self.max_epochs!r}` reached.\")\r\n            return True\r\n\r\n        if self.trainer.should_stop and self._can_stop_early:\r\n            rank_zero_debug(\"`Trainer.fit` stopped: `trainer.should_stop` was set.\")\r\n            return True\r\n\r\n        return False", "language": "python", "code": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self.max_batches == 0:\r\n            rank_zero_info(\"`Trainer.fit` stopped: No training batches.\")\r\n            return True\r\n\r\n        stop_steps = _is_max_limit_reached(self.epoch_loop.global_step, self.max_steps)\r\n        if stop_steps:\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_steps={self.max_steps!r}` reached.\")\r\n            return True\r\n\r\n        assert isinstance(self.max_epochs, int)\r\n        stop_epochs = _is_max_limit_reached(self.epoch_progress.current.processed, self.max_epochs)\r\n        if stop_epochs:\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_epochs={self.max_epochs!r}` reached.\")\r\n            return True\r\n\r\n        if self.trainer.should_stop and self._can_stop_early:\r\n            rank_zero_debug(\"`Trainer.fit` stopped: `trainer.should_stop` was set.\")\r\n            return True\r\n\r\n        return False", "code_tokens": ["def", "done", "(", "self", ")", "-", ">", "bool", ":", "STRING", "if", "self", ".", "max_batches", "=", "=", "0", ":", "rank_zero_info", "(", "STRING", ")", "return", "True", "stop_steps", "=", "_is_max_limit_reached", "(", "self", ".", "epoch_loop", ".", "global_step", ",", "self", ".", "max_steps", ")", "if", "stop_steps", ":", "rank_zero_info", "(", "fSTRING", ")", "return", "True", "assert", "isinstance", "(", "self", ".", "max_epochs", ",", "int", ")", "stop_epochs", "=", "_is_max_limit_reached", "(", "self", ".", "epoch_progress", ".", "current", ".", "processed", ",", "self", ".", "max_epochs", ")", "if", "stop_epochs", ":", "self", ".", "epoch_progress", ".", "current", ".", "completed", "=", "self", ".", "epoch_progress", ".", "current", ".", "processed", "rank_zero_info", "(", "fSTRING", ")", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", "and", "self", ".", "_can_stop_early", ":", "rank_zero_debug", "(", "STRING", ")", "return", "True", "return", "False"], "docstring": "Evaluates when to leave the loop.", "docstring_tokens": ["evaluates", "when", "to", "leave", "the", "loop"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 172, "end_line": 198, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_703", "original_string": "def skip(self) -> bool:\r\n        \"\"\"Whether we should skip the training and immediately return from the call to :meth:`run`.\"\"\"\r\n        return self.done or self.trainer.limit_train_batches == 0", "language": "python", "code": "def skip(self) -> bool:\r\n        \"\"\"Whether we should skip the training and immediately return from the call to :meth:`run`.\"\"\"\r\n        return self.done or self.trainer.limit_train_batches == 0", "code_tokens": ["def", "skip", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "done", "or", "self", ".", "trainer", ".", "limit_train_batches", "=", "=", "0"], "docstring": "Whether we should skip the training and immediately return from the call to :meth:`run`.", "docstring_tokens": ["whether", "we", "should", "skip", "the", "training", "and", "immediately", "return", "from", "the", "call", "to", "meth", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 201, "end_line": 205, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_704", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of this loop.\"\"\"\r\n        assert self.trainer.model is not None\r\n        torch.set_grad_enabled(True)\r\n\r\n        self.update_restart_stage()\r\n\r\n        if self.restarted_on_epoch_start:\r\n            self.epoch_progress.reset_on_restart()\r\n\r\n        if self.resumed_on_epoch_end:\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.restarted_mid_epoch\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_processed()\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n            and not self.restarted_mid_epoch\r\n            and not self.epoch_loop.val_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_completed()", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of this loop.\"\"\"\r\n        assert self.trainer.model is not None\r\n        torch.set_grad_enabled(True)\r\n\r\n        self.update_restart_stage()\r\n\r\n        if self.restarted_on_epoch_start:\r\n            self.epoch_progress.reset_on_restart()\r\n\r\n        if self.resumed_on_epoch_end:\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.restarted_mid_epoch\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_processed()\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n            and not self.restarted_mid_epoch\r\n            and not self.epoch_loop.val_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_completed()", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "trainer", ".", "model", "is", "not", "None", "torch", ".", "set_grad_enabled", "(", "True", ")", "self", ".", "update_restart_stage", "(", ")", "if", "self", ".", "restarted_on_epoch_start", ":", "self", ".", "epoch_progress", ".", "reset_on_restart", "(", ")", "if", "self", ".", "resumed_on_epoch_end", ":", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")", "if", "(", "self", ".", "epoch_loop", ".", "restarted_on_train_batch_end", "and", "self", ".", "restarted_mid_epoch", "and", "self", ".", "epoch_loop", ".", "batch_progress", ".", "is_last_batch", ")", ":", "self", ".", "epoch_progress", ".", "increment_processed", "(", ")", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")", "if", "(", "self", ".", "epoch_loop", ".", "restarted_on_train_batch_end", "and", "self", ".", "epoch_loop", ".", "batch_progress", ".", "is_last_batch", "and", "not", "self", ".", "restarted_mid_epoch", "and", "not", "self", ".", "epoch_loop", ".", "val_loop", ".", "batch_progress", ".", "is_last_batch", ")", ":", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")"], "docstring": "Resets the internal state of this loop.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "this", "loop"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 378, "end_line": 407, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_705", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Calls the ``on_train_start`` hook.\"\"\"\r\n        if not self._iteration_based_training():\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n\r\n        trainer = self.trainer\r\n\r\n        if self.epoch_loop._should_check_val_epoch() and trainer.val_dataloaders is None:\r\n            trainer.validating = True\r\n            self.epoch_loop.val_loop.setup_data()\r\n            trainer.training = True\r\n\r\n        self._warn_if_modules_in_eval_mode()\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_start\")\r\n        call._call_strategy_hook(trainer, \"on_train_start\")", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Calls the ``on_train_start`` hook.\"\"\"\r\n        if not self._iteration_based_training():\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n\r\n        trainer = self.trainer\r\n\r\n        if self.epoch_loop._should_check_val_epoch() and trainer.val_dataloaders is None:\r\n            trainer.validating = True\r\n            self.epoch_loop.val_loop.setup_data()\r\n            trainer.training = True\r\n\r\n        self._warn_if_modules_in_eval_mode()\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_start\")\r\n        call._call_strategy_hook(trainer, \"on_train_start\")", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_iteration_based_training", "(", ")", ":", "self", ".", "epoch_progress", ".", "current", ".", "completed", "=", "self", ".", "epoch_progress", ".", "current", ".", "processed", "trainer", "=", "self", ".", "trainer", "if", "self", ".", "epoch_loop", ".", "_should_check_val_epoch", "(", ")", "and", "trainer", ".", "val_dataloaders", "is", "None", ":", "trainer", ".", "validating", "=", "True", "self", ".", "epoch_loop", ".", "val_loop", ".", "setup_data", "(", ")", "trainer", ".", "training", "=", "True", "self", ".", "_warn_if_modules_in_eval_mode", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ")"], "docstring": "Calls the ``on_train_start`` hook.", "docstring_tokens": ["calls", "the", "on_train_start", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 409, "end_line": 428, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_706", "original_string": "def on_advance_start(self) -> None:\r\n        \"\"\"Prepares the dataloader for training and calls the hook ``on_train_epoch_start``\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self.setup_data()\r\n\r\n        assert self._combined_loader is not None\r\n        for i, dl in enumerate(self._combined_loader.flattened):\r\n            _set_sampler_epoch(dl, self.epoch_progress.current.processed)\r\n\r\n        if not self.restarted_mid_epoch and not self.restarted_on_epoch_end:\r\n            if not self.restarted_on_epoch_start:\r\n                self.epoch_progress.increment_ready()\r\n\r\n            call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n            call._call_lightning_module_hook(trainer, \"on_train_epoch_start\")\r\n\r\n            self.epoch_progress.increment_started()", "language": "python", "code": "def on_advance_start(self) -> None:\r\n        \"\"\"Prepares the dataloader for training and calls the hook ``on_train_epoch_start``\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self.setup_data()\r\n\r\n        assert self._combined_loader is not None\r\n        for i, dl in enumerate(self._combined_loader.flattened):\r\n            _set_sampler_epoch(dl, self.epoch_progress.current.processed)\r\n\r\n        if not self.restarted_mid_epoch and not self.restarted_on_epoch_end:\r\n            if not self.restarted_on_epoch_start:\r\n                self.epoch_progress.increment_ready()\r\n\r\n            call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n            call._call_lightning_module_hook(trainer, \"on_train_epoch_start\")\r\n\r\n            self.epoch_progress.increment_started()", "code_tokens": ["def", "on_advance_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "self", ".", "setup_data", "(", ")", "assert", "self", ".", "_combined_loader", "is", "not", "None", "for", "i", ",", "dl", "in", "enumerate", "(", "self", ".", "_combined_loader", ".", "flattened", ")", ":", "_set_sampler_epoch", "(", "dl", ",", "self", ".", "epoch_progress", ".", "current", ".", "processed", ")", "if", "not", "self", ".", "restarted_mid_epoch", "and", "not", "self", ".", "restarted_on_epoch_end", ":", "if", "not", "self", ".", "restarted_on_epoch_start", ":", "self", ".", "epoch_progress", ".", "increment_ready", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "self", ".", "epoch_progress", ".", "increment_started", "(", ")"], "docstring": "Prepares the dataloader for training and calls the hook ``on_train_epoch_start``", "docstring_tokens": ["prepares", "the", "dataloader", "for", "training", "and", "calls", "the", "hook", "on_train_epoch_start"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 430, "end_line": 449, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_707", "original_string": "def advance(self) -> None:\r\n        \"\"\"Runs one whole epoch.\"\"\"\r\n        log.debug(f\"{type(self).__name__}: advancing loop\")\r\n\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode == \"sequential\":\r\n            raise ValueError(\r\n                f'`{type(self).__name__}` does not support the `CombinedLoader(mode=\"sequential\")` mode.'\r\n                f\" The available modes are: {[m for m in _SUPPORTED_MODES if m != 'sequential']}\"\r\n            )\r\n        with self.trainer.profiler.profile(\"run_training_epoch\"):\r\n            assert self._data_fetcher is not None\r\n            self.epoch_loop.run(self._data_fetcher)", "language": "python", "code": "def advance(self) -> None:\r\n        \"\"\"Runs one whole epoch.\"\"\"\r\n        log.debug(f\"{type(self).__name__}: advancing loop\")\r\n\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode == \"sequential\":\r\n            raise ValueError(\r\n                f'`{type(self).__name__}` does not support the `CombinedLoader(mode=\"sequential\")` mode.'\r\n                f\" The available modes are: {[m for m in _SUPPORTED_MODES if m != 'sequential']}\"\r\n            )\r\n        with self.trainer.profiler.profile(\"run_training_epoch\"):\r\n            assert self._data_fetcher is not None\r\n            self.epoch_loop.run(self._data_fetcher)", "code_tokens": ["def", "advance", "(", "self", ")", "-", ">", "None", ":", "STRING", "log", ".", "debug", "(", "fSTRING", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "combined_loader", ".", "_mode", "=", "=", "STRING", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "with", "self", ".", "trainer", ".", "profiler", ".", "profile", "(", "STRING", ")", ":", "assert", "self", ".", "_data_fetcher", "is", "not", "None", "self", ".", "epoch_loop", ".", "run", "(", "self", ".", "_data_fetcher", ")"], "docstring": "Runs one whole epoch.", "docstring_tokens": ["runs", "one", "whole", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 451, "end_line": 464, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_708", "original_string": "def on_run_end(self) -> None:\r\n        \"\"\"Calls the ``on_train_end`` hook.\"\"\"\r\n        log.debug(f\"{self.__class__.__name__}: train run ended\")\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_train_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_end\")\r\n        call._call_strategy_hook(trainer, \"on_train_end\")", "language": "python", "code": "def on_run_end(self) -> None:\r\n        \"\"\"Calls the ``on_train_end`` hook.\"\"\"\r\n        log.debug(f\"{self.__class__.__name__}: train run ended\")\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_train_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_end\")\r\n        call._call_strategy_hook(trainer, \"on_train_end\")", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "log", ".", "debug", "(", "fSTRING", ")", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ")"], "docstring": "Calls the ``on_train_end`` hook.", "docstring_tokens": ["calls", "the", "on_train_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 500, "end_line": 507, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_709", "original_string": "def _warn_if_modules_in_eval_mode(self) -> None:\r\n        \"\"\"Warn if any modules are in eval mode at the start of training.\"\"\"\r\n        model = self.trainer.lightning_module\r\n        eval_modules = [name for name, module in model.named_modules() if not module.training]\r\n\r\n        if eval_modules:\r\n            rank_zero_warn(\r\n                f\"Found {len(eval_modules)} module(s) in eval mode at the start of training.\"\r\n                \" This may lead to unexpected behavior during training. If this is intentional,\"\r\n                \" you can ignore this warning.\",\r\n                category=PossibleUserWarning,\r\n            )", "language": "python", "code": "def _warn_if_modules_in_eval_mode(self) -> None:\r\n        \"\"\"Warn if any modules are in eval mode at the start of training.\"\"\"\r\n        model = self.trainer.lightning_module\r\n        eval_modules = [name for name, module in model.named_modules() if not module.training]\r\n\r\n        if eval_modules:\r\n            rank_zero_warn(\r\n                f\"Found {len(eval_modules)} module(s) in eval mode at the start of training.\"\r\n                \" This may lead to unexpected behavior during training. If this is intentional,\"\r\n                \" you can ignore this warning.\",\r\n                category=PossibleUserWarning,\r\n            )", "code_tokens": ["def", "_warn_if_modules_in_eval_mode", "(", "self", ")", "-", ">", "None", ":", "STRING", "model", "=", "self", ".", "trainer", ".", "lightning_module", "eval_modules", "=", "[", "name", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", "if", "not", "module", ".", "training", "]", "if", "eval_modules", ":", "rank_zero_warn", "(", "fSTRING", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Warn if any modules are in eval mode at the start of training.", "docstring_tokens": ["warn", "if", "any", "modules", "are", "in", "eval", "mode", "at", "the", "start", "of", "training"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 527, "end_line": 538, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "function_710", "original_string": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Whether the gradients should be accumulated.\"\"\"\r\n        return self.epoch_loop._should_accumulate()", "language": "python", "code": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Whether the gradients should be accumulated.\"\"\"\r\n        return self.epoch_loop._should_accumulate()", "code_tokens": ["def", "_should_accumulate", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "epoch_loop", ".", "_should_accumulate", "(", ")"], "docstring": "Whether the gradients should be accumulated.", "docstring_tokens": ["whether", "the", "gradients", "should", "be", "accumulated"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "start_line": 540, "end_line": 542, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_711", "original_string": "def restarting(self) -> bool:\r\n        \"\"\"Whether the state of this loop was reloaded and it needs to restart.\"\"\"\r\n        return self._restarting", "language": "python", "code": "def restarting(self) -> bool:\r\n        \"\"\"Whether the state of this loop was reloaded and it needs to restart.\"\"\"\r\n        return self._restarting", "code_tokens": ["def", "restarting", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_restarting"], "docstring": "Whether the state of this loop was reloaded and it needs to restart.", "docstring_tokens": ["whether", "the", "state", "of", "this", "loop", "was", "reloaded", "and", "it", "needs", "to", "restart"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 29, "end_line": 31, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_712", "original_string": "def restarting(self, restarting: bool) -> None:\r\n        \"\"\"Connects this loop's restarting value and its children.\"\"\"\r\n        self._restarting = restarting\r\n        for loop in vars(self).values():\r\n            if isinstance(loop, _Loop):\r\n                loop.restarting = restarting", "language": "python", "code": "def restarting(self, restarting: bool) -> None:\r\n        \"\"\"Connects this loop's restarting value and its children.\"\"\"\r\n        self._restarting = restarting\r\n        for loop in vars(self).values():\r\n            if isinstance(loop, _Loop):\r\n                loop.restarting = restarting", "code_tokens": ["def", "restarting", "(", "self", ",", "restarting", ":", "bool", ")", "-", ">", "None", ":", "STRING", "self", ".", "_restarting", "=", "restarting", "for", "loop", "in", "vars", "(", "self", ")", ".", "values", "(", ")", ":", "if", "isinstance", "(", "loop", ",", "_Loop", ")", ":", "loop", ".", "restarting", "=", "restarting"], "docstring": "Connects this loop's restarting value and its children.", "docstring_tokens": ["connects", "this", "loop", "s", "restarting", "value", "and", "its", "children"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 34, "end_line": 39, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_713", "original_string": "def is_resuming(self) -> bool:\r\n        \"\"\"Indicates whether training is being resumed from a checkpoint.\"\"\"\r\n        return self._resuming_from_checkpoint", "language": "python", "code": "def is_resuming(self) -> bool:\r\n        \"\"\"Indicates whether training is being resumed from a checkpoint.\"\"\"\r\n        return self._resuming_from_checkpoint", "code_tokens": ["def", "is_resuming", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_resuming_from_checkpoint"], "docstring": "Indicates whether training is being resumed from a checkpoint.", "docstring_tokens": ["indicates", "whether", "training", "is", "being", "resumed", "from", "a", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 42, "end_line": 44, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_714", "original_string": "def on_save_checkpoint(self) -> dict:\r\n        \"\"\"Called when saving a model checkpoint, use to persist loop state.\r\n\r\n        Returns:\r\n            The current loop state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def on_save_checkpoint(self) -> dict:\r\n        \"\"\"Called when saving a model checkpoint, use to persist loop state.\r\n\r\n        Returns:\r\n            The current loop state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ")", "-", ">", "dict", ":", "STRING", "return", "{", "}"], "docstring": "Called when saving a model checkpoint, use to persist loop state.", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "use", "to", "persist", "loop", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 49, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_715", "original_string": "def on_load_checkpoint(self, state_dict: dict) -> None:\r\n        \"\"\"Called when loading a model checkpoint, use to reload loop state.\"\"\"", "language": "python", "code": "def on_load_checkpoint(self, state_dict: dict) -> None:\r\n        \"\"\"Called when loading a model checkpoint, use to reload loop state.\"\"\"", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "state_dict", ":", "dict", ")", "-", ">", "None", ":", "STRING"], "docstring": "Called when loading a model checkpoint, use to reload loop state.", "docstring_tokens": ["called", "when", "loading", "a", "model", "checkpoint", "use", "to", "reload", "loop", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 58, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_716", "original_string": "def state_dict(self, destination: Optional[dict] = None, prefix: str = \"\") -> dict:\r\n        \"\"\"The state dict is determined by the state and progress of this loop and all its children.\r\n\r\n        Args:\r\n            destination: An existing dictionary to update with this loop's state. By default a new dictionary\r\n                is returned.\r\n            prefix: A prefix for each key in the state dictionary\r\n\r\n        \"\"\"\r\n        if destination is None:\r\n            destination = {}\r\n\r\n        destination[prefix + \"state_dict\"] = self.on_save_checkpoint()\r\n\r\n        for k, v in self.__dict__.items():\r\n            key = prefix + k\r\n            if isinstance(v, _BaseProgress):\r\n                destination[key] = v.state_dict()\r\n            elif isinstance(v, _Loop):\r\n                v.state_dict(destination, key + \".\")\r\n        return destination", "language": "python", "code": "def state_dict(self, destination: Optional[dict] = None, prefix: str = \"\") -> dict:\r\n        \"\"\"The state dict is determined by the state and progress of this loop and all its children.\r\n\r\n        Args:\r\n            destination: An existing dictionary to update with this loop's state. By default a new dictionary\r\n                is returned.\r\n            prefix: A prefix for each key in the state dictionary\r\n\r\n        \"\"\"\r\n        if destination is None:\r\n            destination = {}\r\n\r\n        destination[prefix + \"state_dict\"] = self.on_save_checkpoint()\r\n\r\n        for k, v in self.__dict__.items():\r\n            key = prefix + k\r\n            if isinstance(v, _BaseProgress):\r\n                destination[key] = v.state_dict()\r\n            elif isinstance(v, _Loop):\r\n                v.state_dict(destination, key + \".\")\r\n        return destination", "code_tokens": ["def", "state_dict", "(", "self", ",", "destination", ":", "Optional", "[", "dict", "]", "=", "None", ",", "prefix", ":", "str", "=", "STRING", ")", "-", ">", "dict", ":", "STRING", "if", "destination", "is", "None", ":", "destination", "=", "{", "}", "destination", "[", "prefix", "+", "STRING", "]", "=", "self", ".", "on_save_checkpoint", "(", ")", "for", "k", ",", "v", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "key", "=", "prefix", "+", "k", "if", "isinstance", "(", "v", ",", "_BaseProgress", ")", ":", "destination", "[", "key", "]", "=", "v", ".", "state_dict", "(", ")", "elif", "isinstance", "(", "v", ",", "_Loop", ")", ":", "v", ".", "state_dict", "(", "destination", ",", "key", "+", "STRING", ")", "return", "destination"], "docstring": "The state dict is determined by the state and progress of this loop and all its children.", "docstring_tokens": ["the", "state", "dict", "is", "determined", "by", "the", "state", "and", "progress", "of", "this", "loop", "and", "all", "its", "children"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 61, "end_line": 81, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "function_717", "original_string": "def load_state_dict(\r\n        self,\r\n        state_dict: dict,\r\n        prefix: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Loads the state of this loop and all its children.\"\"\"\r\n        self._load_from_state_dict(state_dict.copy(), prefix)\r\n        for k, v in self.__dict__.items():\r\n            if isinstance(v, _Loop):\r\n                v.load_state_dict(state_dict.copy(), prefix + k + \".\")\r\n        self.restarting = True\r\n        self._loaded_from_state_dict = True\r\n        self._resuming_from_checkpoint = True", "language": "python", "code": "def load_state_dict(\r\n        self,\r\n        state_dict: dict,\r\n        prefix: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Loads the state of this loop and all its children.\"\"\"\r\n        self._load_from_state_dict(state_dict.copy(), prefix)\r\n        for k, v in self.__dict__.items():\r\n            if isinstance(v, _Loop):\r\n                v.load_state_dict(state_dict.copy(), prefix + k + \".\")\r\n        self.restarting = True\r\n        self._loaded_from_state_dict = True\r\n        self._resuming_from_checkpoint = True", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", ",", "prefix", ":", "str", "=", "STRING", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "_load_from_state_dict", "(", "state_dict", ".", "copy", "(", ")", ",", "prefix", ")", "for", "k", ",", "v", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "if", "isinstance", "(", "v", ",", "_Loop", ")", ":", "v", ".", "load_state_dict", "(", "state_dict", ".", "copy", "(", ")", ",", "prefix", "+", "k", "+", "STRING", ")", "self", ".", "restarting", "=", "True", "self", ".", "_loaded_from_state_dict", "=", "True", "self", ".", "_resuming_from_checkpoint", "=", "True"], "docstring": "Loads the state of this loop and all its children.", "docstring_tokens": ["loads", "the", "state", "of", "this", "loop", "and", "all", "its", "children"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\loop.py", "start_line": 83, "end_line": 95, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_718", "original_string": "def return_predictions(self) -> bool:\r\n        \"\"\"Whether to return the predictions or not.\"\"\"\r\n        return self._return_predictions", "language": "python", "code": "def return_predictions(self) -> bool:\r\n        \"\"\"Whether to return the predictions or not.\"\"\"\r\n        return self._return_predictions", "code_tokens": ["def", "return_predictions", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_return_predictions"], "docstring": "Whether to return the predictions or not.", "docstring_tokens": ["whether", "to", "return", "the", "predictions", "or", "not"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 68, "end_line": 70, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_719", "original_string": "def predictions(self) -> list[Any]:\r\n        \"\"\"The cached predictions.\"\"\"\r\n        if self._predictions == []:\r\n            return self._predictions\r\n        return self._predictions[0] if self.num_dataloaders == 1 else self._predictions", "language": "python", "code": "def predictions(self) -> list[Any]:\r\n        \"\"\"The cached predictions.\"\"\"\r\n        if self._predictions == []:\r\n            return self._predictions\r\n        return self._predictions[0] if self.num_dataloaders == 1 else self._predictions", "code_tokens": ["def", "predictions", "(", "self", ")", "-", ">", "list", "[", "Any", "]", ":", "STRING", "if", "self", ".", "_predictions", "=", "=", "[", "]", ":", "return", "self", ".", "_predictions", "return", "self", ".", "_predictions", "[", "0", "]", "if", "self", ".", "num_dataloaders", "=", "=", "1", "else", "self", ".", "_predictions"], "docstring": "The cached predictions.", "docstring_tokens": ["the", "cached", "predictions"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 85, "end_line": 89, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_720", "original_string": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "language": "python", "code": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "code_tokens": ["def", "num_dataloaders", "(", "self", ")", "-", ">", "int", ":", "STRING", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "return", "len", "(", "combined_loader", ".", "flattened", ")"], "docstring": "Returns the number of prediction dataloaders.", "docstring_tokens": ["returns", "the", "number", "of", "prediction", "dataloaders"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 92, "end_line": 96, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_721", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        self.batch_progress.reset_on_run()\r\n\r\n        assert self.trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(self.trainer, self.trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode != \"sequential\":\r\n            raise ValueError('`trainer.predict()` only supports the `CombinedLoader(mode=\"sequential\")` mode.')\r\n\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher\r\n\r\n        num_dataloaders = self.num_dataloaders\r\n        self.epoch_batch_indices = [[] for _ in range(num_dataloaders)]\r\n        self._predictions = [[] for _ in range(num_dataloaders)]", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        self.batch_progress.reset_on_run()\r\n\r\n        assert self.trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(self.trainer, self.trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode != \"sequential\":\r\n            raise ValueError('`trainer.predict()` only supports the `CombinedLoader(mode=\"sequential\")` mode.')\r\n\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher\r\n\r\n        num_dataloaders = self.num_dataloaders\r\n        self.epoch_batch_indices = [[] for _ in range(num_dataloaders)]\r\n        self._predictions = [[] for _ in range(num_dataloaders)]", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "assert", "self", ".", "trainer", ".", "state", ".", "stage", "is", "not", "None", "data_fetcher", "=", "_select_data_fetcher", "(", "self", ".", "trainer", ",", "self", ".", "trainer", ".", "state", ".", "stage", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "combined_loader", ".", "_mode", "!", "=", "STRING", ":", "raise", "ValueError", "(", "STRING", ")", "combined_loader", ".", "limits", "=", "self", ".", "max_batches", "data_fetcher", ".", "setup", "(", "combined_loader", ")", "iter", "(", "data_fetcher", ")", "#", "creates", "the", "iterator", "inside", "the", "fetcher", "data_fetcher", ".", "fetched", "+", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "data_fetcher", ".", "_start_profiler", "=", "self", ".", "_on_before_fetch", "data_fetcher", ".", "_stop_profiler", "=", "self", ".", "_on_after_fetch", "self", ".", "_data_fetcher", "=", "data_fetcher", "num_dataloaders", "=", "self", ".", "num_dataloaders", "self", ".", "epoch_batch_indices", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_dataloaders", ")", "]", "self", ".", "_predictions", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_dataloaders", ")", "]"], "docstring": "Resets the internal state of the loop for a new run.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 167, "end_line": 191, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_722", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_predict_model_eval()\r\n        self._on_predict_start()\r\n        self._on_predict_epoch_start()", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_predict_model_eval()\r\n        self._on_predict_start()\r\n        self._on_predict_epoch_start()", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "_verify_dataloader_idx_requirement", "(", ")", "self", ".", "_on_predict_model_eval", "(", ")", "self", ".", "_on_predict_start", "(", ")", "self", ".", "_on_predict_epoch_start", "(", ")"], "docstring": "Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.", "docstring_tokens": ["calls", "_on_predict_model_eval", "_on_predict_start", "and", "_on_predict_epoch_start", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 193, "end_line": 198, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_723", "original_string": "def on_run_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.\"\"\"\r\n        results = self._on_predict_epoch_end()\r\n        self._on_predict_end()\r\n        self._on_predict_model_train()\r\n        return results", "language": "python", "code": "def on_run_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.\"\"\"\r\n        results = self._on_predict_epoch_end()\r\n        self._on_predict_end()\r\n        self._on_predict_model_train()\r\n        return results", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "STRING", "results", "=", "self", ".", "_on_predict_epoch_end", "(", ")", "self", ".", "_on_predict_end", "(", ")", "self", ".", "_on_predict_model_train", "(", ")", "return", "results"], "docstring": "Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.", "docstring_tokens": ["calls", "on_predict_epoch_end", "and", "on_predict_end", "hooks", "and", "returns", "results", "from", "all", "dataloaders"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 200, "end_line": 205, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_724", "original_string": "def _predict_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        any_on_epoch = (\r\n            self._store_data_for_prediction_writer(batch_idx, dataloader_idx) if not using_dataloader_iter else False\r\n        )\r\n\r\n        hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, \"predict_step\")\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        predictions = call._call_strategy_hook(trainer, \"predict_step\", *step_args)\r\n        if predictions is None:\r\n            self._warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if self._return_predictions or any_on_epoch:\r\n            self._predictions[dataloader_idx].append(move_data_to_device(predictions, torch.device(\"cpu\")))", "language": "python", "code": "def _predict_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        any_on_epoch = (\r\n            self._store_data_for_prediction_writer(batch_idx, dataloader_idx) if not using_dataloader_iter else False\r\n        )\r\n\r\n        hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, \"predict_step\")\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        predictions = call._call_strategy_hook(trainer, \"predict_step\", *step_args)\r\n        if predictions is None:\r\n            self._warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if self._return_predictions or any_on_epoch:\r\n            self._predictions[dataloader_idx].append(move_data_to_device(predictions, torch.device(\"cpu\")))", "code_tokens": ["def", "_predict_step", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", ",", "dataloader_iter", ":", "Optional", "[", "Iterator", "]", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "data_fetcher", "=", "self", ".", "_data_fetcher", "assert", "data_fetcher", "is", "not", "None", "if", "not", "(", "using_dataloader_iter", ":", "=", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ")", ":", "batch", "=", "trainer", ".", "precision_plugin", ".", "convert_input", "(", "batch", ")", "batch", "=", "trainer", ".", "lightning_module", ".", "_on_before_batch_transfer", "(", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "batch", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "self", ".", "batch_progress", ".", "increment_ready", "(", ")", "any_on_epoch", "=", "(", "self", ".", "_store_data_for_prediction_writer", "(", "batch_idx", ",", "dataloader_idx", ")", "if", "not", "using_dataloader_iter", "else", "False", ")", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "self", ".", "batch_progress", ".", "increment_started", "(", ")", "step_args", "=", "(", "self", ".", "_build_step_args_from_hook_kwargs", "(", "hook_kwargs", ",", "STRING", ")", "if", "not", "using_dataloader_iter", "else", "(", "dataloader_iter", ",", ")", ")", "predictions", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "*", "step_args", ")", "if", "predictions", "is", "None", ":", "self", ".", "_warning_cache", ".", "warn", "(", "STRING", ")", "self", ".", "batch_progress", ".", "increment_processed", "(", ")", "if", "using_dataloader_iter", ":", "batch", "=", "data_fetcher", ".", "_batch", "batch_idx", "=", "data_fetcher", ".", "_batch_idx", "dataloader_idx", "=", "data_fetcher", ".", "_dataloader_idx", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "predictions", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "predictions", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "if", "self", ".", "_return_predictions", "or", "any_on_epoch", ":", "self", ".", "_predictions", "[", "dataloader_idx", "]", ".", "append", "(", "move_data_to_device", "(", "predictions", ",", "torch", ".", "device", "(", "STRING", ")", ")", ")"], "docstring": "Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.", "docstring_tokens": ["runs", "the", "actual", "predict", "step", "together", "with", "all", "the", "necessary", "bookkeeping", "and", "the", "hooks", "tied", "to", "it"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 212, "end_line": 273, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_725", "original_string": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Assembles the keyword arguments for the ``predict_step``\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the predict step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "language": "python", "code": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Assembles the keyword arguments for the ``predict_step``\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the predict step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "Optional", "[", "int", "]", ")", "-", ">", "OrderedDict", ":", "STRING", "step_kwargs", "=", "OrderedDict", "(", "[", "(", "STRING", ",", "batch", ")", ",", "(", "STRING", ",", "batch_idx", ")", "]", ")", "if", "dataloader_idx", "is", "not", "None", ":", "step_kwargs", "[", "STRING", "]", "=", "dataloader_idx", "return", "step_kwargs"], "docstring": "Assembles the keyword arguments for the ``predict_step``", "docstring_tokens": ["assembles", "the", "keyword", "arguments", "for", "the", "predict_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 275, "end_line": 291, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_726", "original_string": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `predict_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "language": "python", "code": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `predict_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "code_tokens": ["def", "_build_step_args_from_hook_kwargs", "(", "self", ",", "hook_kwargs", ":", "OrderedDict", ",", "step_hook_name", ":", "str", ")", "-", ">", "tuple", ":", "STRING", "kwargs", "=", "hook_kwargs", ".", "copy", "(", ")", "step_hook_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "step_hook_name", ")", "if", "not", "is_param_in_hook_signature", "(", "step_hook_fx", ",", "STRING", ",", "min_args", "=", "2", ")", ":", "kwargs", ".", "pop", "(", "STRING", ",", "None", ")", "return", "tuple", "(", "kwargs", ".", "values", "(", ")", ")"], "docstring": "Helper method to build args for `predict_step`.", "docstring_tokens": ["helper", "method", "to", "build", "args", "for", "predict_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 293, "end_line": 299, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_727", "original_string": "def _get_batch_indices(self, dataloader: object) -> list[list[int]]:  # batches x samples\r\n        \"\"\"Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\r\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.\"\"\"\r\n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\r\n        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):\r\n            self._warning_cache.warn(\r\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\r\n            )\r\n            return []\r\n        return batch_sampler.seen_batch_indices", "language": "python", "code": "def _get_batch_indices(self, dataloader: object) -> list[list[int]]:  # batches x samples\r\n        \"\"\"Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\r\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.\"\"\"\r\n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\r\n        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):\r\n            self._warning_cache.warn(\r\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\r\n            )\r\n            return []\r\n        return batch_sampler.seen_batch_indices", "code_tokens": ["def", "_get_batch_indices", "(", "self", ",", "dataloader", ":", "object", ")", "-", ">", "list", "[", "list", "[", "int", "]", "]", ":", "#", "batches", "x", "samples", "STRING", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "STRING", ",", "None", ")", "if", "not", "isinstance", "(", "batch_sampler", ",", "_IndexBatchSamplerWrapper", ")", ":", "self", ".", "_warning_cache", ".", "warn", "(", "fSTRING", ")", "return", "[", "]", "return", "batch_sampler", ".", "seen_batch_indices"], "docstring": "Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our", "docstring_tokens": ["returns", "a", "reference", "to", "the", "seen", "batch", "indices", "if", "the", "dataloader", "has", "a", "batch", "sampler", "wrapped", "by", "our"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 301, "end_line": 310, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_728", "original_string": "def _on_predict_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_start\")\r\n        call._call_strategy_hook(trainer, \"on_predict_start\")", "language": "python", "code": "def _on_predict_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_start\")\r\n        call._call_strategy_hook(trainer, \"on_predict_start\")", "code_tokens": ["def", "_on_predict_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ")"], "docstring": "Calls ``on_predict_start`` hooks.", "docstring_tokens": ["calls", "on_predict_start", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 340, "end_line": 345, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_729", "original_string": "def _on_predict_epoch_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_start\")", "language": "python", "code": "def _on_predict_epoch_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_start\")", "code_tokens": ["def", "_on_predict_epoch_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")"], "docstring": "Calls ``on_predict_epoch_start`` hooks.", "docstring_tokens": ["calls", "on_predict_epoch_start", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 354, "end_line": 358, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_730", "original_string": "def _on_predict_epoch_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` hook.\r\n\r\n        Returns:\r\n            the results for all dataloaders\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_end\")\r\n\r\n        if self.return_predictions:\r\n            return self.predictions\r\n        return None", "language": "python", "code": "def _on_predict_epoch_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` hook.\r\n\r\n        Returns:\r\n            the results for all dataloaders\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_end\")\r\n\r\n        if self.return_predictions:\r\n            return self.predictions\r\n        return None", "code_tokens": ["def", "_on_predict_epoch_end", "(", "self", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "STRING", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "if", "self", ".", "return_predictions", ":", "return", "self", ".", "predictions", "return", "None"], "docstring": "Calls ``on_predict_epoch_end`` hook.", "docstring_tokens": ["calls", "on_predict_epoch_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 360, "end_line": 373, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "function_731", "original_string": "def _on_predict_end(self) -> None:\r\n        \"\"\"Resets previous gradient status and calls ``on_predict_end`` hook.\"\"\"\r\n        if not self.return_predictions:\r\n            self._predictions = []\r\n        self.epoch_batch_indices = []\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_end\")\r\n        call._call_strategy_hook(trainer, \"on_predict_end\")", "language": "python", "code": "def _on_predict_end(self) -> None:\r\n        \"\"\"Resets previous gradient status and calls ``on_predict_end`` hook.\"\"\"\r\n        if not self.return_predictions:\r\n            self._predictions = []\r\n        self.epoch_batch_indices = []\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_end\")\r\n        call._call_strategy_hook(trainer, \"on_predict_end\")", "code_tokens": ["def", "_on_predict_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "return_predictions", ":", "self", ".", "_predictions", "=", "[", "]", "self", ".", "epoch_batch_indices", "=", "[", "]", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ")"], "docstring": "Resets previous gradient status and calls ``on_predict_end`` hook.", "docstring_tokens": ["resets", "previous", "gradient", "status", "and", "calls", "on_predict_end", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "start_line": 375, "end_line": 385, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\progress.py", "func_name": "function_732", "original_string": "def reset(self) -> None:\r\n        \"\"\"Reset the object's state.\"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Reset the object's state.\"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "raise", "NotImplementedError"], "docstring": "Reset the object's state.", "docstring_tokens": ["reset", "the", "object", "s", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\progress.py", "start_line": 34, "end_line": 36, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\progress.py", "func_name": "function_733", "original_string": "def reset(self) -> None:\r\n        \"\"\"Reset the state.\"\"\"\r\n        self.ready = 0\r\n        self.completed = 0", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Reset the state.\"\"\"\r\n        self.ready = 0\r\n        self.completed = 0", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "ready", "=", "0", "self", ".", "completed", "=", "0"], "docstring": "Reset the state.", "docstring_tokens": ["reset", "the", "state"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\progress.py", "start_line": 55, "end_line": 58, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\progress.py", "func_name": "function_734", "original_string": "def reset_on_restart(self) -> None:\r\n        \"\"\"Reset the progress on restart.\r\n\r\n        If there is a failure before all attributes are increased, restore the attributes to the last fully completed\r\n        value.\r\n\r\n        \"\"\"\r\n        self.ready = self.completed", "language": "python", "code": "def reset_on_restart(self) -> None:\r\n        \"\"\"Reset the progress on restart.\r\n\r\n        If there is a failure before all attributes are increased, restore the attributes to the last fully completed\r\n        value.\r\n\r\n        \"\"\"\r\n        self.ready = self.completed", "code_tokens": ["def", "reset_on_restart", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "ready", "=", "self", ".", "completed"], "docstring": "Reset the progress on restart.", "docstring_tokens": ["reset", "the", "progress", "on", "restart"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\progress.py", "start_line": 60, "end_line": 67, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\progress.py", "func_name": "function_735", "original_string": "def from_defaults(cls, tracker_cls: type[_ReadyCompletedTracker], **kwargs: int) -> \"_Progress\":\r\n        \"\"\"Utility function to easily create an instance from keyword arguments to both ``Tracker``s.\"\"\"\r\n        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))", "language": "python", "code": "def from_defaults(cls, tracker_cls: type[_ReadyCompletedTracker], **kwargs: int) -> \"_Progress\":\r\n        \"\"\"Utility function to easily create an instance from keyword arguments to both ``Tracker``s.\"\"\"\r\n        return cls(total=tracker_cls(**kwargs), current=tracker_cls(**kwargs))", "code_tokens": ["def", "from_defaults", "(", "cls", ",", "tracker_cls", ":", "type", "[", "_ReadyCompletedTracker", "]", ",", "*", "*", "kwargs", ":", "int", ")", "-", ">", "STRING", ":", "STRING", "return", "cls", "(", "total", "=", "tracker_cls", "(", "*", "*", "kwargs", ")", ",", "current", "=", "tracker_cls", "(", "*", "*", "kwargs", ")", ")"], "docstring": "Utility function to easily create an instance from keyword arguments to both ``Tracker``s.", "docstring_tokens": ["utility", "function", "to", "easily", "create", "an", "instance", "from", "keyword", "arguments", "to", "both", "tracker", "s"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\progress.py", "start_line": 175, "end_line": 177, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_736", "original_string": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        return self.batch_progress.total.ready - 1", "language": "python", "code": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        return self.batch_progress.total.ready - 1", "code_tokens": ["def", "total_batch_idx", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "batch_progress", ".", "total", ".", "ready", "-", "1"], "docstring": "Returns the current batch index (across epochs)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "across", "epochs"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 98, "end_line": 102, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_737", "original_string": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        return self.batch_progress.current.ready - 1", "language": "python", "code": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        return self.batch_progress.current.ready - 1", "code_tokens": ["def", "batch_idx", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "batch_progress", ".", "current", ".", "ready", "-", "1"], "docstring": "Returns the current batch index (within this epoch)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "within", "this", "epoch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 105, "end_line": 109, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_738", "original_string": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self._is_training_done and self._is_validation_done:\r\n            return True\r\n\r\n        if self.trainer.should_stop:\r\n            min_epochs = self.trainer.fit_loop.min_epochs\r\n            can_stop_early = self.trainer.fit_loop._can_stop_early\r\n            if not can_stop_early:\r\n                self._warning_cache.info(\r\n                    f\"Trainer was signaled to stop but the required `min_epochs={min_epochs!r}` or\"\r\n                    f\" `min_steps={self.min_steps!r}` has not been met. Training will continue...\"\r\n                )\r\n            return can_stop_early\r\n\r\n        return False", "language": "python", "code": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self._is_training_done and self._is_validation_done:\r\n            return True\r\n\r\n        if self.trainer.should_stop:\r\n            min_epochs = self.trainer.fit_loop.min_epochs\r\n            can_stop_early = self.trainer.fit_loop._can_stop_early\r\n            if not can_stop_early:\r\n                self._warning_cache.info(\r\n                    f\"Trainer was signaled to stop but the required `min_epochs={min_epochs!r}` or\"\r\n                    f\" `min_steps={self.min_steps!r}` has not been met. Training will continue...\"\r\n                )\r\n            return can_stop_early\r\n\r\n        return False", "code_tokens": ["def", "done", "(", "self", ")", "-", ">", "bool", ":", "STRING", "if", "self", ".", "_is_training_done", "and", "self", ".", "_is_validation_done", ":", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", ":", "min_epochs", "=", "self", ".", "trainer", ".", "fit_loop", ".", "min_epochs", "can_stop_early", "=", "self", ".", "trainer", ".", "fit_loop", ".", "_can_stop_early", "if", "not", "can_stop_early", ":", "self", ".", "_warning_cache", ".", "info", "(", "fSTRING", "fSTRING", ")", "return", "can_stop_early", "return", "False"], "docstring": "Evaluates when to leave the loop.", "docstring_tokens": ["evaluates", "when", "to", "leave", "the", "loop"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 129, "end_line": 145, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_739", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        if (\r\n            self.restarting\r\n            and not self._should_accumulate()\r\n            and (self.restarted_on_train_batch_end or not self.restarted_on_last)\r\n        ):\r\n            self._batches_that_stepped += 1\r\n\r\n        if self.restarted_on_train_batch_end:\r\n            self.batch_progress.increment_completed()\r\n            if self.batch_progress.current.completed >= self.trainer.num_training_batches:\r\n                self.batch_progress.reset_on_run()\r\n                self.scheduler_progress.reset_on_run()\r\n                self.automatic_optimization.optim_progress.reset_on_run()\r\n                self.val_loop.batch_progress.total.reset()\r\n\r\n        if self.restarting:\r\n            self.batch_progress.reset_on_restart()\r\n            self.scheduler_progress.reset_on_restart()\r\n            self.automatic_optimization.optim_progress.reset_on_restart()\r\n\r\n            trainer = self.trainer\r\n            if trainer.num_training_batches != float(\"inf\"):\r\n                expected_steps = math.ceil(trainer.num_training_batches / trainer.accumulate_grad_batches)\r\n                loader = trainer.fit_loop._combined_loader\r\n                assert loader is not None\r\n                is_resumable_loader = all(isinstance(loader, _Stateful) for loader in loader.flattened)\r\n                if self.global_step % expected_steps != 0 and not is_resumable_loader:\r\n                    rank_zero_warn(\r\n                        \"You're resuming from a checkpoint that ended before the epoch ended and your dataloader is\"\r\n                        \" not resumable. This can cause unreliable results if further training is done.\"\r\n                        \" Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing\"\r\n                        \" the `state_dict` / `load_state_dict` interface.\",\r\n                        category=PossibleUserWarning,\r\n                    )\r\n        else:\r\n            self.batch_progress.reset_on_run()\r\n            self.scheduler_progress.reset_on_run()\r\n            self.automatic_optimization.optim_progress.reset_on_run()\r\n            self.val_loop.batch_progress.total.reset()", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        if (\r\n            self.restarting\r\n            and not self._should_accumulate()\r\n            and (self.restarted_on_train_batch_end or not self.restarted_on_last)\r\n        ):\r\n            self._batches_that_stepped += 1\r\n\r\n        if self.restarted_on_train_batch_end:\r\n            self.batch_progress.increment_completed()\r\n            if self.batch_progress.current.completed >= self.trainer.num_training_batches:\r\n                self.batch_progress.reset_on_run()\r\n                self.scheduler_progress.reset_on_run()\r\n                self.automatic_optimization.optim_progress.reset_on_run()\r\n                self.val_loop.batch_progress.total.reset()\r\n\r\n        if self.restarting:\r\n            self.batch_progress.reset_on_restart()\r\n            self.scheduler_progress.reset_on_restart()\r\n            self.automatic_optimization.optim_progress.reset_on_restart()\r\n\r\n            trainer = self.trainer\r\n            if trainer.num_training_batches != float(\"inf\"):\r\n                expected_steps = math.ceil(trainer.num_training_batches / trainer.accumulate_grad_batches)\r\n                loader = trainer.fit_loop._combined_loader\r\n                assert loader is not None\r\n                is_resumable_loader = all(isinstance(loader, _Stateful) for loader in loader.flattened)\r\n                if self.global_step % expected_steps != 0 and not is_resumable_loader:\r\n                    rank_zero_warn(\r\n                        \"You're resuming from a checkpoint that ended before the epoch ended and your dataloader is\"\r\n                        \" not resumable. This can cause unreliable results if further training is done.\"\r\n                        \" Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing\"\r\n                        \" the `state_dict` / `load_state_dict` interface.\",\r\n                        category=PossibleUserWarning,\r\n                    )\r\n        else:\r\n            self.batch_progress.reset_on_run()\r\n            self.scheduler_progress.reset_on_run()\r\n            self.automatic_optimization.optim_progress.reset_on_run()\r\n            self.val_loop.batch_progress.total.reset()", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "(", "self", ".", "restarting", "and", "not", "self", ".", "_should_accumulate", "(", ")", "and", "(", "self", ".", "restarted_on_train_batch_end", "or", "not", "self", ".", "restarted_on_last", ")", ")", ":", "self", ".", "_batches_that_stepped", "+", "=", "1", "if", "self", ".", "restarted_on_train_batch_end", ":", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "if", "self", ".", "batch_progress", ".", "current", ".", "completed", ">", "=", "self", ".", "trainer", ".", "num_training_batches", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_run", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_run", "(", ")", "self", ".", "val_loop", ".", "batch_progress", ".", "total", ".", "reset", "(", ")", "if", "self", ".", "restarting", ":", "self", ".", "batch_progress", ".", "reset_on_restart", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_restart", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_restart", "(", ")", "trainer", "=", "self", ".", "trainer", "if", "trainer", ".", "num_training_batches", "!", "=", "float", "(", "STRING", ")", ":", "expected_steps", "=", "math", ".", "ceil", "(", "trainer", ".", "num_training_batches", "/", "trainer", ".", "accumulate_grad_batches", ")", "loader", "=", "trainer", ".", "fit_loop", ".", "_combined_loader", "assert", "loader", "is", "not", "None", "is_resumable_loader", "=", "all", "(", "isinstance", "(", "loader", ",", "_Stateful", ")", "for", "loader", "in", "loader", ".", "flattened", ")", "if", "self", ".", "global_step", "%", "expected_steps", "!", "=", "0", "and", "not", "is_resumable_loader", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "else", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_run", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_run", "(", ")", "self", ".", "val_loop", ".", "batch_progress", ".", "total", ".", "reset", "(", ")"], "docstring": "Resets the internal state of the loop for a new run.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 190, "end_line": 236, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_740", "original_string": "def advance(self, data_fetcher: _DataFetcher) -> None:\r\n        \"\"\"Runs a single training batch.\r\n\r\n        Raises:\r\n            StopIteration: When the epoch is canceled by the user returning -1\r\n\r\n        \"\"\"\r\n        if self.restarting and self._should_check_val_fx(data_fetcher):\r\n            if self.val_loop.restarted_mid_evaluation:\r\n                return\r\n\r\n            if self.restarted_on_last:\r\n                self._skip_next_val = True\r\n                return\r\n\r\n            self.val_loop.increment_progress_to_evaluation_end()\r\n\r\n        self.val_loop.restarting = False\r\n\r\n\r\n        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.trainer.world_size > 1:\r\n            self._broadcast_sigterm_tensor()\r\n\r\n\r\n        if using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher):\r\n            dataloader_iter = next(data_fetcher)\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n        else:\r\n            dataloader_iter = None\r\n            batch, _, __ = next(data_fetcher)\r\n            batch_idx = self.batch_idx + 1\r\n        self.batch_progress.is_last_batch = data_fetcher.done\r\n\r\n        trainer = self.trainer\r\n        if not using_dataloader_iter:\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=0)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=0)\r\n\r\n        self.batch_progress.increment_ready()\r\n        trainer._logger_connector.on_batch_start(batch)\r\n\r\n        batch_output: _BATCH_OUTPUTS_TYPE = None  # for mypy\r\n        if batch is None and not using_dataloader_iter:\r\n            self._warning_cache.warn(\"train_dataloader yielded None. If this was on purpose, ignore this warning...\")\r\n        else:\r\n            call._call_callback_hooks(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            response = call._call_lightning_module_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            call._call_strategy_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            if response == -1:\r\n                self.batch_progress.increment_processed()\r\n                raise StopIteration\r\n\r\n            self.batch_progress.increment_started()\r\n\r\n            kwargs = (\r\n                self._build_kwargs(OrderedDict(), batch, batch_idx)\r\n                if not using_dataloader_iter\r\n                else OrderedDict(any=dataloader_iter)\r\n            )\r\n            with trainer.profiler.profile(\"run_training_batch\"):\r\n                if trainer.lightning_module.automatic_optimization:\r\n                    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\r\n                else:\r\n                    batch_output = self.manual_optimization.run(kwargs)\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        self.update_lr_schedulers(\"step\", update_plateau_schedulers=False)\r\n        if self._num_ready_batches_reached():\r\n            self.update_lr_schedulers(\"epoch\", update_plateau_schedulers=False)\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            self.batch_progress.is_last_batch = data_fetcher.done\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\r\n        call._call_lightning_module_hook(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\r\n        trainer._logger_connector.on_batch_end()\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        trainer._logger_connector.update_train_step_metrics()", "language": "python", "code": "def advance(self, data_fetcher: _DataFetcher) -> None:\r\n        \"\"\"Runs a single training batch.\r\n\r\n        Raises:\r\n            StopIteration: When the epoch is canceled by the user returning -1\r\n\r\n        \"\"\"\r\n        if self.restarting and self._should_check_val_fx(data_fetcher):\r\n            if self.val_loop.restarted_mid_evaluation:\r\n                return\r\n\r\n            if self.restarted_on_last:\r\n                self._skip_next_val = True\r\n                return\r\n\r\n            self.val_loop.increment_progress_to_evaluation_end()\r\n\r\n        self.val_loop.restarting = False\r\n\r\n\r\n        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.trainer.world_size > 1:\r\n            self._broadcast_sigterm_tensor()\r\n\r\n\r\n        if using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher):\r\n            dataloader_iter = next(data_fetcher)\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n        else:\r\n            dataloader_iter = None\r\n            batch, _, __ = next(data_fetcher)\r\n            batch_idx = self.batch_idx + 1\r\n        self.batch_progress.is_last_batch = data_fetcher.done\r\n\r\n        trainer = self.trainer\r\n        if not using_dataloader_iter:\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=0)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=0)\r\n\r\n        self.batch_progress.increment_ready()\r\n        trainer._logger_connector.on_batch_start(batch)\r\n\r\n        batch_output: _BATCH_OUTPUTS_TYPE = None  # for mypy\r\n        if batch is None and not using_dataloader_iter:\r\n            self._warning_cache.warn(\"train_dataloader yielded None. If this was on purpose, ignore this warning...\")\r\n        else:\r\n            call._call_callback_hooks(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            response = call._call_lightning_module_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            call._call_strategy_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n            if response == -1:\r\n                self.batch_progress.increment_processed()\r\n                raise StopIteration\r\n\r\n            self.batch_progress.increment_started()\r\n\r\n            kwargs = (\r\n                self._build_kwargs(OrderedDict(), batch, batch_idx)\r\n                if not using_dataloader_iter\r\n                else OrderedDict(any=dataloader_iter)\r\n            )\r\n            with trainer.profiler.profile(\"run_training_batch\"):\r\n                if trainer.lightning_module.automatic_optimization:\r\n                    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\r\n                else:\r\n                    batch_output = self.manual_optimization.run(kwargs)\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        self.update_lr_schedulers(\"step\", update_plateau_schedulers=False)\r\n        if self._num_ready_batches_reached():\r\n            self.update_lr_schedulers(\"epoch\", update_plateau_schedulers=False)\r\n\r\n        if using_dataloader_iter:\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            self.batch_progress.is_last_batch = data_fetcher.done\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\r\n        call._call_lightning_module_hook(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\r\n        trainer._logger_connector.on_batch_end()\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        trainer._logger_connector.update_train_step_metrics()", "code_tokens": ["def", "advance", "(", "self", ",", "data_fetcher", ":", "_DataFetcher", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "restarting", "and", "self", ".", "_should_check_val_fx", "(", "data_fetcher", ")", ":", "if", "self", ".", "val_loop", ".", "restarted_mid_evaluation", ":", "return", "if", "self", ".", "restarted_on_last", ":", "self", ".", "_skip_next_val", "=", "True", "return", "self", ".", "val_loop", ".", "increment_progress_to_evaluation_end", "(", ")", "self", ".", "val_loop", ".", "restarting", "=", "False", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "and", "self", ".", "trainer", ".", "world_size", ">", "1", ":", "self", ".", "_broadcast_sigterm_tensor", "(", ")", "if", "using_dataloader_iter", ":", "=", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ":", "dataloader_iter", "=", "next", "(", "data_fetcher", ")", "batch", "=", "data_fetcher", ".", "_batch", "batch_idx", "=", "data_fetcher", ".", "_batch_idx", "else", ":", "dataloader_iter", "=", "None", "batch", ",", "_", ",", "__", "=", "next", "(", "data_fetcher", ")", "batch_idx", "=", "self", ".", "batch_idx", "+", "1", "self", ".", "batch_progress", ".", "is_last_batch", "=", "data_fetcher", ".", "done", "trainer", "=", "self", ".", "trainer", "if", "not", "using_dataloader_iter", ":", "batch", "=", "trainer", ".", "precision_plugin", ".", "convert_input", "(", "batch", ")", "batch", "=", "trainer", ".", "lightning_module", ".", "_on_before_batch_transfer", "(", "batch", ",", "dataloader_idx", "=", "0", ")", "batch", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "batch", ",", "dataloader_idx", "=", "0", ")", "self", ".", "batch_progress", ".", "increment_ready", "(", ")", "trainer", ".", "_logger_connector", ".", "on_batch_start", "(", "batch", ")", "batch_output", ":", "_BATCH_OUTPUTS_TYPE", "=", "None", "#", "for", "mypy", "if", "batch", "is", "None", "and", "not", "using_dataloader_iter", ":", "self", ".", "_warning_cache", ".", "warn", "(", "STRING", ")", "else", ":", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "batch", ",", "batch_idx", ")", "response", "=", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "batch", ",", "batch_idx", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "batch", ",", "batch_idx", ")", "if", "response", "=", "=", "-", "1", ":", "self", ".", "batch_progress", ".", "increment_processed", "(", ")", "raise", "StopIteration", "self", ".", "batch_progress", ".", "increment_started", "(", ")", "kwargs", "=", "(", "self", ".", "_build_kwargs", "(", "OrderedDict", "(", ")", ",", "batch", ",", "batch_idx", ")", "if", "not", "using_dataloader_iter", "else", "OrderedDict", "(", "any", "=", "dataloader_iter", ")", ")", "with", "trainer", ".", "profiler", ".", "profile", "(", "STRING", ")", ":", "if", "trainer", ".", "lightning_module", ".", "automatic_optimization", ":", "batch_output", "=", "self", ".", "automatic_optimization", ".", "run", "(", "trainer", ".", "optimizers", "[", "0", "]", ",", "batch_idx", ",", "kwargs", ")", "else", ":", "batch_output", "=", "self", ".", "manual_optimization", ".", "run", "(", "kwargs", ")", "self", ".", "batch_progress", ".", "increment_processed", "(", ")", "self", ".", "update_lr_schedulers", "(", "STRING", ",", "update_plateau_schedulers", "=", "False", ")", "if", "self", ".", "_num_ready_batches_reached", "(", ")", ":", "self", ".", "update_lr_schedulers", "(", "STRING", ",", "update_plateau_schedulers", "=", "False", ")", "if", "using_dataloader_iter", ":", "batch", "=", "data_fetcher", ".", "_batch", "batch_idx", "=", "data_fetcher", ".", "_batch_idx", "self", ".", "batch_progress", ".", "is_last_batch", "=", "data_fetcher", ".", "done", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "batch_output", ",", "batch", ",", "batch_idx", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "batch_output", ",", "batch", ",", "batch_idx", ")", "trainer", ".", "_logger_connector", ".", "on_batch_end", "(", ")", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "trainer", ".", "_logger_connector", ".", "update_train_step_metrics", "(", ")"], "docstring": "Runs a single training batch.", "docstring_tokens": ["runs", "a", "single", "training", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 273, "end_line": 376, "has_examples": false, "num_comments": 13, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_741", "original_string": "def _accumulated_batches_reached(self) -> bool:\r\n        \"\"\"Determine if accumulation will be finished by the end of the current batch.\"\"\"\r\n        return self.batch_progress.current.ready % self.trainer.accumulate_grad_batches == 0", "language": "python", "code": "def _accumulated_batches_reached(self) -> bool:\r\n        \"\"\"Determine if accumulation will be finished by the end of the current batch.\"\"\"\r\n        return self.batch_progress.current.ready % self.trainer.accumulate_grad_batches == 0", "code_tokens": ["def", "_accumulated_batches_reached", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "batch_progress", ".", "current", ".", "ready", "%", "self", ".", "trainer", ".", "accumulate_grad_batches", "=", "=", "0"], "docstring": "Determine if accumulation will be finished by the end of the current batch.", "docstring_tokens": ["determine", "if", "accumulation", "will", "be", "finished", "by", "the", "end", "of", "the", "current", "batch"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 430, "end_line": 432, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_742", "original_string": "def _num_ready_batches_reached(self) -> bool:\r\n        \"\"\"Checks if we are in the last batch or if there are more batches to follow.\"\"\"\r\n        epoch_finished_on_ready = self.batch_progress.current.ready == self.trainer.num_training_batches\r\n        return epoch_finished_on_ready or self.batch_progress.is_last_batch", "language": "python", "code": "def _num_ready_batches_reached(self) -> bool:\r\n        \"\"\"Checks if we are in the last batch or if there are more batches to follow.\"\"\"\r\n        epoch_finished_on_ready = self.batch_progress.current.ready == self.trainer.num_training_batches\r\n        return epoch_finished_on_ready or self.batch_progress.is_last_batch", "code_tokens": ["def", "_num_ready_batches_reached", "(", "self", ")", "-", ">", "bool", ":", "STRING", "epoch_finished_on_ready", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "=", "=", "self", ".", "trainer", ".", "num_training_batches", "return", "epoch_finished_on_ready", "or", "self", ".", "batch_progress", ".", "is_last_batch"], "docstring": "Checks if we are in the last batch or if there are more batches to follow.", "docstring_tokens": ["checks", "if", "we", "are", "in", "the", "last", "batch", "or", "if", "there", "are", "more", "batches", "to", "follow"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 434, "end_line": 437, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_743", "original_string": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Checks if the optimizer step should be performed or gradients should be accumulated for the current step.\"\"\"\r\n        accumulation_done = self._accumulated_batches_reached()\r\n        is_final_batch = self._num_ready_batches_reached()\r\n        strategy_accumulates_on_final_batch = self.trainer.strategy.handles_gradient_accumulation or not is_final_batch\r\n        return not accumulation_done and strategy_accumulates_on_final_batch", "language": "python", "code": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Checks if the optimizer step should be performed or gradients should be accumulated for the current step.\"\"\"\r\n        accumulation_done = self._accumulated_batches_reached()\r\n        is_final_batch = self._num_ready_batches_reached()\r\n        strategy_accumulates_on_final_batch = self.trainer.strategy.handles_gradient_accumulation or not is_final_batch\r\n        return not accumulation_done and strategy_accumulates_on_final_batch", "code_tokens": ["def", "_should_accumulate", "(", "self", ")", "-", ">", "bool", ":", "STRING", "accumulation_done", "=", "self", ".", "_accumulated_batches_reached", "(", ")", "is_final_batch", "=", "self", ".", "_num_ready_batches_reached", "(", ")", "strategy_accumulates_on_final_batch", "=", "self", ".", "trainer", ".", "strategy", ".", "handles_gradient_accumulation", "or", "not", "is_final_batch", "return", "not", "accumulation_done", "and", "strategy_accumulates_on_final_batch"], "docstring": "Checks if the optimizer step should be performed or gradients should be accumulated for the current step.", "docstring_tokens": ["checks", "if", "the", "optimizer", "step", "should", "be", "performed", "or", "gradients", "should", "be", "accumulated", "for", "the", "current", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 439, "end_line": 446, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_744", "original_string": "def update_lr_schedulers(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Updates the lr schedulers based on the given interval.\"\"\"\r\n        if interval == \"step\" and self._should_accumulate():\r\n            return\r\n        self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)", "language": "python", "code": "def update_lr_schedulers(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Updates the lr schedulers based on the given interval.\"\"\"\r\n        if interval == \"step\" and self._should_accumulate():\r\n            return\r\n        self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)", "code_tokens": ["def", "update_lr_schedulers", "(", "self", ",", "interval", ":", "str", ",", "update_plateau_schedulers", ":", "bool", ")", "-", ">", "None", ":", "STRING", "if", "interval", "=", "=", "STRING", "and", "self", ".", "_should_accumulate", "(", ")", ":", "return", "self", ".", "_update_learning_rates", "(", "interval", "=", "interval", ",", "update_plateau_schedulers", "=", "update_plateau_schedulers", ")"], "docstring": "Updates the lr schedulers based on the given interval.", "docstring_tokens": ["updates", "the", "lr", "schedulers", "based", "on", "the", "given", "interval"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 448, "end_line": 452, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_745", "original_string": "def _update_learning_rates(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Update learning rates.\r\n\r\n        Args:\r\n            interval: either 'epoch' or 'step'.\r\n            update_plateau_schedulers: control whether ``ReduceLROnPlateau`` or non-plateau schedulers get updated.\r\n                This is used so non-plateau schedulers can be updated before running validation. Checkpoints are\r\n                commonly saved during validation, however, on-plateau schedulers might monitor a validation metric\r\n                so they have to be updated separately.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        if not trainer.lr_scheduler_configs or not trainer.lightning_module.automatic_optimization:\r\n            return\r\n\r\n        for config in trainer.lr_scheduler_configs:\r\n            if update_plateau_schedulers ^ config.reduce_on_plateau:\r\n                continue\r\n\r\n            current_idx = self.batch_idx if interval == \"step\" else trainer.current_epoch\r\n            current_idx += 1  # account for both batch and epoch starts from 0\r\n            if config.interval == interval and current_idx % config.frequency == 0:\r\n                monitor_val = None\r\n                if config.reduce_on_plateau:\r\n                    monitor_key = config.monitor\r\n                    assert monitor_key is not None\r\n                    monitor_val = self._get_monitor_value(monitor_key)\r\n                    if monitor_val is None:\r\n                        if config.strict:\r\n                            avail_metrics = list(trainer.callback_metrics)\r\n                            raise MisconfigurationException(\r\n                                f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                                f\" which is not available. Available metrics are: {avail_metrics}.\"\r\n                                \" Condition can be set using `monitor` key in lr scheduler dict\"\r\n                            )\r\n                        rank_zero_warn(\r\n                            f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                            \" which is not available but strict is set to `False`.\"\r\n                            \" Skipping learning rate update.\",\r\n                            category=RuntimeWarning,\r\n                        )\r\n                        continue\r\n\r\n                self.scheduler_progress.increment_ready()\r\n\r\n                call._call_lightning_module_hook(\r\n                    trainer,\r\n                    \"lr_scheduler_step\",\r\n                    config.scheduler,\r\n                    monitor_val,\r\n                )\r\n                self.scheduler_progress.increment_completed()", "language": "python", "code": "def _update_learning_rates(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Update learning rates.\r\n\r\n        Args:\r\n            interval: either 'epoch' or 'step'.\r\n            update_plateau_schedulers: control whether ``ReduceLROnPlateau`` or non-plateau schedulers get updated.\r\n                This is used so non-plateau schedulers can be updated before running validation. Checkpoints are\r\n                commonly saved during validation, however, on-plateau schedulers might monitor a validation metric\r\n                so they have to be updated separately.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        if not trainer.lr_scheduler_configs or not trainer.lightning_module.automatic_optimization:\r\n            return\r\n\r\n        for config in trainer.lr_scheduler_configs:\r\n            if update_plateau_schedulers ^ config.reduce_on_plateau:\r\n                continue\r\n\r\n            current_idx = self.batch_idx if interval == \"step\" else trainer.current_epoch\r\n            current_idx += 1  # account for both batch and epoch starts from 0\r\n            if config.interval == interval and current_idx % config.frequency == 0:\r\n                monitor_val = None\r\n                if config.reduce_on_plateau:\r\n                    monitor_key = config.monitor\r\n                    assert monitor_key is not None\r\n                    monitor_val = self._get_monitor_value(monitor_key)\r\n                    if monitor_val is None:\r\n                        if config.strict:\r\n                            avail_metrics = list(trainer.callback_metrics)\r\n                            raise MisconfigurationException(\r\n                                f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                                f\" which is not available. Available metrics are: {avail_metrics}.\"\r\n                                \" Condition can be set using `monitor` key in lr scheduler dict\"\r\n                            )\r\n                        rank_zero_warn(\r\n                            f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                            \" which is not available but strict is set to `False`.\"\r\n                            \" Skipping learning rate update.\",\r\n                            category=RuntimeWarning,\r\n                        )\r\n                        continue\r\n\r\n                self.scheduler_progress.increment_ready()\r\n\r\n                call._call_lightning_module_hook(\r\n                    trainer,\r\n                    \"lr_scheduler_step\",\r\n                    config.scheduler,\r\n                    monitor_val,\r\n                )\r\n                self.scheduler_progress.increment_completed()", "code_tokens": ["def", "_update_learning_rates", "(", "self", ",", "interval", ":", "str", ",", "update_plateau_schedulers", ":", "bool", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "if", "not", "trainer", ".", "lr_scheduler_configs", "or", "not", "trainer", ".", "lightning_module", ".", "automatic_optimization", ":", "return", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ":", "if", "update_plateau_schedulers", "^", "config", ".", "reduce_on_plateau", ":", "continue", "current_idx", "=", "self", ".", "batch_idx", "if", "interval", "=", "=", "STRING", "else", "trainer", ".", "current_epoch", "current_idx", "+", "=", "1", "#", "account", "for", "both", "batch", "and", "epoch", "starts", "from", "0", "if", "config", ".", "interval", "=", "=", "interval", "and", "current_idx", "%", "config", ".", "frequency", "=", "=", "0", ":", "monitor_val", "=", "None", "if", "config", ".", "reduce_on_plateau", ":", "monitor_key", "=", "config", ".", "monitor", "assert", "monitor_key", "is", "not", "None", "monitor_val", "=", "self", ".", "_get_monitor_value", "(", "monitor_key", ")", "if", "monitor_val", "is", "None", ":", "if", "config", ".", "strict", ":", "avail_metrics", "=", "list", "(", "trainer", ".", "callback_metrics", ")", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", "STRING", ")", "rank_zero_warn", "(", "fSTRING", "STRING", "STRING", ",", "category", "=", "RuntimeWarning", ",", ")", "continue", "self", ".", "scheduler_progress", ".", "increment_ready", "(", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "config", ".", "scheduler", ",", "monitor_val", ",", ")", "self", ".", "scheduler_progress", ".", "increment_completed", "(", ")"], "docstring": "Update learning rates.", "docstring_tokens": ["update", "learning", "rates"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 454, "end_line": 509, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_746", "original_string": "def _should_check_val_fx(self, data_fetcher: _DataFetcher) -> bool:\r\n        \"\"\"Decide if we should run validation.\"\"\"\r\n        if not self._should_check_val_epoch():\r\n            return False\r\n\r\n        is_infinite_dataset = self.trainer.val_check_batch == float(\"inf\")\r\n        is_last_batch = self.batch_progress.is_last_batch\r\n        if is_last_batch and (is_infinite_dataset or isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            return True\r\n\r\n        if self.trainer.should_stop and self.trainer.fit_loop._can_stop_early:\r\n            return True\r\n\r\n        interval = self.trainer._val_check_time_interval\r\n        if interval is not None:\r\n            now = time.monotonic()\r\n            return now - self.trainer._last_val_time >= interval\r\n        is_val_check_batch = is_last_batch\r\n        if isinstance(self.trainer.limit_train_batches, int) and is_infinite_dataset:\r\n            is_val_check_batch = (self.batch_idx + 1) % self.trainer.limit_train_batches == 0\r\n        elif self.trainer.val_check_batch != float(\"inf\"):\r\n            assert self.trainer.val_check_batch is not None\r\n            current_iteration = self.total_batch_idx if self.trainer.check_val_every_n_epoch is None else self.batch_idx\r\n            is_val_check_batch = (current_iteration + 1) % self.trainer.val_check_batch == 0\r\n\r\n        return is_val_check_batch", "language": "python", "code": "def _should_check_val_fx(self, data_fetcher: _DataFetcher) -> bool:\r\n        \"\"\"Decide if we should run validation.\"\"\"\r\n        if not self._should_check_val_epoch():\r\n            return False\r\n\r\n        is_infinite_dataset = self.trainer.val_check_batch == float(\"inf\")\r\n        is_last_batch = self.batch_progress.is_last_batch\r\n        if is_last_batch and (is_infinite_dataset or isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            return True\r\n\r\n        if self.trainer.should_stop and self.trainer.fit_loop._can_stop_early:\r\n            return True\r\n\r\n        interval = self.trainer._val_check_time_interval\r\n        if interval is not None:\r\n            now = time.monotonic()\r\n            return now - self.trainer._last_val_time >= interval\r\n        is_val_check_batch = is_last_batch\r\n        if isinstance(self.trainer.limit_train_batches, int) and is_infinite_dataset:\r\n            is_val_check_batch = (self.batch_idx + 1) % self.trainer.limit_train_batches == 0\r\n        elif self.trainer.val_check_batch != float(\"inf\"):\r\n            assert self.trainer.val_check_batch is not None\r\n            current_iteration = self.total_batch_idx if self.trainer.check_val_every_n_epoch is None else self.batch_idx\r\n            is_val_check_batch = (current_iteration + 1) % self.trainer.val_check_batch == 0\r\n\r\n        return is_val_check_batch", "code_tokens": ["def", "_should_check_val_fx", "(", "self", ",", "data_fetcher", ":", "_DataFetcher", ")", "-", ">", "bool", ":", "STRING", "if", "not", "self", ".", "_should_check_val_epoch", "(", ")", ":", "return", "False", "is_infinite_dataset", "=", "self", ".", "trainer", ".", "val_check_batch", "=", "=", "float", "(", "STRING", ")", "is_last_batch", "=", "self", ".", "batch_progress", ".", "is_last_batch", "if", "is_last_batch", "and", "(", "is_infinite_dataset", "or", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ")", ":", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", "and", "self", ".", "trainer", ".", "fit_loop", ".", "_can_stop_early", ":", "return", "True", "interval", "=", "self", ".", "trainer", ".", "_val_check_time_interval", "if", "interval", "is", "not", "None", ":", "now", "=", "time", ".", "monotonic", "(", ")", "return", "now", "-", "self", ".", "trainer", ".", "_last_val_time", ">", "=", "interval", "is_val_check_batch", "=", "is_last_batch", "if", "isinstance", "(", "self", ".", "trainer", ".", "limit_train_batches", ",", "int", ")", "and", "is_infinite_dataset", ":", "is_val_check_batch", "=", "(", "self", ".", "batch_idx", "+", "1", ")", "%", "self", ".", "trainer", ".", "limit_train_batches", "=", "=", "0", "elif", "self", ".", "trainer", ".", "val_check_batch", "!", "=", "float", "(", "STRING", ")", ":", "assert", "self", ".", "trainer", ".", "val_check_batch", "is", "not", "None", "current_iteration", "=", "self", ".", "total_batch_idx", "if", "self", ".", "trainer", ".", "check_val_every_n_epoch", "is", "None", "else", "self", ".", "batch_idx", "is_val_check_batch", "=", "(", "current_iteration", "+", "1", ")", "%", "self", ".", "trainer", ".", "val_check_batch", "=", "=", "0", "return", "is_val_check_batch"], "docstring": "Decide if we should run validation.", "docstring_tokens": ["decide", "if", "we", "should", "run", "validation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 521, "end_line": 554, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_747", "original_string": "def _save_loggers_on_train_batch_end(self) -> None:\r\n        \"\"\"Flushes loggers to disk.\"\"\"\r\n        if self.trainer.should_stop:\r\n            for logger in self.trainer.loggers:\r\n                logger.save()", "language": "python", "code": "def _save_loggers_on_train_batch_end(self) -> None:\r\n        \"\"\"Flushes loggers to disk.\"\"\"\r\n        if self.trainer.should_stop:\r\n            for logger in self.trainer.loggers:\r\n                logger.save()", "code_tokens": ["def", "_save_loggers_on_train_batch_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "trainer", ".", "should_stop", ":", "for", "logger", "in", "self", ".", "trainer", ".", "loggers", ":", "logger", ".", "save", "(", ")"], "docstring": "Flushes loggers to disk.", "docstring_tokens": ["flushes", "loggers", "to", "disk"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 556, "end_line": 560, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "function_748", "original_string": "def _build_kwargs(self, kwargs: OrderedDict, batch: Any, batch_idx: int) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n            batch: The current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n\r\n        Returns:\r\n            The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        kwargs[\"batch\"] = batch\r\n        training_step_fx = getattr(self.trainer.lightning_module, \"training_step\")\r\n        if is_param_in_hook_signature(training_step_fx, \"batch_idx\", min_args=2):\r\n            kwargs[\"batch_idx\"] = batch_idx\r\n        return kwargs", "language": "python", "code": "def _build_kwargs(self, kwargs: OrderedDict, batch: Any, batch_idx: int) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n            batch: The current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n\r\n        Returns:\r\n            The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        kwargs[\"batch\"] = batch\r\n        training_step_fx = getattr(self.trainer.lightning_module, \"training_step\")\r\n        if is_param_in_hook_signature(training_step_fx, \"batch_idx\", min_args=2):\r\n            kwargs[\"batch_idx\"] = batch_idx\r\n        return kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "kwargs", ":", "OrderedDict", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "OrderedDict", ":", "STRING", "kwargs", "[", "STRING", "]", "=", "batch", "training_step_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "STRING", ")", "if", "is_param_in_hook_signature", "(", "training_step_fx", ",", "STRING", ",", "min_args", "=", "2", ")", ":", "kwargs", "[", "STRING", "]", "=", "batch_idx", "return", "kwargs"], "docstring": "Helper method to build the arguments for the current step.", "docstring_tokens": ["helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "start_line": 562, "end_line": 580, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "function_749", "original_string": "def check_finite_loss(loss: Optional[Tensor]) -> None:\r\n    \"\"\"Checks for finite loss value.\r\n\r\n    Args:\r\n        loss: the loss value to check to be finite\r\n\r\n    \"\"\"\r\n    if loss is not None and not torch.isfinite(loss).all():\r\n        raise ValueError(f\"The loss returned in `training_step` is {loss}.\")", "language": "python", "code": "def check_finite_loss(loss: Optional[Tensor]) -> None:\r\n    \"\"\"Checks for finite loss value.\r\n\r\n    Args:\r\n        loss: the loss value to check to be finite\r\n\r\n    \"\"\"\r\n    if loss is not None and not torch.isfinite(loss).all():\r\n        raise ValueError(f\"The loss returned in `training_step` is {loss}.\")", "code_tokens": ["def", "check_finite_loss", "(", "loss", ":", "Optional", "[", "Tensor", "]", ")", "-", ">", "None", ":", "STRING", "if", "loss", "is", "not", "None", "and", "not", "torch", ".", "isfinite", "(", "loss", ")", ".", "all", "(", ")", ":", "raise", "ValueError", "(", "fSTRING", ")"], "docstring": "Checks for finite loss value.", "docstring_tokens": ["checks", "for", "finite", "loss", "value"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\utilities.py", "start_line": 38, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "function_750", "original_string": "def _parse_loop_limits(\r\n    min_steps: Optional[int],\r\n    max_steps: int,\r\n    min_epochs: Optional[int],\r\n    max_epochs: Optional[int],\r\n    trainer: \"pl.Trainer\",\r\n) -> tuple[int, int]:\r\n    \"\"\"This utility computes the default values for the minimum and maximum number of steps and epochs given the values\r\n    the user has selected.\r\n\r\n    Args:\r\n        min_steps: Minimum number of steps.\r\n        max_steps: Maximum number of steps.\r\n        min_epochs: Minimum number of epochs.\r\n        max_epochs: Maximum number of epochs.\r\n        trainer: Trainer instance.\r\n\r\n    Returns:\r\n        The parsed limits, with default values being set for the ones that the user did not specify.\r\n\r\n    \"\"\"\r\n    if max_epochs is None:\r\n        if max_steps == -1 and not any(isinstance(cb, Timer) for cb in trainer.callbacks):\r\n            rank_zero_warn(\r\n                \"`max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit,\"\r\n                \" set `max_epochs=-1`.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            max_epochs = 1000\r\n        else:\r\n            max_epochs = -1\r\n\r\n    if min_epochs is None and min_steps is not None:\r\n        min_epochs = 1\r\n\r\n    if min_epochs is None:\r\n        min_epochs = 0\r\n\r\n    return min_epochs, max_epochs", "language": "python", "code": "def _parse_loop_limits(\r\n    min_steps: Optional[int],\r\n    max_steps: int,\r\n    min_epochs: Optional[int],\r\n    max_epochs: Optional[int],\r\n    trainer: \"pl.Trainer\",\r\n) -> tuple[int, int]:\r\n    \"\"\"This utility computes the default values for the minimum and maximum number of steps and epochs given the values\r\n    the user has selected.\r\n\r\n    Args:\r\n        min_steps: Minimum number of steps.\r\n        max_steps: Maximum number of steps.\r\n        min_epochs: Minimum number of epochs.\r\n        max_epochs: Maximum number of epochs.\r\n        trainer: Trainer instance.\r\n\r\n    Returns:\r\n        The parsed limits, with default values being set for the ones that the user did not specify.\r\n\r\n    \"\"\"\r\n    if max_epochs is None:\r\n        if max_steps == -1 and not any(isinstance(cb, Timer) for cb in trainer.callbacks):\r\n            rank_zero_warn(\r\n                \"`max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit,\"\r\n                \" set `max_epochs=-1`.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            max_epochs = 1000\r\n        else:\r\n            max_epochs = -1\r\n\r\n    if min_epochs is None and min_steps is not None:\r\n        min_epochs = 1\r\n\r\n    if min_epochs is None:\r\n        min_epochs = 0\r\n\r\n    return min_epochs, max_epochs", "code_tokens": ["def", "_parse_loop_limits", "(", "min_steps", ":", "Optional", "[", "int", "]", ",", "max_steps", ":", "int", ",", "min_epochs", ":", "Optional", "[", "int", "]", ",", "max_epochs", ":", "Optional", "[", "int", "]", ",", "trainer", ":", "STRING", ",", ")", "-", ">", "tuple", "[", "int", ",", "int", "]", ":", "STRING", "if", "max_epochs", "is", "None", ":", "if", "max_steps", "=", "=", "-", "1", "and", "not", "any", "(", "isinstance", "(", "cb", ",", "Timer", ")", "for", "cb", "in", "trainer", ".", "callbacks", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "max_epochs", "=", "1000", "else", ":", "max_epochs", "=", "-", "1", "if", "min_epochs", "is", "None", "and", "min_steps", "is", "not", "None", ":", "min_epochs", "=", "1", "if", "min_epochs", "is", "None", ":", "min_epochs", "=", "0", "return", "min_epochs", ",", "max_epochs"], "docstring": "This utility computes the default values for the minimum and maximum number of steps and epochs given the values", "docstring_tokens": ["this", "utility", "computes", "the", "default", "values", "for", "the", "minimum", "and", "maximum", "number", "of", "steps", "and", "epochs", "given", "the", "values"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\utilities.py", "start_line": 49, "end_line": 89, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "function_751", "original_string": "def _block_parallel_sync_behavior(strategy: Strategy, block: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for\r\n    example when accumulating gradients to reduce communication when it is not needed.\r\n\r\n    Args:\r\n        strategy: the strategy instance to use.\r\n        block: whether the context manager is enabled or not\r\n\r\n    Returns:\r\n        context manager with sync behaviour off\r\n\r\n    \"\"\"\r\n    if isinstance(strategy, ParallelStrategy) and block:\r\n        with strategy.block_backward_sync():\r\n            yield None\r\n    else:\r\n        yield None", "language": "python", "code": "def _block_parallel_sync_behavior(strategy: Strategy, block: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for\r\n    example when accumulating gradients to reduce communication when it is not needed.\r\n\r\n    Args:\r\n        strategy: the strategy instance to use.\r\n        block: whether the context manager is enabled or not\r\n\r\n    Returns:\r\n        context manager with sync behaviour off\r\n\r\n    \"\"\"\r\n    if isinstance(strategy, ParallelStrategy) and block:\r\n        with strategy.block_backward_sync():\r\n            yield None\r\n    else:\r\n        yield None", "code_tokens": ["def", "_block_parallel_sync_behavior", "(", "strategy", ":", "Strategy", ",", "block", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "if", "isinstance", "(", "strategy", ",", "ParallelStrategy", ")", "and", "block", ":", "with", "strategy", ".", "block_backward_sync", "(", ")", ":", "yield", "None", "else", ":", "yield", "None"], "docstring": "Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for", "docstring_tokens": ["blocks", "synchronization", "in", "class", "lightning", "pytorch", "strategies", "parallel", "parallelstrategy", "this", "is", "useful", "for"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\utilities.py", "start_line": 93, "end_line": 109, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "function_752", "original_string": "def _is_max_limit_reached(current: int, maximum: int = -1) -> bool:\r\n    \"\"\"Check if the limit has been reached (if enabled).\r\n\r\n    Args:\r\n        current: the current value\r\n        maximum: the maximum value (or -1 to disable limit)\r\n\r\n    Returns:\r\n        bool: whether the limit has been reached\r\n\r\n    \"\"\"\r\n    return maximum != -1 and current >= maximum", "language": "python", "code": "def _is_max_limit_reached(current: int, maximum: int = -1) -> bool:\r\n    \"\"\"Check if the limit has been reached (if enabled).\r\n\r\n    Args:\r\n        current: the current value\r\n        maximum: the maximum value (or -1 to disable limit)\r\n\r\n    Returns:\r\n        bool: whether the limit has been reached\r\n\r\n    \"\"\"\r\n    return maximum != -1 and current >= maximum", "code_tokens": ["def", "_is_max_limit_reached", "(", "current", ":", "int", ",", "maximum", ":", "int", "=", "-", "1", ")", "-", ">", "bool", ":", "STRING", "return", "maximum", "!", "=", "-", "1", "and", "current", ">", "=", "maximum"], "docstring": "Check if the limit has been reached (if enabled).", "docstring_tokens": ["check", "if", "the", "limit", "has", "been", "reached", "if", "enabled"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\utilities.py", "start_line": 112, "end_line": 123, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_753", "original_string": "def run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\r\n        \"\"\"Runs closure (train step + backward) together with optimization if necessary.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks\r\n            batch_idx: the current batch index.\r\n            optimizer: the optimizer\r\n\r\n        \"\"\"\r\n        closure = self._make_closure(kwargs, optimizer, batch_idx)\r\n\r\n        if (\r\n            not self.trainer.strategy.handles_gradient_accumulation and self.trainer.fit_loop._should_accumulate()\r\n        ):\r\n\r\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\r\n                closure()\r\n\r\n        else:\r\n            self._optimizer_step(batch_idx, closure)\r\n\r\n        result = closure.consume_result()\r\n        if result.loss is None:\r\n            return {}\r\n        return result.asdict()", "language": "python", "code": "def run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\r\n        \"\"\"Runs closure (train step + backward) together with optimization if necessary.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks\r\n            batch_idx: the current batch index.\r\n            optimizer: the optimizer\r\n\r\n        \"\"\"\r\n        closure = self._make_closure(kwargs, optimizer, batch_idx)\r\n\r\n        if (\r\n            not self.trainer.strategy.handles_gradient_accumulation and self.trainer.fit_loop._should_accumulate()\r\n        ):\r\n\r\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\r\n                closure()\r\n\r\n        else:\r\n            self._optimizer_step(batch_idx, closure)\r\n\r\n        result = closure.consume_result()\r\n        if result.loss is None:\r\n            return {}\r\n        return result.asdict()", "code_tokens": ["def", "run", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "batch_idx", ":", "int", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "_OUTPUTS_TYPE", ":", "STRING", "closure", "=", "self", ".", "_make_closure", "(", "kwargs", ",", "optimizer", ",", "batch_idx", ")", "if", "(", "not", "self", ".", "trainer", ".", "strategy", ".", "handles_gradient_accumulation", "and", "self", ".", "trainer", ".", "fit_loop", ".", "_should_accumulate", "(", ")", ")", ":", "with", "_block_parallel_sync_behavior", "(", "self", ".", "trainer", ".", "strategy", ",", "block", "=", "True", ")", ":", "closure", "(", ")", "else", ":", "self", ".", "_optimizer_step", "(", "batch_idx", ",", "closure", ")", "result", "=", "closure", ".", "consume_result", "(", ")", "if", "result", ".", "loss", "is", "None", ":", "return", "{", "}", "return", "result", ".", "asdict", "(", ")"], "docstring": "Runs closure (train step + backward) together with optimization if necessary.", "docstring_tokens": ["runs", "closure", "train", "step", "backward", "together", "with", "optimization", "if", "necessary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 162, "end_line": 196, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_754", "original_string": "def _make_closure(self, kwargs: OrderedDict, optimizer: Optimizer, batch_idx: int) -> Closure:\r\n        \"\"\"Build a closure object that captures the given arguments and runs the `training_step` function and\r\n        optionally other functions such as `backward` and `zero_grad`.\"\"\"\r\n        step_fn = self._make_step_fn(kwargs)\r\n        backward_fn = self._make_backward_fn(optimizer)\r\n        zero_grad_fn = self._make_zero_grad_fn(batch_idx, optimizer)\r\n        return Closure(step_fn=step_fn, backward_fn=backward_fn, zero_grad_fn=zero_grad_fn)", "language": "python", "code": "def _make_closure(self, kwargs: OrderedDict, optimizer: Optimizer, batch_idx: int) -> Closure:\r\n        \"\"\"Build a closure object that captures the given arguments and runs the `training_step` function and\r\n        optionally other functions such as `backward` and `zero_grad`.\"\"\"\r\n        step_fn = self._make_step_fn(kwargs)\r\n        backward_fn = self._make_backward_fn(optimizer)\r\n        zero_grad_fn = self._make_zero_grad_fn(batch_idx, optimizer)\r\n        return Closure(step_fn=step_fn, backward_fn=backward_fn, zero_grad_fn=zero_grad_fn)", "code_tokens": ["def", "_make_closure", "(", "self", ",", "kwargs", ":", "OrderedDict", ",", "optimizer", ":", "Optimizer", ",", "batch_idx", ":", "int", ")", "-", ">", "Closure", ":", "STRING", "step_fn", "=", "self", ".", "_make_step_fn", "(", "kwargs", ")", "backward_fn", "=", "self", ".", "_make_backward_fn", "(", "optimizer", ")", "zero_grad_fn", "=", "self", ".", "_make_zero_grad_fn", "(", "batch_idx", ",", "optimizer", ")", "return", "Closure", "(", "step_fn", "=", "step_fn", ",", "backward_fn", "=", "backward_fn", ",", "zero_grad_fn", "=", "zero_grad_fn", ")"], "docstring": "Build a closure object that captures the given arguments and runs the `training_step` function and", "docstring_tokens": ["build", "a", "closure", "object", "that", "captures", "the", "given", "arguments", "and", "runs", "the", "training_step", "function", "and"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 198, "end_line": 204, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_755", "original_string": "def _make_step_fn(self, kwargs: OrderedDict) -> Callable[[], ClosureResult]:\r\n        \"\"\"Build the step function that runs the `training_step` and processes its output.\"\"\"\r\n        return partial(self._training_step, kwargs)", "language": "python", "code": "def _make_step_fn(self, kwargs: OrderedDict) -> Callable[[], ClosureResult]:\r\n        \"\"\"Build the step function that runs the `training_step` and processes its output.\"\"\"\r\n        return partial(self._training_step, kwargs)", "code_tokens": ["def", "_make_step_fn", "(", "self", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "Callable", "[", "[", "]", ",", "ClosureResult", "]", ":", "STRING", "return", "partial", "(", "self", ".", "_training_step", ",", "kwargs", ")"], "docstring": "Build the step function that runs the `training_step` and processes its output.", "docstring_tokens": ["build", "the", "step", "function", "that", "runs", "the", "training_step", "and", "processes", "its", "output"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 206, "end_line": 208, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_756", "original_string": "def _make_zero_grad_fn(self, batch_idx: int, optimizer: Optimizer) -> Optional[Callable[[], None]]:\r\n        \"\"\"Build a `zero_grad` function that zeroes the gradients before back-propagation.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        is_first_batch_to_accumulate = batch_idx % self.trainer.accumulate_grad_batches == 0\r\n        if not is_first_batch_to_accumulate:\r\n            return None\r\n\r\n        def zero_grad_fn() -> None:\r\n            self._on_before_zero_grad(optimizer)\r\n            self._optimizer_zero_grad(batch_idx, optimizer)\r\n\r\n        return zero_grad_fn", "language": "python", "code": "def _make_zero_grad_fn(self, batch_idx: int, optimizer: Optimizer) -> Optional[Callable[[], None]]:\r\n        \"\"\"Build a `zero_grad` function that zeroes the gradients before back-propagation.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        is_first_batch_to_accumulate = batch_idx % self.trainer.accumulate_grad_batches == 0\r\n        if not is_first_batch_to_accumulate:\r\n            return None\r\n\r\n        def zero_grad_fn() -> None:\r\n            self._on_before_zero_grad(optimizer)\r\n            self._optimizer_zero_grad(batch_idx, optimizer)\r\n\r\n        return zero_grad_fn", "code_tokens": ["def", "_make_zero_grad_fn", "(", "self", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optional", "[", "Callable", "[", "[", "]", ",", "None", "]", "]", ":", "STRING", "if", "self", ".", "_skip_backward", ":", "return", "None", "is_first_batch_to_accumulate", "=", "batch_idx", "%", "self", ".", "trainer", ".", "accumulate_grad_batches", "=", "=", "0", "if", "not", "is_first_batch_to_accumulate", ":", "return", "None", "def", "zero_grad_fn", "(", ")", "-", ">", "None", ":", "self", ".", "_on_before_zero_grad", "(", "optimizer", ")", "self", ".", "_optimizer_zero_grad", "(", "batch_idx", ",", "optimizer", ")", "return", "zero_grad_fn"], "docstring": "Build a `zero_grad` function that zeroes the gradients before back-propagation.", "docstring_tokens": ["build", "a", "zero_grad", "function", "that", "zeroes", "the", "gradients", "before", "back", "propagation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 210, "end_line": 227, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_757", "original_string": "def _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn", "language": "python", "code": "def _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn", "code_tokens": ["def", "_make_backward_fn", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optional", "[", "Callable", "[", "[", "Tensor", "]", ",", "None", "]", "]", ":", "STRING", "if", "self", ".", "_skip_backward", ":", "return", "None", "def", "backward_fn", "(", "loss", ":", "Tensor", ")", "-", ">", "None", ":", "call", ".", "_call_strategy_hook", "(", "self", ".", "trainer", ",", "STRING", ",", "loss", ",", "optimizer", ")", "return", "backward_fn"], "docstring": "Build a `backward` function that handles back-propagation through the output produced by the `training_step`", "docstring_tokens": ["build", "a", "backward", "function", "that", "handles", "back", "propagation", "through", "the", "output", "produced", "by", "the", "training_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 229, "end_line": 242, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_758", "original_string": "def _optimizer_step(\r\n        self,\r\n        batch_idx: int,\r\n        train_step_and_backward_closure: Callable[[], Optional[Tensor]],\r\n    ) -> None:\r\n        \"\"\"Performs the optimizer step and some sanity checking.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            train_step_and_backward_closure: the closure function performing the train step and computing the\r\n                gradients. By default, called by the optimizer (if possible)\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        optimizer = trainer.strategy._lightning_optimizers[0]\r\n\r\n        should_accumulate = trainer.fit_loop._should_accumulate()\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_ready()\r\n\r\n        call._call_lightning_module_hook(\r\n            trainer,\r\n            \"optimizer_step\",\r\n            trainer.current_epoch,\r\n            batch_idx,\r\n            optimizer,\r\n            train_step_and_backward_closure,\r\n        )\r\n\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_completed()", "language": "python", "code": "def _optimizer_step(\r\n        self,\r\n        batch_idx: int,\r\n        train_step_and_backward_closure: Callable[[], Optional[Tensor]],\r\n    ) -> None:\r\n        \"\"\"Performs the optimizer step and some sanity checking.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            train_step_and_backward_closure: the closure function performing the train step and computing the\r\n                gradients. By default, called by the optimizer (if possible)\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        optimizer = trainer.strategy._lightning_optimizers[0]\r\n\r\n        should_accumulate = trainer.fit_loop._should_accumulate()\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_ready()\r\n\r\n        call._call_lightning_module_hook(\r\n            trainer,\r\n            \"optimizer_step\",\r\n            trainer.current_epoch,\r\n            batch_idx,\r\n            optimizer,\r\n            train_step_and_backward_closure,\r\n        )\r\n\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_completed()", "code_tokens": ["def", "_optimizer_step", "(", "self", ",", "batch_idx", ":", "int", ",", "train_step_and_backward_closure", ":", "Callable", "[", "[", "]", ",", "Optional", "[", "Tensor", "]", "]", ",", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "optimizer", "=", "trainer", ".", "strategy", ".", "_lightning_optimizers", "[", "0", "]", "should_accumulate", "=", "trainer", ".", "fit_loop", ".", "_should_accumulate", "(", ")", "if", "not", "should_accumulate", ":", "self", ".", "optim_progress", ".", "optimizer", ".", "step", ".", "increment_ready", "(", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "trainer", ".", "current_epoch", ",", "batch_idx", ",", "optimizer", ",", "train_step_and_backward_closure", ",", ")", "if", "not", "should_accumulate", ":", "self", ".", "optim_progress", ".", "optimizer", ".", "step", ".", "increment_completed", "(", ")"], "docstring": "Performs the optimizer step and some sanity checking.", "docstring_tokens": ["performs", "the", "optimizer", "step", "and", "some", "sanity", "checking"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 244, "end_line": 279, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_759", "original_string": "def _on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Calls the ``on_before_zero_grad`` hook.\r\n\r\n        Args:\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        self.optim_progress.optimizer.zero_grad.increment_ready()\r\n        call._call_callback_hooks(trainer, \"on_before_zero_grad\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_zero_grad\", optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_started()", "language": "python", "code": "def _on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Calls the ``on_before_zero_grad`` hook.\r\n\r\n        Args:\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        self.optim_progress.optimizer.zero_grad.increment_ready()\r\n        call._call_callback_hooks(trainer, \"on_before_zero_grad\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_zero_grad\", optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_started()", "code_tokens": ["def", "_on_before_zero_grad", "(", "self", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_ready", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "optimizer", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "optimizer", ")", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_started", "(", ")"], "docstring": "Calls the ``on_before_zero_grad`` hook.", "docstring_tokens": ["calls", "the", "on_before_zero_grad", "hook"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 281, "end_line": 292, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_760", "original_string": "def _optimizer_zero_grad(self, batch_idx: int, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Zeroes out all gradients of parameters optimized by the current optimizer.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_lightning_module_hook(trainer, \"optimizer_zero_grad\", trainer.current_epoch, batch_idx, optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_completed()", "language": "python", "code": "def _optimizer_zero_grad(self, batch_idx: int, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Zeroes out all gradients of parameters optimized by the current optimizer.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_lightning_module_hook(trainer, \"optimizer_zero_grad\", trainer.current_epoch, batch_idx, optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_completed()", "code_tokens": ["def", "_optimizer_zero_grad", "(", "self", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "trainer", ".", "current_epoch", ",", "batch_idx", ",", "optimizer", ")", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_completed", "(", ")"], "docstring": "Zeroes out all gradients of parameters optimized by the current optimizer.", "docstring_tokens": ["zeroes", "out", "all", "gradients", "of", "parameters", "optimized", "by", "the", "current", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 294, "end_line": 304, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "function_761", "original_string": "def _training_step(self, kwargs: OrderedDict) -> ClosureResult:\r\n        \"\"\"Performs the actual train step with the tied hooks.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks.\r\n\r\n        Returns:\r\n            A ``ClosureResult`` containing the training step output.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n\r\n        if training_step_output is None and trainer.world_size > 1:\r\n            raise RuntimeError(\r\n                \"Skipping the `training_step` by returning None in distributed training is not supported.\"\r\n                \" It is recommended that you rewrite your training logic to avoid having to skip the step in the first\"\r\n                \" place.\"\r\n            )\r\n\r\n        return self.output_result_cls.from_training_step_output(training_step_output, trainer.accumulate_grad_batches)", "language": "python", "code": "def _training_step(self, kwargs: OrderedDict) -> ClosureResult:\r\n        \"\"\"Performs the actual train step with the tied hooks.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks.\r\n\r\n        Returns:\r\n            A ``ClosureResult`` containing the training step output.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n\r\n        if training_step_output is None and trainer.world_size > 1:\r\n            raise RuntimeError(\r\n                \"Skipping the `training_step` by returning None in distributed training is not supported.\"\r\n                \" It is recommended that you rewrite your training logic to avoid having to skip the step in the first\"\r\n                \" place.\"\r\n            )\r\n\r\n        return self.output_result_cls.from_training_step_output(training_step_output, trainer.accumulate_grad_batches)", "code_tokens": ["def", "_training_step", "(", "self", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "ClosureResult", ":", "STRING", "trainer", "=", "self", ".", "trainer", "training_step_output", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "*", "kwargs", ".", "values", "(", ")", ")", "self", ".", "trainer", ".", "strategy", ".", "post_training_step", "(", ")", "#", "unused", "hook", "-", "call", "anyway", "for", "backward", "compatibility", "if", "training_step_output", "is", "None", "and", "trainer", ".", "world_size", ">", "1", ":", "raise", "RuntimeError", "(", "STRING", "STRING", "STRING", ")", "return", "self", ".", "output_result_cls", ".", "from_training_step_output", "(", "training_step_output", ",", "trainer", ".", "accumulate_grad_batches", ")"], "docstring": "Performs the actual train step with the tied hooks.", "docstring_tokens": ["performs", "the", "actual", "train", "step", "with", "the", "tied", "hooks"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "start_line": 306, "end_line": 328, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\closure.py", "func_name": "function_762", "original_string": "def consume_result(self) -> T:\r\n        \"\"\"The cached result from the last time the closure was called.\r\n\r\n        Once accessed, the internal reference gets reset and the consumer will have to hold on to the reference as long\r\n        as necessary.\r\n\r\n        \"\"\"\r\n        if self._result is None:\r\n            raise MisconfigurationException(\r\n                \"The closure hasn't been executed.\"\r\n                \" HINT: did you call `optimizer_closure()` in your `optimizer_step` hook? It could also happen because\"\r\n                \" the `optimizer.step(optimizer_closure)` call did not execute it internally.\"\r\n            )\r\n        result, self._result = self._result, None  # free memory\r\n        return result", "language": "python", "code": "def consume_result(self) -> T:\r\n        \"\"\"The cached result from the last time the closure was called.\r\n\r\n        Once accessed, the internal reference gets reset and the consumer will have to hold on to the reference as long\r\n        as necessary.\r\n\r\n        \"\"\"\r\n        if self._result is None:\r\n            raise MisconfigurationException(\r\n                \"The closure hasn't been executed.\"\r\n                \" HINT: did you call `optimizer_closure()` in your `optimizer_step` hook? It could also happen because\"\r\n                \" the `optimizer.step(optimizer_closure)` call did not execute it internally.\"\r\n            )\r\n        result, self._result = self._result, None  # free memory\r\n        return result", "code_tokens": ["def", "consume_result", "(", "self", ")", "-", ">", "T", ":", "STRING", "if", "self", ".", "_result", "is", "None", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", "STRING", ")", "result", ",", "self", ".", "_result", "=", "self", ".", "_result", ",", "None", "#", "free", "memory", "return", "result"], "docstring": "The cached result from the last time the closure was called.", "docstring_tokens": ["the", "cached", "result", "from", "the", "last", "time", "the", "closure", "was", "called"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\closure.py", "start_line": 44, "end_line": 58, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\closure.py", "func_name": "function_763", "original_string": "def closure(self, *args: Any, **kwargs: Any) -> T:\r\n        \"\"\"Implements the behavior of the closure once it is getting called.\"\"\"\r\n        pass", "language": "python", "code": "def closure(self, *args: Any, **kwargs: Any) -> T:\r\n        \"\"\"Implements the behavior of the closure once it is getting called.\"\"\"\r\n        pass", "code_tokens": ["def", "closure", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "T", ":", "STRING", "pass"], "docstring": "Implements the behavior of the closure once it is getting called.", "docstring_tokens": ["implements", "the", "behavior", "of", "the", "closure", "once", "it", "is", "getting", "called"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\closure.py", "start_line": 61, "end_line": 63, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "func_name": "function_764", "original_string": "def advance(self, kwargs: OrderedDict) -> None:\r\n        \"\"\"Performs the training step for manual optimization.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        del kwargs  # release the batch from memory\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n        result = self.output_result_cls.from_training_step_output(training_step_output)\r\n\r\n        self._output = result.asdict()", "language": "python", "code": "def advance(self, kwargs: OrderedDict) -> None:\r\n        \"\"\"Performs the training step for manual optimization.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        del kwargs  # release the batch from memory\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n        result = self.output_result_cls.from_training_step_output(training_step_output)\r\n\r\n        self._output = result.asdict()", "code_tokens": ["def", "advance", "(", "self", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "training_step_output", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "STRING", ",", "*", "kwargs", ".", "values", "(", ")", ")", "del", "kwargs", "#", "release", "the", "batch", "from", "memory", "self", ".", "trainer", ".", "strategy", ".", "post_training_step", "(", ")", "#", "unused", "hook", "-", "call", "anyway", "for", "backward", "compatibility", "result", "=", "self", ".", "output_result_cls", ".", "from_training_step_output", "(", "training_step_output", ")", "self", ".", "_output", "=", "result", ".", "asdict", "(", ")"], "docstring": "Performs the training step for manual optimization.", "docstring_tokens": ["performs", "the", "training", "step", "for", "manual", "optimization"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "start_line": 104, "end_line": 119, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "func_name": "function_765", "original_string": "def on_run_end(self) -> _OUTPUTS_TYPE:\r\n        \"\"\"Returns the result of this loop, i.e., the post-processed outputs from the training step.\"\"\"\r\n        output, self._output = self._output, {}  # free memory\r\n        for lightning_optimizer in self.trainer.strategy._lightning_optimizers:\r\n            lightning_optimizer._on_before_step = do_nothing_closure\r\n            lightning_optimizer._on_after_step = do_nothing_closure\r\n        return output", "language": "python", "code": "def on_run_end(self) -> _OUTPUTS_TYPE:\r\n        \"\"\"Returns the result of this loop, i.e., the post-processed outputs from the training step.\"\"\"\r\n        output, self._output = self._output, {}  # free memory\r\n        for lightning_optimizer in self.trainer.strategy._lightning_optimizers:\r\n            lightning_optimizer._on_before_step = do_nothing_closure\r\n            lightning_optimizer._on_after_step = do_nothing_closure\r\n        return output", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "_OUTPUTS_TYPE", ":", "STRING", "output", ",", "self", ".", "_output", "=", "self", ".", "_output", ",", "{", "}", "#", "free", "memory", "for", "lightning_optimizer", "in", "self", ".", "trainer", ".", "strategy", ".", "_lightning_optimizers", ":", "lightning_optimizer", ".", "_on_before_step", "=", "do_nothing_closure", "lightning_optimizer", ".", "_on_after_step", "=", "do_nothing_closure", "return", "output"], "docstring": "Returns the result of this loop, i.e., the post-processed outputs from the training step.", "docstring_tokens": ["returns", "the", "result", "of", "this", "loop", "i", "e", "the", "post", "processed", "outputs", "from", "the", "training", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "start_line": 121, "end_line": 128, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "function_766", "original_string": "def _find_tensors(\r\n    obj: Union[Tensor, list, tuple, dict, Any],\r\n) -> Union[list[Tensor], itertools.chain]:  # pragma: no-cover\r\n    \"\"\"Recursively find all tensors contained in the specified object.\"\"\"\r\n    if isinstance(obj, Tensor):\r\n        return [obj]\r\n    if isinstance(obj, (list, tuple)):\r\n        return itertools.chain(*map(_find_tensors, obj))\r\n    if isinstance(obj, dict):\r\n        return itertools.chain(*map(_find_tensors, obj.values()))\r\n    return []", "language": "python", "code": "def _find_tensors(\r\n    obj: Union[Tensor, list, tuple, dict, Any],\r\n) -> Union[list[Tensor], itertools.chain]:  # pragma: no-cover\r\n    \"\"\"Recursively find all tensors contained in the specified object.\"\"\"\r\n    if isinstance(obj, Tensor):\r\n        return [obj]\r\n    if isinstance(obj, (list, tuple)):\r\n        return itertools.chain(*map(_find_tensors, obj))\r\n    if isinstance(obj, dict):\r\n        return itertools.chain(*map(_find_tensors, obj.values()))\r\n    return []", "code_tokens": ["def", "_find_tensors", "(", "obj", ":", "Union", "[", "Tensor", ",", "list", ",", "tuple", ",", "dict", ",", "Any", "]", ",", ")", "-", ">", "Union", "[", "list", "[", "Tensor", "]", ",", "itertools", ".", "chain", "]", ":", "#", "pragma", ":", "no", "-", "cover", "STRING", "if", "isinstance", "(", "obj", ",", "Tensor", ")", ":", "return", "[", "obj", "]", "if", "isinstance", "(", "obj", ",", "(", "list", ",", "tuple", ")", ")", ":", "return", "itertools", ".", "chain", "(", "*", "map", "(", "_find_tensors", ",", "obj", ")", ")", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "return", "itertools", ".", "chain", "(", "*", "map", "(", "_find_tensors", ",", "obj", ".", "values", "(", ")", ")", ")", "return", "[", "]"], "docstring": "Recursively find all tensors contained in the specified object.", "docstring_tokens": ["recursively", "find", "all", "tensors", "contained", "in", "the", "specified", "object"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\overrides\\distributed.py", "start_line": 28, "end_line": 38, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "function_767", "original_string": "def _register_ddp_comm_hook(\r\n    model: DistributedDataParallel,\r\n    ddp_comm_state: Optional[object] = None,\r\n    ddp_comm_hook: Optional[Callable] = None,\r\n    ddp_comm_wrapper: Optional[Callable] = None,\r\n) -> None:\r\n    \"\"\"Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.\r\n\r\n    Args:\r\n        model:\r\n            DDP model\r\n        ddp_comm_state:\r\n            state is passed to the hook and can be used to maintain\r\n            and update any state information that users would like to\r\n            maintain as part of the training process. Examples: error\r\n            feedback in gradient compression, peers to communicate with\r\n            next in GossipGrad etc.\r\n        ddp_comm_hook:\r\n            hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future\r\n\r\n            This callable function is called once the bucket is ready. The\r\n            hook can perform whatever processing is needed and return\r\n            a Future indicating completion of any async work (ex: allreduce).\r\n            If the hook doesn't perform any communication, it can also\r\n            just return a completed Future. The Future should hold the\r\n            new value of grad bucket's tensors. Once a bucket is ready,\r\n            c10d reducer would call this hook and use the tensors returned\r\n            by the Future and copy grads to individual parameters.\r\n        ddp_comm_wrapper:\r\n            communication hook wrapper to support a communication hook such\r\n            as FP16 compression as wrapper, which could be combined with\r\n            ddp_comm_hook\r\n\r\n    Examples::\r\n\r\n        from torch.distributed.algorithms.ddp_comm_hooks import (\r\n            default_hooks as default,\r\n            powerSGD_hook as powerSGD,\r\n            post_localSGD_hook as post_localSGD,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_hook=default.fp16_compress_hook,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n        )\r\n\r\n        subgroup, _ = torch.distributed.new_subgroups()\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            state=post_localSGD.PostLocalSGDState(\r\n                process_group=None,\r\n                subgroup=subgroup,\r\n                start_localSGD_iter=1_000,\r\n            ),\r\n            ddp_comm_hook=post_localSGD.post_localSGD_hook,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n            ddp_comm_wrapper=default.fp16_compress_wrapper,\r\n        )\r\n\r\n    \"\"\"\r\n    if ddp_comm_hook is None:\r\n        return\r\n    ddp_comm_hook: Callable = ddp_comm_hook\r\n\r\n    if ddp_comm_wrapper is not None:\r\n        rank_zero_info(\r\n            f\"DDP comm wrapper is provided, apply {ddp_comm_wrapper.__qualname__}({ddp_comm_hook.__qualname__}).\"\r\n        )\r\n        ddp_comm_hook = ddp_comm_wrapper(ddp_comm_hook)\r\n\r\n    rank_zero_debug(f\"Registering DDP comm hook: {ddp_comm_hook.__qualname__}.\")\r\n    model.register_comm_hook(state=ddp_comm_state, hook=ddp_comm_hook)", "language": "python", "code": "def _register_ddp_comm_hook(\r\n    model: DistributedDataParallel,\r\n    ddp_comm_state: Optional[object] = None,\r\n    ddp_comm_hook: Optional[Callable] = None,\r\n    ddp_comm_wrapper: Optional[Callable] = None,\r\n) -> None:\r\n    \"\"\"Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.\r\n\r\n    Args:\r\n        model:\r\n            DDP model\r\n        ddp_comm_state:\r\n            state is passed to the hook and can be used to maintain\r\n            and update any state information that users would like to\r\n            maintain as part of the training process. Examples: error\r\n            feedback in gradient compression, peers to communicate with\r\n            next in GossipGrad etc.\r\n        ddp_comm_hook:\r\n            hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future\r\n\r\n            This callable function is called once the bucket is ready. The\r\n            hook can perform whatever processing is needed and return\r\n            a Future indicating completion of any async work (ex: allreduce).\r\n            If the hook doesn't perform any communication, it can also\r\n            just return a completed Future. The Future should hold the\r\n            new value of grad bucket's tensors. Once a bucket is ready,\r\n            c10d reducer would call this hook and use the tensors returned\r\n            by the Future and copy grads to individual parameters.\r\n        ddp_comm_wrapper:\r\n            communication hook wrapper to support a communication hook such\r\n            as FP16 compression as wrapper, which could be combined with\r\n            ddp_comm_hook\r\n\r\n    Examples::\r\n\r\n        from torch.distributed.algorithms.ddp_comm_hooks import (\r\n            default_hooks as default,\r\n            powerSGD_hook as powerSGD,\r\n            post_localSGD_hook as post_localSGD,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_hook=default.fp16_compress_hook,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n        )\r\n\r\n        subgroup, _ = torch.distributed.new_subgroups()\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            state=post_localSGD.PostLocalSGDState(\r\n                process_group=None,\r\n                subgroup=subgroup,\r\n                start_localSGD_iter=1_000,\r\n            ),\r\n            ddp_comm_hook=post_localSGD.post_localSGD_hook,\r\n        )\r\n\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n            ddp_comm_wrapper=default.fp16_compress_wrapper,\r\n        )\r\n\r\n    \"\"\"\r\n    if ddp_comm_hook is None:\r\n        return\r\n    ddp_comm_hook: Callable = ddp_comm_hook\r\n\r\n    if ddp_comm_wrapper is not None:\r\n        rank_zero_info(\r\n            f\"DDP comm wrapper is provided, apply {ddp_comm_wrapper.__qualname__}({ddp_comm_hook.__qualname__}).\"\r\n        )\r\n        ddp_comm_hook = ddp_comm_wrapper(ddp_comm_hook)\r\n\r\n    rank_zero_debug(f\"Registering DDP comm hook: {ddp_comm_hook.__qualname__}.\")\r\n    model.register_comm_hook(state=ddp_comm_state, hook=ddp_comm_hook)", "code_tokens": ["def", "_register_ddp_comm_hook", "(", "model", ":", "DistributedDataParallel", ",", "ddp_comm_state", ":", "Optional", "[", "object", "]", "=", "None", ",", "ddp_comm_hook", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "ddp_comm_wrapper", ":", "Optional", "[", "Callable", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "ddp_comm_hook", "is", "None", ":", "return", "ddp_comm_hook", ":", "Callable", "=", "ddp_comm_hook", "if", "ddp_comm_wrapper", "is", "not", "None", ":", "rank_zero_info", "(", "fSTRING", ")", "ddp_comm_hook", "=", "ddp_comm_wrapper", "(", "ddp_comm_hook", ")", "rank_zero_debug", "(", "fSTRING", ")", "model", ".", "register_comm_hook", "(", "state", "=", "ddp_comm_state", ",", "hook", "=", "ddp_comm_hook", ")"], "docstring": "Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.", "docstring_tokens": ["function", "to", "register", "communication", "hook", "for", "ddp", "model", "https", "pytorch", "org", "docs", "master", "ddp_comm_hooks", "html"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\overrides\\distributed.py", "start_line": 61, "end_line": 160, "has_examples": true, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "function_768", "original_string": "def _sync_module_states(module: torch.nn.Module) -> None:\r\n    \"\"\"Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.\"\"\"\r\n    parameters_to_ignore = set(getattr(module, \"_ddp_params_and_buffers_to_ignore\", []))\r\n    from torch.distributed.distributed_c10d import _get_default_group\r\n    from torch.distributed.utils import _sync_module_states as torch_sync_module_states\r\n\r\n    torch_sync_module_states(\r\n        module,\r\n        _get_default_group(),\r\n        250 * 1024 * 1024,\r\n        src=0,\r\n        params_and_buffers_to_ignore=parameters_to_ignore,\r\n    )", "language": "python", "code": "def _sync_module_states(module: torch.nn.Module) -> None:\r\n    \"\"\"Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.\"\"\"\r\n    parameters_to_ignore = set(getattr(module, \"_ddp_params_and_buffers_to_ignore\", []))\r\n    from torch.distributed.distributed_c10d import _get_default_group\r\n    from torch.distributed.utils import _sync_module_states as torch_sync_module_states\r\n\r\n    torch_sync_module_states(\r\n        module,\r\n        _get_default_group(),\r\n        250 * 1024 * 1024,\r\n        src=0,\r\n        params_and_buffers_to_ignore=parameters_to_ignore,\r\n    )", "code_tokens": ["def", "_sync_module_states", "(", "module", ":", "torch", ".", "nn", ".", "Module", ")", "-", ">", "None", ":", "STRING", "parameters_to_ignore", "=", "set", "(", "getattr", "(", "module", ",", "STRING", ",", "[", "]", ")", ")", "from", "torch", ".", "distributed", ".", "distributed_c10d", "import", "_get_default_group", "from", "torch", ".", "distributed", ".", "utils", "import", "_sync_module_states", "as", "torch_sync_module_states", "torch_sync_module_states", "(", "module", ",", "_get_default_group", "(", ")", ",", "250", "*", "1024", "*", "1024", ",", "src", "=", "0", ",", "params_and_buffers_to_ignore", "=", "parameters_to_ignore", ",", ")"], "docstring": "Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.", "docstring_tokens": ["taken", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v2", "0", "0", "torch", "nn", "parallel", "distributed", "py", "l675", "l682"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\overrides\\distributed.py", "start_line": 163, "end_line": 175, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "function_769", "original_string": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Override this method to apply synchronization to the layers of this model.\"\"\"", "language": "python", "code": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Override this method to apply synchronization to the layers of this model.\"\"\"", "code_tokens": ["def", "apply", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING"], "docstring": "Override this method to apply synchronization to the layers of this model.", "docstring_tokens": ["override", "this", "method", "to", "apply", "synchronization", "to", "the", "layers", "of", "this", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "start_line": 27, "end_line": 28, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "function_770", "original_string": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Override this method to undo all modifications made in :meth:`apply`.\"\"\"", "language": "python", "code": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Override this method to undo all modifications made in :meth:`apply`.\"\"\"", "code_tokens": ["def", "revert", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING"], "docstring": "Override this method to undo all modifications made in :meth:`apply`.", "docstring_tokens": ["override", "this", "method", "to", "undo", "all", "modifications", "made", "in", "meth", "apply"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "start_line": 31, "end_line": 32, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "function_771", "original_string": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Add global batchnorm for a model spread across multiple GPUs and nodes.\r\n\r\n        Override this method to synchronize batchnorm layers between specific process groups instead\r\n        of the whole world.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with batchnorm layers synchronized within the process groups.\r\n\r\n        \"\"\"\r\n        return torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)", "language": "python", "code": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Add global batchnorm for a model spread across multiple GPUs and nodes.\r\n\r\n        Override this method to synchronize batchnorm layers between specific process groups instead\r\n        of the whole world.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with batchnorm layers synchronized within the process groups.\r\n\r\n        \"\"\"\r\n        return torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)", "code_tokens": ["def", "apply", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "return", "torch", ".", "nn", ".", "SyncBatchNorm", ".", "convert_sync_batchnorm", "(", "model", ")"], "docstring": "Add global batchnorm for a model spread across multiple GPUs and nodes.", "docstring_tokens": ["add", "global", "batchnorm", "for", "a", "model", "spread", "across", "multiple", "gpus", "and", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "start_line": 43, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "function_772", "original_string": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Convert the wrapped batchnorm layers back to regular batchnorm layers.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with regular batchnorm layers that will no longer sync across processes.\r\n\r\n        \"\"\"\r\n        converted_module = model\r\n        if isinstance(model, torch.nn.modules.batchnorm.SyncBatchNorm):\r\n            converted_module = _BatchNormXd(\r\n                model.num_features, model.eps, model.momentum, model.affine, model.track_running_stats\r\n            )\r\n            if model.affine:\r\n                with torch.no_grad():\r\n                    converted_module.weight = model.weight\r\n                    converted_module.bias = model.bias\r\n            converted_module.running_mean = model.running_mean\r\n            converted_module.running_var = model.running_var\r\n            converted_module.num_batches_tracked = model.num_batches_tracked\r\n            if hasattr(model, \"qconfig\"):\r\n                converted_module.qconfig = model.qconfig\r\n        for name, child in model.named_children():\r\n            converted_module.add_module(name, self.revert(child))\r\n        del model\r\n        return converted_module", "language": "python", "code": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Convert the wrapped batchnorm layers back to regular batchnorm layers.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with regular batchnorm layers that will no longer sync across processes.\r\n\r\n        \"\"\"\r\n        converted_module = model\r\n        if isinstance(model, torch.nn.modules.batchnorm.SyncBatchNorm):\r\n            converted_module = _BatchNormXd(\r\n                model.num_features, model.eps, model.momentum, model.affine, model.track_running_stats\r\n            )\r\n            if model.affine:\r\n                with torch.no_grad():\r\n                    converted_module.weight = model.weight\r\n                    converted_module.bias = model.bias\r\n            converted_module.running_mean = model.running_mean\r\n            converted_module.running_var = model.running_var\r\n            converted_module.num_batches_tracked = model.num_batches_tracked\r\n            if hasattr(model, \"qconfig\"):\r\n                converted_module.qconfig = model.qconfig\r\n        for name, child in model.named_children():\r\n            converted_module.add_module(name, self.revert(child))\r\n        del model\r\n        return converted_module", "code_tokens": ["def", "revert", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "converted_module", "=", "model", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ":", "converted_module", "=", "_BatchNormXd", "(", "model", ".", "num_features", ",", "model", ".", "eps", ",", "model", ".", "momentum", ",", "model", ".", "affine", ",", "model", ".", "track_running_stats", ")", "if", "model", ".", "affine", ":", "with", "torch", ".", "no_grad", "(", ")", ":", "converted_module", ".", "weight", "=", "model", ".", "weight", "converted_module", ".", "bias", "=", "model", ".", "bias", "converted_module", ".", "running_mean", "=", "model", ".", "running_mean", "converted_module", ".", "running_var", "=", "model", ".", "running_var", "converted_module", ".", "num_batches_tracked", "=", "model", ".", "num_batches_tracked", "if", "hasattr", "(", "model", ",", "STRING", ")", ":", "converted_module", ".", "qconfig", "=", "model", ".", "qconfig", "for", "name", ",", "child", "in", "model", ".", "named_children", "(", ")", ":", "converted_module", ".", "add_module", "(", "name", ",", "self", ".", "revert", "(", "child", ")", ")", "del", "model", "return", "converted_module"], "docstring": "Convert the wrapped batchnorm layers back to regular batchnorm layers.", "docstring_tokens": ["convert", "the", "wrapped", "batchnorm", "layers", "back", "to", "regular", "batchnorm", "layers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "start_line": 59, "end_line": 90, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "function_773", "original_string": "def _ensure_setup(self) -> None:\r\n        \"\"\"Ensures that the executor is setup.\r\n\r\n        We can't do setup in __init__ because if train or validate is called more than once, the teardown method deletes\r\n        the executor.\r\n\r\n        \"\"\"\r\n        if self._executor is None:\r\n            self._executor = ThreadPoolExecutor(max_workers=1)", "language": "python", "code": "def _ensure_setup(self) -> None:\r\n        \"\"\"Ensures that the executor is setup.\r\n\r\n        We can't do setup in __init__ because if train or validate is called more than once, the teardown method deletes\r\n        the executor.\r\n\r\n        \"\"\"\r\n        if self._executor is None:\r\n            self._executor = ThreadPoolExecutor(max_workers=1)", "code_tokens": ["def", "_ensure_setup", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_executor", "is", "None", ":", "self", ".", "_executor", "=", "ThreadPoolExecutor", "(", "max_workers", "=", "1", ")"], "docstring": "Ensures that the executor is setup.", "docstring_tokens": ["ensures", "that", "the", "executor", "is", "setup"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "start_line": 46, "end_line": 54, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "function_774", "original_string": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.\"\"\"\r\n\r\n        self._ensure_setup()\r\n\r\n        if \"checkpoint\" in kwargs:\r\n            kwargs = {**kwargs, \"checkpoint\": apply_to_collection(kwargs[\"checkpoint\"], torch.Tensor, _clone_tensor)}\r\n        elif len(args) >= 1:\r\n            args = (apply_to_collection(args[0], torch.Tensor, _clone_tensor), *args[1:])\r\n\r\n        def _save_checkpoint(*args: Any, **kwargs: Any) -> None:\r\n            try:\r\n                assert self.checkpoint_io is not None\r\n                self.checkpoint_io.save_checkpoint(*args, **kwargs)\r\n            except BaseException as ex:\r\n                self._error = ex\r\n\r\n        assert self._executor is not None\r\n        self._executor.submit(_save_checkpoint, *args, **kwargs)\r\n\r\n        if self._error:\r\n            raise self._error", "language": "python", "code": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.\"\"\"\r\n\r\n        self._ensure_setup()\r\n\r\n        if \"checkpoint\" in kwargs:\r\n            kwargs = {**kwargs, \"checkpoint\": apply_to_collection(kwargs[\"checkpoint\"], torch.Tensor, _clone_tensor)}\r\n        elif len(args) >= 1:\r\n            args = (apply_to_collection(args[0], torch.Tensor, _clone_tensor), *args[1:])\r\n\r\n        def _save_checkpoint(*args: Any, **kwargs: Any) -> None:\r\n            try:\r\n                assert self.checkpoint_io is not None\r\n                self.checkpoint_io.save_checkpoint(*args, **kwargs)\r\n            except BaseException as ex:\r\n                self._error = ex\r\n\r\n        assert self._executor is not None\r\n        self._executor.submit(_save_checkpoint, *args, **kwargs)\r\n\r\n        if self._error:\r\n            raise self._error", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "self", ".", "_ensure_setup", "(", ")", "if", "STRING", "in", "kwargs", ":", "kwargs", "=", "{", "*", "*", "kwargs", ",", "STRING", ":", "apply_to_collection", "(", "kwargs", "[", "STRING", "]", ",", "torch", ".", "Tensor", ",", "_clone_tensor", ")", "}", "elif", "len", "(", "args", ")", ">", "=", "1", ":", "args", "=", "(", "apply_to_collection", "(", "args", "[", "0", "]", ",", "torch", ".", "Tensor", ",", "_clone_tensor", ")", ",", "*", "args", "[", "1", ":", "]", ")", "def", "_save_checkpoint", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "try", ":", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")", "except", "BaseException", "as", "ex", ":", "self", ".", "_error", "=", "ex", "assert", "self", ".", "_executor", "is", "not", "None", "self", ".", "_executor", ".", "submit", "(", "_save_checkpoint", ",", "*", "args", ",", "*", "*", "kwargs", ")", "if", "self", ".", "_error", ":", "raise", "self", ".", "_error"], "docstring": "Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.", "docstring_tokens": ["uses", "the", "threadpoolexecutor", "to", "save", "the", "checkpoints", "using", "the", "base", "checkpoint_io"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "start_line": 57, "end_line": 81, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "function_775", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to close the threads.\"\"\"\r\n        if self._executor is not None:\r\n            self._executor.shutdown(wait=True)\r\n            self._executor = None\r\n\r\n        if self._error:\r\n            raise self._error", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to close the threads.\"\"\"\r\n        if self._executor is not None:\r\n            self._executor.shutdown(wait=True)\r\n            self._executor = None\r\n\r\n        if self._error:\r\n            raise self._error", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_executor", "is", "not", "None", ":", "self", ".", "_executor", ".", "shutdown", "(", "wait", "=", "True", ")", "self", ".", "_executor", "=", "None", "if", "self", ".", "_error", ":", "raise", "self", ".", "_error"], "docstring": "This method is called to close the threads.", "docstring_tokens": ["this", "method", "is", "called", "to", "close", "the", "threads"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "start_line": 84, "end_line": 92, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "function_776", "original_string": "def _clone_tensor(t: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Clones a tensor on the caller thread.\"\"\"\r\n    return t.detach().clone()", "language": "python", "code": "def _clone_tensor(t: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Clones a tensor on the caller thread.\"\"\"\r\n    return t.detach().clone()", "code_tokens": ["def", "_clone_tensor", "(", "t", ":", "torch", ".", "Tensor", ")", "-", ">", "torch", ".", "Tensor", ":", "STRING", "return", "t", ".", "detach", "(", ")", ".", "clone", "(", ")"], "docstring": "Clones a tensor on the caller thread.", "docstring_tokens": ["clones", "a", "tensor", "on", "the", "caller", "thread"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "start_line": 96, "end_line": 99, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "function_777", "original_string": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to save the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.save_checkpoint(*args, **kwargs)", "language": "python", "code": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to save the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.save_checkpoint(*args, **kwargs)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to save the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "save", "the", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "start_line": 56, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "function_778", "original_string": "def remove_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to remove the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.remove_checkpoint(*args, **kwargs)", "language": "python", "code": "def remove_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to remove the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.remove_checkpoint(*args, **kwargs)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to remove the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "remove", "the", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "start_line": 62, "end_line": 65, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "function_779", "original_string": "def load_checkpoint(self, *args: Any, **kwargs: Any) -> dict[str, Any]:\r\n        \"\"\"Uses the base ``checkpoint_io`` to load the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        return self.checkpoint_io.load_checkpoint(*args, **kwargs)", "language": "python", "code": "def load_checkpoint(self, *args: Any, **kwargs: Any) -> dict[str, Any]:\r\n        \"\"\"Uses the base ``checkpoint_io`` to load the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        return self.checkpoint_io.load_checkpoint(*args, **kwargs)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "return", "self", ".", "checkpoint_io", ".", "load_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to load the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "load", "the", "checkpoint"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "start_line": 68, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\amp.py", "func_name": "function_780", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Enable autocast context.\"\"\"\r\n        with self.autocast_context_manager():\r\n            yield", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Enable autocast context.\"\"\"\r\n        with self.autocast_context_manager():\r\n            yield", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "autocast_context_manager", "(", ")", ":", "yield"], "docstring": "Enable autocast context.", "docstring_tokens": ["enable", "autocast", "context"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\amp.py", "start_line": 119, "end_line": 122, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "func_name": "function_781", "original_string": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs back-propagation using DeepSpeed's engine.\r\n\r\n        Args:\r\n            tensor: the loss tensor\r\n            model: the model to be optimized\r\n            optimizer: ignored for DeepSpeed\r\n            \\*args: additional positional arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n            \\**kwargs: additional keyword arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n\r\n        \"\"\"\r\n        if is_overridden(\"backward\", model):\r\n            warning_cache.warn(\r\n                \"You have overridden the `LightningModule.backward` hook but it will be ignored since DeepSpeed handles\"\r\n                \" the backward logic internally.\"\r\n            )\r\n        deepspeed_engine: deepspeed.DeepSpeedEngine = model.trainer.model\r\n        deepspeed_engine.backward(tensor, *args, **kwargs)", "language": "python", "code": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs back-propagation using DeepSpeed's engine.\r\n\r\n        Args:\r\n            tensor: the loss tensor\r\n            model: the model to be optimized\r\n            optimizer: ignored for DeepSpeed\r\n            \\*args: additional positional arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n            \\**kwargs: additional keyword arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n\r\n        \"\"\"\r\n        if is_overridden(\"backward\", model):\r\n            warning_cache.warn(\r\n                \"You have overridden the `LightningModule.backward` hook but it will be ignored since DeepSpeed handles\"\r\n                \" the backward logic internally.\"\r\n            )\r\n        deepspeed_engine: deepspeed.DeepSpeedEngine = model.trainer.model\r\n        deepspeed_engine.backward(tensor, *args, **kwargs)", "code_tokens": ["def", "backward", "(", "#", "type", ":", "ignore", "[", "override", "]", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "STRING", ",", "optimizer", ":", "Optional", "[", "Steppable", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "rSTRING", "if", "is_overridden", "(", "STRING", ",", "model", ")", ":", "warning_cache", ".", "warn", "(", "STRING", "STRING", ")", "deepspeed_engine", ":", "deepspeed", ".", "DeepSpeedEngine", "=", "model", ".", "trainer", ".", "model", "deepspeed_engine", ".", "backward", "(", "tensor", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs back-propagation using DeepSpeed's engine.", "docstring_tokens": ["r", "performs", "back", "propagation", "using", "deepspeed", "s", "engine"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "start_line": 92, "end_line": 116, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "func_name": "function_782", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"DeepSpeed handles gradient clipping internally.\"\"\"", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"DeepSpeed handles gradient clipping internally.\"\"\"", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "0", ",", "gradient_clip_algorithm", ":", "GradClipAlgorithmType", "=", "GradClipAlgorithmType", ".", "NORM", ",", ")", "-", ">", "None", ":", "STRING"], "docstring": "DeepSpeed handles gradient clipping internally.", "docstring_tokens": ["deepspeed", "handles", "gradient", "clipping", "internally"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "start_line": 141, "end_line": 147, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\double.py", "func_name": "function_783", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type.\r\n\r\n        See: :func:`torch.set_default_dtype`\r\n\r\n        \"\"\"\r\n        with self.tensor_init_context():\r\n            yield", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type.\r\n\r\n        See: :func:`torch.set_default_dtype`\r\n\r\n        \"\"\"\r\n        with self.tensor_init_context():\r\n            yield", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "tensor_init_context", "(", ")", ":", "yield"], "docstring": "A context manager to change the default tensor type.", "docstring_tokens": ["a", "context", "manager", "to", "change", "the", "default", "tensor", "type"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\double.py", "start_line": 49, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\half.py", "func_name": "function_784", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type when tensors get created during the module's forward.\r\n\r\n        See: :meth:`torch.set_default_tensor_type`\r\n\r\n        \"\"\"\r\n        default_dtype = torch.get_default_dtype()\r\n        torch.set_default_dtype(self._desired_input_dtype)\r\n        try:\r\n            yield\r\n        finally:\r\n            torch.set_default_dtype(default_dtype)", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type when tensors get created during the module's forward.\r\n\r\n        See: :meth:`torch.set_default_tensor_type`\r\n\r\n        \"\"\"\r\n        default_dtype = torch.get_default_dtype()\r\n        torch.set_default_dtype(self._desired_input_dtype)\r\n        try:\r\n            yield\r\n        finally:\r\n            torch.set_default_dtype(default_dtype)", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "default_dtype", "=", "torch", ".", "get_default_dtype", "(", ")", "torch", ".", "set_default_dtype", "(", "self", ".", "_desired_input_dtype", ")", "try", ":", "yield", "finally", ":", "torch", ".", "set_default_dtype", "(", "default_dtype", ")"], "docstring": "A context manager to change the default tensor type when tensors get created during the module's forward.", "docstring_tokens": ["a", "context", "manager", "to", "change", "the", "default", "tensor", "type", "when", "tensors", "get", "created", "during", "the", "module", "s", "forward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\half.py", "start_line": 55, "end_line": 66, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_785", "original_string": "def connect(\r\n        self, model: Module, optimizers: list[Optimizer], lr_schedulers: list[Any]\r\n    ) -> tuple[Module, list[Optimizer], list[Any]]:\r\n        \"\"\"Connects this plugin to the accelerator and the training process.\"\"\"\r\n        return model, optimizers, lr_schedulers", "language": "python", "code": "def connect(\r\n        self, model: Module, optimizers: list[Optimizer], lr_schedulers: list[Any]\r\n    ) -> tuple[Module, list[Optimizer], list[Any]]:\r\n        \"\"\"Connects this plugin to the accelerator and the training process.\"\"\"\r\n        return model, optimizers, lr_schedulers", "code_tokens": ["def", "connect", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "lr_schedulers", ":", "list", "[", "Any", "]", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "list", "[", "Any", "]", "]", ":", "STRING", "return", "model", ",", "optimizers", ",", "lr_schedulers"], "docstring": "Connects this plugin to the accelerator and the training process.", "docstring_tokens": ["connects", "this", "plugin", "to", "the", "accelerator", "and", "the", "training", "process"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 39, "end_line": 43, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_786", "original_string": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: the loss value obtained from the closure\r\n            model: the model to be optimized\r\n            optimizer: current optimizer being used. ``None`` if using manual optimization\r\n            \\*args: Positional arguments intended for the actual function that performs the backward, like\r\n                :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "language": "python", "code": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: the loss value obtained from the closure\r\n            model: the model to be optimized\r\n            optimizer: current optimizer being used. ``None`` if using manual optimization\r\n            \\*args: Positional arguments intended for the actual function that performs the backward, like\r\n                :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "code_tokens": ["def", "backward", "(", "#", "type", ":", "ignore", "[", "override", "]", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "STRING", ",", "optimizer", ":", "Optional", "[", "Steppable", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "rSTRING", "model", ".", "backward", "(", "tensor", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs the actual backpropagation.", "docstring_tokens": ["r", "performs", "the", "actual", "backpropagation"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 53, "end_line": 72, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_787", "original_string": "def _after_closure(self, model: \"pl.LightningModule\", optimizer: Steppable) -> None:\r\n        \"\"\"Utility to share some code after the closure has been run.\"\"\"\r\n        trainer = model.trainer\r\n        call._call_callback_hooks(trainer, \"on_before_optimizer_step\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_optimizer_step\", optimizer)\r\n        self._clip_gradients(\r\n            model,\r\n            optimizer,\r\n            trainer.gradient_clip_val,\r\n            gradient_clip_algorithm=trainer.gradient_clip_algorithm,\r\n        )", "language": "python", "code": "def _after_closure(self, model: \"pl.LightningModule\", optimizer: Steppable) -> None:\r\n        \"\"\"Utility to share some code after the closure has been run.\"\"\"\r\n        trainer = model.trainer\r\n        call._call_callback_hooks(trainer, \"on_before_optimizer_step\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_optimizer_step\", optimizer)\r\n        self._clip_gradients(\r\n            model,\r\n            optimizer,\r\n            trainer.gradient_clip_val,\r\n            gradient_clip_algorithm=trainer.gradient_clip_algorithm,\r\n        )", "code_tokens": ["def", "_after_closure", "(", "self", ",", "model", ":", "STRING", ",", "optimizer", ":", "Steppable", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "model", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "STRING", ",", "optimizer", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "optimizer", ")", "self", ".", "_clip_gradients", "(", "model", ",", "optimizer", ",", "trainer", ".", "gradient_clip_val", ",", "gradient_clip_algorithm", "=", "trainer", ".", "gradient_clip_algorithm", ",", ")"], "docstring": "Utility to share some code after the closure has been run.", "docstring_tokens": ["utility", "to", "share", "some", "code", "after", "the", "closure", "has", "been", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 83, "end_line": 93, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_788", "original_string": "def _wrap_closure(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Steppable,\r\n        closure: Callable[[], Any],\r\n    ) -> Any:\r\n        \"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\r\n        hook is called.\r\n\r\n        The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\r\n        consistent with the ``Precision`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\r\n\r\n        \"\"\"\r\n        closure_result = closure()\r\n        self._after_closure(model, optimizer)\r\n        return closure_result", "language": "python", "code": "def _wrap_closure(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Steppable,\r\n        closure: Callable[[], Any],\r\n    ) -> Any:\r\n        \"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\r\n        hook is called.\r\n\r\n        The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\r\n        consistent with the ``Precision`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\r\n\r\n        \"\"\"\r\n        closure_result = closure()\r\n        self._after_closure(model, optimizer)\r\n        return closure_result", "code_tokens": ["def", "_wrap_closure", "(", "self", ",", "model", ":", "STRING", ",", "optimizer", ":", "Steppable", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", ")", "-", ">", "Any", ":", "STRING", "closure_result", "=", "closure", "(", ")", "self", ".", "_after_closure", "(", "model", ",", "optimizer", ")", "return", "closure_result"], "docstring": "This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``", "docstring_tokens": ["this", "double", "closure", "allows", "makes", "sure", "the", "closure", "is", "executed", "before", "the", "on_before_optimizer_step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 95, "end_line": 110, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_789", "original_string": "def optimizer_step(  # type: ignore[override]\r\n        self,\r\n        optimizer: Steppable,\r\n        model: \"pl.LightningModule\",\r\n        closure: Callable[[], Any],\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        closure = partial(self._wrap_closure, model, optimizer, closure)\r\n        return optimizer.step(closure=closure, **kwargs)", "language": "python", "code": "def optimizer_step(  # type: ignore[override]\r\n        self,\r\n        optimizer: Steppable,\r\n        model: \"pl.LightningModule\",\r\n        closure: Callable[[], Any],\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        closure = partial(self._wrap_closure, model, optimizer, closure)\r\n        return optimizer.step(closure=closure, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "#", "type", ":", "ignore", "[", "override", "]", "self", ",", "optimizer", ":", "Steppable", ",", "model", ":", "STRING", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "STRING", "closure", "=", "partial", "(", "self", ".", "_wrap_closure", ",", "model", ",", "optimizer", ",", "closure", ")", "return", "optimizer", ".", "step", "(", "closure", "=", "closure", ",", "*", "*", "kwargs", ")"], "docstring": "Hook to run the optimizer step.", "docstring_tokens": ["hook", "to", "run", "the", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 113, "end_line": 122, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_790", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"Clips the gradients.\"\"\"\r\n        if clip_val <= 0:\r\n            return\r\n        if gradient_clip_algorithm == GradClipAlgorithmType.VALUE:\r\n            self.clip_grad_by_value(optimizer, clip_val)\r\n        elif gradient_clip_algorithm == GradClipAlgorithmType.NORM:\r\n            self.clip_grad_by_norm(optimizer, clip_val)", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"Clips the gradients.\"\"\"\r\n        if clip_val <= 0:\r\n            return\r\n        if gradient_clip_algorithm == GradClipAlgorithmType.VALUE:\r\n            self.clip_grad_by_value(optimizer, clip_val)\r\n        elif gradient_clip_algorithm == GradClipAlgorithmType.NORM:\r\n            self.clip_grad_by_norm(optimizer, clip_val)", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "0", ",", "gradient_clip_algorithm", ":", "GradClipAlgorithmType", "=", "GradClipAlgorithmType", ".", "NORM", ",", ")", "-", ">", "None", ":", "STRING", "if", "clip_val", "<", "=", "0", ":", "return", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", ":", "self", ".", "clip_grad_by_value", "(", "optimizer", ",", "clip_val", ")", "elif", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "NORM", ":", "self", ".", "clip_grad_by_norm", "(", "optimizer", ",", "clip_val", ")"], "docstring": "Clips the gradients.", "docstring_tokens": ["clips", "the", "gradients"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 143, "end_line": 155, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_791", "original_string": "def clip_grad_by_value(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "language": "python", "code": "def clip_grad_by_value(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "code_tokens": ["def", "clip_grad_by_value", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "STRING", "parameters", "=", "self", ".", "main_params", "(", "optimizer", ")", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "clip_value", "=", "clip_val", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 157, "end_line": 160, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_792", "original_string": "def clip_grad_by_norm(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_norm_(parameters, clip_val)", "language": "python", "code": "def clip_grad_by_norm(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_norm_(parameters, clip_val)", "code_tokens": ["def", "clip_grad_by_norm", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "STRING", "parameters", "=", "self", ".", "main_params", "(", "optimizer", ")", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "clip_val", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 162, "end_line": 165, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_793", "original_string": "def train_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the training step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def train_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the training step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "train_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the training step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "training", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 168, "end_line": 171, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_794", "original_string": "def val_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the validation step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def val_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the validation step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "val_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the validation step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "validation", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 174, "end_line": 177, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_795", "original_string": "def test_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the test step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def test_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the test step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "test_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the test step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "test", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 180, "end_line": 183, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "function_796", "original_string": "def predict_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the predict step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def predict_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the predict step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "predict_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the predict step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "predict", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "start_line": 186, "end_line": 189, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\advanced.py", "func_name": "function_797", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        line_count_restriction: float = 1.0,\r\n        dump_stats: bool = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            line_count_restriction: this can be used to limit the number of functions\r\n                reported for each action. either an integer (to select a count of lines),\r\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\r\n\r\n            dump_stats: Whether to save raw profiler results. When ``True`` then ``dirpath`` must be provided.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.profiled_actions: dict[str, cProfile.Profile] = defaultdict(cProfile.Profile)\r\n        self.line_count_restriction = line_count_restriction\r\n        self.dump_stats = dump_stats", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        line_count_restriction: float = 1.0,\r\n        dump_stats: bool = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            line_count_restriction: this can be used to limit the number of functions\r\n                reported for each action. either an integer (to select a count of lines),\r\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\r\n\r\n            dump_stats: Whether to save raw profiler results. When ``True`` then ``dirpath`` must be provided.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.profiled_actions: dict[str, cProfile.Profile] = defaultdict(cProfile.Profile)\r\n        self.line_count_restriction = line_count_restriction\r\n        self.dump_stats = dump_stats", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "line_count_restriction", ":", "float", "=", "1", ".", "0", ",", "dump_stats", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "profiled_actions", ":", "dict", "[", "str", ",", "cProfile", ".", "Profile", "]", "=", "defaultdict", "(", "cProfile", ".", "Profile", ")", "self", ".", "line_count_restriction", "=", "line_count_restriction", "self", ".", "dump_stats", "=", "dump_stats"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\advanced.py", "start_line": 42, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_798", "original_string": "def start(self, action_name: str) -> None:\r\n        \"\"\"Defines how to start recording an action.\"\"\"", "language": "python", "code": "def start(self, action_name: str) -> None:\r\n        \"\"\"Defines how to start recording an action.\"\"\"", "code_tokens": ["def", "start", "(", "self", ",", "action_name", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Defines how to start recording an action.", "docstring_tokens": ["defines", "how", "to", "start", "recording", "an", "action"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 45, "end_line": 46, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_799", "original_string": "def stop(self, action_name: str) -> None:\r\n        \"\"\"Defines how to record the duration once an action is complete.\"\"\"", "language": "python", "code": "def stop(self, action_name: str) -> None:\r\n        \"\"\"Defines how to record the duration once an action is complete.\"\"\"", "code_tokens": ["def", "stop", "(", "self", ",", "action_name", ":", "str", ")", "-", ">", "None", ":", "STRING"], "docstring": "Defines how to record the duration once an action is complete.", "docstring_tokens": ["defines", "how", "to", "record", "the", "duration", "once", "an", "action", "is", "complete"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 49, "end_line": 50, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_800", "original_string": "def profile(self, action_name: str) -> Generator:\r\n        \"\"\"Yields a context manager to encapsulate the scope of a profiled action.\r\n\r\n        Example::\r\n\r\n            with self.profile('load training data'):\r\n\r\n        The profiler will start once you've entered the context and will automatically\r\n        stop once you exit the code block.\r\n\r\n        \"\"\"\r\n        try:\r\n            self.start(action_name)\r\n            yield action_name\r\n        finally:\r\n            self.stop(action_name)", "language": "python", "code": "def profile(self, action_name: str) -> Generator:\r\n        \"\"\"Yields a context manager to encapsulate the scope of a profiled action.\r\n\r\n        Example::\r\n\r\n            with self.profile('load training data'):\r\n\r\n        The profiler will start once you've entered the context and will automatically\r\n        stop once you exit the code block.\r\n\r\n        \"\"\"\r\n        try:\r\n            self.start(action_name)\r\n            yield action_name\r\n        finally:\r\n            self.stop(action_name)", "code_tokens": ["def", "profile", "(", "self", ",", "action_name", ":", "str", ")", "-", ">", "Generator", ":", "STRING", "try", ":", "self", ".", "start", "(", "action_name", ")", "yield", "action_name", "finally", ":", "self", ".", "stop", "(", "action_name", ")"], "docstring": "Yields a context manager to encapsulate the scope of a profiled action.", "docstring_tokens": ["yields", "a", "context", "manager", "to", "encapsulate", "the", "scope", "of", "a", "profiled", "action"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 56, "end_line": 72, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_801", "original_string": "def describe(self) -> None:\r\n        \"\"\"Logs a profile report after the conclusion of run.\"\"\"\r\n        self._prepare_streams()\r\n        summary = self.summary()\r\n        if summary and self._write_stream is not None:\r\n            self._write_stream(summary)\r\n        if self._output_file is not None:\r\n            self._output_file.flush()\r\n        self.teardown(stage=self._stage)", "language": "python", "code": "def describe(self) -> None:\r\n        \"\"\"Logs a profile report after the conclusion of run.\"\"\"\r\n        self._prepare_streams()\r\n        summary = self.summary()\r\n        if summary and self._write_stream is not None:\r\n            self._write_stream(summary)\r\n        if self._output_file is not None:\r\n            self._output_file.flush()\r\n        self.teardown(stage=self._stage)", "code_tokens": ["def", "describe", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "_prepare_streams", "(", ")", "summary", "=", "self", ".", "summary", "(", ")", "if", "summary", "and", "self", ".", "_write_stream", "is", "not", "None", ":", "self", ".", "_write_stream", "(", "summary", ")", "if", "self", ".", "_output_file", "is", "not", "None", ":", "self", ".", "_output_file", ".", "flush", "(", ")", "self", ".", "teardown", "(", "stage", "=", "self", ".", "_stage", ")"], "docstring": "Logs a profile report after the conclusion of run.", "docstring_tokens": ["logs", "a", "profile", "report", "after", "the", "conclusion", "of", "run"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 105, "end_line": 116, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_802", "original_string": "def setup(self, stage: str, local_rank: Optional[int] = None, log_dir: Optional[str] = None) -> None:\r\n        \"\"\"Execute arbitrary pre-profiling set-up steps.\"\"\"\r\n        self._stage = stage\r\n        self._local_rank = local_rank\r\n        self.dirpath = self.dirpath or log_dir", "language": "python", "code": "def setup(self, stage: str, local_rank: Optional[int] = None, log_dir: Optional[str] = None) -> None:\r\n        \"\"\"Execute arbitrary pre-profiling set-up steps.\"\"\"\r\n        self._stage = stage\r\n        self._local_rank = local_rank\r\n        self.dirpath = self.dirpath or log_dir", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ",", "local_rank", ":", "Optional", "[", "int", "]", "=", "None", ",", "log_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "_stage", "=", "stage", "self", ".", "_local_rank", "=", "local_rank", "self", ".", "dirpath", "=", "self", ".", "dirpath", "or", "log_dir"], "docstring": "Execute arbitrary pre-profiling set-up steps.", "docstring_tokens": ["execute", "arbitrary", "pre", "profiling", "set", "up", "steps"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 129, "end_line": 133, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "function_803", "original_string": "def teardown(self, stage: Optional[str]) -> None:\r\n        \"\"\"Execute arbitrary post-profiling tear-down steps.\r\n\r\n        Closes the currently open file and stream.\r\n\r\n        \"\"\"\r\n        self._write_stream = None\r\n        if self._output_file is not None:\r\n            self._output_file.close()\r\n            self._output_file = None  # can't pickle TextIOWrapper\r", "language": "python", "code": "def teardown(self, stage: Optional[str]) -> None:\r\n        \"\"\"Execute arbitrary post-profiling tear-down steps.\r\n\r\n        Closes the currently open file and stream.\r\n\r\n        \"\"\"\r\n        self._write_stream = None\r\n        if self._output_file is not None:\r\n            self._output_file.close()\r\n            self._output_file = None  # can't pickle TextIOWrapper\r", "code_tokens": ["def", "teardown", "(", "self", ",", "stage", ":", "Optional", "[", "str", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "_write_stream", "=", "None", "if", "self", ".", "_output_file", "is", "not", "None", ":", "self", ".", "_output_file", ".", "close", "(", ")", "self", ".", "_output_file", "=", "None", "#", "can", "'", "t", "pickle", "TextIOWrapper"], "docstring": "Execute arbitrary post-profiling tear-down steps.", "docstring_tokens": ["execute", "arbitrary", "post", "profiling", "tear", "down", "steps"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\profiler.py", "start_line": 135, "end_line": 144, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\pytorch.py", "func_name": "function_804", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        group_by_input_shapes: bool = False,\r\n        emit_nvtx: bool = False,\r\n        export_to_chrome: bool = True,\r\n        row_limit: int = 20,\r\n        sort_by_key: Optional[str] = None,\r\n        record_module_names: bool = True,\r\n        table_kwargs: Optional[dict[str, Any]] = None,\r\n        **profiler_kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of\r\n        different operators inside your model - both on the CPU and GPU.\r\n\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            group_by_input_shapes: Include operator input shapes and group calls by shape.\r\n\r\n            emit_nvtx: Context manager that makes every autograd operation emit an NVTX range\r\n                Run::\r\n\r\n                    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\r\n\r\n                To visualize, you can either use::\r\n\r\n                    nvvp trace_name.prof\r\n                    torch.autograd.profiler.load_nvprof(path)\r\n\r\n            export_to_chrome: Whether to export the sequence of profiled operators for Chrome.\r\n                It will generate a ``.json`` file which can be read by Chrome.\r\n\r\n            row_limit: Limit the number of rows in a table, ``-1`` is a special value that\r\n                removes the limit completely.\r\n\r\n            sort_by_key: Attribute used to sort entries. By default\r\n                they are printed in the same order as they were registered.\r\n                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,\r\n                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,\r\n                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.\r\n\r\n            record_module_names: Whether to add module names while recording autograd operation.\r\n\r\n            table_kwargs: Dictionary with keyword arguments for the summary table.\r\n\r\n            \\**profiler_kwargs: Keyword arguments for the PyTorch profiler. This depends on your PyTorch version\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If arg ``sort_by_key`` is not present in ``AVAILABLE_SORT_KEYS``.\r\n                If arg ``schedule`` is not a ``Callable``.\r\n                If arg ``schedule`` does not return a ``torch.profiler.ProfilerAction``.\r\n\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n\r\n        self._group_by_input_shapes = group_by_input_shapes and profiler_kwargs.get(\"record_shapes\", False)\r\n        self._emit_nvtx = emit_nvtx\r\n        self._export_to_chrome = export_to_chrome\r\n        self._row_limit = row_limit\r\n        self._sort_by_key = sort_by_key or _default_sort_by_key(profiler_kwargs)\r\n        self._record_module_names = record_module_names\r\n        self._profiler_kwargs = profiler_kwargs\r\n        self._table_kwargs = table_kwargs if table_kwargs is not None else {}\r\n\r\n        self.profiler: Optional[_PROFILER] = None\r\n        self.function_events: Optional[EventList] = None\r\n        self._lightning_module: Optional[LightningModule] = None  # set by ProfilerConnector\r\n        self._register: Optional[RegisterRecordFunction] = None\r\n        self._parent_profiler: Optional[AbstractContextManager] = None\r\n        self._recording_map: dict[str, record_function] = {}\r\n        self._start_action_name: Optional[str] = None\r\n        self._schedule: Optional[ScheduleWrapper] = None\r\n\r\n        if _KINETO_AVAILABLE:\r\n            self._init_kineto(profiler_kwargs)\r\n\r\n        if self._sort_by_key not in self.AVAILABLE_SORT_KEYS:\r\n            raise MisconfigurationException(\r\n                f\"Found sort_by_key: {self._sort_by_key}. Should be within {self.AVAILABLE_SORT_KEYS}. \"\r\n            )\r\n\r\n        for key in self._table_kwargs:\r\n            if key in {\"sort_by\", \"row_limit\"}:\r\n                raise KeyError(\r\n                    f\"Found invalid table_kwargs key: {key}. This is already a positional argument of the Profiler.\"\r\n                )\r\n            valid_table_keys = set(inspect.signature(EventList.table).parameters.keys()) - {\r\n                \"self\",\r\n                \"sort_by\",\r\n                \"row_limit\",\r\n            }\r\n            if key not in valid_table_keys:\r\n                raise KeyError(f\"Found invalid table_kwargs key: {key}. Should be within {valid_table_keys}.\")", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        group_by_input_shapes: bool = False,\r\n        emit_nvtx: bool = False,\r\n        export_to_chrome: bool = True,\r\n        row_limit: int = 20,\r\n        sort_by_key: Optional[str] = None,\r\n        record_module_names: bool = True,\r\n        table_kwargs: Optional[dict[str, Any]] = None,\r\n        **profiler_kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of\r\n        different operators inside your model - both on the CPU and GPU.\r\n\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            group_by_input_shapes: Include operator input shapes and group calls by shape.\r\n\r\n            emit_nvtx: Context manager that makes every autograd operation emit an NVTX range\r\n                Run::\r\n\r\n                    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\r\n\r\n                To visualize, you can either use::\r\n\r\n                    nvvp trace_name.prof\r\n                    torch.autograd.profiler.load_nvprof(path)\r\n\r\n            export_to_chrome: Whether to export the sequence of profiled operators for Chrome.\r\n                It will generate a ``.json`` file which can be read by Chrome.\r\n\r\n            row_limit: Limit the number of rows in a table, ``-1`` is a special value that\r\n                removes the limit completely.\r\n\r\n            sort_by_key: Attribute used to sort entries. By default\r\n                they are printed in the same order as they were registered.\r\n                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,\r\n                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,\r\n                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.\r\n\r\n            record_module_names: Whether to add module names while recording autograd operation.\r\n\r\n            table_kwargs: Dictionary with keyword arguments for the summary table.\r\n\r\n            \\**profiler_kwargs: Keyword arguments for the PyTorch profiler. This depends on your PyTorch version\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If arg ``sort_by_key`` is not present in ``AVAILABLE_SORT_KEYS``.\r\n                If arg ``schedule`` is not a ``Callable``.\r\n                If arg ``schedule`` does not return a ``torch.profiler.ProfilerAction``.\r\n\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n\r\n        self._group_by_input_shapes = group_by_input_shapes and profiler_kwargs.get(\"record_shapes\", False)\r\n        self._emit_nvtx = emit_nvtx\r\n        self._export_to_chrome = export_to_chrome\r\n        self._row_limit = row_limit\r\n        self._sort_by_key = sort_by_key or _default_sort_by_key(profiler_kwargs)\r\n        self._record_module_names = record_module_names\r\n        self._profiler_kwargs = profiler_kwargs\r\n        self._table_kwargs = table_kwargs if table_kwargs is not None else {}\r\n\r\n        self.profiler: Optional[_PROFILER] = None\r\n        self.function_events: Optional[EventList] = None\r\n        self._lightning_module: Optional[LightningModule] = None  # set by ProfilerConnector\r\n        self._register: Optional[RegisterRecordFunction] = None\r\n        self._parent_profiler: Optional[AbstractContextManager] = None\r\n        self._recording_map: dict[str, record_function] = {}\r\n        self._start_action_name: Optional[str] = None\r\n        self._schedule: Optional[ScheduleWrapper] = None\r\n\r\n        if _KINETO_AVAILABLE:\r\n            self._init_kineto(profiler_kwargs)\r\n\r\n        if self._sort_by_key not in self.AVAILABLE_SORT_KEYS:\r\n            raise MisconfigurationException(\r\n                f\"Found sort_by_key: {self._sort_by_key}. Should be within {self.AVAILABLE_SORT_KEYS}. \"\r\n            )\r\n\r\n        for key in self._table_kwargs:\r\n            if key in {\"sort_by\", \"row_limit\"}:\r\n                raise KeyError(\r\n                    f\"Found invalid table_kwargs key: {key}. This is already a positional argument of the Profiler.\"\r\n                )\r\n            valid_table_keys = set(inspect.signature(EventList.table).parameters.keys()) - {\r\n                \"self\",\r\n                \"sort_by\",\r\n                \"row_limit\",\r\n            }\r\n            if key not in valid_table_keys:\r\n                raise KeyError(f\"Found invalid table_kwargs key: {key}. Should be within {valid_table_keys}.\")", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "group_by_input_shapes", ":", "bool", "=", "False", ",", "emit_nvtx", ":", "bool", "=", "False", ",", "export_to_chrome", ":", "bool", "=", "True", ",", "row_limit", ":", "int", "=", "20", ",", "sort_by_key", ":", "Optional", "[", "str", "]", "=", "None", ",", "record_module_names", ":", "bool", "=", "True", ",", "table_kwargs", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "*", "*", "profiler_kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "rSTRING", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "_group_by_input_shapes", "=", "group_by_input_shapes", "and", "profiler_kwargs", ".", "get", "(", "STRING", ",", "False", ")", "self", ".", "_emit_nvtx", "=", "emit_nvtx", "self", ".", "_export_to_chrome", "=", "export_to_chrome", "self", ".", "_row_limit", "=", "row_limit", "self", ".", "_sort_by_key", "=", "sort_by_key", "or", "_default_sort_by_key", "(", "profiler_kwargs", ")", "self", ".", "_record_module_names", "=", "record_module_names", "self", ".", "_profiler_kwargs", "=", "profiler_kwargs", "self", ".", "_table_kwargs", "=", "table_kwargs", "if", "table_kwargs", "is", "not", "None", "else", "{", "}", "self", ".", "profiler", ":", "Optional", "[", "_PROFILER", "]", "=", "None", "self", ".", "function_events", ":", "Optional", "[", "EventList", "]", "=", "None", "self", ".", "_lightning_module", ":", "Optional", "[", "LightningModule", "]", "=", "None", "#", "set", "by", "ProfilerConnector", "self", ".", "_register", ":", "Optional", "[", "RegisterRecordFunction", "]", "=", "None", "self", ".", "_parent_profiler", ":", "Optional", "[", "AbstractContextManager", "]", "=", "None", "self", ".", "_recording_map", ":", "dict", "[", "str", ",", "record_function", "]", "=", "{", "}", "self", ".", "_start_action_name", ":", "Optional", "[", "str", "]", "=", "None", "self", ".", "_schedule", ":", "Optional", "[", "ScheduleWrapper", "]", "=", "None", "if", "_KINETO_AVAILABLE", ":", "self", ".", "_init_kineto", "(", "profiler_kwargs", ")", "if", "self", ".", "_sort_by_key", "not", "in", "self", ".", "AVAILABLE_SORT_KEYS", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "for", "key", "in", "self", ".", "_table_kwargs", ":", "if", "key", "in", "{", "STRING", ",", "STRING", "}", ":", "raise", "KeyError", "(", "fSTRING", ")", "valid_table_keys", "=", "set", "(", "inspect", ".", "signature", "(", "EventList", ".", "table", ")", ".", "parameters", ".", "keys", "(", ")", ")", "-", "{", "STRING", ",", "STRING", ",", "STRING", ",", "}", "if", "key", "not", "in", "valid_table_keys", ":", "raise", "KeyError", "(", "fSTRING", ")"], "docstring": "r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of", "docstring_tokens": ["r", "this", "profiler", "uses", "pytorch", "s", "autograd", "profiler", "and", "lets", "you", "inspect", "the", "cost", "of"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\pytorch.py", "start_line": 232, "end_line": 332, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\simple.py", "func_name": "function_805", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        extended: bool = True,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            extended: If ``True``, adds extra columns representing number of calls and percentage of total time spent on\r\n                respective action.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to start an action which has already started, or\r\n                if you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.current_actions: dict[str, float] = {}\r\n        self.recorded_durations: dict = defaultdict(list)\r\n        self.extended = extended\r\n        self.start_time = time.monotonic()", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        extended: bool = True,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            extended: If ``True``, adds extra columns representing number of calls and percentage of total time spent on\r\n                respective action.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to start an action which has already started, or\r\n                if you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.current_actions: dict[str, float] = {}\r\n        self.recorded_durations: dict = defaultdict(list)\r\n        self.extended = extended\r\n        self.start_time = time.monotonic()", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "extended", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "current_actions", ":", "dict", "[", "str", ",", "float", "]", "=", "{", "}", "self", ".", "recorded_durations", ":", "dict", "=", "defaultdict", "(", "list", ")", "self", ".", "extended", "=", "extended", "self", ".", "start_time", "=", "time", ".", "monotonic", "(", ")"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\simple.py", "start_line": 39, "end_line": 66, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\xla.py", "func_name": "function_806", "original_string": "def __init__(self, port: int = 9012) -> None:\r\n        \"\"\"XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU\r\n        performance tools.\r\n\r\n        Args:\r\n            port: the port to start the profiler server on. An exception is\r\n                raised if the provided port is invalid or busy.\r\n\r\n        \"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            raise ModuleNotFoundError(str(_XLA_AVAILABLE))\r\n        super().__init__(dirpath=None, filename=None)\r\n        self.port = port\r\n        self._recording_map: dict = {}\r\n        self._step_recoding_map: dict = {}\r\n        self._start_trace: bool = False", "language": "python", "code": "def __init__(self, port: int = 9012) -> None:\r\n        \"\"\"XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU\r\n        performance tools.\r\n\r\n        Args:\r\n            port: the port to start the profiler server on. An exception is\r\n                raised if the provided port is invalid or busy.\r\n\r\n        \"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            raise ModuleNotFoundError(str(_XLA_AVAILABLE))\r\n        super().__init__(dirpath=None, filename=None)\r\n        self.port = port\r\n        self._recording_map: dict = {}\r\n        self._step_recoding_map: dict = {}\r\n        self._start_trace: bool = False", "code_tokens": ["def", "__init__", "(", "self", ",", "port", ":", "int", "=", "9012", ")", "-", ">", "None", ":", "STRING", "if", "not", "_XLA_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "str", "(", "_XLA_AVAILABLE", ")", ")", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "None", ",", "filename", "=", "None", ")", "self", ".", "port", "=", "port", "self", ".", "_recording_map", ":", "dict", "=", "{", "}", "self", ".", "_step_recoding_map", ":", "dict", "=", "{", "}", "self", ".", "_start_trace", ":", "bool", "=", "False"], "docstring": "XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU", "docstring_tokens": ["xla", "profiler", "will", "help", "you", "debug", "and", "optimize", "training", "workload", "performance", "for", "your", "models", "using", "cloud", "tpu"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\profilers\\xla.py", "start_line": 33, "end_line": 48, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "function_807", "original_string": "def configure_payload(self) -> dict[str, Any]:\r\n        \"\"\"Returns a request payload as a dictionary.\"\"\"", "language": "python", "code": "def configure_payload(self) -> dict[str, Any]:\r\n        \"\"\"Returns a request payload as a dictionary.\"\"\"", "code_tokens": ["def", "configure_payload", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING"], "docstring": "Returns a request payload as a dictionary.", "docstring_tokens": ["returns", "a", "request", "payload", "as", "a", "dictionary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module.py", "start_line": 58, "end_line": 59, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "function_808", "original_string": "def configure_serialization(self) -> tuple[dict[str, Callable], dict[str, Callable]]:\r\n        \"\"\"Returns a tuple of dictionaries.\r\n\r\n        The first dictionary contains the name of the ``serve_step`` input variables name as its keys\r\n        and the associated de-serialization function (e.g function to convert a payload to tensors).\r\n\r\n        The second dictionary contains the name of the ``serve_step`` output variables name as its keys\r\n        and the associated serialization function (e.g function to convert a tensors into payload).\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_serialization(self) -> tuple[dict[str, Callable], dict[str, Callable]]:\r\n        \"\"\"Returns a tuple of dictionaries.\r\n\r\n        The first dictionary contains the name of the ``serve_step`` input variables name as its keys\r\n        and the associated de-serialization function (e.g function to convert a payload to tensors).\r\n\r\n        The second dictionary contains the name of the ``serve_step`` output variables name as its keys\r\n        and the associated serialization function (e.g function to convert a tensors into payload).\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_serialization", "(", "self", ")", "-", ">", "tuple", "[", "dict", "[", "str", ",", "Callable", "]", ",", "dict", "[", "str", ",", "Callable", "]", "]", ":", "STRING"], "docstring": "Returns a tuple of dictionaries.", "docstring_tokens": ["returns", "a", "tuple", "of", "dictionaries"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module.py", "start_line": 62, "end_line": 71, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "function_809", "original_string": "def serve_step(self, *args: Tensor, **kwargs: Tensor) -> dict[str, Tensor]:\r\n        r\"\"\"Returns the predictions of your model as a dictionary.\r\n\r\n        .. code-block:: python\r\n\r\n            def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n                return {\"predictions\": self(x)}\r\n\r\n        Args:\r\n            args: The output from de-serializer functions provided by the ``configure_serialization`` hook.\r\n            kwargs: The keyword output of the de-serializer functions provided by the ``configure_serialization`` hook.\r\n\r\n        Return:\r\n            - ``dict`` - A dictionary with their associated tensors.\r\n\r\n        \"\"\"", "language": "python", "code": "def serve_step(self, *args: Tensor, **kwargs: Tensor) -> dict[str, Tensor]:\r\n        r\"\"\"Returns the predictions of your model as a dictionary.\r\n\r\n        .. code-block:: python\r\n\r\n            def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n                return {\"predictions\": self(x)}\r\n\r\n        Args:\r\n            args: The output from de-serializer functions provided by the ``configure_serialization`` hook.\r\n            kwargs: The keyword output of the de-serializer functions provided by the ``configure_serialization`` hook.\r\n\r\n        Return:\r\n            - ``dict`` - A dictionary with their associated tensors.\r\n\r\n        \"\"\"", "code_tokens": ["def", "serve_step", "(", "self", ",", "*", "args", ":", "Tensor", ",", "*", "*", "kwargs", ":", "Tensor", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "rSTRING"], "docstring": "r\"\"\"Returns the predictions of your model as a dictionary.", "docstring_tokens": ["r", "returns", "the", "predictions", "of", "your", "model", "as", "a", "dictionary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module.py", "start_line": 74, "end_line": 89, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "function_810", "original_string": "def configure_response(self) -> dict[str, Any]:\r\n        \"\"\"Returns a response to validate the server response.\"\"\"", "language": "python", "code": "def configure_response(self) -> dict[str, Any]:\r\n        \"\"\"Returns a response to validate the server response.\"\"\"", "code_tokens": ["def", "configure_response", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING"], "docstring": "Returns a response to validate the server response.", "docstring_tokens": ["returns", "a", "response", "to", "validate", "the", "server", "response"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module.py", "start_line": 92, "end_line": 93, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module_validator.py", "func_name": "function_811", "original_string": "def successful(self) -> Optional[bool]:\r\n        \"\"\"Returns whether the model was successfully served.\"\"\"\r\n        return self.resp.status_code == 200 if self.resp else None", "language": "python", "code": "def successful(self) -> Optional[bool]:\r\n        \"\"\"Returns whether the model was successfully served.\"\"\"\r\n        return self.resp.status_code == 200 if self.resp else None", "code_tokens": ["def", "successful", "(", "self", ")", "-", ">", "Optional", "[", "bool", "]", ":", "STRING", "return", "self", ".", "resp", ".", "status_code", "=", "=", "200", "if", "self", ".", "resp", "else", "None"], "docstring": "Returns whether the model was successfully served.", "docstring_tokens": ["returns", "whether", "the", "model", "was", "successfully", "served"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module_validator.py", "start_line": 133, "end_line": 135, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module_validator.py", "func_name": "function_812", "original_string": "def _start_server(servable_model: ServableModule, host: str, port: int, _: bool) -> None:\r\n        \"\"\"This method starts a server with a serve and ping endpoints.\"\"\"\r\n        from fastapi import Body, FastAPI\r\n        from uvicorn import run\r\n\r\n        app = FastAPI()\r\n\r\n        deserializers, serializers = servable_model.configure_serialization()\r\n\r\n        servable_model.eval()\r\n\r\n        @app.get(\"/ping\")\r\n        def ping() -> bool:\r\n            return True\r\n\r\n        @app.post(\"/serve\")\r\n        async def serve(payload: dict = Body(...)) -> dict[str, Any]:\r\n            body = payload[\"body\"]\r\n\r\n            for key, deserializer in deserializers.items():\r\n                body[key] = deserializer(body[key])\r\n\r\n            with torch.no_grad():\r\n                output = servable_model.serve_step(**body)\r\n\r\n            if not isinstance(output, dict):\r\n                raise Exception(f\"Please, return your outputs as a dictionary. Found {output}\")\r\n\r\n            for key, serializer in serializers.items():\r\n                output[key] = serializer(output[key])\r\n\r\n            return output\r\n\r\n        run(app, host=host, port=port, log_level=\"error\")", "language": "python", "code": "def _start_server(servable_model: ServableModule, host: str, port: int, _: bool) -> None:\r\n        \"\"\"This method starts a server with a serve and ping endpoints.\"\"\"\r\n        from fastapi import Body, FastAPI\r\n        from uvicorn import run\r\n\r\n        app = FastAPI()\r\n\r\n        deserializers, serializers = servable_model.configure_serialization()\r\n\r\n        servable_model.eval()\r\n\r\n        @app.get(\"/ping\")\r\n        def ping() -> bool:\r\n            return True\r\n\r\n        @app.post(\"/serve\")\r\n        async def serve(payload: dict = Body(...)) -> dict[str, Any]:\r\n            body = payload[\"body\"]\r\n\r\n            for key, deserializer in deserializers.items():\r\n                body[key] = deserializer(body[key])\r\n\r\n            with torch.no_grad():\r\n                output = servable_model.serve_step(**body)\r\n\r\n            if not isinstance(output, dict):\r\n                raise Exception(f\"Please, return your outputs as a dictionary. Found {output}\")\r\n\r\n            for key, serializer in serializers.items():\r\n                output[key] = serializer(output[key])\r\n\r\n            return output\r\n\r\n        run(app, host=host, port=port, log_level=\"error\")", "code_tokens": ["def", "_start_server", "(", "servable_model", ":", "ServableModule", ",", "host", ":", "str", ",", "port", ":", "int", ",", "_", ":", "bool", ")", "-", ">", "None", ":", "STRING", "from", "fastapi", "import", "Body", ",", "FastAPI", "from", "uvicorn", "import", "run", "app", "=", "FastAPI", "(", ")", "deserializers", ",", "serializers", "=", "servable_model", ".", "configure_serialization", "(", ")", "servable_model", ".", "eval", "(", ")", "@", "app", ".", "get", "(", "STRING", ")", "def", "ping", "(", ")", "-", ">", "bool", ":", "return", "True", "@", "app", ".", "post", "(", "STRING", ")", "async", "def", "serve", "(", "payload", ":", "dict", "=", "Body", "(", ".", ".", ".", ")", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "body", "=", "payload", "[", "STRING", "]", "for", "key", ",", "deserializer", "in", "deserializers", ".", "items", "(", ")", ":", "body", "[", "key", "]", "=", "deserializer", "(", "body", "[", "key", "]", ")", "with", "torch", ".", "no_grad", "(", ")", ":", "output", "=", "servable_model", ".", "serve_step", "(", "*", "*", "body", ")", "if", "not", "isinstance", "(", "output", ",", "dict", ")", ":", "raise", "Exception", "(", "fSTRING", ")", "for", "key", ",", "serializer", "in", "serializers", ".", "items", "(", ")", ":", "output", "[", "key", "]", "=", "serializer", "(", "output", "[", "key", "]", ")", "return", "output", "run", "(", "app", ",", "host", "=", "host", ",", "port", "=", "port", ",", "log_level", "=", "STRING", ")"], "docstring": "This method starts a server with a serve and ping endpoints.", "docstring_tokens": ["this", "method", "starts", "a", "server", "with", "a", "serve", "and", "ping", "endpoints"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\serve\\servable_module_validator.py", "start_line": 142, "end_line": 176, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "function_813", "original_string": "def is_distributed(self) -> bool:  # pragma: no-cover\r\n        \"\"\"Legacy property kept for backwards compatibility.\"\"\"\r\n        rank_zero_deprecation(\r\n            f\"`{type(self).__name__}.is_distributed` is deprecated. Use is discouraged.\", stacklevel=6\r\n        )\r\n        return True", "language": "python", "code": "def is_distributed(self) -> bool:  # pragma: no-cover\r\n        \"\"\"Legacy property kept for backwards compatibility.\"\"\"\r\n        rank_zero_deprecation(\r\n            f\"`{type(self).__name__}.is_distributed` is deprecated. Use is discouraged.\", stacklevel=6\r\n        )\r\n        return True", "code_tokens": ["def", "is_distributed", "(", "self", ")", "-", ">", "bool", ":", "#", "pragma", ":", "no", "-", "cover", "STRING", "rank_zero_deprecation", "(", "fSTRING", ",", "stacklevel", "=", "6", ")", "return", "True"], "docstring": "Legacy property kept for backwards compatibility.", "docstring_tokens": ["legacy", "property", "kept", "for", "backwards", "compatibility"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\ddp.py", "start_line": 107, "end_line": 112, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "function_814", "original_string": "def _setup_model(self, model: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self.determine_ddp_device_ids()\r\n        log.debug(f\"setting up DDP model with device ids: {device_ids}, kwargs: {self._ddp_kwargs}\")\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)", "language": "python", "code": "def _setup_model(self, model: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self.determine_ddp_device_ids()\r\n        log.debug(f\"setting up DDP model with device ids: {device_ids}, kwargs: {self._ddp_kwargs}\")\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "DistributedDataParallel", ":", "STRING", "device_ids", "=", "self", ".", "determine_ddp_device_ids", "(", ")", "log", ".", "debug", "(", "fSTRING", ")", "ctx", "=", "torch", ".", "cuda", ".", "stream", "(", "torch", ".", "cuda", ".", "Stream", "(", ")", ")", "if", "device_ids", "is", "not", "None", "else", "nullcontext", "(", ")", "with", "ctx", ":", "return", "DistributedDataParallel", "(", "module", "=", "model", ",", "device_ids", "=", "device_ids", ",", "*", "*", "self", ".", "_ddp_kwargs", ")"], "docstring": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel", "module"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\ddp.py", "start_line": 187, "end_line": 194, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "function_815", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\r\n\r\n        if self._model_averager is None:\r\n            return optimizer_output\r\n\r\n        params = [param for group in optimizer.param_groups for param in group[\"params\"] if param.grad is not None]\r\n        self._model_averager.average_parameters(iter(params))\r\n\r\n        return optimizer_output", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\r\n\r\n        if self._model_averager is None:\r\n            return optimizer_output\r\n\r\n        params = [param for group in optimizer.param_groups for param in group[\"params\"] if param.grad is not None]\r\n        self._model_averager.average_parameters(iter(params))\r\n\r\n        return optimizer_output", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "model", ":", "Optional", "[", "Union", "[", "STRING", ",", "Module", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "STRING", "optimizer_output", "=", "super", "(", ")", ".", "optimizer_step", "(", "optimizer", ",", "closure", ",", "model", ",", "*", "*", "kwargs", ")", "if", "self", ".", "_model_averager", "is", "None", ":", "return", "optimizer_output", "params", "=", "[", "param", "for", "group", "in", "optimizer", ".", "param_groups", "for", "param", "in", "group", "[", "STRING", "]", "if", "param", ".", "grad", "is", "not", "None", "]", "self", ".", "_model_averager", ".", "average_parameters", "(", "iter", "(", "params", ")", ")", "return", "optimizer_output"], "docstring": "Performs the actual optimizer step.", "docstring_tokens": ["performs", "the", "actual", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\ddp.py", "start_line": 256, "end_line": 280, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "function_816", "original_string": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"\r\n        if not isinstance(self.model, DistributedDataParallel):\r\n            return\r\n        assert self.lightning_module is not None\r\n        if not self.lightning_module.automatic_optimization:\r\n            prepare_for_backward(self.model, closure_loss)", "language": "python", "code": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"\r\n        if not isinstance(self.model, DistributedDataParallel):\r\n            return\r\n        assert self.lightning_module is not None\r\n        if not self.lightning_module.automatic_optimization:\r\n            prepare_for_backward(self.model, closure_loss)", "code_tokens": ["def", "pre_backward", "(", "self", ",", "closure_loss", ":", "Tensor", ")", "-", ">", "None", ":", "STRING", "if", "not", "isinstance", "(", "self", ".", "model", ",", "DistributedDataParallel", ")", ":", "return", "assert", "self", ".", "lightning_module", "is", "not", "None", "if", "not", "self", ".", "lightning_module", ".", "automatic_optimization", ":", "prepare_for_backward", "(", "self", ".", "model", ",", "closure_loss", ")"], "docstring": "Run before precision plugin executes backward.", "docstring_tokens": ["run", "before", "precision", "plugin", "executes", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\ddp.py", "start_line": 313, "end_line": 319, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "function_817", "original_string": "def reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ")", "-", ">", "Tensor", ":", "STRING", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\ddp.py", "start_line": 328, "end_line": 345, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_818", "original_string": "def __init__(\r\n        self,\r\n        accelerator: Optional[\"pl.accelerators.Accelerator\"] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Union[str, int] = \"auto\",\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision_plugin: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. *For more information:* :ref:`deepspeed_advanced`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        *For more information:* https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either `precision=\"16-mixed\"` or\r\n                `precision=\"bf16-mixed\"`.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                If set to \"auto\", the strategy tries to infer this from\r\n                the train DataLoader's BatchSampler, else defaults to 1.\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size (trainer.batch_size).\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision_plugin=precision_plugin,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale", "language": "python", "code": "def __init__(\r\n        self,\r\n        accelerator: Optional[\"pl.accelerators.Accelerator\"] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Union[str, int] = \"auto\",\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision_plugin: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. *For more information:* :ref:`deepspeed_advanced`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        *For more information:* https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either `precision=\"16-mixed\"` or\r\n                `precision=\"bf16-mixed\"`.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                If set to \"auto\", the strategy tries to infer this from\r\n                the train DataLoader's BatchSampler, else defaults to 1.\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size (trainer.batch_size).\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision_plugin=precision_plugin,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale", "code_tokens": ["def", "__init__", "(", "self", ",", "accelerator", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "zero_optimization", ":", "bool", "=", "True", ",", "stage", ":", "int", "=", "2", ",", "remote_device", ":", "Optional", "[", "str", "]", "=", "None", ",", "offload_optimizer", ":", "bool", "=", "False", ",", "offload_parameters", ":", "bool", "=", "False", ",", "offload_params_device", ":", "str", "=", "STRING", ",", "nvme_path", ":", "str", "=", "STRING", ",", "params_buffer_count", ":", "int", "=", "5", ",", "params_buffer_size", ":", "int", "=", "100_000_000", ",", "max_in_cpu", ":", "int", "=", "1_000_000_000", ",", "offload_optimizer_device", ":", "str", "=", "STRING", ",", "optimizer_buffer_count", ":", "int", "=", "4", ",", "block_size", ":", "int", "=", "1048576", ",", "queue_depth", ":", "int", "=", "8", ",", "single_submit", ":", "bool", "=", "False", ",", "overlap_events", ":", "bool", "=", "True", ",", "thread_count", ":", "int", "=", "1", ",", "pin_memory", ":", "bool", "=", "False", ",", "sub_group_size", ":", "int", "=", "1_000_000_000_000", ",", "contiguous_gradients", ":", "bool", "=", "True", ",", "overlap_comm", ":", "bool", "=", "True", ",", "allgather_partitions", ":", "bool", "=", "True", ",", "reduce_scatter", ":", "bool", "=", "True", ",", "allgather_bucket_size", ":", "int", "=", "200_000_000", ",", "reduce_bucket_size", ":", "int", "=", "200_000_000", ",", "zero_allow_untested_optimizer", ":", "bool", "=", "True", ",", "logging_batch_size_per_gpu", ":", "Union", "[", "str", ",", "int", "]", "=", "STRING", ",", "config", ":", "Optional", "[", "Union", "[", "_PATH", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "logging_level", ":", "int", "=", "logging", ".", "WARN", ",", "parallel_devices", ":", "Optional", "[", "list", "[", "torch", ".", "device", "]", "]", "=", "None", ",", "cluster_environment", ":", "Optional", "[", "ClusterEnvironment", "]", "=", "None", ",", "loss_scale", ":", "float", "=", "0", ",", "initial_scale_power", ":", "int", "=", "16", ",", "loss_scale_window", ":", "int", "=", "1000", ",", "hysteresis", ":", "int", "=", "2", ",", "min_loss_scale", ":", "int", "=", "1", ",", "partition_activations", ":", "bool", "=", "False", ",", "cpu_checkpointing", ":", "bool", "=", "False", ",", "contiguous_memory_optimization", ":", "bool", "=", "False", ",", "synchronize_checkpoint_boundary", ":", "bool", "=", "False", ",", "load_full_weights", ":", "bool", "=", "False", ",", "precision_plugin", ":", "Optional", "[", "Precision", "]", "=", "None", ",", "process_group_backend", ":", "Optional", "[", "str", "]", "=", "None", ",", "timeout", ":", "Optional", "[", "timedelta", "]", "=", "default_pg_timeout", ",", "exclude_frozen_parameters", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", ")", "if", "_TORCH_GREATER_EQUAL_2_6", "and", "not", "_DEEPSPEED_GREATER_EQUAL_0_16", ":", "import", "deepspeed", "deepspeed_version", "=", "deepspeed", ".", "__version__", "raise", "ImportError", "(", "fSTRING", "fSTRING", "STRING", ")", "super", "(", ")", ".", "__init__", "(", "accelerator", "=", "accelerator", ",", "parallel_devices", "=", "parallel_devices", ",", "cluster_environment", "=", "cluster_environment", ",", "precision_plugin", "=", "precision_plugin", ",", "process_group_backend", "=", "process_group_backend", ",", ")", "self", ".", "_timeout", ":", "Optional", "[", "timedelta", "]", "=", "timeout", "self", ".", "config", "=", "self", ".", "_load_config", "(", "config", ")", "if", "self", ".", "config", "is", "None", ":", "self", ".", "config", "=", "self", ".", "_create_default_config", "(", "zero_optimization", ",", "zero_allow_untested_optimizer", ",", "logging_batch_size_per_gpu", ",", "offload_optimizer", "=", "offload_optimizer", ",", "offload_parameters", "=", "offload_parameters", ",", "nvme_path", "=", "nvme_path", ",", "offload_params_device", "=", "offload_params_device", ",", "params_buffer_count", "=", "params_buffer_count", ",", "params_buffer_size", "=", "params_buffer_size", ",", "max_in_cpu", "=", "max_in_cpu", ",", "pin_memory", "=", "pin_memory", ",", "offload_optimizer_device", "=", "offload_optimizer_device", ",", "optimizer_buffer_count", "=", "optimizer_buffer_count", ",", "block_size", "=", "block_size", ",", "queue_depth", "=", "queue_depth", ",", "single_submit", "=", "single_submit", ",", "overlap_events", "=", "overlap_events", ",", "thread_count", "=", "thread_count", ",", "partition_activations", "=", "partition_activations", ",", "cpu_checkpointing", "=", "cpu_checkpointing", ",", "contiguous_memory_optimization", "=", "contiguous_memory_optimization", ",", "synchronize_checkpoint_boundary", "=", "synchronize_checkpoint_boundary", ",", "stage", "=", "stage", ",", "contiguous_gradients"], "docstring": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large", "docstring_tokens": ["provides", "capabilities", "to", "run", "training", "using", "the", "deepspeed", "library", "with", "training", "optimizations", "for", "large"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 79, "end_line": 337, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_819", "original_string": "def _setup_model_and_optimizers(\r\n        self, model: Module, optimizers: list[Optimizer]\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        Currently only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine` and a list with a single\r\n            deepspeed optimizer.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        assert self.config is not None\r\n        self.config.setdefault(\"train_micro_batch_size_per_gpu\", 1)\r\n        self.model, optimizer = self._setup_model_and_optimizer(model, optimizers[0])\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self.model, [optimizer]", "language": "python", "code": "def _setup_model_and_optimizers(\r\n        self, model: Module, optimizers: list[Optimizer]\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        Currently only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine` and a list with a single\r\n            deepspeed optimizer.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        assert self.config is not None\r\n        self.config.setdefault(\"train_micro_batch_size_per_gpu\", 1)\r\n        self.model, optimizer = self._setup_model_and_optimizer(model, optimizers[0])\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self.model, [optimizer]", "code_tokens": ["def", "_setup_model_and_optimizers", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ")", "-", ">", "tuple", "[", "STRING", ",", "list", "[", "Optimizer", "]", "]", ":", "STRING", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "fSTRING", ")", "assert", "self", ".", "config", "is", "not", "None", "self", ".", "config", ".", "setdefault", "(", "STRING", ",", "1", ")", "self", ".", "model", ",", "optimizer", "=", "self", ".", "_setup_model_and_optimizer", "(", "model", ",", "optimizers", "[", "0", "]", ")", "self", ".", "_set_deepspeed_activation_checkpointing", "(", ")", "return", "self", ".", "model", ",", "[", "optimizer", "]"], "docstring": "Setup a model and multiple optimizers together.", "docstring_tokens": ["setup", "a", "model", "and", "multiple", "optimizers", "together"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 406, "end_line": 430, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_820", "original_string": "def _setup_model_and_optimizer(\r\n        self,\r\n        model: Module,\r\n        optimizer: Optional[Optimizer],\r\n        lr_scheduler: Optional[Union[LRScheduler, ReduceLROnPlateau]] = None,\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", Optimizer]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=lr_scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer", "language": "python", "code": "def _setup_model_and_optimizer(\r\n        self,\r\n        model: Module,\r\n        optimizer: Optional[Optimizer],\r\n        lr_scheduler: Optional[Union[LRScheduler, ReduceLROnPlateau]] = None,\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", Optimizer]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=lr_scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer", "code_tokens": ["def", "_setup_model_and_optimizer", "(", "self", ",", "model", ":", "Module", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", ",", "lr_scheduler", ":", "Optional", "[", "Union", "[", "LRScheduler", ",", "ReduceLROnPlateau", "]", "]", "=", "None", ",", ")", "-", ">", "tuple", "[", "STRING", ",", "Optimizer", "]", ":", "STRING", "import", "deepspeed", "model_parameters", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "_", ",", "_", "=", "deepspeed", ".", "initialize", "(", "args", "=", "argparse", ".", "Namespace", "(", "device_rank", "=", "self", ".", "root_device", ".", "index", ")", ",", "config", "=", "self", ".", "config", ",", "model", "=", "model", ",", "model_parameters", "=", "model_parameters", ",", "optimizer", "=", "optimizer", ",", "lr_scheduler", "=", "lr_scheduler", ",", "dist_init_required", "=", "False", ",", ")", "return", "deepspeed_engine", ",", "deepspeed_optimizer"], "docstring": "Initialize one model and one optimizer with an optional learning rate scheduler.", "docstring_tokens": ["initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 432, "end_line": 455, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_821", "original_string": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        self.optimizers = []\r\n        self.lr_scheduler_configs = []", "language": "python", "code": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        self.optimizers = []\r\n        self.lr_scheduler_configs = []", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "self", ".", "optimizers", "=", "[", "]", "self", ".", "lr_scheduler_configs", "=", "[", "]"], "docstring": "Creates optimizers and schedulers.", "docstring_tokens": ["creates", "optimizers", "and", "schedulers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 602, "end_line": 614, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_822", "original_string": "def handles_gradient_accumulation(self) -> bool:\r\n        \"\"\"Whether the strategy handles gradient accumulation internally.\"\"\"\r\n        return True", "language": "python", "code": "def handles_gradient_accumulation(self) -> bool:\r\n        \"\"\"Whether the strategy handles gradient accumulation internally.\"\"\"\r\n        return True", "code_tokens": ["def", "handles_gradient_accumulation", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "True"], "docstring": "Whether the strategy handles gradient accumulation internally.", "docstring_tokens": ["whether", "the", "strategy", "handles", "gradient", "accumulation", "internally"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 621, "end_line": 623, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_823", "original_string": "def save_checkpoint(self, checkpoint: dict, filepath: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: The checkpoint state dictionary\r\n            filepath: write-target file's path\r\n            storage_options: not used for ``DeepSpeedStrategy`` as ``CheckpointIO`` is not used\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        filepath = self.broadcast(filepath)\r\n\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}` as `CheckpointIO` is not used.\"\r\n            )\r\n\r\n        if self.zero_stage_3 and self._multi_device and self.is_global_zero:\r\n            warning_cache.warn(\r\n                \"When saving the DeepSpeed Stage 3 checkpoint, \"\r\n                \"each worker will save a shard of the checkpoint within a directory. \"\r\n                \"If a single file is required after training, \"\r\n                \"see https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#\"\r\n                \"deepspeed-zero-stage-3-single-file for instructions.\"\r\n            )\r\n        _exclude_keys = [\"state_dict\", \"optimizer_states\"]\r\n        checkpoint = {k: v for k, v in checkpoint.items() if k not in _exclude_keys}\r\n        self.deepspeed_engine.save_checkpoint(\r\n            filepath,\r\n            client_state=checkpoint,\r\n            tag=\"checkpoint\",\r\n            exclude_frozen_parameters=self.exclude_frozen_parameters,\r\n        )", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict, filepath: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: The checkpoint state dictionary\r\n            filepath: write-target file's path\r\n            storage_options: not used for ``DeepSpeedStrategy`` as ``CheckpointIO`` is not used\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        filepath = self.broadcast(filepath)\r\n\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}` as `CheckpointIO` is not used.\"\r\n            )\r\n\r\n        if self.zero_stage_3 and self._multi_device and self.is_global_zero:\r\n            warning_cache.warn(\r\n                \"When saving the DeepSpeed Stage 3 checkpoint, \"\r\n                \"each worker will save a shard of the checkpoint within a directory. \"\r\n                \"If a single file is required after training, \"\r\n                \"see https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#\"\r\n                \"deepspeed-zero-stage-3-single-file for instructions.\"\r\n            )\r\n        _exclude_keys = [\"state_dict\", \"optimizer_states\"]\r\n        checkpoint = {k: v for k, v in checkpoint.items() if k not in _exclude_keys}\r\n        self.deepspeed_engine.save_checkpoint(\r\n            filepath,\r\n            client_state=checkpoint,\r\n            tag=\"checkpoint\",\r\n            exclude_frozen_parameters=self.exclude_frozen_parameters,\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", ",", "filepath", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "filepath", "=", "self", ".", "broadcast", "(", "filepath", ")", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "STRING", "fSTRING", ")", "if", "self", ".", "zero_stage_3", "and", "self", ".", "_multi_device", "and", "self", ".", "is_global_zero", ":", "warning_cache", ".", "warn", "(", "STRING", "STRING", "STRING", "STRING", "STRING", ")", "_exclude_keys", "=", "[", "STRING", ",", "STRING", "]", "checkpoint", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "checkpoint", ".", "items", "(", ")", "if", "k", "not", "in", "_exclude_keys", "}", "self", ".", "deepspeed_engine", ".", "save_checkpoint", "(", "filepath", ",", "client_state", "=", "checkpoint", ",", "tag", "=", "STRING", ",", "exclude_frozen_parameters", "=", "self", ".", "exclude_frozen_parameters", ",", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 634, "end_line": 673, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "function_824", "original_string": "def _restore_zero_state(self, ckpt: Mapping[str, Any], strict: bool) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        assert self.lightning_module is not None\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=strict,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(self.lightning_module, prefix=\"\")", "language": "python", "code": "def _restore_zero_state(self, ckpt: Mapping[str, Any], strict: bool) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        assert self.lightning_module is not None\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=strict,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(self.lightning_module, prefix=\"\")", "code_tokens": ["def", "_restore_zero_state", "(", "self", ",", "ckpt", ":", "Mapping", "[", "str", ",", "Any", "]", ",", "strict", ":", "bool", ")", "-", ">", "None", ":", "STRING", "import", "deepspeed", "assert", "self", ".", "lightning_module", "is", "not", "None", "def", "load", "(", "module", ":", "torch", ".", "nn", ".", "Module", ",", "prefix", ":", "str", "=", "STRING", ")", "-", ">", "None", ":", "missing_keys", ":", "list", "[", "str", "]", "=", "[", "]", "unexpected_keys", ":", "list", "[", "str", "]", "=", "[", "]", "error_msgs", ":", "list", "[", "str", "]", "=", "[", "]", "state_dict", "=", "ckpt", "[", "STRING", "]", "metadata", "=", "getattr", "(", "state_dict", ",", "STRING", ",", "None", ")", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "if", "metadata", "is", "not", "None", ":", "state_dict", ".", "_metadata", "=", "metadata", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "with", "deepspeed", ".", "zero", ".", "GatheredParameters", "(", "list", "(", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ")", ",", "modifier_rank", "=", "0", ")", ":", "if", "self", ".", "is_global_zero", ":", "module", ".", "_load_from_state_dict", "(", "state_dict", "=", "state_dict", ",", "prefix", "=", "prefix", ",", "local_metadata", "=", "local_metadata", ",", "strict", "=", "strict", ",", "missing_keys", "=", "missing_keys", ",", "unexpected_keys", "=", "unexpected_keys", ",", "error_msgs", "=", "error_msgs", ",", ")", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "if", "child", "is", "not", "None", ":", "load", "(", "child", ",", "prefix", "+", "name", "+", "STRING", ")", "load", "(", "self", ".", "lightning_module", ",", "prefix", "=", "STRING", ")"], "docstring": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded", "docstring_tokens": ["overrides", "the", "normal", "load_state_dict", "behaviour", "in", "pytorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "start_line": 725, "end_line": 770, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "func_name": "function_825", "original_string": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in model.modules()):\r\n            if _has_meta_device_parameters_or_buffers(model):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self.kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self.kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            log.debug(f\"setting up FSDP model with device id: {self.root_device.index}, kwargs: {self.kwargs}\")\r\n            model = FullyShardedDataParallel(\r\n                module=model,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self.kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(model, self.root_device)\r\n\r\n        _setup_activation_checkpointing(model, self._activation_checkpointing_kwargs)\r\n\r\n        return model", "language": "python", "code": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in model.modules()):\r\n            if _has_meta_device_parameters_or_buffers(model):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self.kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self.kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            log.debug(f\"setting up FSDP model with device id: {self.root_device.index}, kwargs: {self.kwargs}\")\r\n            model = FullyShardedDataParallel(\r\n                module=model,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self.kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(model, self.root_device)\r\n\r\n        _setup_activation_checkpointing(model, self._activation_checkpointing_kwargs)\r\n\r\n        return model", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "if", "any", "(", "isinstance", "(", "mod", ",", "FullyShardedDataParallel", ")", "for", "mod", "in", "model", ".", "modules", "(", ")", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "model", ")", ":", "rank_zero_warn", "(", "STRING", ")", "if", "STRING", "in", "self", ".", "kwargs", ":", "rank_zero_warn", "(", "STRING", ")", "del", "self", ".", "kwargs", "[", "STRING", "]", "else", ":", "log", ".", "debug", "(", "fSTRING", ")", "model", "=", "FullyShardedDataParallel", "(", "module", "=", "model", ",", "cpu_offload", "=", "self", ".", "cpu_offload", ",", "mixed_precision", "=", "self", ".", "mixed_precision_config", ",", "sharding_strategy", "=", "self", ".", "sharding_strategy", ",", "device_id", "=", "self", ".", "root_device", ".", "index", ",", "*", "*", "self", ".", "kwargs", ",", ")", "_move_torchmetrics_to_device", "(", "model", ",", "self", ".", "root_device", ")", "_setup_activation_checkpointing", "(", "model", ",", "self", ".", "_activation_checkpointing_kwargs", ")", "return", "model"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "start_line": 291, "end_line": 323, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "func_name": "function_826", "original_string": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ",", ")", "-", ">", "Tensor", ":", "STRING", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "start_line": 432, "end_line": 452, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "func_name": "function_827", "original_string": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Collects the state dict of the model.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        assert self.model is not None\r\n        return get_model_state_dict(self.model, options=state_dict_options)", "language": "python", "code": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Collects the state dict of the model.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        assert self.model is not None\r\n        return get_model_state_dict(self.model, options=state_dict_options)", "code_tokens": ["def", "lightning_module_state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "get_model_state_dict", "state_dict_options", "=", "StateDictOptions", "(", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "cpu_offload", "=", "True", ")", "assert", "self", ".", "model", "is", "not", "None", "return", "get_model_state_dict", "(", "self", ".", "model", ",", "options", "=", "state_dict_options", ")"], "docstring": "Collects the state dict of the model.", "docstring_tokens": ["collects", "the", "state", "dict", "of", "the", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "start_line": 252, "end_line": 262, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "func_name": "function_828", "original_string": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Any]:\r\n        \"\"\"Collects the state of the given optimizer.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n        from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        assert self.model is not None\r\n\r\n        state_dict = get_optimizer_state_dict(self.model, optimizer, options=state_dict_options)\r\n        if not self._save_distributed_checkpoint and self.global_rank == 0:\r\n            state_dict = FSDP.rekey_optim_state_dict(state_dict, OptimStateKeyType.PARAM_ID, self.model)\r\n        return state_dict", "language": "python", "code": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Any]:\r\n        \"\"\"Collects the state of the given optimizer.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n        from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        assert self.model is not None\r\n\r\n        state_dict = get_optimizer_state_dict(self.model, optimizer, options=state_dict_options)\r\n        if not self._save_distributed_checkpoint and self.global_rank == 0:\r\n            state_dict = FSDP.rekey_optim_state_dict(state_dict, OptimStateKeyType.PARAM_ID, self.model)\r\n        return state_dict", "code_tokens": ["def", "optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "get_optimizer_state_dict", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "from", "torch", ".", "distributed", ".", "fsdp", "import", "OptimStateKeyType", "state_dict_options", "=", "StateDictOptions", "(", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "cpu_offload", "=", "True", ")", "if", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", ":", "optimizer", "=", "optimizer", ".", "_optimizer", "assert", "self", ".", "model", "is", "not", "None", "state_dict", "=", "get_optimizer_state_dict", "(", "self", ".", "model", ",", "optimizer", ",", "options", "=", "state_dict_options", ")", "if", "not", "self", ".", "_save_distributed_checkpoint", "and", "self", ".", "global_rank", "=", "=", "0", ":", "state_dict", "=", "FSDP", ".", "rekey_optim_state_dict", "(", "state_dict", ",", "OptimStateKeyType", ".", "PARAM_ID", ",", "self", ".", "model", ")", "return", "state_dict"], "docstring": "Collects the state of the given optimizer.", "docstring_tokens": ["collects", "the", "state", "of", "the", "given", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "start_line": 270, "end_line": 290, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "function_829", "original_string": "def root_device(self) -> torch.device:\r\n        \"\"\"Return the root device.\"\"\"", "language": "python", "code": "def root_device(self) -> torch.device:\r\n        \"\"\"Return the root device.\"\"\"", "code_tokens": ["def", "root_device", "(", "self", ")", "-", ">", "torch", ".", "device", ":", "STRING"], "docstring": "Return the root device.", "docstring_tokens": ["return", "the", "root", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\parallel.py", "start_line": 49, "end_line": 50, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "function_830", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return _all_gather_ddp_if_available(tensor, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return _all_gather_ddp_if_available(tensor, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "return", "_all_gather_ddp_if_available", "(", "tensor", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "Perform a all_gather on all processes.", "docstring_tokens": ["perform", "a", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\parallel.py", "start_line": 89, "end_line": 91, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "function_831", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "STRING", "decision", "=", "torch", ".", "tensor", "(", "int", "(", "decision", ")", ",", "device", "=", "self", ".", "root_device", ")", "decision", "=", "self", ".", "reduce", "(", "decision", ",", "reduce_op", "=", "ReduceOp", ".", "SUM", ",", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", ")", "decision", "=", "bool", "(", "decision", "=", "=", "self", ".", "world_size", ")", "if", "all", "else", "bool", "(", "decision", ")", "return", "decision"], "docstring": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard", "docstring_tokens": ["reduces", "a", "boolean", "decision", "over", "distributed", "processes", "by", "default", "is", "analogous", "to", "all", "from", "the", "standard"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\parallel.py", "start_line": 94, "end_line": 113, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "function_832", "original_string": "def block_backward_sync(self) -> Generator:\r\n        \"\"\"Blocks ddp sync gradients behaviour on backwards pass.\r\n\r\n        This is useful for skipping sync when accumulating gradients, reducing communication overhead\r\n        Returns: context manager with sync behaviour off\r\n\r\n        \"\"\"\r\n        if isinstance(self.model, pl.utilities.types.DistributedDataParallel):\r\n            with self.model.no_sync():\r\n                yield None\r\n        else:\r\n            yield None", "language": "python", "code": "def block_backward_sync(self) -> Generator:\r\n        \"\"\"Blocks ddp sync gradients behaviour on backwards pass.\r\n\r\n        This is useful for skipping sync when accumulating gradients, reducing communication overhead\r\n        Returns: context manager with sync behaviour off\r\n\r\n        \"\"\"\r\n        if isinstance(self.model, pl.utilities.types.DistributedDataParallel):\r\n            with self.model.no_sync():\r\n                yield None\r\n        else:\r\n            yield None", "code_tokens": ["def", "block_backward_sync", "(", "self", ")", "-", ">", "Generator", ":", "STRING", "if", "isinstance", "(", "self", ".", "model", ",", "pl", ".", "utilities", ".", "types", ".", "DistributedDataParallel", ")", ":", "with", "self", ".", "model", ".", "no_sync", "(", ")", ":", "yield", "None", "else", ":", "yield", "None"], "docstring": "Blocks ddp sync gradients behaviour on backwards pass.", "docstring_tokens": ["blocks", "ddp", "sync", "gradients", "behaviour", "on", "backwards", "pass"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\parallel.py", "start_line": 116, "end_line": 127, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\single_device.py", "func_name": "function_833", "original_string": "def reduce(self, tensor: Any | Tensor, *args: Any, **kwargs: Any) -> Any | Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only\r\n        operates with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "language": "python", "code": "def reduce(self, tensor: Any | Tensor, *args: Any, **kwargs: Any) -> Any | Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only\r\n        operates with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Any", "|", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", "|", "Tensor", ":", "STRING", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "since", "this", "strategy", "only"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\single_device.py", "start_line": 50, "end_line": 63, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\single_device.py", "func_name": "function_834", "original_string": "def all_gather(self, tensor: Tensor, group: Any | None = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Any | None = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform a all_gather on all processes.\"\"\"\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Any", "|", "None", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "return", "tensor"], "docstring": "Perform a all_gather on all processes.", "docstring_tokens": ["perform", "a", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\single_device.py", "start_line": 66, "end_line": 68, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_835", "original_string": "def connect(self, model: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called by the Trainer to connect the strategy with the model.\"\"\"\r\n        self._lightning_module = model\r\n        self.model = model", "language": "python", "code": "def connect(self, model: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called by the Trainer to connect the strategy with the model.\"\"\"\r\n        self._lightning_module = model\r\n        self.model = model", "code_tokens": ["def", "connect", "(", "self", ",", "model", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "self", ".", "_lightning_module", "=", "model", "self", ".", "model", "=", "model"], "docstring": "Called by the Trainer to connect the strategy with the model.", "docstring_tokens": ["called", "by", "the", "trainer", "to", "connect", "the", "strategy", "with", "the", "model"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 110, "end_line": 115, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_836", "original_string": "def _configure_launcher(self) -> None:\r\n        \"\"\"Attach the launcher based on Strategy.\"\"\"", "language": "python", "code": "def _configure_launcher(self) -> None:\r\n        \"\"\"Attach the launcher based on Strategy.\"\"\"", "code_tokens": ["def", "_configure_launcher", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Attach the launcher based on Strategy.", "docstring_tokens": ["attach", "the", "launcher", "based", "on", "strategy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 117, "end_line": 118, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_837", "original_string": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\r\n        environment before setup is complete.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "language": "python", "code": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\r\n        environment before setup is complete.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "code_tokens": ["def", "setup_environment", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup_device", "(", "self", ".", "root_device", ")"], "docstring": "Setup any processes or distributed connections.", "docstring_tokens": ["setup", "any", "processes", "or", "distributed", "connections"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 120, "end_line": 128, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_838", "original_string": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)", "language": "python", "code": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "self", ".", "optimizers", ",", "self", ".", "lr_scheduler_configs", "=", "_init_optimizers_and_lr_schedulers", "(", "self", ".", "lightning_module", ")"], "docstring": "Creates optimizers and schedulers.", "docstring_tokens": ["creates", "optimizers", "and", "schedulers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 130, "end_line": 138, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_839", "original_string": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Sets up the accelerator, plugins and initializes the optimizers (if needed).\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup(trainer)\r\n\r\n        assert self.model is not None\r\n        self.model = self.precision_plugin.convert_module(self.model)\r\n        self.model_to_device()\r\n        self.model = self._setup_model(self.model)\r\n\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            self.setup_optimizers(trainer)\r\n        self.setup_precision_plugin()\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            _optimizers_to_device(self.optimizers, self.root_device)", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Sets up the accelerator, plugins and initializes the optimizers (if needed).\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup(trainer)\r\n\r\n        assert self.model is not None\r\n        self.model = self.precision_plugin.convert_module(self.model)\r\n        self.model_to_device()\r\n        self.model = self._setup_model(self.model)\r\n\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            self.setup_optimizers(trainer)\r\n        self.setup_precision_plugin()\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            _optimizers_to_device(self.optimizers, self.root_device)", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup", "(", "trainer", ")", "assert", "self", ".", "model", "is", "not", "None", "self", ".", "model", "=", "self", ".", "precision_plugin", ".", "convert_module", "(", "self", ".", "model", ")", "self", ".", "model_to_device", "(", ")", "self", ".", "model", "=", "self", ".", "_setup_model", "(", "self", ".", "model", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "setup_optimizers", "(", "trainer", ")", "self", ".", "setup_precision_plugin", "(", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "_optimizers_to_device", "(", "self", ".", "optimizers", ",", "self", ".", "root_device", ")"], "docstring": "Sets up the accelerator, plugins and initializes the optimizers (if needed).", "docstring_tokens": ["sets", "up", "the", "accelerator", "plugins", "and", "initializes", "the", "optimizers", "if", "needed"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 140, "end_line": 161, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_840", "original_string": "def setup_precision_plugin(self) -> None:\r\n        \"\"\"Attaches the precision plugin to the strategy.\"\"\"\r\n        assert self.model is not None\r\n        model, optimizers, lr_scheduler_configs = self.precision_plugin.connect(\r\n            self.model, self.optimizers, self.lr_scheduler_configs\r\n        )\r\n        self.model = model\r\n        self.optimizers = optimizers\r\n        self.lr_scheduler_configs = lr_scheduler_configs", "language": "python", "code": "def setup_precision_plugin(self) -> None:\r\n        \"\"\"Attaches the precision plugin to the strategy.\"\"\"\r\n        assert self.model is not None\r\n        model, optimizers, lr_scheduler_configs = self.precision_plugin.connect(\r\n            self.model, self.optimizers, self.lr_scheduler_configs\r\n        )\r\n        self.model = model\r\n        self.optimizers = optimizers\r\n        self.lr_scheduler_configs = lr_scheduler_configs", "code_tokens": ["def", "setup_precision_plugin", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "model", "is", "not", "None", "model", ",", "optimizers", ",", "lr_scheduler_configs", "=", "self", ".", "precision_plugin", ".", "connect", "(", "self", ".", "model", ",", "self", ".", "optimizers", ",", "self", ".", "lr_scheduler_configs", ")", "self", ".", "model", "=", "model", "self", ".", "optimizers", "=", "optimizers", "self", ".", "lr_scheduler_configs", "=", "lr_scheduler_configs"], "docstring": "Attaches the precision plugin to the strategy.", "docstring_tokens": ["attaches", "the", "precision", "plugin", "to", "the", "strategy"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 163, "end_line": 171, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_841", "original_string": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom strategies.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        return optimizer.state_dict()", "language": "python", "code": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom strategies.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        return optimizer.state_dict()", "code_tokens": ["def", "optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "STRING", "if", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", ":", "optimizer", "=", "optimizer", ".", "_optimizer", "if", "hasattr", "(", "optimizer", ",", "STRING", ")", ":", "optimizer", ".", "consolidate_state_dict", "(", ")", "return", "optimizer", ".", "state_dict", "(", ")", "if", "self", ".", "is_global_zero", "else", "{", "}", "return", "optimizer", ".", "state_dict", "(", ")"], "docstring": "Returns state of an optimizer.", "docstring_tokens": ["returns", "state", "of", "an", "optimizer"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 173, "end_line": 189, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_842", "original_string": "def backward(\r\n        self,\r\n        closure_loss: Tensor,\r\n        optimizer: Optional[Optimizer],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> Tensor:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\r\n\r\n        Args:\r\n            closure_loss: a tensor holding the loss value to backpropagate\r\n            optimizer: An optional optimizer that gets passed down to the precision plugin's backward\r\n            \\*args: Positional arguments that get passed down to the precision plugin's backward, intended as arguments\r\n                for the actual function that performs the backward, like :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        self.pre_backward(closure_loss)\r\n        assert self.lightning_module is not None\r\n        closure_loss = self.precision_plugin.pre_backward(closure_loss, self.lightning_module)\r\n\r\n        self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\r\n\r\n        closure_loss = self.precision_plugin.post_backward(closure_loss, self.lightning_module)\r\n        self.post_backward(closure_loss)\r\n\r\n        return closure_loss", "language": "python", "code": "def backward(\r\n        self,\r\n        closure_loss: Tensor,\r\n        optimizer: Optional[Optimizer],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> Tensor:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\r\n\r\n        Args:\r\n            closure_loss: a tensor holding the loss value to backpropagate\r\n            optimizer: An optional optimizer that gets passed down to the precision plugin's backward\r\n            \\*args: Positional arguments that get passed down to the precision plugin's backward, intended as arguments\r\n                for the actual function that performs the backward, like :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        self.pre_backward(closure_loss)\r\n        assert self.lightning_module is not None\r\n        closure_loss = self.precision_plugin.pre_backward(closure_loss, self.lightning_module)\r\n\r\n        self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\r\n\r\n        closure_loss = self.precision_plugin.post_backward(closure_loss, self.lightning_module)\r\n        self.post_backward(closure_loss)\r\n\r\n        return closure_loss", "code_tokens": ["def", "backward", "(", "self", ",", "closure_loss", ":", "Tensor", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Tensor", ":", "rSTRING", "self", ".", "pre_backward", "(", "closure_loss", ")", "assert", "self", ".", "lightning_module", "is", "not", "None", "closure_loss", "=", "self", ".", "precision_plugin", ".", "pre_backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ")", "self", ".", "precision_plugin", ".", "backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ",", "optimizer", ",", "*", "args", ",", "*", "*", "kwargs", ")", "closure_loss", "=", "self", ".", "precision_plugin", ".", "post_backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ")", "self", ".", "post_backward", "(", "closure_loss", ")", "return", "closure_loss"], "docstring": "r\"\"\"Forwards backward-calls to the precision plugin.", "docstring_tokens": ["r", "forwards", "backward", "calls", "to", "the", "precision", "plugin"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 191, "end_line": 217, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_843", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        r\"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            \\**kwargs: Keyword arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        model = model or self.lightning_module\r\n        assert isinstance(model, pl.LightningModule)\r\n        return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        r\"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            \\**kwargs: Keyword arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        model = model or self.lightning_module\r\n        assert isinstance(model, pl.LightningModule)\r\n        return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "model", ":", "Optional", "[", "Union", "[", "STRING", ",", "Module", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "rSTRING", "model", "=", "model", "or", "self", ".", "lightning_module", "assert", "isinstance", "(", "model", ",", "pl", ".", "LightningModule", ")", "return", "self", ".", "precision_plugin", ".", "optimizer_step", "(", "optimizer", ",", "model", "=", "model", ",", "closure", "=", "closure", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs the actual optimizer step.", "docstring_tokens": ["r", "performs", "the", "actual", "optimizer", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 219, "end_line": 238, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_844", "original_string": "def _setup_model_and_optimizers(self, model: Module, optimizers: list[Optimizer]) -> tuple[Module, list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`_setup_model` and :meth:`_setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        model = self._setup_model(model)\r\n        optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return model, optimizers", "language": "python", "code": "def _setup_model_and_optimizers(self, model: Module, optimizers: list[Optimizer]) -> tuple[Module, list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`_setup_model` and :meth:`_setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        model = self._setup_model(model)\r\n        optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return model, optimizers", "code_tokens": ["def", "_setup_model_and_optimizers", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", "]", ":", "STRING", "model", "=", "self", ".", "_setup_model", "(", "model", ")", "optimizers", "=", "[", "self", ".", "_setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "return", "model", ",", "optimizers"], "docstring": "Setup a model and multiple optimizers together.", "docstring_tokens": ["setup", "a", "model", "and", "multiple", "optimizers", "together"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 240, "end_line": 250, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_845", "original_string": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        return model", "language": "python", "code": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        return model", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "STRING", "return", "model"], "docstring": "Performs setup for the model, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "model", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 252, "end_line": 255, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_846", "original_string": "def _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        return optimizer", "language": "python", "code": "def _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        return optimizer", "code_tokens": ["def", "_setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "STRING", "return", "optimizer"], "docstring": "Performs setup for the optimizer, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "optimizer", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 257, "end_line": 260, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_847", "original_string": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None, dataloader_idx: int = 0) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        \"\"\"\r\n        model = self.lightning_module\r\n        device = device or self.root_device\r\n        if model is not None:\r\n            return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None, dataloader_idx: int = 0) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        \"\"\"\r\n        model = self.lightning_module\r\n        device = device or self.root_device\r\n        if model is not None:\r\n            return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "Any", ":", "STRING", "model", "=", "self", ".", "lightning_module", "device", "=", "device", "or", "self", ".", "root_device", "if", "model", "is", "not", "None", ":", "return", "model", ".", "_apply_batch_transfer_handler", "(", "batch", ",", "device", "=", "device", ",", "dataloader_idx", "=", "dataloader_idx", ")", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Moves the batch to the correct device.", "docstring_tokens": ["moves", "the", "batch", "to", "the", "correct", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 262, "end_line": 278, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_848", "original_string": "def root_device(self) -> torch.device:\r\n        \"\"\"Returns the root device.\"\"\"", "language": "python", "code": "def root_device(self) -> torch.device:\r\n        \"\"\"Returns the root device.\"\"\"", "code_tokens": ["def", "root_device", "(", "self", ")", "-", ">", "torch", ".", "device", ":", "STRING"], "docstring": "Returns the root device.", "docstring_tokens": ["returns", "the", "root", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 282, "end_line": 283, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_849", "original_string": "def model_to_device(self) -> None:\r\n        \"\"\"Moves the model to the correct device.\"\"\"", "language": "python", "code": "def model_to_device(self) -> None:\r\n        \"\"\"Moves the model to the correct device.\"\"\"", "code_tokens": ["def", "model_to_device", "(", "self", ")", "-", ">", "None", ":", "STRING"], "docstring": "Moves the model to the correct device.", "docstring_tokens": ["moves", "the", "model", "to", "the", "correct", "device"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 286, "end_line": 287, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_850", "original_string": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether the current process is the rank zero process not only on the local node, but for all nodes.\"\"\"", "language": "python", "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether the current process is the rank zero process not only on the local node, but for all nodes.\"\"\"", "code_tokens": ["def", "is_global_zero", "(", "self", ")", "-", ">", "bool", ":", "STRING"], "docstring": "Whether the current process is the rank zero process not only on the local node, but for all nodes.", "docstring_tokens": ["whether", "the", "current", "process", "is", "the", "rank", "zero", "process", "not", "only", "on", "the", "local", "node", "but", "for", "all", "nodes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 291, "end_line": 292, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_851", "original_string": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "language": "python", "code": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "STRING", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "Any", "]", ":", "STRING"], "docstring": "Reduces the given tensor (e.g. across GPUs/processes).", "docstring_tokens": ["reduces", "the", "given", "tensor", "e", "g", "across", "gpus", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 295, "end_line": 309, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_852", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "STRING"], "docstring": "Synchronizes all processes which blocks processes until the whole group enters this function.", "docstring_tokens": ["synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 312, "end_line": 318, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_853", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "STRING"], "docstring": "Broadcasts an object to all processes.", "docstring_tokens": ["broadcasts", "an", "object", "to", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 321, "end_line": 328, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_854", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING"], "docstring": "Perform an all_gather on all processes.", "docstring_tokens": ["perform", "an", "all_gather", "on", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 331, "end_line": 339, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_855", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduce a boolean decision across all processes.\"\"\"\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduce a boolean decision across all processes.\"\"\"\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "STRING", "return", "decision"], "docstring": "Reduce a boolean decision across all processes.", "docstring_tokens": ["reduce", "a", "boolean", "decision", "across", "all", "processes"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 341, "end_line": 343, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_856", "original_string": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"", "language": "python", "code": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"", "code_tokens": ["def", "pre_backward", "(", "self", ",", "closure_loss", ":", "Tensor", ")", "-", ">", "None", ":", "STRING"], "docstring": "Run before precision plugin executes backward.", "docstring_tokens": ["run", "before", "precision", "plugin", "executes", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 345, "end_line": 346, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_857", "original_string": "def post_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run after precision plugin executes backward.\"\"\"", "language": "python", "code": "def post_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run after precision plugin executes backward.\"\"\"", "code_tokens": ["def", "post_backward", "(", "self", ",", "closure_loss", ":", "Tensor", ")", "-", ">", "None", ":", "STRING"], "docstring": "Run after precision plugin executes backward.", "docstring_tokens": ["run", "after", "precision", "plugin", "executes", "backward"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 348, "end_line": 349, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_858", "original_string": "def model(self) -> Optional[Module]:\r\n        \"\"\"Returns the potentially wrapped LightningModule.\"\"\"\r\n        return self._model if self._model is not None else self._lightning_module", "language": "python", "code": "def model(self) -> Optional[Module]:\r\n        \"\"\"Returns the potentially wrapped LightningModule.\"\"\"\r\n        return self._model if self._model is not None else self._lightning_module", "code_tokens": ["def", "model", "(", "self", ")", "-", ">", "Optional", "[", "Module", "]", ":", "STRING", "return", "self", ".", "_model", "if", "self", ".", "_model", "is", "not", "None", "else", "self", ".", "_lightning_module"], "docstring": "Returns the potentially wrapped LightningModule.", "docstring_tokens": ["returns", "the", "potentially", "wrapped", "lightningmodule"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 352, "end_line": 354, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_859", "original_string": "def lightning_module(self) -> Optional[\"pl.LightningModule\"]:\r\n        \"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\r\n        return self._lightning_module", "language": "python", "code": "def lightning_module(self) -> Optional[\"pl.LightningModule\"]:\r\n        \"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\r\n        return self._lightning_module", "code_tokens": ["def", "lightning_module", "(", "self", ")", "-", ">", "Optional", "[", "STRING", "]", ":", "STRING", "return", "self", ".", "_lightning_module"], "docstring": "Returns the pure LightningModule without potential wrappers.", "docstring_tokens": ["returns", "the", "pure", "lightningmodule", "without", "potential", "wrappers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 361, "end_line": 363, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_860", "original_string": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual training step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.training_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.train_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\r\n            return self.lightning_module.training_step(*args, **kwargs)", "language": "python", "code": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual training step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.training_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.train_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\r\n            return self.lightning_module.training_step(*args, **kwargs)", "code_tokens": ["def", "training_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "train_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "STRING", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "training_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual training step.", "docstring_tokens": ["the", "actual", "training", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 379, "end_line": 390, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_861", "original_string": "def post_training_step(self) -> None:\r\n        \"\"\"This hook is deprecated.\r\n\r\n        Override :meth:`training_step` instead.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def post_training_step(self) -> None:\r\n        \"\"\"This hook is deprecated.\r\n\r\n        Override :meth:`training_step` instead.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "post_training_step", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "This hook is deprecated.", "docstring_tokens": ["this", "hook", "is", "deprecated"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 392, "end_line": 398, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_862", "original_string": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual validation step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.validation_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.val_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\r\n            return self.lightning_module.validation_step(*args, **kwargs)", "language": "python", "code": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual validation step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.validation_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.val_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\r\n            return self.lightning_module.validation_step(*args, **kwargs)", "code_tokens": ["def", "validation_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "val_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "STRING", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "validation_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual validation step.", "docstring_tokens": ["the", "actual", "validation", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 400, "end_line": 411, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_863", "original_string": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual test step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.test_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.test_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"test_step\", *args, **kwargs)\r\n            return self.lightning_module.test_step(*args, **kwargs)", "language": "python", "code": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        \"\"\"The actual test step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.test_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.test_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"test_step\", *args, **kwargs)\r\n            return self.lightning_module.test_step(*args, **kwargs)", "code_tokens": ["def", "test_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "test_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "STRING", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "test_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual test step.", "docstring_tokens": ["the", "actual", "test", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 413, "end_line": 424, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_864", "original_string": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"The actual predict step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.predict_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.predict_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"predict_step\", *args, **kwargs)\r\n            return self.lightning_module.predict_step(*args, **kwargs)", "language": "python", "code": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"The actual predict step.\r\n\r\n        See :meth:`~lightning.pytorch.core.LightningModule.predict_step` for more details\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        assert self.model is not None\r\n        with self.precision_plugin.predict_step_context():\r\n            if self.model != self.lightning_module:\r\n                return self._forward_redirection(self.model, self.lightning_module, \"predict_step\", *args, **kwargs)\r\n            return self.lightning_module.predict_step(*args, **kwargs)", "code_tokens": ["def", "predict_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "assert", "self", ".", "model", "is", "not", "None", "with", "self", ".", "precision_plugin", ".", "predict_step_context", "(", ")", ":", "if", "self", ".", "model", "!", "=", "self", ".", "lightning_module", ":", "return", "self", ".", "_forward_redirection", "(", "self", ".", "model", ",", "self", ".", "lightning_module", ",", "STRING", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "lightning_module", ".", "predict_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "The actual predict step.", "docstring_tokens": ["the", "actual", "predict", "step"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 426, "end_line": 437, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_865", "original_string": "def process_dataloader(self, dataloader: object) -> object:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "language": "python", "code": "def process_dataloader(self, dataloader: object) -> object:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "code_tokens": ["def", "process_dataloader", "(", "self", ",", "dataloader", ":", "object", ")", "-", ">", "object", ":", "STRING", "return", "dataloader"], "docstring": "Wraps the dataloader if necessary.", "docstring_tokens": ["wraps", "the", "dataloader", "if", "necessary"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 439, "end_line": 446, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_866", "original_string": "def restore_checkpoint_after_setup(self) -> bool:\r\n        \"\"\"Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when\r\n        the strategy requires all the setup hooks to run before loading checkpoint.\r\n\r\n        Returns:\r\n            If ``True``, restore checkpoint after strategy setup.\r\n\r\n        \"\"\"\r\n        return False", "language": "python", "code": "def restore_checkpoint_after_setup(self) -> bool:\r\n        \"\"\"Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when\r\n        the strategy requires all the setup hooks to run before loading checkpoint.\r\n\r\n        Returns:\r\n            If ``True``, restore checkpoint after strategy setup.\r\n\r\n        \"\"\"\r\n        return False", "code_tokens": ["def", "restore_checkpoint_after_setup", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "False"], "docstring": "Override to delay restoring from checkpoint till after the setup phase has completed. This is useful when", "docstring_tokens": ["override", "to", "delay", "restoring", "from", "checkpoint", "till", "after", "the", "setup", "phase", "has", "completed", "this", "is", "useful", "when"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 449, "end_line": 457, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_867", "original_string": "def lightning_restore_optimizer(self) -> bool:\r\n        \"\"\"Override to disable Lightning restoring optimizers/schedulers.\r\n\r\n        This is useful for strategies which manage restoring optimizers/schedulers.\r\n\r\n        \"\"\"\r\n        return True", "language": "python", "code": "def lightning_restore_optimizer(self) -> bool:\r\n        \"\"\"Override to disable Lightning restoring optimizers/schedulers.\r\n\r\n        This is useful for strategies which manage restoring optimizers/schedulers.\r\n\r\n        \"\"\"\r\n        return True", "code_tokens": ["def", "lightning_restore_optimizer", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "True"], "docstring": "Override to disable Lightning restoring optimizers/schedulers.", "docstring_tokens": ["override", "to", "disable", "lightning", "restoring", "optimizers", "schedulers"], "partition": "train", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 460, "end_line": 466, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_868", "original_string": "def handles_gradient_accumulation(self) -> bool:\r\n        \"\"\"Whether the strategy handles gradient accumulation internally.\"\"\"\r\n        return False", "language": "python", "code": "def handles_gradient_accumulation(self) -> bool:\r\n        \"\"\"Whether the strategy handles gradient accumulation internally.\"\"\"\r\n        return False", "code_tokens": ["def", "handles_gradient_accumulation", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "False"], "docstring": "Whether the strategy handles gradient accumulation internally.", "docstring_tokens": ["whether", "the", "strategy", "handles", "gradient", "accumulation", "internally"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 469, "end_line": 471, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_869", "original_string": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        assert self.lightning_module is not None\r\n        return self.lightning_module.state_dict()", "language": "python", "code": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Returns model state.\"\"\"\r\n        assert self.lightning_module is not None\r\n        return self.lightning_module.state_dict()", "code_tokens": ["def", "lightning_module_state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "assert", "self", ".", "lightning_module", "is", "not", "None", "return", "self", ".", "lightning_module", ".", "state_dict", "(", ")"], "docstring": "Returns model state.", "docstring_tokens": ["returns", "model", "state"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 473, "end_line": 476, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_870", "original_string": "def save_checkpoint(\r\n        self, checkpoint: dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            filepath: write-target file's path\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)", "language": "python", "code": "def save_checkpoint(\r\n        self, checkpoint: dict[str, Any], filepath: _PATH, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            filepath: write-target file's path\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "filepath", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "checkpoint", ",", "filepath", ",", "storage_options", "=", "storage_options", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 478, "end_line": 490, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_871", "original_string": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "language": "python", "code": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "filepath", ")"], "docstring": "Remove checkpoint filepath from the filesystem.", "docstring_tokens": ["remove", "checkpoint", "filepath", "from", "the", "filesystem"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 492, "end_line": 500, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_872", "original_string": "def tensor_init_context(self, empty_init: Optional[bool] = None) -> Generator[None, None, None]:\r\n        \"\"\"Controls how tensors get created (device, dtype).\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        empty_init_context = _EmptyInit(enabled=bool(empty_init))\r\n        with empty_init_context, self.root_device, self.precision_plugin.tensor_init_context():\r\n            yield", "language": "python", "code": "def tensor_init_context(self, empty_init: Optional[bool] = None) -> Generator[None, None, None]:\r\n        \"\"\"Controls how tensors get created (device, dtype).\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        empty_init_context = _EmptyInit(enabled=bool(empty_init))\r\n        with empty_init_context, self.root_device, self.precision_plugin.tensor_init_context():\r\n            yield", "code_tokens": ["def", "tensor_init_context", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "empty_init_context", "=", "_EmptyInit", "(", "enabled", "=", "bool", "(", "empty_init", ")", ")", "with", "empty_init_context", ",", "self", ".", "root_device", ",", "self", ".", "precision_plugin", ".", "tensor_init_context", "(", ")", ":", "yield"], "docstring": "Controls how tensors get created (device, dtype).", "docstring_tokens": ["controls", "how", "tensors", "get", "created", "device", "dtype"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 503, "end_line": 513, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_873", "original_string": "def model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield", "language": "python", "code": "def model_sharded_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard\r\n        the model instantly, which is useful for extremely large models which can save memory and initialization time.\r\n\r\n        Returns: Model parallel context.\r\n\r\n        \"\"\"\r\n        yield", "code_tokens": ["def", "model_sharded_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "yield"], "docstring": "Provide hook to create modules in a distributed aware context. This is useful for when we'd like to shard", "docstring_tokens": ["provide", "hook", "to", "create", "modules", "in", "a", "distributed", "aware", "context", "this", "is", "useful", "for", "when", "we", "d", "like", "to", "shard"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 516, "end_line": 523, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_874", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        _optimizers_to_device(self.optimizers, torch.device(\"cpu\"))\r\n\r\n        if self.lightning_module is not None:\r\n            log.debug(f\"{self.__class__.__name__}: moving model to CPU\")\r\n            self.lightning_module.cpu()\r\n        self.precision_plugin.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        _optimizers_to_device(self.optimizers, torch.device(\"cpu\"))\r\n\r\n        if self.lightning_module is not None:\r\n            log.debug(f\"{self.__class__.__name__}: moving model to CPU\")\r\n            self.lightning_module.cpu()\r\n        self.precision_plugin.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "_optimizers_to_device", "(", "self", ".", "optimizers", ",", "torch", ".", "device", "(", "STRING", ")", ")", "if", "self", ".", "lightning_module", "is", "not", "None", ":", "log", ".", "debug", "(", "fSTRING", ")", "self", ".", "lightning_module", ".", "cpu", "(", ")", "self", ".", "precision_plugin", ".", "teardown", "(", ")", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "teardown", "(", ")", "self", ".", "checkpoint_io", ".", "teardown", "(", ")"], "docstring": "This method is called to teardown the training process.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 525, "end_line": 539, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_875", "original_string": "def on_train_start(self) -> None:\r\n        \"\"\"Called when train begins.\"\"\"\r\n        pass", "language": "python", "code": "def on_train_start(self) -> None:\r\n        \"\"\"Called when train begins.\"\"\"\r\n        pass", "code_tokens": ["def", "on_train_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when train begins.", "docstring_tokens": ["called", "when", "train", "begins"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 545, "end_line": 547, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_876", "original_string": "def on_validation_start(self) -> None:\r\n        \"\"\"Called when validation begins.\"\"\"\r\n        pass", "language": "python", "code": "def on_validation_start(self) -> None:\r\n        \"\"\"Called when validation begins.\"\"\"\r\n        pass", "code_tokens": ["def", "on_validation_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when validation begins.", "docstring_tokens": ["called", "when", "validation", "begins"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 549, "end_line": 551, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_877", "original_string": "def on_test_start(self) -> None:\r\n        \"\"\"Called when test begins.\"\"\"\r\n        pass", "language": "python", "code": "def on_test_start(self) -> None:\r\n        \"\"\"Called when test begins.\"\"\"\r\n        pass", "code_tokens": ["def", "on_test_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when test begins.", "docstring_tokens": ["called", "when", "test", "begins"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 553, "end_line": 555, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_878", "original_string": "def on_predict_start(self) -> None:\r\n        \"\"\"Called when predict begins.\"\"\"\r\n        pass", "language": "python", "code": "def on_predict_start(self) -> None:\r\n        \"\"\"Called when predict begins.\"\"\"\r\n        pass", "code_tokens": ["def", "on_predict_start", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when predict begins.", "docstring_tokens": ["called", "when", "predict", "begins"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 557, "end_line": 559, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_879", "original_string": "def on_train_end(self) -> None:\r\n        \"\"\"Called when train ends.\"\"\"\r\n        pass", "language": "python", "code": "def on_train_end(self) -> None:\r\n        \"\"\"Called when train ends.\"\"\"\r\n        pass", "code_tokens": ["def", "on_train_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when train ends.", "docstring_tokens": ["called", "when", "train", "ends"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 561, "end_line": 563, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_880", "original_string": "def on_validation_end(self) -> None:\r\n        \"\"\"Called when validation ends.\"\"\"\r\n        pass", "language": "python", "code": "def on_validation_end(self) -> None:\r\n        \"\"\"Called when validation ends.\"\"\"\r\n        pass", "code_tokens": ["def", "on_validation_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when validation ends.", "docstring_tokens": ["called", "when", "validation", "ends"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 565, "end_line": 567, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_881", "original_string": "def on_test_end(self) -> None:\r\n        \"\"\"Called when test end.\"\"\"\r\n        pass", "language": "python", "code": "def on_test_end(self) -> None:\r\n        \"\"\"Called when test end.\"\"\"\r\n        pass", "code_tokens": ["def", "on_test_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when test end.", "docstring_tokens": ["called", "when", "test", "end"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 569, "end_line": 571, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_882", "original_string": "def on_predict_end(self) -> None:\r\n        \"\"\"Called when predict ends.\"\"\"\r\n        pass", "language": "python", "code": "def on_predict_end(self) -> None:\r\n        \"\"\"Called when predict ends.\"\"\"\r\n        pass", "code_tokens": ["def", "on_predict_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when predict ends.", "docstring_tokens": ["called", "when", "predict", "ends"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 573, "end_line": 575, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_883", "original_string": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\"\"\"\r\n        pass", "language": "python", "code": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\"\"\"\r\n        pass", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called in the training loop before anything happens for that batch.", "docstring_tokens": ["called", "in", "the", "training", "loop", "before", "anything", "happens", "for", "that", "batch"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 577, "end_line": 579, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_884", "original_string": "def on_exception(self, exception: BaseException) -> None:\r\n        \"\"\"Called when the trainer execution is interrupted by an exception.\"\"\"\r\n        pass", "language": "python", "code": "def on_exception(self, exception: BaseException) -> None:\r\n        \"\"\"Called when the trainer execution is interrupted by an exception.\"\"\"\r\n        pass", "code_tokens": ["def", "on_exception", "(", "self", ",", "exception", ":", "BaseException", ")", "-", ">", "None", ":", "STRING", "pass"], "docstring": "Called when the trainer execution is interrupted by an exception.", "docstring_tokens": ["called", "when", "the", "trainer", "execution", "is", "interrupted", "by", "an", "exception"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 581, "end_line": 583, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "function_885", "original_string": "def __call__(\r\n        self, wrapper_module: Module, original_module: \"pl.LightningModule\", method_name: str, *args: Any, **kwargs: Any\r\n    ) -> STEP_OUTPUT:\r\n        \"\"\"Reroutes a method call through the `wrapper_module`'s `forward` method.\r\n\r\n        Args:\r\n            wrapper_module: The module that has `original_module` wrapped.\r\n            original_module: The module that was wrapped inside `wrapper_module`.\r\n            method_name: The name of the method that should be called on the `original_module` after inputs get\r\n                redirected through the `wrapper_module`'s `forward` method.\r\n            *args: The positional arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n\r\n        \"\"\"\r\n        assert method_name != \"forward\"\r\n        original_forward = original_module.forward\r\n\r\n        def wrapped_forward(*_args: Any, **_kwargs: Any) -> Any:\r\n            original_module.forward = original_forward  # type: ignore[method-assign]\r\n            method = getattr(original_module, method_name)\r\n            out = method(*_args, **_kwargs)\r\n            self.on_after_inner_forward(wrapper_module, original_module)\r\n            return out\r\n\r\n        original_module.forward = wrapped_forward  # type: ignore[method-assign]\r\n\r\n        wrapper_output = wrapper_module(*args, **kwargs)\r\n        self.on_after_outer_forward(wrapper_module, original_module)\r\n        return wrapper_output", "language": "python", "code": "def __call__(\r\n        self, wrapper_module: Module, original_module: \"pl.LightningModule\", method_name: str, *args: Any, **kwargs: Any\r\n    ) -> STEP_OUTPUT:\r\n        \"\"\"Reroutes a method call through the `wrapper_module`'s `forward` method.\r\n\r\n        Args:\r\n            wrapper_module: The module that has `original_module` wrapped.\r\n            original_module: The module that was wrapped inside `wrapper_module`.\r\n            method_name: The name of the method that should be called on the `original_module` after inputs get\r\n                redirected through the `wrapper_module`'s `forward` method.\r\n            *args: The positional arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched\r\n                `forward` method instead.\r\n\r\n        \"\"\"\r\n        assert method_name != \"forward\"\r\n        original_forward = original_module.forward\r\n\r\n        def wrapped_forward(*_args: Any, **_kwargs: Any) -> Any:\r\n            original_module.forward = original_forward  # type: ignore[method-assign]\r\n            method = getattr(original_module, method_name)\r\n            out = method(*_args, **_kwargs)\r\n            self.on_after_inner_forward(wrapper_module, original_module)\r\n            return out\r\n\r\n        original_module.forward = wrapped_forward  # type: ignore[method-assign]\r\n\r\n        wrapper_output = wrapper_module(*args, **kwargs)\r\n        self.on_after_outer_forward(wrapper_module, original_module)\r\n        return wrapper_output", "code_tokens": ["def", "__call__", "(", "self", ",", "wrapper_module", ":", "Module", ",", "original_module", ":", "STRING", ",", "method_name", ":", "str", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "STRING", "assert", "method_name", "!", "=", "STRING", "original_forward", "=", "original_module", ".", "forward", "def", "wrapped_forward", "(", "*", "_args", ":", "Any", ",", "*", "*", "_kwargs", ":", "Any", ")", "-", ">", "Any", ":", "original_module", ".", "forward", "=", "original_forward", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "method", "=", "getattr", "(", "original_module", ",", "method_name", ")", "out", "=", "method", "(", "*", "_args", ",", "*", "*", "_kwargs", ")", "self", ".", "on_after_inner_forward", "(", "wrapper_module", ",", "original_module", ")", "return", "out", "original_module", ".", "forward", "=", "wrapped_forward", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "wrapper_output", "=", "wrapper_module", "(", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "on_after_outer_forward", "(", "wrapper_module", ",", "original_module", ")", "return", "wrapper_output"], "docstring": "Reroutes a method call through the `wrapper_module`'s `forward` method.", "docstring_tokens": ["reroutes", "a", "method", "call", "through", "the", "wrapper_module", "s", "forward", "method"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\strategy.py", "start_line": 608, "end_line": 642, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\xla.py", "func_name": "function_886", "original_string": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "language": "python", "code": "def remove_checkpoint(self, filepath: _PATH) -> None:\r\n        \"\"\"Remove checkpoint filepath from the filesystem.\r\n\r\n        Args:\r\n            filepath: Path to checkpoint\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            self.checkpoint_io.remove_checkpoint(filepath)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "local_rank", "=", "=", "0", ":", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "filepath", ")"], "docstring": "Remove checkpoint filepath from the filesystem.", "docstring_tokens": ["remove", "checkpoint", "filepath", "from", "the", "filesystem"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\xla.py", "start_line": 310, "end_line": 318, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\xla.py", "func_name": "function_887", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "STRING", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "fSTRING", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\xla.py", "start_line": 321, "end_line": 348, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\launcher.py", "func_name": "function_888", "original_string": "def kill(self, signum: _SIGNUM) -> None:\r\n        \"\"\"Kill existing alive processes.\"\"\"", "language": "python", "code": "def kill(self, signum: _SIGNUM) -> None:\r\n        \"\"\"Kill existing alive processes.\"\"\"", "code_tokens": ["def", "kill", "(", "self", ",", "signum", ":", "_SIGNUM", ")", "-", ">", "None", ":", "STRING"], "docstring": "Kill existing alive processes.", "docstring_tokens": ["kill", "existing", "alive", "processes"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\launcher.py", "start_line": 21, "end_line": 22, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "function_889", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this limitation by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [trainer, function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [trainer, function, args, kwargs, return_queue]\r\n\r\n        process_context = mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n        )\r\n        self.procs = process_context.processes\r\n        while not process_context.join():\r\n            pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this limitation by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [trainer, function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [trainer, function, args, kwargs, return_queue]\r\n\r\n        process_context = mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n        )\r\n        self.procs = process_context.processes\r\n        while not process_context.join():\r\n            pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "if", "self", ".", "_start_method", "in", "(", "STRING", ",", "STRING", ")", ":", "_check_bad_cuda_fork", "(", ")", "if", "self", ".", "_start_method", "=", "=", "STRING", ":", "_check_missing_main_guard", "(", ")", "if", "self", ".", "_already_fit", "and", "trainer", "is", "not", "None", "and", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "raise", "NotImplementedError", "(", "STRING", "STRING", "STRING", ")", "assert", "self", ".", "_strategy", ".", "cluster_environment", "is", "not", "None", "os", ".", "environ", "[", "STRING", "]", "=", "str", "(", "self", ".", "_strategy", ".", "cluster_environment", ".", "main_port", ")", "context", "=", "mp", ".", "get_context", "(", "self", ".", "_start_method", ")", "return_queue", "=", "context", ".", "SimpleQueue", "(", ")", "if", "self", ".", "_start_method", "=", "=", "STRING", ":", "global_states", "=", "_GlobalStateSnapshot", ".", "capture", "(", ")", "process_args", "=", "[", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", ",", "global_states", "]", "else", ":", "process_args", "=", "[", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", "]", "process_context", "=", "mp", ".", "start_processes", "(", "self", ".", "_wrapping_function", ",", "args", "=", "process_args", ",", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", ",", "start_method", "=", "self", ".", "_start_method", ",", "join", "=", "False", ",", "#", "we", "will", "join", "ourselves", "to", "get", "the", "process", "references", ")", "self", ".", "procs", "=", "process_context", ".", "processes", "while", "not", "process_context", ".", "join", "(", ")", ":", "pass", "worker_output", "=", "return_queue", ".", "get", "(", ")", "if", "trainer", "is", "None", ":", "return", "worker_output", "self", ".", "_already_fit", "|", "=", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "self", ".", "_recover_results_in_main_process", "(", "worker_output", ",", "trainer", ")", "return", "worker_output", ".", "trainer_results"], "docstring": "Launches processes that run the given function in parallel.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "start_line": 94, "end_line": 152, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "function_890", "original_string": "def get_extra_results(self, trainer: \"pl.Trainer\") -> dict[str, Any]:\r\n        \"\"\"Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To\r\n        avoid issues with memory sharing, we convert tensors to bytes.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n\r\n        Returns:\r\n            A dictionary with items to send back to the main process where :meth:`update_main_process_results` will\r\n            process this output.\r\n\r\n        \"\"\"\r\n        callback_metrics = apply_to_collection(trainer.callback_metrics, Tensor, lambda t: t.cpu())\r\n        buffer = io.BytesIO()\r\n        torch.save(callback_metrics, buffer)\r\n        return {\"callback_metrics_bytes\": buffer.getvalue()}", "language": "python", "code": "def get_extra_results(self, trainer: \"pl.Trainer\") -> dict[str, Any]:\r\n        \"\"\"Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To\r\n        avoid issues with memory sharing, we convert tensors to bytes.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n\r\n        Returns:\r\n            A dictionary with items to send back to the main process where :meth:`update_main_process_results` will\r\n            process this output.\r\n\r\n        \"\"\"\r\n        callback_metrics = apply_to_collection(trainer.callback_metrics, Tensor, lambda t: t.cpu())\r\n        buffer = io.BytesIO()\r\n        torch.save(callback_metrics, buffer)\r\n        return {\"callback_metrics_bytes\": buffer.getvalue()}", "code_tokens": ["def", "get_extra_results", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "callback_metrics", "=", "apply_to_collection", "(", "trainer", ".", "callback_metrics", ",", "Tensor", ",", "lambda", "t", ":", "t", ".", "cpu", "(", ")", ")", "buffer", "=", "io", ".", "BytesIO", "(", ")", "torch", ".", "save", "(", "callback_metrics", ",", "buffer", ")", "return", "{", "STRING", ":", "buffer", ".", "getvalue", "(", ")", "}"], "docstring": "Gather extra state from the Trainer and return it as a dictionary for sending back to the main process. To", "docstring_tokens": ["gather", "extra", "state", "from", "the", "trainer", "and", "return", "it", "as", "a", "dictionary", "for", "sending", "back", "to", "the", "main", "process", "to"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "start_line": 226, "end_line": 242, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "function_891", "original_string": "def update_main_process_results(self, trainer: \"pl.Trainer\", extra: dict[str, Any]) -> None:\r\n        \"\"\"Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we\r\n        convert bytes back to ``torch.Tensor``.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n            extra: A dictionary with trainer state that was sent from the worker process and needs to be restored\r\n                on the current trainer.\r\n\r\n        \"\"\"\r\n        callback_metrics_bytes = extra[\"callback_metrics_bytes\"]\r\n        callback_metrics = torch.load(io.BytesIO(callback_metrics_bytes), weights_only=True)\r\n        trainer.callback_metrics.update(callback_metrics)", "language": "python", "code": "def update_main_process_results(self, trainer: \"pl.Trainer\", extra: dict[str, Any]) -> None:\r\n        \"\"\"Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we\r\n        convert bytes back to ``torch.Tensor``.\r\n\r\n        Args:\r\n            trainer: reference to the Trainer.\r\n            extra: A dictionary with trainer state that was sent from the worker process and needs to be restored\r\n                on the current trainer.\r\n\r\n        \"\"\"\r\n        callback_metrics_bytes = extra[\"callback_metrics_bytes\"]\r\n        callback_metrics = torch.load(io.BytesIO(callback_metrics_bytes), weights_only=True)\r\n        trainer.callback_metrics.update(callback_metrics)", "code_tokens": ["def", "update_main_process_results", "(", "self", ",", "trainer", ":", "STRING", ",", "extra", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "callback_metrics_bytes", "=", "extra", "[", "STRING", "]", "callback_metrics", "=", "torch", ".", "load", "(", "io", ".", "BytesIO", "(", "callback_metrics_bytes", ")", ",", "weights_only", "=", "True", ")", "trainer", ".", "callback_metrics", ".", "update", "(", "callback_metrics", ")"], "docstring": "Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we", "docstring_tokens": ["retrieve", "the", "attr", "trainer", "callback_metrics", "dictionary", "from", "the", "given", "queue", "to", "preserve", "consistency", "we"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "start_line": 244, "end_line": 257, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "function_892", "original_string": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "language": "python", "code": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "code_tokens": ["def", "capture", "(", "cls", ")", "-", ">", "STRING", ":", "STRING", "return", "cls", "(", "use_deterministic_algorithms", "=", "torch", ".", "are_deterministic_algorithms_enabled", "(", ")", ",", "use_deterministic_algorithms_warn_only", "=", "torch", ".", "is_deterministic_algorithms_warn_only_enabled", "(", ")", ",", "cudnn_benchmark", "=", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", ",", "rng_states", "=", "_collect_rng_states", "(", ")", ",", ")"], "docstring": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "docstring_tokens": ["capture", "a", "few", "global", "states", "from", "torch", "numpy", "etc", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "start_line": 306, "end_line": 313, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "func_name": "function_893", "original_string": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "language": "python", "code": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "code_tokens": ["def", "restore", "(", "self", ")", "-", ">", "None", ":", "STRING", "torch", ".", "use_deterministic_algorithms", "(", "self", ".", "use_deterministic_algorithms", ",", "warn_only", "=", "self", ".", "use_deterministic_algorithms_warn_only", ")", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "self", ".", "cudnn_benchmark", "_set_rng_states", "(", "self", ".", "rng_states", ")"], "docstring": "Restores all globals to the values captured in the :meth:`capture` method.", "docstring_tokens": ["restores", "all", "globals", "to", "the", "values", "captured", "in", "the", "meth", "capture", "method"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\multiprocessing.py", "start_line": 315, "end_line": 321, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\subprocess_script.py", "func_name": "function_894", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "self", ".", "cluster_environment", ".", "validate_settings", "(", "num_devices", "=", "self", ".", "num_processes", ",", "num_nodes", "=", "self", ".", "num_nodes", ")", "if", "not", "self", ".", "cluster_environment", ".", "creates_processes_externally", ":", "self", ".", "_call_children_scripts", "(", ")", "_launch_process_observer", "(", "self", ".", "procs", ")", "_set_num_threads_if_needed", "(", "num_processes", "=", "self", ".", "num_processes", ")", "return", "function", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Creates new processes, then calls the given function.", "docstring_tokens": ["creates", "new", "processes", "then", "calls", "the", "given", "function"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\subprocess_script.py", "start_line": 87, "end_line": 104, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\launchers\\xla.py", "func_name": "function_895", "original_string": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        process_context = xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(trainer, function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n            **spawn_kwargs,\r\n        )\r\n        if process_context is not None:\r\n            self.procs = process_context.processes\r\n            while not process_context.join():\r\n                pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "language": "python", "code": "def launch(self, function: Callable, *args: Any, trainer: Optional[\"pl.Trainer\"] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            trainer: Optional reference to the :class:`~lightning.pytorch.trainer.trainer.Trainer` for which\r\n                a selected set of attributes get restored in the main process after processes join.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._already_fit and trainer is not None and trainer.state.fn == TrainerFn.FITTING:\r\n            raise NotImplementedError(\r\n                \"Calling `trainer.fit()` twice on the same Trainer instance using a spawn-based strategy is not\"\r\n                \" supported. You can work around this by creating a new Trainer instance and passing the\"\r\n                \" `fit(ckpt_path=...)` argument.\"\r\n            )\r\n\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        process_context = xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(trainer, function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            join=False,  # we will join ourselves to get the process references\r\n            **spawn_kwargs,\r\n        )\r\n        if process_context is not None:\r\n            self.procs = process_context.processes\r\n            while not process_context.join():\r\n                pass\r\n\r\n        worker_output = return_queue.get()\r\n        if trainer is None:\r\n            return worker_output\r\n\r\n        self._already_fit |= trainer.state.fn == TrainerFn.FITTING\r\n        self._recover_results_in_main_process(worker_output, trainer)\r\n        return worker_output.trainer_results", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "trainer", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "STRING", "if", "self", ".", "_already_fit", "and", "trainer", "is", "not", "None", "and", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "raise", "NotImplementedError", "(", "STRING", "STRING", "STRING", ")", "return_queue", "=", "mp", ".", "Manager", "(", ")", ".", "Queue", "(", ")", "import", "torch_xla", ".", "distributed", ".", "xla_multiprocessing", "as", "xmp", "spawn_kwargs", "=", "{", "}", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", "if", "nprocs", "=", "=", "1", ":", "spawn_kwargs", "[", "STRING", "]", "=", "nprocs", "process_context", "=", "xmp", ".", "spawn", "(", "self", ".", "_wrapping_function", ",", "args", "=", "(", "trainer", ",", "function", ",", "args", ",", "kwargs", ",", "return_queue", ")", ",", "start_method", "=", "self", ".", "_start_method", ",", "join", "=", "False", ",", "#", "we", "will", "join", "ourselves", "to", "get", "the", "process", "references", "*", "*", "spawn_kwargs", ",", ")", "if", "process_context", "is", "not", "None", ":", "self", ".", "procs", "=", "process_context", ".", "processes", "while", "not", "process_context", ".", "join", "(", ")", ":", "pass", "worker_output", "=", "return_queue", ".", "get", "(", ")", "if", "trainer", "is", "None", ":", "return", "worker_output", "self", ".", "_already_fit", "|", "=", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "self", ".", "_recover_results_in_main_process", "(", "worker_output", ",", "trainer", ")", "return", "worker_output", ".", "trainer_results"], "docstring": "Launches processes that run the given function in parallel.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\strategies\\launchers\\xla.py", "start_line": 63, "end_line": 116, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "function_896", "original_string": "def _call_and_handle_interrupt(trainer: \"pl.Trainer\", trainer_fn: Callable, *args: Any, **kwargs: Any) -> Any:\r\n    r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\r\n    as all errors should funnel through them.\r\n\r\n    Args:\r\n        trainer_fn: one of (fit, validate, test, predict)\r\n        *args: positional arguments to be passed to the `trainer_fn`\r\n        **kwargs: keyword arguments to be passed to `trainer_fn`\r\n\r\n    \"\"\"\r\n    try:\r\n        if trainer.strategy.launcher is not None:\r\n            return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n        return trainer_fn(*args, **kwargs)\r\n\r\n    except _TunerExitException:\r\n        _call_teardown_hook(trainer)\r\n        trainer._teardown()\r\n        trainer.state.status = TrainerStatus.FINISHED\r\n        trainer.state.stage = None\r\n\r\n    except KeyboardInterrupt as exception:\r\n        rank_zero_info(\"\\nDetected KeyboardInterrupt, attempting graceful shutdown ...\")\r\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        launcher = trainer.strategy.launcher\r\n        if isinstance(launcher, _SubprocessScriptLauncher):\r\n            launcher.kill(_get_sigkill_signal())\r\n        sys.exit(1)\r\n\r\n    except BaseException as exception:\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        trainer.state.stage = None\r\n        raise", "language": "python", "code": "def _call_and_handle_interrupt(trainer: \"pl.Trainer\", trainer_fn: Callable, *args: Any, **kwargs: Any) -> Any:\r\n    r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\r\n    as all errors should funnel through them.\r\n\r\n    Args:\r\n        trainer_fn: one of (fit, validate, test, predict)\r\n        *args: positional arguments to be passed to the `trainer_fn`\r\n        **kwargs: keyword arguments to be passed to `trainer_fn`\r\n\r\n    \"\"\"\r\n    try:\r\n        if trainer.strategy.launcher is not None:\r\n            return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n        return trainer_fn(*args, **kwargs)\r\n\r\n    except _TunerExitException:\r\n        _call_teardown_hook(trainer)\r\n        trainer._teardown()\r\n        trainer.state.status = TrainerStatus.FINISHED\r\n        trainer.state.stage = None\r\n\r\n    except KeyboardInterrupt as exception:\r\n        rank_zero_info(\"\\nDetected KeyboardInterrupt, attempting graceful shutdown ...\")\r\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        launcher = trainer.strategy.launcher\r\n        if isinstance(launcher, _SubprocessScriptLauncher):\r\n            launcher.kill(_get_sigkill_signal())\r\n        sys.exit(1)\r\n\r\n    except BaseException as exception:\r\n        _interrupt(trainer, exception)\r\n        trainer._teardown()\r\n        trainer.state.stage = None\r\n        raise", "code_tokens": ["def", "_call_and_handle_interrupt", "(", "trainer", ":", "STRING", ",", "trainer_fn", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "rSTRING", "try", ":", "if", "trainer", ".", "strategy", ".", "launcher", "is", "not", "None", ":", "return", "trainer", ".", "strategy", ".", "launcher", ".", "launch", "(", "trainer_fn", ",", "*", "args", ",", "trainer", "=", "trainer", ",", "*", "*", "kwargs", ")", "return", "trainer_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "except", "_TunerExitException", ":", "_call_teardown_hook", "(", "trainer", ")", "trainer", ".", "_teardown", "(", ")", "trainer", ".", "state", ".", "status", "=", "TrainerStatus", ".", "FINISHED", "trainer", ".", "state", ".", "stage", "=", "None", "except", "KeyboardInterrupt", "as", "exception", ":", "rank_zero_info", "(", "STRING", ")", "signal", ".", "signal", "(", "signal", ".", "SIGINT", ",", "signal", ".", "SIG_IGN", ")", "_interrupt", "(", "trainer", ",", "exception", ")", "trainer", ".", "_teardown", "(", ")", "launcher", "=", "trainer", ".", "strategy", ".", "launcher", "if", "isinstance", "(", "launcher", ",", "_SubprocessScriptLauncher", ")", ":", "launcher", ".", "kill", "(", "_get_sigkill_signal", "(", ")", ")", "sys", ".", "exit", "(", "1", ")", "except", "BaseException", "as", "exception", ":", "_interrupt", "(", "trainer", ",", "exception", ")", "trainer", ".", "_teardown", "(", ")", "trainer", ".", "state", ".", "stage", "=", "None", "raise"], "docstring": "r\"\"\"Error handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)", "docstring_tokens": ["r", "error", "handling", "intended", "to", "be", "used", "only", "for", "main", "trainer", "function", "entry", "points", "fit", "validate", "test", "predict"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\call.py", "start_line": 35, "end_line": 72, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "function_897", "original_string": "def _call_callbacks_state_dict(trainer: \"pl.Trainer\") -> dict[str, dict]:\r\n    \"\"\"Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by\r\n    `Callback.state_key`.\"\"\"\r\n    callback_state_dicts = {}\r\n    for callback in trainer.callbacks:\r\n        state_dict = callback.state_dict()\r\n        if state_dict:\r\n            callback_state_dicts[callback.state_key] = state_dict\r\n    return callback_state_dicts", "language": "python", "code": "def _call_callbacks_state_dict(trainer: \"pl.Trainer\") -> dict[str, dict]:\r\n    \"\"\"Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by\r\n    `Callback.state_key`.\"\"\"\r\n    callback_state_dicts = {}\r\n    for callback in trainer.callbacks:\r\n        state_dict = callback.state_dict()\r\n        if state_dict:\r\n            callback_state_dicts[callback.state_key] = state_dict\r\n    return callback_state_dicts", "code_tokens": ["def", "_call_callbacks_state_dict", "(", "trainer", ":", "STRING", ")", "-", ">", "dict", "[", "str", ",", "dict", "]", ":", "STRING", "callback_state_dicts", "=", "{", "}", "for", "callback", "in", "trainer", ".", "callbacks", ":", "state_dict", "=", "callback", ".", "state_dict", "(", ")", "if", "state_dict", ":", "callback_state_dicts", "[", "callback", ".", "state_key", "]", "=", "state_dict", "return", "callback_state_dicts"], "docstring": "Called when saving a model checkpoint, calls and returns every callback's `state_dict`, keyed by", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "calls", "and", "returns", "every", "callback", "s", "state_dict", "keyed", "by"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\call.py", "start_line": 234, "end_line": 242, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "function_898", "original_string": "def _call_callbacks_on_save_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.\"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_save_checkpoint\"\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_save_checkpoint\"):\r\n            callback.on_save_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        pl_module._current_fx_name = prev_fx_name", "language": "python", "code": "def _call_callbacks_on_save_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.\"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_save_checkpoint\"\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_save_checkpoint\"):\r\n            callback.on_save_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        pl_module._current_fx_name = prev_fx_name", "code_tokens": ["def", "_call_callbacks_on_save_checkpoint", "(", "trainer", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "pl_module", "=", "trainer", ".", "lightning_module", "if", "pl_module", ":", "prev_fx_name", "=", "pl_module", ".", "_current_fx_name", "pl_module", ".", "_current_fx_name", "=", "STRING", "for", "callback", "in", "trainer", ".", "callbacks", ":", "with", "trainer", ".", "profiler", ".", "profile", "(", "fSTRING", ")", ":", "callback", ".", "on_save_checkpoint", "(", "trainer", ",", "trainer", ".", "lightning_module", ",", "checkpoint", ")", "if", "pl_module", ":", "pl_module", ".", "_current_fx_name", "=", "prev_fx_name"], "docstring": "Called when saving a model checkpoint, calls every callback's `on_save_checkpoint` hook.", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "calls", "every", "callback", "s", "on_save_checkpoint", "hook"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\call.py", "start_line": 245, "end_line": 258, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "function_899", "original_string": "def _call_callbacks_on_load_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint.\r\n\r\n    Calls every callback's `on_load_checkpoint` hook. We have a dedicated function for this rather than using\r\n    `_call_callback_hooks` because we have special logic for getting callback_states.\r\n\r\n    \"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_load_checkpoint\"\r\n\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    is_legacy_ckpt = Version(checkpoint[\"pytorch-lightning_version\"]) < Version(\"1.5.0dev\")\r\n    current_callbacks_keys = {cb._legacy_state_key if is_legacy_ckpt else cb.state_key for cb in trainer.callbacks}\r\n    difference = callback_states.keys() - current_callbacks_keys\r\n    if difference:\r\n        rank_zero_warn(\r\n            \"Be aware that when using `ckpt_path`,\"\r\n            \" callbacks used to create the checkpoint need to be provided during `Trainer` instantiation.\"\r\n            f\" Please add the following callbacks: {list(difference)}.\",\r\n        )\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_load_checkpoint\"):\r\n            callback.on_load_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        pl_module._current_fx_name = prev_fx_name", "language": "python", "code": "def _call_callbacks_on_load_checkpoint(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint.\r\n\r\n    Calls every callback's `on_load_checkpoint` hook. We have a dedicated function for this rather than using\r\n    `_call_callback_hooks` because we have special logic for getting callback_states.\r\n\r\n    \"\"\"\r\n    pl_module = trainer.lightning_module\r\n    if pl_module:\r\n        prev_fx_name = pl_module._current_fx_name\r\n        pl_module._current_fx_name = \"on_load_checkpoint\"\r\n\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    is_legacy_ckpt = Version(checkpoint[\"pytorch-lightning_version\"]) < Version(\"1.5.0dev\")\r\n    current_callbacks_keys = {cb._legacy_state_key if is_legacy_ckpt else cb.state_key for cb in trainer.callbacks}\r\n    difference = callback_states.keys() - current_callbacks_keys\r\n    if difference:\r\n        rank_zero_warn(\r\n            \"Be aware that when using `ckpt_path`,\"\r\n            \" callbacks used to create the checkpoint need to be provided during `Trainer` instantiation.\"\r\n            f\" Please add the following callbacks: {list(difference)}.\",\r\n        )\r\n\r\n    for callback in trainer.callbacks:\r\n        with trainer.profiler.profile(f\"[Callback]{callback.state_key}.on_load_checkpoint\"):\r\n            callback.on_load_checkpoint(trainer, trainer.lightning_module, checkpoint)\r\n\r\n    if pl_module:\r\n        pl_module._current_fx_name = prev_fx_name", "code_tokens": ["def", "_call_callbacks_on_load_checkpoint", "(", "trainer", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "pl_module", "=", "trainer", ".", "lightning_module", "if", "pl_module", ":", "prev_fx_name", "=", "pl_module", ".", "_current_fx_name", "pl_module", ".", "_current_fx_name", "=", "STRING", "callback_states", ":", "Optional", "[", "dict", "[", "Union", "[", "type", ",", "str", "]", ",", "dict", "]", "]", "=", "checkpoint", ".", "get", "(", "STRING", ")", "if", "callback_states", "is", "None", ":", "return", "is_legacy_ckpt", "=", "Version", "(", "checkpoint", "[", "STRING", "]", ")", "<", "Version", "(", "STRING", ")", "current_callbacks_keys", "=", "{", "cb", ".", "_legacy_state_key", "if", "is_legacy_ckpt", "else", "cb", ".", "state_key", "for", "cb", "in", "trainer", ".", "callbacks", "}", "difference", "=", "callback_states", ".", "keys", "(", ")", "-", "current_callbacks_keys", "if", "difference", ":", "rank_zero_warn", "(", "STRING", "STRING", "fSTRING", ",", ")", "for", "callback", "in", "trainer", ".", "callbacks", ":", "with", "trainer", ".", "profiler", ".", "profile", "(", "fSTRING", ")", ":", "callback", ".", "on_load_checkpoint", "(", "trainer", ",", "trainer", ".", "lightning_module", ",", "checkpoint", ")", "if", "pl_module", ":", "pl_module", ".", "_current_fx_name", "=", "prev_fx_name"], "docstring": "Called when loading a model checkpoint.", "docstring_tokens": ["called", "when", "loading", "a", "model", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\call.py", "start_line": 261, "end_line": 294, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\call.py", "func_name": "function_900", "original_string": "def _call_callbacks_load_state_dict(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint, calls every callback's `load_state_dict`.\"\"\"\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    for callback in trainer.callbacks:\r\n        state = callback_states.get(callback.state_key, callback_states.get(callback._legacy_state_key))\r\n        if state:\r\n            state = deepcopy(state)\r\n            callback.load_state_dict(state)", "language": "python", "code": "def _call_callbacks_load_state_dict(trainer: \"pl.Trainer\", checkpoint: dict[str, Any]) -> None:\r\n    \"\"\"Called when loading a model checkpoint, calls every callback's `load_state_dict`.\"\"\"\r\n    callback_states: Optional[dict[Union[type, str], dict]] = checkpoint.get(\"callbacks\")\r\n\r\n    if callback_states is None:\r\n        return\r\n\r\n    for callback in trainer.callbacks:\r\n        state = callback_states.get(callback.state_key, callback_states.get(callback._legacy_state_key))\r\n        if state:\r\n            state = deepcopy(state)\r\n            callback.load_state_dict(state)", "code_tokens": ["def", "_call_callbacks_load_state_dict", "(", "trainer", ":", "STRING", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "STRING", "callback_states", ":", "Optional", "[", "dict", "[", "Union", "[", "type", ",", "str", "]", ",", "dict", "]", "]", "=", "checkpoint", ".", "get", "(", "STRING", ")", "if", "callback_states", "is", "None", ":", "return", "for", "callback", "in", "trainer", ".", "callbacks", ":", "state", "=", "callback_states", ".", "get", "(", "callback", ".", "state_key", ",", "callback_states", ".", "get", "(", "callback", ".", "_legacy_state_key", ")", ")", "if", "state", ":", "state", "=", "deepcopy", "(", "state", ")", "callback", ".", "load_state_dict", "(", "state", ")"], "docstring": "Called when loading a model checkpoint, calls every callback's `load_state_dict`.", "docstring_tokens": ["called", "when", "loading", "a", "model", "checkpoint", "calls", "every", "callback", "s", "load_state_dict"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\call.py", "start_line": 297, "end_line": 308, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "func_name": "function_901", "original_string": "def _verify_loop_configurations(trainer: \"pl.Trainer\") -> None:\r\n    r\"\"\"Checks that the model is configured correctly before the run is started.\r\n\r\n    Args:\r\n        trainer: Lightning Trainer. Its `lightning_module` (the model) to check the configuration.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n\r\n    if trainer.state.fn is None:\r\n        raise ValueError(\"Unexpected: Trainer state fn must be set before validating loop configuration.\")\r\n    if trainer.state.fn == TrainerFn.FITTING:\r\n        __verify_train_val_loop_configuration(trainer, model)\r\n        __verify_manual_optimization_support(trainer, model)\r\n    elif trainer.state.fn == TrainerFn.VALIDATING:\r\n        __verify_eval_loop_configuration(model, \"val\")\r\n    elif trainer.state.fn == TrainerFn.TESTING:\r\n        __verify_eval_loop_configuration(model, \"test\")\r\n    elif trainer.state.fn == TrainerFn.PREDICTING:\r\n        __verify_eval_loop_configuration(model, \"predict\")\r\n\r\n    __verify_configure_model_configuration(model)\r\n    __warn_dataloader_iter_limitations(model)", "language": "python", "code": "def _verify_loop_configurations(trainer: \"pl.Trainer\") -> None:\r\n    r\"\"\"Checks that the model is configured correctly before the run is started.\r\n\r\n    Args:\r\n        trainer: Lightning Trainer. Its `lightning_module` (the model) to check the configuration.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n\r\n    if trainer.state.fn is None:\r\n        raise ValueError(\"Unexpected: Trainer state fn must be set before validating loop configuration.\")\r\n    if trainer.state.fn == TrainerFn.FITTING:\r\n        __verify_train_val_loop_configuration(trainer, model)\r\n        __verify_manual_optimization_support(trainer, model)\r\n    elif trainer.state.fn == TrainerFn.VALIDATING:\r\n        __verify_eval_loop_configuration(model, \"val\")\r\n    elif trainer.state.fn == TrainerFn.TESTING:\r\n        __verify_eval_loop_configuration(model, \"test\")\r\n    elif trainer.state.fn == TrainerFn.PREDICTING:\r\n        __verify_eval_loop_configuration(model, \"predict\")\r\n\r\n    __verify_configure_model_configuration(model)\r\n    __warn_dataloader_iter_limitations(model)", "code_tokens": ["def", "_verify_loop_configurations", "(", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "rSTRING", "model", "=", "trainer", ".", "lightning_module", "if", "trainer", ".", "state", ".", "fn", "is", "None", ":", "raise", "ValueError", "(", "STRING", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "__verify_train_val_loop_configuration", "(", "trainer", ",", "model", ")", "__verify_manual_optimization_support", "(", "trainer", ",", "model", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "STRING", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "TESTING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "STRING", ")", "elif", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "PREDICTING", ":", "__verify_eval_loop_configuration", "(", "model", ",", "STRING", ")", "__verify_configure_model_configuration", "(", "model", ")", "__warn_dataloader_iter_limitations", "(", "model", ")"], "docstring": "r\"\"\"Checks that the model is configured correctly before the run is started.", "docstring_tokens": ["r", "checks", "that", "the", "model", "is", "configured", "correctly", "before", "the", "run", "is", "started"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "start_line": 23, "end_line": 45, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "func_name": "function_902", "original_string": "def __warn_dataloader_iter_limitations(model: \"pl.LightningModule\") -> None:\r\n    \"\"\"Check if `dataloader_iter is enabled`.\"\"\"\r\n    if any(\r\n        is_param_in_hook_signature(step_fn, \"dataloader_iter\", explicit=True)\r\n        for step_fn in (model.training_step, model.validation_step, model.predict_step, model.test_step)\r\n        if step_fn is not None\r\n    ):\r\n        rank_zero_warn(\r\n            \"You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the\"\r\n            \" `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch\"\r\n            \" consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index.\"\r\n            \" This will also not work well with gradient accumulation. This feature is very experimental and subject to\"\r\n            \" change. Here be dragons.\",\r\n            category=PossibleUserWarning,\r\n        )", "language": "python", "code": "def __warn_dataloader_iter_limitations(model: \"pl.LightningModule\") -> None:\r\n    \"\"\"Check if `dataloader_iter is enabled`.\"\"\"\r\n    if any(\r\n        is_param_in_hook_signature(step_fn, \"dataloader_iter\", explicit=True)\r\n        for step_fn in (model.training_step, model.validation_step, model.predict_step, model.test_step)\r\n        if step_fn is not None\r\n    ):\r\n        rank_zero_warn(\r\n            \"You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the\"\r\n            \" `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch\"\r\n            \" consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index.\"\r\n            \" This will also not work well with gradient accumulation. This feature is very experimental and subject to\"\r\n            \" change. Here be dragons.\",\r\n            category=PossibleUserWarning,\r\n        )", "code_tokens": ["def", "__warn_dataloader_iter_limitations", "(", "model", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "any", "(", "is_param_in_hook_signature", "(", "step_fn", ",", "STRING", ",", "explicit", "=", "True", ")", "for", "step_fn", "in", "(", "model", ".", "training_step", ",", "model", ".", "validation_step", ",", "model", ".", "predict_step", ",", "model", ".", "test_step", ")", "if", "step_fn", "is", "not", "None", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Check if `dataloader_iter is enabled`.", "docstring_tokens": ["check", "if", "dataloader_iter", "is", "enabled"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\configuration_validator.py", "start_line": 135, "end_line": 149, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\setup.py", "func_name": "function_903", "original_string": "def _parse_time_interval_seconds(value: Union[str, timedelta, dict]) -> float:\r\n    \"\"\"Convert a time interval into seconds.\r\n\r\n    This helper parses different representations of a time interval and\r\n    normalizes them into a float number of seconds.\r\n\r\n    Supported input formats:\r\n      * `timedelta`: The total seconds are returned directly.\r\n      * `dict`: A dictionary of keyword arguments accepted by\r\n        `datetime.timedelta`, e.g. `{\"days\": 1, \"hours\": 2}`.\r\n      * `str`: A string in the format `\"DD:HH:MM:SS\"`, where each\r\n        component must be an integer.\r\n\r\n    Args:\r\n        value (Union[str, timedelta, dict]): The time interval to parse.\r\n\r\n    Returns:\r\n        float: The duration represented by `value` in seconds.\r\n\r\n    Raises:\r\n        MisconfigurationException: If the input type is unsupported, the\r\n        string format is invalid, or any string component is not an integer.\r\n\r\n    Examples:\r\n        >>> _parse_time_interval_seconds(\"01:02:03:04\")\r\n        93784.0\r\n\r\n        >>> _parse_time_interval_seconds({\"hours\": 2, \"minutes\": 30})\r\n        9000.0\r\n\r\n        >>> from datetime import timedelta\r\n        >>> _parse_time_interval_seconds(timedelta(days=1, seconds=30))\r\n        86430.0\r\n\r\n    \"\"\"\r\n    if isinstance(value, timedelta):\r\n        return value.total_seconds()\r\n    if isinstance(value, dict):\r\n        td = timedelta(**value)\r\n        return td.total_seconds()\r\n    if isinstance(value, str):\r\n        parts = value.split(\":\")\r\n        if len(parts) != 4:\r\n            raise MisconfigurationException(\r\n                f\"Invalid time format for `val_check_interval`: {value!r}. Expected 'DD:HH:MM:SS'.\"\r\n            )\r\n        d, h, m, s = parts\r\n        try:\r\n            days = int(d)\r\n            hours = int(h)\r\n            minutes = int(m)\r\n            seconds = int(s)\r\n        except ValueError:\r\n            raise MisconfigurationException(\r\n                f\"Non-integer component in `val_check_interval` string: {value!r}. Use 'DD:HH:MM:SS'.\"\r\n            )\r\n        td = timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds)\r\n        return td.total_seconds()\r\n    raise MisconfigurationException(f\"Unsupported type for `val_check_interval`: {type(value)!r}\")", "language": "python", "code": "def _parse_time_interval_seconds(value: Union[str, timedelta, dict]) -> float:\r\n    \"\"\"Convert a time interval into seconds.\r\n\r\n    This helper parses different representations of a time interval and\r\n    normalizes them into a float number of seconds.\r\n\r\n    Supported input formats:\r\n      * `timedelta`: The total seconds are returned directly.\r\n      * `dict`: A dictionary of keyword arguments accepted by\r\n        `datetime.timedelta`, e.g. `{\"days\": 1, \"hours\": 2}`.\r\n      * `str`: A string in the format `\"DD:HH:MM:SS\"`, where each\r\n        component must be an integer.\r\n\r\n    Args:\r\n        value (Union[str, timedelta, dict]): The time interval to parse.\r\n\r\n    Returns:\r\n        float: The duration represented by `value` in seconds.\r\n\r\n    Raises:\r\n        MisconfigurationException: If the input type is unsupported, the\r\n        string format is invalid, or any string component is not an integer.\r\n\r\n    Examples:\r\n        >>> _parse_time_interval_seconds(\"01:02:03:04\")\r\n        93784.0\r\n\r\n        >>> _parse_time_interval_seconds({\"hours\": 2, \"minutes\": 30})\r\n        9000.0\r\n\r\n        >>> from datetime import timedelta\r\n        >>> _parse_time_interval_seconds(timedelta(days=1, seconds=30))\r\n        86430.0\r\n\r\n    \"\"\"\r\n    if isinstance(value, timedelta):\r\n        return value.total_seconds()\r\n    if isinstance(value, dict):\r\n        td = timedelta(**value)\r\n        return td.total_seconds()\r\n    if isinstance(value, str):\r\n        parts = value.split(\":\")\r\n        if len(parts) != 4:\r\n            raise MisconfigurationException(\r\n                f\"Invalid time format for `val_check_interval`: {value!r}. Expected 'DD:HH:MM:SS'.\"\r\n            )\r\n        d, h, m, s = parts\r\n        try:\r\n            days = int(d)\r\n            hours = int(h)\r\n            minutes = int(m)\r\n            seconds = int(s)\r\n        except ValueError:\r\n            raise MisconfigurationException(\r\n                f\"Non-integer component in `val_check_interval` string: {value!r}. Use 'DD:HH:MM:SS'.\"\r\n            )\r\n        td = timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds)\r\n        return td.total_seconds()\r\n    raise MisconfigurationException(f\"Unsupported type for `val_check_interval`: {type(value)!r}\")", "code_tokens": ["def", "_parse_time_interval_seconds", "(", "value", ":", "Union", "[", "str", ",", "timedelta", ",", "dict", "]", ")", "-", ">", "float", ":", "STRING", "if", "isinstance", "(", "value", ",", "timedelta", ")", ":", "return", "value", ".", "total_seconds", "(", ")", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "td", "=", "timedelta", "(", "*", "*", "value", ")", "return", "td", ".", "total_seconds", "(", ")", "if", "isinstance", "(", "value", ",", "str", ")", ":", "parts", "=", "value", ".", "split", "(", "STRING", ")", "if", "len", "(", "parts", ")", "!", "=", "4", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "d", ",", "h", ",", "m", ",", "s", "=", "parts", "try", ":", "days", "=", "int", "(", "d", ")", "hours", "=", "int", "(", "h", ")", "minutes", "=", "int", "(", "m", ")", "seconds", "=", "int", "(", "s", ")", "except", "ValueError", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "td", "=", "timedelta", "(", "days", "=", "days", ",", "hours", "=", "hours", ",", "minutes", "=", "minutes", ",", "seconds", "=", "seconds", ")", "return", "td", ".", "total_seconds", "(", ")", "raise", "MisconfigurationException", "(", "fSTRING", ")"], "docstring": "Convert a time interval into seconds.", "docstring_tokens": ["convert", "a", "time", "interval", "into", "seconds"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\setup.py", "start_line": 200, "end_line": 259, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_904", "original_string": "def __init__(\r\n        self,\r\n        *,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        logger: Optional[Union[Logger, Iterable[Logger], bool]] = None,\r\n        callbacks: Optional[Union[list[Callback], Callback]] = None,\r\n        fast_dev_run: Union[int, bool] = False,\r\n        max_epochs: Optional[int] = None,\r\n        min_epochs: Optional[int] = None,\r\n        max_steps: int = -1,\r\n        min_steps: Optional[int] = None,\r\n        max_time: Optional[Union[str, timedelta, dict[str, int]]] = None,\r\n        limit_train_batches: Optional[Union[int, float]] = None,\r\n        limit_val_batches: Optional[Union[int, float]] = None,\r\n        limit_test_batches: Optional[Union[int, float]] = None,\r\n        limit_predict_batches: Optional[Union[int, float]] = None,\r\n        overfit_batches: Union[int, float] = 0.0,\r\n        val_check_interval: Optional[Union[int, float, str, timedelta, dict[str, int]]] = None,\r\n        check_val_every_n_epoch: Optional[int] = 1,\r\n        num_sanity_val_steps: Optional[int] = None,\r\n        log_every_n_steps: Optional[int] = None,\r\n        enable_checkpointing: Optional[bool] = None,\r\n        enable_progress_bar: Optional[bool] = None,\r\n        enable_model_summary: Optional[bool] = None,\r\n        accumulate_grad_batches: int = 1,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n        benchmark: Optional[bool] = None,\r\n        inference_mode: bool = True,\r\n        use_distributed_sampler: bool = True,\r\n        profiler: Optional[Union[Profiler, str]] = None,\r\n        detect_anomaly: bool = False,\r\n        barebones: bool = False,\r\n        plugins: Optional[Union[_PLUGIN_INPUT, list[_PLUGIN_INPUT]]] = None,\r\n        sync_batchnorm: bool = False,\r\n        reload_dataloaders_every_n_epochs: int = 0,\r\n        default_root_dir: Optional[_PATH] = None,\r\n        enable_autolog_hparams: bool = True,\r\n        model_registry: Optional[str] = None,\r\n    ) -> None:\r\n        r\"\"\"Customize every aspect of training via flags.\r\n\r\n        Args:\r\n            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\r\n                as well as custom accelerator instances.\r\n\r\n            strategy: Supports different training strategies with aliases as well custom strategies.\r\n                Default: ``\"auto\"``.\r\n\r\n            devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\r\n                (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\r\n                automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\r\n\r\n            num_nodes: Number of GPU nodes for distributed training.\r\n                Default: ``1``.\r\n\r\n            precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\r\n                16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\r\n                Can be used on CPU, GPU, TPUs, or HPUs.\r\n                Default: ``'32-true'``.\r\n\r\n            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\r\n                the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\r\n                ``False`` will disable logging. If multiple loggers are provided, local files\r\n                (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\r\n                Default: ``True``.\r\n\r\n            callbacks: Add a callback or list of callbacks.\r\n                Default: ``None``.\r\n\r\n            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\r\n                of train, val and test to find any bugs (ie: a sort of unit test).\r\n                Default: ``False``.\r\n\r\n            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\r\n                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\r\n                To enable infinite training, set ``max_epochs = -1``.\r\n\r\n            min_epochs: Force training for at least these many epochs. Disabled by default (None).\r\n\r\n            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\r\n                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\r\n                ``max_epochs`` to ``-1``.\r\n\r\n            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\r\n\r\n            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\r\n                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\r\n                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\r\n                :class:`datetime.timedelta`.\r\n\r\n            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\r\n                Default: ``0.0``.\r\n\r\n            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\r\n                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\r\n                batches. An ``int`` value can only be higher than the number of training batches when\r\n                ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\r\n                across epochs or during iteration-based training. Additionally, accepts a time-based duration\r\n                as a string \"DD:HH:MM:SS\", a :class:`datetime.timedelta`, or a dict of kwargs to\r\n                :class:`datetime.timedelta`. When time-based, validation triggers once the elapsed wall-clock time\r\n                since the last validation exceeds the interval; the check occurs after the current batch\r\n                completes, the validation loop runs, and the timer is reset.\r\n                Default: ``1.0``.\r\n\r\n            check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\r\n                validation will be done solely based on the number of training batches, requiring ``val_check_interval``\r\n                to be an integer value. When used together with a time-based ``val_check_interval`` and\r\n                ``check_val_every_n_epoch`` > 1, validation is aligned to epoch multiples: if the interval elapses\r\n                before the next multiple-N epoch, validation runs at the start of that epoch (after the first batch)\r\n                and the timer resets; if it elapses during a multiple-N epoch, validation runs after the current batch.\r\n                For ``None`` or ``1`` cases, the time-based behavior of ``val_check_interval`` applies without\r\n                additional alignment.\r\n                Default: ``1``.\r\n\r\n            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\r\n                Set it to `-1` to run all batches in all validation dataloaders.\r\n                Default: ``2``.\r\n\r\n            log_every_n_steps: How often to log within steps.\r\n                Default: ``50``.\r\n\r\n            enable_checkpointing: If ``True``, enable checkpointing.\r\n                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks`.\r\n                Default: ``True``.\r\n\r\n            enable_progress_bar: Whether to enable to progress bar by default.\r\n                Default: ``True``.\r\n\r\n            enable_model_summary: Whether to enable model summarization by default.\r\n                Default: ``True``.\r\n\r\n            accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\r\n                Default: 1.\r\n\r\n            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\r\n                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\r\n                Default: ``None``.\r\n\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\r\n                be set to ``\"norm\"``.\r\n\r\n            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\r\n                Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\r\n                that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\r\n\r\n            benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\r\n                The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\r\n                (``False`` if not manually set). If :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic`\r\n                is set to ``True``, this will default to ``False``. Override to manually set a different value.\r\n                Default: ``None``.\r\n\r\n            inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\r\n                evaluation (``validate``/``test``/``predict``).\r\n\r\n            use_distributed_sampler: Whether to wrap the DataLoader's sampler with\r\n                :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\r\n                strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\r\n                ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\r\n                ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\r\n                sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\r\n                we don't do this automatically.\r\n\r\n            profiler: To profile individual steps during training and assist in identifying bottlenecks.\r\n                Default: ``None``.\r\n\r\n            detect_anomaly: Enable anomaly detection for the autograd engine.\r\n                Default: ``False``.\r\n\r\n            barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\r\n                disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\r\n                runs. The following features are deactivated:\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_checkpointing`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.logger`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_progress_bar`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.log_every_n_steps`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_model_summary`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.detect_anomaly`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.profiler`,\r\n                :meth:`~lightning.pytorch.core.LightningModule.log`,\r\n                :meth:`~lightning.pytorch.core.LightningModule.log_dict`.\r\n            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\r\n                Default: ``None``.\r\n\r\n            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\r\n                Default: ``False``.\r\n\r\n            reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\r\n                Default: ``0``.\r\n\r\n            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\r\n                Default: ``os.getcwd()``.\r\n                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\r\n\r\n            enable_autolog_hparams: Whether to log hyperparameters at the start of a run.\r\n                Default: ``True``.\r\n\r\n            model_registry: The name of the model being uploaded to Model hub.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``gradient_clip_val`` is not an int or float.\r\n\r\n            MisconfigurationException:\r\n                If ``gradient_clip_algorithm`` is invalid.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        log.debug(f\"{self.__class__.__name__}: Initializing trainer with parameters: {locals()}\")\r\n\r\n        if default_root_dir is not None:\r\n            default_root_dir = os.fspath(default_root_dir)\r\n\r\n        self._model_registry = model_registry.split(\":\")[0] if model_registry else None\r\n\r\n        self.barebones = barebones\r\n        if barebones:\r\n            if enable_checkpointing:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_checkpointing={enable_checkpointing!r})` was passed.\"\r\n                    \" Checkpointing can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_checkpointing = False\r\n            if logger is not None and logger is not False:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, logger={logger!r})` was passed.\"\r\n                    \" Logging can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            logger = False\r\n            if enable_progress_bar:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_progress_bar={enable_progress_bar!r})` was passed.\"\r\n                    \" The progress bar can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_progress_bar = False\r\n            if log_every_n_steps is not None and log_every_n_steps != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, log_every_n_steps={log_every_n_steps!r})` was passed.\"\r\n                    \" Logging can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            log_every_n_steps = 0\r\n            if enable_model_summary:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_model_summary={enable_model_summary!r})` was passed.\"\r\n                    \" Model summary can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_model_summary = False\r\n            if num_sanity_val_steps is not None and num_sanity_val_steps != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, num_sanity_val_steps={num_sanity_val_steps!r})` was passed.\"\r\n                    \" Sanity checking can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            num_sanity_val_steps = 0\r\n            if fast_dev_run is not False and fast_dev_run != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, fast_dev_run={fast_dev_run!r})` was passed.\"\r\n                    \" Development run is not meant for raw speed evaluation so it is disabled in barebones mode.\"\r\n                )\r\n            if detect_anomaly:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, detect_anomaly={detect_anomaly!r})` was passed.\"\r\n                    \" Anomaly detection can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            if profiler is not None:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, profiler={profiler!r})` was passed.\"\r\n                    \" Profiling can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            deactivated = (\r\n                \" - Checkpointing: `Trainer(enable_checkpointing=True)`\",\r\n                \" - Progress bar: `Trainer(enable_progress_bar=True)`\",\r\n                \" - Model summary: `Trainer(enable_model_summary=True)`\",\r\n                \" - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`,\"\r\n                \" `LightningModule.log(...)`, `LightningModule.log_dict(...)`\",\r\n                \" - Sanity checking: `Trainer(num_sanity_val_steps>0)`\",\r\n                \" - Development run: `Trainer(fast_dev_run=True)`\",\r\n                \" - Anomaly detection: `Trainer(detect_anomaly=True)`\",\r\n                \" - Profiling: `Trainer(profiler=...)`\",\r\n            )\r\n            rank_zero_info(\r\n                \"You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been\"\r\n                \" disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are\"\r\n                f\" deactivated:{os.linesep}{os.linesep.join(deactivated)}\"\r\n            )\r\n        else:\r\n            if enable_checkpointing is None:\r\n                enable_checkpointing = True\r\n            if logger is None:\r\n                logger = True\r\n            if enable_progress_bar is None:\r\n                enable_progress_bar = True\r\n            if log_every_n_steps is None:\r\n                log_every_n_steps = 50\r\n            if enable_model_summary is None:\r\n                enable_model_summary = True\r\n            if num_sanity_val_steps is None:\r\n                num_sanity_val_steps = 2\r\n\r\n        self._data_connector = _DataConnector(self)\r\n\r\n        self._accelerator_connector = _AcceleratorConnector(\r\n            devices=devices,\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            num_nodes=num_nodes,\r\n            sync_batchnorm=sync_batchnorm,\r\n            benchmark=benchmark,\r\n            use_distributed_sampler=use_distributed_sampler,\r\n            deterministic=deterministic,\r\n            precision=precision,\r\n            plugins=plugins,\r\n        )\r\n        self._logger_connector = _LoggerConnector(self)\r\n        self._callback_connector = _CallbackConnector(self)\r\n        self._checkpoint_connector = _CheckpointConnector(self)\r\n        self._signal_connector = _SignalConnector(self)\r\n\r\n        self.fit_loop = _FitLoop(self, min_epochs=min_epochs, max_epochs=max_epochs)\r\n        self.fit_loop.epoch_loop = _TrainingEpochLoop(self, min_steps=min_steps, max_steps=max_steps)\r\n        self.validate_loop = _EvaluationLoop(\r\n            self, TrainerFn.VALIDATING, RunningStage.VALIDATING, inference_mode=inference_mode\r\n        )\r\n        self.test_loop = _EvaluationLoop(self, TrainerFn.TESTING, RunningStage.TESTING, inference_mode=inference_mode)\r\n        self.predict_loop = _PredictionLoop(self, inference_mode=inference_mode)\r\n\r\n        self.accumulate_grad_batches = accumulate_grad_batches\r\n\r\n        self._callback_connector.on_trainer_init(\r\n            callbacks,\r\n            enable_checkpointing,\r\n            enable_progress_bar,\r\n            default_root_dir,\r\n            enable_model_summary,\r\n            max_time,\r\n        )\r\n\r\n        self.check_val_every_n_epoch: Optional[int]\r\n        self._data_connector.on_trainer_init(\r\n            val_check_interval,\r\n            reload_dataloaders_every_n_epochs,\r\n            check_val_every_n_epoch,\r\n        )\r\n\r\n        if gradient_clip_val is not None and not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if gradient_clip_algorithm is not None and not GradClipAlgorithmType.supported_type(\r\n            gradient_clip_algorithm.lower()\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid. \"\r\n                f\"Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        self.gradient_clip_val: Optional[Union[int, float]] = gradient_clip_val\r\n        self.gradient_clip_algorithm: Optional[GradClipAlgorithmType] = (\r\n            GradClipAlgorithmType(gradient_clip_algorithm.lower()) if gradient_clip_algorithm is not None else None\r\n        )\r\n\r\n        if detect_anomaly:\r\n            rank_zero_info(\r\n                \"You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and\"\r\n                \" is recommended only for model debugging.\"\r\n            )\r\n        self._detect_anomaly: bool = detect_anomaly\r\n\r\n        setup._log_device_info(self)\r\n\r\n        self.should_stop = False\r\n        self.state = TrainerState()\r\n\r\n        setup._init_profiler(self, profiler)\r\n\r\n        self._loggers: list[Logger]\r\n        self._logger_connector.on_trainer_init(logger, log_every_n_steps)\r\n\r\n        self.val_check_batch: Optional[Union[int, float]] = None\r\n        self.val_check_interval: Union[int, float]\r\n        self.num_sanity_val_steps: Union[int, float]\r\n        self.limit_train_batches: Union[int, float]\r\n        self.limit_val_batches: Union[int, float]\r\n        self.limit_test_batches: Union[int, float]\r\n        self.limit_predict_batches: Union[int, float]\r\n        setup._init_debugging_flags(\r\n            self,\r\n            limit_train_batches,\r\n            limit_val_batches,\r\n            limit_test_batches,\r\n            limit_predict_batches,\r\n            fast_dev_run,\r\n            overfit_batches,\r\n            val_check_interval,\r\n            num_sanity_val_steps,\r\n        )\r\n\r\n        self.enable_autolog_hparams = enable_autolog_hparams", "language": "python", "code": "def __init__(\r\n        self,\r\n        *,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        logger: Optional[Union[Logger, Iterable[Logger], bool]] = None,\r\n        callbacks: Optional[Union[list[Callback], Callback]] = None,\r\n        fast_dev_run: Union[int, bool] = False,\r\n        max_epochs: Optional[int] = None,\r\n        min_epochs: Optional[int] = None,\r\n        max_steps: int = -1,\r\n        min_steps: Optional[int] = None,\r\n        max_time: Optional[Union[str, timedelta, dict[str, int]]] = None,\r\n        limit_train_batches: Optional[Union[int, float]] = None,\r\n        limit_val_batches: Optional[Union[int, float]] = None,\r\n        limit_test_batches: Optional[Union[int, float]] = None,\r\n        limit_predict_batches: Optional[Union[int, float]] = None,\r\n        overfit_batches: Union[int, float] = 0.0,\r\n        val_check_interval: Optional[Union[int, float, str, timedelta, dict[str, int]]] = None,\r\n        check_val_every_n_epoch: Optional[int] = 1,\r\n        num_sanity_val_steps: Optional[int] = None,\r\n        log_every_n_steps: Optional[int] = None,\r\n        enable_checkpointing: Optional[bool] = None,\r\n        enable_progress_bar: Optional[bool] = None,\r\n        enable_model_summary: Optional[bool] = None,\r\n        accumulate_grad_batches: int = 1,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n        benchmark: Optional[bool] = None,\r\n        inference_mode: bool = True,\r\n        use_distributed_sampler: bool = True,\r\n        profiler: Optional[Union[Profiler, str]] = None,\r\n        detect_anomaly: bool = False,\r\n        barebones: bool = False,\r\n        plugins: Optional[Union[_PLUGIN_INPUT, list[_PLUGIN_INPUT]]] = None,\r\n        sync_batchnorm: bool = False,\r\n        reload_dataloaders_every_n_epochs: int = 0,\r\n        default_root_dir: Optional[_PATH] = None,\r\n        enable_autolog_hparams: bool = True,\r\n        model_registry: Optional[str] = None,\r\n    ) -> None:\r\n        r\"\"\"Customize every aspect of training via flags.\r\n\r\n        Args:\r\n            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\r\n                as well as custom accelerator instances.\r\n\r\n            strategy: Supports different training strategies with aliases as well custom strategies.\r\n                Default: ``\"auto\"``.\r\n\r\n            devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\r\n                (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\r\n                automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\r\n\r\n            num_nodes: Number of GPU nodes for distributed training.\r\n                Default: ``1``.\r\n\r\n            precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\r\n                16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\r\n                Can be used on CPU, GPU, TPUs, or HPUs.\r\n                Default: ``'32-true'``.\r\n\r\n            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\r\n                the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\r\n                ``False`` will disable logging. If multiple loggers are provided, local files\r\n                (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\r\n                Default: ``True``.\r\n\r\n            callbacks: Add a callback or list of callbacks.\r\n                Default: ``None``.\r\n\r\n            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\r\n                of train, val and test to find any bugs (ie: a sort of unit test).\r\n                Default: ``False``.\r\n\r\n            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\r\n                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\r\n                To enable infinite training, set ``max_epochs = -1``.\r\n\r\n            min_epochs: Force training for at least these many epochs. Disabled by default (None).\r\n\r\n            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\r\n                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\r\n                ``max_epochs`` to ``-1``.\r\n\r\n            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\r\n\r\n            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\r\n                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\r\n                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\r\n                :class:`datetime.timedelta`.\r\n\r\n            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\r\n                Value is per device. Default: ``1.0``.\r\n\r\n            overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\r\n                Default: ``0.0``.\r\n\r\n            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\r\n                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\r\n                batches. An ``int`` value can only be higher than the number of training batches when\r\n                ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\r\n                across epochs or during iteration-based training. Additionally, accepts a time-based duration\r\n                as a string \"DD:HH:MM:SS\", a :class:`datetime.timedelta`, or a dict of kwargs to\r\n                :class:`datetime.timedelta`. When time-based, validation triggers once the elapsed wall-clock time\r\n                since the last validation exceeds the interval; the check occurs after the current batch\r\n                completes, the validation loop runs, and the timer is reset.\r\n                Default: ``1.0``.\r\n\r\n            check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\r\n                validation will be done solely based on the number of training batches, requiring ``val_check_interval``\r\n                to be an integer value. When used together with a time-based ``val_check_interval`` and\r\n                ``check_val_every_n_epoch`` > 1, validation is aligned to epoch multiples: if the interval elapses\r\n                before the next multiple-N epoch, validation runs at the start of that epoch (after the first batch)\r\n                and the timer resets; if it elapses during a multiple-N epoch, validation runs after the current batch.\r\n                For ``None`` or ``1`` cases, the time-based behavior of ``val_check_interval`` applies without\r\n                additional alignment.\r\n                Default: ``1``.\r\n\r\n            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\r\n                Set it to `-1` to run all batches in all validation dataloaders.\r\n                Default: ``2``.\r\n\r\n            log_every_n_steps: How often to log within steps.\r\n                Default: ``50``.\r\n\r\n            enable_checkpointing: If ``True``, enable checkpointing.\r\n                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks`.\r\n                Default: ``True``.\r\n\r\n            enable_progress_bar: Whether to enable to progress bar by default.\r\n                Default: ``True``.\r\n\r\n            enable_model_summary: Whether to enable model summarization by default.\r\n                Default: ``True``.\r\n\r\n            accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\r\n                Default: 1.\r\n\r\n            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\r\n                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\r\n                Default: ``None``.\r\n\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\r\n                be set to ``\"norm\"``.\r\n\r\n            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\r\n                Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\r\n                that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\r\n\r\n            benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\r\n                The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\r\n                (``False`` if not manually set). If :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic`\r\n                is set to ``True``, this will default to ``False``. Override to manually set a different value.\r\n                Default: ``None``.\r\n\r\n            inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\r\n                evaluation (``validate``/``test``/``predict``).\r\n\r\n            use_distributed_sampler: Whether to wrap the DataLoader's sampler with\r\n                :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\r\n                strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\r\n                ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\r\n                ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\r\n                sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\r\n                we don't do this automatically.\r\n\r\n            profiler: To profile individual steps during training and assist in identifying bottlenecks.\r\n                Default: ``None``.\r\n\r\n            detect_anomaly: Enable anomaly detection for the autograd engine.\r\n                Default: ``False``.\r\n\r\n            barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\r\n                disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\r\n                runs. The following features are deactivated:\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_checkpointing`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.logger`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_progress_bar`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.log_every_n_steps`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_model_summary`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.detect_anomaly`,\r\n                :paramref:`~lightning.pytorch.trainer.trainer.Trainer.profiler`,\r\n                :meth:`~lightning.pytorch.core.LightningModule.log`,\r\n                :meth:`~lightning.pytorch.core.LightningModule.log_dict`.\r\n            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\r\n                Default: ``None``.\r\n\r\n            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\r\n                Default: ``False``.\r\n\r\n            reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\r\n                Default: ``0``.\r\n\r\n            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\r\n                Default: ``os.getcwd()``.\r\n                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\r\n\r\n            enable_autolog_hparams: Whether to log hyperparameters at the start of a run.\r\n                Default: ``True``.\r\n\r\n            model_registry: The name of the model being uploaded to Model hub.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``gradient_clip_val`` is not an int or float.\r\n\r\n            MisconfigurationException:\r\n                If ``gradient_clip_algorithm`` is invalid.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        log.debug(f\"{self.__class__.__name__}: Initializing trainer with parameters: {locals()}\")\r\n\r\n        if default_root_dir is not None:\r\n            default_root_dir = os.fspath(default_root_dir)\r\n\r\n        self._model_registry = model_registry.split(\":\")[0] if model_registry else None\r\n\r\n        self.barebones = barebones\r\n        if barebones:\r\n            if enable_checkpointing:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_checkpointing={enable_checkpointing!r})` was passed.\"\r\n                    \" Checkpointing can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_checkpointing = False\r\n            if logger is not None and logger is not False:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, logger={logger!r})` was passed.\"\r\n                    \" Logging can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            logger = False\r\n            if enable_progress_bar:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_progress_bar={enable_progress_bar!r})` was passed.\"\r\n                    \" The progress bar can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_progress_bar = False\r\n            if log_every_n_steps is not None and log_every_n_steps != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, log_every_n_steps={log_every_n_steps!r})` was passed.\"\r\n                    \" Logging can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            log_every_n_steps = 0\r\n            if enable_model_summary:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, enable_model_summary={enable_model_summary!r})` was passed.\"\r\n                    \" Model summary can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            enable_model_summary = False\r\n            if num_sanity_val_steps is not None and num_sanity_val_steps != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, num_sanity_val_steps={num_sanity_val_steps!r})` was passed.\"\r\n                    \" Sanity checking can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            num_sanity_val_steps = 0\r\n            if fast_dev_run is not False and fast_dev_run != 0:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, fast_dev_run={fast_dev_run!r})` was passed.\"\r\n                    \" Development run is not meant for raw speed evaluation so it is disabled in barebones mode.\"\r\n                )\r\n            if detect_anomaly:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, detect_anomaly={detect_anomaly!r})` was passed.\"\r\n                    \" Anomaly detection can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            if profiler is not None:\r\n                raise ValueError(\r\n                    f\"`Trainer(barebones=True, profiler={profiler!r})` was passed.\"\r\n                    \" Profiling can impact raw speed so it is disabled in barebones mode.\"\r\n                )\r\n            deactivated = (\r\n                \" - Checkpointing: `Trainer(enable_checkpointing=True)`\",\r\n                \" - Progress bar: `Trainer(enable_progress_bar=True)`\",\r\n                \" - Model summary: `Trainer(enable_model_summary=True)`\",\r\n                \" - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`,\"\r\n                \" `LightningModule.log(...)`, `LightningModule.log_dict(...)`\",\r\n                \" - Sanity checking: `Trainer(num_sanity_val_steps>0)`\",\r\n                \" - Development run: `Trainer(fast_dev_run=True)`\",\r\n                \" - Anomaly detection: `Trainer(detect_anomaly=True)`\",\r\n                \" - Profiling: `Trainer(profiler=...)`\",\r\n            )\r\n            rank_zero_info(\r\n                \"You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been\"\r\n                \" disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are\"\r\n                f\" deactivated:{os.linesep}{os.linesep.join(deactivated)}\"\r\n            )\r\n        else:\r\n            if enable_checkpointing is None:\r\n                enable_checkpointing = True\r\n            if logger is None:\r\n                logger = True\r\n            if enable_progress_bar is None:\r\n                enable_progress_bar = True\r\n            if log_every_n_steps is None:\r\n                log_every_n_steps = 50\r\n            if enable_model_summary is None:\r\n                enable_model_summary = True\r\n            if num_sanity_val_steps is None:\r\n                num_sanity_val_steps = 2\r\n\r\n        self._data_connector = _DataConnector(self)\r\n\r\n        self._accelerator_connector = _AcceleratorConnector(\r\n            devices=devices,\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            num_nodes=num_nodes,\r\n            sync_batchnorm=sync_batchnorm,\r\n            benchmark=benchmark,\r\n            use_distributed_sampler=use_distributed_sampler,\r\n            deterministic=deterministic,\r\n            precision=precision,\r\n            plugins=plugins,\r\n        )\r\n        self._logger_connector = _LoggerConnector(self)\r\n        self._callback_connector = _CallbackConnector(self)\r\n        self._checkpoint_connector = _CheckpointConnector(self)\r\n        self._signal_connector = _SignalConnector(self)\r\n\r\n        self.fit_loop = _FitLoop(self, min_epochs=min_epochs, max_epochs=max_epochs)\r\n        self.fit_loop.epoch_loop = _TrainingEpochLoop(self, min_steps=min_steps, max_steps=max_steps)\r\n        self.validate_loop = _EvaluationLoop(\r\n            self, TrainerFn.VALIDATING, RunningStage.VALIDATING, inference_mode=inference_mode\r\n        )\r\n        self.test_loop = _EvaluationLoop(self, TrainerFn.TESTING, RunningStage.TESTING, inference_mode=inference_mode)\r\n        self.predict_loop = _PredictionLoop(self, inference_mode=inference_mode)\r\n\r\n        self.accumulate_grad_batches = accumulate_grad_batches\r\n\r\n        self._callback_connector.on_trainer_init(\r\n            callbacks,\r\n            enable_checkpointing,\r\n            enable_progress_bar,\r\n            default_root_dir,\r\n            enable_model_summary,\r\n            max_time,\r\n        )\r\n\r\n        self.check_val_every_n_epoch: Optional[int]\r\n        self._data_connector.on_trainer_init(\r\n            val_check_interval,\r\n            reload_dataloaders_every_n_epochs,\r\n            check_val_every_n_epoch,\r\n        )\r\n\r\n        if gradient_clip_val is not None and not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if gradient_clip_algorithm is not None and not GradClipAlgorithmType.supported_type(\r\n            gradient_clip_algorithm.lower()\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid. \"\r\n                f\"Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        self.gradient_clip_val: Optional[Union[int, float]] = gradient_clip_val\r\n        self.gradient_clip_algorithm: Optional[GradClipAlgorithmType] = (\r\n            GradClipAlgorithmType(gradient_clip_algorithm.lower()) if gradient_clip_algorithm is not None else None\r\n        )\r\n\r\n        if detect_anomaly:\r\n            rank_zero_info(\r\n                \"You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and\"\r\n                \" is recommended only for model debugging.\"\r\n            )\r\n        self._detect_anomaly: bool = detect_anomaly\r\n\r\n        setup._log_device_info(self)\r\n\r\n        self.should_stop = False\r\n        self.state = TrainerState()\r\n\r\n        setup._init_profiler(self, profiler)\r\n\r\n        self._loggers: list[Logger]\r\n        self._logger_connector.on_trainer_init(logger, log_every_n_steps)\r\n\r\n        self.val_check_batch: Optional[Union[int, float]] = None\r\n        self.val_check_interval: Union[int, float]\r\n        self.num_sanity_val_steps: Union[int, float]\r\n        self.limit_train_batches: Union[int, float]\r\n        self.limit_val_batches: Union[int, float]\r\n        self.limit_test_batches: Union[int, float]\r\n        self.limit_predict_batches: Union[int, float]\r\n        setup._init_debugging_flags(\r\n            self,\r\n            limit_train_batches,\r\n            limit_val_batches,\r\n            limit_test_batches,\r\n            limit_predict_batches,\r\n            fast_dev_run,\r\n            overfit_batches,\r\n            val_check_interval,\r\n            num_sanity_val_steps,\r\n        )\r\n\r\n        self.enable_autolog_hparams = enable_autolog_hparams", "code_tokens": ["def", "__init__", "(", "self", ",", "*", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", "=", "STRING", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", "=", "STRING", ",", "devices", ":", "Union", "[", "list", "[", "int", "]", ",", "str", ",", "int", "]", "=", "STRING", ",", "num_nodes", ":", "int", "=", "1", ",", "precision", ":", "Optional", "[", "_PRECISION_INPUT", "]", "=", "None", ",", "logger", ":", "Optional", "[", "Union", "[", "Logger", ",", "Iterable", "[", "Logger", "]", ",", "bool", "]", "]", "=", "None", ",", "callbacks", ":", "Optional", "[", "Union", "[", "list", "[", "Callback", "]", ",", "Callback", "]", "]", "=", "None", ",", "fast_dev_run", ":", "Union", "[", "int", ",", "bool", "]", "=", "False", ",", "max_epochs", ":", "Optional", "[", "int", "]", "=", "None", ",", "min_epochs", ":", "Optional", "[", "int", "]", "=", "None", ",", "max_steps", ":", "int", "=", "-", "1", ",", "min_steps", ":", "Optional", "[", "int", "]", "=", "None", ",", "max_time", ":", "Optional", "[", "Union", "[", "str", ",", "timedelta", ",", "dict", "[", "str", ",", "int", "]", "]", "]", "=", "None", ",", "limit_train_batches", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "limit_val_batches", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "limit_test_batches", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "limit_predict_batches", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "overfit_batches", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "0", ",", "val_check_interval", ":", "Optional", "[", "Union", "[", "int", ",", "float", ",", "str", ",", "timedelta", ",", "dict", "[", "str", ",", "int", "]", "]", "]", "=", "None", ",", "check_val_every_n_epoch", ":", "Optional", "[", "int", "]", "=", "1", ",", "num_sanity_val_steps", ":", "Optional", "[", "int", "]", "=", "None", ",", "log_every_n_steps", ":", "Optional", "[", "int", "]", "=", "None", ",", "enable_checkpointing", ":", "Optional", "[", "bool", "]", "=", "None", ",", "enable_progress_bar", ":", "Optional", "[", "bool", "]", "=", "None", ",", "enable_model_summary", ":", "Optional", "[", "bool", "]", "=", "None", ",", "accumulate_grad_batches", ":", "int", "=", "1", ",", "gradient_clip_val", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "gradient_clip_algorithm", ":", "Optional", "[", "str", "]", "=", "None", ",", "deterministic", ":", "Optional", "[", "Union", "[", "bool", ",", "_LITERAL_WARN", "]", "]", "=", "None", ",", "benchmark", ":", "Optional", "[", "bool", "]", "=", "None", ",", "inference_mode", ":", "bool", "=", "True", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "profiler", ":", "Optional", "[", "Union", "[", "Profiler", ",", "str", "]", "]", "=", "None", ",", "detect_anomaly", ":", "bool", "=", "False", ",", "barebones", ":", "bool", "=", "False", ",", "plugins", ":", "Optional", "[", "Union", "[", "_PLUGIN_INPUT", ",", "list", "[", "_PLUGIN_INPUT", "]", "]", "]", "=", "None", ",", "sync_batchnorm", ":", "bool", "=", "False", ",", "reload_dataloaders_every_n_epochs", ":", "int", "=", "0", ",", "default_root_dir", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "enable_autolog_hparams", ":", "bool", "=", "True", ",", "model_registry", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "None", ":", "rSTRING", "super", "(", ")", ".", "__init__", "(", ")", "log", ".", "debug", "(", "fSTRING", ")", "if", "default_root_dir", "is", "not", "None", ":", "default_root_dir", "=", "os", ".", "fspath", "(", "default_root_dir", ")", "self", ".", "_model_registry", "=", "model_registry", ".", "split", "(", "STRING", ")", "[", "0", "]", "if", "model_registry", "else", "None", "self", ".", "barebones", "=", "barebones", "if", "barebones", ":", "if"], "docstring": "r\"\"\"Customize every aspect of training via flags.", "docstring_tokens": ["r", "customize", "every", "aspect", "of", "training", "via", "flags"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 90, "end_line": 519, "has_examples": false, "num_comments": 12, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_905", "original_string": "def fit(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> None:\r\n        r\"\"\"Runs the full optimization routine.\r\n\r\n        Args:\r\n            model: Model to fit.\r\n\r\n            train_dataloaders: An iterable or collection of iterables specifying training samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            val_dataloaders: An iterable or collection of iterables specifying validation samples.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of three special\r\n                keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\r\n                Otherwise, if there is no checkpoint file at the path, an exception is raised.\r\n\r\n                    - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - registry: the model will be downloaded from the Lightning Model Registry with following notations:\r\n\r\n                        - ``'registry'``: uses the latest/default version of default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")``\r\n                        - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\r\n                        - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\r\n                        - ``'registry:version:v2'``: uses the default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")`` and version 'v2'\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\r\n                2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\r\n                :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        \"\"\"\r\n        model = _maybe_unwrap_optimized(model)\r\n        self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(model, self.strategy)\r\n        self.state.fn = TrainerFn.FITTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.training = True\r\n        self.should_stop = False\r\n        call._call_and_handle_interrupt(\r\n            self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n        )", "language": "python", "code": "def fit(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> None:\r\n        r\"\"\"Runs the full optimization routine.\r\n\r\n        Args:\r\n            model: Model to fit.\r\n\r\n            train_dataloaders: An iterable or collection of iterables specifying training samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            val_dataloaders: An iterable or collection of iterables specifying validation samples.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\r\n\r\n            ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of three special\r\n                keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\r\n                Otherwise, if there is no checkpoint file at the path, an exception is raised.\r\n\r\n                    - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                    - registry: the model will be downloaded from the Lightning Model Registry with following notations:\r\n\r\n                        - ``'registry'``: uses the latest/default version of default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")``\r\n                        - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\r\n                        - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\r\n                        - ``'registry:version:v2'``: uses the default model set\r\n                          with ``Trainer(..., model_registry=\"my-model\")`` and version 'v2'\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\r\n                2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\r\n                :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        \"\"\"\r\n        model = _maybe_unwrap_optimized(model)\r\n        self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(model, self.strategy)\r\n        self.state.fn = TrainerFn.FITTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.training = True\r\n        self.should_stop = False\r\n        call._call_and_handle_interrupt(\r\n            self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n        )", "code_tokens": ["def", "fit", "(", "self", ",", "model", ":", "STRING", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", ")", "-", ">", "None", ":", "rSTRING", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "model", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "FITTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "training", "=", "True", "self", ".", "should_stop", "=", "False", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_fit_impl", ",", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ",", "ckpt_path", ")"], "docstring": "r\"\"\"Runs the full optimization routine.", "docstring_tokens": ["r", "runs", "the", "full", "optimization", "routine"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 521, "end_line": 576, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_906", "original_string": "def validate(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the validation set.\r\n\r\n        Args:\r\n            model: The model to validate.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying validation samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to validate. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the validation results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\r\n            The length of the list corresponds to the number of validation dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.validate()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.VALIDATING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.validating = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "language": "python", "code": "def validate(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the validation set.\r\n\r\n        Args:\r\n            model: The model to validate.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying validation samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to validate. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the validation results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\r\n            The length of the list corresponds to the number of validation dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.validate()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.VALIDATING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.validating = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._validate_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "code_tokens": ["def", "validate", "(", "self", ",", "model", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", ")", "-", ">", "_EVALUATE_OUTPUT", ":", "rSTRING", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "STRING", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "VALIDATING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "validating", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_validate_impl", ",", "model", ",", "dataloaders", ",", "ckpt_path", ",", "verbose", ",", "datamodule", ")"], "docstring": "r\"\"\"Perform one evaluation epoch over the validation set.", "docstring_tokens": ["r", "perform", "one", "evaluation", "epoch", "over", "the", "validation", "set"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 618, "end_line": 679, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_907", "original_string": "def test(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\r\n        test set until you want to.\r\n\r\n        Args:\r\n            model: The model to test.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying test samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to test. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the test results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\r\n            The length of the list corresponds to the number of test dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.test()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.TESTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.testing = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "language": "python", "code": "def test(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n        verbose: bool = True,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n    ) -> _EVALUATE_OUTPUT:\r\n        r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\r\n        test set until you want to.\r\n\r\n        Args:\r\n            model: The model to test.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying test samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to test. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n            verbose: If True, prints the test results.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\r\n            like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\r\n            The length of the list corresponds to the number of test dataloaders used.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.test()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.TESTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.testing = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule\r\n        )", "code_tokens": ["def", "test", "(", "self", ",", "model", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", ")", "-", ">", "_EVALUATE_OUTPUT", ":", "rSTRING", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "STRING", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "TESTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "testing", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_test_impl", ",", "model", ",", "dataloaders", ",", "ckpt_path", ",", "verbose", ",", "datamodule", ")"], "docstring": "r\"\"\"Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your", "docstring_tokens": ["r", "perform", "one", "evaluation", "epoch", "over", "the", "test", "set", "it", "s", "separated", "from", "fit", "to", "make", "sure", "you", "never", "run", "on", "your"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 728, "end_line": 790, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_908", "original_string": "def predict(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        return_predictions: Optional[bool] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> Optional[_PREDICT_OUTPUT]:\r\n        r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to\r\n        perform distributed and batched predictions. Logging is disabled in the predict hooks.\r\n\r\n        Args:\r\n            model: The model to predict with.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying predict samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            return_predictions: Whether to return predictions.\r\n                ``True`` by default except when an accelerator that spawns processes is used (not supported).\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to predict. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.predict()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.PREDICTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.predicting = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n        )", "language": "python", "code": "def predict(\r\n        self,\r\n        model: Optional[\"pl.LightningModule\"] = None,\r\n        dataloaders: Optional[Union[EVAL_DATALOADERS, LightningDataModule]] = None,\r\n        datamodule: Optional[LightningDataModule] = None,\r\n        return_predictions: Optional[bool] = None,\r\n        ckpt_path: Optional[_PATH] = None,\r\n    ) -> Optional[_PREDICT_OUTPUT]:\r\n        r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to\r\n        perform distributed and batched predictions. Logging is disabled in the predict hooks.\r\n\r\n        Args:\r\n            model: The model to predict with.\r\n\r\n            dataloaders: An iterable or collection of iterables specifying predict samples.\r\n                Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\r\n                the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\r\n\r\n            return_predictions: Whether to return predictions.\r\n                ``True`` by default except when an accelerator that spawns processes is used (not supported).\r\n\r\n            ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\r\n                to predict. If ``None`` and the model instance was passed, use the current weights.\r\n                Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\r\n                if a checkpoint callback is configured.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        Returns:\r\n            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\r\n                If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\r\n\r\n            MisconfigurationException:\r\n                If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\r\n\r\n            RuntimeError:\r\n                If a compiled ``model`` is passed and the strategy is not supported.\r\n\r\n        See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\r\n\r\n        \"\"\"\r\n        if model is None:\r\n            if self.lightning_module is None:\r\n                raise TypeError(\r\n                    \"`Trainer.predict()` requires a `LightningModule` when it hasn't been passed in a previous run\"\r\n                )\r\n        else:\r\n            model = _maybe_unwrap_optimized(model)\r\n            self.strategy._lightning_module = model\r\n        _verify_strategy_supports_compile(self.lightning_module, self.strategy)\r\n        self.state.fn = TrainerFn.PREDICTING\r\n        self.state.status = TrainerStatus.RUNNING\r\n        self.predicting = True\r\n        return call._call_and_handle_interrupt(\r\n            self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n        )", "code_tokens": ["def", "predict", "(", "self", ",", "model", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "Union", "[", "EVAL_DATALOADERS", ",", "LightningDataModule", "]", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "LightningDataModule", "]", "=", "None", ",", "return_predictions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "rSTRING", "if", "model", "is", "None", ":", "if", "self", ".", "lightning_module", "is", "None", ":", "raise", "TypeError", "(", "STRING", ")", "else", ":", "model", "=", "_maybe_unwrap_optimized", "(", "model", ")", "self", ".", "strategy", ".", "_lightning_module", "=", "model", "_verify_strategy_supports_compile", "(", "self", ".", "lightning_module", ",", "self", ".", "strategy", ")", "self", ".", "state", ".", "fn", "=", "TrainerFn", ".", "PREDICTING", "self", ".", "state", ".", "status", "=", "TrainerStatus", ".", "RUNNING", "self", ".", "predicting", "=", "True", "return", "call", ".", "_call_and_handle_interrupt", "(", "self", ",", "self", ".", "_predict_impl", ",", "model", ",", "dataloaders", ",", "datamodule", ",", "return_predictions", ",", "ckpt_path", ")"], "docstring": "r\"\"\"Run inference on your data. This will call the model forward function to compute predictions. Useful to", "docstring_tokens": ["r", "run", "inference", "on", "your", "data", "this", "will", "call", "the", "model", "forward", "function", "to", "compute", "predictions", "useful", "to"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 839, "end_line": 902, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_909", "original_string": "def _teardown(self) -> None:\r\n        \"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\r\n        those are handled by :meth:`_call_teardown_hook`.\"\"\"\r\n        self.strategy.teardown()\r\n        loop = self._active_loop\r\n        if loop is not None:\r\n            loop.teardown()\r\n        self._logger_connector.teardown()\r\n        self._signal_connector.teardown()", "language": "python", "code": "def _teardown(self) -> None:\r\n        \"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\r\n        those are handled by :meth:`_call_teardown_hook`.\"\"\"\r\n        self.strategy.teardown()\r\n        loop = self._active_loop\r\n        if loop is not None:\r\n            loop.teardown()\r\n        self._logger_connector.teardown()\r\n        self._signal_connector.teardown()", "code_tokens": ["def", "_teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "strategy", ".", "teardown", "(", ")", "loop", "=", "self", ".", "_active_loop", "if", "loop", "is", "not", "None", ":", "loop", ".", "teardown", "(", ")", "self", ".", "_logger_connector", ".", "teardown", "(", ")", "self", ".", "_signal_connector", ".", "teardown", "(", ")"], "docstring": "This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;", "docstring_tokens": ["this", "is", "the", "trainer", "s", "internal", "teardown", "unrelated", "to", "the", "teardown", "hooks", "in", "lightningmodule", "and", "callback"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1047, "end_line": 1056, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_910", "original_string": "def init_module(self, empty_init: Optional[bool] = None) -> Generator:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in the Trainer.\r\n\r\n        The parameters and tensors get created on the device and with the right data type right away without wasting\r\n        memory being allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        if is_overridden(\"model_sharded_context\", self.strategy, parent=Strategy):\r\n            rank_zero_warn(\r\n                f\"`trainer.init_module` cannot fully support proper instantiation of your model with the\"\r\n                f\" `{type(self.strategy).__name__}` strategy. Please instantiate your model inside the\"\r\n                f\"`LightningModule.configure_model` hook instead\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        with self.strategy.tensor_init_context(empty_init=empty_init):\r\n            yield", "language": "python", "code": "def init_module(self, empty_init: Optional[bool] = None) -> Generator:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in the Trainer.\r\n\r\n        The parameters and tensors get created on the device and with the right data type right away without wasting\r\n        memory being allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        if is_overridden(\"model_sharded_context\", self.strategy, parent=Strategy):\r\n            rank_zero_warn(\r\n                f\"`trainer.init_module` cannot fully support proper instantiation of your model with the\"\r\n                f\" `{type(self.strategy).__name__}` strategy. Please instantiate your model inside the\"\r\n                f\"`LightningModule.configure_model` hook instead\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        with self.strategy.tensor_init_context(empty_init=empty_init):\r\n            yield", "code_tokens": ["def", "init_module", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "Generator", ":", "STRING", "if", "is_overridden", "(", "STRING", ",", "self", ".", "strategy", ",", "parent", "=", "Strategy", ")", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", "fSTRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "with", "self", ".", "strategy", ".", "tensor_init_context", "(", "empty_init", "=", "empty_init", ")", ":", "yield"], "docstring": "Tensors that you instantiate under this context manager will be created on the device right away and have", "docstring_tokens": ["tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1120, "end_line": 1145, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_911", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "local_rank", "=", "=", "0", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Print something only on the first process. If running on multiple machines, it will print from the first", "docstring_tokens": ["print", "something", "only", "on", "the", "first", "process", "if", "running", "on", "multiple", "machines", "it", "will", "print", "from", "the", "first"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1147, "end_line": 1155, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_912", "original_string": "def device_ids(self) -> list[int]:\r\n        \"\"\"List of device indexes per node.\"\"\"\r\n        devices = (\r\n            self.strategy.parallel_devices\r\n            if isinstance(self.strategy, ParallelStrategy)\r\n            else [self.strategy.root_device]\r\n        )\r\n        assert devices is not None\r\n        device_ids = []\r\n        for idx, device in enumerate(devices):\r\n            if isinstance(device, torch.device):\r\n                device_ids.append(device.index or idx)\r\n            elif isinstance(device, int):\r\n                device_ids.append(device)\r\n        return device_ids", "language": "python", "code": "def device_ids(self) -> list[int]:\r\n        \"\"\"List of device indexes per node.\"\"\"\r\n        devices = (\r\n            self.strategy.parallel_devices\r\n            if isinstance(self.strategy, ParallelStrategy)\r\n            else [self.strategy.root_device]\r\n        )\r\n        assert devices is not None\r\n        device_ids = []\r\n        for idx, device in enumerate(devices):\r\n            if isinstance(device, torch.device):\r\n                device_ids.append(device.index or idx)\r\n            elif isinstance(device, int):\r\n                device_ids.append(device)\r\n        return device_ids", "code_tokens": ["def", "device_ids", "(", "self", ")", "-", ">", "list", "[", "int", "]", ":", "STRING", "devices", "=", "(", "self", ".", "strategy", ".", "parallel_devices", "if", "isinstance", "(", "self", ".", "strategy", ",", "ParallelStrategy", ")", "else", "[", "self", ".", "strategy", ".", "root_device", "]", ")", "assert", "devices", "is", "not", "None", "device_ids", "=", "[", "]", "for", "idx", ",", "device", "in", "enumerate", "(", "devices", ")", ":", "if", "isinstance", "(", "device", ",", "torch", ".", "device", ")", ":", "device_ids", ".", "append", "(", "device", ".", "index", "or", "idx", ")", "elif", "isinstance", "(", "device", ",", "int", ")", ":", "device_ids", ".", "append", "(", "device", ")", "return", "device_ids"], "docstring": "List of device indexes per node.", "docstring_tokens": ["list", "of", "device", "indexes", "per", "node"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1198, "end_line": 1212, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_913", "original_string": "def num_devices(self) -> int:\r\n        \"\"\"Number of devices the trainer uses per node.\"\"\"\r\n        return len(self.device_ids)", "language": "python", "code": "def num_devices(self) -> int:\r\n        \"\"\"Number of devices the trainer uses per node.\"\"\"\r\n        return len(self.device_ids)", "code_tokens": ["def", "num_devices", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "len", "(", "self", ".", "device_ids", ")"], "docstring": "Number of devices the trainer uses per node.", "docstring_tokens": ["number", "of", "devices", "the", "trainer", "uses", "per", "node"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1215, "end_line": 1217, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_914", "original_string": "def model(self) -> Optional[torch.nn.Module]:\r\n        \"\"\"The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\r\n\r\n        To access the pure LightningModule, use\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.\r\n\r\n        \"\"\"\r\n        return self.strategy.model", "language": "python", "code": "def model(self) -> Optional[torch.nn.Module]:\r\n        \"\"\"The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\r\n\r\n        To access the pure LightningModule, use\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.\r\n\r\n        \"\"\"\r\n        return self.strategy.model", "code_tokens": ["def", "model", "(", "self", ")", "-", ">", "Optional", "[", "torch", ".", "nn", ".", "Module", "]", ":", "STRING", "return", "self", ".", "strategy", ".", "model"], "docstring": "The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.", "docstring_tokens": ["the", "lightningmodule", "but", "possibly", "wrapped", "into", "dataparallel", "or", "distributeddataparallel"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1245, "end_line": 1252, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_915", "original_string": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"The directory for the current experiment. Use this to save images to, etc...\r\n\r\n        .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n         .. code-block:: python\r\n\r\n             def training_step(self, batch, batch_idx):\r\n                 img = ...\r\n                 save_img(img, self.trainer.log_dir)\r\n\r\n        \"\"\"\r\n        if len(self.loggers) > 0:\r\n            if not isinstance(self.loggers[0], (TensorBoardLogger, CSVLogger)):\r\n                dirpath = self.loggers[0].save_dir\r\n            else:\r\n                dirpath = self.loggers[0].log_dir\r\n        else:\r\n            dirpath = self.default_root_dir\r\n\r\n        dirpath = self.strategy.broadcast(dirpath)\r\n        return dirpath", "language": "python", "code": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"The directory for the current experiment. Use this to save images to, etc...\r\n\r\n        .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n         .. code-block:: python\r\n\r\n             def training_step(self, batch, batch_idx):\r\n                 img = ...\r\n                 save_img(img, self.trainer.log_dir)\r\n\r\n        \"\"\"\r\n        if len(self.loggers) > 0:\r\n            if not isinstance(self.loggers[0], (TensorBoardLogger, CSVLogger)):\r\n                dirpath = self.loggers[0].save_dir\r\n            else:\r\n                dirpath = self.loggers[0].log_dir\r\n        else:\r\n            dirpath = self.default_root_dir\r\n\r\n        dirpath = self.strategy.broadcast(dirpath)\r\n        return dirpath", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "STRING", "if", "len", "(", "self", ".", "loggers", ")", ">", "0", ":", "if", "not", "isinstance", "(", "self", ".", "loggers", "[", "0", "]", ",", "(", "TensorBoardLogger", ",", "CSVLogger", ")", ")", ":", "dirpath", "=", "self", ".", "loggers", "[", "0", "]", ".", "save_dir", "else", ":", "dirpath", "=", "self", ".", "loggers", "[", "0", "]", ".", "log_dir", "else", ":", "dirpath", "=", "self", ".", "default_root_dir", "dirpath", "=", "self", ".", "strategy", ".", "broadcast", "(", "dirpath", ")", "return", "dirpath"], "docstring": "The directory for the current experiment. Use this to save images to, etc...", "docstring_tokens": ["the", "directory", "for", "the", "current", "experiment", "use", "this", "to", "save", "images", "to", "etc"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1259, "end_line": 1280, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_916", "original_string": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this process is the global zero in multi-node training.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                if self.trainer.is_global_zero:\r\n                    print(\"in node 0, accelerator 0\")\r\n\r\n        \"\"\"\r\n        return self.strategy.is_global_zero", "language": "python", "code": "def is_global_zero(self) -> bool:\r\n        \"\"\"Whether this process is the global zero in multi-node training.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                if self.trainer.is_global_zero:\r\n                    print(\"in node 0, accelerator 0\")\r\n\r\n        \"\"\"\r\n        return self.strategy.is_global_zero", "code_tokens": ["def", "is_global_zero", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "strategy", ".", "is_global_zero"], "docstring": "Whether this process is the global zero in multi-node training.", "docstring_tokens": ["whether", "this", "process", "is", "the", "global", "zero", "in", "multi", "node", "training"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1283, "end_line": 1293, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_917", "original_string": "def enable_validation(self) -> bool:\r\n        \"\"\"Check if we should run validation during training.\"\"\"\r\n        return (\r\n            self.fit_loop.epoch_loop.val_loop._data_source.is_defined()\r\n            and is_overridden(\"validation_step\", self.lightning_module)\r\n            and self.limit_val_batches > 0\r\n        )", "language": "python", "code": "def enable_validation(self) -> bool:\r\n        \"\"\"Check if we should run validation during training.\"\"\"\r\n        return (\r\n            self.fit_loop.epoch_loop.val_loop._data_source.is_defined()\r\n            and is_overridden(\"validation_step\", self.lightning_module)\r\n            and self.limit_val_batches > 0\r\n        )", "code_tokens": ["def", "enable_validation", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "(", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_data_source", ".", "is_defined", "(", ")", "and", "is_overridden", "(", "STRING", ",", "self", ".", "lightning_module", ")", "and", "self", ".", "limit_val_batches", ">", "0", ")"], "docstring": "Check if we should run validation during training.", "docstring_tokens": ["check", "if", "we", "should", "run", "validation", "during", "training"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1302, "end_line": 1308, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_918", "original_string": "def default_root_dir(self) -> str:\r\n        \"\"\"The default location to save artifacts of loggers, checkpoints etc.\r\n\r\n        It is used as a fallback if logger or checkpoint callback do not define specific save paths.\r\n\r\n        \"\"\"\r\n        if _is_local_file_protocol(self._default_root_dir):\r\n            return os.path.normpath(os.path.expanduser(self._default_root_dir))\r\n        return self._default_root_dir", "language": "python", "code": "def default_root_dir(self) -> str:\r\n        \"\"\"The default location to save artifacts of loggers, checkpoints etc.\r\n\r\n        It is used as a fallback if logger or checkpoint callback do not define specific save paths.\r\n\r\n        \"\"\"\r\n        if _is_local_file_protocol(self._default_root_dir):\r\n            return os.path.normpath(os.path.expanduser(self._default_root_dir))\r\n        return self._default_root_dir", "code_tokens": ["def", "default_root_dir", "(", "self", ")", "-", ">", "str", ":", "STRING", "if", "_is_local_file_protocol", "(", "self", ".", "_default_root_dir", ")", ":", "return", "os", ".", "path", ".", "normpath", "(", "os", ".", "path", ".", "expanduser", "(", "self", ".", "_default_root_dir", ")", ")", "return", "self", ".", "_default_root_dir"], "docstring": "The default location to save artifacts of loggers, checkpoints etc.", "docstring_tokens": ["the", "default", "location", "to", "save", "artifacts", "of", "loggers", "checkpoints", "etc"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1311, "end_line": 1319, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_919", "original_string": "def early_stopping_callback(self) -> Optional[EarlyStopping]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.early_stopping_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "language": "python", "code": "def early_stopping_callback(self) -> Optional[EarlyStopping]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.early_stopping_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "code_tokens": ["def", "early_stopping_callback", "(", "self", ")", "-", ">", "Optional", "[", "EarlyStopping", "]", ":", "STRING", "callbacks", "=", "self", ".", "early_stopping_callbacks", "return", "callbacks", "[", "0", "]", "if", "len", "(", "callbacks", ")", ">", "0", "else", "None"], "docstring": "The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the", "docstring_tokens": ["the", "first", "class", "lightning", "pytorch", "callbacks", "early_stopping", "earlystopping", "callback", "in", "the"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1322, "end_line": 1326, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_920", "original_string": "def early_stopping_callbacks(self) -> list[EarlyStopping]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\r\n        Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, EarlyStopping)]", "language": "python", "code": "def early_stopping_callbacks(self) -> list[EarlyStopping]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\r\n        Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, EarlyStopping)]", "code_tokens": ["def", "early_stopping_callbacks", "(", "self", ")", "-", ">", "list", "[", "EarlyStopping", "]", ":", "STRING", "return", "[", "c", "for", "c", "in", "self", ".", "callbacks", "if", "isinstance", "(", "c", ",", "EarlyStopping", ")", "]"], "docstring": "A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the", "docstring_tokens": ["a", "list", "of", "all", "instances", "of", "class", "lightning", "pytorch", "callbacks", "early_stopping", "earlystopping", "found", "in", "the"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1329, "end_line": 1332, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_921", "original_string": "def checkpoint_callback(self) -> Optional[Checkpoint]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.checkpoint_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "language": "python", "code": "def checkpoint_callback(self) -> Optional[Checkpoint]:\r\n        \"\"\"The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\r\n        Trainer.callbacks list, or ``None`` if it doesn't exist.\"\"\"\r\n        callbacks = self.checkpoint_callbacks\r\n        return callbacks[0] if len(callbacks) > 0 else None", "code_tokens": ["def", "checkpoint_callback", "(", "self", ")", "-", ">", "Optional", "[", "Checkpoint", "]", ":", "STRING", "callbacks", "=", "self", ".", "checkpoint_callbacks", "return", "callbacks", "[", "0", "]", "if", "len", "(", "callbacks", ")", ">", "0", "else", "None"], "docstring": "The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the", "docstring_tokens": ["the", "first", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "callback", "in", "the"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1335, "end_line": 1339, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_922", "original_string": "def checkpoint_callbacks(self) -> list[Checkpoint]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\r\n        the Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, Checkpoint)]", "language": "python", "code": "def checkpoint_callbacks(self) -> list[Checkpoint]:\r\n        \"\"\"A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\r\n        the Trainer.callbacks list.\"\"\"\r\n        return [c for c in self.callbacks if isinstance(c, Checkpoint)]", "code_tokens": ["def", "checkpoint_callbacks", "(", "self", ")", "-", ">", "list", "[", "Checkpoint", "]", ":", "STRING", "return", "[", "c", "for", "c", "in", "self", ".", "callbacks", "if", "isinstance", "(", "c", ",", "Checkpoint", ")", "]"], "docstring": "A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in", "docstring_tokens": ["a", "list", "of", "all", "instances", "of", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "found", "in"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1342, "end_line": 1345, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_923", "original_string": "def progress_bar_callback(self) -> Optional[ProgressBar]:\r\n        \"\"\"An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\r\n        Trainer.callbacks list, or ``None`` if one doesn't exist.\"\"\"\r\n        for c in self.callbacks:\r\n            if isinstance(c, ProgressBar):\r\n                return c\r\n        return None", "language": "python", "code": "def progress_bar_callback(self) -> Optional[ProgressBar]:\r\n        \"\"\"An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\r\n        Trainer.callbacks list, or ``None`` if one doesn't exist.\"\"\"\r\n        for c in self.callbacks:\r\n            if isinstance(c, ProgressBar):\r\n                return c\r\n        return None", "code_tokens": ["def", "progress_bar_callback", "(", "self", ")", "-", ">", "Optional", "[", "ProgressBar", "]", ":", "STRING", "for", "c", "in", "self", ".", "callbacks", ":", "if", "isinstance", "(", "c", ",", "ProgressBar", ")", ":", "return", "c", "return", "None"], "docstring": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the", "docstring_tokens": ["an", "instance", "of", "class", "lightning", "pytorch", "callbacks", "progress", "progress_bar", "progressbar", "found", "in", "the"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1348, "end_line": 1354, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_924", "original_string": "def ckpt_path(self) -> Optional[_PATH]:\r\n        \"\"\"Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\r\n\r\n        ``None`` otherwise.\r\n\r\n        \"\"\"\r\n        return self._checkpoint_connector._ckpt_path", "language": "python", "code": "def ckpt_path(self) -> Optional[_PATH]:\r\n        \"\"\"Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\r\n\r\n        ``None`` otherwise.\r\n\r\n        \"\"\"\r\n        return self._checkpoint_connector._ckpt_path", "code_tokens": ["def", "ckpt_path", "(", "self", ")", "-", ">", "Optional", "[", "_PATH", "]", ":", "STRING", "return", "self", ".", "_checkpoint_connector", ".", "_ckpt_path"], "docstring": "Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,", "docstring_tokens": ["set", "to", "the", "path", "url", "of", "a", "checkpoint", "loaded", "via", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "fit"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1357, "end_line": 1366, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_925", "original_string": "def ckpt_path(self, ckpt_path: Optional[_PATH]) -> None:\r\n        \"\"\"Allows you to manage which checkpoint is loaded statefully.\r\n\r\n        .. code-block:: python\r\n\r\n            trainer = Trainer()\r\n            trainer.ckpt_path = \"my/checkpoint/file.ckpt\"\r\n            trainer.fit(model)\r\n            ...\r\n\r\n            trainer.ckpt_path = None\r\n            trainer.test(model)\r\n\r\n        \"\"\"\r\n        self._checkpoint_connector._ckpt_path = ckpt_path\r\n        self._checkpoint_connector._user_managed = bool(ckpt_path)", "language": "python", "code": "def ckpt_path(self, ckpt_path: Optional[_PATH]) -> None:\r\n        \"\"\"Allows you to manage which checkpoint is loaded statefully.\r\n\r\n        .. code-block:: python\r\n\r\n            trainer = Trainer()\r\n            trainer.ckpt_path = \"my/checkpoint/file.ckpt\"\r\n            trainer.fit(model)\r\n            ...\r\n\r\n            trainer.ckpt_path = None\r\n            trainer.test(model)\r\n\r\n        \"\"\"\r\n        self._checkpoint_connector._ckpt_path = ckpt_path\r\n        self._checkpoint_connector._user_managed = bool(ckpt_path)", "code_tokens": ["def", "ckpt_path", "(", "self", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "None", ":", "STRING", "self", ".", "_checkpoint_connector", ".", "_ckpt_path", "=", "ckpt_path", "self", ".", "_checkpoint_connector", ".", "_user_managed", "=", "bool", "(", "ckpt_path", ")"], "docstring": "Allows you to manage which checkpoint is loaded statefully.", "docstring_tokens": ["allows", "you", "to", "manage", "which", "checkpoint", "is", "loaded", "statefully"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1369, "end_line": 1385, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_926", "original_string": "def save_checkpoint(\r\n        self, filepath: _PATH, weights_only: bool = False, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        r\"\"\"Runs routine to create a checkpoint.\r\n\r\n        This method needs to be called on all processes in case the selected strategy is handling distributed\r\n        checkpointing.\r\n\r\n        Args:\r\n            filepath: Path where checkpoint is saved.\r\n            weights_only: If ``True``, will only save the model weights.\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        Raises:\r\n            AttributeError:\r\n                If the model is not attached to the Trainer before calling this method.\r\n\r\n        \"\"\"\r\n        if self.model is None:\r\n            raise AttributeError(\r\n                \"Saving a checkpoint is only possible if a model is attached to the Trainer. Did you call\"\r\n                \" `Trainer.save_checkpoint()` before calling `Trainer.{fit,validate,test,predict}`?\"\r\n            )\r\n        with self.profiler.profile(\"save_checkpoint\"):\r\n            checkpoint = self._checkpoint_connector.dump_checkpoint(weights_only)\r\n            self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\r\n            self.strategy.barrier(\"Trainer.save_checkpoint\")", "language": "python", "code": "def save_checkpoint(\r\n        self, filepath: _PATH, weights_only: bool = False, storage_options: Optional[Any] = None\r\n    ) -> None:\r\n        r\"\"\"Runs routine to create a checkpoint.\r\n\r\n        This method needs to be called on all processes in case the selected strategy is handling distributed\r\n        checkpointing.\r\n\r\n        Args:\r\n            filepath: Path where checkpoint is saved.\r\n            weights_only: If ``True``, will only save the model weights.\r\n            storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\r\n\r\n        Raises:\r\n            AttributeError:\r\n                If the model is not attached to the Trainer before calling this method.\r\n\r\n        \"\"\"\r\n        if self.model is None:\r\n            raise AttributeError(\r\n                \"Saving a checkpoint is only possible if a model is attached to the Trainer. Did you call\"\r\n                \" `Trainer.save_checkpoint()` before calling `Trainer.{fit,validate,test,predict}`?\"\r\n            )\r\n        with self.profiler.profile(\"save_checkpoint\"):\r\n            checkpoint = self._checkpoint_connector.dump_checkpoint(weights_only)\r\n            self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\r\n            self.strategy.barrier(\"Trainer.save_checkpoint\")", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "filepath", ":", "_PATH", ",", "weights_only", ":", "bool", "=", "False", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "rSTRING", "if", "self", ".", "model", "is", "None", ":", "raise", "AttributeError", "(", "STRING", "STRING", ")", "with", "self", ".", "profiler", ".", "profile", "(", "STRING", ")", ":", "checkpoint", "=", "self", ".", "_checkpoint_connector", ".", "dump_checkpoint", "(", "weights_only", ")", "self", ".", "strategy", ".", "save_checkpoint", "(", "checkpoint", ",", "filepath", ",", "storage_options", "=", "storage_options", ")", "self", ".", "strategy", ".", "barrier", "(", "STRING", ")"], "docstring": "r\"\"\"Runs routine to create a checkpoint.", "docstring_tokens": ["r", "runs", "routine", "to", "create", "a", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1387, "end_line": 1413, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_927", "original_string": "def sanity_checking(self) -> bool:\r\n        \"\"\"Whether sanity checking is running.\r\n\r\n        Useful to disable some hooks, logging or callbacks during the sanity checking.\r\n\r\n        \"\"\"\r\n        return self.state.stage == RunningStage.SANITY_CHECKING", "language": "python", "code": "def sanity_checking(self) -> bool:\r\n        \"\"\"Whether sanity checking is running.\r\n\r\n        Useful to disable some hooks, logging or callbacks during the sanity checking.\r\n\r\n        \"\"\"\r\n        return self.state.stage == RunningStage.SANITY_CHECKING", "code_tokens": ["def", "sanity_checking", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "state", ".", "stage", "=", "=", "RunningStage", ".", "SANITY_CHECKING"], "docstring": "Whether sanity checking is running.", "docstring_tokens": ["whether", "sanity", "checking", "is", "running"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1472, "end_line": 1478, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_928", "original_string": "def received_sigterm(self) -> bool:\r\n        \"\"\"Whether a ``signal.SIGTERM`` signal was received.\r\n\r\n        For example, this can be checked to exit gracefully.\r\n\r\n        \"\"\"\r\n        return self._signal_connector.received_sigterm", "language": "python", "code": "def received_sigterm(self) -> bool:\r\n        \"\"\"Whether a ``signal.SIGTERM`` signal was received.\r\n\r\n        For example, this can be checked to exit gracefully.\r\n\r\n        \"\"\"\r\n        return self._signal_connector.received_sigterm", "code_tokens": ["def", "received_sigterm", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_signal_connector", ".", "received_sigterm"], "docstring": "Whether a ``signal.SIGTERM`` signal was received.", "docstring_tokens": ["whether", "a", "signal", "sigterm", "signal", "was", "received"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1488, "end_line": 1494, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_929", "original_string": "def global_step(self) -> int:\r\n        \"\"\"The number of optimizer steps taken (does not reset each epoch).\r\n\r\n        This includes multiple optimizers (if enabled).\r\n\r\n        \"\"\"\r\n        return self.fit_loop.epoch_loop.global_step", "language": "python", "code": "def global_step(self) -> int:\r\n        \"\"\"The number of optimizer steps taken (does not reset each epoch).\r\n\r\n        This includes multiple optimizers (if enabled).\r\n\r\n        \"\"\"\r\n        return self.fit_loop.epoch_loop.global_step", "code_tokens": ["def", "global_step", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "fit_loop", ".", "epoch_loop", ".", "global_step"], "docstring": "The number of optimizer steps taken (does not reset each epoch).", "docstring_tokens": ["the", "number", "of", "optimizer", "steps", "taken", "does", "not", "reset", "each", "epoch"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1501, "end_line": 1507, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_930", "original_string": "def current_epoch(self) -> int:\r\n        \"\"\"The current epoch, updated after the epoch end hooks are run.\"\"\"\r\n        return self.fit_loop.epoch_progress.current.completed", "language": "python", "code": "def current_epoch(self) -> int:\r\n        \"\"\"The current epoch, updated after the epoch end hooks are run.\"\"\"\r\n        return self.fit_loop.epoch_progress.current.completed", "code_tokens": ["def", "current_epoch", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "self", ".", "fit_loop", ".", "epoch_progress", ".", "current", ".", "completed"], "docstring": "The current epoch, updated after the epoch end hooks are run.", "docstring_tokens": ["the", "current", "epoch", "updated", "after", "the", "epoch", "end", "hooks", "are", "run"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1510, "end_line": 1512, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_931", "original_string": "def is_last_batch(self) -> bool:\r\n        \"\"\"Whether trainer is executing the last batch.\"\"\"\r\n        return self.fit_loop.epoch_loop.batch_progress.is_last_batch", "language": "python", "code": "def is_last_batch(self) -> bool:\r\n        \"\"\"Whether trainer is executing the last batch.\"\"\"\r\n        return self.fit_loop.epoch_loop.batch_progress.is_last_batch", "code_tokens": ["def", "is_last_batch", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "fit_loop", ".", "epoch_loop", ".", "batch_progress", ".", "is_last_batch"], "docstring": "Whether trainer is executing the last batch.", "docstring_tokens": ["whether", "trainer", "is", "executing", "the", "last", "batch"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1531, "end_line": 1533, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_932", "original_string": "def train_dataloader(self) -> Optional[TRAIN_DATALOADERS]:\r\n        \"\"\"The training dataloader(s) used during ``trainer.fit()``.\"\"\"\r\n        if (combined_loader := self.fit_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def train_dataloader(self) -> Optional[TRAIN_DATALOADERS]:\r\n        \"\"\"The training dataloader(s) used during ``trainer.fit()``.\"\"\"\r\n        if (combined_loader := self.fit_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "Optional", "[", "TRAIN_DATALOADERS", "]", ":", "STRING", "if", "(", "combined_loader", ":", "=", "self", ".", "fit_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The training dataloader(s) used during ``trainer.fit()``.", "docstring_tokens": ["the", "training", "dataloader", "s", "used", "during", "trainer", "fit"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1536, "end_line": 1540, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_933", "original_string": "def val_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if (combined_loader := self.fit_loop.epoch_loop.val_loop._combined_loader) is not None or (\r\n            combined_loader := self.validate_loop._combined_loader\r\n        ) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def val_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if (combined_loader := self.fit_loop.epoch_loop.val_loop._combined_loader) is not None or (\r\n            combined_loader := self.validate_loop._combined_loader\r\n        ) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "val_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "STRING", "if", "(", "combined_loader", ":", "=", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_combined_loader", ")", "is", "not", "None", "or", "(", "combined_loader", ":", "=", "self", ".", "validate_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.", "docstring_tokens": ["the", "validation", "dataloader", "s", "used", "during", "trainer", "fit", "or", "trainer", "validate"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1543, "end_line": 1549, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_934", "original_string": "def test_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The test dataloader(s) used during ``trainer.test()``.\"\"\"\r\n        if (combined_loader := self.test_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def test_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The test dataloader(s) used during ``trainer.test()``.\"\"\"\r\n        if (combined_loader := self.test_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "test_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "STRING", "if", "(", "combined_loader", ":", "=", "self", ".", "test_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The test dataloader(s) used during ``trainer.test()``.", "docstring_tokens": ["the", "test", "dataloader", "s", "used", "during", "trainer", "test"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1552, "end_line": 1556, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_935", "original_string": "def predict_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The prediction dataloader(s) used during ``trainer.predict()``.\"\"\"\r\n        if (combined_loader := self.predict_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "language": "python", "code": "def predict_dataloaders(self) -> Optional[EVAL_DATALOADERS]:\r\n        \"\"\"The prediction dataloader(s) used during ``trainer.predict()``.\"\"\"\r\n        if (combined_loader := self.predict_loop._combined_loader) is not None:\r\n            return combined_loader.iterables\r\n        return None", "code_tokens": ["def", "predict_dataloaders", "(", "self", ")", "-", ">", "Optional", "[", "EVAL_DATALOADERS", "]", ":", "STRING", "if", "(", "combined_loader", ":", "=", "self", ".", "predict_loop", ".", "_combined_loader", ")", "is", "not", "None", ":", "return", "combined_loader", ".", "iterables", "return", "None"], "docstring": "The prediction dataloader(s) used during ``trainer.predict()``.", "docstring_tokens": ["the", "prediction", "dataloader", "s", "used", "during", "trainer", "predict"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1559, "end_line": 1563, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_936", "original_string": "def num_training_batches(self) -> Union[int, float]:\r\n        \"\"\"The number of training batches that will be used during ``trainer.fit()``.\"\"\"\r\n        return self.fit_loop.max_batches", "language": "python", "code": "def num_training_batches(self) -> Union[int, float]:\r\n        \"\"\"The number of training batches that will be used during ``trainer.fit()``.\"\"\"\r\n        return self.fit_loop.max_batches", "code_tokens": ["def", "num_training_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "STRING", "return", "self", ".", "fit_loop", ".", "max_batches"], "docstring": "The number of training batches that will be used during ``trainer.fit()``.", "docstring_tokens": ["the", "number", "of", "training", "batches", "that", "will", "be", "used", "during", "trainer", "fit"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1566, "end_line": 1568, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_937", "original_string": "def num_sanity_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\"\"\"\r\n        max_batches = self.fit_loop.epoch_loop.val_loop.max_batches\r\n        return [min(self.num_sanity_val_steps, batches) for batches in max_batches]", "language": "python", "code": "def num_sanity_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\"\"\"\r\n        max_batches = self.fit_loop.epoch_loop.val_loop.max_batches\r\n        return [min(self.num_sanity_val_steps, batches) for batches in max_batches]", "code_tokens": ["def", "num_sanity_val_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "max_batches", "=", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "max_batches", "return", "[", "min", "(", "self", ".", "num_sanity_val_steps", ",", "batches", ")", "for", "batches", "in", "max_batches", "]"], "docstring": "The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.", "docstring_tokens": ["the", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "the", "sanity", "checking", "part", "of", "trainer", "fit"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1571, "end_line": 1575, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_938", "original_string": "def num_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if self.state.fn == TrainerFn.VALIDATING:\r\n            return self.validate_loop.max_batches\r\n        return self.fit_loop.epoch_loop.val_loop._max_batches", "language": "python", "code": "def num_val_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\"\"\"\r\n        if self.state.fn == TrainerFn.VALIDATING:\r\n            return self.validate_loop.max_batches\r\n        return self.fit_loop.epoch_loop.val_loop._max_batches", "code_tokens": ["def", "num_val_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "if", "self", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "return", "self", ".", "validate_loop", ".", "max_batches", "return", "self", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_max_batches"], "docstring": "The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.", "docstring_tokens": ["the", "number", "of", "validation", "batches", "that", "will", "be", "used", "during", "trainer", "fit", "or", "trainer", "validate"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1578, "end_line": 1584, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_939", "original_string": "def num_test_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of test batches that will be used during ``trainer.test()``.\"\"\"\r\n        return self.test_loop.max_batches", "language": "python", "code": "def num_test_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of test batches that will be used during ``trainer.test()``.\"\"\"\r\n        return self.test_loop.max_batches", "code_tokens": ["def", "num_test_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "return", "self", ".", "test_loop", ".", "max_batches"], "docstring": "The number of test batches that will be used during ``trainer.test()``.", "docstring_tokens": ["the", "number", "of", "test", "batches", "that", "will", "be", "used", "during", "trainer", "test"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1587, "end_line": 1589, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_940", "original_string": "def num_predict_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of prediction batches that will be used during ``trainer.predict()``.\"\"\"\r\n        return self.predict_loop.max_batches", "language": "python", "code": "def num_predict_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The number of prediction batches that will be used during ``trainer.predict()``.\"\"\"\r\n        return self.predict_loop.max_batches", "code_tokens": ["def", "num_predict_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "STRING", "return", "self", ".", "predict_loop", ".", "max_batches"], "docstring": "The number of prediction batches that will be used during ``trainer.predict()``.", "docstring_tokens": ["the", "number", "of", "prediction", "batches", "that", "will", "be", "used", "during", "trainer", "predict"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1592, "end_line": 1594, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_941", "original_string": "def logger(self) -> Optional[Logger]:\r\n        \"\"\"The first :class:`~lightning.pytorch.loggers.logger.Logger` being used.\"\"\"\r\n        return self.loggers[0] if len(self.loggers) > 0 else None", "language": "python", "code": "def logger(self) -> Optional[Logger]:\r\n        \"\"\"The first :class:`~lightning.pytorch.loggers.logger.Logger` being used.\"\"\"\r\n        return self.loggers[0] if len(self.loggers) > 0 else None", "code_tokens": ["def", "logger", "(", "self", ")", "-", ">", "Optional", "[", "Logger", "]", ":", "STRING", "return", "self", ".", "loggers", "[", "0", "]", "if", "len", "(", "self", ".", "loggers", ")", ">", "0", "else", "None"], "docstring": "The first :class:`~lightning.pytorch.loggers.logger.Logger` being used.", "docstring_tokens": ["the", "first", "class", "lightning", "pytorch", "loggers", "logger", "logger", "being", "used"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1621, "end_line": 1623, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_942", "original_string": "def loggers(self) -> list[Logger]:\r\n        \"\"\"The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\r\n\r\n        .. code-block:: python\r\n\r\n            for logger in trainer.loggers:\r\n                logger.log_metrics({\"foo\": 1.0})\r\n\r\n        \"\"\"\r\n        return self._loggers", "language": "python", "code": "def loggers(self) -> list[Logger]:\r\n        \"\"\"The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\r\n\r\n        .. code-block:: python\r\n\r\n            for logger in trainer.loggers:\r\n                logger.log_metrics({\"foo\": 1.0})\r\n\r\n        \"\"\"\r\n        return self._loggers", "code_tokens": ["def", "loggers", "(", "self", ")", "-", ">", "list", "[", "Logger", "]", ":", "STRING", "return", "self", ".", "_loggers"], "docstring": "The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.", "docstring_tokens": ["the", "list", "of", "class", "lightning", "pytorch", "loggers", "logger", "logger", "used"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1633, "end_line": 1642, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_943", "original_string": "def callback_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics available to callbacks.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                self.log(\"a_val\", 2.0)\r\n\r\n\r\n            callback_metrics = trainer.callback_metrics\r\n            assert callback_metrics[\"a_val\"] == 2.0\r\n\r\n        \"\"\"\r\n        return self._logger_connector.callback_metrics", "language": "python", "code": "def callback_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics available to callbacks.\r\n\r\n        .. code-block:: python\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                self.log(\"a_val\", 2.0)\r\n\r\n\r\n            callback_metrics = trainer.callback_metrics\r\n            assert callback_metrics[\"a_val\"] == 2.0\r\n\r\n        \"\"\"\r\n        return self._logger_connector.callback_metrics", "code_tokens": ["def", "callback_metrics", "(", "self", ")", "-", ">", "_OUT_DICT", ":", "STRING", "return", "self", ".", "_logger_connector", ".", "callback_metrics"], "docstring": "The metrics available to callbacks.", "docstring_tokens": ["the", "metrics", "available", "to", "callbacks"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1649, "end_line": 1662, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_944", "original_string": "def logged_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics sent to the loggers.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.logged_metrics", "language": "python", "code": "def logged_metrics(self) -> _OUT_DICT:\r\n        \"\"\"The metrics sent to the loggers.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.logged_metrics", "code_tokens": ["def", "logged_metrics", "(", "self", ")", "-", ">", "_OUT_DICT", ":", "STRING", "return", "self", ".", "_logger_connector", ".", "logged_metrics"], "docstring": "The metrics sent to the loggers.", "docstring_tokens": ["the", "metrics", "sent", "to", "the", "loggers"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1665, "end_line": 1672, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_945", "original_string": "def progress_bar_metrics(self) -> _PBAR_DICT:\r\n        \"\"\"The metrics sent to the progress bar.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.progress_bar_metrics", "language": "python", "code": "def progress_bar_metrics(self) -> _PBAR_DICT:\r\n        \"\"\"The metrics sent to the progress bar.\r\n\r\n        This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\r\n        :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\r\n\r\n        \"\"\"\r\n        return self._logger_connector.progress_bar_metrics", "code_tokens": ["def", "progress_bar_metrics", "(", "self", ")", "-", ">", "_PBAR_DICT", ":", "STRING", "return", "self", ".", "_logger_connector", ".", "progress_bar_metrics"], "docstring": "The metrics sent to the progress bar.", "docstring_tokens": ["the", "metrics", "sent", "to", "the", "progress", "bar"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1675, "end_line": 1682, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\trainer.py", "func_name": "function_946", "original_string": "def estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps", "language": "python", "code": "def estimated_stepping_batches(self) -> Union[int, float]:\r\n        r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.\r\n\r\n        This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\r\n        up your training dataloader, if it hasn't been set up already.\r\n\r\n        .. code-block:: python\r\n\r\n            def configure_optimizers(self):\r\n                optimizer = ...\r\n                stepping_batches = self.trainer.estimated_stepping_batches\r\n                scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\r\n                return [optimizer], [scheduler]\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\r\n                at different epochs.\r\n\r\n        \"\"\"\r\n        if self.max_epochs == -1:\r\n            return float(\"inf\") if self.max_steps == -1 else self.max_steps\r\n\r\n        if self.train_dataloader is None:\r\n            rank_zero_info(\"Loading `train_dataloader` to estimate number of stepping batches.\")\r\n            self.fit_loop.setup_data()\r\n\r\n        total_batches = self.num_training_batches\r\n\r\n        if total_batches == float(\"inf\"):\r\n            return self.max_steps\r\n\r\n        assert self.max_epochs is not None\r\n        max_estimated_steps = math.ceil(total_batches / self.accumulate_grad_batches) * max(self.max_epochs, 1)\r\n\r\n        max_estimated_steps = min(max_estimated_steps, self.max_steps) if self.max_steps != -1 else max_estimated_steps\r\n        return max_estimated_steps", "code_tokens": ["def", "estimated_stepping_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "rSTRING", "if", "self", ".", "max_epochs", "=", "=", "-", "1", ":", "return", "float", "(", "STRING", ")", "if", "self", ".", "max_steps", "=", "=", "-", "1", "else", "self", ".", "max_steps", "if", "self", ".", "train_dataloader", "is", "None", ":", "rank_zero_info", "(", "STRING", ")", "self", ".", "fit_loop", ".", "setup_data", "(", ")", "total_batches", "=", "self", ".", "num_training_batches", "if", "total_batches", "=", "=", "float", "(", "STRING", ")", ":", "return", "self", ".", "max_steps", "assert", "self", ".", "max_epochs", "is", "not", "None", "max_estimated_steps", "=", "math", ".", "ceil", "(", "total_batches", "/", "self", ".", "accumulate_grad_batches", ")", "*", "max", "(", "self", ".", "max_epochs", ",", "1", ")", "max_estimated_steps", "=", "min", "(", "max_estimated_steps", ",", "self", ".", "max_steps", ")", "if", "self", ".", "max_steps", "!", "=", "-", "1", "else", "max_estimated_steps", "return", "max_estimated_steps"], "docstring": "r\"\"\"The estimated number of batches that will ``optimizer.step()`` during training.", "docstring_tokens": ["r", "the", "estimated", "number", "of", "batches", "that", "will", "optimizer", "step", "during", "training"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\trainer.py", "start_line": 1696, "end_line": 1734, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_947", "original_string": "def __init__(\r\n        self,\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]] = None,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        sync_batchnorm: bool = False,\r\n        benchmark: Optional[bool] = None,\r\n        use_distributed_sampler: bool = True,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n    ) -> None:\r\n        \"\"\"The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other\r\n        components such as the Accelerator and Precision plugins.\r\n\r\n            A. accelerator flag could be:\r\n                1. accelerator class\r\n                2. accelerator str\r\n                3. accelerator auto\r\n\r\n            B. strategy flag could be:\r\n                1. strategy class\r\n                2. strategy str registered with StrategyRegistry\r\n\r\n            C. plugins flag could be:\r\n                1. precision class (should be removed, and precision flag should allow user pass classes)\r\n                2. checkpoint_io class\r\n                3. cluster_environment class\r\n\r\n        priorities which to take when:\r\n            A. Class > str\r\n            B. Strategy > Accelerator/precision/plugins\r\n\r\n        \"\"\"\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        _set_torch_flags(deterministic=deterministic, benchmark=benchmark)\r\n\r\n        _register_external_accelerators_and_strategies()\r\n        self._registered_strategies = StrategyRegistry.available_strategies()\r\n        self._accelerator_types = AcceleratorRegistry.available_accelerators()\r\n\r\n        self._strategy_flag: Union[Strategy, str] = \"auto\"\r\n        self._accelerator_flag: Union[Accelerator, str] = \"auto\"\r\n        self._precision_flag: _PRECISION_INPUT_STR = \"32-true\"\r\n        self._precision_plugin_flag: Optional[Precision] = None\r\n        self._cluster_environment_flag: Optional[Union[ClusterEnvironment, str]] = None\r\n        self._parallel_devices: list[Union[int, torch.device, str]] = []\r\n        self._layer_sync: Optional[LayerSync] = TorchSyncBatchNorm() if sync_batchnorm else None\r\n        self.checkpoint_io: Optional[CheckpointIO] = None\r\n\r\n        self._check_config_and_set_final_flags(\r\n            strategy=strategy,\r\n            accelerator=accelerator,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            sync_batchnorm=sync_batchnorm,\r\n        )\r\n\r\n        if self._accelerator_flag == \"auto\":\r\n            self._accelerator_flag = self._choose_auto_accelerator()\r\n        elif self._accelerator_flag == \"gpu\":\r\n            self._accelerator_flag = self._choose_gpu_accelerator_backend()\r\n\r\n        self._check_device_config_and_set_final_flags(devices=devices, num_nodes=num_nodes)\r\n        self._set_parallel_devices_and_init_accelerator()\r\n\r\n        self.cluster_environment: ClusterEnvironment = self._choose_and_init_cluster_environment()\r\n\r\n        if self._strategy_flag == \"auto\":\r\n            self._strategy_flag = self._choose_strategy()\r\n        self._check_strategy_and_fallback()\r\n        self._init_strategy()\r\n\r\n        self.precision_plugin = self._check_and_init_precision()\r\n\r\n        self._lazy_init_strategy()", "language": "python", "code": "def __init__(\r\n        self,\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        num_nodes: int = 1,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]] = None,\r\n        precision: Optional[_PRECISION_INPUT] = None,\r\n        sync_batchnorm: bool = False,\r\n        benchmark: Optional[bool] = None,\r\n        use_distributed_sampler: bool = True,\r\n        deterministic: Optional[Union[bool, _LITERAL_WARN]] = None,\r\n    ) -> None:\r\n        \"\"\"The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other\r\n        components such as the Accelerator and Precision plugins.\r\n\r\n            A. accelerator flag could be:\r\n                1. accelerator class\r\n                2. accelerator str\r\n                3. accelerator auto\r\n\r\n            B. strategy flag could be:\r\n                1. strategy class\r\n                2. strategy str registered with StrategyRegistry\r\n\r\n            C. plugins flag could be:\r\n                1. precision class (should be removed, and precision flag should allow user pass classes)\r\n                2. checkpoint_io class\r\n                3. cluster_environment class\r\n\r\n        priorities which to take when:\r\n            A. Class > str\r\n            B. Strategy > Accelerator/precision/plugins\r\n\r\n        \"\"\"\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        _set_torch_flags(deterministic=deterministic, benchmark=benchmark)\r\n\r\n        _register_external_accelerators_and_strategies()\r\n        self._registered_strategies = StrategyRegistry.available_strategies()\r\n        self._accelerator_types = AcceleratorRegistry.available_accelerators()\r\n\r\n        self._strategy_flag: Union[Strategy, str] = \"auto\"\r\n        self._accelerator_flag: Union[Accelerator, str] = \"auto\"\r\n        self._precision_flag: _PRECISION_INPUT_STR = \"32-true\"\r\n        self._precision_plugin_flag: Optional[Precision] = None\r\n        self._cluster_environment_flag: Optional[Union[ClusterEnvironment, str]] = None\r\n        self._parallel_devices: list[Union[int, torch.device, str]] = []\r\n        self._layer_sync: Optional[LayerSync] = TorchSyncBatchNorm() if sync_batchnorm else None\r\n        self.checkpoint_io: Optional[CheckpointIO] = None\r\n\r\n        self._check_config_and_set_final_flags(\r\n            strategy=strategy,\r\n            accelerator=accelerator,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            sync_batchnorm=sync_batchnorm,\r\n        )\r\n\r\n        if self._accelerator_flag == \"auto\":\r\n            self._accelerator_flag = self._choose_auto_accelerator()\r\n        elif self._accelerator_flag == \"gpu\":\r\n            self._accelerator_flag = self._choose_gpu_accelerator_backend()\r\n\r\n        self._check_device_config_and_set_final_flags(devices=devices, num_nodes=num_nodes)\r\n        self._set_parallel_devices_and_init_accelerator()\r\n\r\n        self.cluster_environment: ClusterEnvironment = self._choose_and_init_cluster_environment()\r\n\r\n        if self._strategy_flag == \"auto\":\r\n            self._strategy_flag = self._choose_strategy()\r\n        self._check_strategy_and_fallback()\r\n        self._init_strategy()\r\n\r\n        self.precision_plugin = self._check_and_init_precision()\r\n\r\n        self._lazy_init_strategy()", "code_tokens": ["def", "__init__", "(", "self", ",", "devices", ":", "Union", "[", "list", "[", "int", "]", ",", "str", ",", "int", "]", "=", "STRING", ",", "num_nodes", ":", "int", "=", "1", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", "=", "STRING", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", "=", "STRING", ",", "plugins", ":", "Optional", "[", "Union", "[", "_PLUGIN_INPUT", ",", "Iterable", "[", "_PLUGIN_INPUT", "]", "]", "]", "=", "None", ",", "precision", ":", "Optional", "[", "_PRECISION_INPUT", "]", "=", "None", ",", "sync_batchnorm", ":", "bool", "=", "False", ",", "benchmark", ":", "Optional", "[", "bool", "]", "=", "None", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "deterministic", ":", "Optional", "[", "Union", "[", "bool", ",", "_LITERAL_WARN", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "self", ".", "use_distributed_sampler", "=", "use_distributed_sampler", "_set_torch_flags", "(", "deterministic", "=", "deterministic", ",", "benchmark", "=", "benchmark", ")", "_register_external_accelerators_and_strategies", "(", ")", "self", ".", "_registered_strategies", "=", "StrategyRegistry", ".", "available_strategies", "(", ")", "self", ".", "_accelerator_types", "=", "AcceleratorRegistry", ".", "available_accelerators", "(", ")", "self", ".", "_strategy_flag", ":", "Union", "[", "Strategy", ",", "str", "]", "=", "STRING", "self", ".", "_accelerator_flag", ":", "Union", "[", "Accelerator", ",", "str", "]", "=", "STRING", "self", ".", "_precision_flag", ":", "_PRECISION_INPUT_STR", "=", "STRING", "self", ".", "_precision_plugin_flag", ":", "Optional", "[", "Precision", "]", "=", "None", "self", ".", "_cluster_environment_flag", ":", "Optional", "[", "Union", "[", "ClusterEnvironment", ",", "str", "]", "]", "=", "None", "self", ".", "_parallel_devices", ":", "list", "[", "Union", "[", "int", ",", "torch", ".", "device", ",", "str", "]", "]", "=", "[", "]", "self", ".", "_layer_sync", ":", "Optional", "[", "LayerSync", "]", "=", "TorchSyncBatchNorm", "(", ")", "if", "sync_batchnorm", "else", "None", "self", ".", "checkpoint_io", ":", "Optional", "[", "CheckpointIO", "]", "=", "None", "self", ".", "_check_config_and_set_final_flags", "(", "strategy", "=", "strategy", ",", "accelerator", "=", "accelerator", ",", "precision", "=", "precision", ",", "plugins", "=", "plugins", ",", "sync_batchnorm", "=", "sync_batchnorm", ",", ")", "if", "self", ".", "_accelerator_flag", "=", "=", "STRING", ":", "self", ".", "_accelerator_flag", "=", "self", ".", "_choose_auto_accelerator", "(", ")", "elif", "self", ".", "_accelerator_flag", "=", "=", "STRING", ":", "self", ".", "_accelerator_flag", "=", "self", ".", "_choose_gpu_accelerator_backend", "(", ")", "self", ".", "_check_device_config_and_set_final_flags", "(", "devices", "=", "devices", ",", "num_nodes", "=", "num_nodes", ")", "self", ".", "_set_parallel_devices_and_init_accelerator", "(", ")", "self", ".", "cluster_environment", ":", "ClusterEnvironment", "=", "self", ".", "_choose_and_init_cluster_environment", "(", ")", "if", "self", ".", "_strategy_flag", "=", "=", "STRING", ":", "self", ".", "_strategy_flag", "=", "self", ".", "_choose_strategy", "(", ")", "self", ".", "_check_strategy_and_fallback", "(", ")", "self", ".", "_init_strategy", "(", ")", "self", ".", "precision_plugin", "=", "self", ".", "_check_and_init_precision", "(", ")", "self", ".", "_lazy_init_strategy", "(", ")"], "docstring": "The AcceleratorConnector parses several Trainer arguments and instantiates the Strategy including other", "docstring_tokens": ["the", "acceleratorconnector", "parses", "several", "trainer", "arguments", "and", "instantiates", "the", "strategy", "including", "other"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 75, "end_line": 162, "has_examples": false, "num_comments": 8, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_948", "original_string": "def _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n        sync_batchnorm: bool,\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` or `sync_batchnorm` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.pytorch.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._accelerator_types\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._accelerator_types)}.\"\r\n            )\r\n\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_flag = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_plugin_flag = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                elif isinstance(plugin, LayerSync):\r\n                    if sync_batchnorm and not isinstance(plugin, TorchSyncBatchNorm):\r\n                        raise MisconfigurationException(\r\n                            f\"You set `Trainer(sync_batchnorm=True)` and provided a `{plugin.__class__.__name__}`\"\r\n                            \" plugin, but this is not allowed. Choose one or the other.\"\r\n                        )\r\n                    self._layer_sync = plugin\r\n                    plugins_flags_types[TorchSyncBatchNorm.__name__] += 1\r\n                else:\r\n                    raise MisconfigurationException(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment, or LayerSync.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise MisconfigurationException(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_flag is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_flag}` and `plugins={self._precision_plugin_flag}`.\"\r\n                    f\" Choose one.\"\r\n                )\r\n\r\n        self._precision_flag = \"32-true\" if precision_flag is None else precision_flag\r\n\r\n        if self._strategy_flag and isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise MisconfigurationException(\r\n                        \"accelerator set through both strategy class and accelerator flag, choose one\"\r\n                    )\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision_plugin:\r\n                if self._precision_plugin_flag:\r\n                    raise MisconfigurationException(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_plugin_flag = self._strategy_flag._precision_plugin\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise MisconfigurationException(\r\n                        \"checkpoint_io set through both strategy class and plugins, choose one\"\r\n                    )\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise MisconfigurationException(\r\n                        \"cluster_environment set through both strategy class and plugins, choose one\"\r\n                    )\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise MisconfigurationException(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise MisconfigurationException(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices", "language": "python", "code": "def _check_config_and_set_final_flags(\r\n        self,\r\n        strategy: Union[str, Strategy],\r\n        accelerator: Union[str, Accelerator],\r\n        precision: Optional[_PRECISION_INPUT],\r\n        plugins: Optional[Union[_PLUGIN_INPUT, Iterable[_PLUGIN_INPUT]]],\r\n        sync_batchnorm: bool,\r\n    ) -> None:\r\n        \"\"\"This method checks:\r\n\r\n        1. strategy: whether the strategy name is valid, and sets the internal flags if it is.\r\n        2. accelerator: if the value of the accelerator argument is a type of accelerator (instance or string),\r\n            set self._accelerator_flag accordingly.\r\n        3. precision: The final value of the precision flag may be determined either by the precision argument or\r\n            by a plugin instance.\r\n        4. plugins: The list of plugins may contain a Precision plugin, CheckpointIO, ClusterEnvironment and others.\r\n            Additionally, other flags such as `precision` or `sync_batchnorm` can populate the list with the\r\n            corresponding plugin instances.\r\n\r\n        \"\"\"\r\n        if plugins is not None:\r\n            plugins = [plugins] if not isinstance(plugins, Iterable) else plugins\r\n\r\n        if isinstance(strategy, str):\r\n            strategy = strategy.lower()\r\n\r\n        self._strategy_flag = strategy\r\n\r\n        if strategy != \"auto\" and strategy not in self._registered_strategies and not isinstance(strategy, Strategy):\r\n            raise ValueError(\r\n                f\"You selected an invalid strategy name: `strategy={strategy!r}`.\"\r\n                \" It must be either a string or an instance of `lightning.pytorch.strategies.Strategy`.\"\r\n                \" Example choices: auto, ddp, ddp_spawn, deepspeed, ...\"\r\n                \" Find a complete list of options in our documentation at https://lightning.ai\"\r\n            )\r\n\r\n        if (\r\n            accelerator not in self._accelerator_types\r\n            and accelerator not in (\"auto\", \"gpu\")\r\n            and not isinstance(accelerator, Accelerator)\r\n        ):\r\n            raise ValueError(\r\n                f\"You selected an invalid accelerator name: `accelerator={accelerator!r}`.\"\r\n                f\" Available names are: auto, {', '.join(self._accelerator_types)}.\"\r\n            )\r\n\r\n        is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n        is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n        is_parallel_strategy = isinstance(strategy, ParallelStrategy) or is_ddp_str or is_deepspeed_str\r\n        is_mps_accelerator = MPSAccelerator.is_available() and (\r\n            accelerator in (\"mps\", \"auto\", \"gpu\", None) or isinstance(accelerator, MPSAccelerator)\r\n        )\r\n        if is_mps_accelerator and is_parallel_strategy:\r\n            raise ValueError(\r\n                f\"You set `strategy={strategy}` but strategies from the DDP family are not supported on the\"\r\n                f\" MPS accelerator. Either explicitly set `accelerator='cpu'` or change the strategy.\"\r\n            )\r\n\r\n        self._accelerator_flag = accelerator\r\n\r\n        precision_flag = _convert_precision_to_unified_args(precision)\r\n\r\n        if plugins:\r\n            plugins_flags_types: dict[str, int] = Counter()\r\n            for plugin in plugins:\r\n                if isinstance(plugin, Precision):\r\n                    self._precision_plugin_flag = plugin\r\n                    plugins_flags_types[Precision.__name__] += 1\r\n                elif isinstance(plugin, CheckpointIO):\r\n                    self.checkpoint_io = plugin\r\n                    plugins_flags_types[CheckpointIO.__name__] += 1\r\n                elif isinstance(plugin, ClusterEnvironment):\r\n                    self._cluster_environment_flag = plugin\r\n                    plugins_flags_types[ClusterEnvironment.__name__] += 1\r\n                elif isinstance(plugin, LayerSync):\r\n                    if sync_batchnorm and not isinstance(plugin, TorchSyncBatchNorm):\r\n                        raise MisconfigurationException(\r\n                            f\"You set `Trainer(sync_batchnorm=True)` and provided a `{plugin.__class__.__name__}`\"\r\n                            \" plugin, but this is not allowed. Choose one or the other.\"\r\n                        )\r\n                    self._layer_sync = plugin\r\n                    plugins_flags_types[TorchSyncBatchNorm.__name__] += 1\r\n                else:\r\n                    raise MisconfigurationException(\r\n                        f\"Found invalid type for plugin {plugin}. Expected one of: Precision, \"\r\n                        \"CheckpointIO, ClusterEnvironment, or LayerSync.\"\r\n                    )\r\n\r\n            duplicated_plugin_key = [k for k, v in plugins_flags_types.items() if v > 1]\r\n            if duplicated_plugin_key:\r\n                raise MisconfigurationException(\r\n                    f\"Received multiple values for {', '.join(duplicated_plugin_key)} flags in `plugins`.\"\r\n                    \" Expected one value for each type at most.\"\r\n                )\r\n\r\n            if plugins_flags_types.get(Precision.__name__) and precision_flag is not None:\r\n                raise ValueError(\r\n                    f\"Received both `precision={precision_flag}` and `plugins={self._precision_plugin_flag}`.\"\r\n                    f\" Choose one.\"\r\n                )\r\n\r\n        self._precision_flag = \"32-true\" if precision_flag is None else precision_flag\r\n\r\n        if self._strategy_flag and isinstance(self._strategy_flag, Strategy):\r\n            if self._strategy_flag._accelerator:\r\n                if self._accelerator_flag != \"auto\":\r\n                    raise MisconfigurationException(\r\n                        \"accelerator set through both strategy class and accelerator flag, choose one\"\r\n                    )\r\n                self._accelerator_flag = self._strategy_flag._accelerator\r\n            if self._strategy_flag._precision_plugin:\r\n                if self._precision_plugin_flag:\r\n                    raise MisconfigurationException(\"precision set through both strategy class and plugins, choose one\")\r\n                self._precision_plugin_flag = self._strategy_flag._precision_plugin\r\n            if self._strategy_flag._checkpoint_io:\r\n                if self.checkpoint_io:\r\n                    raise MisconfigurationException(\r\n                        \"checkpoint_io set through both strategy class and plugins, choose one\"\r\n                    )\r\n                self.checkpoint_io = self._strategy_flag._checkpoint_io\r\n            if getattr(self._strategy_flag, \"cluster_environment\", None):\r\n                if self._cluster_environment_flag:\r\n                    raise MisconfigurationException(\r\n                        \"cluster_environment set through both strategy class and plugins, choose one\"\r\n                    )\r\n                self._cluster_environment_flag = getattr(self._strategy_flag, \"cluster_environment\")\r\n\r\n            if hasattr(self._strategy_flag, \"parallel_devices\") and self._strategy_flag.parallel_devices:\r\n                if self._strategy_flag.parallel_devices[0].type == \"cpu\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cpu\"):\r\n                        raise MisconfigurationException(\r\n                            f\"CPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cpu\"\r\n                if self._strategy_flag.parallel_devices[0].type == \"cuda\":\r\n                    if self._accelerator_flag and self._accelerator_flag not in (\"auto\", \"cuda\", \"gpu\"):\r\n                        raise MisconfigurationException(\r\n                            f\"GPU parallel_devices set through {self._strategy_flag.__class__.__name__} class,\"\r\n                            f\" but accelerator set to {self._accelerator_flag}, please choose one device type\"\r\n                        )\r\n                    self._accelerator_flag = \"cuda\"\r\n                self._parallel_devices = self._strategy_flag.parallel_devices", "code_tokens": ["def", "_check_config_and_set_final_flags", "(", "self", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", ",", "precision", ":", "Optional", "[", "_PRECISION_INPUT", "]", ",", "plugins", ":", "Optional", "[", "Union", "[", "_PLUGIN_INPUT", ",", "Iterable", "[", "_PLUGIN_INPUT", "]", "]", "]", ",", "sync_batchnorm", ":", "bool", ",", ")", "-", ">", "None", ":", "STRING", "if", "plugins", "is", "not", "None", ":", "plugins", "=", "[", "plugins", "]", "if", "not", "isinstance", "(", "plugins", ",", "Iterable", ")", "else", "plugins", "if", "isinstance", "(", "strategy", ",", "str", ")", ":", "strategy", "=", "strategy", ".", "lower", "(", ")", "self", ".", "_strategy_flag", "=", "strategy", "if", "strategy", "!", "=", "STRING", "and", "strategy", "not", "in", "self", ".", "_registered_strategies", "and", "not", "isinstance", "(", "strategy", ",", "Strategy", ")", ":", "raise", "ValueError", "(", "fSTRING", "STRING", "STRING", "STRING", ")", "if", "(", "accelerator", "not", "in", "self", ".", "_accelerator_types", "and", "accelerator", "not", "in", "(", "STRING", ",", "STRING", ")", "and", "not", "isinstance", "(", "accelerator", ",", "Accelerator", ")", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "is_ddp_str", "=", "isinstance", "(", "strategy", ",", "str", ")", "and", "STRING", "in", "strategy", "is_deepspeed_str", "=", "isinstance", "(", "strategy", ",", "str", ")", "and", "STRING", "in", "strategy", "is_parallel_strategy", "=", "isinstance", "(", "strategy", ",", "ParallelStrategy", ")", "or", "is_ddp_str", "or", "is_deepspeed_str", "is_mps_accelerator", "=", "MPSAccelerator", ".", "is_available", "(", ")", "and", "(", "accelerator", "in", "(", "STRING", ",", "STRING", ",", "STRING", ",", "None", ")", "or", "isinstance", "(", "accelerator", ",", "MPSAccelerator", ")", ")", "if", "is_mps_accelerator", "and", "is_parallel_strategy", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "self", ".", "_accelerator_flag", "=", "accelerator", "precision_flag", "=", "_convert_precision_to_unified_args", "(", "precision", ")", "if", "plugins", ":", "plugins_flags_types", ":", "dict", "[", "str", ",", "int", "]", "=", "Counter", "(", ")", "for", "plugin", "in", "plugins", ":", "if", "isinstance", "(", "plugin", ",", "Precision", ")", ":", "self", ".", "_precision_plugin_flag", "=", "plugin", "plugins_flags_types", "[", "Precision", ".", "__name__", "]", "+", "=", "1", "elif", "isinstance", "(", "plugin", ",", "CheckpointIO", ")", ":", "self", ".", "checkpoint_io", "=", "plugin", "plugins_flags_types", "[", "CheckpointIO", ".", "__name__", "]", "+", "=", "1", "elif", "isinstance", "(", "plugin", ",", "ClusterEnvironment", ")", ":", "self", ".", "_cluster_environment_flag", "=", "plugin", "plugins_flags_types", "[", "ClusterEnvironment", ".", "__name__", "]", "+", "=", "1", "elif", "isinstance", "(", "plugin", ",", "LayerSync", ")", ":", "if", "sync_batchnorm", "and", "not", "isinstance", "(", "plugin", ",", "TorchSyncBatchNorm", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")", "self", ".", "_layer_sync", "=", "plugin", "plugins_flags_types", "[", "TorchSyncBatchNorm", ".", "__name__", "]", "+", "=", "1", "else", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")", "duplicated_plugin_key", "=", "[", "k", "for", "k", ",", "v", "in", "plugins_flags_types", ".", "items", "(", ")", "if", "v", ">", "1", "]", "if", "duplicated_plugin_key", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")", "if", "plugins_flags_types", ".", "get", "(", "Precision", ".", "__name__", ")", "and", "precision_flag", "is", "not", "None", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "self", ".", "_precision_flag", "=", "STRING", "if", "precision_flag", "is", "None", "else", "precision_flag", "if", "self", ".", "_strategy_flag", "and", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", ":", "if", "self", ".", "_strategy_flag", ".", "_accelerator", ":", "if", "self", ".", "_accelerator_flag", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "self", ".", "_accelerator_flag", "=", "self", ".", "_strategy_flag", ".", "_accelerator", "if", "self", ".", "_strategy_flag", ".", "_precision_plugin", ":", "if", "self", ".", "_precision_plugin_flag", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "self", ".", "_precision_plugin_flag", "=", "self", ".", "_strategy_flag", ".", "_precision_plugin", "if", "self", ".", "_strategy_flag", ".", "_checkpoint_io"], "docstring": "This method checks:", "docstring_tokens": ["this", "method", "checks"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 164, "end_line": 311, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_949", "original_string": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if HPUAccelerator.is_available():\r\n                return \"hpu\"\r\n        return _select_auto_accelerator()", "language": "python", "code": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if HPUAccelerator.is_available():\r\n                return \"hpu\"\r\n        return _select_auto_accelerator()", "code_tokens": ["def", "_choose_auto_accelerator", "(", ")", "-", ">", "str", ":", "STRING", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", "if", "HPUAccelerator", ".", "is_available", "(", ")", ":", "return", "STRING", "return", "_select_auto_accelerator", "(", ")"], "docstring": "Choose the accelerator type (str) based on availability.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 332, "end_line": 339, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_950", "original_string": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if (\r\n            strategy_flag in FSDPStrategy.get_registered_strategies() or type(self._strategy_flag) is FSDPStrategy\r\n        ) and not (self._accelerator_flag in (\"cuda\", \"gpu\") or isinstance(self._accelerator_flag, CUDAAccelerator)):\r\n            raise ValueError(\r\n                f\"The strategy `{FSDPStrategy.strategy_name}` requires a GPU accelerator, but received \"\r\n                f\"`accelerator={self._accelerator_flag!r}`. Please set `accelerator='cuda'`, `accelerator='gpu'`,\"\r\n                \" or pass a `CUDAAccelerator()` instance to use FSDP.\"\r\n            )\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Trainer(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Trainer(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "language": "python", "code": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        if (\r\n            strategy_flag in FSDPStrategy.get_registered_strategies() or type(self._strategy_flag) is FSDPStrategy\r\n        ) and not (self._accelerator_flag in (\"cuda\", \"gpu\") or isinstance(self._accelerator_flag, CUDAAccelerator)):\r\n            raise ValueError(\r\n                f\"The strategy `{FSDPStrategy.strategy_name}` requires a GPU accelerator, but received \"\r\n                f\"`accelerator={self._accelerator_flag!r}`. Please set `accelerator='cuda'`, `accelerator='gpu'`,\"\r\n                \" or pass a `CUDAAccelerator()` instance to use FSDP.\"\r\n            )\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Trainer(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Trainer(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "code_tokens": ["def", "_check_strategy_and_fallback", "(", "self", ")", "-", ">", "None", ":", "STRING", "strategy_flag", "=", "STRING", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", "else", "self", ".", "_strategy_flag", "if", "(", "strategy_flag", "in", "FSDPStrategy", ".", "get_registered_strategies", "(", ")", "or", "type", "(", "self", ".", "_strategy_flag", ")", "is", "FSDPStrategy", ")", "and", "not", "(", "self", ".", "_accelerator_flag", "in", "(", "STRING", ",", "STRING", ")", "or", "isinstance", "(", "self", ".", "_accelerator_flag", ",", "CUDAAccelerator", ")", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", "STRING", ")", "if", "strategy_flag", "in", "_DDP_FORK_ALIASES", "and", "STRING", "not", "in", "torch", ".", "multiprocessing", ".", "get_all_start_methods", "(", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "if", "strategy_flag", ":", "self", ".", "_strategy_flag", "=", "strategy_flag"], "docstring": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different", "docstring_tokens": ["checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", "and", "we", "need", "to", "fall", "back", "to", "a", "different"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 446, "end_line": 467, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_951", "original_string": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = StrategyRegistry.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "language": "python", "code": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = StrategyRegistry.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "code_tokens": ["def", "_init_strategy", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "isinstance", "(", "self", ".", "_strategy_flag", ",", "(", "str", ",", "Strategy", ")", ")", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "str", ")", ":", "self", ".", "strategy", "=", "StrategyRegistry", ".", "get", "(", "self", ".", "_strategy_flag", ")", "else", ":", "self", ".", "strategy", "=", "self", ".", "_strategy_flag"], "docstring": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "docstring_tokens": ["instantiate", "the", "strategy", "given", "depending", "on", "the", "setting", "of", "_strategy_flag"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 469, "end_line": 476, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_952", "original_string": "def _validate_precision_choice(self) -> None:\r\n        \"\"\"Validate the combination of choices for precision, AMP type, and accelerator.\"\"\"\r\n        if isinstance(self._precision_plugin_flag, BitsandbytesPrecision) and not isinstance(\r\n            self.accelerator, CUDAAccelerator\r\n        ):\r\n            raise RuntimeError(\"Bitsandbytes is only supported on CUDA GPUs.\")\r\n        mp_precision_supported = (\"32-true\", \"bf16-mixed\", \"bf16-true\", \"16-true\")\r\n        if (\r\n            isinstance(self._strategy_flag, ModelParallelStrategy)\r\n            and self._precision_flag not in mp_precision_supported\r\n        ):\r\n            raise ValueError(\r\n                f\"The `ModelParallelStrategy` does not support `Fabric(..., precision={self._precision_flag!r})`.\"\r\n                f\" Choose a different precision among: {', '.join(mp_precision_supported)}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and self._precision_flag not in (\r\n                \"16-mixed\",\r\n                \"bf16-mixed\",\r\n                \"32-true\",\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"`Trainer(accelerator='hpu', precision={self._precision_flag!r})` is not supported.\"\r\n                )", "language": "python", "code": "def _validate_precision_choice(self) -> None:\r\n        \"\"\"Validate the combination of choices for precision, AMP type, and accelerator.\"\"\"\r\n        if isinstance(self._precision_plugin_flag, BitsandbytesPrecision) and not isinstance(\r\n            self.accelerator, CUDAAccelerator\r\n        ):\r\n            raise RuntimeError(\"Bitsandbytes is only supported on CUDA GPUs.\")\r\n        mp_precision_supported = (\"32-true\", \"bf16-mixed\", \"bf16-true\", \"16-true\")\r\n        if (\r\n            isinstance(self._strategy_flag, ModelParallelStrategy)\r\n            and self._precision_flag not in mp_precision_supported\r\n        ):\r\n            raise ValueError(\r\n                f\"The `ModelParallelStrategy` does not support `Fabric(..., precision={self._precision_flag!r})`.\"\r\n                f\" Choose a different precision among: {', '.join(mp_precision_supported)}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and self._precision_flag not in (\r\n                \"16-mixed\",\r\n                \"bf16-mixed\",\r\n                \"32-true\",\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"`Trainer(accelerator='hpu', precision={self._precision_flag!r})` is not supported.\"\r\n                )", "code_tokens": ["def", "_validate_precision_choice", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "isinstance", "(", "self", ".", "_precision_plugin_flag", ",", "BitsandbytesPrecision", ")", "and", "not", "isinstance", "(", "self", ".", "accelerator", ",", "CUDAAccelerator", ")", ":", "raise", "RuntimeError", "(", "STRING", ")", "mp_precision_supported", "=", "(", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ")", "if", "(", "isinstance", "(", "self", ".", "_strategy_flag", ",", "ModelParallelStrategy", ")", "and", "self", ".", "_precision_flag", "not", "in", "mp_precision_supported", ")", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", "if", "isinstance", "(", "self", ".", "accelerator", ",", "HPUAccelerator", ")", "and", "self", ".", "_precision_flag", "not", "in", "(", "STRING", ",", "STRING", ",", "STRING", ",", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")"], "docstring": "Validate the combination of choices for precision, AMP type, and accelerator.", "docstring_tokens": ["validate", "the", "combination", "of", "choices", "for", "precision", "amp", "type", "and", "accelerator"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 522, "end_line": 548, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_953", "original_string": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision_plugin:\r\n            self.strategy.precision_plugin = self.precision_plugin\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy.num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_layer_sync\"):\r\n            self.strategy._layer_sync = self._layer_sync\r\n        if hasattr(self.strategy, \"set_world_ranks\"):\r\n            self.strategy.set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise MisconfigurationException(\r\n                f\"`Trainer(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose a notebook-compatible strategy:\"\r\n                f\" `Trainer(strategy='ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Trainer\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy` or `XLAStrategy`,\"\r\n                f\" found {self.strategy.__class__.__name__}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and not isinstance(\r\n                self.strategy, (SingleHPUStrategy, HPUParallelStrategy)\r\n            ):\r\n                raise ValueError(\r\n                    \"The `HPUAccelerator` can only be used with a `SingleHPUStrategy` or `HPUParallelStrategy`,\"\r\n                    f\" found {self.strategy.__class__.__name__}.\"\r\n                )", "language": "python", "code": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision_plugin:\r\n            self.strategy.precision_plugin = self.precision_plugin\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy.num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_layer_sync\"):\r\n            self.strategy._layer_sync = self._layer_sync\r\n        if hasattr(self.strategy, \"set_world_ranks\"):\r\n            self.strategy.set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise MisconfigurationException(\r\n                f\"`Trainer(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose a notebook-compatible strategy:\"\r\n                f\" `Trainer(strategy='ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Trainer\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy` or `XLAStrategy`,\"\r\n                f\" found {self.strategy.__class__.__name__}.\"\r\n            )\r\n\r\n        if _habana_available_and_importable():\r\n            from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n            if isinstance(self.accelerator, HPUAccelerator) and not isinstance(\r\n                self.strategy, (SingleHPUStrategy, HPUParallelStrategy)\r\n            ):\r\n                raise ValueError(\r\n                    \"The `HPUAccelerator` can only be used with a `SingleHPUStrategy` or `HPUParallelStrategy`,\"\r\n                    f\" found {self.strategy.__class__.__name__}.\"\r\n                )", "code_tokens": ["def", "_lazy_init_strategy", "(", "self", ")", "-", ">", "None", ":", "STRING", "self", ".", "strategy", ".", "accelerator", "=", "self", ".", "accelerator", "if", "self", ".", "precision_plugin", ":", "self", ".", "strategy", ".", "precision_plugin", "=", "self", ".", "precision_plugin", "if", "self", ".", "checkpoint_io", ":", "self", ".", "strategy", ".", "checkpoint_io", "=", "self", ".", "checkpoint_io", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "if", "self", ".", "strategy", ".", "cluster_environment", "is", "None", ":", "self", ".", "strategy", ".", "cluster_environment", "=", "self", ".", "cluster_environment", "self", ".", "cluster_environment", "=", "self", ".", "strategy", ".", "cluster_environment", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "if", "self", ".", "strategy", ".", "parallel_devices", ":", "self", ".", "_parallel_devices", "=", "self", ".", "strategy", ".", "parallel_devices", "else", ":", "self", ".", "strategy", ".", "parallel_devices", "=", "self", ".", "_parallel_devices", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "self", ".", "strategy", ".", "num_nodes", "=", "self", ".", "_num_nodes_flag", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "self", ".", "strategy", ".", "_layer_sync", "=", "self", ".", "_layer_sync", "if", "hasattr", "(", "self", ".", "strategy", ",", "STRING", ")", ":", "self", ".", "strategy", ".", "set_world_ranks", "(", ")", "self", ".", "strategy", ".", "_configure_launcher", "(", ")", "if", "_IS_INTERACTIVE", "and", "self", ".", "strategy", ".", "launcher", "and", "not", "self", ".", "strategy", ".", "launcher", ".", "is_interactive_compatible", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", "fSTRING", "STRING", "STRING", ")", "if", "isinstance", "(", "self", ".", "accelerator", ",", "XLAAccelerator", ")", "and", "not", "isinstance", "(", "self", ".", "strategy", ",", "(", "SingleDeviceXLAStrategy", ",", "XLAStrategy", ")", ")", ":", "raise", "ValueError", "(", "STRING", "fSTRING", ")", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", ",", "HPUParallelStrategy", ",", "SingleHPUStrategy", "if", "isinstance", "(", "self", ".", "accelerator", ",", "HPUAccelerator", ")", "and", "not", "isinstance", "(", "self", ".", "strategy", ",", "(", "SingleHPUStrategy", ",", "HPUParallelStrategy", ")", ")", ":", "raise", "ValueError", "(", "STRING", "fSTRING", ")"], "docstring": "Lazily set missing attributes on the previously instantiated strategy.", "docstring_tokens": ["lazily", "set", "missing", "attributes", "on", "the", "previously", "instantiated", "strategy"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 550, "end_line": 602, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "func_name": "function_954", "original_string": "def _register_external_accelerators_and_strategies() -> None:\r\n    \"\"\"Registers all known strategies in other packages.\"\"\"\r\n    if _habana_available_and_importable():\r\n        from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n        if \"hpu\" not in AcceleratorRegistry:\r\n            HPUAccelerator.register_accelerators(AcceleratorRegistry)\r\n        if \"hpu_parallel\" not in StrategyRegistry:\r\n            HPUParallelStrategy.register_strategies(StrategyRegistry)\r\n        if \"hpu_single\" not in StrategyRegistry:\r\n            SingleHPUStrategy.register_strategies(StrategyRegistry)", "language": "python", "code": "def _register_external_accelerators_and_strategies() -> None:\r\n    \"\"\"Registers all known strategies in other packages.\"\"\"\r\n    if _habana_available_and_importable():\r\n        from lightning_habana import HPUAccelerator, HPUParallelStrategy, SingleHPUStrategy\r\n\r\n        if \"hpu\" not in AcceleratorRegistry:\r\n            HPUAccelerator.register_accelerators(AcceleratorRegistry)\r\n        if \"hpu_parallel\" not in StrategyRegistry:\r\n            HPUParallelStrategy.register_strategies(StrategyRegistry)\r\n        if \"hpu_single\" not in StrategyRegistry:\r\n            SingleHPUStrategy.register_strategies(StrategyRegistry)", "code_tokens": ["def", "_register_external_accelerators_and_strategies", "(", ")", "-", ">", "None", ":", "STRING", "if", "_habana_available_and_importable", "(", ")", ":", "from", "lightning_habana", "import", "HPUAccelerator", ",", "HPUParallelStrategy", ",", "SingleHPUStrategy", "if", "STRING", "not", "in", "AcceleratorRegistry", ":", "HPUAccelerator", ".", "register_accelerators", "(", "AcceleratorRegistry", ")", "if", "STRING", "not", "in", "StrategyRegistry", ":", "HPUParallelStrategy", ".", "register_strategies", "(", "StrategyRegistry", ")", "if", "STRING", "not", "in", "StrategyRegistry", ":", "SingleHPUStrategy", ".", "register_strategies", "(", "StrategyRegistry", ")"], "docstring": "Registers all known strategies in other packages.", "docstring_tokens": ["registers", "all", "known", "strategies", "in", "other", "packages"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py", "start_line": 650, "end_line": 661, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "func_name": "function_955", "original_string": "def _attach_model_callbacks(self) -> None:\r\n        \"\"\"Attaches the callbacks defined in the model.\r\n\r\n        If a callback returned by the model's configure_callback method has the same type as one or several\r\n        callbacks already present in the trainer callbacks list, it will replace them.\r\n        In addition, all :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks\r\n        will be pushed to the end of the list, ensuring they run last.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        model_callbacks = call._call_lightning_module_hook(trainer, \"configure_callbacks\")\r\n        if not model_callbacks:\r\n            return\r\n\r\n        model_callbacks = [model_callbacks] if not isinstance(model_callbacks, Sequence) else model_callbacks\r\n        model_callback_types = {type(c) for c in model_callbacks}\r\n        trainer_callback_types = {type(c) for c in trainer.callbacks}\r\n        trainer_callback_types.discard(Callback)\r\n        override_types = set()\r\n        for model_cb in model_callback_types:\r\n            for trainer_cb in trainer_callback_types:\r\n                if issubclass(model_cb, trainer_cb):\r\n                    override_types.add(trainer_cb)\r\n                    break\r\n        if override_types:\r\n            rank_zero_info(\r\n                \"The following callbacks returned in `LightningModule.configure_callbacks` will override\"\r\n                \" existing callbacks passed to Trainer:\"\r\n                f\" {', '.join(sorted(t.__name__ for t in override_types))}\"\r\n            )\r\n        all_callbacks = [c for c in trainer.callbacks if type(c) not in override_types]\r\n        all_callbacks.extend(model_callbacks)\r\n        all_callbacks = _CallbackConnector._reorder_callbacks(all_callbacks)\r\n        trainer.callbacks = all_callbacks", "language": "python", "code": "def _attach_model_callbacks(self) -> None:\r\n        \"\"\"Attaches the callbacks defined in the model.\r\n\r\n        If a callback returned by the model's configure_callback method has the same type as one or several\r\n        callbacks already present in the trainer callbacks list, it will replace them.\r\n        In addition, all :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks\r\n        will be pushed to the end of the list, ensuring they run last.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        model_callbacks = call._call_lightning_module_hook(trainer, \"configure_callbacks\")\r\n        if not model_callbacks:\r\n            return\r\n\r\n        model_callbacks = [model_callbacks] if not isinstance(model_callbacks, Sequence) else model_callbacks\r\n        model_callback_types = {type(c) for c in model_callbacks}\r\n        trainer_callback_types = {type(c) for c in trainer.callbacks}\r\n        trainer_callback_types.discard(Callback)\r\n        override_types = set()\r\n        for model_cb in model_callback_types:\r\n            for trainer_cb in trainer_callback_types:\r\n                if issubclass(model_cb, trainer_cb):\r\n                    override_types.add(trainer_cb)\r\n                    break\r\n        if override_types:\r\n            rank_zero_info(\r\n                \"The following callbacks returned in `LightningModule.configure_callbacks` will override\"\r\n                \" existing callbacks passed to Trainer:\"\r\n                f\" {', '.join(sorted(t.__name__ for t in override_types))}\"\r\n            )\r\n        all_callbacks = [c for c in trainer.callbacks if type(c) not in override_types]\r\n        all_callbacks.extend(model_callbacks)\r\n        all_callbacks = _CallbackConnector._reorder_callbacks(all_callbacks)\r\n        trainer.callbacks = all_callbacks", "code_tokens": ["def", "_attach_model_callbacks", "(", "self", ")", "-", ">", "None", ":", "STRING", "trainer", "=", "self", ".", "trainer", "model_callbacks", "=", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ")", "if", "not", "model_callbacks", ":", "return", "model_callbacks", "=", "[", "model_callbacks", "]", "if", "not", "isinstance", "(", "model_callbacks", ",", "Sequence", ")", "else", "model_callbacks", "model_callback_types", "=", "{", "type", "(", "c", ")", "for", "c", "in", "model_callbacks", "}", "trainer_callback_types", "=", "{", "type", "(", "c", ")", "for", "c", "in", "trainer", ".", "callbacks", "}", "trainer_callback_types", ".", "discard", "(", "Callback", ")", "override_types", "=", "set", "(", ")", "for", "model_cb", "in", "model_callback_types", ":", "for", "trainer_cb", "in", "trainer_callback_types", ":", "if", "issubclass", "(", "model_cb", ",", "trainer_cb", ")", ":", "override_types", ".", "add", "(", "trainer_cb", ")", "break", "if", "override_types", ":", "rank_zero_info", "(", "STRING", "STRING", "fSTRING", ")", "all_callbacks", "=", "[", "c", "for", "c", "in", "trainer", ".", "callbacks", "if", "type", "(", "c", ")", "not", "in", "override_types", "]", "all_callbacks", ".", "extend", "(", "model_callbacks", ")", "all_callbacks", "=", "_CallbackConnector", ".", "_reorder_callbacks", "(", "all_callbacks", ")", "trainer", ".", "callbacks", "=", "all_callbacks"], "docstring": "Attaches the callbacks defined in the model.", "docstring_tokens": ["attaches", "the", "callbacks", "defined", "in", "the", "model"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "start_line": 172, "end_line": 210, "has_examples": false, "num_comments": 4, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "func_name": "function_956", "original_string": "def _reorder_callbacks(callbacks: list[Callback]) -> list[Callback]:\r\n        \"\"\"Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks\r\n        to the end of the list. The sequential order within the group of checkpoint callbacks is preserved, as well as\r\n        the order of all other callbacks.\r\n\r\n        Args:\r\n            callbacks: A list of callbacks.\r\n\r\n        Return:\r\n            A new list in which the first elements are tuner specific callbacks and last elements are ModelCheckpoints\r\n            if there were any present in the input.\r\n\r\n        \"\"\"\r\n        tuner_callbacks: list[Callback] = []\r\n        other_callbacks: list[Callback] = []\r\n        checkpoint_callbacks: list[Callback] = []\r\n\r\n        for cb in callbacks:\r\n            if isinstance(cb, (BatchSizeFinder, LearningRateFinder)):\r\n                tuner_callbacks.append(cb)\r\n            elif isinstance(cb, Checkpoint):\r\n                checkpoint_callbacks.append(cb)\r\n            else:\r\n                other_callbacks.append(cb)\r\n\r\n        return tuner_callbacks + other_callbacks + checkpoint_callbacks", "language": "python", "code": "def _reorder_callbacks(callbacks: list[Callback]) -> list[Callback]:\r\n        \"\"\"Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks\r\n        to the end of the list. The sequential order within the group of checkpoint callbacks is preserved, as well as\r\n        the order of all other callbacks.\r\n\r\n        Args:\r\n            callbacks: A list of callbacks.\r\n\r\n        Return:\r\n            A new list in which the first elements are tuner specific callbacks and last elements are ModelCheckpoints\r\n            if there were any present in the input.\r\n\r\n        \"\"\"\r\n        tuner_callbacks: list[Callback] = []\r\n        other_callbacks: list[Callback] = []\r\n        checkpoint_callbacks: list[Callback] = []\r\n\r\n        for cb in callbacks:\r\n            if isinstance(cb, (BatchSizeFinder, LearningRateFinder)):\r\n                tuner_callbacks.append(cb)\r\n            elif isinstance(cb, Checkpoint):\r\n                checkpoint_callbacks.append(cb)\r\n            else:\r\n                other_callbacks.append(cb)\r\n\r\n        return tuner_callbacks + other_callbacks + checkpoint_callbacks", "code_tokens": ["def", "_reorder_callbacks", "(", "callbacks", ":", "list", "[", "Callback", "]", ")", "-", ">", "list", "[", "Callback", "]", ":", "STRING", "tuner_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "other_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "checkpoint_callbacks", ":", "list", "[", "Callback", "]", "=", "[", "]", "for", "cb", "in", "callbacks", ":", "if", "isinstance", "(", "cb", ",", "(", "BatchSizeFinder", ",", "LearningRateFinder", ")", ")", ":", "tuner_callbacks", ".", "append", "(", "cb", ")", "elif", "isinstance", "(", "cb", ",", "Checkpoint", ")", ":", "checkpoint_callbacks", ".", "append", "(", "cb", ")", "else", ":", "other_callbacks", ".", "append", "(", "cb", ")", "return", "tuner_callbacks", "+", "other_callbacks", "+", "checkpoint_callbacks"], "docstring": "Moves all the tuner specific callbacks at the beginning of the list and all the `ModelCheckpoint` callbacks", "docstring_tokens": ["moves", "all", "the", "tuner", "specific", "callbacks", "at", "the", "beginning", "of", "the", "list", "and", "all", "the", "modelcheckpoint", "callbacks"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\callback_connector.py", "start_line": 213, "end_line": 238, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_957", "original_string": "def resume_start(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:\r\n\r\n        1. from HPC weights if `checkpoint_path` is ``None`` and on SLURM or passed keyword `\"hpc\"`.\r\n        2. from fault-tolerant auto-saved checkpoint if found\r\n        3. from `checkpoint_path` file if provided\r\n        4. don't restore\r\n\r\n        \"\"\"\r\n        self._ckpt_path = checkpoint_path\r\n        if not checkpoint_path:\r\n            log.debug(\"`checkpoint_path` not specified. Skipping checkpoint loading.\")\r\n            return\r\n\r\n        rank_zero_info(f\"Restoring states from the checkpoint path at {checkpoint_path}\")\r\n        with pl_legacy_patch():\r\n            loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\r\n        self._loaded_checkpoint = _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)", "language": "python", "code": "def resume_start(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:\r\n\r\n        1. from HPC weights if `checkpoint_path` is ``None`` and on SLURM or passed keyword `\"hpc\"`.\r\n        2. from fault-tolerant auto-saved checkpoint if found\r\n        3. from `checkpoint_path` file if provided\r\n        4. don't restore\r\n\r\n        \"\"\"\r\n        self._ckpt_path = checkpoint_path\r\n        if not checkpoint_path:\r\n            log.debug(\"`checkpoint_path` not specified. Skipping checkpoint loading.\")\r\n            return\r\n\r\n        rank_zero_info(f\"Restoring states from the checkpoint path at {checkpoint_path}\")\r\n        with pl_legacy_patch():\r\n            loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\r\n        self._loaded_checkpoint = _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)", "code_tokens": ["def", "resume_start", "(", "self", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "_ckpt_path", "=", "checkpoint_path", "if", "not", "checkpoint_path", ":", "log", ".", "debug", "(", "STRING", ")", "return", "rank_zero_info", "(", "fSTRING", ")", "with", "pl_legacy_patch", "(", ")", ":", "loaded_checkpoint", "=", "self", ".", "trainer", ".", "strategy", ".", "load_checkpoint", "(", "checkpoint_path", ")", "self", ".", "_loaded_checkpoint", "=", "_pl_migrate_checkpoint", "(", "loaded_checkpoint", ",", "checkpoint_path", ")"], "docstring": "Attempts to pre-load the checkpoint file to memory, with the source path determined in this priority:", "docstring_tokens": ["attempts", "to", "pre", "load", "the", "checkpoint", "file", "to", "memory", "with", "the", "source", "path", "determined", "in", "this", "priority"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 66, "end_line": 83, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_958", "original_string": "def _select_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Called by the ``Trainer`` to select the checkpoint path source.\"\"\"\r\n        if self._user_managed:\r\n            if ckpt_path:\r\n                rank_zero_warn(\r\n                    f\"`trainer.ckpt_path = {self._ckpt_path!r}` was called but then you\"\r\n                    f\" passed `trainer.fit(ckpt_path={ckpt_path!r})`. The latter will be loaded.\"\r\n                )\r\n                self._ckpt_path = None\r\n                self._user_managed = False\r\n                ckpt_path = self._parse_ckpt_path(\r\n                    state_fn,\r\n                    ckpt_path,\r\n                    model_provided=model_provided,\r\n                    model_connected=model_connected,\r\n                )\r\n            else:\r\n                ckpt_path = self._ckpt_path\r\n        else:\r\n            ckpt_path = self._parse_ckpt_path(\r\n                state_fn,\r\n                ckpt_path,\r\n                model_provided=model_provided,\r\n                model_connected=model_connected,\r\n            )\r\n        return ckpt_path", "language": "python", "code": "def _select_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Called by the ``Trainer`` to select the checkpoint path source.\"\"\"\r\n        if self._user_managed:\r\n            if ckpt_path:\r\n                rank_zero_warn(\r\n                    f\"`trainer.ckpt_path = {self._ckpt_path!r}` was called but then you\"\r\n                    f\" passed `trainer.fit(ckpt_path={ckpt_path!r})`. The latter will be loaded.\"\r\n                )\r\n                self._ckpt_path = None\r\n                self._user_managed = False\r\n                ckpt_path = self._parse_ckpt_path(\r\n                    state_fn,\r\n                    ckpt_path,\r\n                    model_provided=model_provided,\r\n                    model_connected=model_connected,\r\n                )\r\n            else:\r\n                ckpt_path = self._ckpt_path\r\n        else:\r\n            ckpt_path = self._parse_ckpt_path(\r\n                state_fn,\r\n                ckpt_path,\r\n                model_provided=model_provided,\r\n                model_connected=model_connected,\r\n            )\r\n        return ckpt_path", "code_tokens": ["def", "_select_ckpt_path", "(", "self", ",", "state_fn", ":", "TrainerFn", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "model_provided", ":", "bool", ",", "model_connected", ":", "bool", ")", "-", ">", "Optional", "[", "_PATH", "]", ":", "STRING", "if", "self", ".", "_user_managed", ":", "if", "ckpt_path", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", ")", "self", ".", "_ckpt_path", "=", "None", "self", ".", "_user_managed", "=", "False", "ckpt_path", "=", "self", ".", "_parse_ckpt_path", "(", "state_fn", ",", "ckpt_path", ",", "model_provided", "=", "model_provided", ",", "model_connected", "=", "model_connected", ",", ")", "else", ":", "ckpt_path", "=", "self", ".", "_ckpt_path", "else", ":", "ckpt_path", "=", "self", ".", "_parse_ckpt_path", "(", "state_fn", ",", "ckpt_path", ",", "model_provided", "=", "model_provided", ",", "model_connected", "=", "model_connected", ",", ")", "return", "ckpt_path"], "docstring": "Called by the ``Trainer`` to select the checkpoint path source.", "docstring_tokens": ["called", "by", "the", "trainer", "to", "select", "the", "checkpoint", "path", "source"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 85, "end_line": 113, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_959", "original_string": "def _parse_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Converts the ``ckpt_path`` special values into an actual filepath, depending on the trainer\r\n        configuration.\"\"\"\r\n        if ckpt_path is None and SLURMEnvironment.detect() and self._hpc_resume_path is not None:\r\n            ckpt_path = \"hpc\"\r\n\r\n        from lightning.pytorch.callbacks.on_exception_checkpoint import OnExceptionCheckpoint\r\n\r\n        ft_checkpoints = [cb for cb in self.trainer.callbacks if isinstance(cb, OnExceptionCheckpoint)]\r\n        fn = state_fn.value\r\n        if ckpt_path is None and ft_checkpoints and self.trainer.state.fn == TrainerFn.FITTING:\r\n            ckpt_path = \"last\"\r\n            rank_zero_warn(\r\n                f\"`.{fn}(ckpt_path=None)` was called without a model.\"\r\n                \" The last model of the previous `fit` call will be used.\"\r\n                f\" You can pass `{fn}(ckpt_path='best')` to use the best model or\"\r\n                f\" `{fn}(ckpt_path='last')` to use the last model.\"\r\n                \" If you pass a value, this warning will be silenced.\"\r\n            )\r\n\r\n        if model_provided and ckpt_path is None:\r\n            return None\r\n\r\n        if model_connected and ckpt_path is None:\r\n            ckpt_path = \"best\"\r\n            ft_tip = (\r\n                \" There is also an on-exception checkpoint available, however it is used by default only when fitting.\"\r\n                if ft_checkpoints\r\n                else \"\"\r\n            )\r\n            rank_zero_warn(\r\n                f\"`.{fn}(ckpt_path=None)` was called without a model.\"\r\n                \" The best model of the previous `fit` call will be used.\"\r\n                + ft_tip\r\n                + f\" You can pass `.{fn}(ckpt_path='best')` to use the best model or\"\r\n                f\" `.{fn}(ckpt_path='last')` to use the last model.\"\r\n                \" If you pass a value, this warning will be silenced.\"\r\n            )\r\n\r\n        if ckpt_path == \"best\":\r\n            if len(self.trainer.checkpoint_callbacks) > 1:\r\n                rank_zero_warn(\r\n                    f'`.{fn}(ckpt_path=\"best\")` is called with Trainer configured with multiple `ModelCheckpoint`'\r\n                    \" callbacks. It will use the best checkpoint path from first checkpoint callback.\"\r\n                )\r\n\r\n            if not self.trainer.checkpoint_callback:\r\n                raise ValueError(f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured.')\r\n\r\n            has_best_model_path = self.trainer.checkpoint_callback.best_model_path\r\n            if hasattr(self.trainer.checkpoint_callback, \"best_model_path\") and not has_best_model_path:\r\n                if self.trainer.fast_dev_run:\r\n                    raise ValueError(\r\n                        f'You cannot execute `.{fn}(ckpt_path=\"best\")` with `fast_dev_run=True`.'\r\n                        f\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\r\n                    )\r\n                raise ValueError(\r\n                    f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\r\n                )\r\n            ckpt_path = getattr(self.trainer.checkpoint_callback, \"best_model_path\", None)\r\n\r\n        elif ckpt_path == \"last\":\r\n            candidates = {getattr(ft, \"ckpt_path\", None) for ft in ft_checkpoints}\r\n            for callback in self.trainer.checkpoint_callbacks:\r\n                if isinstance(callback, ModelCheckpoint):\r\n                    candidates |= callback._find_last_checkpoints(self.trainer)\r\n            candidates_fs = {path: get_filesystem(path) for path in candidates if path}\r\n            candidates_ts = {path: fs.modified(path) for path, fs in candidates_fs.items() if fs.exists(path)}\r\n            if not candidates_ts:\r\n                rank_zero_warn(\r\n                    f'.{fn}(ckpt_path=\"last\") is set, but there is no last checkpoint available.'\r\n                    \" No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\"\r\n                )\r\n                return None\r\n            ckpt_path = max(candidates_ts, key=candidates_ts.get)  # type: ignore[arg-type]\r\n\r\n        elif ckpt_path == \"hpc\":\r\n            if not self._hpc_resume_path:\r\n                raise ValueError(\r\n                    f'`.{fn}(ckpt_path=\"hpc\")` is set but no HPC checkpoint was found.'\r\n                    f\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\r\n                )\r\n            ckpt_path = self._hpc_resume_path\r\n\r\n        elif _is_registry(ckpt_path) and module_available(\"litmodels\"):\r\n            ckpt_path = find_model_local_ckpt_path(\r\n                ckpt_path,\r\n                default_model_registry=self.trainer._model_registry,\r\n                default_root_dir=self.trainer.default_root_dir,\r\n            )\r\n\r\n        if not ckpt_path:\r\n            raise ValueError(\r\n                f\"`.{fn}()` found no path for the best weights: {ckpt_path!r}. Please\"\r\n                f\" specify a path for a checkpoint `.{fn}(ckpt_path=PATH)`\"\r\n            )\r\n        return ckpt_path", "language": "python", "code": "def _parse_ckpt_path(\r\n        self, state_fn: TrainerFn, ckpt_path: Optional[_PATH], model_provided: bool, model_connected: bool\r\n    ) -> Optional[_PATH]:\r\n        \"\"\"Converts the ``ckpt_path`` special values into an actual filepath, depending on the trainer\r\n        configuration.\"\"\"\r\n        if ckpt_path is None and SLURMEnvironment.detect() and self._hpc_resume_path is not None:\r\n            ckpt_path = \"hpc\"\r\n\r\n        from lightning.pytorch.callbacks.on_exception_checkpoint import OnExceptionCheckpoint\r\n\r\n        ft_checkpoints = [cb for cb in self.trainer.callbacks if isinstance(cb, OnExceptionCheckpoint)]\r\n        fn = state_fn.value\r\n        if ckpt_path is None and ft_checkpoints and self.trainer.state.fn == TrainerFn.FITTING:\r\n            ckpt_path = \"last\"\r\n            rank_zero_warn(\r\n                f\"`.{fn}(ckpt_path=None)` was called without a model.\"\r\n                \" The last model of the previous `fit` call will be used.\"\r\n                f\" You can pass `{fn}(ckpt_path='best')` to use the best model or\"\r\n                f\" `{fn}(ckpt_path='last')` to use the last model.\"\r\n                \" If you pass a value, this warning will be silenced.\"\r\n            )\r\n\r\n        if model_provided and ckpt_path is None:\r\n            return None\r\n\r\n        if model_connected and ckpt_path is None:\r\n            ckpt_path = \"best\"\r\n            ft_tip = (\r\n                \" There is also an on-exception checkpoint available, however it is used by default only when fitting.\"\r\n                if ft_checkpoints\r\n                else \"\"\r\n            )\r\n            rank_zero_warn(\r\n                f\"`.{fn}(ckpt_path=None)` was called without a model.\"\r\n                \" The best model of the previous `fit` call will be used.\"\r\n                + ft_tip\r\n                + f\" You can pass `.{fn}(ckpt_path='best')` to use the best model or\"\r\n                f\" `.{fn}(ckpt_path='last')` to use the last model.\"\r\n                \" If you pass a value, this warning will be silenced.\"\r\n            )\r\n\r\n        if ckpt_path == \"best\":\r\n            if len(self.trainer.checkpoint_callbacks) > 1:\r\n                rank_zero_warn(\r\n                    f'`.{fn}(ckpt_path=\"best\")` is called with Trainer configured with multiple `ModelCheckpoint`'\r\n                    \" callbacks. It will use the best checkpoint path from first checkpoint callback.\"\r\n                )\r\n\r\n            if not self.trainer.checkpoint_callback:\r\n                raise ValueError(f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured.')\r\n\r\n            has_best_model_path = self.trainer.checkpoint_callback.best_model_path\r\n            if hasattr(self.trainer.checkpoint_callback, \"best_model_path\") and not has_best_model_path:\r\n                if self.trainer.fast_dev_run:\r\n                    raise ValueError(\r\n                        f'You cannot execute `.{fn}(ckpt_path=\"best\")` with `fast_dev_run=True`.'\r\n                        f\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\r\n                    )\r\n                raise ValueError(\r\n                    f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\r\n                )\r\n            ckpt_path = getattr(self.trainer.checkpoint_callback, \"best_model_path\", None)\r\n\r\n        elif ckpt_path == \"last\":\r\n            candidates = {getattr(ft, \"ckpt_path\", None) for ft in ft_checkpoints}\r\n            for callback in self.trainer.checkpoint_callbacks:\r\n                if isinstance(callback, ModelCheckpoint):\r\n                    candidates |= callback._find_last_checkpoints(self.trainer)\r\n            candidates_fs = {path: get_filesystem(path) for path in candidates if path}\r\n            candidates_ts = {path: fs.modified(path) for path, fs in candidates_fs.items() if fs.exists(path)}\r\n            if not candidates_ts:\r\n                rank_zero_warn(\r\n                    f'.{fn}(ckpt_path=\"last\") is set, but there is no last checkpoint available.'\r\n                    \" No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\"\r\n                )\r\n                return None\r\n            ckpt_path = max(candidates_ts, key=candidates_ts.get)  # type: ignore[arg-type]\r\n\r\n        elif ckpt_path == \"hpc\":\r\n            if not self._hpc_resume_path:\r\n                raise ValueError(\r\n                    f'`.{fn}(ckpt_path=\"hpc\")` is set but no HPC checkpoint was found.'\r\n                    f\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\r\n                )\r\n            ckpt_path = self._hpc_resume_path\r\n\r\n        elif _is_registry(ckpt_path) and module_available(\"litmodels\"):\r\n            ckpt_path = find_model_local_ckpt_path(\r\n                ckpt_path,\r\n                default_model_registry=self.trainer._model_registry,\r\n                default_root_dir=self.trainer.default_root_dir,\r\n            )\r\n\r\n        if not ckpt_path:\r\n            raise ValueError(\r\n                f\"`.{fn}()` found no path for the best weights: {ckpt_path!r}. Please\"\r\n                f\" specify a path for a checkpoint `.{fn}(ckpt_path=PATH)`\"\r\n            )\r\n        return ckpt_path", "code_tokens": ["def", "_parse_ckpt_path", "(", "self", ",", "state_fn", ":", "TrainerFn", ",", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "model_provided", ":", "bool", ",", "model_connected", ":", "bool", ")", "-", ">", "Optional", "[", "_PATH", "]", ":", "STRING", "if", "ckpt_path", "is", "None", "and", "SLURMEnvironment", ".", "detect", "(", ")", "and", "self", ".", "_hpc_resume_path", "is", "not", "None", ":", "ckpt_path", "=", "STRING", "from", "lightning", ".", "pytorch", ".", "callbacks", ".", "on_exception_checkpoint", "import", "OnExceptionCheckpoint", "ft_checkpoints", "=", "[", "cb", "for", "cb", "in", "self", ".", "trainer", ".", "callbacks", "if", "isinstance", "(", "cb", ",", "OnExceptionCheckpoint", ")", "]", "fn", "=", "state_fn", ".", "value", "if", "ckpt_path", "is", "None", "and", "ft_checkpoints", "and", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "ckpt_path", "=", "STRING", "rank_zero_warn", "(", "fSTRING", "STRING", "fSTRING", "fSTRING", "STRING", ")", "if", "model_provided", "and", "ckpt_path", "is", "None", ":", "return", "None", "if", "model_connected", "and", "ckpt_path", "is", "None", ":", "ckpt_path", "=", "STRING", "ft_tip", "=", "(", "STRING", "if", "ft_checkpoints", "else", "STRING", ")", "rank_zero_warn", "(", "fSTRING", "STRING", "+", "ft_tip", "+", "fSTRING", "fSTRING", "STRING", ")", "if", "ckpt_path", "=", "=", "STRING", ":", "if", "len", "(", "self", ".", "trainer", ".", "checkpoint_callbacks", ")", ">", "1", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ")", "if", "not", "self", ".", "trainer", ".", "checkpoint_callback", ":", "raise", "ValueError", "(", "fSTRING", ")", "has_best_model_path", "=", "self", ".", "trainer", ".", "checkpoint_callback", ".", "best_model_path", "if", "hasattr", "(", "self", ".", "trainer", ".", "checkpoint_callback", ",", "STRING", ")", "and", "not", "has_best_model_path", ":", "if", "self", ".", "trainer", ".", "fast_dev_run", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "raise", "ValueError", "(", "fSTRING", ")", "ckpt_path", "=", "getattr", "(", "self", ".", "trainer", ".", "checkpoint_callback", ",", "STRING", ",", "None", ")", "elif", "ckpt_path", "=", "=", "STRING", ":", "candidates", "=", "{", "getattr", "(", "ft", ",", "STRING", ",", "None", ")", "for", "ft", "in", "ft_checkpoints", "}", "for", "callback", "in", "self", ".", "trainer", ".", "checkpoint_callbacks", ":", "if", "isinstance", "(", "callback", ",", "ModelCheckpoint", ")", ":", "candidates", "|", "=", "callback", ".", "_find_last_checkpoints", "(", "self", ".", "trainer", ")", "candidates_fs", "=", "{", "path", ":", "get_filesystem", "(", "path", ")", "for", "path", "in", "candidates", "if", "path", "}", "candidates_ts", "=", "{", "path", ":", "fs", ".", "modified", "(", "path", ")", "for", "path", ",", "fs", "in", "candidates_fs", ".", "items", "(", ")", "if", "fs", ".", "exists", "(", "path", ")", "}", "if", "not", "candidates_ts", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ")", "return", "None", "ckpt_path", "=", "max", "(", "candidates_ts", ",", "key", "=", "candidates_ts", ".", "get", ")", "#", "type", ":", "ignore", "[", "arg", "-", "type", "]", "elif", "ckpt_path", "=", "=", "STRING", ":", "if", "not", "self", ".", "_hpc_resume_path", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "ckpt_path", "=", "self", ".", "_hpc_resume_path", "elif", "_is_registry", "(", "ckpt_path", ")", "and", "module_available", "(", "STRING", ")", ":", "ckpt_path", "=", "find_model_local_ckpt_path", "(", "ckpt_path", ",", "default_model_registry", "=", "self", ".", "trainer", ".", "_model_registry", ",", "default_root_dir", "=", "self", ".", "trainer", ".", "default_root_dir", ",", ")", "if", "not", "ckpt_path", ":", "raise", "ValueError", "(", "fSTRING", "fSTRING", ")", "return", "ckpt_path"], "docstring": "Converts the ``ckpt_path`` special values into an actual filepath, depending on the trainer", "docstring_tokens": ["converts", "the", "ckpt_path", "special", "values", "into", "an", "actual", "filepath", "depending", "on", "the", "trainer"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 115, "end_line": 216, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_960", "original_string": "def resume_end(self) -> None:\r\n        \"\"\"Signal the connector that all states have resumed and memory for the checkpoint object can be released.\"\"\"\r\n        assert self.trainer.state.fn is not None\r\n        if self._ckpt_path:\r\n            message = \"Restored all states\" if self.trainer.state.fn == TrainerFn.FITTING else \"Loaded model weights\"\r\n            rank_zero_info(f\"{message} from the checkpoint at {self._ckpt_path}\")\r\n\r\n        self._loaded_checkpoint = {}\r\n        torch.cuda.empty_cache()\r\n\r\n        self.trainer.strategy.barrier(\"_CheckpointConnector.resume_end\")", "language": "python", "code": "def resume_end(self) -> None:\r\n        \"\"\"Signal the connector that all states have resumed and memory for the checkpoint object can be released.\"\"\"\r\n        assert self.trainer.state.fn is not None\r\n        if self._ckpt_path:\r\n            message = \"Restored all states\" if self.trainer.state.fn == TrainerFn.FITTING else \"Loaded model weights\"\r\n            rank_zero_info(f\"{message} from the checkpoint at {self._ckpt_path}\")\r\n\r\n        self._loaded_checkpoint = {}\r\n        torch.cuda.empty_cache()\r\n\r\n        self.trainer.strategy.barrier(\"_CheckpointConnector.resume_end\")", "code_tokens": ["def", "resume_end", "(", "self", ")", "-", ">", "None", ":", "STRING", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "if", "self", ".", "_ckpt_path", ":", "message", "=", "STRING", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", "else", "STRING", "rank_zero_info", "(", "fSTRING", ")", "self", ".", "_loaded_checkpoint", "=", "{", "}", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "self", ".", "trainer", ".", "strategy", ".", "barrier", "(", "STRING", ")"], "docstring": "Signal the connector that all states have resumed and memory for the checkpoint object can be released.", "docstring_tokens": ["signal", "the", "connector", "that", "all", "states", "have", "resumed", "and", "memory", "for", "the", "checkpoint", "object", "can", "be", "released"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 218, "end_line": 230, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_961", "original_string": "def restore(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\r\n        state-restore, in this priority:\r\n\r\n        1. from HPC weights if found\r\n        2. from `checkpoint_path` file if provided\r\n        3. don't restore\r\n\r\n        All restored states are listed in return value description of `dump_checkpoint`.\r\n\r\n        Args:\r\n            checkpoint_path: Path to a PyTorch Lightning checkpoint file.\r\n\r\n        \"\"\"\r\n        self.resume_start(checkpoint_path)\r\n\r\n        self.restore_datamodule()\r\n        self.restore_model()\r\n\r\n        self.restore_callbacks()\r\n\r\n        self.restore_training_state()\r\n        self.resume_end()", "language": "python", "code": "def restore(self, checkpoint_path: Optional[_PATH] = None) -> None:\r\n        \"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\r\n        state-restore, in this priority:\r\n\r\n        1. from HPC weights if found\r\n        2. from `checkpoint_path` file if provided\r\n        3. don't restore\r\n\r\n        All restored states are listed in return value description of `dump_checkpoint`.\r\n\r\n        Args:\r\n            checkpoint_path: Path to a PyTorch Lightning checkpoint file.\r\n\r\n        \"\"\"\r\n        self.resume_start(checkpoint_path)\r\n\r\n        self.restore_datamodule()\r\n        self.restore_model()\r\n\r\n        self.restore_callbacks()\r\n\r\n        self.restore_training_state()\r\n        self.resume_end()", "code_tokens": ["def", "restore", "(", "self", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "self", ".", "resume_start", "(", "checkpoint_path", ")", "self", ".", "restore_datamodule", "(", ")", "self", ".", "restore_model", "(", ")", "self", ".", "restore_callbacks", "(", ")", "self", ".", "restore_training_state", "(", ")", "self", ".", "resume_end", "(", ")"], "docstring": "Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and", "docstring_tokens": ["attempt", "to", "restore", "everything", "at", "once", "from", "a", "pytorch", "lightning", "checkpoint", "file", "through", "file", "read", "and"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 232, "end_line": 257, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_962", "original_string": "def restore_datamodule(self) -> None:\r\n        \"\"\"Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        datamodule = trainer.datamodule\r\n        if datamodule is not None and datamodule.__class__.__qualname__ in self._loaded_checkpoint:\r\n            call._call_lightning_datamodule_hook(\r\n                trainer, \"load_state_dict\", self._loaded_checkpoint[datamodule.__class__.__qualname__]\r\n            )", "language": "python", "code": "def restore_datamodule(self) -> None:\r\n        \"\"\"Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        datamodule = trainer.datamodule\r\n        if datamodule is not None and datamodule.__class__.__qualname__ in self._loaded_checkpoint:\r\n            call._call_lightning_datamodule_hook(\r\n                trainer, \"load_state_dict\", self._loaded_checkpoint[datamodule.__class__.__qualname__]\r\n            )", "code_tokens": ["def", "restore_datamodule", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "trainer", "=", "self", ".", "trainer", "datamodule", "=", "trainer", ".", "datamodule", "if", "datamodule", "is", "not", "None", "and", "datamodule", ".", "__class__", ".", "__qualname__", "in", "self", ".", "_loaded_checkpoint", ":", "call", ".", "_call_lightning_datamodule_hook", "(", "trainer", ",", "STRING", ",", "self", ".", "_loaded_checkpoint", "[", "datamodule", ".", "__class__", ".", "__qualname__", "]", ")"], "docstring": "Calls hooks on the datamodule to give it a chance to restore its state from the checkpoint.", "docstring_tokens": ["calls", "hooks", "on", "the", "datamodule", "to", "give", "it", "a", "chance", "to", "restore", "its", "state", "from", "the", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 259, "end_line": 269, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_963", "original_string": "def restore_model(self) -> None:\r\n        \"\"\"Restores a model's weights from a PyTorch Lightning checkpoint.\r\n\r\n        Hooks are called first to give the LightningModule a chance to modify the contents, then finally the model gets\r\n        updated with the loaded weights.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        call._call_lightning_module_hook(self.trainer, \"on_load_checkpoint\", self._loaded_checkpoint)\r\n\r\n        self.trainer.strategy.load_model_state_dict(\r\n            self._loaded_checkpoint,\r\n            strict=self.trainer.lightning_module.strict_loading,\r\n        )", "language": "python", "code": "def restore_model(self) -> None:\r\n        \"\"\"Restores a model's weights from a PyTorch Lightning checkpoint.\r\n\r\n        Hooks are called first to give the LightningModule a chance to modify the contents, then finally the model gets\r\n        updated with the loaded weights.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        call._call_lightning_module_hook(self.trainer, \"on_load_checkpoint\", self._loaded_checkpoint)\r\n\r\n        self.trainer.strategy.load_model_state_dict(\r\n            self._loaded_checkpoint,\r\n            strict=self.trainer.lightning_module.strict_loading,\r\n        )", "code_tokens": ["def", "restore_model", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "call", ".", "_call_lightning_module_hook", "(", "self", ".", "trainer", ",", "STRING", ",", "self", ".", "_loaded_checkpoint", ")", "self", ".", "trainer", ".", "strategy", ".", "load_model_state_dict", "(", "self", ".", "_loaded_checkpoint", ",", "strict", "=", "self", ".", "trainer", ".", "lightning_module", ".", "strict_loading", ",", ")"], "docstring": "Restores a model's weights from a PyTorch Lightning checkpoint.", "docstring_tokens": ["restores", "a", "model", "s", "weights", "from", "a", "pytorch", "lightning", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 271, "end_line": 288, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_964", "original_string": "def restore_training_state(self) -> None:\r\n        \"\"\"Restore the trainer state from the pre-loaded checkpoint.\r\n\r\n        This includes the precision settings, loop progress, optimizer states and learning rate scheduler states.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        self.restore_precision_plugin_state()\r\n\r\n        self.restore_loops()\r\n\r\n        assert self.trainer.state.fn is not None\r\n        if self.trainer.state.fn == TrainerFn.FITTING:\r\n            self.restore_optimizers_and_schedulers()", "language": "python", "code": "def restore_training_state(self) -> None:\r\n        \"\"\"Restore the trainer state from the pre-loaded checkpoint.\r\n\r\n        This includes the precision settings, loop progress, optimizer states and learning rate scheduler states.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        self.restore_precision_plugin_state()\r\n\r\n        self.restore_loops()\r\n\r\n        assert self.trainer.state.fn is not None\r\n        if self.trainer.state.fn == TrainerFn.FITTING:\r\n            self.restore_optimizers_and_schedulers()", "code_tokens": ["def", "restore_training_state", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "self", ".", "restore_precision_plugin_state", "(", ")", "self", ".", "restore_loops", "(", ")", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "restore_optimizers_and_schedulers", "(", ")"], "docstring": "Restore the trainer state from the pre-loaded checkpoint.", "docstring_tokens": ["restore", "the", "trainer", "state", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 290, "end_line": 308, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_965", "original_string": "def restore_precision_plugin_state(self) -> None:\r\n        \"\"\"Restore the precision plugin state from the pre-loaded checkpoint.\"\"\"\r\n        prec_plugin = self.trainer.precision_plugin\r\n        prec_plugin.on_load_checkpoint(self._loaded_checkpoint)\r\n        if prec_plugin.__class__.__qualname__ in self._loaded_checkpoint:\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[prec_plugin.__class__.__qualname__])\r\n\r\n        if \"native_amp_scaling_state\" in self._loaded_checkpoint and isinstance(prec_plugin, MixedPrecision):\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[\"native_amp_scaling_state\"])", "language": "python", "code": "def restore_precision_plugin_state(self) -> None:\r\n        \"\"\"Restore the precision plugin state from the pre-loaded checkpoint.\"\"\"\r\n        prec_plugin = self.trainer.precision_plugin\r\n        prec_plugin.on_load_checkpoint(self._loaded_checkpoint)\r\n        if prec_plugin.__class__.__qualname__ in self._loaded_checkpoint:\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[prec_plugin.__class__.__qualname__])\r\n\r\n        if \"native_amp_scaling_state\" in self._loaded_checkpoint and isinstance(prec_plugin, MixedPrecision):\r\n            prec_plugin.load_state_dict(self._loaded_checkpoint[\"native_amp_scaling_state\"])", "code_tokens": ["def", "restore_precision_plugin_state", "(", "self", ")", "-", ">", "None", ":", "STRING", "prec_plugin", "=", "self", ".", "trainer", ".", "precision_plugin", "prec_plugin", ".", "on_load_checkpoint", "(", "self", ".", "_loaded_checkpoint", ")", "if", "prec_plugin", ".", "__class__", ".", "__qualname__", "in", "self", ".", "_loaded_checkpoint", ":", "prec_plugin", ".", "load_state_dict", "(", "self", ".", "_loaded_checkpoint", "[", "prec_plugin", ".", "__class__", ".", "__qualname__", "]", ")", "if", "STRING", "in", "self", ".", "_loaded_checkpoint", "and", "isinstance", "(", "prec_plugin", ",", "MixedPrecision", ")", ":", "prec_plugin", ".", "load_state_dict", "(", "self", ".", "_loaded_checkpoint", "[", "STRING", "]", ")"], "docstring": "Restore the precision plugin state from the pre-loaded checkpoint.", "docstring_tokens": ["restore", "the", "precision", "plugin", "state", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 310, "end_line": 319, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_966", "original_string": "def restore_callbacks(self) -> None:\r\n        \"\"\"Restores all callbacks from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        call._call_callbacks_on_load_checkpoint(trainer, self._loaded_checkpoint)\r\n        call._call_callbacks_load_state_dict(trainer, self._loaded_checkpoint)", "language": "python", "code": "def restore_callbacks(self) -> None:\r\n        \"\"\"Restores all callbacks from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        trainer = self.trainer\r\n        call._call_callbacks_on_load_checkpoint(trainer, self._loaded_checkpoint)\r\n        call._call_callbacks_load_state_dict(trainer, self._loaded_checkpoint)", "code_tokens": ["def", "restore_callbacks", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callbacks_on_load_checkpoint", "(", "trainer", ",", "self", ".", "_loaded_checkpoint", ")", "call", ".", "_call_callbacks_load_state_dict", "(", "trainer", ",", "self", ".", "_loaded_checkpoint", ")"], "docstring": "Restores all callbacks from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "all", "callbacks", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 321, "end_line": 328, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_967", "original_string": "def restore_loops(self) -> None:\r\n        \"\"\"Restores the loop progress from the pre-loaded checkpoint.\r\n\r\n        Calls hooks on the loops to give it a chance to restore its state from the checkpoint.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        fit_loop = self.trainer.fit_loop\r\n        assert self.trainer.state.fn is not None\r\n        state_dict = self._loaded_checkpoint.get(\"loops\")\r\n        if state_dict is not None:\r\n            if self.trainer.state.fn == TrainerFn.FITTING:\r\n                fit_loop.load_state_dict(state_dict[\"fit_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.VALIDATING:\r\n                self.trainer.validate_loop.load_state_dict(state_dict[\"validate_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.TESTING:\r\n                self.trainer.test_loop.load_state_dict(state_dict[\"test_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.PREDICTING:\r\n                self.trainer.predict_loop.load_state_dict(state_dict[\"predict_loop\"])\r\n\r\n        if self.trainer.state.fn != TrainerFn.FITTING:\r\n            return\r\n\r\n        if (\r\n            self.trainer.max_epochs != -1\r\n            and self.trainer.max_epochs is not None\r\n            and self.trainer.current_epoch > self.trainer.max_epochs\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"You restored a checkpoint with current_epoch={self.trainer.current_epoch},\"\r\n                f\" but you have set Trainer(max_epochs={self.trainer.max_epochs}).\"\r\n            )", "language": "python", "code": "def restore_loops(self) -> None:\r\n        \"\"\"Restores the loop progress from the pre-loaded checkpoint.\r\n\r\n        Calls hooks on the loops to give it a chance to restore its state from the checkpoint.\r\n\r\n        \"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        fit_loop = self.trainer.fit_loop\r\n        assert self.trainer.state.fn is not None\r\n        state_dict = self._loaded_checkpoint.get(\"loops\")\r\n        if state_dict is not None:\r\n            if self.trainer.state.fn == TrainerFn.FITTING:\r\n                fit_loop.load_state_dict(state_dict[\"fit_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.VALIDATING:\r\n                self.trainer.validate_loop.load_state_dict(state_dict[\"validate_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.TESTING:\r\n                self.trainer.test_loop.load_state_dict(state_dict[\"test_loop\"])\r\n            elif self.trainer.state.fn == TrainerFn.PREDICTING:\r\n                self.trainer.predict_loop.load_state_dict(state_dict[\"predict_loop\"])\r\n\r\n        if self.trainer.state.fn != TrainerFn.FITTING:\r\n            return\r\n\r\n        if (\r\n            self.trainer.max_epochs != -1\r\n            and self.trainer.max_epochs is not None\r\n            and self.trainer.current_epoch > self.trainer.max_epochs\r\n        ):\r\n            raise MisconfigurationException(\r\n                f\"You restored a checkpoint with current_epoch={self.trainer.current_epoch},\"\r\n                f\" but you have set Trainer(max_epochs={self.trainer.max_epochs}).\"\r\n            )", "code_tokens": ["def", "restore_loops", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "fit_loop", "=", "self", ".", "trainer", ".", "fit_loop", "assert", "self", ".", "trainer", ".", "state", ".", "fn", "is", "not", "None", "state_dict", "=", "self", ".", "_loaded_checkpoint", ".", "get", "(", "STRING", ")", "if", "state_dict", "is", "not", "None", ":", "if", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "fit_loop", ".", "load_state_dict", "(", "state_dict", "[", "STRING", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "VALIDATING", ":", "self", ".", "trainer", ".", "validate_loop", ".", "load_state_dict", "(", "state_dict", "[", "STRING", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "TESTING", ":", "self", ".", "trainer", ".", "test_loop", ".", "load_state_dict", "(", "state_dict", "[", "STRING", "]", ")", "elif", "self", ".", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "PREDICTING", ":", "self", ".", "trainer", ".", "predict_loop", ".", "load_state_dict", "(", "state_dict", "[", "STRING", "]", ")", "if", "self", ".", "trainer", ".", "state", ".", "fn", "!", "=", "TrainerFn", ".", "FITTING", ":", "return", "if", "(", "self", ".", "trainer", ".", "max_epochs", "!", "=", "-", "1", "and", "self", ".", "trainer", ".", "max_epochs", "is", "not", "None", "and", "self", ".", "trainer", ".", "current_epoch", ">", "self", ".", "trainer", ".", "max_epochs", ")", ":", "raise", "MisconfigurationException", "(", "fSTRING", "fSTRING", ")"], "docstring": "Restores the loop progress from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "loop", "progress", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 330, "end_line": 364, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_968", "original_string": "def restore_optimizers_and_schedulers(self) -> None:\r\n        \"\"\"Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        if self.trainer.strategy.lightning_restore_optimizer:\r\n            if \"optimizer_states\" not in self._loaded_checkpoint:\r\n                raise KeyError(\r\n                    \"Trying to restore optimizer state but checkpoint contains only the model.\"\r\n                    \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n                )\r\n            self.restore_optimizers()\r\n\r\n        if \"lr_schedulers\" not in self._loaded_checkpoint:\r\n            raise KeyError(\r\n                \"Trying to restore learning rate scheduler state but checkpoint contains only the model.\"\r\n                \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n            )\r\n        self.restore_lr_schedulers()", "language": "python", "code": "def restore_optimizers_and_schedulers(self) -> None:\r\n        \"\"\"Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        if self.trainer.strategy.lightning_restore_optimizer:\r\n            if \"optimizer_states\" not in self._loaded_checkpoint:\r\n                raise KeyError(\r\n                    \"Trying to restore optimizer state but checkpoint contains only the model.\"\r\n                    \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n                )\r\n            self.restore_optimizers()\r\n\r\n        if \"lr_schedulers\" not in self._loaded_checkpoint:\r\n            raise KeyError(\r\n                \"Trying to restore learning rate scheduler state but checkpoint contains only the model.\"\r\n                \" This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\"\r\n            )\r\n        self.restore_lr_schedulers()", "code_tokens": ["def", "restore_optimizers_and_schedulers", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "if", "self", ".", "trainer", ".", "strategy", ".", "lightning_restore_optimizer", ":", "if", "STRING", "not", "in", "self", ".", "_loaded_checkpoint", ":", "raise", "KeyError", "(", "STRING", "STRING", ")", "self", ".", "restore_optimizers", "(", ")", "if", "STRING", "not", "in", "self", ".", "_loaded_checkpoint", ":", "raise", "KeyError", "(", "STRING", "STRING", ")", "self", ".", "restore_lr_schedulers", "(", ")"], "docstring": "Restores the optimizers and learning rate scheduler states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "optimizers", "and", "learning", "rate", "scheduler", "states", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 366, "end_line": 385, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_969", "original_string": "def restore_optimizers(self) -> None:\r\n        \"\"\"Restores the optimizer states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        self.trainer.strategy.load_optimizer_state_dict(self._loaded_checkpoint)", "language": "python", "code": "def restore_optimizers(self) -> None:\r\n        \"\"\"Restores the optimizer states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        self.trainer.strategy.load_optimizer_state_dict(self._loaded_checkpoint)", "code_tokens": ["def", "restore_optimizers", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "self", ".", "trainer", ".", "strategy", ".", "load_optimizer_state_dict", "(", "self", ".", "_loaded_checkpoint", ")"], "docstring": "Restores the optimizer states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "optimizer", "states", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 387, "end_line": 393, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_970", "original_string": "def restore_lr_schedulers(self) -> None:\r\n        \"\"\"Restores the learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        lr_schedulers = self._loaded_checkpoint[\"lr_schedulers\"]\r\n        for config, lrs_state in zip(self.trainer.lr_scheduler_configs, lr_schedulers):\r\n            config.scheduler.load_state_dict(lrs_state)", "language": "python", "code": "def restore_lr_schedulers(self) -> None:\r\n        \"\"\"Restores the learning rate scheduler states from the pre-loaded checkpoint.\"\"\"\r\n        if not self._loaded_checkpoint:\r\n            return\r\n\r\n        lr_schedulers = self._loaded_checkpoint[\"lr_schedulers\"]\r\n        for config, lrs_state in zip(self.trainer.lr_scheduler_configs, lr_schedulers):\r\n            config.scheduler.load_state_dict(lrs_state)", "code_tokens": ["def", "restore_lr_schedulers", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "_loaded_checkpoint", ":", "return", "lr_schedulers", "=", "self", ".", "_loaded_checkpoint", "[", "STRING", "]", "for", "config", ",", "lrs_state", "in", "zip", "(", "self", ".", "trainer", ".", "lr_scheduler_configs", ",", "lr_schedulers", ")", ":", "config", ".", "scheduler", ".", "load_state_dict", "(", "lrs_state", ")"], "docstring": "Restores the learning rate scheduler states from the pre-loaded checkpoint.", "docstring_tokens": ["restores", "the", "learning", "rate", "scheduler", "states", "from", "the", "pre", "loaded", "checkpoint"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 395, "end_line": 403, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_971", "original_string": "def dump_checkpoint(self, weights_only: bool = False) -> dict:\r\n        \"\"\"Creating a model checkpoint dictionary object from various component states.\r\n\r\n        Args:\r\n            weights_only: saving model weights only\r\n        Return:\r\n            structured dictionary: {\r\n                'epoch':                     training epoch\r\n                'global_step':               training global step\r\n                'pytorch-lightning_version': The version of PyTorch Lightning that produced this checkpoint\r\n                'callbacks':                 \"callback specific state\"[] # if not weights_only\r\n                'optimizer_states':          \"PT optim's state_dict\"[]   # if not weights_only\r\n                'lr_schedulers':             \"PT sched's state_dict\"[]   # if not weights_only\r\n                'state_dict':                Model's state_dict (e.g. network weights)\r\n                precision_plugin.__class__.__qualname__:  precision plugin state_dict # if not weights_only\r\n                CHECKPOINT_HYPER_PARAMS_NAME:\r\n                CHECKPOINT_HYPER_PARAMS_KEY:\r\n                CHECKPOINT_HYPER_PARAMS_TYPE:\r\n                something_cool_i_want_to_save: anything you define through model.on_save_checkpoint\r\n                LightningDataModule.__class__.__qualname__: pl DataModule's state\r\n            }\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        model = trainer.lightning_module\r\n        datamodule = trainer.datamodule\r\n\r\n        checkpoint = {\r\n            \"epoch\": trainer.current_epoch,\r\n            \"global_step\": trainer.global_step,\r\n            \"pytorch-lightning_version\": pl.__version__,\r\n            \"state_dict\": self._get_lightning_module_state_dict(),\r\n            \"loops\": self._get_loops_state_dict(),\r\n        }\r\n\r\n        if not weights_only:\r\n            checkpoint[\"callbacks\"] = call._call_callbacks_state_dict(trainer)\r\n\r\n            optimizer_states = []\r\n            for i, optimizer in enumerate(trainer.optimizers):\r\n                optimizer_state = trainer.strategy.optimizer_state(optimizer)\r\n                optimizer_states.append(optimizer_state)\r\n\r\n            checkpoint[\"optimizer_states\"] = optimizer_states\r\n\r\n            lr_schedulers = []\r\n            for config in trainer.lr_scheduler_configs:\r\n                lr_schedulers.append(config.scheduler.state_dict())\r\n            checkpoint[\"lr_schedulers\"] = lr_schedulers\r\n\r\n            prec_plugin = trainer.precision_plugin\r\n            prec_plugin_state_dict = prec_plugin.state_dict()\r\n            if prec_plugin_state_dict:\r\n                checkpoint[prec_plugin.__class__.__qualname__] = prec_plugin_state_dict\r\n            prec_plugin.on_save_checkpoint(checkpoint)\r\n\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container\r\n\r\n        for obj in (model, datamodule):\r\n            if obj and obj.hparams:\r\n                if hasattr(obj, \"_hparams_name\"):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_NAME] = obj._hparams_name\r\n                if _OMEGACONF_AVAILABLE and isinstance(obj.hparams, Container):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = obj.hparams\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_TYPE] = type(obj.hparams)\r\n                else:\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = dict(obj.hparams)\r\n\r\n        if datamodule is not None:\r\n            datamodule_state_dict = call._call_lightning_datamodule_hook(trainer, \"state_dict\")\r\n            if datamodule_state_dict:\r\n                checkpoint[datamodule.__class__.__qualname__] = datamodule_state_dict\r\n\r\n        if not weights_only:\r\n            call._call_callbacks_on_save_checkpoint(trainer, checkpoint)\r\n        call._call_lightning_module_hook(trainer, \"on_save_checkpoint\", checkpoint)\r\n        return checkpoint", "language": "python", "code": "def dump_checkpoint(self, weights_only: bool = False) -> dict:\r\n        \"\"\"Creating a model checkpoint dictionary object from various component states.\r\n\r\n        Args:\r\n            weights_only: saving model weights only\r\n        Return:\r\n            structured dictionary: {\r\n                'epoch':                     training epoch\r\n                'global_step':               training global step\r\n                'pytorch-lightning_version': The version of PyTorch Lightning that produced this checkpoint\r\n                'callbacks':                 \"callback specific state\"[] # if not weights_only\r\n                'optimizer_states':          \"PT optim's state_dict\"[]   # if not weights_only\r\n                'lr_schedulers':             \"PT sched's state_dict\"[]   # if not weights_only\r\n                'state_dict':                Model's state_dict (e.g. network weights)\r\n                precision_plugin.__class__.__qualname__:  precision plugin state_dict # if not weights_only\r\n                CHECKPOINT_HYPER_PARAMS_NAME:\r\n                CHECKPOINT_HYPER_PARAMS_KEY:\r\n                CHECKPOINT_HYPER_PARAMS_TYPE:\r\n                something_cool_i_want_to_save: anything you define through model.on_save_checkpoint\r\n                LightningDataModule.__class__.__qualname__: pl DataModule's state\r\n            }\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        model = trainer.lightning_module\r\n        datamodule = trainer.datamodule\r\n\r\n        checkpoint = {\r\n            \"epoch\": trainer.current_epoch,\r\n            \"global_step\": trainer.global_step,\r\n            \"pytorch-lightning_version\": pl.__version__,\r\n            \"state_dict\": self._get_lightning_module_state_dict(),\r\n            \"loops\": self._get_loops_state_dict(),\r\n        }\r\n\r\n        if not weights_only:\r\n            checkpoint[\"callbacks\"] = call._call_callbacks_state_dict(trainer)\r\n\r\n            optimizer_states = []\r\n            for i, optimizer in enumerate(trainer.optimizers):\r\n                optimizer_state = trainer.strategy.optimizer_state(optimizer)\r\n                optimizer_states.append(optimizer_state)\r\n\r\n            checkpoint[\"optimizer_states\"] = optimizer_states\r\n\r\n            lr_schedulers = []\r\n            for config in trainer.lr_scheduler_configs:\r\n                lr_schedulers.append(config.scheduler.state_dict())\r\n            checkpoint[\"lr_schedulers\"] = lr_schedulers\r\n\r\n            prec_plugin = trainer.precision_plugin\r\n            prec_plugin_state_dict = prec_plugin.state_dict()\r\n            if prec_plugin_state_dict:\r\n                checkpoint[prec_plugin.__class__.__qualname__] = prec_plugin_state_dict\r\n            prec_plugin.on_save_checkpoint(checkpoint)\r\n\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container\r\n\r\n        for obj in (model, datamodule):\r\n            if obj and obj.hparams:\r\n                if hasattr(obj, \"_hparams_name\"):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_NAME] = obj._hparams_name\r\n                if _OMEGACONF_AVAILABLE and isinstance(obj.hparams, Container):\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = obj.hparams\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_TYPE] = type(obj.hparams)\r\n                else:\r\n                    checkpoint[obj.CHECKPOINT_HYPER_PARAMS_KEY] = dict(obj.hparams)\r\n\r\n        if datamodule is not None:\r\n            datamodule_state_dict = call._call_lightning_datamodule_hook(trainer, \"state_dict\")\r\n            if datamodule_state_dict:\r\n                checkpoint[datamodule.__class__.__qualname__] = datamodule_state_dict\r\n\r\n        if not weights_only:\r\n            call._call_callbacks_on_save_checkpoint(trainer, checkpoint)\r\n        call._call_lightning_module_hook(trainer, \"on_save_checkpoint\", checkpoint)\r\n        return checkpoint", "code_tokens": ["def", "dump_checkpoint", "(", "self", ",", "weights_only", ":", "bool", "=", "False", ")", "-", ">", "dict", ":", "STRING", "trainer", "=", "self", ".", "trainer", "model", "=", "trainer", ".", "lightning_module", "datamodule", "=", "trainer", ".", "datamodule", "checkpoint", "=", "{", "STRING", ":", "trainer", ".", "current_epoch", ",", "STRING", ":", "trainer", ".", "global_step", ",", "STRING", ":", "pl", ".", "__version__", ",", "STRING", ":", "self", ".", "_get_lightning_module_state_dict", "(", ")", ",", "STRING", ":", "self", ".", "_get_loops_state_dict", "(", ")", ",", "}", "if", "not", "weights_only", ":", "checkpoint", "[", "STRING", "]", "=", "call", ".", "_call_callbacks_state_dict", "(", "trainer", ")", "optimizer_states", "=", "[", "]", "for", "i", ",", "optimizer", "in", "enumerate", "(", "trainer", ".", "optimizers", ")", ":", "optimizer_state", "=", "trainer", ".", "strategy", ".", "optimizer_state", "(", "optimizer", ")", "optimizer_states", ".", "append", "(", "optimizer_state", ")", "checkpoint", "[", "STRING", "]", "=", "optimizer_states", "lr_schedulers", "=", "[", "]", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ":", "lr_schedulers", ".", "append", "(", "config", ".", "scheduler", ".", "state_dict", "(", ")", ")", "checkpoint", "[", "STRING", "]", "=", "lr_schedulers", "prec_plugin", "=", "trainer", ".", "precision_plugin", "prec_plugin_state_dict", "=", "prec_plugin", ".", "state_dict", "(", ")", "if", "prec_plugin_state_dict", ":", "checkpoint", "[", "prec_plugin", ".", "__class__", ".", "__qualname__", "]", "=", "prec_plugin_state_dict", "prec_plugin", ".", "on_save_checkpoint", "(", "checkpoint", ")", "if", "_OMEGACONF_AVAILABLE", ":", "from", "omegaconf", "import", "Container", "for", "obj", "in", "(", "model", ",", "datamodule", ")", ":", "if", "obj", "and", "obj", ".", "hparams", ":", "if", "hasattr", "(", "obj", ",", "STRING", ")", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_NAME", "]", "=", "obj", ".", "_hparams_name", "if", "_OMEGACONF_AVAILABLE", "and", "isinstance", "(", "obj", ".", "hparams", ",", "Container", ")", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_KEY", "]", "=", "obj", ".", "hparams", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_TYPE", "]", "=", "type", "(", "obj", ".", "hparams", ")", "else", ":", "checkpoint", "[", "obj", ".", "CHECKPOINT_HYPER_PARAMS_KEY", "]", "=", "dict", "(", "obj", ".", "hparams", ")", "if", "datamodule", "is", "not", "None", ":", "datamodule_state_dict", "=", "call", ".", "_call_lightning_datamodule_hook", "(", "trainer", ",", "STRING", ")", "if", "datamodule_state_dict", ":", "checkpoint", "[", "datamodule", ".", "__class__", ".", "__qualname__", "]", "=", "datamodule_state_dict", "if", "not", "weights_only", ":", "call", ".", "_call_callbacks_on_save_checkpoint", "(", "trainer", ",", "checkpoint", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "STRING", ",", "checkpoint", ")", "return", "checkpoint"], "docstring": "Creating a model checkpoint dictionary object from various component states.", "docstring_tokens": ["creating", "a", "model", "checkpoint", "dictionary", "object", "from", "various", "component", "states"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 412, "end_line": 502, "has_examples": false, "num_comments": 10, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_972", "original_string": "def __max_ckpt_version_in_folder(dir_path: _PATH, name_key: str = \"ckpt_\") -> Optional[int]:\r\n        \"\"\"List up files in `dir_path` with `name_key`, then yield maximum suffix number.\r\n\r\n        Args:\r\n            dir_path: path of directory which may contain files whose name include `name_key`\r\n            name_key: file name prefix\r\n        Returns:\r\n            None if no-corresponding-file else maximum suffix number\r\n\r\n        \"\"\"\r\n        fs, uri = url_to_fs(str(dir_path))\r\n        if not fs.exists(dir_path):\r\n            return None\r\n\r\n        files = [os.path.basename(f[\"name\"]) for f in fs.listdir(uri)]\r\n        files = [x for x in files if name_key in x]\r\n        if len(files) == 0:\r\n            return None\r\n\r\n        ckpt_vs = []\r\n        for name in files:\r\n            name = name.split(name_key)[-1]\r\n            name = re.sub(\"[^0-9]\", \"\", name)\r\n            ckpt_vs.append(int(name))\r\n\r\n        return max(ckpt_vs)", "language": "python", "code": "def __max_ckpt_version_in_folder(dir_path: _PATH, name_key: str = \"ckpt_\") -> Optional[int]:\r\n        \"\"\"List up files in `dir_path` with `name_key`, then yield maximum suffix number.\r\n\r\n        Args:\r\n            dir_path: path of directory which may contain files whose name include `name_key`\r\n            name_key: file name prefix\r\n        Returns:\r\n            None if no-corresponding-file else maximum suffix number\r\n\r\n        \"\"\"\r\n        fs, uri = url_to_fs(str(dir_path))\r\n        if not fs.exists(dir_path):\r\n            return None\r\n\r\n        files = [os.path.basename(f[\"name\"]) for f in fs.listdir(uri)]\r\n        files = [x for x in files if name_key in x]\r\n        if len(files) == 0:\r\n            return None\r\n\r\n        ckpt_vs = []\r\n        for name in files:\r\n            name = name.split(name_key)[-1]\r\n            name = re.sub(\"[^0-9]\", \"\", name)\r\n            ckpt_vs.append(int(name))\r\n\r\n        return max(ckpt_vs)", "code_tokens": ["def", "__max_ckpt_version_in_folder", "(", "dir_path", ":", "_PATH", ",", "name_key", ":", "str", "=", "STRING", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "fs", ",", "uri", "=", "url_to_fs", "(", "str", "(", "dir_path", ")", ")", "if", "not", "fs", ".", "exists", "(", "dir_path", ")", ":", "return", "None", "files", "=", "[", "os", ".", "path", ".", "basename", "(", "f", "[", "STRING", "]", ")", "for", "f", "in", "fs", ".", "listdir", "(", "uri", ")", "]", "files", "=", "[", "x", "for", "x", "in", "files", "if", "name_key", "in", "x", "]", "if", "len", "(", "files", ")", "=", "=", "0", ":", "return", "None", "ckpt_vs", "=", "[", "]", "for", "name", "in", "files", ":", "name", "=", "name", ".", "split", "(", "name_key", ")", "[", "-", "1", "]", "name", "=", "re", ".", "sub", "(", "STRING", ",", "STRING", ",", "name", ")", "ckpt_vs", ".", "append", "(", "int", "(", "name", ")", ")", "return", "max", "(", "ckpt_vs", ")"], "docstring": "List up files in `dir_path` with `name_key`, then yield maximum suffix number.", "docstring_tokens": ["list", "up", "files", "in", "dir_path", "with", "name_key", "then", "yield", "maximum", "suffix", "number"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 516, "end_line": 544, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "function_973", "original_string": "def __get_max_ckpt_path_from_folder(folder_path: _PATH) -> str:\r\n        \"\"\"Get path of maximum-epoch checkpoint in the folder.\"\"\"\r\n        max_suffix = _CheckpointConnector.__max_ckpt_version_in_folder(folder_path)\r\n        ckpt_number = max_suffix if max_suffix is not None else 0\r\n        return f\"{folder_path}/hpc_ckpt_{ckpt_number}.ckpt\"", "language": "python", "code": "def __get_max_ckpt_path_from_folder(folder_path: _PATH) -> str:\r\n        \"\"\"Get path of maximum-epoch checkpoint in the folder.\"\"\"\r\n        max_suffix = _CheckpointConnector.__max_ckpt_version_in_folder(folder_path)\r\n        ckpt_number = max_suffix if max_suffix is not None else 0\r\n        return f\"{folder_path}/hpc_ckpt_{ckpt_number}.ckpt\"", "code_tokens": ["def", "__get_max_ckpt_path_from_folder", "(", "folder_path", ":", "_PATH", ")", "-", ">", "str", ":", "STRING", "max_suffix", "=", "_CheckpointConnector", ".", "__max_ckpt_version_in_folder", "(", "folder_path", ")", "ckpt_number", "=", "max_suffix", "if", "max_suffix", "is", "not", "None", "else", "0", "return", "fSTRING"], "docstring": "Get path of maximum-epoch checkpoint in the folder.", "docstring_tokens": ["get", "path", "of", "maximum", "epoch", "checkpoint", "in", "the", "folder"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "start_line": 547, "end_line": 551, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_974", "original_string": "def _prepare_dataloader(self, dataloader: object, shuffle: bool, mode: RunningStage) -> object:\r\n        \"\"\"This function handles the following functionalities:\r\n\r\n        - Injecting a `DistributedDataSamplerWrapper` into the `DataLoader` if on a distributed environment\r\n        - Wrapping the dataloader based on strategy-specific logic\r\n\r\n        \"\"\"\r\n        if not isinstance(dataloader, DataLoader):\r\n            return dataloader\r\n        if (\r\n            self._requires_distributed_sampler(dataloader)  # sets the distributed sampler\r\n            or mode == RunningStage.PREDICTING  # to track indices for the predictions\r\n        ):\r\n            sampler = self._resolve_sampler(dataloader, shuffle=shuffle, mode=mode)\r\n            return _update_dataloader(dataloader, sampler, mode=mode)\r\n\r\n        return dataloader", "language": "python", "code": "def _prepare_dataloader(self, dataloader: object, shuffle: bool, mode: RunningStage) -> object:\r\n        \"\"\"This function handles the following functionalities:\r\n\r\n        - Injecting a `DistributedDataSamplerWrapper` into the `DataLoader` if on a distributed environment\r\n        - Wrapping the dataloader based on strategy-specific logic\r\n\r\n        \"\"\"\r\n        if not isinstance(dataloader, DataLoader):\r\n            return dataloader\r\n        if (\r\n            self._requires_distributed_sampler(dataloader)  # sets the distributed sampler\r\n            or mode == RunningStage.PREDICTING  # to track indices for the predictions\r\n        ):\r\n            sampler = self._resolve_sampler(dataloader, shuffle=shuffle, mode=mode)\r\n            return _update_dataloader(dataloader, sampler, mode=mode)\r\n\r\n        return dataloader", "code_tokens": ["def", "_prepare_dataloader", "(", "self", ",", "dataloader", ":", "object", ",", "shuffle", ":", "bool", ",", "mode", ":", "RunningStage", ")", "-", ">", "object", ":", "STRING", "if", "not", "isinstance", "(", "dataloader", ",", "DataLoader", ")", ":", "return", "dataloader", "if", "(", "self", ".", "_requires_distributed_sampler", "(", "dataloader", ")", "#", "sets", "the", "distributed", "sampler", "or", "mode", "=", "=", "RunningStage", ".", "PREDICTING", "#", "to", "track", "indices", "for", "the", "predictions", ")", ":", "sampler", "=", "self", ".", "_resolve_sampler", "(", "dataloader", ",", "shuffle", "=", "shuffle", ",", "mode", "=", "mode", ")", "return", "_update_dataloader", "(", "dataloader", ",", "sampler", ",", "mode", "=", "mode", ")", "return", "dataloader"], "docstring": "This function handles the following functionalities:", "docstring_tokens": ["this", "function", "handles", "the", "following", "functionalities"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 176, "end_line": 193, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_975", "original_string": "def _get_distributed_sampler(\r\n    dataloader: DataLoader,\r\n    shuffle: bool,\r\n    overfit_batches: Union[int, float],\r\n    mode: Optional[RunningStage] = None,\r\n    **kwargs: Any,\r\n) -> DistributedSampler:\r\n    \"\"\"This function is used to created the distributed sampler injected within the user DataLoader.\"\"\"\r\n    kwargs[\"shuffle\"] = shuffle and not overfit_batches\r\n    kwargs.setdefault(\"seed\", int(os.getenv(\"PL_GLOBAL_SEED\", 0)))\r\n    if mode == RunningStage.PREDICTING:\r\n        return UnrepeatedDistributedSamplerWrapper(dataloader.sampler, **kwargs)\r\n    if isinstance(dataloader.sampler, (RandomSampler, SequentialSampler)):\r\n        return DistributedSampler(dataloader.dataset, **kwargs)\r\n    return DistributedSamplerWrapper(dataloader.sampler, **kwargs)", "language": "python", "code": "def _get_distributed_sampler(\r\n    dataloader: DataLoader,\r\n    shuffle: bool,\r\n    overfit_batches: Union[int, float],\r\n    mode: Optional[RunningStage] = None,\r\n    **kwargs: Any,\r\n) -> DistributedSampler:\r\n    \"\"\"This function is used to created the distributed sampler injected within the user DataLoader.\"\"\"\r\n    kwargs[\"shuffle\"] = shuffle and not overfit_batches\r\n    kwargs.setdefault(\"seed\", int(os.getenv(\"PL_GLOBAL_SEED\", 0)))\r\n    if mode == RunningStage.PREDICTING:\r\n        return UnrepeatedDistributedSamplerWrapper(dataloader.sampler, **kwargs)\r\n    if isinstance(dataloader.sampler, (RandomSampler, SequentialSampler)):\r\n        return DistributedSampler(dataloader.dataset, **kwargs)\r\n    return DistributedSamplerWrapper(dataloader.sampler, **kwargs)", "code_tokens": ["def", "_get_distributed_sampler", "(", "dataloader", ":", "DataLoader", ",", "shuffle", ":", "bool", ",", "overfit_batches", ":", "Union", "[", "int", ",", "float", "]", ",", "mode", ":", "Optional", "[", "RunningStage", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "DistributedSampler", ":", "STRING", "kwargs", "[", "STRING", "]", "=", "shuffle", "and", "not", "overfit_batches", "kwargs", ".", "setdefault", "(", "STRING", ",", "int", "(", "os", ".", "getenv", "(", "STRING", ",", "0", ")", ")", ")", "if", "mode", "=", "=", "RunningStage", ".", "PREDICTING", ":", "return", "UnrepeatedDistributedSamplerWrapper", "(", "dataloader", ".", "sampler", ",", "*", "*", "kwargs", ")", "if", "isinstance", "(", "dataloader", ".", "sampler", ",", "(", "RandomSampler", ",", "SequentialSampler", ")", ")", ":", "return", "DistributedSampler", "(", "dataloader", ".", "dataset", ",", "*", "*", "kwargs", ")", "return", "DistributedSamplerWrapper", "(", "dataloader", ".", "sampler", ",", "*", "*", "kwargs", ")"], "docstring": "This function is used to created the distributed sampler injected within the user DataLoader.", "docstring_tokens": ["this", "function", "is", "used", "to", "created", "the", "distributed", "sampler", "injected", "within", "the", "user", "dataloader"], "partition": "valid", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 229, "end_line": 243, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_976", "original_string": "def _resolve_overfit_batches(combined_loader: CombinedLoader, mode: RunningStage) -> None:\r\n    \"\"\"Resolve overfit batches by disabling shuffling.\r\n\r\n    When overfit_batches > 0, this function ensures that sequential sampling is used without shuffling for consistent\r\n    batches across epochs. Training and validation use different sets of data.\r\n\r\n    \"\"\"\r\n    all_have_sequential_sampler = all(\r\n        isinstance(dl.sampler, SequentialSampler) for dl in combined_loader.flattened if hasattr(dl, \"sampler\")\r\n    )\r\n    if all_have_sequential_sampler:\r\n        return\r\n\r\n    rank_zero_warn(\r\n        f\"You requested to overfit but enabled {mode.dataloader_prefix} dataloader shuffling.\"\r\n        f\" We are turning off the {mode.dataloader_prefix} dataloader shuffling for you.\"\r\n    )\r\n\r\n    updated = [\r\n        _update_dataloader(dl, sampler=SequentialSampler(dl.dataset), mode=mode) if hasattr(dl, \"dataset\") else dl\r\n        for dl in combined_loader.flattened\r\n    ]\r\n    combined_loader.flattened = updated", "language": "python", "code": "def _resolve_overfit_batches(combined_loader: CombinedLoader, mode: RunningStage) -> None:\r\n    \"\"\"Resolve overfit batches by disabling shuffling.\r\n\r\n    When overfit_batches > 0, this function ensures that sequential sampling is used without shuffling for consistent\r\n    batches across epochs. Training and validation use different sets of data.\r\n\r\n    \"\"\"\r\n    all_have_sequential_sampler = all(\r\n        isinstance(dl.sampler, SequentialSampler) for dl in combined_loader.flattened if hasattr(dl, \"sampler\")\r\n    )\r\n    if all_have_sequential_sampler:\r\n        return\r\n\r\n    rank_zero_warn(\r\n        f\"You requested to overfit but enabled {mode.dataloader_prefix} dataloader shuffling.\"\r\n        f\" We are turning off the {mode.dataloader_prefix} dataloader shuffling for you.\"\r\n    )\r\n\r\n    updated = [\r\n        _update_dataloader(dl, sampler=SequentialSampler(dl.dataset), mode=mode) if hasattr(dl, \"dataset\") else dl\r\n        for dl in combined_loader.flattened\r\n    ]\r\n    combined_loader.flattened = updated", "code_tokens": ["def", "_resolve_overfit_batches", "(", "combined_loader", ":", "CombinedLoader", ",", "mode", ":", "RunningStage", ")", "-", ">", "None", ":", "STRING", "all_have_sequential_sampler", "=", "all", "(", "isinstance", "(", "dl", ".", "sampler", ",", "SequentialSampler", ")", "for", "dl", "in", "combined_loader", ".", "flattened", "if", "hasattr", "(", "dl", ",", "STRING", ")", ")", "if", "all_have_sequential_sampler", ":", "return", "rank_zero_warn", "(", "fSTRING", "fSTRING", ")", "updated", "=", "[", "_update_dataloader", "(", "dl", ",", "sampler", "=", "SequentialSampler", "(", "dl", ".", "dataset", ")", ",", "mode", "=", "mode", ")", "if", "hasattr", "(", "dl", ",", "STRING", ")", "else", "dl", "for", "dl", "in", "combined_loader", ".", "flattened", "]", "combined_loader", ".", "flattened", "=", "updated"], "docstring": "Resolve overfit batches by disabling shuffling.", "docstring_tokens": ["resolve", "overfit", "batches", "by", "disabling", "shuffling"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 246, "end_line": 268, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_977", "original_string": "def dataloader(self) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n        \"\"\"Returns the dataloader from the source.\r\n\r\n        If the source is a module, the method with the corresponding :attr:`name` gets called.\r\n\r\n        \"\"\"\r\n        if isinstance(self.instance, pl.LightningModule):\r\n            return call._call_lightning_module_hook(self.instance.trainer, self.name, pl_module=self.instance)\r\n        if isinstance(self.instance, pl.LightningDataModule):\r\n            assert self.instance.trainer is not None\r\n            return call._call_lightning_datamodule_hook(self.instance.trainer, self.name)\r\n        assert self.instance is not None\r\n        return self.instance", "language": "python", "code": "def dataloader(self) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n        \"\"\"Returns the dataloader from the source.\r\n\r\n        If the source is a module, the method with the corresponding :attr:`name` gets called.\r\n\r\n        \"\"\"\r\n        if isinstance(self.instance, pl.LightningModule):\r\n            return call._call_lightning_module_hook(self.instance.trainer, self.name, pl_module=self.instance)\r\n        if isinstance(self.instance, pl.LightningDataModule):\r\n            assert self.instance.trainer is not None\r\n            return call._call_lightning_datamodule_hook(self.instance.trainer, self.name)\r\n        assert self.instance is not None\r\n        return self.instance", "code_tokens": ["def", "dataloader", "(", "self", ")", "-", ">", "Union", "[", "TRAIN_DATALOADERS", ",", "EVAL_DATALOADERS", "]", ":", "STRING", "if", "isinstance", "(", "self", ".", "instance", ",", "pl", ".", "LightningModule", ")", ":", "return", "call", ".", "_call_lightning_module_hook", "(", "self", ".", "instance", ".", "trainer", ",", "self", ".", "name", ",", "pl_module", "=", "self", ".", "instance", ")", "if", "isinstance", "(", "self", ".", "instance", ",", "pl", ".", "LightningDataModule", ")", ":", "assert", "self", ".", "instance", ".", "trainer", "is", "not", "None", "return", "call", ".", "_call_lightning_datamodule_hook", "(", "self", ".", "instance", ".", "trainer", ",", "self", ".", "name", ")", "assert", "self", ".", "instance", "is", "not", "None", "return", "self", ".", "instance"], "docstring": "Returns the dataloader from the source.", "docstring_tokens": ["returns", "the", "dataloader", "from", "the", "source"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 291, "end_line": 303, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_978", "original_string": "def is_defined(self) -> bool:\r\n        \"\"\"Returns whether the source dataloader can be retrieved or not.\r\n\r\n        If the source is a module it checks that the method with given :attr:`name` is overridden.\r\n\r\n        \"\"\"\r\n        return not self.is_module() or is_overridden(self.name, self.instance)", "language": "python", "code": "def is_defined(self) -> bool:\r\n        \"\"\"Returns whether the source dataloader can be retrieved or not.\r\n\r\n        If the source is a module it checks that the method with given :attr:`name` is overridden.\r\n\r\n        \"\"\"\r\n        return not self.is_module() or is_overridden(self.name, self.instance)", "code_tokens": ["def", "is_defined", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "not", "self", ".", "is_module", "(", ")", "or", "is_overridden", "(", "self", ".", "name", ",", "self", ".", "instance", ")"], "docstring": "Returns whether the source dataloader can be retrieved or not.", "docstring_tokens": ["returns", "whether", "the", "source", "dataloader", "can", "be", "retrieved", "or", "not"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 305, "end_line": 311, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_979", "original_string": "def is_module(self) -> bool:\r\n        \"\"\"Returns whether the DataLoader source is a LightningModule or a LightningDataModule.\r\n\r\n        It does not check whether ``*_dataloader`` methods are actually overridden.\r\n\r\n        \"\"\"\r\n        return isinstance(self.instance, (pl.LightningModule, pl.LightningDataModule))", "language": "python", "code": "def is_module(self) -> bool:\r\n        \"\"\"Returns whether the DataLoader source is a LightningModule or a LightningDataModule.\r\n\r\n        It does not check whether ``*_dataloader`` methods are actually overridden.\r\n\r\n        \"\"\"\r\n        return isinstance(self.instance, (pl.LightningModule, pl.LightningDataModule))", "code_tokens": ["def", "is_module", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "isinstance", "(", "self", ".", "instance", ",", "(", "pl", ".", "LightningModule", ",", "pl", ".", "LightningDataModule", ")", ")"], "docstring": "Returns whether the DataLoader source is a LightningModule or a LightningDataModule.", "docstring_tokens": ["returns", "whether", "the", "dataloader", "source", "is", "a", "lightningmodule", "or", "a", "lightningdatamodule"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 313, "end_line": 319, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "function_980", "original_string": "def _request_dataloader(data_source: _DataLoaderSource) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n    \"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader\r\n\r\n    \"\"\"\r\n    with _replace_dunder_methods(DataLoader, \"dataset\"), _replace_dunder_methods(BatchSampler):\r\n        return data_source.dataloader()", "language": "python", "code": "def _request_dataloader(data_source: _DataLoaderSource) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n    \"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader\r\n\r\n    \"\"\"\r\n    with _replace_dunder_methods(DataLoader, \"dataset\"), _replace_dunder_methods(BatchSampler):\r\n        return data_source.dataloader()", "code_tokens": ["def", "_request_dataloader", "(", "data_source", ":", "_DataLoaderSource", ")", "-", ">", "Union", "[", "TRAIN_DATALOADERS", ",", "EVAL_DATALOADERS", "]", ":", "STRING", "with", "_replace_dunder_methods", "(", "DataLoader", ",", "STRING", ")", ",", "_replace_dunder_methods", "(", "BatchSampler", ")", ":", "return", "data_source", ".", "dataloader", "(", ")"], "docstring": "Requests a dataloader by calling dataloader hooks corresponding to the given stage.", "docstring_tokens": ["requests", "a", "dataloader", "by", "calling", "dataloader", "hooks", "corresponding", "to", "the", "given", "stage"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "start_line": 322, "end_line": 334, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "func_name": "function_981", "original_string": "def teardown(self) -> None:\r\n        \"\"\"Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.\"\"\"\r\n        for signum, handler in self._original_handlers.items():\r\n            if handler is not None:\r\n                self._register_signal(signum, handler)\r\n        self._original_handlers = {}", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.\"\"\"\r\n        for signum, handler in self._original_handlers.items():\r\n            if handler is not None:\r\n                self._register_signal(signum, handler)\r\n        self._original_handlers = {}", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "STRING", "for", "signum", ",", "handler", "in", "self", ".", "_original_handlers", ".", "items", "(", ")", ":", "if", "handler", "is", "not", "None", ":", "self", ".", "_register_signal", "(", "signum", ",", "handler", ")", "self", ".", "_original_handlers", "=", "{", "}"], "docstring": "Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.", "docstring_tokens": ["restores", "the", "signals", "that", "were", "previously", "configured", "before", "class", "_signalconnector", "replaced", "them"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "start_line": 124, "end_line": 129, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "func_name": "function_982", "original_string": "def _get_current_signal_handlers() -> dict[_SIGNUM, _HANDLER]:\r\n        \"\"\"Collects the currently assigned signal handlers.\"\"\"\r\n        valid_signals = _SignalConnector._valid_signals()\r\n        if not _IS_WINDOWS:\r\n            valid_signals -= {signal.SIGKILL, signal.SIGSTOP}\r\n        return {signum: signal.getsignal(signum) for signum in valid_signals}", "language": "python", "code": "def _get_current_signal_handlers() -> dict[_SIGNUM, _HANDLER]:\r\n        \"\"\"Collects the currently assigned signal handlers.\"\"\"\r\n        valid_signals = _SignalConnector._valid_signals()\r\n        if not _IS_WINDOWS:\r\n            valid_signals -= {signal.SIGKILL, signal.SIGSTOP}\r\n        return {signum: signal.getsignal(signum) for signum in valid_signals}", "code_tokens": ["def", "_get_current_signal_handlers", "(", ")", "-", ">", "dict", "[", "_SIGNUM", ",", "_HANDLER", "]", ":", "STRING", "valid_signals", "=", "_SignalConnector", ".", "_valid_signals", "(", ")", "if", "not", "_IS_WINDOWS", ":", "valid_signals", "-", "=", "{", "signal", ".", "SIGKILL", ",", "signal", ".", "SIGSTOP", "}", "return", "{", "signum", ":", "signal", ".", "getsignal", "(", "signum", ")", "for", "signum", "in", "valid_signals", "}"], "docstring": "Collects the currently assigned signal handlers.", "docstring_tokens": ["collects", "the", "currently", "assigned", "signal", "handlers"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "start_line": 132, "end_line": 138, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "func_name": "function_983", "original_string": "def _valid_signals() -> set[signal.Signals]:\r\n        \"\"\"Returns all valid signals supported on the current platform.\"\"\"\r\n        return signal.valid_signals()", "language": "python", "code": "def _valid_signals() -> set[signal.Signals]:\r\n        \"\"\"Returns all valid signals supported on the current platform.\"\"\"\r\n        return signal.valid_signals()", "code_tokens": ["def", "_valid_signals", "(", ")", "-", ">", "set", "[", "signal", ".", "Signals", "]", ":", "STRING", "return", "signal", ".", "valid_signals", "(", ")"], "docstring": "Returns all valid signals supported on the current platform.", "docstring_tokens": ["returns", "all", "valid", "signals", "supported", "on", "the", "current", "platform"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "start_line": 141, "end_line": 143, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "function_984", "original_string": "def check_logging(cls, fx_name: str) -> None:\r\n        \"\"\"Check if the given hook is allowed to log.\"\"\"\r\n        if fx_name not in cls.functions:\r\n            raise RuntimeError(\r\n                f\"Logging inside `{fx_name}` is not implemented.\"\r\n                \" Please, open an issue in `https://github.com/Lightning-AI/pytorch-lightning/issues`.\"\r\n            )\r\n\r\n        if cls.functions[fx_name] is None:\r\n            raise MisconfigurationException(\r\n                f\"You can't `self.log()` inside `{fx_name}`. HINT: You can still log directly to the logger by using\"\r\n                \" `self.logger.experiment`.\"\r\n            )", "language": "python", "code": "def check_logging(cls, fx_name: str) -> None:\r\n        \"\"\"Check if the given hook is allowed to log.\"\"\"\r\n        if fx_name not in cls.functions:\r\n            raise RuntimeError(\r\n                f\"Logging inside `{fx_name}` is not implemented.\"\r\n                \" Please, open an issue in `https://github.com/Lightning-AI/pytorch-lightning/issues`.\"\r\n            )\r\n\r\n        if cls.functions[fx_name] is None:\r\n            raise MisconfigurationException(\r\n                f\"You can't `self.log()` inside `{fx_name}`. HINT: You can still log directly to the logger by using\"\r\n                \" `self.logger.experiment`.\"\r\n            )", "code_tokens": ["def", "check_logging", "(", "cls", ",", "fx_name", ":", "str", ")", "-", ">", "None", ":", "STRING", "if", "fx_name", "not", "in", "cls", ".", "functions", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", ")", "if", "cls", ".", "functions", "[", "fx_name", "]", "is", "None", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")"], "docstring": "Check if the given hook is allowed to log.", "docstring_tokens": ["check", "if", "the", "given", "hook", "is", "allowed", "to", "log"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "start_line": 151, "end_line": 163, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "function_985", "original_string": "def get_default_logging_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Return default logging levels for given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        on_step = fx_config[\"default_on_step\"] if on_step is None else on_step\r\n        on_epoch = fx_config[\"default_on_epoch\"] if on_epoch is None else on_epoch\r\n        return on_step, on_epoch", "language": "python", "code": "def get_default_logging_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Return default logging levels for given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        on_step = fx_config[\"default_on_step\"] if on_step is None else on_step\r\n        on_epoch = fx_config[\"default_on_epoch\"] if on_epoch is None else on_epoch\r\n        return on_step, on_epoch", "code_tokens": ["def", "get_default_logging_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "Optional", "[", "bool", "]", ",", "on_epoch", ":", "Optional", "[", "bool", "]", ")", "-", ">", "tuple", "[", "bool", ",", "bool", "]", ":", "STRING", "fx_config", "=", "cls", ".", "functions", "[", "fx_name", "]", "assert", "fx_config", "is", "not", "None", "on_step", "=", "fx_config", "[", "STRING", "]", "if", "on_step", "is", "None", "else", "on_step", "on_epoch", "=", "fx_config", "[", "STRING", "]", "if", "on_epoch", "is", "None", "else", "on_epoch", "return", "on_step", ",", "on_epoch"], "docstring": "Return default logging levels for given hook.", "docstring_tokens": ["return", "default", "logging", "levels", "for", "given", "hook"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "start_line": 166, "end_line": 174, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "function_986", "original_string": "def check_logging_levels(cls, fx_name: str, on_step: bool, on_epoch: bool) -> None:\r\n        \"\"\"Check if the logging levels are allowed in the given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        m = \"You can't `self.log({}={})` inside `{}`, must be one of {}.\"\r\n        if on_step not in fx_config[\"allowed_on_step\"]:\r\n            msg = m.format(\"on_step\", on_step, fx_name, fx_config[\"allowed_on_step\"])\r\n            raise MisconfigurationException(msg)\r\n\r\n        if on_epoch not in fx_config[\"allowed_on_epoch\"]:\r\n            msg = m.format(\"on_epoch\", on_epoch, fx_name, fx_config[\"allowed_on_epoch\"])\r\n            raise MisconfigurationException(msg)", "language": "python", "code": "def check_logging_levels(cls, fx_name: str, on_step: bool, on_epoch: bool) -> None:\r\n        \"\"\"Check if the logging levels are allowed in the given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        m = \"You can't `self.log({}={})` inside `{}`, must be one of {}.\"\r\n        if on_step not in fx_config[\"allowed_on_step\"]:\r\n            msg = m.format(\"on_step\", on_step, fx_name, fx_config[\"allowed_on_step\"])\r\n            raise MisconfigurationException(msg)\r\n\r\n        if on_epoch not in fx_config[\"allowed_on_epoch\"]:\r\n            msg = m.format(\"on_epoch\", on_epoch, fx_name, fx_config[\"allowed_on_epoch\"])\r\n            raise MisconfigurationException(msg)", "code_tokens": ["def", "check_logging_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "bool", ",", "on_epoch", ":", "bool", ")", "-", ">", "None", ":", "STRING", "fx_config", "=", "cls", ".", "functions", "[", "fx_name", "]", "assert", "fx_config", "is", "not", "None", "m", "=", "STRING", "if", "on_step", "not", "in", "fx_config", "[", "STRING", "]", ":", "msg", "=", "m", ".", "format", "(", "STRING", ",", "on_step", ",", "fx_name", ",", "fx_config", "[", "STRING", "]", ")", "raise", "MisconfigurationException", "(", "msg", ")", "if", "on_epoch", "not", "in", "fx_config", "[", "STRING", "]", ":", "msg", "=", "m", ".", "format", "(", "STRING", ",", "on_epoch", ",", "fx_name", ",", "fx_config", "[", "STRING", "]", ")", "raise", "MisconfigurationException", "(", "msg", ")"], "docstring": "Check if the logging levels are allowed in the given hook.", "docstring_tokens": ["check", "if", "the", "logging", "levels", "are", "allowed", "in", "the", "given", "hook"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "start_line": 177, "end_line": 188, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "function_987", "original_string": "def check_logging_and_get_default_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Check if the given hook name is allowed to log and return logging levels.\"\"\"\r\n        cls.check_logging(fx_name)\r\n        on_step, on_epoch = cls.get_default_logging_levels(fx_name, on_step, on_epoch)\r\n        cls.check_logging_levels(fx_name, on_step, on_epoch)\r\n        return on_step, on_epoch", "language": "python", "code": "def check_logging_and_get_default_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Check if the given hook name is allowed to log and return logging levels.\"\"\"\r\n        cls.check_logging(fx_name)\r\n        on_step, on_epoch = cls.get_default_logging_levels(fx_name, on_step, on_epoch)\r\n        cls.check_logging_levels(fx_name, on_step, on_epoch)\r\n        return on_step, on_epoch", "code_tokens": ["def", "check_logging_and_get_default_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "Optional", "[", "bool", "]", ",", "on_epoch", ":", "Optional", "[", "bool", "]", ")", "-", ">", "tuple", "[", "bool", ",", "bool", "]", ":", "STRING", "cls", ".", "check_logging", "(", "fx_name", ")", "on_step", ",", "on_epoch", "=", "cls", ".", "get_default_logging_levels", "(", "fx_name", ",", "on_step", ",", "on_epoch", ")", "cls", ".", "check_logging_levels", "(", "fx_name", ",", "on_step", ",", "on_epoch", ")", "return", "on_step", ",", "on_epoch"], "docstring": "Check if the given hook name is allowed to log and return logging levels.", "docstring_tokens": ["check", "if", "the", "given", "hook", "name", "is", "allowed", "to", "log", "and", "return", "logging", "levels"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "start_line": 191, "end_line": 198, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "func_name": "function_988", "original_string": "def log_metrics(self, metrics: _OUT_DICT, step: Optional[int] = None) -> None:\r\n        \"\"\"Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses\r\n        metrics[\"step\"] as a step.\r\n\r\n        Args:\r\n            metrics: Metric values\r\n            step: Step for which metrics should be logged. If a `step` metric is logged, this value will\r\n                be used else will default to `self.global_step` during training or the total log step count\r\n                during validation and testing.\r\n\r\n        \"\"\"\r\n        if not self.trainer.loggers or not metrics:\r\n            return\r\n\r\n        self._logged_metrics.update(metrics)\r\n\r\n        scalar_metrics = convert_tensors_to_scalars(metrics)\r\n\r\n        if step is None:\r\n            step_metric = scalar_metrics.pop(\"step\", None)\r\n            if step_metric is not None:\r\n                step = int(step_metric)\r\n            else:\r\n                scalar_metrics.setdefault(\"epoch\", self.trainer.current_epoch)\r\n                step = self.trainer.fit_loop.epoch_loop._batches_that_stepped\r\n\r\n        for logger in self.trainer.loggers:\r\n            logger.log_metrics(metrics=scalar_metrics, step=step)\r\n            logger.save()", "language": "python", "code": "def log_metrics(self, metrics: _OUT_DICT, step: Optional[int] = None) -> None:\r\n        \"\"\"Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses\r\n        metrics[\"step\"] as a step.\r\n\r\n        Args:\r\n            metrics: Metric values\r\n            step: Step for which metrics should be logged. If a `step` metric is logged, this value will\r\n                be used else will default to `self.global_step` during training or the total log step count\r\n                during validation and testing.\r\n\r\n        \"\"\"\r\n        if not self.trainer.loggers or not metrics:\r\n            return\r\n\r\n        self._logged_metrics.update(metrics)\r\n\r\n        scalar_metrics = convert_tensors_to_scalars(metrics)\r\n\r\n        if step is None:\r\n            step_metric = scalar_metrics.pop(\"step\", None)\r\n            if step_metric is not None:\r\n                step = int(step_metric)\r\n            else:\r\n                scalar_metrics.setdefault(\"epoch\", self.trainer.current_epoch)\r\n                step = self.trainer.fit_loop.epoch_loop._batches_that_stepped\r\n\r\n        for logger in self.trainer.loggers:\r\n            logger.log_metrics(metrics=scalar_metrics, step=step)\r\n            logger.save()", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "_OUT_DICT", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "if", "not", "self", ".", "trainer", ".", "loggers", "or", "not", "metrics", ":", "return", "self", ".", "_logged_metrics", ".", "update", "(", "metrics", ")", "scalar_metrics", "=", "convert_tensors_to_scalars", "(", "metrics", ")", "if", "step", "is", "None", ":", "step_metric", "=", "scalar_metrics", ".", "pop", "(", "STRING", ",", "None", ")", "if", "step_metric", "is", "not", "None", ":", "step", "=", "int", "(", "step_metric", ")", "else", ":", "scalar_metrics", ".", "setdefault", "(", "STRING", ",", "self", ".", "trainer", ".", "current_epoch", ")", "step", "=", "self", ".", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "_batches_that_stepped", "for", "logger", "in", "self", ".", "trainer", ".", "loggers", ":", "logger", ".", "log_metrics", "(", "metrics", "=", "scalar_metrics", ",", "step", "=", "step", ")", "logger", ".", "save", "(", ")"], "docstring": "Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses", "docstring_tokens": ["logs", "the", "metric", "dict", "passed", "in", "if", "step", "parameter", "is", "none", "and", "step", "key", "is", "presented", "is", "metrics", "uses"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "start_line": 89, "end_line": 120, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "func_name": "function_989", "original_string": "def metrics(self) -> _METRICS:\r\n        \"\"\"This function returns either batch or epoch metrics.\"\"\"\r\n        on_step = self._first_loop_iter is not None\r\n        assert self.trainer._results is not None\r\n        return self.trainer._results.metrics(on_step)", "language": "python", "code": "def metrics(self) -> _METRICS:\r\n        \"\"\"This function returns either batch or epoch metrics.\"\"\"\r\n        on_step = self._first_loop_iter is not None\r\n        assert self.trainer._results is not None\r\n        return self.trainer._results.metrics(on_step)", "code_tokens": ["def", "metrics", "(", "self", ")", "-", ">", "_METRICS", ":", "STRING", "on_step", "=", "self", ".", "_first_loop_iter", "is", "not", "None", "assert", "self", ".", "trainer", ".", "_results", "is", "not", "None", "return", "self", ".", "trainer", ".", "_results", ".", "metrics", "(", "on_step", ")"], "docstring": "This function returns either batch or epoch metrics.", "docstring_tokens": ["this", "function", "returns", "either", "batch", "or", "epoch", "metrics"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "start_line": 232, "end_line": 236, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_990", "original_string": "def _generate_sync_fn(self) -> None:\r\n        \"\"\"Used to compute the syncing function and cache it.\"\"\"\r\n        fn = self.no_op if self.fn is None or not self.should or self.rank_zero_only else self.fn\r\n        self._fn: Callable = partial(fn, reduce_op=self.op, group=self.group)", "language": "python", "code": "def _generate_sync_fn(self) -> None:\r\n        \"\"\"Used to compute the syncing function and cache it.\"\"\"\r\n        fn = self.no_op if self.fn is None or not self.should or self.rank_zero_only else self.fn\r\n        self._fn: Callable = partial(fn, reduce_op=self.op, group=self.group)", "code_tokens": ["def", "_generate_sync_fn", "(", "self", ")", "-", ">", "None", ":", "STRING", "fn", "=", "self", ".", "no_op", "if", "self", ".", "fn", "is", "None", "or", "not", "self", ".", "should", "or", "self", ".", "rank_zero_only", "else", "self", ".", "fn", "self", ".", "_fn", ":", "Callable", "=", "partial", "(", "fn", ",", "reduce_op", "=", "self", ".", "op", ",", "group", "=", "self", ".", "group", ")"], "docstring": "Used to compute the syncing function and cache it.", "docstring_tokens": ["used", "to", "compute", "the", "syncing", "function", "and", "cache", "it"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 89, "end_line": 93, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_991", "original_string": "def log(\r\n        self,\r\n        fx: str,\r\n        name: str,\r\n        value: _VALUE,\r\n        prog_bar: bool = False,\r\n        logger: bool = True,\r\n        on_step: bool = False,\r\n        on_epoch: bool = True,\r\n        reduce_fx: Callable = torch.mean,\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_fn: Callable = _Sync.no_op,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"See :meth:`~lightning.pytorch.core.LightningModule.log`\"\"\"\r\n        if not enable_graph:\r\n            value = recursive_detach(value)\r\n\r\n        key = f\"{fx}.{name}\"\r\n        if add_dataloader_idx and self.dataloader_idx is not None:\r\n            key += f\".{self.dataloader_idx}\"\r\n            fx += f\".{self.dataloader_idx}\"\r\n\r\n        meta = _Metadata(\r\n            fx=fx,\r\n            name=name,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            dataloader_idx=self.dataloader_idx,\r\n            metric_attribute=metric_attribute,\r\n        )\r\n        meta.sync = _Sync(_should=sync_dist, fn=sync_dist_fn, _group=sync_dist_group, rank_zero_only=rank_zero_only)\r\n\r\n        if key not in self:\r\n            metric = _ResultMetric(meta, isinstance(value, Tensor))\r\n            self[key] = metric\r\n\r\n        elif meta != self[key].meta:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log({name}, ...)` twice in `{fx}` with different arguments. This is not allowed\"\r\n            )\r\n        self[key].to(value.device)\r\n\r\n        batch_size = self._extract_batch_size(self[key], batch_size, meta)\r\n        self.update_metrics(key, value, batch_size)", "language": "python", "code": "def log(\r\n        self,\r\n        fx: str,\r\n        name: str,\r\n        value: _VALUE,\r\n        prog_bar: bool = False,\r\n        logger: bool = True,\r\n        on_step: bool = False,\r\n        on_epoch: bool = True,\r\n        reduce_fx: Callable = torch.mean,\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_fn: Callable = _Sync.no_op,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"See :meth:`~lightning.pytorch.core.LightningModule.log`\"\"\"\r\n        if not enable_graph:\r\n            value = recursive_detach(value)\r\n\r\n        key = f\"{fx}.{name}\"\r\n        if add_dataloader_idx and self.dataloader_idx is not None:\r\n            key += f\".{self.dataloader_idx}\"\r\n            fx += f\".{self.dataloader_idx}\"\r\n\r\n        meta = _Metadata(\r\n            fx=fx,\r\n            name=name,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            dataloader_idx=self.dataloader_idx,\r\n            metric_attribute=metric_attribute,\r\n        )\r\n        meta.sync = _Sync(_should=sync_dist, fn=sync_dist_fn, _group=sync_dist_group, rank_zero_only=rank_zero_only)\r\n\r\n        if key not in self:\r\n            metric = _ResultMetric(meta, isinstance(value, Tensor))\r\n            self[key] = metric\r\n\r\n        elif meta != self[key].meta:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log({name}, ...)` twice in `{fx}` with different arguments. This is not allowed\"\r\n            )\r\n        self[key].to(value.device)\r\n\r\n        batch_size = self._extract_batch_size(self[key], batch_size, meta)\r\n        self.update_metrics(key, value, batch_size)", "code_tokens": ["def", "log", "(", "self", ",", "fx", ":", "str", ",", "name", ":", "str", ",", "value", ":", "_VALUE", ",", "prog_bar", ":", "bool", "=", "False", ",", "logger", ":", "bool", "=", "True", ",", "on_step", ":", "bool", "=", "False", ",", "on_epoch", ":", "bool", "=", "True", ",", "reduce_fx", ":", "Callable", "=", "torch", ".", "mean", ",", "enable_graph", ":", "bool", "=", "False", ",", "sync_dist", ":", "bool", "=", "False", ",", "sync_dist_fn", ":", "Callable", "=", "_Sync", ".", "no_op", ",", "sync_dist_group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "add_dataloader_idx", ":", "bool", "=", "True", ",", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "metric_attribute", ":", "Optional", "[", "str", "]", "=", "None", ",", "rank_zero_only", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "STRING", "if", "not", "enable_graph", ":", "value", "=", "recursive_detach", "(", "value", ")", "key", "=", "fSTRING", "if", "add_dataloader_idx", "and", "self", ".", "dataloader_idx", "is", "not", "None", ":", "key", "+", "=", "fSTRING", "fx", "+", "=", "fSTRING", "meta", "=", "_Metadata", "(", "fx", "=", "fx", ",", "name", "=", "name", ",", "prog_bar", "=", "prog_bar", ",", "logger", "=", "logger", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ",", "reduce_fx", "=", "reduce_fx", ",", "enable_graph", "=", "enable_graph", ",", "add_dataloader_idx", "=", "add_dataloader_idx", ",", "dataloader_idx", "=", "self", ".", "dataloader_idx", ",", "metric_attribute", "=", "metric_attribute", ",", ")", "meta", ".", "sync", "=", "_Sync", "(", "_should", "=", "sync_dist", ",", "fn", "=", "sync_dist_fn", ",", "_group", "=", "sync_dist_group", ",", "rank_zero_only", "=", "rank_zero_only", ")", "if", "key", "not", "in", "self", ":", "metric", "=", "_ResultMetric", "(", "meta", ",", "isinstance", "(", "value", ",", "Tensor", ")", ")", "self", "[", "key", "]", "=", "metric", "elif", "meta", "!", "=", "self", "[", "key", "]", ".", "meta", ":", "raise", "MisconfigurationException", "(", "fSTRING", ")", "self", "[", "key", "]", ".", "to", "(", "value", ".", "device", ")", "batch_size", "=", "self", ".", "_extract_batch_size", "(", "self", "[", "key", "]", ",", "batch_size", ",", "meta", ")", "self", ".", "update_metrics", "(", "key", ",", "value", ",", "batch_size", ")"], "docstring": "See :meth:`~lightning.pytorch.core.LightningModule.log`", "docstring_tokens": ["see", "meth", "lightning", "pytorch", "core", "lightningmodule", "log"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 354, "end_line": 414, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_992", "original_string": "def valid_items(self) -> Generator:\r\n        \"\"\"This function is used to iterate over current valid metrics.\"\"\"\r\n        return ((k, v) for k, v in self.items() if not v.has_reset and self.dataloader_idx == v.meta.dataloader_idx)", "language": "python", "code": "def valid_items(self) -> Generator:\r\n        \"\"\"This function is used to iterate over current valid metrics.\"\"\"\r\n        return ((k, v) for k, v in self.items() if not v.has_reset and self.dataloader_idx == v.meta.dataloader_idx)", "code_tokens": ["def", "valid_items", "(", "self", ")", "-", ">", "Generator", ":", "STRING", "return", "(", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "self", ".", "items", "(", ")", "if", "not", "v", ".", "has_reset", "and", "self", ".", "dataloader_idx", "=", "=", "v", ".", "meta", ".", "dataloader_idx", ")"], "docstring": "This function is used to iterate over current valid metrics.", "docstring_tokens": ["this", "function", "is", "used", "to", "iterate", "over", "current", "valid", "metrics"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 454, "end_line": 456, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_993", "original_string": "def reset(self, metrics: Optional[bool] = None, fx: Optional[str] = None) -> None:\r\n        \"\"\"Reset the result collection.\r\n\r\n        Args:\r\n            metrics: If True, only ``torchmetrics.Metric`` results are reset,\r\n                if False, only ``torch.Tensors`` are reset,\r\n                if ``None``, both are.\r\n            fx: Function to reset\r\n\r\n        \"\"\"\r\n        for item in self.values():\r\n            requested_type = metrics is None or metrics ^ item.is_tensor\r\n            same_fx = fx is None or fx == item.meta.fx\r\n            if requested_type and same_fx:\r\n                item.reset()", "language": "python", "code": "def reset(self, metrics: Optional[bool] = None, fx: Optional[str] = None) -> None:\r\n        \"\"\"Reset the result collection.\r\n\r\n        Args:\r\n            metrics: If True, only ``torchmetrics.Metric`` results are reset,\r\n                if False, only ``torch.Tensors`` are reset,\r\n                if ``None``, both are.\r\n            fx: Function to reset\r\n\r\n        \"\"\"\r\n        for item in self.values():\r\n            requested_type = metrics is None or metrics ^ item.is_tensor\r\n            same_fx = fx is None or fx == item.meta.fx\r\n            if requested_type and same_fx:\r\n                item.reset()", "code_tokens": ["def", "reset", "(", "self", ",", "metrics", ":", "Optional", "[", "bool", "]", "=", "None", ",", "fx", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "STRING", "for", "item", "in", "self", ".", "values", "(", ")", ":", "requested_type", "=", "metrics", "is", "None", "or", "metrics", "^", "item", ".", "is_tensor", "same_fx", "=", "fx", "is", "None", "or", "fx", "=", "=", "item", ".", "meta", ".", "fx", "if", "requested_type", "and", "same_fx", ":", "item", ".", "reset", "(", ")"], "docstring": "Reset the result collection.", "docstring_tokens": ["reset", "the", "result", "collection"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 495, "end_line": 509, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_994", "original_string": "def to(self, *args: Any, **kwargs: Any) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to the given device.\"\"\"\r\n        self.update(apply_to_collection(dict(self), (Tensor, Metric), move_data_to_device, *args, **kwargs))\r\n        return self", "language": "python", "code": "def to(self, *args: Any, **kwargs: Any) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to the given device.\"\"\"\r\n        self.update(apply_to_collection(dict(self), (Tensor, Metric), move_data_to_device, *args, **kwargs))\r\n        return self", "code_tokens": ["def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STRING", ":", "STRING", "self", ".", "update", "(", "apply_to_collection", "(", "dict", "(", "self", ")", ",", "(", "Tensor", ",", "Metric", ")", ",", "move_data_to_device", ",", "*", "args", ",", "*", "*", "kwargs", ")", ")", "return", "self"], "docstring": "Move all data to the given device.", "docstring_tokens": ["move", "all", "data", "to", "the", "given", "device"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 511, "end_line": 514, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_995", "original_string": "def cpu(self) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to CPU.\"\"\"\r\n        return self.to(device=\"cpu\")", "language": "python", "code": "def cpu(self) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to CPU.\"\"\"\r\n        return self.to(device=\"cpu\")", "code_tokens": ["def", "cpu", "(", "self", ")", "-", ">", "STRING", ":", "STRING", "return", "self", ".", "to", "(", "device", "=", "STRING", ")"], "docstring": "Move all data to CPU.", "docstring_tokens": ["move", "all", "data", "to", "cpu"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 516, "end_line": 518, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "function_996", "original_string": "def _get_default_dtype() -> torch.dtype:\r\n    \"\"\"The default dtype for new tensors, but no lower than float32.\"\"\"\r\n    dtype = torch.get_default_dtype()\r\n    return dtype if dtype in (torch.float32, torch.float64) else torch.float32", "language": "python", "code": "def _get_default_dtype() -> torch.dtype:\r\n    \"\"\"The default dtype for new tensors, but no lower than float32.\"\"\"\r\n    dtype = torch.get_default_dtype()\r\n    return dtype if dtype in (torch.float32, torch.float64) else torch.float32", "code_tokens": ["def", "_get_default_dtype", "(", ")", "-", ">", "torch", ".", "dtype", ":", "STRING", "dtype", "=", "torch", ".", "get_default_dtype", "(", ")", "return", "dtype", "if", "dtype", "in", "(", "torch", ".", "float32", ",", "torch", ".", "float64", ")", "else", "torch", ".", "float32"], "docstring": "The default dtype for new tensors, but no lower than float32.", "docstring_tokens": ["the", "default", "dtype", "for", "new", "tensors", "but", "no", "lower", "than", "float32"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "start_line": 529, "end_line": 532, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "function_997", "original_string": "def _scale_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    mode: str = \"power\",\r\n    steps_per_trial: int = 3,\r\n    init_val: int = 2,\r\n    max_trials: int = 25,\r\n    batch_arg_name: str = \"batch_size\",\r\n) -> Optional[int]:\r\n    \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n    error.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        mode: Search strategy to update the batch size:\r\n\r\n            - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n            - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n        steps_per_trial: number of steps to run with a given batch size.\r\n            Ideally 1 should be enough to test if an OOM error occurs,\r\n            however in practise a few are needed\r\n        init_val: initial batch size to start the search with\r\n        max_trials: max number of increases in batch size done before\r\n           algorithm is terminated\r\n        batch_arg_name: name of the attribute that stores the batch size.\r\n            It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n            with that name. We will look for this attribute name in the following places\r\n\r\n            - ``model``\r\n            - ``model.hparams``\r\n            - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping batch size scaler since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".scale_batch_size_{uuid.uuid4()}.ckpt\")\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    params = __scale_batch_dump_params(trainer)\r\n\r\n    __scale_batch_reset_params(trainer, steps_per_trial)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    try:\r\n        new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=init_val)\r\n\r\n        if mode == \"power\":\r\n            new_size = _run_power_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n        elif mode == \"binsearch\":\r\n            new_size = _run_binary_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n\r\n        garbage_collection_cuda()\r\n\r\n        log.info(f\"Finished batch size finder, will continue with full run using batch size {new_size}\")\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        __scale_batch_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n\r\n    return new_size", "language": "python", "code": "def _scale_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    mode: str = \"power\",\r\n    steps_per_trial: int = 3,\r\n    init_val: int = 2,\r\n    max_trials: int = 25,\r\n    batch_arg_name: str = \"batch_size\",\r\n) -> Optional[int]:\r\n    \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n    error.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        mode: Search strategy to update the batch size:\r\n\r\n            - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n            - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n        steps_per_trial: number of steps to run with a given batch size.\r\n            Ideally 1 should be enough to test if an OOM error occurs,\r\n            however in practise a few are needed\r\n        init_val: initial batch size to start the search with\r\n        max_trials: max number of increases in batch size done before\r\n           algorithm is terminated\r\n        batch_arg_name: name of the attribute that stores the batch size.\r\n            It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n            with that name. We will look for this attribute name in the following places\r\n\r\n            - ``model``\r\n            - ``model.hparams``\r\n            - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping batch size scaler since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".scale_batch_size_{uuid.uuid4()}.ckpt\")\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    params = __scale_batch_dump_params(trainer)\r\n\r\n    __scale_batch_reset_params(trainer, steps_per_trial)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    try:\r\n        new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=init_val)\r\n\r\n        if mode == \"power\":\r\n            new_size = _run_power_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n        elif mode == \"binsearch\":\r\n            new_size = _run_binary_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n\r\n        garbage_collection_cuda()\r\n\r\n        log.info(f\"Finished batch size finder, will continue with full run using batch size {new_size}\")\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        __scale_batch_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n\r\n    return new_size", "code_tokens": ["def", "_scale_batch_size", "(", "trainer", ":", "STRING", ",", "mode", ":", "str", "=", "STRING", ",", "steps_per_trial", ":", "int", "=", "3", ",", "init_val", ":", "int", "=", "2", ",", "max_trials", ":", "int", "=", "25", ",", "batch_arg_name", ":", "str", "=", "STRING", ",", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "if", "trainer", ".", "fast_dev_run", ":", "rank_zero_warn", "(", "STRING", ")", "return", "None", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "fSTRING", ")", "trainer", ".", "save_checkpoint", "(", "ckpt_path", ")", "params", "=", "__scale_batch_dump_params", "(", "trainer", ")", "__scale_batch_reset_params", "(", "trainer", ",", "steps_per_trial", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "disable", "(", ")", "try", ":", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "init_val", ")", "if", "mode", "=", "=", "STRING", ":", "new_size", "=", "_run_power_scaling", "(", "trainer", ",", "new_size", ",", "batch_arg_name", ",", "max_trials", ",", "params", ")", "elif", "mode", "=", "=", "STRING", ":", "new_size", "=", "_run_binary_scaling", "(", "trainer", ",", "new_size", ",", "batch_arg_name", ",", "max_trials", ",", "params", ")", "garbage_collection_cuda", "(", ")", "log", ".", "info", "(", "fSTRING", ")", "except", "Exception", "as", "ex", ":", "raise", "ex", "finally", ":", "__scale_batch_restore_params", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "enable", "(", ")", "trainer", ".", "_checkpoint_connector", ".", "restore", "(", "ckpt_path", ")", "trainer", ".", "strategy", ".", "remove_checkpoint", "(", "ckpt_path", ")", "return", "new_size"], "docstring": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)", "docstring_tokens": ["iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "oom"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "start_line": 27, "end_line": 100, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "function_998", "original_string": "def _run_power_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.\"\"\"\r\n    any_success = False\r\n    last_successful_size = new_size\r\n    for i in range(max_trials):\r\n        garbage_collection_cuda()\r\n\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n\r\n            if i + 1 >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            _reset_dataloaders(trainer)\r\n            any_success = True\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                garbage_collection_cuda()\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, factor=0.5, desc=\"failed\")\r\n                _reset_dataloaders(trainer)\r\n                if any_success:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "language": "python", "code": "def _run_power_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.\"\"\"\r\n    any_success = False\r\n    last_successful_size = new_size\r\n    for i in range(max_trials):\r\n        garbage_collection_cuda()\r\n\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n\r\n            if i + 1 >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            _reset_dataloaders(trainer)\r\n            any_success = True\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                garbage_collection_cuda()\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, factor=0.5, desc=\"failed\")\r\n                _reset_dataloaders(trainer)\r\n                if any_success:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "code_tokens": ["def", "_run_power_scaling", "(", "trainer", ":", "STRING", ",", "new_size", ":", "int", ",", "batch_arg_name", ":", "str", ",", "max_trials", ":", "int", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ",", ")", "-", ">", "int", ":", "STRING", "any_success", "=", "False", "last_successful_size", "=", "new_size", "for", "i", "in", "range", "(", "max_trials", ")", ":", "garbage_collection_cuda", "(", ")", "_reset_progress", "(", "trainer", ")", "try", ":", "_try_loop_run", "(", "trainer", ",", "params", ")", "last_successful_size", "=", "new_size", "#", "Store", "the", "current", "size", "before", "doubling", "if", "i", "+", "1", ">", "=", "max_trials", ":", "new_size", "=", "last_successful_size", "break", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "2", ".", "0", ",", "desc", "=", "STRING", ")", "if", "not", "changed", ":", "break", "_reset_dataloaders", "(", "trainer", ")", "any_success", "=", "True", "except", "RuntimeError", "as", "exception", ":", "if", "is_oom_error", "(", "exception", ")", ":", "garbage_collection_cuda", "(", ")", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "0", ".", "5", ",", "desc", "=", "STRING", ")", "_reset_dataloaders", "(", "trainer", ")", "if", "any_success", ":", "break", "else", ":", "raise", "#", "some", "other", "error", "not", "memory", "related", "return", "new_size"], "docstring": "Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.", "docstring_tokens": ["batch", "scaling", "mode", "where", "the", "size", "is", "doubled", "at", "each", "iteration", "until", "an", "oom", "error", "is", "encountered"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "start_line": 169, "end_line": 216, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "function_999", "original_string": "def _run_binary_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.\r\n\r\n    Hereafter, the batch size is further refined using a binary search\r\n\r\n    \"\"\"\r\n    low = 1\r\n    high = None\r\n    count = 0\r\n    last_successful_size = new_size\r\n    while True:\r\n        garbage_collection_cuda()\r\n\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n            count += 1\r\n\r\n            if count >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            low = new_size\r\n            if high:\r\n                if high - low <= 1:\r\n                    break\r\n                midval = (high + low) // 2\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"succeeded\")\r\n            else:\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            _reset_dataloaders(trainer)\r\n\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                garbage_collection_cuda()\r\n\r\n                high = new_size\r\n                midval = (high + low) // 2\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"failed\")\r\n\r\n                _reset_dataloaders(trainer)\r\n\r\n                if high - low <= 1:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "language": "python", "code": "def _run_binary_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.\r\n\r\n    Hereafter, the batch size is further refined using a binary search\r\n\r\n    \"\"\"\r\n    low = 1\r\n    high = None\r\n    count = 0\r\n    last_successful_size = new_size\r\n    while True:\r\n        garbage_collection_cuda()\r\n\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n            count += 1\r\n\r\n            if count >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            low = new_size\r\n            if high:\r\n                if high - low <= 1:\r\n                    break\r\n                midval = (high + low) // 2\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"succeeded\")\r\n            else:\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            _reset_dataloaders(trainer)\r\n\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                garbage_collection_cuda()\r\n\r\n                high = new_size\r\n                midval = (high + low) // 2\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"failed\")\r\n\r\n                _reset_dataloaders(trainer)\r\n\r\n                if high - low <= 1:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "code_tokens": ["def", "_run_binary_scaling", "(", "trainer", ":", "STRING", ",", "new_size", ":", "int", ",", "batch_arg_name", ":", "str", ",", "max_trials", ":", "int", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ",", ")", "-", ">", "int", ":", "STRING", "low", "=", "1", "high", "=", "None", "count", "=", "0", "last_successful_size", "=", "new_size", "while", "True", ":", "garbage_collection_cuda", "(", ")", "_reset_progress", "(", "trainer", ")", "try", ":", "_try_loop_run", "(", "trainer", ",", "params", ")", "last_successful_size", "=", "new_size", "#", "Store", "the", "current", "size", "before", "doubling", "count", "+", "=", "1", "if", "count", ">", "=", "max_trials", ":", "new_size", "=", "last_successful_size", "break", "low", "=", "new_size", "if", "high", ":", "if", "high", "-", "low", "<", "=", "1", ":", "break", "midval", "=", "(", "high", "+", "low", ")", "/", "/", "2", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "midval", ",", "desc", "=", "STRING", ")", "else", ":", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "2", ".", "0", ",", "desc", "=", "STRING", ")", "if", "not", "changed", ":", "break", "_reset_dataloaders", "(", "trainer", ")", "except", "RuntimeError", "as", "exception", ":", "if", "is_oom_error", "(", "exception", ")", ":", "garbage_collection_cuda", "(", ")", "high", "=", "new_size", "midval", "=", "(", "high", "+", "low", ")", "/", "/", "2", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "midval", ",", "desc", "=", "STRING", ")", "_reset_dataloaders", "(", "trainer", ")", "if", "high", "-", "low", "<", "=", "1", ":", "break", "else", ":", "raise", "#", "some", "other", "error", "not", "memory", "related", "return", "new_size"], "docstring": "Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.", "docstring_tokens": ["batch", "scaling", "mode", "where", "the", "size", "is", "initially", "is", "doubled", "at", "each", "iteration", "until", "an", "oom", "error", "is", "encountered"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "start_line": 219, "end_line": 286, "has_examples": false, "num_comments": 8, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "function_1000", "original_string": "def _adjust_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    batch_arg_name: str = \"batch_size\",\r\n    factor: float = 1.0,\r\n    value: Optional[int] = None,\r\n    desc: Optional[str] = None,\r\n) -> tuple[int, bool]:\r\n    \"\"\"Helper function for adjusting the batch size.\r\n\r\n    Args:\r\n        trainer: instance of lightning.pytorch.Trainer\r\n        factor: value which the old batch size is multiplied by to get the\r\n            new batch size\r\n        value: if a value is given, will override the batch size with this value.\r\n            Note that the value of `factor` will not have an effect in this case\r\n        desc: either ``\"succeeded\"`` or ``\"failed\"``. Used purely for logging\r\n\r\n    Returns:\r\n        The new batch size for the next trial and a bool that signals whether the\r\n        new value is different than the previous batch size.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n    batch_size = lightning_getattr(model, batch_arg_name)\r\n    assert batch_size is not None\r\n\r\n    loop = trainer._active_loop\r\n    assert loop is not None\r\n    loop.setup_data()\r\n    combined_loader = loop._combined_loader\r\n    assert combined_loader is not None\r\n    try:\r\n        combined_dataset_length = combined_loader._dataset_length()\r\n        if batch_size >= combined_dataset_length:\r\n            rank_zero_info(f\"The batch size {batch_size} is greater or equal than the length of your dataset.\")\r\n            return batch_size, False\r\n    except NotImplementedError:\r\n        pass\r\n\r\n    new_size = value if value is not None else int(batch_size * factor)\r\n    if desc:\r\n        rank_zero_info(f\"Batch size {batch_size} {desc}, trying batch size {new_size}\")\r\n    changed = new_size != batch_size\r\n    lightning_setattr(model, batch_arg_name, new_size)\r\n    return new_size, changed", "language": "python", "code": "def _adjust_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    batch_arg_name: str = \"batch_size\",\r\n    factor: float = 1.0,\r\n    value: Optional[int] = None,\r\n    desc: Optional[str] = None,\r\n) -> tuple[int, bool]:\r\n    \"\"\"Helper function for adjusting the batch size.\r\n\r\n    Args:\r\n        trainer: instance of lightning.pytorch.Trainer\r\n        factor: value which the old batch size is multiplied by to get the\r\n            new batch size\r\n        value: if a value is given, will override the batch size with this value.\r\n            Note that the value of `factor` will not have an effect in this case\r\n        desc: either ``\"succeeded\"`` or ``\"failed\"``. Used purely for logging\r\n\r\n    Returns:\r\n        The new batch size for the next trial and a bool that signals whether the\r\n        new value is different than the previous batch size.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n    batch_size = lightning_getattr(model, batch_arg_name)\r\n    assert batch_size is not None\r\n\r\n    loop = trainer._active_loop\r\n    assert loop is not None\r\n    loop.setup_data()\r\n    combined_loader = loop._combined_loader\r\n    assert combined_loader is not None\r\n    try:\r\n        combined_dataset_length = combined_loader._dataset_length()\r\n        if batch_size >= combined_dataset_length:\r\n            rank_zero_info(f\"The batch size {batch_size} is greater or equal than the length of your dataset.\")\r\n            return batch_size, False\r\n    except NotImplementedError:\r\n        pass\r\n\r\n    new_size = value if value is not None else int(batch_size * factor)\r\n    if desc:\r\n        rank_zero_info(f\"Batch size {batch_size} {desc}, trying batch size {new_size}\")\r\n    changed = new_size != batch_size\r\n    lightning_setattr(model, batch_arg_name, new_size)\r\n    return new_size, changed", "code_tokens": ["def", "_adjust_batch_size", "(", "trainer", ":", "STRING", ",", "batch_arg_name", ":", "str", "=", "STRING", ",", "factor", ":", "float", "=", "1", ".", "0", ",", "value", ":", "Optional", "[", "int", "]", "=", "None", ",", "desc", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "tuple", "[", "int", ",", "bool", "]", ":", "STRING", "model", "=", "trainer", ".", "lightning_module", "batch_size", "=", "lightning_getattr", "(", "model", ",", "batch_arg_name", ")", "assert", "batch_size", "is", "not", "None", "loop", "=", "trainer", ".", "_active_loop", "assert", "loop", "is", "not", "None", "loop", ".", "setup_data", "(", ")", "combined_loader", "=", "loop", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "try", ":", "combined_dataset_length", "=", "combined_loader", ".", "_dataset_length", "(", ")", "if", "batch_size", ">", "=", "combined_dataset_length", ":", "rank_zero_info", "(", "fSTRING", ")", "return", "batch_size", ",", "False", "except", "NotImplementedError", ":", "pass", "new_size", "=", "value", "if", "value", "is", "not", "None", "else", "int", "(", "batch_size", "*", "factor", ")", "if", "desc", ":", "rank_zero_info", "(", "fSTRING", ")", "changed", "=", "new_size", "!", "=", "batch_size", "lightning_setattr", "(", "model", ",", "batch_arg_name", ",", "new_size", ")", "return", "new_size", ",", "changed"], "docstring": "Helper function for adjusting the batch size.", "docstring_tokens": ["helper", "function", "for", "adjusting", "the", "batch", "size"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "start_line": 289, "end_line": 334, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1001", "original_string": "def _exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\r\n        optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\r\n        from lightning.pytorch.core.optimizer import _validate_optimizers_attached\r\n\r\n        optimizers = trainer.strategy.optimizers\r\n\r\n        if len(optimizers) != 1:\r\n            raise MisconfigurationException(\r\n                f\"`model.configure_optimizers()` returned {len(optimizers)}, but\"\r\n                \" learning rate finder only works with single optimizer\"\r\n            )\r\n\r\n        optimizer = optimizers[0]\r\n\r\n        new_lrs = [self.lr_min] * len(optimizer.param_groups)\r\n        for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\r\n            param_group[\"lr\"] = new_lr\r\n            param_group[\"initial_lr\"] = new_lr\r\n\r\n        args = (optimizer, self.lr_max, self.num_training)\r\n        scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\r\n\r\n        trainer.strategy.optimizers = [optimizer]\r\n        trainer.strategy.lr_scheduler_configs = [LRSchedulerConfig(scheduler, interval=\"step\")]\r\n        _validate_optimizers_attached(trainer.optimizers, trainer.lr_scheduler_configs)", "language": "python", "code": "def _exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\r\n        optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\r\n        from lightning.pytorch.core.optimizer import _validate_optimizers_attached\r\n\r\n        optimizers = trainer.strategy.optimizers\r\n\r\n        if len(optimizers) != 1:\r\n            raise MisconfigurationException(\r\n                f\"`model.configure_optimizers()` returned {len(optimizers)}, but\"\r\n                \" learning rate finder only works with single optimizer\"\r\n            )\r\n\r\n        optimizer = optimizers[0]\r\n\r\n        new_lrs = [self.lr_min] * len(optimizer.param_groups)\r\n        for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\r\n            param_group[\"lr\"] = new_lr\r\n            param_group[\"initial_lr\"] = new_lr\r\n\r\n        args = (optimizer, self.lr_max, self.num_training)\r\n        scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\r\n\r\n        trainer.strategy.optimizers = [optimizer]\r\n        trainer.strategy.lr_scheduler_configs = [LRSchedulerConfig(scheduler, interval=\"step\")]\r\n        _validate_optimizers_attached(trainer.optimizers, trainer.lr_scheduler_configs)", "code_tokens": ["def", "_exchange_scheduler", "(", "self", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "from", "lightning", ".", "pytorch", ".", "core", ".", "optimizer", "import", "_validate_optimizers_attached", "optimizers", "=", "trainer", ".", "strategy", ".", "optimizers", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "MisconfigurationException", "(", "fSTRING", "STRING", ")", "optimizer", "=", "optimizers", "[", "0", "]", "new_lrs", "=", "[", "self", ".", "lr_min", "]", "*", "len", "(", "optimizer", ".", "param_groups", ")", "for", "param_group", ",", "new_lr", "in", "zip", "(", "optimizer", ".", "param_groups", ",", "new_lrs", ")", ":", "param_group", "[", "STRING", "]", "=", "new_lr", "param_group", "[", "STRING", "]", "=", "new_lr", "args", "=", "(", "optimizer", ",", "self", ".", "lr_max", ",", "self", ".", "num_training", ")", "scheduler", "=", "_LinearLR", "(", "*", "args", ")", "if", "self", ".", "mode", "=", "=", "STRING", "else", "_ExponentialLR", "(", "*", "args", ")", "trainer", ".", "strategy", ".", "optimizers", "=", "[", "optimizer", "]", "trainer", ".", "strategy", ".", "lr_scheduler_configs", "=", "[", "LRSchedulerConfig", "(", "scheduler", ",", "interval", "=", "STRING", ")", "]", "_validate_optimizers_attached", "(", "trainer", ".", "optimizers", ",", "trainer", ".", "lr_scheduler_configs", ")"], "docstring": "Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified", "docstring_tokens": ["decorate", "trainer", "strategy", "setup_optimizers", "method", "such", "that", "it", "sets", "the", "user", "s", "originally", "specified"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 90, "end_line": 116, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1002", "original_string": "def plot(\r\n        self, suggest: bool = False, show: bool = False, ax: Optional[\"Axes\"] = None\r\n    ) -> Optional[Union[\"plt.Figure\", \"plt.SubFigure\"]]:\r\n        \"\"\"Plot results from lr_find run\r\n        Args:\r\n            suggest: if True, will mark suggested lr to use with a red point\r\n            show: if True, will show figure\r\n            ax: Axes object to which the plot is to be drawn. If not provided, a new figure is created.\r\n\r\n        \"\"\"\r\n        if not _MATPLOTLIB_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `plot` method, you must have Matplotlib installed.\"\r\n                \" Install it by running `pip install -U matplotlib`.\"\r\n            )\r\n        import matplotlib.pyplot as plt\r\n\r\n        lrs = self.results[\"lr\"]\r\n        losses = self.results[\"loss\"]\r\n\r\n        fig: Optional[Union[plt.Figure, plt.SubFigure]]\r\n        if ax is None:\r\n            fig, ax = plt.subplots()\r\n        else:\r\n            fig = ax.figure\r\n\r\n        ax.plot(lrs, losses)\r\n        if self.mode == \"exponential\":\r\n            ax.set_xscale(\"log\")\r\n        ax.set_xlabel(\"Learning rate\")\r\n        ax.set_ylabel(\"Loss\")\r\n\r\n        if suggest:\r\n            _ = self.suggestion()\r\n            if self._optimal_idx:\r\n                ax.plot(lrs[self._optimal_idx], losses[self._optimal_idx], markersize=10, marker=\"o\", color=\"red\")\r\n\r\n        if show:\r\n            plt.show()\r\n\r\n        return fig", "language": "python", "code": "def plot(\r\n        self, suggest: bool = False, show: bool = False, ax: Optional[\"Axes\"] = None\r\n    ) -> Optional[Union[\"plt.Figure\", \"plt.SubFigure\"]]:\r\n        \"\"\"Plot results from lr_find run\r\n        Args:\r\n            suggest: if True, will mark suggested lr to use with a red point\r\n            show: if True, will show figure\r\n            ax: Axes object to which the plot is to be drawn. If not provided, a new figure is created.\r\n\r\n        \"\"\"\r\n        if not _MATPLOTLIB_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `plot` method, you must have Matplotlib installed.\"\r\n                \" Install it by running `pip install -U matplotlib`.\"\r\n            )\r\n        import matplotlib.pyplot as plt\r\n\r\n        lrs = self.results[\"lr\"]\r\n        losses = self.results[\"loss\"]\r\n\r\n        fig: Optional[Union[plt.Figure, plt.SubFigure]]\r\n        if ax is None:\r\n            fig, ax = plt.subplots()\r\n        else:\r\n            fig = ax.figure\r\n\r\n        ax.plot(lrs, losses)\r\n        if self.mode == \"exponential\":\r\n            ax.set_xscale(\"log\")\r\n        ax.set_xlabel(\"Learning rate\")\r\n        ax.set_ylabel(\"Loss\")\r\n\r\n        if suggest:\r\n            _ = self.suggestion()\r\n            if self._optimal_idx:\r\n                ax.plot(lrs[self._optimal_idx], losses[self._optimal_idx], markersize=10, marker=\"o\", color=\"red\")\r\n\r\n        if show:\r\n            plt.show()\r\n\r\n        return fig", "code_tokens": ["def", "plot", "(", "self", ",", "suggest", ":", "bool", "=", "False", ",", "show", ":", "bool", "=", "False", ",", "ax", ":", "Optional", "[", "STRING", "]", "=", "None", ")", "-", ">", "Optional", "[", "Union", "[", "STRING", ",", "STRING", "]", "]", ":", "STRING", "if", "not", "_MATPLOTLIB_AVAILABLE", ":", "raise", "MisconfigurationException", "(", "STRING", "STRING", ")", "import", "matplotlib", ".", "pyplot", "as", "plt", "lrs", "=", "self", ".", "results", "[", "STRING", "]", "losses", "=", "self", ".", "results", "[", "STRING", "]", "fig", ":", "Optional", "[", "Union", "[", "plt", ".", "Figure", ",", "plt", ".", "SubFigure", "]", "]", "if", "ax", "is", "None", ":", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "else", ":", "fig", "=", "ax", ".", "figure", "ax", ".", "plot", "(", "lrs", ",", "losses", ")", "if", "self", ".", "mode", "=", "=", "STRING", ":", "ax", ".", "set_xscale", "(", "STRING", ")", "ax", ".", "set_xlabel", "(", "STRING", ")", "ax", ".", "set_ylabel", "(", "STRING", ")", "if", "suggest", ":", "_", "=", "self", ".", "suggestion", "(", ")", "if", "self", ".", "_optimal_idx", ":", "ax", ".", "plot", "(", "lrs", "[", "self", ".", "_optimal_idx", "]", ",", "losses", "[", "self", ".", "_optimal_idx", "]", ",", "markersize", "=", "10", ",", "marker", "=", "STRING", ",", "color", "=", "STRING", ")", "if", "show", ":", "plt", ".", "show", "(", ")", "return", "fig"], "docstring": "Plot results from lr_find run", "docstring_tokens": ["plot", "results", "from", "lr_find", "run"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 118, "end_line": 159, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1003", "original_string": "def suggestion(self, skip_begin: int = 10, skip_end: int = 1) -> Optional[float]:\r\n        \"\"\"This will propose a suggestion for an initial learning rate based on the point with the steepest negative\r\n        gradient.\r\n\r\n        Args:\r\n            skip_begin: how many samples to skip in the beginning; helps to avoid too naive estimates\r\n            skip_end: how many samples to skip in the end; helps to avoid too optimistic estimates\r\n\r\n        Returns:\r\n            The suggested initial learning rate to use, or `None` if a suggestion is not possible due to too few\r\n            loss samples.\r\n\r\n        \"\"\"\r\n        losses = torch.tensor(self.results[\"loss\"][skip_begin:-skip_end])\r\n        lrs = torch.tensor(self.results[\"lr\"][skip_begin:-skip_end])\r\n        is_finite = torch.isfinite(losses)\r\n        losses = losses[is_finite]\r\n        lrs = lrs[is_finite]\r\n\r\n        if len(losses) < 2:\r\n            log.error(\r\n                \"Failed to compute suggestion for learning rate because there are not enough points. Increase the loop\"\r\n                \" iteration limits or the size of your dataset/dataloader.\"\r\n            )\r\n            self._optimal_idx = None\r\n            return None\r\n\r\n        gradients = torch.gradient(losses, spacing=[lrs])[0]  # Compute the gradient of losses w.r.t. learning rates\r\n        min_grad = torch.argmin(gradients).item()\r\n        all_losses_idx = torch.arange(len(self.results[\"loss\"]))\r\n        idx_non_skipped = all_losses_idx[skip_begin:-skip_end]\r\n        idx_finite = idx_non_skipped[is_finite]\r\n        self._optimal_idx = idx_finite[min_grad].item()  # type: ignore\r\n        return self.results[\"lr\"][self._optimal_idx]", "language": "python", "code": "def suggestion(self, skip_begin: int = 10, skip_end: int = 1) -> Optional[float]:\r\n        \"\"\"This will propose a suggestion for an initial learning rate based on the point with the steepest negative\r\n        gradient.\r\n\r\n        Args:\r\n            skip_begin: how many samples to skip in the beginning; helps to avoid too naive estimates\r\n            skip_end: how many samples to skip in the end; helps to avoid too optimistic estimates\r\n\r\n        Returns:\r\n            The suggested initial learning rate to use, or `None` if a suggestion is not possible due to too few\r\n            loss samples.\r\n\r\n        \"\"\"\r\n        losses = torch.tensor(self.results[\"loss\"][skip_begin:-skip_end])\r\n        lrs = torch.tensor(self.results[\"lr\"][skip_begin:-skip_end])\r\n        is_finite = torch.isfinite(losses)\r\n        losses = losses[is_finite]\r\n        lrs = lrs[is_finite]\r\n\r\n        if len(losses) < 2:\r\n            log.error(\r\n                \"Failed to compute suggestion for learning rate because there are not enough points. Increase the loop\"\r\n                \" iteration limits or the size of your dataset/dataloader.\"\r\n            )\r\n            self._optimal_idx = None\r\n            return None\r\n\r\n        gradients = torch.gradient(losses, spacing=[lrs])[0]  # Compute the gradient of losses w.r.t. learning rates\r\n        min_grad = torch.argmin(gradients).item()\r\n        all_losses_idx = torch.arange(len(self.results[\"loss\"]))\r\n        idx_non_skipped = all_losses_idx[skip_begin:-skip_end]\r\n        idx_finite = idx_non_skipped[is_finite]\r\n        self._optimal_idx = idx_finite[min_grad].item()  # type: ignore\r\n        return self.results[\"lr\"][self._optimal_idx]", "code_tokens": ["def", "suggestion", "(", "self", ",", "skip_begin", ":", "int", "=", "10", ",", "skip_end", ":", "int", "=", "1", ")", "-", ">", "Optional", "[", "float", "]", ":", "STRING", "losses", "=", "torch", ".", "tensor", "(", "self", ".", "results", "[", "STRING", "]", "[", "skip_begin", ":", "-", "skip_end", "]", ")", "lrs", "=", "torch", ".", "tensor", "(", "self", ".", "results", "[", "STRING", "]", "[", "skip_begin", ":", "-", "skip_end", "]", ")", "is_finite", "=", "torch", ".", "isfinite", "(", "losses", ")", "losses", "=", "losses", "[", "is_finite", "]", "lrs", "=", "lrs", "[", "is_finite", "]", "if", "len", "(", "losses", ")", "<", "2", ":", "log", ".", "error", "(", "STRING", "STRING", ")", "self", ".", "_optimal_idx", "=", "None", "return", "None", "gradients", "=", "torch", ".", "gradient", "(", "losses", ",", "spacing", "=", "[", "lrs", "]", ")", "[", "0", "]", "#", "Compute", "the", "gradient", "of", "losses", "w", ".", "r", ".", "t", ".", "learning", "rates", "min_grad", "=", "torch", ".", "argmin", "(", "gradients", ")", ".", "item", "(", ")", "all_losses_idx", "=", "torch", ".", "arange", "(", "len", "(", "self", ".", "results", "[", "STRING", "]", ")", ")", "idx_non_skipped", "=", "all_losses_idx", "[", "skip_begin", ":", "-", "skip_end", "]", "idx_finite", "=", "idx_non_skipped", "[", "is_finite", "]", "self", ".", "_optimal_idx", "=", "idx_finite", "[", "min_grad", "]", ".", "item", "(", ")", "#", "type", ":", "ignore", "return", "self", ".", "results", "[", "STRING", "]", "[", "self", ".", "_optimal_idx", "]"], "docstring": "This will propose a suggestion for an initial learning rate based on the point with the steepest negative", "docstring_tokens": ["this", "will", "propose", "a", "suggestion", "for", "an", "initial", "learning", "rate", "based", "on", "the", "point", "with", "the", "steepest", "negative"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 161, "end_line": 195, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1004", "original_string": "def _lr_find(\r\n    trainer: \"pl.Trainer\",\r\n    model: \"pl.LightningModule\",\r\n    min_lr: float = 1e-8,\r\n    max_lr: float = 1,\r\n    num_training: int = 100,\r\n    mode: str = \"exponential\",\r\n    early_stop_threshold: Optional[float] = 4.0,\r\n    update_attr: bool = False,\r\n    attr_name: str = \"\",\r\n) -> Optional[_LRFinder]:\r\n    \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking\r\n    a good starting learning rate.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        model: Model to tune.\r\n        min_lr: minimum learning rate to investigate\r\n        max_lr: maximum learning rate to investigate\r\n        num_training: number of learning rates to test\r\n        mode: Search strategy to update learning rate after each batch:\r\n\r\n            - ``'exponential'``: Increases the learning rate exponentially.\r\n            - ``'linear'``: Increases the learning rate linearly.\r\n\r\n        early_stop_threshold: Threshold for stopping the search. If the\r\n            loss at any point is larger than early_stop_threshold*best_loss\r\n            then the search is stopped. To disable, set to None.\r\n        update_attr: Whether to update the learning rate attribute or not.\r\n        attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n            automatically detected. Otherwise, set the name here.\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping learning rate finder since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    if update_attr:\r\n        attr_name = _determine_lr_attr_name(model, attr_name)\r\n\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".lr_find_{uuid.uuid4()}.ckpt\")\r\n    ckpt_path = trainer.strategy.broadcast(ckpt_path)\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    start_steps = trainer.global_step\r\n\r\n    params = __lr_finder_dump_params(trainer)\r\n\r\n    __lr_finder_reset_params(trainer, num_training, early_stop_threshold)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    lr_finder = _LRFinder(mode, min_lr, max_lr, num_training)\r\n\r\n    lr_finder_finished = False\r\n    try:\r\n        lr_finder._exchange_scheduler(trainer)\r\n\r\n        _try_loop_run(trainer, params)\r\n\r\n        if trainer.global_step != num_training + start_steps:\r\n            log.info(f\"LR finder stopped early after {trainer.global_step} steps due to diverging loss.\")\r\n\r\n        lr_finder.results.update({\"lr\": trainer.callbacks[0].lrs, \"loss\": trainer.callbacks[0].losses})\r\n        lr_finder._total_batch_idx = trainer.fit_loop.total_batch_idx  # for debug purpose\r\n\r\n        __lr_finder_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        lr_finder.results = trainer.strategy.broadcast(lr_finder.results)\r\n        lr_finder_finished = True\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n        trainer.fit_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.val_loop._combined_loader = None\r\n        trainer.fit_loop._combined_loader = None  # reset data fetcher to avoid issues with the next fit\r\n        trainer.fit_loop.setup_data()\r\n\r\n    if update_attr and lr_finder_finished:\r\n        lr = lr_finder.suggestion()\r\n        if lr is not None:\r\n            lightning_setattr(model, attr_name, lr)\r\n            for opt in trainer.optimizers or []:\r\n                for pg in opt.param_groups:\r\n                    pg[\"lr\"] = lr\r\n            log.info(f\"Learning rate set to {lr}\")\r\n    return lr_finder", "language": "python", "code": "def _lr_find(\r\n    trainer: \"pl.Trainer\",\r\n    model: \"pl.LightningModule\",\r\n    min_lr: float = 1e-8,\r\n    max_lr: float = 1,\r\n    num_training: int = 100,\r\n    mode: str = \"exponential\",\r\n    early_stop_threshold: Optional[float] = 4.0,\r\n    update_attr: bool = False,\r\n    attr_name: str = \"\",\r\n) -> Optional[_LRFinder]:\r\n    \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking\r\n    a good starting learning rate.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        model: Model to tune.\r\n        min_lr: minimum learning rate to investigate\r\n        max_lr: maximum learning rate to investigate\r\n        num_training: number of learning rates to test\r\n        mode: Search strategy to update learning rate after each batch:\r\n\r\n            - ``'exponential'``: Increases the learning rate exponentially.\r\n            - ``'linear'``: Increases the learning rate linearly.\r\n\r\n        early_stop_threshold: Threshold for stopping the search. If the\r\n            loss at any point is larger than early_stop_threshold*best_loss\r\n            then the search is stopped. To disable, set to None.\r\n        update_attr: Whether to update the learning rate attribute or not.\r\n        attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n            automatically detected. Otherwise, set the name here.\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping learning rate finder since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    if update_attr:\r\n        attr_name = _determine_lr_attr_name(model, attr_name)\r\n\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".lr_find_{uuid.uuid4()}.ckpt\")\r\n    ckpt_path = trainer.strategy.broadcast(ckpt_path)\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    start_steps = trainer.global_step\r\n\r\n    params = __lr_finder_dump_params(trainer)\r\n\r\n    __lr_finder_reset_params(trainer, num_training, early_stop_threshold)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    lr_finder = _LRFinder(mode, min_lr, max_lr, num_training)\r\n\r\n    lr_finder_finished = False\r\n    try:\r\n        lr_finder._exchange_scheduler(trainer)\r\n\r\n        _try_loop_run(trainer, params)\r\n\r\n        if trainer.global_step != num_training + start_steps:\r\n            log.info(f\"LR finder stopped early after {trainer.global_step} steps due to diverging loss.\")\r\n\r\n        lr_finder.results.update({\"lr\": trainer.callbacks[0].lrs, \"loss\": trainer.callbacks[0].losses})\r\n        lr_finder._total_batch_idx = trainer.fit_loop.total_batch_idx  # for debug purpose\r\n\r\n        __lr_finder_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        lr_finder.results = trainer.strategy.broadcast(lr_finder.results)\r\n        lr_finder_finished = True\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n        trainer.fit_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.val_loop._combined_loader = None\r\n        trainer.fit_loop._combined_loader = None  # reset data fetcher to avoid issues with the next fit\r\n        trainer.fit_loop.setup_data()\r\n\r\n    if update_attr and lr_finder_finished:\r\n        lr = lr_finder.suggestion()\r\n        if lr is not None:\r\n            lightning_setattr(model, attr_name, lr)\r\n            for opt in trainer.optimizers or []:\r\n                for pg in opt.param_groups:\r\n                    pg[\"lr\"] = lr\r\n            log.info(f\"Learning rate set to {lr}\")\r\n    return lr_finder", "code_tokens": ["def", "_lr_find", "(", "trainer", ":", "STRING", ",", "model", ":", "STRING", ",", "min_lr", ":", "float", "=", "1e", "-", "8", ",", "max_lr", ":", "float", "=", "1", ",", "num_training", ":", "int", "=", "100", ",", "mode", ":", "str", "=", "STRING", ",", "early_stop_threshold", ":", "Optional", "[", "float", "]", "=", "4", ".", "0", ",", "update_attr", ":", "bool", "=", "False", ",", "attr_name", ":", "str", "=", "STRING", ",", ")", "-", ">", "Optional", "[", "_LRFinder", "]", ":", "STRING", "if", "trainer", ".", "fast_dev_run", ":", "rank_zero_warn", "(", "STRING", ")", "return", "None", "if", "update_attr", ":", "attr_name", "=", "_determine_lr_attr_name", "(", "model", ",", "attr_name", ")", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "fSTRING", ")", "ckpt_path", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "ckpt_path", ")", "trainer", ".", "save_checkpoint", "(", "ckpt_path", ")", "start_steps", "=", "trainer", ".", "global_step", "params", "=", "__lr_finder_dump_params", "(", "trainer", ")", "__lr_finder_reset_params", "(", "trainer", ",", "num_training", ",", "early_stop_threshold", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "disable", "(", ")", "lr_finder", "=", "_LRFinder", "(", "mode", ",", "min_lr", ",", "max_lr", ",", "num_training", ")", "lr_finder_finished", "=", "False", "try", ":", "lr_finder", ".", "_exchange_scheduler", "(", "trainer", ")", "_try_loop_run", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "global_step", "!", "=", "num_training", "+", "start_steps", ":", "log", ".", "info", "(", "fSTRING", ")", "lr_finder", ".", "results", ".", "update", "(", "{", "STRING", ":", "trainer", ".", "callbacks", "[", "0", "]", ".", "lrs", ",", "STRING", ":", "trainer", ".", "callbacks", "[", "0", "]", ".", "losses", "}", ")", "lr_finder", ".", "_total_batch_idx", "=", "trainer", ".", "fit_loop", ".", "total_batch_idx", "#", "for", "debug", "purpose", "__lr_finder_restore_params", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "enable", "(", ")", "lr_finder", ".", "results", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "lr_finder", ".", "results", ")", "lr_finder_finished", "=", "True", "except", "Exception", "as", "ex", ":", "raise", "ex", "finally", ":", "trainer", ".", "_checkpoint_connector", ".", "restore", "(", "ckpt_path", ")", "trainer", ".", "strategy", ".", "remove_checkpoint", "(", "ckpt_path", ")", "trainer", ".", "fit_loop", ".", "restarting", "=", "False", "#", "reset", "restarting", "flag", "as", "checkpoint", "restoring", "sets", "it", "to", "True", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "restarting", "=", "False", "#", "reset", "restarting", "flag", "as", "checkpoint", "restoring", "sets", "it", "to", "True", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_combined_loader", "=", "None", "trainer", ".", "fit_loop", ".", "_combined_loader", "=", "None", "#", "reset", "data", "fetcher", "to", "avoid", "issues", "with", "the", "next", "fit", "trainer", ".", "fit_loop", ".", "setup_data", "(", ")", "if", "update_attr", "and", "lr_finder_finished", ":", "lr", "=", "lr_finder", ".", "suggestion", "(", ")", "if", "lr", "is", "not", "None", ":", "lightning_setattr", "(", "model", ",", "attr_name", ",", "lr", ")", "for", "opt", "in", "trainer", ".", "optimizers", "or", "[", "]", ":", "for", "pg", "in", "opt", ".", "param_groups", ":", "pg", "[", "STRING", "]", "=", "lr", "log", ".", "info", "(", "fSTRING", ")", "return", "lr_finder"], "docstring": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking", "docstring_tokens": ["enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", "to", "reduce", "the", "amount", "of", "guesswork", "in", "picking"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 198, "end_line": 307, "has_examples": false, "num_comments": 15, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1005", "original_string": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called before each training batch, logs the lr that will be used.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if self.progress_bar_refresh_rate and self.progress_bar is None:\r\n            self.progress_bar = tqdm(desc=\"Finding best initial lr\", total=self.num_training)\r\n\r\n        self.lrs.append(trainer.lr_scheduler_configs[0].scheduler.lr[0])  # type: ignore[union-attr]\r", "language": "python", "code": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called before each training batch, logs the lr that will be used.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if self.progress_bar_refresh_rate and self.progress_bar is None:\r\n            self.progress_bar = tqdm(desc=\"Finding best initial lr\", total=self.num_training)\r\n\r\n        self.lrs.append(trainer.lr_scheduler_configs[0].scheduler.lr[0])  # type: ignore[union-attr]\r", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING", "if", "(", "trainer", ".", "fit_loop", ".", "batch_idx", "+", "1", ")", "%", "trainer", ".", "accumulate_grad_batches", "!", "=", "0", ":", "return", "if", "self", ".", "progress_bar_refresh_rate", "and", "self", ".", "progress_bar", "is", "None", ":", "self", ".", "progress_bar", "=", "tqdm", "(", "desc", "=", "STRING", ",", "total", "=", "self", ".", "num_training", ")", "self", ".", "lrs", ".", "append", "(", "trainer", ".", "lr_scheduler_configs", "[", "0", "]", ".", "scheduler", ".", "lr", "[", "0", "]", ")", "#", "type", ":", "ignore", "[", "union", "-", "attr", "]"], "docstring": "Called before each training batch, logs the lr that will be used.", "docstring_tokens": ["called", "before", "each", "training", "batch", "logs", "the", "lr", "that", "will", "be", "used"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 384, "end_line": 394, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "function_1006", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the training batch ends, logs the calculated loss.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if not outputs:\r\n            self.losses.append(float(\"nan\"))\r\n            return\r\n\r\n        if self.progress_bar:\r\n            self.progress_bar.update()\r\n\r\n        loss_tensor = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n        assert loss_tensor is not None\r\n        current_loss = loss_tensor.item()\r\n        current_step = trainer.global_step\r\n\r\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * current_loss\r\n        smoothed_loss = self.avg_loss / (1 - self.beta ** (current_step + 1))\r\n\r\n        if (\r\n            self.early_stop_threshold is not None\r\n            and current_step > 1\r\n            and smoothed_loss > self.early_stop_threshold * self.best_loss\r\n        ):\r\n            trainer.should_stop = True  # stop signal\r\n            if self.progress_bar:\r\n                self.progress_bar.close()\r\n\r\n        trainer.should_stop = trainer.strategy.broadcast(trainer.should_stop)\r\n\r\n        if smoothed_loss < self.best_loss or current_step == 1:\r\n            self.best_loss = smoothed_loss\r\n\r\n        self.losses.append(smoothed_loss)", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the training batch ends, logs the calculated loss.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if not outputs:\r\n            self.losses.append(float(\"nan\"))\r\n            return\r\n\r\n        if self.progress_bar:\r\n            self.progress_bar.update()\r\n\r\n        loss_tensor = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n        assert loss_tensor is not None\r\n        current_loss = loss_tensor.item()\r\n        current_step = trainer.global_step\r\n\r\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * current_loss\r\n        smoothed_loss = self.avg_loss / (1 - self.beta ** (current_step + 1))\r\n\r\n        if (\r\n            self.early_stop_threshold is not None\r\n            and current_step > 1\r\n            and smoothed_loss > self.early_stop_threshold * self.best_loss\r\n        ):\r\n            trainer.should_stop = True  # stop signal\r\n            if self.progress_bar:\r\n                self.progress_bar.close()\r\n\r\n        trainer.should_stop = trainer.strategy.broadcast(trainer.should_stop)\r\n\r\n        if smoothed_loss < self.best_loss or current_step == 1:\r\n            self.best_loss = smoothed_loss\r\n\r\n        self.losses.append(smoothed_loss)", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "STRING", ",", "pl_module", ":", "STRING", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "STRING", "if", "(", "trainer", ".", "fit_loop", ".", "batch_idx", "+", "1", ")", "%", "trainer", ".", "accumulate_grad_batches", "!", "=", "0", ":", "return", "if", "not", "outputs", ":", "self", ".", "losses", ".", "append", "(", "float", "(", "STRING", ")", ")", "return", "if", "self", ".", "progress_bar", ":", "self", ".", "progress_bar", ".", "update", "(", ")", "loss_tensor", "=", "outputs", "if", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", "else", "outputs", "[", "STRING", "]", "assert", "loss_tensor", "is", "not", "None", "current_loss", "=", "loss_tensor", ".", "item", "(", ")", "current_step", "=", "trainer", ".", "global_step", "self", ".", "avg_loss", "=", "self", ".", "beta", "*", "self", ".", "avg_loss", "+", "(", "1", "-", "self", ".", "beta", ")", "*", "current_loss", "smoothed_loss", "=", "self", ".", "avg_loss", "/", "(", "1", "-", "self", ".", "beta", "*", "*", "(", "current_step", "+", "1", ")", ")", "if", "(", "self", ".", "early_stop_threshold", "is", "not", "None", "and", "current_step", ">", "1", "and", "smoothed_loss", ">", "self", ".", "early_stop_threshold", "*", "self", ".", "best_loss", ")", ":", "trainer", ".", "should_stop", "=", "True", "#", "stop", "signal", "if", "self", ".", "progress_bar", ":", "self", ".", "progress_bar", ".", "close", "(", ")", "trainer", ".", "should_stop", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "trainer", ".", "should_stop", ")", "if", "smoothed_loss", "<", "self", ".", "best_loss", "or", "current_step", "=", "=", "1", ":", "self", ".", "best_loss", "=", "smoothed_loss", "self", ".", "losses", ".", "append", "(", "smoothed_loss", ")"], "docstring": "Called when the training batch ends, logs the calculated loss.", "docstring_tokens": ["called", "when", "the", "training", "batch", "ends", "logs", "the", "calculated", "loss"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "start_line": 397, "end_line": 439, "has_examples": false, "num_comments": 5, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\tuning.py", "func_name": "function_1007", "original_string": "def scale_batch_size(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        mode: str = \"power\",\r\n        steps_per_trial: int = 3,\r\n        init_val: int = 2,\r\n        max_trials: int = 25,\r\n        batch_arg_name: str = \"batch_size\",\r\n    ) -> Optional[int]:\r\n        \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n        error.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            mode: Search strategy to update the batch size:\r\n\r\n                - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n                - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                    do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n            steps_per_trial: number of steps to run with a given batch size.\r\n                Ideally 1 should be enough to test if an OOM error occurs,\r\n                however in practise a few are needed\r\n            init_val: initial batch size to start the search with\r\n            max_trials: max number of increases in batch size done before\r\n               algorithm is terminated\r\n            batch_arg_name: name of the attribute that stores the batch size.\r\n                It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n                with that name. We will look for this attribute name in the following places\r\n\r\n                - ``model``\r\n                - ``model.hparams``\r\n                - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n        \"\"\"\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_scale_batch_size_configuration(self._trainer)\r\n\r\n        from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\r\n\r\n        batch_size_finder: Callback = BatchSizeFinder(\r\n            mode=mode,\r\n            steps_per_trial=steps_per_trial,\r\n            init_val=init_val,\r\n            max_trials=max_trials,\r\n            batch_arg_name=batch_arg_name,\r\n        )\r\n        batch_size_finder._early_exit = True\r\n        self._trainer.callbacks = [batch_size_finder] + self._trainer.callbacks\r\n\r\n        if method == \"fit\":\r\n            self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n        elif method == \"validate\":\r\n            self._trainer.validate(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"test\":\r\n            self._trainer.test(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"predict\":\r\n            self._trainer.predict(model, dataloaders, datamodule=datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not batch_size_finder]\r\n        return batch_size_finder.optimal_batch_size", "language": "python", "code": "def scale_batch_size(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        mode: str = \"power\",\r\n        steps_per_trial: int = 3,\r\n        init_val: int = 2,\r\n        max_trials: int = 25,\r\n        batch_arg_name: str = \"batch_size\",\r\n    ) -> Optional[int]:\r\n        \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n        error.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            mode: Search strategy to update the batch size:\r\n\r\n                - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n                - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                    do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n            steps_per_trial: number of steps to run with a given batch size.\r\n                Ideally 1 should be enough to test if an OOM error occurs,\r\n                however in practise a few are needed\r\n            init_val: initial batch size to start the search with\r\n            max_trials: max number of increases in batch size done before\r\n               algorithm is terminated\r\n            batch_arg_name: name of the attribute that stores the batch size.\r\n                It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n                with that name. We will look for this attribute name in the following places\r\n\r\n                - ``model``\r\n                - ``model.hparams``\r\n                - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n        \"\"\"\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_scale_batch_size_configuration(self._trainer)\r\n\r\n        from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\r\n\r\n        batch_size_finder: Callback = BatchSizeFinder(\r\n            mode=mode,\r\n            steps_per_trial=steps_per_trial,\r\n            init_val=init_val,\r\n            max_trials=max_trials,\r\n            batch_arg_name=batch_arg_name,\r\n        )\r\n        batch_size_finder._early_exit = True\r\n        self._trainer.callbacks = [batch_size_finder] + self._trainer.callbacks\r\n\r\n        if method == \"fit\":\r\n            self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n        elif method == \"validate\":\r\n            self._trainer.validate(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"test\":\r\n            self._trainer.test(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"predict\":\r\n            self._trainer.predict(model, dataloaders, datamodule=datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not batch_size_finder]\r\n        return batch_size_finder.optimal_batch_size", "code_tokens": ["def", "scale_batch_size", "(", "self", ",", "model", ":", "STRING", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "STRING", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "method", ":", "Literal", "[", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", "]", "=", "STRING", ",", "mode", ":", "str", "=", "STRING", ",", "steps_per_trial", ":", "int", "=", "3", ",", "init_val", ":", "int", "=", "2", ",", "max_trials", ":", "int", "=", "25", ",", "batch_arg_name", ":", "str", "=", "STRING", ",", ")", "-", ">", "Optional", "[", "int", "]", ":", "STRING", "_check_tuner_configuration", "(", "train_dataloaders", ",", "val_dataloaders", ",", "dataloaders", ",", "method", ")", "_check_scale_batch_size_configuration", "(", "self", ".", "_trainer", ")", "from", "lightning", ".", "pytorch", ".", "callbacks", ".", "batch_size_finder", "import", "BatchSizeFinder", "batch_size_finder", ":", "Callback", "=", "BatchSizeFinder", "(", "mode", "=", "mode", ",", "steps_per_trial", "=", "steps_per_trial", ",", "init_val", "=", "init_val", ",", "max_trials", "=", "max_trials", ",", "batch_arg_name", "=", "batch_arg_name", ",", ")", "batch_size_finder", ".", "_early_exit", "=", "True", "self", ".", "_trainer", ".", "callbacks", "=", "[", "batch_size_finder", "]", "+", "self", ".", "_trainer", ".", "callbacks", "if", "method", "=", "=", "STRING", ":", "self", ".", "_trainer", ".", "fit", "(", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ")", "elif", "method", "=", "=", "STRING", ":", "self", ".", "_trainer", ".", "validate", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "elif", "method", "=", "=", "STRING", ":", "self", ".", "_trainer", ".", "test", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "elif", "method", "=", "=", "STRING", ":", "self", ".", "_trainer", ".", "predict", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "self", ".", "_trainer", ".", "callbacks", "=", "[", "cb", "for", "cb", "in", "self", ".", "_trainer", ".", "callbacks", "if", "cb", "is", "not", "batch_size_finder", "]", "return", "batch_size_finder", ".", "optimal_batch_size"], "docstring": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)", "docstring_tokens": ["iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "oom"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\tuning.py", "start_line": 30, "end_line": 105, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\tuning.py", "func_name": "function_1008", "original_string": "def lr_find(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        min_lr: float = 1e-8,\r\n        max_lr: float = 1,\r\n        num_training: int = 100,\r\n        mode: str = \"exponential\",\r\n        early_stop_threshold: Optional[float] = 4.0,\r\n        update_attr: bool = True,\r\n        attr_name: str = \"\",\r\n    ) -> Optional[\"_LRFinder\"]:\r\n        \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\r\n        picking a good starting learning rate.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            min_lr: minimum learning rate to investigate\r\n            max_lr: maximum learning rate to investigate\r\n            num_training: number of learning rates to test\r\n            mode: Search strategy to update learning rate after each batch:\r\n\r\n                - ``'exponential'``: Increases the learning rate exponentially.\r\n                - ``'linear'``: Increases the learning rate linearly.\r\n\r\n            early_stop_threshold: Threshold for stopping the search. If the\r\n                loss at any point is larger than early_stop_threshold*best_loss\r\n                then the search is stopped. To disable, set to None.\r\n            update_attr: Whether to update the learning rate attribute or not.\r\n            attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n                automatically detected. Otherwise, set the name here.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If learning rate/lr in ``model`` or ``model.hparams`` isn't overridden,\r\n                or if you are using more than one optimizer.\r\n\r\n        \"\"\"\r\n        if method != \"fit\":\r\n            raise MisconfigurationException(\"method='fit' is the only valid configuration to run lr finder.\")\r\n\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_lr_find_configuration(self._trainer)\r\n\r\n        from lightning.pytorch.callbacks.lr_finder import LearningRateFinder\r\n\r\n        lr_finder_callback: Callback = LearningRateFinder(\r\n            min_lr=min_lr,\r\n            max_lr=max_lr,\r\n            num_training_steps=num_training,\r\n            mode=mode,\r\n            early_stop_threshold=early_stop_threshold,\r\n            update_attr=update_attr,\r\n            attr_name=attr_name,\r\n        )\r\n\r\n        lr_finder_callback._early_exit = True\r\n        self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\r\n\r\n        self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\r\n\r\n        return lr_finder_callback.optimal_lr", "language": "python", "code": "def lr_find(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        min_lr: float = 1e-8,\r\n        max_lr: float = 1,\r\n        num_training: int = 100,\r\n        mode: str = \"exponential\",\r\n        early_stop_threshold: Optional[float] = 4.0,\r\n        update_attr: bool = True,\r\n        attr_name: str = \"\",\r\n    ) -> Optional[\"_LRFinder\"]:\r\n        \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\r\n        picking a good starting learning rate.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            min_lr: minimum learning rate to investigate\r\n            max_lr: maximum learning rate to investigate\r\n            num_training: number of learning rates to test\r\n            mode: Search strategy to update learning rate after each batch:\r\n\r\n                - ``'exponential'``: Increases the learning rate exponentially.\r\n                - ``'linear'``: Increases the learning rate linearly.\r\n\r\n            early_stop_threshold: Threshold for stopping the search. If the\r\n                loss at any point is larger than early_stop_threshold*best_loss\r\n                then the search is stopped. To disable, set to None.\r\n            update_attr: Whether to update the learning rate attribute or not.\r\n            attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n                automatically detected. Otherwise, set the name here.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If learning rate/lr in ``model`` or ``model.hparams`` isn't overridden,\r\n                or if you are using more than one optimizer.\r\n\r\n        \"\"\"\r\n        if method != \"fit\":\r\n            raise MisconfigurationException(\"method='fit' is the only valid configuration to run lr finder.\")\r\n\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_lr_find_configuration(self._trainer)\r\n\r\n        from lightning.pytorch.callbacks.lr_finder import LearningRateFinder\r\n\r\n        lr_finder_callback: Callback = LearningRateFinder(\r\n            min_lr=min_lr,\r\n            max_lr=max_lr,\r\n            num_training_steps=num_training,\r\n            mode=mode,\r\n            early_stop_threshold=early_stop_threshold,\r\n            update_attr=update_attr,\r\n            attr_name=attr_name,\r\n        )\r\n\r\n        lr_finder_callback._early_exit = True\r\n        self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\r\n\r\n        self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\r\n\r\n        return lr_finder_callback.optimal_lr", "code_tokens": ["def", "lr_find", "(", "self", ",", "model", ":", "STRING", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "STRING", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "STRING", "]", "=", "None", ",", "method", ":", "Literal", "[", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", "]", "=", "STRING", ",", "min_lr", ":", "float", "=", "1e", "-", "8", ",", "max_lr", ":", "float", "=", "1", ",", "num_training", ":", "int", "=", "100", ",", "mode", ":", "str", "=", "STRING", ",", "early_stop_threshold", ":", "Optional", "[", "float", "]", "=", "4", ".", "0", ",", "update_attr", ":", "bool", "=", "True", ",", "attr_name", ":", "str", "=", "STRING", ",", ")", "-", ">", "Optional", "[", "STRING", "]", ":", "STRING", "if", "method", "!", "=", "STRING", ":", "raise", "MisconfigurationException", "(", "STRING", ")", "_check_tuner_configuration", "(", "train_dataloaders", ",", "val_dataloaders", ",", "dataloaders", ",", "method", ")", "_check_lr_find_configuration", "(", "self", ".", "_trainer", ")", "from", "lightning", ".", "pytorch", ".", "callbacks", ".", "lr_finder", "import", "LearningRateFinder", "lr_finder_callback", ":", "Callback", "=", "LearningRateFinder", "(", "min_lr", "=", "min_lr", ",", "max_lr", "=", "max_lr", ",", "num_training_steps", "=", "num_training", ",", "mode", "=", "mode", ",", "early_stop_threshold", "=", "early_stop_threshold", ",", "update_attr", "=", "update_attr", ",", "attr_name", "=", "attr_name", ",", ")", "lr_finder_callback", ".", "_early_exit", "=", "True", "self", ".", "_trainer", ".", "callbacks", "=", "[", "lr_finder_callback", "]", "+", "self", ".", "_trainer", ".", "callbacks", "self", ".", "_trainer", ".", "fit", "(", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ")", "self", ".", "_trainer", ".", "callbacks", "=", "[", "cb", "for", "cb", "in", "self", ".", "_trainer", ".", "callbacks", "if", "cb", "is", "not", "lr_finder_callback", "]", "return", "lr_finder_callback", ".", "optimal_lr"], "docstring": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in", "docstring_tokens": ["enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", "to", "reduce", "the", "amount", "of", "guesswork", "in"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\tuner\\tuning.py", "start_line": 107, "end_line": 183, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\argparse.py", "func_name": "function_1009", "original_string": "def _parse_env_variables(cls: type, template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\r\n    \"\"\"Parse environment arguments if they are defined.\r\n\r\n    Examples:\r\n\r\n        >>> from lightning.pytorch import Trainer\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace()\r\n        >>> import os\r\n        >>> os.environ[\"PL_TRAINER_DEVICES\"] = '42'\r\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace(devices=42)\r\n        >>> del os.environ[\"PL_TRAINER_DEVICES\"]\r\n\r\n    \"\"\"\r\n    env_args = {}\r\n    for arg_name in inspect.signature(cls).parameters:\r\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\r\n        val = os.environ.get(env)\r\n        if not (val is None or val == \"\"):\r\n            with suppress(Exception):\r\n                val = literal_eval(val)\r\n            env_args[arg_name] = val\r\n    return Namespace(**env_args)", "language": "python", "code": "def _parse_env_variables(cls: type, template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\r\n    \"\"\"Parse environment arguments if they are defined.\r\n\r\n    Examples:\r\n\r\n        >>> from lightning.pytorch import Trainer\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace()\r\n        >>> import os\r\n        >>> os.environ[\"PL_TRAINER_DEVICES\"] = '42'\r\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace(devices=42)\r\n        >>> del os.environ[\"PL_TRAINER_DEVICES\"]\r\n\r\n    \"\"\"\r\n    env_args = {}\r\n    for arg_name in inspect.signature(cls).parameters:\r\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\r\n        val = os.environ.get(env)\r\n        if not (val is None or val == \"\"):\r\n            with suppress(Exception):\r\n                val = literal_eval(val)\r\n            env_args[arg_name] = val\r\n    return Namespace(**env_args)", "code_tokens": ["def", "_parse_env_variables", "(", "cls", ":", "type", ",", "template", ":", "str", "=", "STRING", ")", "-", ">", "Namespace", ":", "STRING", "env_args", "=", "{", "}", "for", "arg_name", "in", "inspect", ".", "signature", "(", "cls", ")", ".", "parameters", ":", "env", "=", "template", "%", "{", "STRING", ":", "cls", ".", "__name__", ".", "upper", "(", ")", ",", "STRING", ":", "arg_name", ".", "upper", "(", ")", "}", "val", "=", "os", ".", "environ", ".", "get", "(", "env", ")", "if", "not", "(", "val", "is", "None", "or", "val", "=", "=", "STRING", ")", ":", "with", "suppress", "(", "Exception", ")", ":", "val", "=", "literal_eval", "(", "val", ")", "env_args", "[", "arg_name", "]", "=", "val", "return", "Namespace", "(", "*", "*", "env_args", ")"], "docstring": "Parse environment arguments if they are defined.", "docstring_tokens": ["parse", "environment", "arguments", "if", "they", "are", "defined"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\argparse.py", "start_line": 26, "end_line": 52, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1010", "original_string": "def iterables(self) -> Any:\r\n        \"\"\"Return the original collection of iterables.\"\"\"\r\n        return self._iterables", "language": "python", "code": "def iterables(self) -> Any:\r\n        \"\"\"Return the original collection of iterables.\"\"\"\r\n        return self._iterables", "code_tokens": ["def", "iterables", "(", "self", ")", "-", ">", "Any", ":", "STRING", "return", "self", ".", "_iterables"], "docstring": "Return the original collection of iterables.", "docstring_tokens": ["return", "the", "original", "collection", "of", "iterables"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 293, "end_line": 295, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1011", "original_string": "def sampler(self) -> Any:\r\n        \"\"\"Return a collections of samplers extracted from iterables.\"\"\"\r\n        return _map_and_unflatten(lambda x: getattr(x, \"sampler\", None), self.flattened, self._spec)", "language": "python", "code": "def sampler(self) -> Any:\r\n        \"\"\"Return a collections of samplers extracted from iterables.\"\"\"\r\n        return _map_and_unflatten(lambda x: getattr(x, \"sampler\", None), self.flattened, self._spec)", "code_tokens": ["def", "sampler", "(", "self", ")", "-", ">", "Any", ":", "STRING", "return", "_map_and_unflatten", "(", "lambda", "x", ":", "getattr", "(", "x", ",", "STRING", ",", "None", ")", ",", "self", ".", "flattened", ",", "self", ".", "_spec", ")"], "docstring": "Return a collections of samplers extracted from iterables.", "docstring_tokens": ["return", "a", "collections", "of", "samplers", "extracted", "from", "iterables"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 298, "end_line": 300, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1012", "original_string": "def batch_sampler(self) -> Any:\r\n        \"\"\"Return a collections of batch samplers extracted from iterables.\"\"\"\r\n        return _map_and_unflatten(lambda x: getattr(x, \"batch_sampler\", None), self.flattened, self._spec)", "language": "python", "code": "def batch_sampler(self) -> Any:\r\n        \"\"\"Return a collections of batch samplers extracted from iterables.\"\"\"\r\n        return _map_and_unflatten(lambda x: getattr(x, \"batch_sampler\", None), self.flattened, self._spec)", "code_tokens": ["def", "batch_sampler", "(", "self", ")", "-", ">", "Any", ":", "STRING", "return", "_map_and_unflatten", "(", "lambda", "x", ":", "getattr", "(", "x", ",", "STRING", ",", "None", ")", ",", "self", ".", "flattened", ",", "self", ".", "_spec", ")"], "docstring": "Return a collections of batch samplers extracted from iterables.", "docstring_tokens": ["return", "a", "collections", "of", "batch", "samplers", "extracted", "from", "iterables"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 303, "end_line": 305, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1013", "original_string": "def flattened(self) -> list[Any]:\r\n        \"\"\"Return the flat list of iterables.\"\"\"\r\n        return self._flattened", "language": "python", "code": "def flattened(self) -> list[Any]:\r\n        \"\"\"Return the flat list of iterables.\"\"\"\r\n        return self._flattened", "code_tokens": ["def", "flattened", "(", "self", ")", "-", ">", "list", "[", "Any", "]", ":", "STRING", "return", "self", ".", "_flattened"], "docstring": "Return the flat list of iterables.", "docstring_tokens": ["return", "the", "flat", "list", "of", "iterables"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 308, "end_line": 310, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1014", "original_string": "def flattened(self, flattened: list[Any]) -> None:\r\n        \"\"\"Setter to conveniently update the list of iterables.\"\"\"\r\n        if len(flattened) != len(self._flattened):\r\n            raise ValueError(\r\n                f\"Mismatch in flattened length ({len(flattened)}) and existing length ({len(self._flattened)})\"\r\n            )\r\n        self._iterables = tree_unflatten(flattened, self._spec)\r\n        self._flattened = flattened", "language": "python", "code": "def flattened(self, flattened: list[Any]) -> None:\r\n        \"\"\"Setter to conveniently update the list of iterables.\"\"\"\r\n        if len(flattened) != len(self._flattened):\r\n            raise ValueError(\r\n                f\"Mismatch in flattened length ({len(flattened)}) and existing length ({len(self._flattened)})\"\r\n            )\r\n        self._iterables = tree_unflatten(flattened, self._spec)\r\n        self._flattened = flattened", "code_tokens": ["def", "flattened", "(", "self", ",", "flattened", ":", "list", "[", "Any", "]", ")", "-", ">", "None", ":", "STRING", "if", "len", "(", "flattened", ")", "!", "=", "len", "(", "self", ".", "_flattened", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "self", ".", "_iterables", "=", "tree_unflatten", "(", "flattened", ",", "self", ".", "_spec", ")", "self", ".", "_flattened", "=", "flattened"], "docstring": "Setter to conveniently update the list of iterables.", "docstring_tokens": ["setter", "to", "conveniently", "update", "the", "list", "of", "iterables"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 313, "end_line": 321, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1015", "original_string": "def limits(self) -> Optional[list[Union[int, float]]]:\r\n        \"\"\"Optional limits per iterator.\"\"\"\r\n        return self._limits", "language": "python", "code": "def limits(self) -> Optional[list[Union[int, float]]]:\r\n        \"\"\"Optional limits per iterator.\"\"\"\r\n        return self._limits", "code_tokens": ["def", "limits", "(", "self", ")", "-", ">", "Optional", "[", "list", "[", "Union", "[", "int", ",", "float", "]", "]", "]", ":", "STRING", "return", "self", ".", "_limits"], "docstring": "Optional limits per iterator.", "docstring_tokens": ["optional", "limits", "per", "iterator"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 324, "end_line": 326, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1016", "original_string": "def __len__(self) -> int:\r\n        \"\"\"Compute the number of batches.\"\"\"\r\n        if self._iterator is None:\r\n            raise RuntimeError(\"Please call `iter(combined_loader)` first.\")\r\n        return len(self._iterator)", "language": "python", "code": "def __len__(self) -> int:\r\n        \"\"\"Compute the number of batches.\"\"\"\r\n        if self._iterator is None:\r\n            raise RuntimeError(\"Please call `iter(combined_loader)` first.\")\r\n        return len(self._iterator)", "code_tokens": ["def", "__len__", "(", "self", ")", "-", ">", "int", ":", "STRING", "if", "self", ".", "_iterator", "is", "None", ":", "raise", "RuntimeError", "(", "STRING", ")", "return", "len", "(", "self", ".", "_iterator", ")"], "docstring": "Compute the number of batches.", "docstring_tokens": ["compute", "the", "number", "of", "batches"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 354, "end_line": 358, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1017", "original_string": "def reset(self) -> None:\r\n        \"\"\"Reset the state and shutdown any workers.\"\"\"\r\n        if self._iterator is not None:\r\n            self._iterator.reset()\r\n            self._iterator = None\r\n        for iterable in self.flattened:\r\n            _shutdown_workers_and_reset_iterator(iterable)", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Reset the state and shutdown any workers.\"\"\"\r\n        if self._iterator is not None:\r\n            self._iterator.reset()\r\n            self._iterator = None\r\n        for iterable in self.flattened:\r\n            _shutdown_workers_and_reset_iterator(iterable)", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_iterator", "is", "not", "None", ":", "self", ".", "_iterator", ".", "reset", "(", ")", "self", ".", "_iterator", "=", "None", "for", "iterable", "in", "self", ".", "flattened", ":", "_shutdown_workers_and_reset_iterator", "(", "iterable", ")"], "docstring": "Reset the state and shutdown any workers.", "docstring_tokens": ["reset", "the", "state", "and", "shutdown", "any", "workers"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 360, "end_line": 366, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1018", "original_string": "def _dataset_length(self) -> int:\r\n        \"\"\"Compute the total length of the datasets according to the current mode.\"\"\"\r\n        datasets = [getattr(dl, \"dataset\", None) for dl in self.flattened]\r\n        lengths = [length for ds in datasets if (length := sized_len(ds)) is not None]\r\n        if not lengths:\r\n            raise NotImplementedError(\"All datasets are iterable-style datasets.\")\r\n        fn = _SUPPORTED_MODES[self._mode][\"fn\"]\r\n        return fn(lengths)", "language": "python", "code": "def _dataset_length(self) -> int:\r\n        \"\"\"Compute the total length of the datasets according to the current mode.\"\"\"\r\n        datasets = [getattr(dl, \"dataset\", None) for dl in self.flattened]\r\n        lengths = [length for ds in datasets if (length := sized_len(ds)) is not None]\r\n        if not lengths:\r\n            raise NotImplementedError(\"All datasets are iterable-style datasets.\")\r\n        fn = _SUPPORTED_MODES[self._mode][\"fn\"]\r\n        return fn(lengths)", "code_tokens": ["def", "_dataset_length", "(", "self", ")", "-", ">", "int", ":", "STRING", "datasets", "=", "[", "getattr", "(", "dl", ",", "STRING", ",", "None", ")", "for", "dl", "in", "self", ".", "flattened", "]", "lengths", "=", "[", "length", "for", "ds", "in", "datasets", "if", "(", "length", ":", "=", "sized_len", "(", "ds", ")", ")", "is", "not", "None", "]", "if", "not", "lengths", ":", "raise", "NotImplementedError", "(", "STRING", ")", "fn", "=", "_SUPPORTED_MODES", "[", "self", ".", "_mode", "]", "[", "STRING", "]", "return", "fn", "(", "lengths", ")"], "docstring": "Compute the total length of the datasets according to the current mode.", "docstring_tokens": ["compute", "the", "total", "length", "of", "the", "datasets", "according", "to", "the", "current", "mode"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 368, "end_line": 375, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1019", "original_string": "def _state_dicts(self) -> list[dict[str, Any]]:\r\n        \"\"\"Returns the list of state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        return [loader.state_dict() for loader in self.flattened if isinstance(loader, _Stateful)]", "language": "python", "code": "def _state_dicts(self) -> list[dict[str, Any]]:\r\n        \"\"\"Returns the list of state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        return [loader.state_dict() for loader in self.flattened if isinstance(loader, _Stateful)]", "code_tokens": ["def", "_state_dicts", "(", "self", ")", "-", ">", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ":", "STRING", "return", "[", "loader", ".", "state_dict", "(", ")", "for", "loader", "in", "self", ".", "flattened", "if", "isinstance", "(", "loader", ",", "_Stateful", ")", "]"], "docstring": "Returns the list of state dicts for iterables in `self.flattened` that are stateful.", "docstring_tokens": ["returns", "the", "list", "of", "state", "dicts", "for", "iterables", "in", "self", "flattened", "that", "are", "stateful"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 377, "end_line": 379, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "function_1020", "original_string": "def _load_state_dicts(self, states: list[dict[str, Any]]) -> None:\r\n        \"\"\"Loads the state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        if not states:\r\n            return\r\n        stateful_loaders = [loader for loader in self.flattened if isinstance(loader, _Stateful)]\r\n        if len(stateful_loaders) != len(states):\r\n            raise RuntimeError(\r\n                f\"The CombinedLoader has {len(stateful_loaders)} stateful loaders, but found {len(states)} states\"\r\n                \" in the checkpoint. Please make sure you define the same dataloaders that were used when saving\"\r\n                \" the checkpoint.\"\r\n            )\r\n        for loader, state_dict in zip(stateful_loaders, states):\r\n            loader.load_state_dict(state_dict)", "language": "python", "code": "def _load_state_dicts(self, states: list[dict[str, Any]]) -> None:\r\n        \"\"\"Loads the state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        if not states:\r\n            return\r\n        stateful_loaders = [loader for loader in self.flattened if isinstance(loader, _Stateful)]\r\n        if len(stateful_loaders) != len(states):\r\n            raise RuntimeError(\r\n                f\"The CombinedLoader has {len(stateful_loaders)} stateful loaders, but found {len(states)} states\"\r\n                \" in the checkpoint. Please make sure you define the same dataloaders that were used when saving\"\r\n                \" the checkpoint.\"\r\n            )\r\n        for loader, state_dict in zip(stateful_loaders, states):\r\n            loader.load_state_dict(state_dict)", "code_tokens": ["def", "_load_state_dicts", "(", "self", ",", "states", ":", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ")", "-", ">", "None", ":", "STRING", "if", "not", "states", ":", "return", "stateful_loaders", "=", "[", "loader", "for", "loader", "in", "self", ".", "flattened", "if", "isinstance", "(", "loader", ",", "_Stateful", ")", "]", "if", "len", "(", "stateful_loaders", ")", "!", "=", "len", "(", "states", ")", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", "STRING", ")", "for", "loader", ",", "state_dict", "in", "zip", "(", "stateful_loaders", ",", "states", ")", ":", "loader", ".", "load_state_dict", "(", "state_dict", ")"], "docstring": "Loads the state dicts for iterables in `self.flattened` that are stateful.", "docstring_tokens": ["loads", "the", "state", "dicts", "for", "iterables", "in", "self", "flattened", "that", "are", "stateful"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "start_line": 381, "end_line": 393, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\compile.py", "func_name": "function_1021", "original_string": "def from_compiled(model: OptimizedModule) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance LightningModule from the output of ``torch.compile``.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    The ``torch.compile`` function returns a ``torch._dynamo.OptimizedModule``, which wraps the LightningModule\r\n    passed in as an argument, but doesn't inherit from it. This means that the output of ``torch.compile`` behaves\r\n    like a LightningModule, but it doesn't inherit from it (i.e. `isinstance` will fail).\r\n\r\n    Use this method to obtain a LightningModule that still runs with all the optimizations from ``torch.compile``.\r\n\r\n    \"\"\"\r\n    if not isinstance(model, OptimizedModule):\r\n        raise ValueError(f\"`model` is required to be a `OptimizedModule`. Found a `{type(model).__name__}` instead.\")\r\n\r\n    orig_module = model._orig_mod\r\n\r\n    if not isinstance(orig_module, pl.LightningModule):\r\n        _check_mixed_imports(model)\r\n        raise ValueError(\r\n            f\"`model` is expected to be a compiled LightningModule. Found a `{type(orig_module).__name__}` instead\"\r\n        )\r\n\r\n    orig_module._compiler_ctx = {\r\n        \"compiler\": \"dynamo\",\r\n        \"dynamo_ctx\": model.dynamo_ctx,\r\n        \"original_forward\": orig_module.forward,\r\n        \"original_training_step\": orig_module.training_step,\r\n        \"original_validation_step\": orig_module.validation_step,\r\n        \"original_test_step\": orig_module.test_step,\r\n        \"original_predict_step\": orig_module.predict_step,\r\n    }\r\n\r\n    orig_module.forward = model.dynamo_ctx(orig_module.forward)  # type: ignore[method-assign]\r\n    orig_module.training_step = model.dynamo_ctx(orig_module.training_step)  # type: ignore[method-assign]\r\n    orig_module.validation_step = model.dynamo_ctx(orig_module.validation_step)  # type: ignore[method-assign]\r\n    orig_module.test_step = model.dynamo_ctx(orig_module.test_step)  # type: ignore[method-assign]\r\n    orig_module.predict_step = model.dynamo_ctx(orig_module.predict_step)  # type: ignore[method-assign]\r\n    return orig_module", "language": "python", "code": "def from_compiled(model: OptimizedModule) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance LightningModule from the output of ``torch.compile``.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    The ``torch.compile`` function returns a ``torch._dynamo.OptimizedModule``, which wraps the LightningModule\r\n    passed in as an argument, but doesn't inherit from it. This means that the output of ``torch.compile`` behaves\r\n    like a LightningModule, but it doesn't inherit from it (i.e. `isinstance` will fail).\r\n\r\n    Use this method to obtain a LightningModule that still runs with all the optimizations from ``torch.compile``.\r\n\r\n    \"\"\"\r\n    if not isinstance(model, OptimizedModule):\r\n        raise ValueError(f\"`model` is required to be a `OptimizedModule`. Found a `{type(model).__name__}` instead.\")\r\n\r\n    orig_module = model._orig_mod\r\n\r\n    if not isinstance(orig_module, pl.LightningModule):\r\n        _check_mixed_imports(model)\r\n        raise ValueError(\r\n            f\"`model` is expected to be a compiled LightningModule. Found a `{type(orig_module).__name__}` instead\"\r\n        )\r\n\r\n    orig_module._compiler_ctx = {\r\n        \"compiler\": \"dynamo\",\r\n        \"dynamo_ctx\": model.dynamo_ctx,\r\n        \"original_forward\": orig_module.forward,\r\n        \"original_training_step\": orig_module.training_step,\r\n        \"original_validation_step\": orig_module.validation_step,\r\n        \"original_test_step\": orig_module.test_step,\r\n        \"original_predict_step\": orig_module.predict_step,\r\n    }\r\n\r\n    orig_module.forward = model.dynamo_ctx(orig_module.forward)  # type: ignore[method-assign]\r\n    orig_module.training_step = model.dynamo_ctx(orig_module.training_step)  # type: ignore[method-assign]\r\n    orig_module.validation_step = model.dynamo_ctx(orig_module.validation_step)  # type: ignore[method-assign]\r\n    orig_module.test_step = model.dynamo_ctx(orig_module.test_step)  # type: ignore[method-assign]\r\n    orig_module.predict_step = model.dynamo_ctx(orig_module.predict_step)  # type: ignore[method-assign]\r\n    return orig_module", "code_tokens": ["def", "from_compiled", "(", "model", ":", "OptimizedModule", ")", "-", ">", "STRING", ":", "STRING", "if", "not", "isinstance", "(", "model", ",", "OptimizedModule", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "orig_module", "=", "model", ".", "_orig_mod", "if", "not", "isinstance", "(", "orig_module", ",", "pl", ".", "LightningModule", ")", ":", "_check_mixed_imports", "(", "model", ")", "raise", "ValueError", "(", "fSTRING", ")", "orig_module", ".", "_compiler_ctx", "=", "{", "STRING", ":", "STRING", ",", "STRING", ":", "model", ".", "dynamo_ctx", ",", "STRING", ":", "orig_module", ".", "forward", ",", "STRING", ":", "orig_module", ".", "training_step", ",", "STRING", ":", "orig_module", ".", "validation_step", ",", "STRING", ":", "orig_module", ".", "test_step", ",", "STRING", ":", "orig_module", ".", "predict_step", ",", "}", "orig_module", ".", "forward", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "forward", ")", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "orig_module", ".", "training_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "training_step", ")", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "orig_module", ".", "validation_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "validation_step", ")", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "orig_module", ".", "test_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "test_step", ")", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "orig_module", ".", "predict_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "predict_step", ")", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "return", "orig_module"], "docstring": "Returns an instance LightningModule from the output of ``torch.compile``.", "docstring_tokens": ["returns", "an", "instance", "lightningmodule", "from", "the", "output", "of", "torch", "compile"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\compile.py", "start_line": 23, "end_line": 61, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\compile.py", "func_name": "function_1022", "original_string": "def to_uncompiled(model: Union[\"pl.LightningModule\", \"torch._dynamo.OptimizedModule\"]) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance of LightningModule without any compilation optimizations from a compiled model.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    This takes either a ``torch._dynamo.OptimizedModule`` returned by ``torch.compile()`` or a ``LightningModule``\r\n    returned by ``from_compiled``.\r\n\r\n    Note: this method will in-place modify the ``LightningModule`` that is passed in.\r\n\r\n    \"\"\"\r\n    if isinstance(model, OptimizedModule):\r\n        original = model._orig_mod\r\n        if not isinstance(original, pl.LightningModule):\r\n            raise TypeError(\r\n                f\"Unexpected error, the wrapped model should be a LightningModule, found {type(model).__name__}\"\r\n            )\r\n\r\n    elif isinstance(model, pl.LightningModule):\r\n        if model._compiler_ctx is None:\r\n            raise ValueError(\r\n                \"`model` is required to be a compiled LightningModule. Found a non-compiled LightningModule instead.\"\r\n            )\r\n        original = model\r\n\r\n    else:\r\n        raise ValueError(\"`model` must either be an instance of OptimizedModule or LightningModule\")\r\n\r\n    ctx = original._compiler_ctx\r\n    if ctx is not None:\r\n        original.forward = ctx[\"original_forward\"]  # type: ignore[method-assign]\r\n        original.training_step = ctx[\"original_training_step\"]  # type: ignore[method-assign]\r\n        original.validation_step = ctx[\"original_validation_step\"]  # type: ignore[method-assign]\r\n        original.test_step = ctx[\"original_test_step\"]  # type: ignore[method-assign]\r\n        original.predict_step = ctx[\"original_predict_step\"]  # type: ignore[method-assign]\r\n        original._compiler_ctx = None\r\n\r\n    return original", "language": "python", "code": "def to_uncompiled(model: Union[\"pl.LightningModule\", \"torch._dynamo.OptimizedModule\"]) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance of LightningModule without any compilation optimizations from a compiled model.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    This takes either a ``torch._dynamo.OptimizedModule`` returned by ``torch.compile()`` or a ``LightningModule``\r\n    returned by ``from_compiled``.\r\n\r\n    Note: this method will in-place modify the ``LightningModule`` that is passed in.\r\n\r\n    \"\"\"\r\n    if isinstance(model, OptimizedModule):\r\n        original = model._orig_mod\r\n        if not isinstance(original, pl.LightningModule):\r\n            raise TypeError(\r\n                f\"Unexpected error, the wrapped model should be a LightningModule, found {type(model).__name__}\"\r\n            )\r\n\r\n    elif isinstance(model, pl.LightningModule):\r\n        if model._compiler_ctx is None:\r\n            raise ValueError(\r\n                \"`model` is required to be a compiled LightningModule. Found a non-compiled LightningModule instead.\"\r\n            )\r\n        original = model\r\n\r\n    else:\r\n        raise ValueError(\"`model` must either be an instance of OptimizedModule or LightningModule\")\r\n\r\n    ctx = original._compiler_ctx\r\n    if ctx is not None:\r\n        original.forward = ctx[\"original_forward\"]  # type: ignore[method-assign]\r\n        original.training_step = ctx[\"original_training_step\"]  # type: ignore[method-assign]\r\n        original.validation_step = ctx[\"original_validation_step\"]  # type: ignore[method-assign]\r\n        original.test_step = ctx[\"original_test_step\"]  # type: ignore[method-assign]\r\n        original.predict_step = ctx[\"original_predict_step\"]  # type: ignore[method-assign]\r\n        original._compiler_ctx = None\r\n\r\n    return original", "code_tokens": ["def", "to_uncompiled", "(", "model", ":", "Union", "[", "STRING", ",", "STRING", "]", ")", "-", ">", "STRING", ":", "STRING", "if", "isinstance", "(", "model", ",", "OptimizedModule", ")", ":", "original", "=", "model", ".", "_orig_mod", "if", "not", "isinstance", "(", "original", ",", "pl", ".", "LightningModule", ")", ":", "raise", "TypeError", "(", "fSTRING", ")", "elif", "isinstance", "(", "model", ",", "pl", ".", "LightningModule", ")", ":", "if", "model", ".", "_compiler_ctx", "is", "None", ":", "raise", "ValueError", "(", "STRING", ")", "original", "=", "model", "else", ":", "raise", "ValueError", "(", "STRING", ")", "ctx", "=", "original", ".", "_compiler_ctx", "if", "ctx", "is", "not", "None", ":", "original", ".", "forward", "=", "ctx", "[", "STRING", "]", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "original", ".", "training_step", "=", "ctx", "[", "STRING", "]", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "original", ".", "validation_step", "=", "ctx", "[", "STRING", "]", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "original", ".", "test_step", "=", "ctx", "[", "STRING", "]", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "original", ".", "predict_step", "=", "ctx", "[", "STRING", "]", "#", "type", ":", "ignore", "[", "method", "-", "assign", "]", "original", ".", "_compiler_ctx", "=", "None", "return", "original"], "docstring": "Returns an instance of LightningModule without any compilation optimizations from a compiled model.", "docstring_tokens": ["returns", "an", "instance", "of", "lightningmodule", "without", "any", "compilation", "optimizations", "from", "a", "compiled", "model"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\compile.py", "start_line": 64, "end_line": 101, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\consolidate_checkpoint.py", "func_name": "function_1023", "original_string": "def _format_checkpoint(checkpoint: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.\"\"\"\r\n    checkpoint[\"state_dict\"] = checkpoint.pop(\"model\")\r\n\r\n    optimizer_keys = [key for key in checkpoint if re.match(\"optimizer_[0-9]+\", key)]\r\n    if not optimizer_keys:\r\n        return checkpoint\r\n\r\n    checkpoint[\"optimizer_states\"] = [checkpoint.pop(f\"optimizer_{opt_idx}\") for opt_idx in range(len(optimizer_keys))]\r\n    return checkpoint", "language": "python", "code": "def _format_checkpoint(checkpoint: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.\"\"\"\r\n    checkpoint[\"state_dict\"] = checkpoint.pop(\"model\")\r\n\r\n    optimizer_keys = [key for key in checkpoint if re.match(\"optimizer_[0-9]+\", key)]\r\n    if not optimizer_keys:\r\n        return checkpoint\r\n\r\n    checkpoint[\"optimizer_states\"] = [checkpoint.pop(f\"optimizer_{opt_idx}\") for opt_idx in range(len(optimizer_keys))]\r\n    return checkpoint", "code_tokens": ["def", "_format_checkpoint", "(", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "checkpoint", "[", "STRING", "]", "=", "checkpoint", ".", "pop", "(", "STRING", ")", "optimizer_keys", "=", "[", "key", "for", "key", "in", "checkpoint", "if", "re", ".", "match", "(", "STRING", ",", "key", ")", "]", "if", "not", "optimizer_keys", ":", "return", "checkpoint", "checkpoint", "[", "STRING", "]", "=", "[", "checkpoint", ".", "pop", "(", "fSTRING", ")", "for", "opt_idx", "in", "range", "(", "len", "(", "optimizer_keys", ")", ")", "]", "return", "checkpoint"], "docstring": "Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.", "docstring_tokens": ["converts", "the", "special", "fsdp", "checkpoint", "format", "to", "the", "standard", "format", "the", "lightning", "trainer", "can", "load"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\consolidate_checkpoint.py", "start_line": 9, "end_line": 21, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "function_1024", "original_string": "def extract_batch_size(batch: BType) -> int:\r\n    \"\"\"Unpack a batch to find a ``torch.Tensor``.\r\n\r\n    Returns:\r\n        ``len(tensor)`` when found, or ``1`` when it hits an empty or non iterable.\r\n\r\n    \"\"\"\r\n    error_msg = (\r\n        \"We could not infer the batch_size from the batch. Either simplify its structure\"\r\n        \" or provide the batch_size as `self.log(..., batch_size=batch_size)`.\"\r\n    )\r\n    batch_size = None\r\n    try:\r\n        for bs in _extract_batch_size(batch):\r\n            if batch_size is None:\r\n                batch_size = bs\r\n            elif batch_size != bs:\r\n                warning_cache.warn(\r\n                    \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\r\n                    f\" found is {batch_size}. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\"\r\n                )\r\n                break\r\n    except RecursionError:\r\n        raise RecursionError(error_msg)\r\n\r\n    if batch_size is None:\r\n        raise MisconfigurationException(error_msg)\r\n\r\n    return batch_size", "language": "python", "code": "def extract_batch_size(batch: BType) -> int:\r\n    \"\"\"Unpack a batch to find a ``torch.Tensor``.\r\n\r\n    Returns:\r\n        ``len(tensor)`` when found, or ``1`` when it hits an empty or non iterable.\r\n\r\n    \"\"\"\r\n    error_msg = (\r\n        \"We could not infer the batch_size from the batch. Either simplify its structure\"\r\n        \" or provide the batch_size as `self.log(..., batch_size=batch_size)`.\"\r\n    )\r\n    batch_size = None\r\n    try:\r\n        for bs in _extract_batch_size(batch):\r\n            if batch_size is None:\r\n                batch_size = bs\r\n            elif batch_size != bs:\r\n                warning_cache.warn(\r\n                    \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\r\n                    f\" found is {batch_size}. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\"\r\n                )\r\n                break\r\n    except RecursionError:\r\n        raise RecursionError(error_msg)\r\n\r\n    if batch_size is None:\r\n        raise MisconfigurationException(error_msg)\r\n\r\n    return batch_size", "code_tokens": ["def", "extract_batch_size", "(", "batch", ":", "BType", ")", "-", ">", "int", ":", "STRING", "error_msg", "=", "(", "STRING", "STRING", ")", "batch_size", "=", "None", "try", ":", "for", "bs", "in", "_extract_batch_size", "(", "batch", ")", ":", "if", "batch_size", "is", "None", ":", "batch_size", "=", "bs", "elif", "batch_size", "!", "=", "bs", ":", "warning_cache", ".", "warn", "(", "STRING", "fSTRING", ")", "break", "except", "RecursionError", ":", "raise", "RecursionError", "(", "error_msg", ")", "if", "batch_size", "is", "None", ":", "raise", "MisconfigurationException", "(", "error_msg", ")", "return", "batch_size"], "docstring": "Unpack a batch to find a ``torch.Tensor``.", "docstring_tokens": ["unpack", "a", "batch", "to", "find", "a", "torch", "tensor"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\data.py", "start_line": 61, "end_line": 89, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "function_1025", "original_string": "def has_len_all_ranks(\r\n    dataloader: object,\r\n    strategy: \"pl.strategies.Strategy\",\r\n    allow_zero_length_dataloader_with_multiple_devices: bool = False,\r\n) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented on all ranks.\"\"\"\r\n    local_length = sized_len(dataloader)\r\n    if local_length is None:\r\n        return False\r\n\r\n    total_length = strategy.reduce(torch.tensor(local_length, device=strategy.root_device), reduce_op=\"sum\")\r\n    if total_length == 0:\r\n        rank_zero_warn(\r\n            f\"Total length of `{type(dataloader).__name__}` across ranks is zero.\"\r\n            \" Please make sure this was your intention.\"\r\n        )\r\n    if total_length > 0 and local_length == 0:\r\n        dataloader_cls_name = type(dataloader).__name__\r\n        if not allow_zero_length_dataloader_with_multiple_devices:\r\n            raise RuntimeError(\r\n                f\"`{dataloader_cls_name}` within local rank has zero length.\"\r\n                \" Please make sure that it returns at least 1 batch.\"\r\n            )\r\n        rank_zero_warn(\r\n            f\"Total length of `{dataloader_cls_name}` across ranks is zero, but local rank has zero\"\r\n            \" length. Please be cautious of uneven batch length.\"\r\n        )\r\n\r\n    if has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return True", "language": "python", "code": "def has_len_all_ranks(\r\n    dataloader: object,\r\n    strategy: \"pl.strategies.Strategy\",\r\n    allow_zero_length_dataloader_with_multiple_devices: bool = False,\r\n) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented on all ranks.\"\"\"\r\n    local_length = sized_len(dataloader)\r\n    if local_length is None:\r\n        return False\r\n\r\n    total_length = strategy.reduce(torch.tensor(local_length, device=strategy.root_device), reduce_op=\"sum\")\r\n    if total_length == 0:\r\n        rank_zero_warn(\r\n            f\"Total length of `{type(dataloader).__name__}` across ranks is zero.\"\r\n            \" Please make sure this was your intention.\"\r\n        )\r\n    if total_length > 0 and local_length == 0:\r\n        dataloader_cls_name = type(dataloader).__name__\r\n        if not allow_zero_length_dataloader_with_multiple_devices:\r\n            raise RuntimeError(\r\n                f\"`{dataloader_cls_name}` within local rank has zero length.\"\r\n                \" Please make sure that it returns at least 1 batch.\"\r\n            )\r\n        rank_zero_warn(\r\n            f\"Total length of `{dataloader_cls_name}` across ranks is zero, but local rank has zero\"\r\n            \" length. Please be cautious of uneven batch length.\"\r\n        )\r\n\r\n    if has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return True", "code_tokens": ["def", "has_len_all_ranks", "(", "dataloader", ":", "object", ",", "strategy", ":", "STRING", ",", "allow_zero_length_dataloader_with_multiple_devices", ":", "bool", "=", "False", ",", ")", "-", ">", "TypeGuard", "[", "Sized", "]", ":", "STRING", "local_length", "=", "sized_len", "(", "dataloader", ")", "if", "local_length", "is", "None", ":", "return", "False", "total_length", "=", "strategy", ".", "reduce", "(", "torch", ".", "tensor", "(", "local_length", ",", "device", "=", "strategy", ".", "root_device", ")", ",", "reduce_op", "=", "STRING", ")", "if", "total_length", "=", "=", "0", ":", "rank_zero_warn", "(", "fSTRING", "STRING", ")", "if", "total_length", ">", "0", "and", "local_length", "=", "=", "0", ":", "dataloader_cls_name", "=", "type", "(", "dataloader", ")", ".", "__name__", "if", "not", "allow_zero_length_dataloader_with_multiple_devices", ":", "raise", "RuntimeError", "(", "fSTRING", "STRING", ")", "rank_zero_warn", "(", "fSTRING", "STRING", ")", "if", "has_iterable_dataset", "(", "dataloader", ")", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "STRING", ")", "return", "True"], "docstring": "Checks if a given object has ``__len__`` method implemented on all ranks.", "docstring_tokens": ["checks", "if", "a", "given", "object", "has", "__len__", "method", "implemented", "on", "all", "ranks"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\data.py", "start_line": 92, "end_line": 128, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "function_1026", "original_string": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n    mode: Optional[RunningStage] = None,\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\r\n\r\n    If the dataloader is being used for prediction, the sampler will be wrapped into an `_IndexBatchSamplerWrapper`, so\r\n    Lightning can keep track of its indices.\r\n\r\n    \"\"\"\r\n    is_predicting = mode == RunningStage.PREDICTING\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n    batch_sampler_cls = type(batch_sampler)\r\n\r\n    if batch_sampler is not None and (batch_sampler_cls is not BatchSampler or is_predicting):\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            if is_predicting:\r\n                success, args, kwargs = _replace_value_in_saved_args(\r\n                    \"drop_last\", False, args, kwargs, default_kwargs, arg_names\r\n                )\r\n                if not success:\r\n                    rank_zero_warn(\r\n                        f\"Trying to inject `drop_last=False` into batch sampler since you are predicting, however \"\r\n                        f\"it seems the class `{batch_sampler_cls.__qualname__}` does not support it. \"\r\n                        \"Your predictions might be incomplete. To mitigate this, expose `drop_last` in \"\r\n                        \"the `__init__` method of your custom class.\"\r\n                    )\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=(False if is_predicting else batch_sampler.drop_last),\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    raise\r\n\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler` and\"\r\n                    \" instantiate your custom batch sampler inside the `*_dataloader` hook of your module,\"\r\n                    \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                    \" responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        elif is_predicting:\r\n            rank_zero_warn(\r\n                f\"You are using a custom batch sampler `{batch_sampler_cls.__qualname__}` for prediction.\"\r\n                \" Lightning would normally set `drop_last=False` to ensure all samples are returned, but for\"\r\n                \" custom samplers it can't guarantee this. Make sure your sampler is configured correctly to return\"\r\n                \" all indices.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        else:\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                \" responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        if is_predicting:\r\n            batch_sampler = _IndexBatchSamplerWrapper(batch_sampler)\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "language": "python", "code": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n    mode: Optional[RunningStage] = None,\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\r\n\r\n    If the dataloader is being used for prediction, the sampler will be wrapped into an `_IndexBatchSamplerWrapper`, so\r\n    Lightning can keep track of its indices.\r\n\r\n    \"\"\"\r\n    is_predicting = mode == RunningStage.PREDICTING\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n    batch_sampler_cls = type(batch_sampler)\r\n\r\n    if batch_sampler is not None and (batch_sampler_cls is not BatchSampler or is_predicting):\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            if is_predicting:\r\n                success, args, kwargs = _replace_value_in_saved_args(\r\n                    \"drop_last\", False, args, kwargs, default_kwargs, arg_names\r\n                )\r\n                if not success:\r\n                    rank_zero_warn(\r\n                        f\"Trying to inject `drop_last=False` into batch sampler since you are predicting, however \"\r\n                        f\"it seems the class `{batch_sampler_cls.__qualname__}` does not support it. \"\r\n                        \"Your predictions might be incomplete. To mitigate this, expose `drop_last` in \"\r\n                        \"the `__init__` method of your custom class.\"\r\n                    )\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=(False if is_predicting else batch_sampler.drop_last),\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    raise\r\n\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler` and\"\r\n                    \" instantiate your custom batch sampler inside the `*_dataloader` hook of your module,\"\r\n                    \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                    \" responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        elif is_predicting:\r\n            rank_zero_warn(\r\n                f\"You are using a custom batch sampler `{batch_sampler_cls.__qualname__}` for prediction.\"\r\n                \" Lightning would normally set `drop_last=False` to ensure all samples are returned, but for\"\r\n                \" custom samplers it can't guarantee this. Make sure your sampler is configured correctly to return\"\r\n                \" all indices.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        else:\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                \" responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        if is_predicting:\r\n            batch_sampler = _IndexBatchSamplerWrapper(batch_sampler)\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "code_tokens": ["def", "_dataloader_init_kwargs_resolve_sampler", "(", "dataloader", ":", "DataLoader", ",", "sampler", ":", "Union", "[", "Sampler", ",", "Iterable", "]", ",", "mode", ":", "Optional", "[", "RunningStage", "]", "=", "None", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "is_predicting", "=", "mode", "=", "=", "RunningStage", ".", "PREDICTING", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "STRING", ")", "batch_sampler_cls", "=", "type", "(", "batch_sampler", ")", "if", "batch_sampler", "is", "not", "None", "and", "(", "batch_sampler_cls", "is", "not", "BatchSampler", "or", "is_predicting", ")", ":", "if", "hasattr", "(", "batch_sampler", ",", "STRING", ")", ":", "args", "=", "batch_sampler", ".", "__pl_saved_args", "kwargs", "=", "batch_sampler", ".", "__pl_saved_kwargs", "default_kwargs", "=", "batch_sampler", ".", "__pl_saved_default_kwargs", "arg_names", "=", "batch_sampler", ".", "__pl_saved_arg_names", "if", "is_predicting", ":", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "STRING", ",", "False", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", "STRING", "STRING", ")", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "STRING", ",", "sampler", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "raise", "TypeError", "(", "STRING", "fSTRING", "STRING", ")", "batch_sampler", "=", "_reinstantiate_wrapped_cls", "(", "batch_sampler", ",", "*", "args", ",", "*", "*", "kwargs", ")", "elif", "hasattr", "(", "batch_sampler", ",", "STRING", ")", "and", "hasattr", "(", "batch_sampler", ",", "STRING", ")", ":", "try", ":", "batch_sampler", "=", "batch_sampler_cls", "(", "sampler", ",", "batch_size", "=", "batch_sampler", ".", "batch_size", ",", "drop_last", "=", "(", "False", "if", "is_predicting", "else", "batch_sampler", ".", "drop_last", ")", ",", ")", "except", "TypeError", "as", "ex", ":", "import", "re", "match", "=", "re", ".", "match", "(", "rSTRING", ",", "str", "(", "ex", ")", ")", "if", "not", "match", ":", "raise", "raise", "TypeError", "(", "STRING", "STRING", "STRING", "STRING", "STRING", ")", "from", "ex", "elif", "is_predicting", ":", "rank_zero_warn", "(", "fSTRING", "STRING", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "else", ":", "raise", "TypeError", "(", "STRING", "STRING", "STRING", "STRING", ")", "if", "is_predicting", ":", "batch_sampler", "=", "_IndexBatchSamplerWrapper", "(", "batch_sampler", ")", "return", "{", "STRING", ":", "None", ",", "STRING", ":", "False", ",", "STRING", ":", "batch_sampler", ",", "STRING", ":", "1", ",", "STRING", ":", "False", ",", "}", "return", "{", "STRING", ":", "sampler", ",", "STRING", ":", "False", ",", "STRING", ":", "None", "}"], "docstring": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-", "docstring_tokens": ["this", "function", "is", "used", "to", "handle", "the", "sampler", "batch_sampler", "arguments", "associated", "within", "a", "dataloader", "for", "its", "re"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\data.py", "start_line": 232, "end_line": 335, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\deepspeed.py", "func_name": "function_1027", "original_string": "def convert_zero_checkpoint_to_fp32_state_dict(\r\n    checkpoint_dir: _PATH, output_file: _PATH, tag: str | None = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with\r\n    ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed. It gets copied into the top\r\n    level checkpoint dir, so the user can easily do the conversion at any point in the future. Once extracted, the\r\n    weights don't require DeepSpeed and can be used in any application. Additionally the script has been modified to\r\n    ensure we keep the lightning state inside the state dict for being able to run\r\n    ``LightningModule.load_from_checkpoint('...')```.\r\n\r\n    Args:\r\n        checkpoint_dir: path to the desired checkpoint folder.\r\n            (one that contains the tag-folder, like ``global_step14``)\r\n        output_file: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\r\n        tag: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt\r\n            to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\r\n\r\n    Examples::\r\n\r\n        convert_zero_checkpoint_to_fp32_state_dict(\r\n            \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\",\r\n            \"lightning_model.pt\"\r\n        )\r\n\r\n    \"\"\"\r\n    if not _DEEPSPEED_AVAILABLE:\r\n        raise ModuleNotFoundError(str(_DEEPSPEED_AVAILABLE))\r\n\r\n    from deepspeed.utils.zero_to_fp32 import (\r\n        get_fp32_state_dict_from_zero_checkpoint,\r\n        get_model_state_file,\r\n        get_optim_files,\r\n    )\r\n\r\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\r\n\r\n    deepspeed_states = [\r\n        \"module\",\r\n        \"optimizer\",\r\n        \"lr_scheduler\",\r\n        \"csr_tensor_module_names\",\r\n        \"skipped_steps\",\r\n        \"global_steps\",\r\n        \"dp_world_size\",\r\n        \"mp_world_size\",\r\n    ]\r\n    checkpoint_dir = ds_checkpoint_dir(checkpoint_dir)\r\n    optim_files = get_optim_files(checkpoint_dir)\r\n    optim_state = torch.load(optim_files[0], map_location=CPU_DEVICE, weights_only=False)\r\n    zero_stage = optim_state[\"optimizer_state_dict\"][\"zero_stage\"]\r\n    model_file = get_model_state_file(checkpoint_dir, zero_stage)\r\n    client_state = torch.load(model_file, map_location=CPU_DEVICE, weights_only=False)\r\n    client_state = {key: value for key, value in client_state.items() if key not in deepspeed_states}\r\n    state_dict = {_remove_prefix(k, \"_forward_module.\"): state_dict[k] for k in state_dict}\r\n    client_state[\"state_dict\"] = state_dict\r\n\r\n    print(f\"Saving fp32 state dict to {output_file}\")\r\n    torch.save(client_state, output_file)\r\n\r\n    return client_state", "language": "python", "code": "def convert_zero_checkpoint_to_fp32_state_dict(\r\n    checkpoint_dir: _PATH, output_file: _PATH, tag: str | None = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with\r\n    ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed. It gets copied into the top\r\n    level checkpoint dir, so the user can easily do the conversion at any point in the future. Once extracted, the\r\n    weights don't require DeepSpeed and can be used in any application. Additionally the script has been modified to\r\n    ensure we keep the lightning state inside the state dict for being able to run\r\n    ``LightningModule.load_from_checkpoint('...')```.\r\n\r\n    Args:\r\n        checkpoint_dir: path to the desired checkpoint folder.\r\n            (one that contains the tag-folder, like ``global_step14``)\r\n        output_file: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\r\n        tag: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt\r\n            to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\r\n\r\n    Examples::\r\n\r\n        convert_zero_checkpoint_to_fp32_state_dict(\r\n            \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\",\r\n            \"lightning_model.pt\"\r\n        )\r\n\r\n    \"\"\"\r\n    if not _DEEPSPEED_AVAILABLE:\r\n        raise ModuleNotFoundError(str(_DEEPSPEED_AVAILABLE))\r\n\r\n    from deepspeed.utils.zero_to_fp32 import (\r\n        get_fp32_state_dict_from_zero_checkpoint,\r\n        get_model_state_file,\r\n        get_optim_files,\r\n    )\r\n\r\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\r\n\r\n    deepspeed_states = [\r\n        \"module\",\r\n        \"optimizer\",\r\n        \"lr_scheduler\",\r\n        \"csr_tensor_module_names\",\r\n        \"skipped_steps\",\r\n        \"global_steps\",\r\n        \"dp_world_size\",\r\n        \"mp_world_size\",\r\n    ]\r\n    checkpoint_dir = ds_checkpoint_dir(checkpoint_dir)\r\n    optim_files = get_optim_files(checkpoint_dir)\r\n    optim_state = torch.load(optim_files[0], map_location=CPU_DEVICE, weights_only=False)\r\n    zero_stage = optim_state[\"optimizer_state_dict\"][\"zero_stage\"]\r\n    model_file = get_model_state_file(checkpoint_dir, zero_stage)\r\n    client_state = torch.load(model_file, map_location=CPU_DEVICE, weights_only=False)\r\n    client_state = {key: value for key, value in client_state.items() if key not in deepspeed_states}\r\n    state_dict = {_remove_prefix(k, \"_forward_module.\"): state_dict[k] for k in state_dict}\r\n    client_state[\"state_dict\"] = state_dict\r\n\r\n    print(f\"Saving fp32 state dict to {output_file}\")\r\n    torch.save(client_state, output_file)\r\n\r\n    return client_state", "code_tokens": ["def", "convert_zero_checkpoint_to_fp32_state_dict", "(", "checkpoint_dir", ":", "_PATH", ",", "output_file", ":", "_PATH", ",", "tag", ":", "str", "|", "None", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "STRING", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "str", "(", "_DEEPSPEED_AVAILABLE", ")", ")", "from", "deepspeed", ".", "utils", ".", "zero_to_fp32", "import", "(", "get_fp32_state_dict_from_zero_checkpoint", ",", "get_model_state_file", ",", "get_optim_files", ",", ")", "state_dict", "=", "get_fp32_state_dict_from_zero_checkpoint", "(", "checkpoint_dir", ",", "tag", ")", "deepspeed_states", "=", "[", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ",", "STRING", ",", "]", "checkpoint_dir", "=", "ds_checkpoint_dir", "(", "checkpoint_dir", ")", "optim_files", "=", "get_optim_files", "(", "checkpoint_dir", ")", "optim_state", "=", "torch", ".", "load", "(", "optim_files", "[", "0", "]", ",", "map_location", "=", "CPU_DEVICE", ",", "weights_only", "=", "False", ")", "zero_stage", "=", "optim_state", "[", "STRING", "]", "[", "STRING", "]", "model_file", "=", "get_model_state_file", "(", "checkpoint_dir", ",", "zero_stage", ")", "client_state", "=", "torch", ".", "load", "(", "model_file", ",", "map_location", "=", "CPU_DEVICE", ",", "weights_only", "=", "False", ")", "client_state", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "client_state", ".", "items", "(", ")", "if", "key", "not", "in", "deepspeed_states", "}", "state_dict", "=", "{", "_remove_prefix", "(", "k", ",", "STRING", ")", ":", "state_dict", "[", "k", "]", "for", "k", "in", "state_dict", "}", "client_state", "[", "STRING", "]", "=", "state_dict", "print", "(", "fSTRING", ")", "torch", ".", "save", "(", "client_state", ",", "output_file", ")", "return", "client_state"], "docstring": "Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with", "docstring_tokens": ["convert", "zero", "2", "or", "3", "checkpoint", "into", "a", "single", "fp32", "consolidated", "state_dict", "file", "that", "can", "be", "loaded", "with"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\deepspeed.py", "start_line": 45, "end_line": 108, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\grads.py", "func_name": "function_1028", "original_string": "def grad_norm(module: Module, norm_type: Union[float, int, str], group_separator: str = \"/\") -> dict[str, float]:\r\n    \"\"\"Compute each parameter's gradient's norm and their overall norm.\r\n\r\n    The overall norm is computed over all gradients together, as if they\r\n    were concatenated into a single vector.\r\n\r\n    Args:\r\n        module: :class:`torch.nn.Module` to inspect.\r\n        norm_type: The type of the used p-norm, cast to float if necessary.\r\n            Can be ``'inf'`` for infinity norm.\r\n        group_separator: The separator string used by the logger to group\r\n            the gradients norms in their own subfolder instead of the logs one.\r\n\r\n    Return:\r\n        norms: The dictionary of p-norms of each parameter's gradient and\r\n            a special entry for the total p-norm of the gradients viewed\r\n            as a single vector.\r\n\r\n    \"\"\"\r\n    norm_type = float(norm_type)\r\n    if norm_type <= 0:\r\n        raise ValueError(f\"`norm_type` must be a positive number or 'inf' (infinity norm). Got {norm_type}\")\r\n\r\n    norms = {\r\n        f\"grad_{norm_type}_norm{group_separator}{name}\": p.grad.data.norm(norm_type)\r\n        for name, p in module.named_parameters()\r\n        if p.grad is not None\r\n    }\r\n    if norms:\r\n        total_norm = torch.tensor(list(norms.values())).norm(norm_type)\r\n        norms[f\"grad_{norm_type}_norm_total\"] = total_norm\r\n    return norms", "language": "python", "code": "def grad_norm(module: Module, norm_type: Union[float, int, str], group_separator: str = \"/\") -> dict[str, float]:\r\n    \"\"\"Compute each parameter's gradient's norm and their overall norm.\r\n\r\n    The overall norm is computed over all gradients together, as if they\r\n    were concatenated into a single vector.\r\n\r\n    Args:\r\n        module: :class:`torch.nn.Module` to inspect.\r\n        norm_type: The type of the used p-norm, cast to float if necessary.\r\n            Can be ``'inf'`` for infinity norm.\r\n        group_separator: The separator string used by the logger to group\r\n            the gradients norms in their own subfolder instead of the logs one.\r\n\r\n    Return:\r\n        norms: The dictionary of p-norms of each parameter's gradient and\r\n            a special entry for the total p-norm of the gradients viewed\r\n            as a single vector.\r\n\r\n    \"\"\"\r\n    norm_type = float(norm_type)\r\n    if norm_type <= 0:\r\n        raise ValueError(f\"`norm_type` must be a positive number or 'inf' (infinity norm). Got {norm_type}\")\r\n\r\n    norms = {\r\n        f\"grad_{norm_type}_norm{group_separator}{name}\": p.grad.data.norm(norm_type)\r\n        for name, p in module.named_parameters()\r\n        if p.grad is not None\r\n    }\r\n    if norms:\r\n        total_norm = torch.tensor(list(norms.values())).norm(norm_type)\r\n        norms[f\"grad_{norm_type}_norm_total\"] = total_norm\r\n    return norms", "code_tokens": ["def", "grad_norm", "(", "module", ":", "Module", ",", "norm_type", ":", "Union", "[", "float", ",", "int", ",", "str", "]", ",", "group_separator", ":", "str", "=", "STRING", ")", "-", ">", "dict", "[", "str", ",", "float", "]", ":", "STRING", "norm_type", "=", "float", "(", "norm_type", ")", "if", "norm_type", "<", "=", "0", ":", "raise", "ValueError", "(", "fSTRING", ")", "norms", "=", "{", "fSTRING", ":", "p", ".", "grad", ".", "data", ".", "norm", "(", "norm_type", ")", "for", "name", ",", "p", "in", "module", ".", "named_parameters", "(", ")", "if", "p", ".", "grad", "is", "not", "None", "}", "if", "norms", ":", "total_norm", "=", "torch", ".", "tensor", "(", "list", "(", "norms", ".", "values", "(", ")", ")", ")", ".", "norm", "(", "norm_type", ")", "norms", "[", "fSTRING", "]", "=", "total_norm", "return", "norms"], "docstring": "Compute each parameter's gradient's norm and their overall norm.", "docstring_tokens": ["compute", "each", "parameter", "s", "gradient", "s", "norm", "and", "their", "overall", "norm"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\grads.py", "start_line": 21, "end_line": 52, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\memory.py", "func_name": "function_1029", "original_string": "def recursive_detach(in_dict: Any, to_cpu: bool = False) -> Any:\r\n    \"\"\"Detach all tensors in `in_dict`.\r\n\r\n    May operate recursively if some of the values in `in_dict` are dictionaries\r\n    which contain instances of `Tensor`. Other types in `in_dict` are\r\n    not affected by this utility function.\r\n\r\n    Args:\r\n        in_dict: Dictionary with tensors to detach\r\n        to_cpu: Whether to move tensor to cpu\r\n\r\n    Return:\r\n        out_dict: Dictionary with detached tensors\r\n\r\n    \"\"\"\r\n\r\n    def detach_and_move(t: Tensor, to_cpu: bool) -> Tensor:\r\n        t = t.detach()\r\n        if to_cpu:\r\n            t = t.cpu()\r\n        return t\r\n\r\n    return apply_to_collection(in_dict, Tensor, detach_and_move, to_cpu=to_cpu)", "language": "python", "code": "def recursive_detach(in_dict: Any, to_cpu: bool = False) -> Any:\r\n    \"\"\"Detach all tensors in `in_dict`.\r\n\r\n    May operate recursively if some of the values in `in_dict` are dictionaries\r\n    which contain instances of `Tensor`. Other types in `in_dict` are\r\n    not affected by this utility function.\r\n\r\n    Args:\r\n        in_dict: Dictionary with tensors to detach\r\n        to_cpu: Whether to move tensor to cpu\r\n\r\n    Return:\r\n        out_dict: Dictionary with detached tensors\r\n\r\n    \"\"\"\r\n\r\n    def detach_and_move(t: Tensor, to_cpu: bool) -> Tensor:\r\n        t = t.detach()\r\n        if to_cpu:\r\n            t = t.cpu()\r\n        return t\r\n\r\n    return apply_to_collection(in_dict, Tensor, detach_and_move, to_cpu=to_cpu)", "code_tokens": ["def", "recursive_detach", "(", "in_dict", ":", "Any", ",", "to_cpu", ":", "bool", "=", "False", ")", "-", ">", "Any", ":", "STRING", "def", "detach_and_move", "(", "t", ":", "Tensor", ",", "to_cpu", ":", "bool", ")", "-", ">", "Tensor", ":", "t", "=", "t", ".", "detach", "(", ")", "if", "to_cpu", ":", "t", "=", "t", ".", "cpu", "(", ")", "return", "t", "return", "apply_to_collection", "(", "in_dict", ",", "Tensor", ",", "detach_and_move", ",", "to_cpu", "=", "to_cpu", ")"], "docstring": "Detach all tensors in `in_dict`.", "docstring_tokens": ["detach", "all", "tensors", "in", "in_dict"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\memory.py", "start_line": 23, "end_line": 45, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\memory.py", "func_name": "function_1030", "original_string": "def garbage_collection_cuda() -> None:\r\n    \"\"\"Garbage collection Torch (CUDA) memory.\"\"\"\r\n    gc.collect()\r\n    try:\r\n        torch.cuda.empty_cache()\r\n    except RuntimeError as exception:\r\n        if not is_oom_error(exception):\r\n            raise", "language": "python", "code": "def garbage_collection_cuda() -> None:\r\n    \"\"\"Garbage collection Torch (CUDA) memory.\"\"\"\r\n    gc.collect()\r\n    try:\r\n        torch.cuda.empty_cache()\r\n    except RuntimeError as exception:\r\n        if not is_oom_error(exception):\r\n            raise", "code_tokens": ["def", "garbage_collection_cuda", "(", ")", "-", ">", "None", ":", "STRING", "gc", ".", "collect", "(", ")", "try", ":", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "except", "RuntimeError", "as", "exception", ":", "if", "not", "is_oom_error", "(", "exception", ")", ":", "raise"], "docstring": "Garbage collection Torch (CUDA) memory.", "docstring_tokens": ["garbage", "collection", "torch", "cuda", "memory"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\memory.py", "start_line": 82, "end_line": 91, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1031", "original_string": "def _is_registry(text: Optional[_PATH]) -> bool:\r\n    \"\"\"Check if a string equals 'registry' or starts with 'registry:'.\r\n\r\n    Args:\r\n        text: The string to check\r\n\r\n    >>> _is_registry(\"registry\")\r\n    True\r\n    >>> _is_registry(\"REGISTRY:model-name\")\r\n    True\r\n    >>> _is_registry(\"something_registry\")\r\n    False\r\n    >>> _is_registry(\"\")\r\n    False\r\n\r\n    \"\"\"\r\n    if not isinstance(text, str):\r\n        return False\r\n\r\n    pattern = r\"^registry(:.*|$)\"\r\n    return bool(re.match(pattern, text.lower()))", "language": "python", "code": "def _is_registry(text: Optional[_PATH]) -> bool:\r\n    \"\"\"Check if a string equals 'registry' or starts with 'registry:'.\r\n\r\n    Args:\r\n        text: The string to check\r\n\r\n    >>> _is_registry(\"registry\")\r\n    True\r\n    >>> _is_registry(\"REGISTRY:model-name\")\r\n    True\r\n    >>> _is_registry(\"something_registry\")\r\n    False\r\n    >>> _is_registry(\"\")\r\n    False\r\n\r\n    \"\"\"\r\n    if not isinstance(text, str):\r\n        return False\r\n\r\n    pattern = r\"^registry(:.*|$)\"\r\n    return bool(re.match(pattern, text.lower()))", "code_tokens": ["def", "_is_registry", "(", "text", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "bool", ":", "STRING", "if", "not", "isinstance", "(", "text", ",", "str", ")", ":", "return", "False", "pattern", "=", "rSTRING", "return", "bool", "(", "re", ".", "match", "(", "pattern", ",", "text", ".", "lower", "(", ")", ")", ")"], "docstring": "Check if a string equals 'registry' or starts with 'registry:'.", "docstring_tokens": ["check", "if", "a", "string", "equals", "registry", "or", "starts", "with", "registry"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 28, "end_line": 49, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1032", "original_string": "def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]:\r\n    \"\"\"Parse the model version from a registry path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n\r\n    Returns:\r\n        string name and version of the model\r\n\r\n    >>> _parse_registry_model_version(\"registry:model-name:version:1.0\")\r\n    ('model-name', '1.0')\r\n    >>> _parse_registry_model_version(\"registry:model-name\")\r\n    ('model-name', '')\r\n    >>> _parse_registry_model_version(\"registry:VERSION:v2\")\r\n    ('', 'v2')\r\n\r\n    \"\"\"\r\n    if not ckpt_path or not _is_registry(ckpt_path):\r\n        raise ValueError(f\"Invalid registry path: {ckpt_path}\")\r\n\r\n    parts = str(ckpt_path).split(\":\")\r\n    model_name, version = \"\", \"\"\r\n\r\n    if len(parts) >= 2 and parts[1].lower() != \"version\":\r\n        model_name = parts[1]\r\n    if len(parts) == 3 and parts[1].lower() == \"version\":\r\n        version = parts[2]\r\n    elif len(parts) == 4 and parts[2].lower() == \"version\":\r\n        version = parts[3]\r\n\r\n    return model_name, version", "language": "python", "code": "def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]:\r\n    \"\"\"Parse the model version from a registry path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n\r\n    Returns:\r\n        string name and version of the model\r\n\r\n    >>> _parse_registry_model_version(\"registry:model-name:version:1.0\")\r\n    ('model-name', '1.0')\r\n    >>> _parse_registry_model_version(\"registry:model-name\")\r\n    ('model-name', '')\r\n    >>> _parse_registry_model_version(\"registry:VERSION:v2\")\r\n    ('', 'v2')\r\n\r\n    \"\"\"\r\n    if not ckpt_path or not _is_registry(ckpt_path):\r\n        raise ValueError(f\"Invalid registry path: {ckpt_path}\")\r\n\r\n    parts = str(ckpt_path).split(\":\")\r\n    model_name, version = \"\", \"\"\r\n\r\n    if len(parts) >= 2 and parts[1].lower() != \"version\":\r\n        model_name = parts[1]\r\n    if len(parts) == 3 and parts[1].lower() == \"version\":\r\n        version = parts[2]\r\n    elif len(parts) == 4 and parts[2].lower() == \"version\":\r\n        version = parts[3]\r\n\r\n    return model_name, version", "code_tokens": ["def", "_parse_registry_model_version", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "tuple", "[", "str", ",", "str", "]", ":", "STRING", "if", "not", "ckpt_path", "or", "not", "_is_registry", "(", "ckpt_path", ")", ":", "raise", "ValueError", "(", "fSTRING", ")", "parts", "=", "str", "(", "ckpt_path", ")", ".", "split", "(", "STRING", ")", "model_name", ",", "version", "=", "STRING", ",", "STRING", "if", "len", "(", "parts", ")", ">", "=", "2", "and", "parts", "[", "1", "]", ".", "lower", "(", ")", "!", "=", "STRING", ":", "model_name", "=", "parts", "[", "1", "]", "if", "len", "(", "parts", ")", "=", "=", "3", "and", "parts", "[", "1", "]", ".", "lower", "(", ")", "=", "=", "STRING", ":", "version", "=", "parts", "[", "2", "]", "elif", "len", "(", "parts", ")", "=", "=", "4", "and", "parts", "[", "2", "]", ".", "lower", "(", ")", "=", "=", "STRING", ":", "version", "=", "parts", "[", "3", "]", "return", "model_name", ",", "version"], "docstring": "Parse the model version from a registry path.", "docstring_tokens": ["parse", "the", "model", "version", "from", "a", "registry", "path"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 52, "end_line": 85, "has_examples": true, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1033", "original_string": "def _determine_model_name(ckpt_path: Optional[_PATH], default_model_registry: Optional[str]) -> str:\r\n    \"\"\"Determine the model name from the checkpoint path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n        default_model_registry: The default model registry\r\n\r\n    Returns:\r\n        string name of the model with optional version\r\n\r\n    >>> _determine_model_name(\"registry:model-name:version:1.0\", \"default-model\")\r\n    'model-name:1.0'\r\n    >>> _determine_model_name(\"registry:model-name\", \"default-model\")\r\n    'model-name'\r\n    >>> _determine_model_name(\"registry:version:v2\", \"default-model\")\r\n    'default-model:v2'\r\n\r\n    \"\"\"\r\n    model_name, model_version = _parse_registry_model_version(ckpt_path)\r\n    if not model_name and default_model_registry:\r\n        model_name = default_model_registry\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{ckpt_path}'\")\r\n    model_registry = model_name\r\n    model_registry += f\":{model_version}\" if model_version else \"\"\r\n    return model_registry", "language": "python", "code": "def _determine_model_name(ckpt_path: Optional[_PATH], default_model_registry: Optional[str]) -> str:\r\n    \"\"\"Determine the model name from the checkpoint path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n        default_model_registry: The default model registry\r\n\r\n    Returns:\r\n        string name of the model with optional version\r\n\r\n    >>> _determine_model_name(\"registry:model-name:version:1.0\", \"default-model\")\r\n    'model-name:1.0'\r\n    >>> _determine_model_name(\"registry:model-name\", \"default-model\")\r\n    'model-name'\r\n    >>> _determine_model_name(\"registry:version:v2\", \"default-model\")\r\n    'default-model:v2'\r\n\r\n    \"\"\"\r\n    model_name, model_version = _parse_registry_model_version(ckpt_path)\r\n    if not model_name and default_model_registry:\r\n        model_name = default_model_registry\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{ckpt_path}'\")\r\n    model_registry = model_name\r\n    model_registry += f\":{model_version}\" if model_version else \"\"\r\n    return model_registry", "code_tokens": ["def", "_determine_model_name", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "default_model_registry", ":", "Optional", "[", "str", "]", ")", "-", ">", "str", ":", "STRING", "model_name", ",", "model_version", "=", "_parse_registry_model_version", "(", "ckpt_path", ")", "if", "not", "model_name", "and", "default_model_registry", ":", "model_name", "=", "default_model_registry", "if", "not", "model_name", ":", "raise", "ValueError", "(", "fSTRING", ")", "model_registry", "=", "model_name", "model_registry", "+", "=", "fSTRING", "if", "model_version", "else", "STRING", "return", "model_registry"], "docstring": "Determine the model name from the checkpoint path.", "docstring_tokens": ["determine", "the", "model", "name", "from", "the", "checkpoint", "path"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 88, "end_line": 115, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1034", "original_string": "def _determine_model_folder(model_name: str, default_root_dir: str) -> str:\r\n    \"\"\"Determine the local model folder based on the model registry.\r\n\r\n    Args:\r\n        model_name: The model name\r\n        default_root_dir: The default root directory\r\n\r\n    Returns:\r\n        string path to the local model folder\r\n\r\n    >>> _determine_model_folder(\"model-name\", \"/path/to/root\")\r\n    '/path/to/root/model-name'\r\n    >>> _determine_model_folder(\"model-name:1.0\", \"/path/to/root\")\r\n    '/path/to/root/model-name_1.0'\r\n\r\n    \"\"\"\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{model_name}'\")\r\n    model_name = model_name.replace(\"/\", \"_\")\r\n    model_name = model_name.replace(\":\", \"_\")\r\n    local_model_dir = os.path.join(default_root_dir, model_name)\r\n    return local_model_dir", "language": "python", "code": "def _determine_model_folder(model_name: str, default_root_dir: str) -> str:\r\n    \"\"\"Determine the local model folder based on the model registry.\r\n\r\n    Args:\r\n        model_name: The model name\r\n        default_root_dir: The default root directory\r\n\r\n    Returns:\r\n        string path to the local model folder\r\n\r\n    >>> _determine_model_folder(\"model-name\", \"/path/to/root\")\r\n    '/path/to/root/model-name'\r\n    >>> _determine_model_folder(\"model-name:1.0\", \"/path/to/root\")\r\n    '/path/to/root/model-name_1.0'\r\n\r\n    \"\"\"\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{model_name}'\")\r\n    model_name = model_name.replace(\"/\", \"_\")\r\n    model_name = model_name.replace(\":\", \"_\")\r\n    local_model_dir = os.path.join(default_root_dir, model_name)\r\n    return local_model_dir", "code_tokens": ["def", "_determine_model_folder", "(", "model_name", ":", "str", ",", "default_root_dir", ":", "str", ")", "-", ">", "str", ":", "STRING", "if", "not", "model_name", ":", "raise", "ValueError", "(", "fSTRING", ")", "model_name", "=", "model_name", ".", "replace", "(", "STRING", ",", "STRING", ")", "model_name", "=", "model_name", ".", "replace", "(", "STRING", ",", "STRING", ")", "local_model_dir", "=", "os", ".", "path", ".", "join", "(", "default_root_dir", ",", "model_name", ")", "return", "local_model_dir"], "docstring": "Determine the local model folder based on the model registry.", "docstring_tokens": ["determine", "the", "local", "model", "folder", "based", "on", "the", "model", "registry"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 118, "end_line": 140, "has_examples": true, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1035", "original_string": "def find_model_local_ckpt_path(\r\n    ckpt_path: Optional[_PATH], default_model_registry: Optional[str], default_root_dir: str\r\n) -> str:\r\n    \"\"\"Find the local checkpoint path for a model.\"\"\"\r\n    model_registry = _determine_model_name(ckpt_path, default_model_registry)\r\n    local_model_dir = _determine_model_folder(model_registry, default_root_dir)\r\n\r\n    folder_files = [fn for fn in os.listdir(local_model_dir) if fn.endswith(\".ckpt\")]\r\n    if not folder_files:\r\n        raise RuntimeError(f\"Parsing files from downloaded model: {model_registry}\")\r\n    return os.path.join(local_model_dir, folder_files[0])", "language": "python", "code": "def find_model_local_ckpt_path(\r\n    ckpt_path: Optional[_PATH], default_model_registry: Optional[str], default_root_dir: str\r\n) -> str:\r\n    \"\"\"Find the local checkpoint path for a model.\"\"\"\r\n    model_registry = _determine_model_name(ckpt_path, default_model_registry)\r\n    local_model_dir = _determine_model_folder(model_registry, default_root_dir)\r\n\r\n    folder_files = [fn for fn in os.listdir(local_model_dir) if fn.endswith(\".ckpt\")]\r\n    if not folder_files:\r\n        raise RuntimeError(f\"Parsing files from downloaded model: {model_registry}\")\r\n    return os.path.join(local_model_dir, folder_files[0])", "code_tokens": ["def", "find_model_local_ckpt_path", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "default_model_registry", ":", "Optional", "[", "str", "]", ",", "default_root_dir", ":", "str", ")", "-", ">", "str", ":", "STRING", "model_registry", "=", "_determine_model_name", "(", "ckpt_path", ",", "default_model_registry", ")", "local_model_dir", "=", "_determine_model_folder", "(", "model_registry", ",", "default_root_dir", ")", "folder_files", "=", "[", "fn", "for", "fn", "in", "os", ".", "listdir", "(", "local_model_dir", ")", "if", "fn", ".", "endswith", "(", "STRING", ")", "]", "if", "not", "folder_files", ":", "raise", "RuntimeError", "(", "fSTRING", ")", "return", "os", ".", "path", ".", "join", "(", "local_model_dir", ",", "folder_files", "[", "0", "]", ")"], "docstring": "Find the local checkpoint path for a model.", "docstring_tokens": ["find", "the", "local", "checkpoint", "path", "for", "a", "model"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 143, "end_line": 155, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "function_1036", "original_string": "def download_model_from_registry(ckpt_path: Optional[_PATH], trainer: \"pl.Trainer\") -> None:\r\n    \"\"\"Download a model from the Lightning Model Registry.\"\"\"\r\n    if trainer.local_rank == 0:\r\n        if not module_available(\"litmodels\"):\r\n            raise ImportError(\r\n                \"The `litmodels` package is not installed. Please install it with `pip install litmodels`.\"\r\n            )\r\n\r\n        from litmodels import download_model\r\n\r\n        model_registry = _determine_model_name(ckpt_path, trainer._model_registry)\r\n        local_model_dir = _determine_model_folder(model_registry, trainer.default_root_dir)\r\n\r\n        model_files = download_model(model_registry, download_dir=local_model_dir)\r\n        if not model_files:\r\n            raise RuntimeError(f\"Download model failed - {model_registry}\")\r\n\r\n    trainer.strategy.barrier(\"download_model_from_registry\")", "language": "python", "code": "def download_model_from_registry(ckpt_path: Optional[_PATH], trainer: \"pl.Trainer\") -> None:\r\n    \"\"\"Download a model from the Lightning Model Registry.\"\"\"\r\n    if trainer.local_rank == 0:\r\n        if not module_available(\"litmodels\"):\r\n            raise ImportError(\r\n                \"The `litmodels` package is not installed. Please install it with `pip install litmodels`.\"\r\n            )\r\n\r\n        from litmodels import download_model\r\n\r\n        model_registry = _determine_model_name(ckpt_path, trainer._model_registry)\r\n        local_model_dir = _determine_model_folder(model_registry, trainer.default_root_dir)\r\n\r\n        model_files = download_model(model_registry, download_dir=local_model_dir)\r\n        if not model_files:\r\n            raise RuntimeError(f\"Download model failed - {model_registry}\")\r\n\r\n    trainer.strategy.barrier(\"download_model_from_registry\")", "code_tokens": ["def", "download_model_from_registry", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "trainer", ":", "STRING", ")", "-", ">", "None", ":", "STRING", "if", "trainer", ".", "local_rank", "=", "=", "0", ":", "if", "not", "module_available", "(", "STRING", ")", ":", "raise", "ImportError", "(", "STRING", ")", "from", "litmodels", "import", "download_model", "model_registry", "=", "_determine_model_name", "(", "ckpt_path", ",", "trainer", ".", "_model_registry", ")", "local_model_dir", "=", "_determine_model_folder", "(", "model_registry", ",", "trainer", ".", "default_root_dir", ")", "model_files", "=", "download_model", "(", "model_registry", ",", "download_dir", "=", "local_model_dir", ")", "if", "not", "model_files", ":", "raise", "RuntimeError", "(", "fSTRING", ")", "trainer", ".", "strategy", ".", "barrier", "(", "STRING", ")"], "docstring": "Download a model from the Lightning Model Registry.", "docstring_tokens": ["download", "a", "model", "from", "the", "lightning", "model", "registry"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "start_line": 158, "end_line": 177, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parameter_tying.py", "func_name": "function_1037", "original_string": "def find_shared_parameters(module: nn.Module) -> list[str]:\r\n    \"\"\"Returns a list of names of shared parameters set in the module.\"\"\"\r\n    return _find_shared_parameters(module)", "language": "python", "code": "def find_shared_parameters(module: nn.Module) -> list[str]:\r\n    \"\"\"Returns a list of names of shared parameters set in the module.\"\"\"\r\n    return _find_shared_parameters(module)", "code_tokens": ["def", "find_shared_parameters", "(", "module", ":", "nn", ".", "Module", ")", "-", ">", "list", "[", "str", "]", ":", "STRING", "return", "_find_shared_parameters", "(", "module", ")"], "docstring": "Returns a list of names of shared parameters set in the module.", "docstring_tokens": ["returns", "a", "list", "of", "names", "of", "shared", "parameters", "set", "in", "the", "module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parameter_tying.py", "start_line": 25, "end_line": 27, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1038", "original_string": "def is_picklable(obj: object) -> bool:\r\n    \"\"\"Tests if an object can be pickled.\"\"\"\r\n    try:\r\n        pickle.dumps(obj)\r\n        return True\r\n    except (pickle.PickleError, AttributeError, RuntimeError, TypeError):\r\n        return False", "language": "python", "code": "def is_picklable(obj: object) -> bool:\r\n    \"\"\"Tests if an object can be pickled.\"\"\"\r\n    try:\r\n        pickle.dumps(obj)\r\n        return True\r\n    except (pickle.PickleError, AttributeError, RuntimeError, TypeError):\r\n        return False", "code_tokens": ["def", "is_picklable", "(", "obj", ":", "object", ")", "-", ">", "bool", ":", "STRING", "try", ":", "pickle", ".", "dumps", "(", "obj", ")", "return", "True", "except", "(", "pickle", ".", "PickleError", ",", "AttributeError", ",", "RuntimeError", ",", "TypeError", ")", ":", "return", "False"], "docstring": "Tests if an object can be pickled.", "docstring_tokens": ["tests", "if", "an", "object", "can", "be", "pickled"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 30, "end_line": 36, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1039", "original_string": "def clean_namespace(hparams: MutableMapping) -> None:\r\n    \"\"\"Removes all unpicklable entries from hparams.\"\"\"\r\n    del_attrs = [k for k, v in hparams.items() if not is_picklable(v)]\r\n\r\n    for k in del_attrs:\r\n        rank_zero_warn(\r\n            f\"Attribute '{k}' removed from hparams because it cannot be pickled. You can suppress this warning by\"\r\n            f\" setting `self.save_hyperparameters(ignore=['{k}'])`.\",\r\n        )\r\n        del hparams[k]", "language": "python", "code": "def clean_namespace(hparams: MutableMapping) -> None:\r\n    \"\"\"Removes all unpicklable entries from hparams.\"\"\"\r\n    del_attrs = [k for k, v in hparams.items() if not is_picklable(v)]\r\n\r\n    for k in del_attrs:\r\n        rank_zero_warn(\r\n            f\"Attribute '{k}' removed from hparams because it cannot be pickled. You can suppress this warning by\"\r\n            f\" setting `self.save_hyperparameters(ignore=['{k}'])`.\",\r\n        )\r\n        del hparams[k]", "code_tokens": ["def", "clean_namespace", "(", "hparams", ":", "MutableMapping", ")", "-", ">", "None", ":", "STRING", "del_attrs", "=", "[", "k", "for", "k", ",", "v", "in", "hparams", ".", "items", "(", ")", "if", "not", "is_picklable", "(", "v", ")", "]", "for", "k", "in", "del_attrs", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", ",", ")", "del", "hparams", "[", "k", "]"], "docstring": "Removes all unpicklable entries from hparams.", "docstring_tokens": ["removes", "all", "unpicklable", "entries", "from", "hparams"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 39, "end_line": 48, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1040", "original_string": "def parse_class_init_keys(cls: type) -> tuple[str, Optional[str], Optional[str]]:\r\n    \"\"\"Parse key words for standard ``self``, ``*args`` and ``**kwargs``.\r\n\r\n    Examples:\r\n\r\n        >>> class Model:\r\n        ...     def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):\r\n        ...         pass\r\n        >>> parse_class_init_keys(Model)\r\n        ('self', 'my_args', 'my_kwargs')\r\n\r\n    \"\"\"\r\n    init_parameters = inspect.signature(cls.__init__).parameters  # type: ignore[misc]\r\n    init_params = list(init_parameters.values())\r\n    n_self = init_params[0].name\r\n\r\n    def _get_first_if_any(\r\n        params: list[inspect.Parameter],\r\n        param_type: Literal[inspect._ParameterKind.VAR_POSITIONAL, inspect._ParameterKind.VAR_KEYWORD],\r\n    ) -> Optional[str]:\r\n        for p in params:\r\n            if p.kind == param_type:\r\n                return p.name\r\n        return None\r\n\r\n    n_args = _get_first_if_any(init_params, inspect.Parameter.VAR_POSITIONAL)\r\n    n_kwargs = _get_first_if_any(init_params, inspect.Parameter.VAR_KEYWORD)\r\n\r\n    return n_self, n_args, n_kwargs", "language": "python", "code": "def parse_class_init_keys(cls: type) -> tuple[str, Optional[str], Optional[str]]:\r\n    \"\"\"Parse key words for standard ``self``, ``*args`` and ``**kwargs``.\r\n\r\n    Examples:\r\n\r\n        >>> class Model:\r\n        ...     def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):\r\n        ...         pass\r\n        >>> parse_class_init_keys(Model)\r\n        ('self', 'my_args', 'my_kwargs')\r\n\r\n    \"\"\"\r\n    init_parameters = inspect.signature(cls.__init__).parameters  # type: ignore[misc]\r\n    init_params = list(init_parameters.values())\r\n    n_self = init_params[0].name\r\n\r\n    def _get_first_if_any(\r\n        params: list[inspect.Parameter],\r\n        param_type: Literal[inspect._ParameterKind.VAR_POSITIONAL, inspect._ParameterKind.VAR_KEYWORD],\r\n    ) -> Optional[str]:\r\n        for p in params:\r\n            if p.kind == param_type:\r\n                return p.name\r\n        return None\r\n\r\n    n_args = _get_first_if_any(init_params, inspect.Parameter.VAR_POSITIONAL)\r\n    n_kwargs = _get_first_if_any(init_params, inspect.Parameter.VAR_KEYWORD)\r\n\r\n    return n_self, n_args, n_kwargs", "code_tokens": ["def", "parse_class_init_keys", "(", "cls", ":", "type", ")", "-", ">", "tuple", "[", "str", ",", "Optional", "[", "str", "]", ",", "Optional", "[", "str", "]", "]", ":", "STRING", "init_parameters", "=", "inspect", ".", "signature", "(", "cls", ".", "__init__", ")", ".", "parameters", "#", "type", ":", "ignore", "[", "misc", "]", "init_params", "=", "list", "(", "init_parameters", ".", "values", "(", ")", ")", "n_self", "=", "init_params", "[", "0", "]", ".", "name", "def", "_get_first_if_any", "(", "params", ":", "list", "[", "inspect", ".", "Parameter", "]", ",", "param_type", ":", "Literal", "[", "inspect", ".", "_ParameterKind", ".", "VAR_POSITIONAL", ",", "inspect", ".", "_ParameterKind", ".", "VAR_KEYWORD", "]", ",", ")", "-", ">", "Optional", "[", "str", "]", ":", "for", "p", "in", "params", ":", "if", "p", ".", "kind", "=", "=", "param_type", ":", "return", "p", ".", "name", "return", "None", "n_args", "=", "_get_first_if_any", "(", "init_params", ",", "inspect", ".", "Parameter", ".", "VAR_POSITIONAL", ")", "n_kwargs", "=", "_get_first_if_any", "(", "init_params", ",", "inspect", ".", "Parameter", ".", "VAR_KEYWORD", ")", "return", "n_self", ",", "n_args", ",", "n_kwargs"], "docstring": "Parse key words for standard ``self``, ``*args`` and ``**kwargs``.", "docstring_tokens": ["parse", "key", "words", "for", "standard", "self", "args", "and", "kwargs"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 51, "end_line": 82, "has_examples": true, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1041", "original_string": "def get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args", "language": "python", "code": "def get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args", "code_tokens": ["def", "get_init_args", "(", "frame", ":", "types", ".", "FrameType", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "#", "pragma", ":", "no", "-", "cover", "STRING", "_", ",", "local_args", "=", "_get_init_args", "(", "frame", ")", "return", "local_args"], "docstring": "For backwards compatibility: #16369.", "docstring_tokens": ["for", "backwards", "compatibility", "16369"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 85, "end_line": 88, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1042", "original_string": "def collect_init_args(\r\n    frame: types.FrameType,\r\n    path_args: list[dict[str, Any]],\r\n    inside: bool = False,\r\n    classes: tuple[type, ...] = (),\r\n) -> list[dict[str, Any]]:\r\n    \"\"\"Recursively collects the arguments passed to the child constructors in the inheritance tree.\r\n\r\n    Args:\r\n        frame: the current stack frame\r\n        path_args: a list of dictionaries containing the constructor args in all parent classes\r\n        inside: track if we are inside inheritance path, avoid terminating too soon\r\n        classes: the classes in which to inspect the frames\r\n\r\n    Return:\r\n          A list of dictionaries where each dictionary contains the arguments passed to the\r\n          constructor at that level. The last entry corresponds to the constructor call of the\r\n          most specific class in the hierarchy.\r\n\r\n    \"\"\"\r\n    _, _, _, local_vars = inspect.getargvalues(frame)\r\n    if not isinstance(frame.f_back, types.FrameType):\r\n        return path_args\r\n\r\n    local_self, local_args = _get_init_args(frame)\r\n    if \"__class__\" in local_vars and (not classes or isinstance(local_self, classes)):\r\n        path_args.append(local_args)\r\n        return collect_init_args(frame.f_back, path_args, inside=True, classes=classes)\r\n    if not inside:\r\n        return collect_init_args(frame.f_back, path_args, inside=False, classes=classes)\r\n    return path_args", "language": "python", "code": "def collect_init_args(\r\n    frame: types.FrameType,\r\n    path_args: list[dict[str, Any]],\r\n    inside: bool = False,\r\n    classes: tuple[type, ...] = (),\r\n) -> list[dict[str, Any]]:\r\n    \"\"\"Recursively collects the arguments passed to the child constructors in the inheritance tree.\r\n\r\n    Args:\r\n        frame: the current stack frame\r\n        path_args: a list of dictionaries containing the constructor args in all parent classes\r\n        inside: track if we are inside inheritance path, avoid terminating too soon\r\n        classes: the classes in which to inspect the frames\r\n\r\n    Return:\r\n          A list of dictionaries where each dictionary contains the arguments passed to the\r\n          constructor at that level. The last entry corresponds to the constructor call of the\r\n          most specific class in the hierarchy.\r\n\r\n    \"\"\"\r\n    _, _, _, local_vars = inspect.getargvalues(frame)\r\n    if not isinstance(frame.f_back, types.FrameType):\r\n        return path_args\r\n\r\n    local_self, local_args = _get_init_args(frame)\r\n    if \"__class__\" in local_vars and (not classes or isinstance(local_self, classes)):\r\n        path_args.append(local_args)\r\n        return collect_init_args(frame.f_back, path_args, inside=True, classes=classes)\r\n    if not inside:\r\n        return collect_init_args(frame.f_back, path_args, inside=False, classes=classes)\r\n    return path_args", "code_tokens": ["def", "collect_init_args", "(", "frame", ":", "types", ".", "FrameType", ",", "path_args", ":", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ",", "inside", ":", "bool", "=", "False", ",", "classes", ":", "tuple", "[", "type", ",", ".", ".", ".", "]", "=", "(", ")", ",", ")", "-", ">", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ":", "STRING", "_", ",", "_", ",", "_", ",", "local_vars", "=", "inspect", ".", "getargvalues", "(", "frame", ")", "if", "not", "isinstance", "(", "frame", ".", "f_back", ",", "types", ".", "FrameType", ")", ":", "return", "path_args", "local_self", ",", "local_args", "=", "_get_init_args", "(", "frame", ")", "if", "STRING", "in", "local_vars", "and", "(", "not", "classes", "or", "isinstance", "(", "local_self", ",", "classes", ")", ")", ":", "path_args", ".", "append", "(", "local_args", ")", "return", "collect_init_args", "(", "frame", ".", "f_back", ",", "path_args", ",", "inside", "=", "True", ",", "classes", "=", "classes", ")", "if", "not", "inside", ":", "return", "collect_init_args", "(", "frame", ".", "f_back", ",", "path_args", ",", "inside", "=", "False", ",", "classes", "=", "classes", ")", "return", "path_args"], "docstring": "Recursively collects the arguments passed to the child constructors in the inheritance tree.", "docstring_tokens": ["recursively", "collects", "the", "arguments", "passed", "to", "the", "child", "constructors", "in", "the", "inheritance", "tree"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 110, "end_line": 142, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1043", "original_string": "def save_hyperparameters(\r\n    obj: Any,\r\n    *args: Any,\r\n    ignore: Optional[Union[Sequence[str], str]] = None,\r\n    frame: Optional[types.FrameType] = None,\r\n    given_hparams: Optional[dict[str, Any]] = None,\r\n) -> None:\r\n    \"\"\"See :meth:`~lightning.pytorch.LightningModule.save_hyperparameters`\"\"\"\r\n\r\n    if len(args) == 1 and not isinstance(args, str) and not args[0]:\r\n        return\r\n\r\n    if not frame:\r\n        current_frame = inspect.currentframe()\r\n        if current_frame:\r\n            frame = current_frame.f_back\r\n    if not isinstance(frame, types.FrameType):\r\n        raise AttributeError(\"There is no `frame` available while being required.\")\r\n\r\n    if given_hparams is not None:\r\n        init_args = given_hparams\r\n    elif is_dataclass(obj):\r\n        obj_fields = fields(obj)\r\n        init_args = {f.name: getattr(obj, f.name) for f in obj_fields if f.init}\r\n    else:\r\n        init_args = {}\r\n\r\n        from lightning.pytorch.core.mixins import HyperparametersMixin\r\n\r\n        for local_args in collect_init_args(frame, [], classes=(HyperparametersMixin,)):\r\n            init_args.update(local_args)\r\n\r\n    if ignore is None:\r\n        ignore = []\r\n    elif isinstance(ignore, str):\r\n        ignore = [ignore]\r\n    elif isinstance(ignore, (list, tuple)):\r\n        ignore = [arg for arg in ignore if isinstance(arg, str)]\r\n\r\n    ignore = list(set(ignore))\r\n    init_args = {k: v for k, v in init_args.items() if k not in ignore}\r\n\r\n    if not args:\r\n        hp = init_args\r\n        obj._hparams_name = \"kwargs\" if hp else None\r\n    else:\r\n        isx_non_str = [i for i, arg in enumerate(args) if not isinstance(arg, str)]\r\n        if len(isx_non_str) == 1:\r\n            hp = args[isx_non_str[0]]\r\n            cand_names = [k for k, v in init_args.items() if v == hp]\r\n            obj._hparams_name = cand_names[0] if cand_names else None\r\n        else:\r\n            hp = {arg: init_args[arg] for arg in args if isinstance(arg, str)}\r\n            obj._hparams_name = \"kwargs\"\r\n\r\n    obj._set_hparams(hp)\r\n\r\n    for k, v in obj._hparams.items():\r\n        if isinstance(v, nn.Module):\r\n            rank_zero_warn(\r\n                f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\r\n                f\" It is recommended to ignore them using `self.save_hyperparameters(ignore=[{k!r}])`.\"\r\n            )\r\n\r\n    obj._hparams_initial = copy.deepcopy(obj._hparams)", "language": "python", "code": "def save_hyperparameters(\r\n    obj: Any,\r\n    *args: Any,\r\n    ignore: Optional[Union[Sequence[str], str]] = None,\r\n    frame: Optional[types.FrameType] = None,\r\n    given_hparams: Optional[dict[str, Any]] = None,\r\n) -> None:\r\n    \"\"\"See :meth:`~lightning.pytorch.LightningModule.save_hyperparameters`\"\"\"\r\n\r\n    if len(args) == 1 and not isinstance(args, str) and not args[0]:\r\n        return\r\n\r\n    if not frame:\r\n        current_frame = inspect.currentframe()\r\n        if current_frame:\r\n            frame = current_frame.f_back\r\n    if not isinstance(frame, types.FrameType):\r\n        raise AttributeError(\"There is no `frame` available while being required.\")\r\n\r\n    if given_hparams is not None:\r\n        init_args = given_hparams\r\n    elif is_dataclass(obj):\r\n        obj_fields = fields(obj)\r\n        init_args = {f.name: getattr(obj, f.name) for f in obj_fields if f.init}\r\n    else:\r\n        init_args = {}\r\n\r\n        from lightning.pytorch.core.mixins import HyperparametersMixin\r\n\r\n        for local_args in collect_init_args(frame, [], classes=(HyperparametersMixin,)):\r\n            init_args.update(local_args)\r\n\r\n    if ignore is None:\r\n        ignore = []\r\n    elif isinstance(ignore, str):\r\n        ignore = [ignore]\r\n    elif isinstance(ignore, (list, tuple)):\r\n        ignore = [arg for arg in ignore if isinstance(arg, str)]\r\n\r\n    ignore = list(set(ignore))\r\n    init_args = {k: v for k, v in init_args.items() if k not in ignore}\r\n\r\n    if not args:\r\n        hp = init_args\r\n        obj._hparams_name = \"kwargs\" if hp else None\r\n    else:\r\n        isx_non_str = [i for i, arg in enumerate(args) if not isinstance(arg, str)]\r\n        if len(isx_non_str) == 1:\r\n            hp = args[isx_non_str[0]]\r\n            cand_names = [k for k, v in init_args.items() if v == hp]\r\n            obj._hparams_name = cand_names[0] if cand_names else None\r\n        else:\r\n            hp = {arg: init_args[arg] for arg in args if isinstance(arg, str)}\r\n            obj._hparams_name = \"kwargs\"\r\n\r\n    obj._set_hparams(hp)\r\n\r\n    for k, v in obj._hparams.items():\r\n        if isinstance(v, nn.Module):\r\n            rank_zero_warn(\r\n                f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\r\n                f\" It is recommended to ignore them using `self.save_hyperparameters(ignore=[{k!r}])`.\"\r\n            )\r\n\r\n    obj._hparams_initial = copy.deepcopy(obj._hparams)", "code_tokens": ["def", "save_hyperparameters", "(", "obj", ":", "Any", ",", "*", "args", ":", "Any", ",", "ignore", ":", "Optional", "[", "Union", "[", "Sequence", "[", "str", "]", ",", "str", "]", "]", "=", "None", ",", "frame", ":", "Optional", "[", "types", ".", "FrameType", "]", "=", "None", ",", "given_hparams", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "STRING", "if", "len", "(", "args", ")", "=", "=", "1", "and", "not", "isinstance", "(", "args", ",", "str", ")", "and", "not", "args", "[", "0", "]", ":", "return", "if", "not", "frame", ":", "current_frame", "=", "inspect", ".", "currentframe", "(", ")", "if", "current_frame", ":", "frame", "=", "current_frame", ".", "f_back", "if", "not", "isinstance", "(", "frame", ",", "types", ".", "FrameType", ")", ":", "raise", "AttributeError", "(", "STRING", ")", "if", "given_hparams", "is", "not", "None", ":", "init_args", "=", "given_hparams", "elif", "is_dataclass", "(", "obj", ")", ":", "obj_fields", "=", "fields", "(", "obj", ")", "init_args", "=", "{", "f", ".", "name", ":", "getattr", "(", "obj", ",", "f", ".", "name", ")", "for", "f", "in", "obj_fields", "if", "f", ".", "init", "}", "else", ":", "init_args", "=", "{", "}", "from", "lightning", ".", "pytorch", ".", "core", ".", "mixins", "import", "HyperparametersMixin", "for", "local_args", "in", "collect_init_args", "(", "frame", ",", "[", "]", ",", "classes", "=", "(", "HyperparametersMixin", ",", ")", ")", ":", "init_args", ".", "update", "(", "local_args", ")", "if", "ignore", "is", "None", ":", "ignore", "=", "[", "]", "elif", "isinstance", "(", "ignore", ",", "str", ")", ":", "ignore", "=", "[", "ignore", "]", "elif", "isinstance", "(", "ignore", ",", "(", "list", ",", "tuple", ")", ")", ":", "ignore", "=", "[", "arg", "for", "arg", "in", "ignore", "if", "isinstance", "(", "arg", ",", "str", ")", "]", "ignore", "=", "list", "(", "set", "(", "ignore", ")", ")", "init_args", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "init_args", ".", "items", "(", ")", "if", "k", "not", "in", "ignore", "}", "if", "not", "args", ":", "hp", "=", "init_args", "obj", ".", "_hparams_name", "=", "STRING", "if", "hp", "else", "None", "else", ":", "isx_non_str", "=", "[", "i", "for", "i", ",", "arg", "in", "enumerate", "(", "args", ")", "if", "not", "isinstance", "(", "arg", ",", "str", ")", "]", "if", "len", "(", "isx_non_str", ")", "=", "=", "1", ":", "hp", "=", "args", "[", "isx_non_str", "[", "0", "]", "]", "cand_names", "=", "[", "k", "for", "k", ",", "v", "in", "init_args", ".", "items", "(", ")", "if", "v", "=", "=", "hp", "]", "obj", ".", "_hparams_name", "=", "cand_names", "[", "0", "]", "if", "cand_names", "else", "None", "else", ":", "hp", "=", "{", "arg", ":", "init_args", "[", "arg", "]", "for", "arg", "in", "args", "if", "isinstance", "(", "arg", ",", "str", ")", "}", "obj", ".", "_hparams_name", "=", "STRING", "obj", ".", "_set_hparams", "(", "hp", ")", "for", "k", ",", "v", "in", "obj", ".", "_hparams", ".", "items", "(", ")", ":", "if", "isinstance", "(", "v", ",", "nn", ".", "Module", ")", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", ")", "obj", ".", "_hparams_initial", "=", "copy", ".", "deepcopy", "(", "obj", ".", "_hparams", ")"], "docstring": "See :meth:`~lightning.pytorch.LightningModule.save_hyperparameters`", "docstring_tokens": ["see", "meth", "lightning", "pytorch", "lightningmodule", "save_hyperparameters"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 145, "end_line": 215, "has_examples": false, "num_comments": 6, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1044", "original_string": "def _lightning_get_all_attr_holders(model: \"pl.LightningModule\", attribute: str) -> list[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets all of the objects or dicts that holds attribute. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders: list[Any] = []\r\n\r\n    if hasattr(model, attribute):\r\n        holders.append(model)\r\n\r\n    if hasattr(model, \"hparams\") and attribute in model.hparams:\r\n        holders.append(model.hparams)\r\n\r\n    trainer = model._trainer\r\n    if trainer is not None and trainer.datamodule is not None:\r\n        if hasattr(trainer.datamodule, attribute):\r\n            holders.append(trainer.datamodule)\r\n\r\n        if hasattr(trainer.datamodule, \"hparams\") and attribute in trainer.datamodule.hparams:\r\n            holders.append(trainer.datamodule.hparams)\r\n\r\n    return holders", "language": "python", "code": "def _lightning_get_all_attr_holders(model: \"pl.LightningModule\", attribute: str) -> list[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets all of the objects or dicts that holds attribute. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders: list[Any] = []\r\n\r\n    if hasattr(model, attribute):\r\n        holders.append(model)\r\n\r\n    if hasattr(model, \"hparams\") and attribute in model.hparams:\r\n        holders.append(model.hparams)\r\n\r\n    trainer = model._trainer\r\n    if trainer is not None and trainer.datamodule is not None:\r\n        if hasattr(trainer.datamodule, attribute):\r\n            holders.append(trainer.datamodule)\r\n\r\n        if hasattr(trainer.datamodule, \"hparams\") and attribute in trainer.datamodule.hparams:\r\n            holders.append(trainer.datamodule.hparams)\r\n\r\n    return holders", "code_tokens": ["def", "_lightning_get_all_attr_holders", "(", "model", ":", "STRING", ",", "attribute", ":", "str", ")", "-", ">", "list", "[", "Any", "]", ":", "STRING", "holders", ":", "list", "[", "Any", "]", "=", "[", "]", "if", "hasattr", "(", "model", ",", "attribute", ")", ":", "holders", ".", "append", "(", "model", ")", "if", "hasattr", "(", "model", ",", "STRING", ")", "and", "attribute", "in", "model", ".", "hparams", ":", "holders", ".", "append", "(", "model", ".", "hparams", ")", "trainer", "=", "model", ".", "_trainer", "if", "trainer", "is", "not", "None", "and", "trainer", ".", "datamodule", "is", "not", "None", ":", "if", "hasattr", "(", "trainer", ".", "datamodule", ",", "attribute", ")", ":", "holders", ".", "append", "(", "trainer", ".", "datamodule", ")", "if", "hasattr", "(", "trainer", ".", "datamodule", ",", "STRING", ")", "and", "attribute", "in", "trainer", ".", "datamodule", ".", "hparams", ":", "holders", ".", "append", "(", "trainer", ".", "datamodule", ".", "hparams", ")", "return", "holders"], "docstring": "Special attribute finding for Lightning.", "docstring_tokens": ["special", "attribute", "finding", "for", "lightning"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 236, "end_line": 262, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1045", "original_string": "def _lightning_get_first_attr_holder(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets the object or dict that holds attribute, or None. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule, returns the last one that has it.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        return None\r\n    return holders[-1]", "language": "python", "code": "def _lightning_get_first_attr_holder(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets the object or dict that holds attribute, or None. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule, returns the last one that has it.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        return None\r\n    return holders[-1]", "code_tokens": ["def", "_lightning_get_first_attr_holder", "(", "model", ":", "STRING", ",", "attribute", ":", "str", ")", "-", ">", "Optional", "[", "Any", "]", ":", "STRING", "holders", "=", "_lightning_get_all_attr_holders", "(", "model", ",", "attribute", ")", "if", "len", "(", "holders", ")", "=", "=", "0", ":", "return", "None", "return", "holders", "[", "-", "1", "]"], "docstring": "Special attribute finding for Lightning.", "docstring_tokens": ["special", "attribute", "finding", "for", "lightning"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 265, "end_line": 276, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1046", "original_string": "def lightning_hasattr(model: \"pl.LightningModule\", attribute: str) -> bool:\r\n    \"\"\"Special hasattr for Lightning.\r\n\r\n    Checks for attribute in model namespace, the old hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    return _lightning_get_first_attr_holder(model, attribute) is not None", "language": "python", "code": "def lightning_hasattr(model: \"pl.LightningModule\", attribute: str) -> bool:\r\n    \"\"\"Special hasattr for Lightning.\r\n\r\n    Checks for attribute in model namespace, the old hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    return _lightning_get_first_attr_holder(model, attribute) is not None", "code_tokens": ["def", "lightning_hasattr", "(", "model", ":", "STRING", ",", "attribute", ":", "str", ")", "-", ">", "bool", ":", "STRING", "return", "_lightning_get_first_attr_holder", "(", "model", ",", "attribute", ")", "is", "not", "None"], "docstring": "Special hasattr for Lightning.", "docstring_tokens": ["special", "hasattr", "for", "lightning"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 279, "end_line": 285, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1047", "original_string": "def lightning_getattr(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the\r\n    datamodule.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holder = _lightning_get_first_attr_holder(model, attribute)\r\n    if holder is None:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    if isinstance(holder, dict):\r\n        return holder[attribute]\r\n    return getattr(holder, attribute)", "language": "python", "code": "def lightning_getattr(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the\r\n    datamodule.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holder = _lightning_get_first_attr_holder(model, attribute)\r\n    if holder is None:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    if isinstance(holder, dict):\r\n        return holder[attribute]\r\n    return getattr(holder, attribute)", "code_tokens": ["def", "lightning_getattr", "(", "model", ":", "STRING", ",", "attribute", ":", "str", ")", "-", ">", "Optional", "[", "Any", "]", ":", "STRING", "holder", "=", "_lightning_get_first_attr_holder", "(", "model", ",", "attribute", ")", "if", "holder", "is", "None", ":", "raise", "AttributeError", "(", "fSTRING", "STRING", ")", "if", "isinstance", "(", "holder", ",", "dict", ")", ":", "return", "holder", "[", "attribute", "]", "return", "getattr", "(", "holder", ",", "attribute", ")"], "docstring": "Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the", "docstring_tokens": ["special", "getattr", "for", "lightning", "checks", "for", "attribute", "in", "model", "namespace", "the", "old", "hparams", "namespace", "dict", "and", "the"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 288, "end_line": 307, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "function_1048", "original_string": "def lightning_setattr(model: \"pl.LightningModule\", attribute: str, value: Any) -> None:\r\n    \"\"\"Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will\r\n    also set the attribute on datamodule, if it exists.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    for holder in holders:\r\n        if isinstance(holder, dict):\r\n            holder[attribute] = value\r\n        else:\r\n            setattr(holder, attribute, value)", "language": "python", "code": "def lightning_setattr(model: \"pl.LightningModule\", attribute: str, value: Any) -> None:\r\n    \"\"\"Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will\r\n    also set the attribute on datamodule, if it exists.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    for holder in holders:\r\n        if isinstance(holder, dict):\r\n            holder[attribute] = value\r\n        else:\r\n            setattr(holder, attribute, value)", "code_tokens": ["def", "lightning_setattr", "(", "model", ":", "STRING", ",", "attribute", ":", "str", ",", "value", ":", "Any", ")", "-", ">", "None", ":", "STRING", "holders", "=", "_lightning_get_all_attr_holders", "(", "model", ",", "attribute", ")", "if", "len", "(", "holders", ")", "=", "=", "0", ":", "raise", "AttributeError", "(", "fSTRING", "STRING", ")", "for", "holder", "in", "holders", ":", "if", "isinstance", "(", "holder", ",", "dict", ")", ":", "holder", "[", "attribute", "]", "=", "value", "else", ":", "setattr", "(", "holder", ",", "attribute", ",", "value", ")"], "docstring": "Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will", "docstring_tokens": ["special", "setattr", "for", "lightning", "checks", "for", "attribute", "in", "model", "namespace", "and", "the", "old", "hparams", "namespace", "dict", "will"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\parsing.py", "start_line": 310, "end_line": 331, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\seed.py", "func_name": "function_1049", "original_string": "def isolate_rng(include_cuda: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"A context manager that resets the global random state on exit to what it was before entering.\r\n\r\n    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\r\n\r\n    Args:\r\n        include_cuda: Whether to allow this function to also control the `torch.cuda` random number generator.\r\n            Set this to ``False`` when using the function in a forked process where CUDA re-initialization is\r\n            prohibited.\r\n\r\n    Example:\r\n        >>> import torch\r\n        >>> torch.manual_seed(1)  # doctest: +ELLIPSIS\r\n        <torch._C.Generator object at ...>\r\n        >>> with isolate_rng():\r\n        ...     [torch.rand(1) for _ in range(3)]\r\n        [tensor([0.7576]), tensor([0.2793]), tensor([0.4031])]\r\n        >>> torch.rand(1)\r\n        tensor([0.7576])\r\n\r\n    \"\"\"\r\n    states = _collect_rng_states(include_cuda)\r\n    yield\r\n    _set_rng_states(states)", "language": "python", "code": "def isolate_rng(include_cuda: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"A context manager that resets the global random state on exit to what it was before entering.\r\n\r\n    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\r\n\r\n    Args:\r\n        include_cuda: Whether to allow this function to also control the `torch.cuda` random number generator.\r\n            Set this to ``False`` when using the function in a forked process where CUDA re-initialization is\r\n            prohibited.\r\n\r\n    Example:\r\n        >>> import torch\r\n        >>> torch.manual_seed(1)  # doctest: +ELLIPSIS\r\n        <torch._C.Generator object at ...>\r\n        >>> with isolate_rng():\r\n        ...     [torch.rand(1) for _ in range(3)]\r\n        [tensor([0.7576]), tensor([0.2793]), tensor([0.4031])]\r\n        >>> torch.rand(1)\r\n        tensor([0.7576])\r\n\r\n    \"\"\"\r\n    states = _collect_rng_states(include_cuda)\r\n    yield\r\n    _set_rng_states(states)", "code_tokens": ["def", "isolate_rng", "(", "include_cuda", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "STRING", "states", "=", "_collect_rng_states", "(", "include_cuda", ")", "yield", "_set_rng_states", "(", "states", ")"], "docstring": "A context manager that resets the global random state on exit to what it was before entering.", "docstring_tokens": ["a", "context", "manager", "that", "resets", "the", "global", "random", "state", "on", "exit", "to", "what", "it", "was", "before", "entering"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\seed.py", "start_line": 22, "end_line": 45, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\signature_utils.py", "func_name": "function_1050", "original_string": "def is_param_in_hook_signature(\r\n    hook_fx: Callable, param: str, explicit: bool = False, min_args: Optional[int] = None\r\n) -> bool:\r\n    \"\"\"\r\n    Args:\r\n        hook_fx: the hook callable\r\n        param: the name of the parameter to check\r\n        explicit: whether the parameter has to be explicitly declared\r\n        min_args: whether the `signature` has at least `min_args` parameters\r\n    \"\"\"\r\n    if hasattr(hook_fx, \"__wrapped__\"):\r\n        hook_fx = hook_fx.__wrapped__\r\n    parameters = inspect.getfullargspec(hook_fx)\r\n    args = parameters.args[1:]  # ignore `self`\r\n    return (\r\n        param in args\r\n        or (not explicit and (parameters.varargs is not None))\r\n        or (isinstance(min_args, int) and len(args) >= min_args)\r\n    )", "language": "python", "code": "def is_param_in_hook_signature(\r\n    hook_fx: Callable, param: str, explicit: bool = False, min_args: Optional[int] = None\r\n) -> bool:\r\n    \"\"\"\r\n    Args:\r\n        hook_fx: the hook callable\r\n        param: the name of the parameter to check\r\n        explicit: whether the parameter has to be explicitly declared\r\n        min_args: whether the `signature` has at least `min_args` parameters\r\n    \"\"\"\r\n    if hasattr(hook_fx, \"__wrapped__\"):\r\n        hook_fx = hook_fx.__wrapped__\r\n    parameters = inspect.getfullargspec(hook_fx)\r\n    args = parameters.args[1:]  # ignore `self`\r\n    return (\r\n        param in args\r\n        or (not explicit and (parameters.varargs is not None))\r\n        or (isinstance(min_args, int) and len(args) >= min_args)\r\n    )", "code_tokens": ["def", "is_param_in_hook_signature", "(", "hook_fx", ":", "Callable", ",", "param", ":", "str", ",", "explicit", ":", "bool", "=", "False", ",", "min_args", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "bool", ":", "STRING", "if", "hasattr", "(", "hook_fx", ",", "STRING", ")", ":", "hook_fx", "=", "hook_fx", ".", "__wrapped__", "parameters", "=", "inspect", ".", "getfullargspec", "(", "hook_fx", ")", "args", "=", "parameters", ".", "args", "[", "1", ":", "]", "#", "ignore", "`", "self", "`", "return", "(", "param", "in", "args", "or", "(", "not", "explicit", "and", "(", "parameters", ".", "varargs", "is", "not", "None", ")", ")", "or", "(", "isinstance", "(", "min_args", ",", "int", ")", "and", "len", "(", "args", ")", ">", "=", "min_args", ")", ")"], "docstring": "Args:", "docstring_tokens": ["args"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\signature_utils.py", "start_line": 17, "end_line": 36, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "func_name": "function_1051", "original_string": "def _is_leaf_or_primitive_container(pytree: PyTree) -> bool:\r\n    \"\"\"Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.\"\"\"\r\n    is_leaf = _get_node_type(pytree) not in SUPPORTED_NODES\r\n    if is_leaf:\r\n        return True\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, _ = flatten_fn(pytree)\r\n    return all(isinstance(child, (int, float, str)) for child in child_pytrees)", "language": "python", "code": "def _is_leaf_or_primitive_container(pytree: PyTree) -> bool:\r\n    \"\"\"Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.\"\"\"\r\n    is_leaf = _get_node_type(pytree) not in SUPPORTED_NODES\r\n    if is_leaf:\r\n        return True\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, _ = flatten_fn(pytree)\r\n    return all(isinstance(child, (int, float, str)) for child in child_pytrees)", "code_tokens": ["def", "_is_leaf_or_primitive_container", "(", "pytree", ":", "PyTree", ")", "-", ">", "bool", ":", "STRING", "is_leaf", "=", "_get_node_type", "(", "pytree", ")", "not", "in", "SUPPORTED_NODES", "if", "is_leaf", ":", "return", "True", "node_type", "=", "_get_node_type", "(", "pytree", ")", "flatten_fn", "=", "SUPPORTED_NODES", "[", "node_type", "]", ".", "flatten_fn", "child_pytrees", ",", "_", "=", "flatten_fn", "(", "pytree", ")", "return", "all", "(", "isinstance", "(", "child", ",", "(", "int", ",", "float", ",", "str", ")", ")", "for", "child", "in", "child_pytrees", ")"], "docstring": "Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.", "docstring_tokens": ["customized", "func", "torch", "utils", "_pytree", "_is_leaf", "to", "avoid", "flattening", "containers", "of", "primitives"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "start_line": 5, "end_line": 14, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "func_name": "function_1052", "original_string": "def _tree_flatten(pytree: PyTree) -> tuple[list[Any], TreeSpec]:\r\n    \"\"\"Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.\"\"\"\r\n    if _is_leaf_or_primitive_container(pytree):\r\n        return [pytree], LeafSpec()\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, context = flatten_fn(pytree)\r\n\r\n    result: list[Any] = []\r\n    children_specs: list[TreeSpec] = []\r\n    for child in child_pytrees:\r\n        flat, child_spec = _tree_flatten(child)\r\n        result += flat\r\n        children_specs.append(child_spec)\r\n\r\n    return result, TreeSpec(node_type, context, children_specs)", "language": "python", "code": "def _tree_flatten(pytree: PyTree) -> tuple[list[Any], TreeSpec]:\r\n    \"\"\"Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.\"\"\"\r\n    if _is_leaf_or_primitive_container(pytree):\r\n        return [pytree], LeafSpec()\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, context = flatten_fn(pytree)\r\n\r\n    result: list[Any] = []\r\n    children_specs: list[TreeSpec] = []\r\n    for child in child_pytrees:\r\n        flat, child_spec = _tree_flatten(child)\r\n        result += flat\r\n        children_specs.append(child_spec)\r\n\r\n    return result, TreeSpec(node_type, context, children_specs)", "code_tokens": ["def", "_tree_flatten", "(", "pytree", ":", "PyTree", ")", "-", ">", "tuple", "[", "list", "[", "Any", "]", ",", "TreeSpec", "]", ":", "STRING", "if", "_is_leaf_or_primitive_container", "(", "pytree", ")", ":", "return", "[", "pytree", "]", ",", "LeafSpec", "(", ")", "node_type", "=", "_get_node_type", "(", "pytree", ")", "flatten_fn", "=", "SUPPORTED_NODES", "[", "node_type", "]", ".", "flatten_fn", "child_pytrees", ",", "context", "=", "flatten_fn", "(", "pytree", ")", "result", ":", "list", "[", "Any", "]", "=", "[", "]", "children_specs", ":", "list", "[", "TreeSpec", "]", "=", "[", "]", "for", "child", "in", "child_pytrees", ":", "flat", ",", "child_spec", "=", "_tree_flatten", "(", "child", ")", "result", "+", "=", "flat", "children_specs", ".", "append", "(", "child_spec", ")", "return", "result", ",", "TreeSpec", "(", "node_type", ",", "context", ",", "children_specs", ")"], "docstring": "Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.", "docstring_tokens": ["copy", "of", "func", "torch", "utils", "_pytree", "tree_flatten", "using", "our", "custom", "leaf", "function"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "start_line": 17, "end_line": 33, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "func_name": "function_1053", "original_string": "def _map_and_unflatten(fn: Any, values: list[Any], spec: TreeSpec) -> PyTree:\r\n    \"\"\"Utility function to apply a function and unflatten it.\"\"\"\r\n    return tree_unflatten([fn(i) for i in values], spec)", "language": "python", "code": "def _map_and_unflatten(fn: Any, values: list[Any], spec: TreeSpec) -> PyTree:\r\n    \"\"\"Utility function to apply a function and unflatten it.\"\"\"\r\n    return tree_unflatten([fn(i) for i in values], spec)", "code_tokens": ["def", "_map_and_unflatten", "(", "fn", ":", "Any", ",", "values", ":", "list", "[", "Any", "]", ",", "spec", ":", "TreeSpec", ")", "-", ">", "PyTree", ":", "STRING", "return", "tree_unflatten", "(", "[", "fn", "(", "i", ")", "for", "i", "in", "values", "]", ",", "spec", ")"], "docstring": "Utility function to apply a function and unflatten it.", "docstring_tokens": ["utility", "function", "to", "apply", "a", "function", "and", "unflatten", "it"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "start_line": 36, "end_line": 38, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1054", "original_string": "def _migration_index() -> dict[str, list[Callable[[_CHECKPOINT], _CHECKPOINT]]]:\r\n    \"\"\"Migration functions returned here will get executed in the order they are listed.\"\"\"\r\n    return {\r\n        \"0.10.0\": [_migrate_model_checkpoint_early_stopping],\r\n        \"1.6.0\": [_migrate_loop_global_step_to_progress_tracking, _migrate_loop_current_epoch_to_progress_tracking],\r\n        \"1.6.5\": [_migrate_loop_batches_that_stepped],\r\n        \"1.9.0\": [_migrate_model_checkpoint_save_on_train_epoch_end_default],\r\n        \"2.0.0\": [\r\n            _drop_apex_amp_state,\r\n            _migrate_loop_structure_after_tbptt_removal,\r\n            _migrate_loop_structure_after_optimizer_loop_removal,\r\n            _migrate_loop_structure_after_dataloader_loop_removal,\r\n        ],\r\n    }", "language": "python", "code": "def _migration_index() -> dict[str, list[Callable[[_CHECKPOINT], _CHECKPOINT]]]:\r\n    \"\"\"Migration functions returned here will get executed in the order they are listed.\"\"\"\r\n    return {\r\n        \"0.10.0\": [_migrate_model_checkpoint_early_stopping],\r\n        \"1.6.0\": [_migrate_loop_global_step_to_progress_tracking, _migrate_loop_current_epoch_to_progress_tracking],\r\n        \"1.6.5\": [_migrate_loop_batches_that_stepped],\r\n        \"1.9.0\": [_migrate_model_checkpoint_save_on_train_epoch_end_default],\r\n        \"2.0.0\": [\r\n            _drop_apex_amp_state,\r\n            _migrate_loop_structure_after_tbptt_removal,\r\n            _migrate_loop_structure_after_optimizer_loop_removal,\r\n            _migrate_loop_structure_after_dataloader_loop_removal,\r\n        ],\r\n    }", "code_tokens": ["def", "_migration_index", "(", ")", "-", ">", "dict", "[", "str", ",", "list", "[", "Callable", "[", "[", "_CHECKPOINT", "]", ",", "_CHECKPOINT", "]", "]", "]", ":", "STRING", "return", "{", "STRING", ":", "[", "_migrate_model_checkpoint_early_stopping", "]", ",", "STRING", ":", "[", "_migrate_loop_global_step_to_progress_tracking", ",", "_migrate_loop_current_epoch_to_progress_tracking", "]", ",", "STRING", ":", "[", "_migrate_loop_batches_that_stepped", "]", ",", "STRING", ":", "[", "_migrate_model_checkpoint_save_on_train_epoch_end_default", "]", ",", "STRING", ":", "[", "_drop_apex_amp_state", ",", "_migrate_loop_structure_after_tbptt_removal", ",", "_migrate_loop_structure_after_optimizer_loop_removal", ",", "_migrate_loop_structure_after_dataloader_loop_removal", ",", "]", ",", "}"], "docstring": "Migration functions returned here will get executed in the order they are listed.", "docstring_tokens": ["migration", "functions", "returned", "here", "will", "get", "executed", "in", "the", "order", "they", "are", "listed"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 43, "end_line": 56, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1055", "original_string": "def _migrate_model_checkpoint_early_stopping(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The checkpoint and early stopping keys were renamed.\r\n\r\n    Version: 0.10.0\r\n    Commit: a5d1176\r\n\r\n    \"\"\"\r\n    keys_mapping = {\r\n        \"checkpoint_callback_best_model_score\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"checkpoint_callback_best_model_path\": (ModelCheckpoint, \"best_model_path\"),\r\n        \"checkpoint_callback_best\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"early_stop_callback_wait\": (EarlyStopping, \"wait_count\"),\r\n        \"early_stop_callback_patience\": (EarlyStopping, \"patience\"),\r\n    }\r\n    checkpoint[\"callbacks\"] = checkpoint.get(\"callbacks\") or {}\r\n\r\n    for key, new_path in keys_mapping.items():\r\n        if key in checkpoint:\r\n            value = checkpoint[key]\r\n            callback_type, callback_key = new_path\r\n            checkpoint[\"callbacks\"][callback_type] = checkpoint[\"callbacks\"].get(callback_type) or {}\r\n            checkpoint[\"callbacks\"][callback_type][callback_key] = value\r\n            del checkpoint[key]\r\n    return checkpoint", "language": "python", "code": "def _migrate_model_checkpoint_early_stopping(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The checkpoint and early stopping keys were renamed.\r\n\r\n    Version: 0.10.0\r\n    Commit: a5d1176\r\n\r\n    \"\"\"\r\n    keys_mapping = {\r\n        \"checkpoint_callback_best_model_score\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"checkpoint_callback_best_model_path\": (ModelCheckpoint, \"best_model_path\"),\r\n        \"checkpoint_callback_best\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"early_stop_callback_wait\": (EarlyStopping, \"wait_count\"),\r\n        \"early_stop_callback_patience\": (EarlyStopping, \"patience\"),\r\n    }\r\n    checkpoint[\"callbacks\"] = checkpoint.get(\"callbacks\") or {}\r\n\r\n    for key, new_path in keys_mapping.items():\r\n        if key in checkpoint:\r\n            value = checkpoint[key]\r\n            callback_type, callback_key = new_path\r\n            checkpoint[\"callbacks\"][callback_type] = checkpoint[\"callbacks\"].get(callback_type) or {}\r\n            checkpoint[\"callbacks\"][callback_type][callback_key] = value\r\n            del checkpoint[key]\r\n    return checkpoint", "code_tokens": ["def", "_migrate_model_checkpoint_early_stopping", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "keys_mapping", "=", "{", "STRING", ":", "(", "ModelCheckpoint", ",", "STRING", ")", ",", "STRING", ":", "(", "ModelCheckpoint", ",", "STRING", ")", ",", "STRING", ":", "(", "ModelCheckpoint", ",", "STRING", ")", ",", "STRING", ":", "(", "EarlyStopping", ",", "STRING", ")", ",", "STRING", ":", "(", "EarlyStopping", ",", "STRING", ")", ",", "}", "checkpoint", "[", "STRING", "]", "=", "checkpoint", ".", "get", "(", "STRING", ")", "or", "{", "}", "for", "key", ",", "new_path", "in", "keys_mapping", ".", "items", "(", ")", ":", "if", "key", "in", "checkpoint", ":", "value", "=", "checkpoint", "[", "key", "]", "callback_type", ",", "callback_key", "=", "new_path", "checkpoint", "[", "STRING", "]", "[", "callback_type", "]", "=", "checkpoint", "[", "STRING", "]", ".", "get", "(", "callback_type", ")", "or", "{", "}", "checkpoint", "[", "STRING", "]", "[", "callback_type", "]", "[", "callback_key", "]", "=", "value", "del", "checkpoint", "[", "key", "]", "return", "checkpoint"], "docstring": "The checkpoint and early stopping keys were renamed.", "docstring_tokens": ["the", "checkpoint", "and", "early", "stopping", "keys", "were", "renamed"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 59, "end_line": 82, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1056", "original_string": "def _migrate_loop_global_step_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: c67b075\r\n    PR: #13645, #11805\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    optim_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.optimizer_loop.optim_progress\"]\r\n    optim_progress[\"optimizer\"][\"step\"][\"total\"][\"completed\"] = global_step\r\n    optim_step_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.manual_loop.optim_step_progress\"]\r\n    optim_step_progress[\"total\"][\"completed\"] = global_step\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_global_step_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: c67b075\r\n    PR: #13645, #11805\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    optim_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.optimizer_loop.optim_progress\"]\r\n    optim_progress[\"optimizer\"][\"step\"][\"total\"][\"completed\"] = global_step\r\n    optim_step_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.manual_loop.optim_step_progress\"]\r\n    optim_step_progress[\"total\"][\"completed\"] = global_step\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_global_step_to_progress_tracking", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "global_step", "=", "checkpoint", "[", "STRING", "]", "checkpoint", ".", "setdefault", "(", "STRING", ",", "{", "STRING", ":", "_get_fit_loop_initial_state_1_6_0", "(", ")", "}", ")", "checkpoint", "[", "STRING", "]", ".", "setdefault", "(", "STRING", ",", "_get_fit_loop_initial_state_1_6_0", "(", ")", ")", "optim_progress", "=", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "optim_progress", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "=", "global_step", "optim_step_progress", "=", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "optim_step_progress", "[", "STRING", "]", "[", "STRING", "]", "=", "global_step", "return", "checkpoint"], "docstring": "Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be", "docstring_tokens": ["sets", "the", "global_step", "value", "for", "checkpoints", "before", "v1", "6", "without", "the", "progress", "tracking", "state", "it", "will", "be"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 85, "end_line": 103, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1057", "original_string": "def _migrate_loop_current_epoch_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: aea96e4\r\n    PR: #11805\r\n\r\n    \"\"\"\r\n    epoch = checkpoint[\"epoch\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_progress\"][\"current\"][\"completed\"] = epoch\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_current_epoch_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: aea96e4\r\n    PR: #11805\r\n\r\n    \"\"\"\r\n    epoch = checkpoint[\"epoch\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_progress\"][\"current\"][\"completed\"] = epoch\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_current_epoch_to_progress_tracking", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "epoch", "=", "checkpoint", "[", "STRING", "]", "checkpoint", ".", "setdefault", "(", "STRING", ",", "{", "STRING", ":", "_get_fit_loop_initial_state_1_6_0", "(", ")", "}", ")", "checkpoint", "[", "STRING", "]", ".", "setdefault", "(", "STRING", ",", "_get_fit_loop_initial_state_1_6_0", "(", ")", ")", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", "=", "epoch", "return", "checkpoint"], "docstring": "Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be", "docstring_tokens": ["sets", "the", "current_epoch", "value", "for", "checkpoints", "before", "v1", "6", "without", "the", "progress", "tracking", "state", "it", "will", "be"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 106, "end_line": 119, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1058", "original_string": "def _migrate_loop_batches_that_stepped(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.\r\n\r\n    Version: 1.6.5\r\n    Commit: c67b075\r\n    PR: #13645\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.state_dict\"].setdefault(\"_batches_that_stepped\", global_step)\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_batches_that_stepped(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.\r\n\r\n    Version: 1.6.5\r\n    Commit: c67b075\r\n    PR: #13645\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.state_dict\"].setdefault(\"_batches_that_stepped\", global_step)\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_batches_that_stepped", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "global_step", "=", "checkpoint", "[", "STRING", "]", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "[", "STRING", "]", ".", "setdefault", "(", "STRING", ",", "global_step", ")", "return", "checkpoint"], "docstring": "Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.", "docstring_tokens": ["sets", "the", "_batches_that_stepped", "default", "value", "for", "checkpoints", "before", "v1", "6", "5", "which", "don", "t", "have", "this", "key"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 122, "end_line": 132, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1059", "original_string": "def _migrate_model_checkpoint_save_on_train_epoch_end_default(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this\r\n    migration drops it from the state-keys saved in the checkpoint dict so that the keys match when the Trainer loads\r\n    the callback state.\r\n\r\n    Version: 1.9.0\r\n    Commit: f4ca56\r\n    PR: #15300, #15606\r\n\r\n    \"\"\"\r\n    if \"callbacks\" not in checkpoint:\r\n        return checkpoint\r\n\r\n    def new_key(old_key: str) -> str:\r\n        if not old_key.startswith(\"ModelCheckpoint\"):\r\n            return old_key\r\n        return re.sub(\", 'save_on_train_epoch_end': (None|True|False)\", \"\", old_key)\r\n\r\n    num_keys = len(checkpoint[\"callbacks\"])\r\n    new_callback_states = {\r\n        new_key(old_key): state for old_key, state in checkpoint[\"callbacks\"].items() if isinstance(old_key, str)\r\n    }\r\n    if len(new_callback_states) < num_keys:\r\n        rank_zero_warn(\r\n            \"You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys\"\r\n            \" that would end up colliding with each other after an upgrade, which means we can't differentiate\"\r\n            \" which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint`\"\r\n            \" callbacks will not be able to reload the state.\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint\r\n\r\n    checkpoint[\"callbacks\"] = new_callback_states\r\n    return checkpoint", "language": "python", "code": "def _migrate_model_checkpoint_save_on_train_epoch_end_default(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this\r\n    migration drops it from the state-keys saved in the checkpoint dict so that the keys match when the Trainer loads\r\n    the callback state.\r\n\r\n    Version: 1.9.0\r\n    Commit: f4ca56\r\n    PR: #15300, #15606\r\n\r\n    \"\"\"\r\n    if \"callbacks\" not in checkpoint:\r\n        return checkpoint\r\n\r\n    def new_key(old_key: str) -> str:\r\n        if not old_key.startswith(\"ModelCheckpoint\"):\r\n            return old_key\r\n        return re.sub(\", 'save_on_train_epoch_end': (None|True|False)\", \"\", old_key)\r\n\r\n    num_keys = len(checkpoint[\"callbacks\"])\r\n    new_callback_states = {\r\n        new_key(old_key): state for old_key, state in checkpoint[\"callbacks\"].items() if isinstance(old_key, str)\r\n    }\r\n    if len(new_callback_states) < num_keys:\r\n        rank_zero_warn(\r\n            \"You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys\"\r\n            \" that would end up colliding with each other after an upgrade, which means we can't differentiate\"\r\n            \" which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint`\"\r\n            \" callbacks will not be able to reload the state.\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint\r\n\r\n    checkpoint[\"callbacks\"] = new_callback_states\r\n    return checkpoint", "code_tokens": ["def", "_migrate_model_checkpoint_save_on_train_epoch_end_default", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "if", "STRING", "not", "in", "checkpoint", ":", "return", "checkpoint", "def", "new_key", "(", "old_key", ":", "str", ")", "-", ">", "str", ":", "if", "not", "old_key", ".", "startswith", "(", "STRING", ")", ":", "return", "old_key", "return", "re", ".", "sub", "(", "STRING", ",", "STRING", ",", "old_key", ")", "num_keys", "=", "len", "(", "checkpoint", "[", "STRING", "]", ")", "new_callback_states", "=", "{", "new_key", "(", "old_key", ")", ":", "state", "for", "old_key", ",", "state", "in", "checkpoint", "[", "STRING", "]", ".", "items", "(", ")", "if", "isinstance", "(", "old_key", ",", "str", ")", "}", "if", "len", "(", "new_callback_states", ")", "<", "num_keys", ":", "rank_zero_warn", "(", "STRING", "STRING", "STRING", "STRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "checkpoint", "checkpoint", "[", "STRING", "]", "=", "new_callback_states", "return", "checkpoint"], "docstring": "The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this", "docstring_tokens": ["the", "save_on_train_epoch_end", "was", "removed", "from", "the", "state", "key", "of", "modelcheckpoint", "in", "1", "9", "0", "and", "this"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 183, "end_line": 217, "has_examples": false, "num_comments": 1, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1060", "original_string": "def _drop_apex_amp_state(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint\r\n    dict.\r\n\r\n    Version: 2.0.0\r\n    Commit: e544676ff434ed96c6dd3b4e73a708bcb27ebcf1\r\n    PR: #16149\r\n\r\n    \"\"\"\r\n    key = \"amp_scaling_state\"\r\n    if key in checkpoint:\r\n        rank_zero_warn(\"This checkpoint contains apex AMP data, but apex support has been removed in v2.0.0.\")\r\n        del checkpoint[key]\r\n    return checkpoint", "language": "python", "code": "def _drop_apex_amp_state(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint\r\n    dict.\r\n\r\n    Version: 2.0.0\r\n    Commit: e544676ff434ed96c6dd3b4e73a708bcb27ebcf1\r\n    PR: #16149\r\n\r\n    \"\"\"\r\n    key = \"amp_scaling_state\"\r\n    if key in checkpoint:\r\n        rank_zero_warn(\"This checkpoint contains apex AMP data, but apex support has been removed in v2.0.0.\")\r\n        del checkpoint[key]\r\n    return checkpoint", "code_tokens": ["def", "_drop_apex_amp_state", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "key", "=", "STRING", "if", "key", "in", "checkpoint", ":", "rank_zero_warn", "(", "STRING", ")", "del", "checkpoint", "[", "key", "]", "return", "checkpoint"], "docstring": "Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint", "docstring_tokens": ["apex", "support", "was", "removed", "in", "v2", "0", "0", "and", "this", "migration", "drops", "it", "from", "the", "state", "keys", "saved", "in", "the", "checkpoint"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 220, "end_line": 233, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1061", "original_string": "def _migrate_loop_structure_after_tbptt_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The\r\n    optimizer loop and the manual loop were previously children of the training batch loop. After its removal, they\r\n    became the children of the training epoch loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 7807454\r\n    PR: #16337, #16172\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    old_key_new_key_mapping = {\r\n        \"epoch_loop.batch_loop.manual_loop.optim_step_progress\": \"epoch_loop.manual_loop.optim_step_progress\",\r\n        \"epoch_loop.batch_loop.manual_loop.state_dict\": \"epoch_loop.manual_loop.state_dict\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.optim_progress\": \"epoch_loop.optimizer_loop.optim_progress\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.state_dict\": \"epoch_loop.optimizer_loop.state_dict\",\r\n    }\r\n    for old, new in list(old_key_new_key_mapping.items()):\r\n        if old in fit_loop:\r\n            fit_loop[new] = fit_loop[old]\r\n            del fit_loop[old]\r\n\r\n    if \"epoch_loop.batch_loop.state_dict\" in fit_loop and fit_loop[\"epoch_loop.batch_loop.state_dict\"]:\r\n        fit_loop[\"epoch_loop.state_dict\"][\"old_batch_loop_state_dict\"] = fit_loop[\"epoch_loop.batch_loop.state_dict\"]\r\n    fit_loop.pop(\"epoch_loop.batch_loop.state_dict\", None)\r\n\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_tbptt_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The\r\n    optimizer loop and the manual loop were previously children of the training batch loop. After its removal, they\r\n    became the children of the training epoch loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 7807454\r\n    PR: #16337, #16172\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    old_key_new_key_mapping = {\r\n        \"epoch_loop.batch_loop.manual_loop.optim_step_progress\": \"epoch_loop.manual_loop.optim_step_progress\",\r\n        \"epoch_loop.batch_loop.manual_loop.state_dict\": \"epoch_loop.manual_loop.state_dict\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.optim_progress\": \"epoch_loop.optimizer_loop.optim_progress\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.state_dict\": \"epoch_loop.optimizer_loop.state_dict\",\r\n    }\r\n    for old, new in list(old_key_new_key_mapping.items()):\r\n        if old in fit_loop:\r\n            fit_loop[new] = fit_loop[old]\r\n            del fit_loop[old]\r\n\r\n    if \"epoch_loop.batch_loop.state_dict\" in fit_loop and fit_loop[\"epoch_loop.batch_loop.state_dict\"]:\r\n        fit_loop[\"epoch_loop.state_dict\"][\"old_batch_loop_state_dict\"] = fit_loop[\"epoch_loop.batch_loop.state_dict\"]\r\n    fit_loop.pop(\"epoch_loop.batch_loop.state_dict\", None)\r\n\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_tbptt_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "if", "STRING", "not", "in", "checkpoint", ":", "return", "checkpoint", "if", "STRING", "not", "in", "checkpoint", "[", "STRING", "]", ":", "return", "checkpoint", "fit_loop", "=", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "old_key_new_key_mapping", "=", "{", "STRING", ":", "STRING", ",", "STRING", ":", "STRING", ",", "STRING", ":", "STRING", ",", "STRING", ":", "STRING", ",", "}", "for", "old", ",", "new", "in", "list", "(", "old_key_new_key_mapping", ".", "items", "(", ")", ")", ":", "if", "old", "in", "fit_loop", ":", "fit_loop", "[", "new", "]", "=", "fit_loop", "[", "old", "]", "del", "fit_loop", "[", "old", "]", "if", "STRING", "in", "fit_loop", "and", "fit_loop", "[", "STRING", "]", ":", "fit_loop", "[", "STRING", "]", "[", "STRING", "]", "=", "fit_loop", "[", "STRING", "]", "fit_loop", ".", "pop", "(", "STRING", ",", "None", ")", "return", "checkpoint"], "docstring": "Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The", "docstring_tokens": ["adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "truncated", "backpropagation", "was", "removed", "the"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 236, "end_line": 272, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1062", "original_string": "def _migrate_loop_structure_after_optimizer_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization\r\n    mode was removed. There is no longer a loop over optimizer, and hence no position to store for resuming the loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 6a56586\r\n    PR: #16539, #16598\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    if \"epoch_loop.optimizer_loop.optim_progress\" in fit_loop:\r\n        fit_loop[\"epoch_loop.optimizer_loop.optim_progress\"].pop(\"optimizer_position\", None)\r\n\r\n    if \"epoch_loop.optimizer_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.automatic_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.optimizer_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.automatic_optimization.optim_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.optimizer_loop.optim_progress\"\r\n        )\r\n    if \"epoch_loop.manual_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.manual_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.manual_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.manual_optimization.optim_step_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.manual_loop.optim_step_progress\"\r\n        )\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_optimizer_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization\r\n    mode was removed. There is no longer a loop over optimizer, and hence no position to store for resuming the loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 6a56586\r\n    PR: #16539, #16598\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    if \"epoch_loop.optimizer_loop.optim_progress\" in fit_loop:\r\n        fit_loop[\"epoch_loop.optimizer_loop.optim_progress\"].pop(\"optimizer_position\", None)\r\n\r\n    if \"epoch_loop.optimizer_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.automatic_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.optimizer_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.automatic_optimization.optim_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.optimizer_loop.optim_progress\"\r\n        )\r\n    if \"epoch_loop.manual_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.manual_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.manual_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.manual_optimization.optim_step_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.manual_loop.optim_step_progress\"\r\n        )\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_optimizer_loop_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "if", "STRING", "not", "in", "checkpoint", ":", "return", "checkpoint", "if", "STRING", "not", "in", "checkpoint", "[", "STRING", "]", ":", "return", "checkpoint", "fit_loop", "=", "checkpoint", "[", "STRING", "]", "[", "STRING", "]", "if", "STRING", "in", "fit_loop", ":", "fit_loop", "[", "STRING", "]", ".", "pop", "(", "STRING", ",", "None", ")", "if", "STRING", "in", "fit_loop", ":", "fit_loop", "[", "STRING", "]", "=", "fit_loop", ".", "pop", "(", "STRING", ")", "fit_loop", "[", "STRING", "]", "=", "fit_loop", ".", "pop", "(", "STRING", ")", "if", "STRING", "in", "fit_loop", ":", "fit_loop", "[", "STRING", "]", "=", "fit_loop", ".", "pop", "(", "STRING", ")", "fit_loop", "[", "STRING", "]", "=", "fit_loop", ".", "pop", "(", "STRING", ")", "return", "checkpoint"], "docstring": "Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization", "docstring_tokens": ["adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "multiple", "optimizers", "in", "automatic", "optimization"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 275, "end_line": 305, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "function_1063", "original_string": "def _migrate_loop_structure_after_dataloader_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the\r\n    ``_EvaluationEpochLoop`` (now ``_EvaluationLoop``) and ``_PredictionEpochLoop`` (now ``_PredictionLoop``).\r\n\r\n    Version: 2.0.0\r\n    Commit: ec4f592ecfe238edd83185f6c6905fb1e2406d61\r\n    PR: #16726\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    loops = checkpoint[\"loops\"]\r\n    for loop_key in (\"predict_loop\", \"validate_loop\", \"test_loop\"):\r\n        if loop_key not in loops:\r\n            continue\r\n        loop = loops[loop_key]\r\n        loop.pop(\"dataloader_progress\", None)  # no longer used\r\n        epoch_loop_key = \"epoch_loop.\"\r\n        epoch_loop_dict = {k[len(epoch_loop_key) :]: loop.pop(k) for k in list(loop) if k.startswith(epoch_loop_key)}\r\n        loop.update(epoch_loop_dict)\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_dataloader_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the\r\n    ``_EvaluationEpochLoop`` (now ``_EvaluationLoop``) and ``_PredictionEpochLoop`` (now ``_PredictionLoop``).\r\n\r\n    Version: 2.0.0\r\n    Commit: ec4f592ecfe238edd83185f6c6905fb1e2406d61\r\n    PR: #16726\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    loops = checkpoint[\"loops\"]\r\n    for loop_key in (\"predict_loop\", \"validate_loop\", \"test_loop\"):\r\n        if loop_key not in loops:\r\n            continue\r\n        loop = loops[loop_key]\r\n        loop.pop(\"dataloader_progress\", None)  # no longer used\r\n        epoch_loop_key = \"epoch_loop.\"\r\n        epoch_loop_dict = {k[len(epoch_loop_key) :]: loop.pop(k) for k in list(loop) if k.startswith(epoch_loop_key)}\r\n        loop.update(epoch_loop_dict)\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_dataloader_loop_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "if", "STRING", "not", "in", "checkpoint", ":", "return", "checkpoint", "loops", "=", "checkpoint", "[", "STRING", "]", "for", "loop_key", "in", "(", "STRING", ",", "STRING", ",", "STRING", ")", ":", "if", "loop_key", "not", "in", "loops", ":", "continue", "loop", "=", "loops", "[", "loop_key", "]", "loop", ".", "pop", "(", "STRING", ",", "None", ")", "#", "no", "longer", "used", "epoch_loop_key", "=", "STRING", "epoch_loop_dict", "=", "{", "k", "[", "len", "(", "epoch_loop_key", ")", ":", "]", ":", "loop", ".", "pop", "(", "k", ")", "for", "k", "in", "list", "(", "loop", ")", "if", "k", ".", "startswith", "(", "epoch_loop_key", ")", "}", "loop", ".", "update", "(", "epoch_loop_dict", ")", "return", "checkpoint"], "docstring": "The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the", "docstring_tokens": ["the", "dataloader", "loops", "_dataloaderloop", "_predictionloop", "and", "_evaluationloop", "were", "flattened", "into", "the"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "start_line": 308, "end_line": 328, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1064", "original_string": "def migrate_checkpoint(\r\n    checkpoint: _CHECKPOINT, target_version: Optional[str] = None\r\n) -> tuple[_CHECKPOINT, dict[str, list[str]]]:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary.\r\n\r\n    Args:\r\n        checkpoint: A dictionary with the loaded state from the checkpoint file.\r\n        target_version: Run migrations only up to this version (inclusive), even if migration index contains\r\n            migration functions for newer versions than this target. Mainly useful for testing.\r\n\r\n    Note:\r\n        The migration happens in-place. We specifically avoid copying the dict to avoid memory spikes for large\r\n        checkpoints and objects that do not support being deep-copied.\r\n\r\n    \"\"\"\r\n    ckpt_version = _get_version(checkpoint)\r\n    if Version(ckpt_version) > Version(pl.__version__):\r\n        rank_zero_warn(\r\n            f\"The loaded checkpoint was produced with Lightning v{ckpt_version}, which is newer than your current\"\r\n            f\" Lightning version: v{pl.__version__}\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint, {}\r\n\r\n    index = _migration_index()\r\n    applied_migrations = {}\r\n    for migration_version, migration_functions in index.items():\r\n        if not _should_upgrade(checkpoint, migration_version, target_version):\r\n            continue\r\n        for migration_function in migration_functions:\r\n            checkpoint = migration_function(checkpoint)\r\n\r\n        applied_migrations[migration_version] = [fn.__name__ for fn in migration_functions]\r\n\r\n    if ckpt_version != pl.__version__:\r\n        _set_legacy_version(checkpoint, ckpt_version)\r\n    _set_version(checkpoint, pl.__version__)\r\n    return checkpoint, applied_migrations", "language": "python", "code": "def migrate_checkpoint(\r\n    checkpoint: _CHECKPOINT, target_version: Optional[str] = None\r\n) -> tuple[_CHECKPOINT, dict[str, list[str]]]:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary.\r\n\r\n    Args:\r\n        checkpoint: A dictionary with the loaded state from the checkpoint file.\r\n        target_version: Run migrations only up to this version (inclusive), even if migration index contains\r\n            migration functions for newer versions than this target. Mainly useful for testing.\r\n\r\n    Note:\r\n        The migration happens in-place. We specifically avoid copying the dict to avoid memory spikes for large\r\n        checkpoints and objects that do not support being deep-copied.\r\n\r\n    \"\"\"\r\n    ckpt_version = _get_version(checkpoint)\r\n    if Version(ckpt_version) > Version(pl.__version__):\r\n        rank_zero_warn(\r\n            f\"The loaded checkpoint was produced with Lightning v{ckpt_version}, which is newer than your current\"\r\n            f\" Lightning version: v{pl.__version__}\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint, {}\r\n\r\n    index = _migration_index()\r\n    applied_migrations = {}\r\n    for migration_version, migration_functions in index.items():\r\n        if not _should_upgrade(checkpoint, migration_version, target_version):\r\n            continue\r\n        for migration_function in migration_functions:\r\n            checkpoint = migration_function(checkpoint)\r\n\r\n        applied_migrations[migration_version] = [fn.__name__ for fn in migration_functions]\r\n\r\n    if ckpt_version != pl.__version__:\r\n        _set_legacy_version(checkpoint, ckpt_version)\r\n    _set_version(checkpoint, pl.__version__)\r\n    return checkpoint, applied_migrations", "code_tokens": ["def", "migrate_checkpoint", "(", "checkpoint", ":", "_CHECKPOINT", ",", "target_version", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "tuple", "[", "_CHECKPOINT", ",", "dict", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "STRING", "ckpt_version", "=", "_get_version", "(", "checkpoint", ")", "if", "Version", "(", "ckpt_version", ")", ">", "Version", "(", "pl", ".", "__version__", ")", ":", "rank_zero_warn", "(", "fSTRING", "fSTRING", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "checkpoint", ",", "{", "}", "index", "=", "_migration_index", "(", ")", "applied_migrations", "=", "{", "}", "for", "migration_version", ",", "migration_functions", "in", "index", ".", "items", "(", ")", ":", "if", "not", "_should_upgrade", "(", "checkpoint", ",", "migration_version", ",", "target_version", ")", ":", "continue", "for", "migration_function", "in", "migration_functions", ":", "checkpoint", "=", "migration_function", "(", "checkpoint", ")", "applied_migrations", "[", "migration_version", "]", "=", "[", "fn", ".", "__name__", "for", "fn", "in", "migration_functions", "]", "if", "ckpt_version", "!", "=", "pl", ".", "__version__", ":", "_set_legacy_version", "(", "checkpoint", ",", "ckpt_version", ")", "_set_version", "(", "checkpoint", ",", "pl", ".", "__version__", ")", "return", "checkpoint", ",", "applied_migrations"], "docstring": "Applies Lightning version migrations to a checkpoint dictionary.", "docstring_tokens": ["applies", "lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 38, "end_line": 75, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1065", "original_string": "def _pl_migrate_checkpoint(checkpoint: _CHECKPOINT, checkpoint_path: Optional[_PATH] = None) -> _CHECKPOINT:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.\r\n\r\n    This function is used by the Lightning Trainer when resuming from a checkpoint.\r\n\r\n    \"\"\"\r\n    old_version = _get_version(checkpoint)\r\n    checkpoint, migrations = migrate_checkpoint(checkpoint)\r\n    new_version = _get_version(checkpoint)\r\n    if not migrations or checkpoint_path is None:\r\n        return checkpoint\r\n\r\n    path_hint = os.path.relpath(checkpoint_path, os.getcwd()) if not _IS_WINDOWS else os.path.abspath(checkpoint_path)\r\n    _log.info(\r\n        f\"Lightning automatically upgraded your loaded checkpoint from v{old_version} to v{new_version}.\"\r\n        \" To apply the upgrade to your files permanently, run\"\r\n        f\" `python -m lightning.pytorch.utilities.upgrade_checkpoint {str(path_hint)}`\"\r\n    )\r\n    return checkpoint", "language": "python", "code": "def _pl_migrate_checkpoint(checkpoint: _CHECKPOINT, checkpoint_path: Optional[_PATH] = None) -> _CHECKPOINT:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.\r\n\r\n    This function is used by the Lightning Trainer when resuming from a checkpoint.\r\n\r\n    \"\"\"\r\n    old_version = _get_version(checkpoint)\r\n    checkpoint, migrations = migrate_checkpoint(checkpoint)\r\n    new_version = _get_version(checkpoint)\r\n    if not migrations or checkpoint_path is None:\r\n        return checkpoint\r\n\r\n    path_hint = os.path.relpath(checkpoint_path, os.getcwd()) if not _IS_WINDOWS else os.path.abspath(checkpoint_path)\r\n    _log.info(\r\n        f\"Lightning automatically upgraded your loaded checkpoint from v{old_version} to v{new_version}.\"\r\n        \" To apply the upgrade to your files permanently, run\"\r\n        f\" `python -m lightning.pytorch.utilities.upgrade_checkpoint {str(path_hint)}`\"\r\n    )\r\n    return checkpoint", "code_tokens": ["def", "_pl_migrate_checkpoint", "(", "checkpoint", ":", "_CHECKPOINT", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "_CHECKPOINT", ":", "STRING", "old_version", "=", "_get_version", "(", "checkpoint", ")", "checkpoint", ",", "migrations", "=", "migrate_checkpoint", "(", "checkpoint", ")", "new_version", "=", "_get_version", "(", "checkpoint", ")", "if", "not", "migrations", "or", "checkpoint_path", "is", "None", ":", "return", "checkpoint", "path_hint", "=", "os", ".", "path", ".", "relpath", "(", "checkpoint_path", ",", "os", ".", "getcwd", "(", ")", ")", "if", "not", "_IS_WINDOWS", "else", "os", ".", "path", ".", "abspath", "(", "checkpoint_path", ")", "_log", ".", "info", "(", "fSTRING", "STRING", "fSTRING", ")", "return", "checkpoint"], "docstring": "Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.", "docstring_tokens": ["applies", "lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary", "and", "prints", "infos", "for", "the", "user"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 136, "end_line": 158, "has_examples": false, "num_comments": 2, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1066", "original_string": "def _get_version(checkpoint: _CHECKPOINT) -> str:\r\n    \"\"\"Get the version of a Lightning checkpoint.\"\"\"\r\n    return checkpoint[\"pytorch-lightning_version\"]", "language": "python", "code": "def _get_version(checkpoint: _CHECKPOINT) -> str:\r\n    \"\"\"Get the version of a Lightning checkpoint.\"\"\"\r\n    return checkpoint[\"pytorch-lightning_version\"]", "code_tokens": ["def", "_get_version", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "str", ":", "STRING", "return", "checkpoint", "[", "STRING", "]"], "docstring": "Get the version of a Lightning checkpoint.", "docstring_tokens": ["get", "the", "version", "of", "a", "lightning", "checkpoint"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 161, "end_line": 163, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1067", "original_string": "def _set_version(checkpoint: _CHECKPOINT, version: str) -> None:\r\n    \"\"\"Set the version of a Lightning checkpoint.\"\"\"\r\n    checkpoint[\"pytorch-lightning_version\"] = version", "language": "python", "code": "def _set_version(checkpoint: _CHECKPOINT, version: str) -> None:\r\n    \"\"\"Set the version of a Lightning checkpoint.\"\"\"\r\n    checkpoint[\"pytorch-lightning_version\"] = version", "code_tokens": ["def", "_set_version", "(", "checkpoint", ":", "_CHECKPOINT", ",", "version", ":", "str", ")", "-", ">", "None", ":", "STRING", "checkpoint", "[", "STRING", "]", "=", "version"], "docstring": "Set the version of a Lightning checkpoint.", "docstring_tokens": ["set", "the", "version", "of", "a", "lightning", "checkpoint"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 166, "end_line": 168, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1068", "original_string": "def _set_legacy_version(checkpoint: _CHECKPOINT, version: str) -> None:\r\n    \"\"\"Set the legacy version of a Lightning checkpoint if a legacy version is not already set.\"\"\"\r\n    checkpoint.setdefault(\"legacy_pytorch-lightning_version\", version)", "language": "python", "code": "def _set_legacy_version(checkpoint: _CHECKPOINT, version: str) -> None:\r\n    \"\"\"Set the legacy version of a Lightning checkpoint if a legacy version is not already set.\"\"\"\r\n    checkpoint.setdefault(\"legacy_pytorch-lightning_version\", version)", "code_tokens": ["def", "_set_legacy_version", "(", "checkpoint", ":", "_CHECKPOINT", ",", "version", ":", "str", ")", "-", ">", "None", ":", "STRING", "checkpoint", ".", "setdefault", "(", "STRING", ",", "version", ")"], "docstring": "Set the legacy version of a Lightning checkpoint if a legacy version is not already set.", "docstring_tokens": ["set", "the", "legacy", "version", "of", "a", "lightning", "checkpoint", "if", "a", "legacy", "version", "is", "not", "already", "set"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 171, "end_line": 173, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "function_1069", "original_string": "def _should_upgrade(checkpoint: _CHECKPOINT, target: str, max_version: Optional[str] = None) -> bool:\r\n    \"\"\"Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.\"\"\"\r\n    target_version = Version(target)\r\n    is_lte_max_version = max_version is None or target_version <= Version(max_version)\r\n    return is_lte_max_version and Version(_get_version(checkpoint)) < target_version", "language": "python", "code": "def _should_upgrade(checkpoint: _CHECKPOINT, target: str, max_version: Optional[str] = None) -> bool:\r\n    \"\"\"Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.\"\"\"\r\n    target_version = Version(target)\r\n    is_lte_max_version = max_version is None or target_version <= Version(max_version)\r\n    return is_lte_max_version and Version(_get_version(checkpoint)) < target_version", "code_tokens": ["def", "_should_upgrade", "(", "checkpoint", ":", "_CHECKPOINT", ",", "target", ":", "str", ",", "max_version", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "bool", ":", "STRING", "target_version", "=", "Version", "(", "target", ")", "is_lte_max_version", "=", "max_version", "is", "None", "or", "target_version", "<", "=", "Version", "(", "max_version", ")", "return", "is_lte_max_version", "and", "Version", "(", "_get_version", "(", "checkpoint", ")", ")", "<", "target_version"], "docstring": "Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.", "docstring_tokens": ["returns", "whether", "a", "checkpoint", "qualifies", "for", "an", "upgrade", "when", "the", "version", "is", "lower", "than", "the", "given", "target"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "start_line": 176, "end_line": 180, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1070", "original_string": "def _register_hook(self) -> Optional[RemovableHandle]:\r\n        \"\"\"Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the\r\n        hook is called, it will remove itself from the from the module, meaning that recursive models will only record\r\n        their input- and output shapes once. Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\r\n\r\n        Return:\r\n            A handle for the installed hook, or ``None`` if registering the hook is not possible.\r\n\r\n        \"\"\"\r\n\r\n        def hook(_: nn.Module, inp: Any, out: Any) -> None:\r\n            if len(inp) == 1:\r\n                inp = inp[0]\r\n\r\n            self._in_size = parse_batch_shape(inp)\r\n            self._out_size = parse_batch_shape(out)\r\n            assert self._hook_handle is not None\r\n            self._hook_handle.remove()\r\n\r\n        def hook_with_kwargs(_: nn.Module, args: Any, kwargs: Any, out: Any) -> None:\r\n\r\n            inp = (*args, *kwargs.values()) if kwargs is not None else args\r\n            hook(_, inp, out)\r\n\r\n        handle = None\r\n        if not isinstance(self._module, torch.jit.ScriptModule):\r\n            handle = self._module.register_forward_hook(hook_with_kwargs, with_kwargs=True)\r\n\r\n        return handle", "language": "python", "code": "def _register_hook(self) -> Optional[RemovableHandle]:\r\n        \"\"\"Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the\r\n        hook is called, it will remove itself from the from the module, meaning that recursive models will only record\r\n        their input- and output shapes once. Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\r\n\r\n        Return:\r\n            A handle for the installed hook, or ``None`` if registering the hook is not possible.\r\n\r\n        \"\"\"\r\n\r\n        def hook(_: nn.Module, inp: Any, out: Any) -> None:\r\n            if len(inp) == 1:\r\n                inp = inp[0]\r\n\r\n            self._in_size = parse_batch_shape(inp)\r\n            self._out_size = parse_batch_shape(out)\r\n            assert self._hook_handle is not None\r\n            self._hook_handle.remove()\r\n\r\n        def hook_with_kwargs(_: nn.Module, args: Any, kwargs: Any, out: Any) -> None:\r\n\r\n            inp = (*args, *kwargs.values()) if kwargs is not None else args\r\n            hook(_, inp, out)\r\n\r\n        handle = None\r\n        if not isinstance(self._module, torch.jit.ScriptModule):\r\n            handle = self._module.register_forward_hook(hook_with_kwargs, with_kwargs=True)\r\n\r\n        return handle", "code_tokens": ["def", "_register_hook", "(", "self", ")", "-", ">", "Optional", "[", "RemovableHandle", "]", ":", "STRING", "def", "hook", "(", "_", ":", "nn", ".", "Module", ",", "inp", ":", "Any", ",", "out", ":", "Any", ")", "-", ">", "None", ":", "if", "len", "(", "inp", ")", "=", "=", "1", ":", "inp", "=", "inp", "[", "0", "]", "self", ".", "_in_size", "=", "parse_batch_shape", "(", "inp", ")", "self", ".", "_out_size", "=", "parse_batch_shape", "(", "out", ")", "assert", "self", ".", "_hook_handle", "is", "not", "None", "self", ".", "_hook_handle", ".", "remove", "(", ")", "def", "hook_with_kwargs", "(", "_", ":", "nn", ".", "Module", ",", "args", ":", "Any", ",", "kwargs", ":", "Any", ",", "out", ":", "Any", ")", "-", ">", "None", ":", "inp", "=", "(", "*", "args", ",", "*", "kwargs", ".", "values", "(", ")", ")", "if", "kwargs", "is", "not", "None", "else", "args", "hook", "(", "_", ",", "inp", ",", "out", ")", "handle", "=", "None", "if", "not", "isinstance", "(", "self", ".", "_module", ",", "torch", ".", "jit", ".", "ScriptModule", ")", ":", "handle", "=", "self", ".", "_module", ".", "register_forward_hook", "(", "hook_with_kwargs", ",", "with_kwargs", "=", "True", ")", "return", "handle"], "docstring": "Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the", "docstring_tokens": ["registers", "a", "hook", "on", "the", "module", "that", "computes", "the", "input", "and", "output", "size", "s", "on", "the", "first", "forward", "pass", "if", "the"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 84, "end_line": 114, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1071", "original_string": "def detach_hook(self) -> None:\r\n        \"\"\"Removes the forward hook if it was not already removed in the forward pass.\r\n\r\n        Will be called after the summary is created.\r\n\r\n        \"\"\"\r\n        if self._hook_handle is not None:\r\n            self._hook_handle.remove()", "language": "python", "code": "def detach_hook(self) -> None:\r\n        \"\"\"Removes the forward hook if it was not already removed in the forward pass.\r\n\r\n        Will be called after the summary is created.\r\n\r\n        \"\"\"\r\n        if self._hook_handle is not None:\r\n            self._hook_handle.remove()", "code_tokens": ["def", "detach_hook", "(", "self", ")", "-", ">", "None", ":", "STRING", "if", "self", ".", "_hook_handle", "is", "not", "None", ":", "self", ".", "_hook_handle", ".", "remove", "(", ")"], "docstring": "Removes the forward hook if it was not already removed in the forward pass.", "docstring_tokens": ["removes", "the", "forward", "hook", "if", "it", "was", "not", "already", "removed", "in", "the", "forward", "pass"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 116, "end_line": 123, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1072", "original_string": "def layer_type(self) -> str:\r\n        \"\"\"Returns the class name of the module.\"\"\"\r\n        return str(self._module.__class__.__name__)", "language": "python", "code": "def layer_type(self) -> str:\r\n        \"\"\"Returns the class name of the module.\"\"\"\r\n        return str(self._module.__class__.__name__)", "code_tokens": ["def", "layer_type", "(", "self", ")", "-", ">", "str", ":", "STRING", "return", "str", "(", "self", ".", "_module", ".", "__class__", ".", "__name__", ")"], "docstring": "Returns the class name of the module.", "docstring_tokens": ["returns", "the", "class", "name", "of", "the", "module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 134, "end_line": 136, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1073", "original_string": "def num_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n        return sum(p.numel() if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "language": "python", "code": "def num_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n        return sum(p.numel() if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "code_tokens": ["def", "num_parameters", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "sum", "(", "p", ".", "numel", "(", ")", "if", "not", "_tensor_has_shape", "(", "p", ")", "else", "0", "for", "p", "in", "self", ".", "_module", ".", "parameters", "(", ")", ")"], "docstring": "Returns the number of parameters in this module.", "docstring_tokens": ["returns", "the", "number", "of", "parameters", "in", "this", "module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 139, "end_line": 141, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1074", "original_string": "def training(self) -> bool:\r\n        \"\"\"Returns whether the module is in training mode.\"\"\"\r\n        return self._module.training", "language": "python", "code": "def training(self) -> bool:\r\n        \"\"\"Returns whether the module is in training mode.\"\"\"\r\n        return self._module.training", "code_tokens": ["def", "training", "(", "self", ")", "-", ">", "bool", ":", "STRING", "return", "self", ".", "_module", ".", "training"], "docstring": "Returns whether the module is in training mode.", "docstring_tokens": ["returns", "whether", "the", "module", "is", "in", "training", "mode"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 144, "end_line": 146, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1075", "original_string": "def _forward_example_input(self) -> None:\r\n        \"\"\"Run the example input through each layer to get input- and output sizes.\"\"\"\r\n        model = self._model\r\n        trainer = self._model._trainer\r\n\r\n        input_ = model.example_input_array\r\n        input_ = model._on_before_batch_transfer(input_)\r\n        input_ = model._apply_batch_transfer_handler(input_)\r\n\r\n        mode = _ModuleMode()\r\n        mode.capture(model)\r\n        model.eval()\r\n\r\n        flop_context = (\r\n            contextlib.nullcontext()\r\n            if (\r\n                not _TORCH_GREATER_EQUAL_2_4\r\n                and any(isinstance(m, torch.jit.ScriptModule) for m in self._model.modules())\r\n            )\r\n            else self._flop_counter\r\n        )\r\n\r\n        forward_context = contextlib.nullcontext() if trainer is None else trainer.precision_plugin.forward_context()\r\n        with torch.no_grad(), forward_context, flop_context:\r\n            if isinstance(input_, (list, tuple)):\r\n                model(*input_)\r\n            elif isinstance(input_, dict):\r\n                model(**input_)\r\n            else:\r\n                model(input_)\r\n        mode.restore(model)", "language": "python", "code": "def _forward_example_input(self) -> None:\r\n        \"\"\"Run the example input through each layer to get input- and output sizes.\"\"\"\r\n        model = self._model\r\n        trainer = self._model._trainer\r\n\r\n        input_ = model.example_input_array\r\n        input_ = model._on_before_batch_transfer(input_)\r\n        input_ = model._apply_batch_transfer_handler(input_)\r\n\r\n        mode = _ModuleMode()\r\n        mode.capture(model)\r\n        model.eval()\r\n\r\n        flop_context = (\r\n            contextlib.nullcontext()\r\n            if (\r\n                not _TORCH_GREATER_EQUAL_2_4\r\n                and any(isinstance(m, torch.jit.ScriptModule) for m in self._model.modules())\r\n            )\r\n            else self._flop_counter\r\n        )\r\n\r\n        forward_context = contextlib.nullcontext() if trainer is None else trainer.precision_plugin.forward_context()\r\n        with torch.no_grad(), forward_context, flop_context:\r\n            if isinstance(input_, (list, tuple)):\r\n                model(*input_)\r\n            elif isinstance(input_, dict):\r\n                model(**input_)\r\n            else:\r\n                model(input_)\r\n        mode.restore(model)", "code_tokens": ["def", "_forward_example_input", "(", "self", ")", "-", ">", "None", ":", "STRING", "model", "=", "self", ".", "_model", "trainer", "=", "self", ".", "_model", ".", "_trainer", "input_", "=", "model", ".", "example_input_array", "input_", "=", "model", ".", "_on_before_batch_transfer", "(", "input_", ")", "input_", "=", "model", ".", "_apply_batch_transfer_handler", "(", "input_", ")", "mode", "=", "_ModuleMode", "(", ")", "mode", ".", "capture", "(", "model", ")", "model", ".", "eval", "(", ")", "flop_context", "=", "(", "contextlib", ".", "nullcontext", "(", ")", "if", "(", "not", "_TORCH_GREATER_EQUAL_2_4", "and", "any", "(", "isinstance", "(", "m", ",", "torch", ".", "jit", ".", "ScriptModule", ")", "for", "m", "in", "self", ".", "_model", ".", "modules", "(", ")", ")", ")", "else", "self", ".", "_flop_counter", ")", "forward_context", "=", "contextlib", ".", "nullcontext", "(", ")", "if", "trainer", "is", "None", "else", "trainer", ".", "precision_plugin", ".", "forward_context", "(", ")", "with", "torch", ".", "no_grad", "(", ")", ",", "forward_context", ",", "flop_context", ":", "if", "isinstance", "(", "input_", ",", "(", "list", ",", "tuple", ")", ")", ":", "model", "(", "*", "input_", ")", "elif", "isinstance", "(", "input_", ",", "dict", ")", ":", "model", "(", "*", "*", "input_", ")", "else", ":", "model", "(", "input_", ")", "mode", ".", "restore", "(", "model", ")"], "docstring": "Run the example input through each layer to get input- and output sizes.", "docstring_tokens": ["run", "the", "example", "input", "through", "each", "layer", "to", "get", "input", "and", "output", "sizes"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 338, "end_line": 371, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1076", "original_string": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "language": "python", "code": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "code_tokens": ["def", "_get_summary_data", "(", "self", ")", "-", ">", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "STRING", "arrays", "=", "[", "(", "STRING", ",", "list", "(", "map", "(", "str", ",", "range", "(", "len", "(", "self", ".", "_layer_summary", ")", ")", ")", ")", ")", ",", "(", "STRING", ",", "self", ".", "layer_names", ")", ",", "(", "STRING", ",", "self", ".", "layer_types", ")", ",", "(", "STRING", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "param_nums", ")", ")", ")", ",", "(", "STRING", ",", "[", "STRING", "if", "mode", "else", "STRING", "for", "mode", "in", "self", ".", "training_modes", "]", ")", ",", "(", "STRING", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "(", "sum", "(", "x", ".", "values", "(", ")", ")", "for", "x", "in", "self", ".", "flop_counts", ".", "values", "(", ")", ")", ")", ")", ")", ",", "]", "if", "self", ".", "_model", ".", "example_input_array", "is", "not", "None", ":", "arrays", ".", "append", "(", "(", "STRING", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "in_sizes", "]", ")", ")", "arrays", ".", "append", "(", "(", "STRING", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "out_sizes", "]", ")", ")", "total_leftover_params", "=", "self", ".", "total_parameters", "-", "self", ".", "total_layer_params", "if", "total_leftover_params", ">", "0", ":", "self", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "return", "arrays"], "docstring": "Makes a summary listing with:", "docstring_tokens": ["makes", "a", "summary", "listing", "with"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 373, "end_line": 395, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1077", "original_string": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\" \"].append(\" \")\r\n        layer_summaries[\"Name\"].append(LEFTOVER_PARAMS_NAME)\r\n        layer_summaries[\"Type\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"Params\"].append(get_human_readable_count(total_leftover_params))\r\n        layer_summaries[\"Mode\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"FLOPs\"].append(NOT_APPLICABLE)\r\n        if \"In sizes\" in layer_summaries:\r\n            layer_summaries[\"In sizes\"].append(NOT_APPLICABLE)\r\n        if \"Out sizes\" in layer_summaries:\r\n            layer_summaries[\"Out sizes\"].append(NOT_APPLICABLE)", "language": "python", "code": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\" \"].append(\" \")\r\n        layer_summaries[\"Name\"].append(LEFTOVER_PARAMS_NAME)\r\n        layer_summaries[\"Type\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"Params\"].append(get_human_readable_count(total_leftover_params))\r\n        layer_summaries[\"Mode\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"FLOPs\"].append(NOT_APPLICABLE)\r\n        if \"In sizes\" in layer_summaries:\r\n            layer_summaries[\"In sizes\"].append(NOT_APPLICABLE)\r\n        if \"Out sizes\" in layer_summaries:\r\n            layer_summaries[\"Out sizes\"].append(NOT_APPLICABLE)", "code_tokens": ["def", "_add_leftover_params_to_summary", "(", "self", ",", "arrays", ":", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ",", "total_leftover_params", ":", "int", ")", "-", ">", "None", ":", "STRING", "layer_summaries", "=", "dict", "(", "arrays", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "STRING", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "LEFTOVER_PARAMS_NAME", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "get_human_readable_count", "(", "total_leftover_params", ")", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "if", "STRING", "in", "layer_summaries", ":", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "if", "STRING", "in", "layer_summaries", ":", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")"], "docstring": "Add summary of params not associated with module or layer to model summary.", "docstring_tokens": ["add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 397, "end_line": 409, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1078", "original_string": "def _format_summary_table(\r\n    total_parameters: int,\r\n    trainable_parameters: int,\r\n    model_size: float,\r\n    total_training_modes: dict[str, int],\r\n    total_flops: int,\r\n    *cols: tuple[str, list[str]],\r\n) -> str:\r\n    \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\r\n    string defining the summary table that are nicely formatted.\"\"\"\r\n    n_rows = len(cols[0][1])\r\n    n_cols = 1 + len(cols)\r\n\r\n    col_widths = []\r\n    for c in cols:\r\n        col_width = max(len(str(a)) for a in c[1]) if n_rows else 0\r\n        col_width = max(col_width, len(c[0]))  # minimum length is header length\r\n        col_widths.append(col_width)\r\n\r\n    s = \"{:<{}}\"\r\n    total_width = sum(col_widths) + 3 * n_cols\r\n    header = [s.format(c[0], w) for c, w in zip(cols, col_widths)]\r\n\r\n    summary = \" | \".join(header) + \"\\n\" + \"-\" * total_width\r\n    for i in range(n_rows):\r\n        line = []\r\n        for c, w in zip(cols, col_widths):\r\n            line.append(s.format(str(c[1][i]), w))\r\n        summary += \"\\n\" + \" | \".join(line)\r\n    summary += \"\\n\" + \"-\" * total_width\r\n\r\n    summary += \"\\n\" + s.format(get_human_readable_count(trainable_parameters), 10)\r\n    summary += \"Trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters - trainable_parameters), 10)\r\n    summary += \"Non-trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters), 10)\r\n    summary += \"Total params\"\r\n    summary += \"\\n\" + s.format(get_formatted_model_size(model_size), 10)\r\n    summary += \"Total estimated model params size (MB)\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"train\"], 10)\r\n    summary += \"Modules in train mode\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"eval\"], 10)\r\n    summary += \"Modules in eval mode\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_flops), 10)\r\n    summary += \"Total Flops\"\r\n\r\n    return summary", "language": "python", "code": "def _format_summary_table(\r\n    total_parameters: int,\r\n    trainable_parameters: int,\r\n    model_size: float,\r\n    total_training_modes: dict[str, int],\r\n    total_flops: int,\r\n    *cols: tuple[str, list[str]],\r\n) -> str:\r\n    \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\r\n    string defining the summary table that are nicely formatted.\"\"\"\r\n    n_rows = len(cols[0][1])\r\n    n_cols = 1 + len(cols)\r\n\r\n    col_widths = []\r\n    for c in cols:\r\n        col_width = max(len(str(a)) for a in c[1]) if n_rows else 0\r\n        col_width = max(col_width, len(c[0]))  # minimum length is header length\r\n        col_widths.append(col_width)\r\n\r\n    s = \"{:<{}}\"\r\n    total_width = sum(col_widths) + 3 * n_cols\r\n    header = [s.format(c[0], w) for c, w in zip(cols, col_widths)]\r\n\r\n    summary = \" | \".join(header) + \"\\n\" + \"-\" * total_width\r\n    for i in range(n_rows):\r\n        line = []\r\n        for c, w in zip(cols, col_widths):\r\n            line.append(s.format(str(c[1][i]), w))\r\n        summary += \"\\n\" + \" | \".join(line)\r\n    summary += \"\\n\" + \"-\" * total_width\r\n\r\n    summary += \"\\n\" + s.format(get_human_readable_count(trainable_parameters), 10)\r\n    summary += \"Trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters - trainable_parameters), 10)\r\n    summary += \"Non-trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters), 10)\r\n    summary += \"Total params\"\r\n    summary += \"\\n\" + s.format(get_formatted_model_size(model_size), 10)\r\n    summary += \"Total estimated model params size (MB)\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"train\"], 10)\r\n    summary += \"Modules in train mode\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"eval\"], 10)\r\n    summary += \"Modules in eval mode\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_flops), 10)\r\n    summary += \"Total Flops\"\r\n\r\n    return summary", "code_tokens": ["def", "_format_summary_table", "(", "total_parameters", ":", "int", ",", "trainable_parameters", ":", "int", ",", "model_size", ":", "float", ",", "total_training_modes", ":", "dict", "[", "str", ",", "int", "]", ",", "total_flops", ":", "int", ",", "*", "cols", ":", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", ",", ")", "-", ">", "str", ":", "STRING", "n_rows", "=", "len", "(", "cols", "[", "0", "]", "[", "1", "]", ")", "n_cols", "=", "1", "+", "len", "(", "cols", ")", "col_widths", "=", "[", "]", "for", "c", "in", "cols", ":", "col_width", "=", "max", "(", "len", "(", "str", "(", "a", ")", ")", "for", "a", "in", "c", "[", "1", "]", ")", "if", "n_rows", "else", "0", "col_width", "=", "max", "(", "col_width", ",", "len", "(", "c", "[", "0", "]", ")", ")", "#", "minimum", "length", "is", "header", "length", "col_widths", ".", "append", "(", "col_width", ")", "s", "=", "STRING", "total_width", "=", "sum", "(", "col_widths", ")", "+", "3", "*", "n_cols", "header", "=", "[", "s", ".", "format", "(", "c", "[", "0", "]", ",", "w", ")", "for", "c", ",", "w", "in", "zip", "(", "cols", ",", "col_widths", ")", "]", "summary", "=", "STRING", ".", "join", "(", "header", ")", "+", "STRING", "+", "STRING", "*", "total_width", "for", "i", "in", "range", "(", "n_rows", ")", ":", "line", "=", "[", "]", "for", "c", ",", "w", "in", "zip", "(", "cols", ",", "col_widths", ")", ":", "line", ".", "append", "(", "s", ".", "format", "(", "str", "(", "c", "[", "1", "]", "[", "i", "]", ")", ",", "w", ")", ")", "summary", "+", "=", "STRING", "+", "STRING", ".", "join", "(", "line", ")", "summary", "+", "=", "STRING", "+", "STRING", "*", "total_width", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "trainable_parameters", ")", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_parameters", "-", "trainable_parameters", ")", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_parameters", ")", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "get_formatted_model_size", "(", "model_size", ")", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "total_training_modes", "[", "STRING", "]", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "total_training_modes", "[", "STRING", "]", ",", "10", ")", "summary", "+", "=", "STRING", "summary", "+", "=", "STRING", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_flops", ")", ",", "10", ")", "summary", "+", "=", "STRING", "return", "summary"], "docstring": "Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big", "docstring_tokens": ["takes", "in", "a", "number", "of", "arrays", "each", "specifying", "a", "column", "in", "the", "summary", "table", "and", "combines", "them", "all", "into", "one", "big"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 443, "end_line": 492, "has_examples": false, "num_comments": 3, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1079", "original_string": "def get_human_readable_count(number: int) -> str:\r\n    \"\"\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.\r\n\r\n    Examples:\r\n        >>> get_human_readable_count(123)\r\n        '123  '\r\n        >>> get_human_readable_count(1234)  # (one thousand)\r\n        '1.2 K'\r\n        >>> get_human_readable_count(2e6)   # (two million)\r\n        '2.0 M'\r\n        >>> get_human_readable_count(3e9)   # (three billion)\r\n        '3.0 B'\r\n        >>> get_human_readable_count(4e14)  # (four hundred trillion)\r\n        '400 T'\r\n        >>> get_human_readable_count(5e15)  # (more than trillion)\r\n        '5,000 T'\r\n\r\n    Args:\r\n        number: a positive integer number\r\n\r\n    Return:\r\n        A string formatted according to the pattern described above.\r\n\r\n    \"\"\"\r\n    assert number >= 0\r\n    labels = PARAMETER_NUM_UNITS\r\n    num_digits = int(math.floor(math.log10(number)) + 1 if number > 0 else 1)\r\n    num_groups = int(math.ceil(num_digits / 3))\r\n    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions\r\n    shift = -3 * (num_groups - 1)\r\n    number = number * (10**shift)\r\n    index = num_groups - 1\r\n    if index < 1 or number >= 100:\r\n        return f\"{int(number):,d} {labels[index]}\"\r\n\r\n    return f\"{number:,.1f} {labels[index]}\"", "language": "python", "code": "def get_human_readable_count(number: int) -> str:\r\n    \"\"\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.\r\n\r\n    Examples:\r\n        >>> get_human_readable_count(123)\r\n        '123  '\r\n        >>> get_human_readable_count(1234)  # (one thousand)\r\n        '1.2 K'\r\n        >>> get_human_readable_count(2e6)   # (two million)\r\n        '2.0 M'\r\n        >>> get_human_readable_count(3e9)   # (three billion)\r\n        '3.0 B'\r\n        >>> get_human_readable_count(4e14)  # (four hundred trillion)\r\n        '400 T'\r\n        >>> get_human_readable_count(5e15)  # (more than trillion)\r\n        '5,000 T'\r\n\r\n    Args:\r\n        number: a positive integer number\r\n\r\n    Return:\r\n        A string formatted according to the pattern described above.\r\n\r\n    \"\"\"\r\n    assert number >= 0\r\n    labels = PARAMETER_NUM_UNITS\r\n    num_digits = int(math.floor(math.log10(number)) + 1 if number > 0 else 1)\r\n    num_groups = int(math.ceil(num_digits / 3))\r\n    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions\r\n    shift = -3 * (num_groups - 1)\r\n    number = number * (10**shift)\r\n    index = num_groups - 1\r\n    if index < 1 or number >= 100:\r\n        return f\"{int(number):,d} {labels[index]}\"\r\n\r\n    return f\"{number:,.1f} {labels[index]}\"", "code_tokens": ["def", "get_human_readable_count", "(", "number", ":", "int", ")", "-", ">", "str", ":", "STRING", "assert", "number", ">", "=", "0", "labels", "=", "PARAMETER_NUM_UNITS", "num_digits", "=", "int", "(", "math", ".", "floor", "(", "math", ".", "log10", "(", "number", ")", ")", "+", "1", "if", "number", ">", "0", "else", "1", ")", "num_groups", "=", "int", "(", "math", ".", "ceil", "(", "num_digits", "/", "3", ")", ")", "num_groups", "=", "min", "(", "num_groups", ",", "len", "(", "labels", ")", ")", "#", "don", "'", "t", "abbreviate", "beyond", "trillions", "shift", "=", "-", "3", "*", "(", "num_groups", "-", "1", ")", "number", "=", "number", "*", "(", "10", "*", "*", "shift", ")", "index", "=", "num_groups", "-", "1", "if", "index", "<", "1", "or", "number", ">", "=", "100", ":", "return", "fSTRING", "return", "fSTRING"], "docstring": "Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.", "docstring_tokens": ["abbreviates", "an", "integer", "number", "with", "k", "m", "b", "t", "for", "thousands", "millions", "billions", "and", "trillions", "respectively"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 499, "end_line": 534, "has_examples": true, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "function_1080", "original_string": "def summarize(lightning_module: \"pl.LightningModule\", max_depth: int = 1) -> ModelSummary:\r\n    \"\"\"Summarize the LightningModule specified by `lightning_module`.\r\n\r\n    Args:\r\n        lightning_module: `LightningModule` to summarize.\r\n\r\n        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the\r\n            layer summary off. Default: 1.\r\n\r\n    Return:\r\n        The model summary object\r\n\r\n    \"\"\"\r\n    return ModelSummary(lightning_module, max_depth=max_depth)", "language": "python", "code": "def summarize(lightning_module: \"pl.LightningModule\", max_depth: int = 1) -> ModelSummary:\r\n    \"\"\"Summarize the LightningModule specified by `lightning_module`.\r\n\r\n    Args:\r\n        lightning_module: `LightningModule` to summarize.\r\n\r\n        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the\r\n            layer summary off. Default: 1.\r\n\r\n    Return:\r\n        The model summary object\r\n\r\n    \"\"\"\r\n    return ModelSummary(lightning_module, max_depth=max_depth)", "code_tokens": ["def", "summarize", "(", "lightning_module", ":", "STRING", ",", "max_depth", ":", "int", "=", "1", ")", "-", ">", "ModelSummary", ":", "STRING", "return", "ModelSummary", "(", "lightning_module", ",", "max_depth", "=", "max_depth", ")"], "docstring": "Summarize the LightningModule specified by `lightning_module`.", "docstring_tokens": ["summarize", "the", "lightningmodule", "specified", "by", "lightning_module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "start_line": 551, "end_line": 564, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "function_1081", "original_string": "def num_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n        return sum(deepspeed_param_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "language": "python", "code": "def num_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n        return sum(deepspeed_param_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "code_tokens": ["def", "num_parameters", "(", "self", ")", "-", ">", "int", ":", "STRING", "return", "sum", "(", "deepspeed_param_size", "(", "p", ")", "if", "not", "_tensor_has_shape", "(", "p", ")", "else", "0", "for", "p", "in", "self", ".", "_module", ".", "parameters", "(", ")", ")"], "docstring": "Returns the number of parameters in this module.", "docstring_tokens": ["returns", "the", "number", "of", "parameters", "in", "this", "module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "start_line": 39, "end_line": 41, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "function_1082", "original_string": "def average_shard_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n\r\n        def partitioned_size(p: Parameter) -> int:\r\n            return p.partitioned_size() if RequirementCache(\"deepspeed<0.6.6\") else p.partition_numel()\r\n\r\n        return sum(partitioned_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "language": "python", "code": "def average_shard_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n\r\n        def partitioned_size(p: Parameter) -> int:\r\n            return p.partitioned_size() if RequirementCache(\"deepspeed<0.6.6\") else p.partition_numel()\r\n\r\n        return sum(partitioned_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "code_tokens": ["def", "average_shard_parameters", "(", "self", ")", "-", ">", "int", ":", "STRING", "def", "partitioned_size", "(", "p", ":", "Parameter", ")", "-", ">", "int", ":", "return", "p", ".", "partitioned_size", "(", ")", "if", "RequirementCache", "(", "STRING", ")", "else", "p", ".", "partition_numel", "(", ")", "return", "sum", "(", "partitioned_size", "(", "p", ")", "if", "not", "_tensor_has_shape", "(", "p", ")", "else", "0", "for", "p", "in", "self", ".", "_module", ".", "parameters", "(", ")", ")"], "docstring": "Returns the number of parameters in this module.", "docstring_tokens": ["returns", "the", "number", "of", "parameters", "in", "this", "module"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "start_line": 44, "end_line": 50, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "function_1083", "original_string": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Params per Device\", list(map(get_human_readable_count, self.parameters_per_layer))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "language": "python", "code": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Params per Device\", list(map(get_human_readable_count, self.parameters_per_layer))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "code_tokens": ["def", "_get_summary_data", "(", "self", ")", "-", ">", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "STRING", "arrays", "=", "[", "(", "STRING", ",", "list", "(", "map", "(", "str", ",", "range", "(", "len", "(", "self", ".", "_layer_summary", ")", ")", ")", ")", ")", ",", "(", "STRING", ",", "self", ".", "layer_names", ")", ",", "(", "STRING", ",", "self", ".", "layer_types", ")", ",", "(", "STRING", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "param_nums", ")", ")", ")", ",", "(", "STRING", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "parameters_per_layer", ")", ")", ")", ",", "(", "STRING", ",", "[", "STRING", "if", "mode", "else", "STRING", "for", "mode", "in", "self", ".", "training_modes", "]", ")", ",", "(", "STRING", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "(", "sum", "(", "x", ".", "values", "(", ")", ")", "for", "x", "in", "self", ".", "flop_counts", ".", "values", "(", ")", ")", ")", ")", ")", ",", "]", "if", "self", ".", "_model", ".", "example_input_array", "is", "not", "None", ":", "arrays", ".", "append", "(", "(", "STRING", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "in_sizes", "]", ")", ")", "arrays", ".", "append", "(", "(", "STRING", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "out_sizes", "]", ")", ")", "total_leftover_params", "=", "self", ".", "total_parameters", "-", "self", ".", "total_layer_params", "if", "total_leftover_params", ">", "0", ":", "self", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "return", "arrays"], "docstring": "Makes a summary listing with:", "docstring_tokens": ["makes", "a", "summary", "listing", "with"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "start_line": 88, "end_line": 111, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "function_1084", "original_string": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        super()._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\"Params per Device\"].append(NOT_APPLICABLE)", "language": "python", "code": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        super()._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\"Params per Device\"].append(NOT_APPLICABLE)", "code_tokens": ["def", "_add_leftover_params_to_summary", "(", "self", ",", "arrays", ":", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ",", "total_leftover_params", ":", "int", ")", "-", ">", "None", ":", "STRING", "super", "(", ")", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "layer_summaries", "=", "dict", "(", "arrays", ")", "layer_summaries", "[", "STRING", "]", ".", "append", "(", "NOT_APPLICABLE", ")"], "docstring": "Add summary of params not associated with module or layer to model summary.", "docstring_tokens": ["add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary"], "partition": "test", "url": "https://github.com/Lightning-AI/pytorch-lightning", "metadata": {"file_path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "start_line": 114, "end_line": 118, "has_examples": false, "num_comments": 0, "description": "Short description to clean code"}}
