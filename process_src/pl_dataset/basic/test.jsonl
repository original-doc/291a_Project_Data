{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "func_name": "__get_max_ckpt_path_from_folder", "original_string": "def __get_max_ckpt_path_from_folder(folder_path: _PATH) -> str:\r\n        \"\"\"Get path of maximum-epoch checkpoint in the folder.\"\"\"\r\n        max_suffix = _CheckpointConnector.__max_ckpt_version_in_folder(folder_path)\r\n        ckpt_number = max_suffix if max_suffix is not None else 0\r\n        return f\"{folder_path}/hpc_ckpt_{ckpt_number}.ckpt\"", "language": "python", "code": "def __get_max_ckpt_path_from_folder(folder_path: _PATH) -> str:\r\n        \"\"\"Get path of maximum-epoch checkpoint in the folder.\"\"\"\r\n        max_suffix = _CheckpointConnector.__max_ckpt_version_in_folder(folder_path)\r\n        ckpt_number = max_suffix if max_suffix is not None else 0\r\n        return f\"{folder_path}/hpc_ckpt_{ckpt_number}.ckpt\"", "code_tokens": ["def", "__get_max_ckpt_path_from_folder", "(", "folder_path", ":", "_PATH", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Get", "path", "of", "maximum", "-", "epoch", "checkpoint", "in", "the", "folder", ".", "\"", "\"", "\"", "max_suffix", "=", "_CheckpointConnector", ".", "__max_ckpt_version_in_folder", "(", "folder_path", ")", "ckpt_number", "=", "max_suffix", "if", "max_suffix", "is", "not", "None", "else", "0", "return", "f", "\"", "{", "folder_path", "}", "/", "hpc_ckpt_", "{", "ckpt_number", "}", ".", "ckpt", "\""], "docstring": "Get path of maximum-epoch checkpoint in the folder.", "docstring_tokens": ["get", "path", "of", "maximum", "epoch", "checkpoint", "in", "the", "folder"], "docstring_summary": "Get path of maximum-epoch checkpoint in the folder.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_CheckpointConnector", "start_line": 547, "end_line": 551, "hash": "7e8e5882ad3b5759dbb2c8362a0d05e7", "complexity": 2, "parameters": ["folder_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "_prepare_dataloader", "original_string": "def _prepare_dataloader(self, dataloader: object, shuffle: bool, mode: RunningStage) -> object:\r\n        \"\"\"This function handles the following functionalities:\r\n\r\n        - Injecting a `DistributedDataSamplerWrapper` into the `DataLoader` if on a distributed environment\r\n        - Wrapping the dataloader based on strategy-specific logic\r\n\r\n        \"\"\"\r\n        # don't do anything if it's not a dataloader\r\n        if not isinstance(dataloader, DataLoader):\r\n            return dataloader\r\n        if (\r\n            self._requires_distributed_sampler(dataloader)  # sets the distributed sampler\r\n            or mode == RunningStage.PREDICTING  # to track indices for the predictions\r\n        ):\r\n            sampler = self._resolve_sampler(dataloader, shuffle=shuffle, mode=mode)\r\n            return _update_dataloader(dataloader, sampler, mode=mode)\r\n\r\n        return dataloader", "language": "python", "code": "def _prepare_dataloader(self, dataloader: object, shuffle: bool, mode: RunningStage) -> object:\r\n        \"\"\"This function handles the following functionalities:\r\n\r\n        - Injecting a `DistributedDataSamplerWrapper` into the `DataLoader` if on a distributed environment\r\n        - Wrapping the dataloader based on strategy-specific logic\r\n\r\n        \"\"\"\r\n        # don't do anything if it's not a dataloader\r\n        if not isinstance(dataloader, DataLoader):\r\n            return dataloader\r\n        if (\r\n            self._requires_distributed_sampler(dataloader)  # sets the distributed sampler\r\n            or mode == RunningStage.PREDICTING  # to track indices for the predictions\r\n        ):\r\n            sampler = self._resolve_sampler(dataloader, shuffle=shuffle, mode=mode)\r\n            return _update_dataloader(dataloader, sampler, mode=mode)\r\n\r\n        return dataloader", "code_tokens": ["def", "_prepare_dataloader", "(", "self", ",", "dataloader", ":", "object", ",", "shuffle", ":", "bool", ",", "mode", ":", "RunningStage", ")", "-", ">", "object", ":", "\"", "\"", "\"", "This", "function", "handles", "the", "following", "functionalities", ":", "-", "Injecting", "a", "`", "DistributedDataSamplerWrapper", "`", "into", "the", "`", "DataLoader", "`", "if", "on", "a", "distributed", "environment", "-", "Wrapping", "the", "dataloader", "based", "on", "strategy", "-", "specific", "logic", "\"", "\"", "\"", "if", "not", "isinstance", "(", "dataloader", ",", "DataLoader", ")", ":", "return", "dataloader", "if", "(", "self", ".", "_requires_distributed_sampler", "(", "dataloader", ")", "or", "mode", "=", "=", "RunningStage", ".", "PREDICTING", ")", ":", "sampler", "=", "self", ".", "_resolve_sampler", "(", "dataloader", ",", "shuffle", "=", "shuffle", ",", "mode", "=", "mode", ")", "return", "_update_dataloader", "(", "dataloader", ",", "sampler", ",", "mode", "=", "mode", ")", "return", "dataloader"], "docstring": "This function handles the following functionalities:\r\n\r\n        - Injecting a `DistributedDataSamplerWrapper` into the `DataLoader` if on a distributed environment\r\n        - Wrapping the dataloader based on strategy-specific logic", "docstring_tokens": ["this", "function", "handles", "the", "following", "functionalities", "injecting", "a", "distributeddatasamplerwrapper", "into", "the", "dataloader", "if", "on", "a", "distributed", "environment", "wrapping", "the", "dataloader", "based", "on", "strategy", "specific", "logic"], "docstring_summary": "This function handles the following functionalities:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_DataConnector", "start_line": 176, "end_line": 193, "hash": "2d3ee29de1470bcfc42068a285f56513", "complexity": 4, "parameters": ["dataloader", "shuffle", "mode"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "_get_distributed_sampler", "original_string": "def _get_distributed_sampler(\r\n    dataloader: DataLoader,\r\n    shuffle: bool,\r\n    overfit_batches: Union[int, float],\r\n    mode: Optional[RunningStage] = None,\r\n    **kwargs: Any,\r\n) -> DistributedSampler:\r\n    \"\"\"This function is used to created the distributed sampler injected within the user DataLoader.\"\"\"\r\n    kwargs[\"shuffle\"] = shuffle and not overfit_batches\r\n    kwargs.setdefault(\"seed\", int(os.getenv(\"PL_GLOBAL_SEED\", 0)))\r\n    if mode == RunningStage.PREDICTING:\r\n        return UnrepeatedDistributedSamplerWrapper(dataloader.sampler, **kwargs)\r\n    if isinstance(dataloader.sampler, (RandomSampler, SequentialSampler)):\r\n        return DistributedSampler(dataloader.dataset, **kwargs)\r\n    return DistributedSamplerWrapper(dataloader.sampler, **kwargs)", "language": "python", "code": "def _get_distributed_sampler(\r\n    dataloader: DataLoader,\r\n    shuffle: bool,\r\n    overfit_batches: Union[int, float],\r\n    mode: Optional[RunningStage] = None,\r\n    **kwargs: Any,\r\n) -> DistributedSampler:\r\n    \"\"\"This function is used to created the distributed sampler injected within the user DataLoader.\"\"\"\r\n    kwargs[\"shuffle\"] = shuffle and not overfit_batches\r\n    kwargs.setdefault(\"seed\", int(os.getenv(\"PL_GLOBAL_SEED\", 0)))\r\n    if mode == RunningStage.PREDICTING:\r\n        return UnrepeatedDistributedSamplerWrapper(dataloader.sampler, **kwargs)\r\n    if isinstance(dataloader.sampler, (RandomSampler, SequentialSampler)):\r\n        return DistributedSampler(dataloader.dataset, **kwargs)\r\n    return DistributedSamplerWrapper(dataloader.sampler, **kwargs)", "code_tokens": ["def", "_get_distributed_sampler", "(", "dataloader", ":", "DataLoader", ",", "shuffle", ":", "bool", ",", "overfit_batches", ":", "Union", "[", "int", ",", "float", "]", ",", "mode", ":", "Optional", "[", "RunningStage", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "DistributedSampler", ":", "\"", "\"", "\"", "This", "function", "is", "used", "to", "created", "the", "distributed", "sampler", "injected", "within", "the", "user", "DataLoader", ".", "\"", "\"", "\"", "kwargs", "[", "\"", "shuffle", "\"", "]", "=", "shuffle", "and", "not", "overfit_batches", "kwargs", ".", "setdefault", "(", "\"", "seed", "\"", ",", "int", "(", "os", ".", "getenv", "(", "\"", "PL_GLOBAL_SEED", "\"", ",", "0", ")", ")", ")", "if", "mode", "=", "=", "RunningStage", ".", "PREDICTING", ":", "return", "UnrepeatedDistributedSamplerWrapper", "(", "dataloader", ".", "sampler", ",", "*", "*", "kwargs", ")", "if", "isinstance", "(", "dataloader", ".", "sampler", ",", "(", "RandomSampler", ",", "SequentialSampler", ")", ")", ":", "return", "DistributedSampler", "(", "dataloader", ".", "dataset", ",", "*", "*", "kwargs", ")", "return", "DistributedSamplerWrapper", "(", "dataloader", ".", "sampler", ",", "*", "*", "kwargs", ")"], "docstring": "This function is used to created the distributed sampler injected within the user DataLoader.", "docstring_tokens": ["this", "function", "is", "used", "to", "created", "the", "distributed", "sampler", "injected", "within", "the", "user", "dataloader"], "docstring_summary": "This function is used to created the distributed sampler injected within the user DataLoader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "function", "start_line": 229, "end_line": 243, "hash": "7eb73be8a8730121055deddb85e31912", "complexity": 4, "parameters": ["dataloader", "shuffle", "overfit_batches", "float]", "mode", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "_resolve_overfit_batches", "original_string": "def _resolve_overfit_batches(combined_loader: CombinedLoader, mode: RunningStage) -> None:\r\n    \"\"\"Resolve overfit batches by disabling shuffling.\r\n\r\n    When overfit_batches > 0, this function ensures that sequential sampling is used without shuffling for consistent\r\n    batches across epochs. Training and validation use different sets of data.\r\n\r\n    \"\"\"\r\n    all_have_sequential_sampler = all(\r\n        isinstance(dl.sampler, SequentialSampler) for dl in combined_loader.flattened if hasattr(dl, \"sampler\")\r\n    )\r\n    if all_have_sequential_sampler:\r\n        return\r\n\r\n    rank_zero_warn(\r\n        f\"You requested to overfit but enabled {mode.dataloader_prefix} dataloader shuffling.\"\r\n        f\" We are turning off the {mode.dataloader_prefix} dataloader shuffling for you.\"\r\n    )\r\n\r\n    updated = [\r\n        _update_dataloader(dl, sampler=SequentialSampler(dl.dataset), mode=mode) if hasattr(dl, \"dataset\") else dl\r\n        for dl in combined_loader.flattened\r\n    ]\r\n    combined_loader.flattened = updated", "language": "python", "code": "def _resolve_overfit_batches(combined_loader: CombinedLoader, mode: RunningStage) -> None:\r\n    \"\"\"Resolve overfit batches by disabling shuffling.\r\n\r\n    When overfit_batches > 0, this function ensures that sequential sampling is used without shuffling for consistent\r\n    batches across epochs. Training and validation use different sets of data.\r\n\r\n    \"\"\"\r\n    all_have_sequential_sampler = all(\r\n        isinstance(dl.sampler, SequentialSampler) for dl in combined_loader.flattened if hasattr(dl, \"sampler\")\r\n    )\r\n    if all_have_sequential_sampler:\r\n        return\r\n\r\n    rank_zero_warn(\r\n        f\"You requested to overfit but enabled {mode.dataloader_prefix} dataloader shuffling.\"\r\n        f\" We are turning off the {mode.dataloader_prefix} dataloader shuffling for you.\"\r\n    )\r\n\r\n    updated = [\r\n        _update_dataloader(dl, sampler=SequentialSampler(dl.dataset), mode=mode) if hasattr(dl, \"dataset\") else dl\r\n        for dl in combined_loader.flattened\r\n    ]\r\n    combined_loader.flattened = updated", "code_tokens": ["def", "_resolve_overfit_batches", "(", "combined_loader", ":", "CombinedLoader", ",", "mode", ":", "RunningStage", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resolve", "overfit", "batches", "by", "disabling", "shuffling", ".", "When", "overfit_batches", ">", "0", ",", "this", "function", "ensures", "that", "sequential", "sampling", "is", "used", "without", "shuffling", "for", "consistent", "batches", "across", "epochs", ".", "Training", "and", "validation", "use", "different", "sets", "of", "data", ".", "\"", "\"", "\"", "all_have_sequential_sampler", "=", "all", "(", "isinstance", "(", "dl", ".", "sampler", ",", "SequentialSampler", ")", "for", "dl", "in", "combined_loader", ".", "flattened", "if", "hasattr", "(", "dl", ",", "\"", "sampler", "\"", ")", ")", "if", "all_have_sequential_sampler", ":", "return", "rank_zero_warn", "(", "f", "\"", "You", "requested", "to", "overfit", "but", "enabled", "{", "mode", ".", "dataloader_prefix", "}", "dataloader", "shuffling", ".", "\"", "f", "\"", "We", "are", "turning", "off", "the", "{", "mode", ".", "dataloader_prefix", "}", "dataloader", "shuffling", "for", "you", ".", "\"", ")", "updated", "=", "[", "_update_dataloader", "(", "dl", ",", "sampler", "=", "SequentialSampler", "(", "dl", ".", "dataset", ")", ",", "mode", "=", "mode", ")", "if", "hasattr", "(", "dl", ",", "\"", "dataset", "\"", ")", "else", "dl", "for", "dl", "in", "combined_loader", ".", "flattened", "]", "combined_loader", ".", "flattened", "=", "updated"], "docstring": "Resolve overfit batches by disabling shuffling.\r\n\r\n    When overfit_batches > 0, this function ensures that sequential sampling is used without shuffling for consistent\r\n    batches across epochs. Training and validation use different sets of data.", "docstring_tokens": ["resolve", "overfit", "batches", "by", "disabling", "shuffling", "when", "overfit_batches", "0", "this", "function", "ensures", "that", "sequential", "sampling", "is", "used", "without", "shuffling", "for", "consistent", "batches", "across", "epochs", "training", "and", "validation", "use", "different", "sets", "of", "data"], "docstring_summary": "Resolve overfit batches by disabling shuffling.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "function", "start_line": 246, "end_line": 268, "hash": "39e79783b6e28853c4daa60e75725d40", "complexity": 6, "parameters": ["combined_loader", "mode"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "dataloader", "original_string": "def dataloader(self) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n        \"\"\"Returns the dataloader from the source.\r\n\r\n        If the source is a module, the method with the corresponding :attr:`name` gets called.\r\n\r\n        \"\"\"\r\n        if isinstance(self.instance, pl.LightningModule):\r\n            return call._call_lightning_module_hook(self.instance.trainer, self.name, pl_module=self.instance)\r\n        if isinstance(self.instance, pl.LightningDataModule):\r\n            assert self.instance.trainer is not None\r\n            return call._call_lightning_datamodule_hook(self.instance.trainer, self.name)\r\n        assert self.instance is not None\r\n        return self.instance", "language": "python", "code": "def dataloader(self) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n        \"\"\"Returns the dataloader from the source.\r\n\r\n        If the source is a module, the method with the corresponding :attr:`name` gets called.\r\n\r\n        \"\"\"\r\n        if isinstance(self.instance, pl.LightningModule):\r\n            return call._call_lightning_module_hook(self.instance.trainer, self.name, pl_module=self.instance)\r\n        if isinstance(self.instance, pl.LightningDataModule):\r\n            assert self.instance.trainer is not None\r\n            return call._call_lightning_datamodule_hook(self.instance.trainer, self.name)\r\n        assert self.instance is not None\r\n        return self.instance", "code_tokens": ["def", "dataloader", "(", "self", ")", "-", ">", "Union", "[", "TRAIN_DATALOADERS", ",", "EVAL_DATALOADERS", "]", ":", "\"", "\"", "\"", "Returns", "the", "dataloader", "from", "the", "source", ".", "If", "the", "source", "is", "a", "module", ",", "the", "method", "with", "the", "corresponding", ":", "attr", ":", "`", "name", "`", "gets", "called", ".", "\"", "\"", "\"", "if", "isinstance", "(", "self", ".", "instance", ",", "pl", ".", "LightningModule", ")", ":", "return", "call", ".", "_call_lightning_module_hook", "(", "self", ".", "instance", ".", "trainer", ",", "self", ".", "name", ",", "pl_module", "=", "self", ".", "instance", ")", "if", "isinstance", "(", "self", ".", "instance", ",", "pl", ".", "LightningDataModule", ")", ":", "assert", "self", ".", "instance", ".", "trainer", "is", "not", "None", "return", "call", ".", "_call_lightning_datamodule_hook", "(", "self", ".", "instance", ".", "trainer", ",", "self", ".", "name", ")", "assert", "self", ".", "instance", "is", "not", "None", "return", "self", ".", "instance"], "docstring": "Returns the dataloader from the source.\r\n\r\n        If the source is a module, the method with the corresponding :attr:`name` gets called.", "docstring_tokens": ["returns", "the", "dataloader", "from", "the", "source", "if", "the", "source", "is", "a", "module", "the", "method", "with", "the", "corresponding", "attr", "name", "gets", "called"], "docstring_summary": "Returns the dataloader from the source.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_DataLoaderSource", "start_line": 291, "end_line": 303, "hash": "f811c7a6cb0a0e6fa820a313e35bbda5", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "is_defined", "original_string": "def is_defined(self) -> bool:\r\n        \"\"\"Returns whether the source dataloader can be retrieved or not.\r\n\r\n        If the source is a module it checks that the method with given :attr:`name` is overridden.\r\n\r\n        \"\"\"\r\n        return not self.is_module() or is_overridden(self.name, self.instance)", "language": "python", "code": "def is_defined(self) -> bool:\r\n        \"\"\"Returns whether the source dataloader can be retrieved or not.\r\n\r\n        If the source is a module it checks that the method with given :attr:`name` is overridden.\r\n\r\n        \"\"\"\r\n        return not self.is_module() or is_overridden(self.name, self.instance)", "code_tokens": ["def", "is_defined", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "whether", "the", "source", "dataloader", "can", "be", "retrieved", "or", "not", ".", "If", "the", "source", "is", "a", "module", "it", "checks", "that", "the", "method", "with", "given", ":", "attr", ":", "`", "name", "`", "is", "overridden", ".", "\"", "\"", "\"", "return", "not", "self", ".", "is_module", "(", ")", "or", "is_overridden", "(", "self", ".", "name", ",", "self", ".", "instance", ")"], "docstring": "Returns whether the source dataloader can be retrieved or not.\r\n\r\n        If the source is a module it checks that the method with given :attr:`name` is overridden.", "docstring_tokens": ["returns", "whether", "the", "source", "dataloader", "can", "be", "retrieved", "or", "not", "if", "the", "source", "is", "a", "module", "it", "checks", "that", "the", "method", "with", "given", "attr", "name", "is", "overridden"], "docstring_summary": "Returns whether the source dataloader can be retrieved or not.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_DataLoaderSource", "start_line": 305, "end_line": 311, "hash": "6595a0d34ad793f87ff54c955fb3cd0f", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "is_module", "original_string": "def is_module(self) -> bool:\r\n        \"\"\"Returns whether the DataLoader source is a LightningModule or a LightningDataModule.\r\n\r\n        It does not check whether ``*_dataloader`` methods are actually overridden.\r\n\r\n        \"\"\"\r\n        return isinstance(self.instance, (pl.LightningModule, pl.LightningDataModule))", "language": "python", "code": "def is_module(self) -> bool:\r\n        \"\"\"Returns whether the DataLoader source is a LightningModule or a LightningDataModule.\r\n\r\n        It does not check whether ``*_dataloader`` methods are actually overridden.\r\n\r\n        \"\"\"\r\n        return isinstance(self.instance, (pl.LightningModule, pl.LightningDataModule))", "code_tokens": ["def", "is_module", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "whether", "the", "DataLoader", "source", "is", "a", "LightningModule", "or", "a", "LightningDataModule", ".", "It", "does", "not", "check", "whether", "`", "`", "*", "_dataloader", "`", "`", "methods", "are", "actually", "overridden", ".", "\"", "\"", "\"", "return", "isinstance", "(", "self", ".", "instance", ",", "(", "pl", ".", "LightningModule", ",", "pl", ".", "LightningDataModule", ")", ")"], "docstring": "Returns whether the DataLoader source is a LightningModule or a LightningDataModule.\r\n\r\n        It does not check whether ``*_dataloader`` methods are actually overridden.", "docstring_tokens": ["returns", "whether", "the", "dataloader", "source", "is", "a", "lightningmodule", "or", "a", "lightningdatamodule", "it", "does", "not", "check", "whether", "_dataloader", "methods", "are", "actually", "overridden"], "docstring_summary": "Returns whether the DataLoader source is a LightningModule or a LightningDataModule.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_DataLoaderSource", "start_line": 313, "end_line": 319, "hash": "c870baefc625d15bc6a513a6007eb04a", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "func_name": "_request_dataloader", "original_string": "def _request_dataloader(data_source: _DataLoaderSource) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n    \"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader\r\n\r\n    \"\"\"\r\n    with _replace_dunder_methods(DataLoader, \"dataset\"), _replace_dunder_methods(BatchSampler):\r\n        # under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\r\n        # attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\r\n        # Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\r\n        # methods so that the re-instantiated object is as close to the original as possible.\r\n        return data_source.dataloader()", "language": "python", "code": "def _request_dataloader(data_source: _DataLoaderSource) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n    \"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader\r\n\r\n    \"\"\"\r\n    with _replace_dunder_methods(DataLoader, \"dataset\"), _replace_dunder_methods(BatchSampler):\r\n        # under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\r\n        # attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\r\n        # Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\r\n        # methods so that the re-instantiated object is as close to the original as possible.\r\n        return data_source.dataloader()", "code_tokens": ["def", "_request_dataloader", "(", "data_source", ":", "_DataLoaderSource", ")", "-", ">", "Union", "[", "TRAIN_DATALOADERS", ",", "EVAL_DATALOADERS", "]", ":", "\"", "\"", "\"", "Requests", "a", "dataloader", "by", "calling", "dataloader", "hooks", "corresponding", "to", "the", "given", "stage", ".", "Returns", ":", "The", "requested", "dataloader", "\"", "\"", "\"", "with", "_replace_dunder_methods", "(", "DataLoader", ",", "\"", "dataset", "\"", ")", ",", "_replace_dunder_methods", "(", "BatchSampler", ")", ":", "return", "data_source", ".", "dataloader", "(", ")"], "docstring": "Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader", "docstring_tokens": ["requests", "a", "dataloader", "by", "calling", "dataloader", "hooks", "corresponding", "to", "the", "given", "stage", "returns", "the", "requested", "dataloader"], "docstring_summary": "Requests a dataloader by calling dataloader hooks corresponding to the given stage.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\data_connector.py", "partition": "test", "function_type": "function", "start_line": 322, "end_line": 334, "hash": "8396666579bc0111330149d735934f7d", "complexity": 2, "parameters": ["data_source"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "func_name": "teardown", "original_string": "def teardown(self) -> None:\r\n        \"\"\"Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.\"\"\"\r\n        for signum, handler in self._original_handlers.items():\r\n            if handler is not None:\r\n                self._register_signal(signum, handler)\r\n        self._original_handlers = {}", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.\"\"\"\r\n        for signum, handler in self._original_handlers.items():\r\n            if handler is not None:\r\n                self._register_signal(signum, handler)\r\n        self._original_handlers = {}", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "the", "signals", "that", "were", "previously", "configured", "before", ":", "class", ":", "`", "_SignalConnector", "`", "replaced", "them", ".", "\"", "\"", "\"", "for", "signum", ",", "handler", "in", "self", ".", "_original_handlers", ".", "items", "(", ")", ":", "if", "handler", "is", "not", "None", ":", "self", ".", "_register_signal", "(", "signum", ",", "handler", ")", "self", ".", "_original_handlers", "=", "{", "}"], "docstring": "Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.", "docstring_tokens": ["restores", "the", "signals", "that", "were", "previously", "configured", "before", "class", "_signalconnector", "replaced", "them"], "docstring_summary": "Restores the signals that were previously configured before :class:`_SignalConnector` replaced them.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_SignalConnector", "start_line": 124, "end_line": 129, "hash": "38b6d9925dc8a7a8323901cf66ca00ab", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "func_name": "_get_current_signal_handlers", "original_string": "def _get_current_signal_handlers() -> dict[_SIGNUM, _HANDLER]:\r\n        \"\"\"Collects the currently assigned signal handlers.\"\"\"\r\n        valid_signals = _SignalConnector._valid_signals()\r\n        if not _IS_WINDOWS:\r\n            # SIGKILL and SIGSTOP are not allowed to be modified by the user\r\n            valid_signals -= {signal.SIGKILL, signal.SIGSTOP}\r\n        return {signum: signal.getsignal(signum) for signum in valid_signals}", "language": "python", "code": "def _get_current_signal_handlers() -> dict[_SIGNUM, _HANDLER]:\r\n        \"\"\"Collects the currently assigned signal handlers.\"\"\"\r\n        valid_signals = _SignalConnector._valid_signals()\r\n        if not _IS_WINDOWS:\r\n            # SIGKILL and SIGSTOP are not allowed to be modified by the user\r\n            valid_signals -= {signal.SIGKILL, signal.SIGSTOP}\r\n        return {signum: signal.getsignal(signum) for signum in valid_signals}", "code_tokens": ["def", "_get_current_signal_handlers", "(", ")", "-", ">", "dict", "[", "_SIGNUM", ",", "_HANDLER", "]", ":", "\"", "\"", "\"", "Collects", "the", "currently", "assigned", "signal", "handlers", ".", "\"", "\"", "\"", "valid_signals", "=", "_SignalConnector", ".", "_valid_signals", "(", ")", "if", "not", "_IS_WINDOWS", ":", "valid_signals", "-", "=", "{", "signal", ".", "SIGKILL", ",", "signal", ".", "SIGSTOP", "}", "return", "{", "signum", ":", "signal", ".", "getsignal", "(", "signum", ")", "for", "signum", "in", "valid_signals", "}"], "docstring": "Collects the currently assigned signal handlers.", "docstring_tokens": ["collects", "the", "currently", "assigned", "signal", "handlers"], "docstring_summary": "Collects the currently assigned signal handlers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\signal_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_SignalConnector", "start_line": 132, "end_line": 138, "hash": "6b5edacbaec773d4e0557ae5b6e608e5", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "check_logging", "original_string": "def check_logging(cls, fx_name: str) -> None:\r\n        \"\"\"Check if the given hook is allowed to log.\"\"\"\r\n        if fx_name not in cls.functions:\r\n            raise RuntimeError(\r\n                f\"Logging inside `{fx_name}` is not implemented.\"\r\n                \" Please, open an issue in `https://github.com/Lightning-AI/pytorch-lightning/issues`.\"\r\n            )\r\n\r\n        if cls.functions[fx_name] is None:\r\n            raise MisconfigurationException(\r\n                f\"You can't `self.log()` inside `{fx_name}`. HINT: You can still log directly to the logger by using\"\r\n                \" `self.logger.experiment`.\"\r\n            )", "language": "python", "code": "def check_logging(cls, fx_name: str) -> None:\r\n        \"\"\"Check if the given hook is allowed to log.\"\"\"\r\n        if fx_name not in cls.functions:\r\n            raise RuntimeError(\r\n                f\"Logging inside `{fx_name}` is not implemented.\"\r\n                \" Please, open an issue in `https://github.com/Lightning-AI/pytorch-lightning/issues`.\"\r\n            )\r\n\r\n        if cls.functions[fx_name] is None:\r\n            raise MisconfigurationException(\r\n                f\"You can't `self.log()` inside `{fx_name}`. HINT: You can still log directly to the logger by using\"\r\n                \" `self.logger.experiment`.\"\r\n            )", "code_tokens": ["def", "check_logging", "(", "cls", ",", "fx_name", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Check", "if", "the", "given", "hook", "is", "allowed", "to", "log", ".", "\"", "\"", "\"", "if", "fx_name", "not", "in", "cls", ".", "functions", ":", "raise", "RuntimeError", "(", "f", "\"", "Logging", "inside", "`", "{", "fx_name", "}", "`", "is", "not", "implemented", ".", "\"", "\"", "Please", ",", "open", "an", "issue", "in", "`", "https", ":", "/", "/", "github", ".", "com", "/", "Lightning", "-", "AI", "/", "pytorch", "-", "lightning", "/", "issues", "`", ".", "\"", ")", "if", "cls", ".", "functions", "[", "fx_name", "]", "is", "None", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "can", "'", "t", "`", "self", ".", "log", "(", ")", "`", "inside", "`", "{", "fx_name", "}", "`", ".", "HINT", ":", "You", "can", "still", "log", "directly", "to", "the", "logger", "by", "using", "\"", "\"", "`", "self", ".", "logger", ".", "experiment", "`", ".", "\"", ")"], "docstring": "Check if the given hook is allowed to log.", "docstring_tokens": ["check", "if", "the", "given", "hook", "is", "allowed", "to", "log"], "docstring_summary": "Check if the given hook is allowed to log.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "partition": "test", "function_type": "class_method", "class_name": "_FxValidator", "start_line": 151, "end_line": 163, "hash": "4eab42b1bcb41da90cc500ace5d88b0b", "complexity": 3, "parameters": ["fx_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "get_default_logging_levels", "original_string": "def get_default_logging_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Return default logging levels for given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        on_step = fx_config[\"default_on_step\"] if on_step is None else on_step\r\n        on_epoch = fx_config[\"default_on_epoch\"] if on_epoch is None else on_epoch\r\n        return on_step, on_epoch", "language": "python", "code": "def get_default_logging_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Return default logging levels for given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        on_step = fx_config[\"default_on_step\"] if on_step is None else on_step\r\n        on_epoch = fx_config[\"default_on_epoch\"] if on_epoch is None else on_epoch\r\n        return on_step, on_epoch", "code_tokens": ["def", "get_default_logging_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "Optional", "[", "bool", "]", ",", "on_epoch", ":", "Optional", "[", "bool", "]", ")", "-", ">", "tuple", "[", "bool", ",", "bool", "]", ":", "\"", "\"", "\"", "Return", "default", "logging", "levels", "for", "given", "hook", ".", "\"", "\"", "\"", "fx_config", "=", "cls", ".", "functions", "[", "fx_name", "]", "assert", "fx_config", "is", "not", "None", "on_step", "=", "fx_config", "[", "\"", "default_on_step", "\"", "]", "if", "on_step", "is", "None", "else", "on_step", "on_epoch", "=", "fx_config", "[", "\"", "default_on_epoch", "\"", "]", "if", "on_epoch", "is", "None", "else", "on_epoch", "return", "on_step", ",", "on_epoch"], "docstring": "Return default logging levels for given hook.", "docstring_tokens": ["return", "default", "logging", "levels", "for", "given", "hook"], "docstring_summary": "Return default logging levels for given hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "partition": "test", "function_type": "class_method", "class_name": "_FxValidator", "start_line": 166, "end_line": 174, "hash": "8527c1d121c941d83d8ca4dcb6256912", "complexity": 3, "parameters": ["fx_name", "on_step", "on_epoch"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "check_logging_levels", "original_string": "def check_logging_levels(cls, fx_name: str, on_step: bool, on_epoch: bool) -> None:\r\n        \"\"\"Check if the logging levels are allowed in the given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        m = \"You can't `self.log({}={})` inside `{}`, must be one of {}.\"\r\n        if on_step not in fx_config[\"allowed_on_step\"]:\r\n            msg = m.format(\"on_step\", on_step, fx_name, fx_config[\"allowed_on_step\"])\r\n            raise MisconfigurationException(msg)\r\n\r\n        if on_epoch not in fx_config[\"allowed_on_epoch\"]:\r\n            msg = m.format(\"on_epoch\", on_epoch, fx_name, fx_config[\"allowed_on_epoch\"])\r\n            raise MisconfigurationException(msg)", "language": "python", "code": "def check_logging_levels(cls, fx_name: str, on_step: bool, on_epoch: bool) -> None:\r\n        \"\"\"Check if the logging levels are allowed in the given hook.\"\"\"\r\n        fx_config = cls.functions[fx_name]\r\n        assert fx_config is not None\r\n        m = \"You can't `self.log({}={})` inside `{}`, must be one of {}.\"\r\n        if on_step not in fx_config[\"allowed_on_step\"]:\r\n            msg = m.format(\"on_step\", on_step, fx_name, fx_config[\"allowed_on_step\"])\r\n            raise MisconfigurationException(msg)\r\n\r\n        if on_epoch not in fx_config[\"allowed_on_epoch\"]:\r\n            msg = m.format(\"on_epoch\", on_epoch, fx_name, fx_config[\"allowed_on_epoch\"])\r\n            raise MisconfigurationException(msg)", "code_tokens": ["def", "check_logging_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "bool", ",", "on_epoch", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Check", "if", "the", "logging", "levels", "are", "allowed", "in", "the", "given", "hook", ".", "\"", "\"", "\"", "fx_config", "=", "cls", ".", "functions", "[", "fx_name", "]", "assert", "fx_config", "is", "not", "None", "m", "=", "\"", "You", "can", "'", "t", "`", "self", ".", "log", "(", "{", "}", "=", "{", "}", ")", "`", "inside", "`", "{", "}", "`", ",", "must", "be", "one", "of", "{", "}", ".", "\"", "if", "on_step", "not", "in", "fx_config", "[", "\"", "allowed_on_step", "\"", "]", ":", "msg", "=", "m", ".", "format", "(", "\"", "on_step", "\"", ",", "on_step", ",", "fx_name", ",", "fx_config", "[", "\"", "allowed_on_step", "\"", "]", ")", "raise", "MisconfigurationException", "(", "msg", ")", "if", "on_epoch", "not", "in", "fx_config", "[", "\"", "allowed_on_epoch", "\"", "]", ":", "msg", "=", "m", ".", "format", "(", "\"", "on_epoch", "\"", ",", "on_epoch", ",", "fx_name", ",", "fx_config", "[", "\"", "allowed_on_epoch", "\"", "]", ")", "raise", "MisconfigurationException", "(", "msg", ")"], "docstring": "Check if the logging levels are allowed in the given hook.", "docstring_tokens": ["check", "if", "the", "logging", "levels", "are", "allowed", "in", "the", "given", "hook"], "docstring_summary": "Check if the logging levels are allowed in the given hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "partition": "test", "function_type": "class_method", "class_name": "_FxValidator", "start_line": 177, "end_line": 188, "hash": "1881888edc5ec5f0bf45d2fe4d5ece97", "complexity": 3, "parameters": ["fx_name", "on_step", "on_epoch"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "func_name": "check_logging_and_get_default_levels", "original_string": "def check_logging_and_get_default_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Check if the given hook name is allowed to log and return logging levels.\"\"\"\r\n        cls.check_logging(fx_name)\r\n        on_step, on_epoch = cls.get_default_logging_levels(fx_name, on_step, on_epoch)\r\n        cls.check_logging_levels(fx_name, on_step, on_epoch)\r\n        return on_step, on_epoch", "language": "python", "code": "def check_logging_and_get_default_levels(\r\n        cls, fx_name: str, on_step: Optional[bool], on_epoch: Optional[bool]\r\n    ) -> tuple[bool, bool]:\r\n        \"\"\"Check if the given hook name is allowed to log and return logging levels.\"\"\"\r\n        cls.check_logging(fx_name)\r\n        on_step, on_epoch = cls.get_default_logging_levels(fx_name, on_step, on_epoch)\r\n        cls.check_logging_levels(fx_name, on_step, on_epoch)\r\n        return on_step, on_epoch", "code_tokens": ["def", "check_logging_and_get_default_levels", "(", "cls", ",", "fx_name", ":", "str", ",", "on_step", ":", "Optional", "[", "bool", "]", ",", "on_epoch", ":", "Optional", "[", "bool", "]", ")", "-", ">", "tuple", "[", "bool", ",", "bool", "]", ":", "\"", "\"", "\"", "Check", "if", "the", "given", "hook", "name", "is", "allowed", "to", "log", "and", "return", "logging", "levels", ".", "\"", "\"", "\"", "cls", ".", "check_logging", "(", "fx_name", ")", "on_step", ",", "on_epoch", "=", "cls", ".", "get_default_logging_levels", "(", "fx_name", ",", "on_step", ",", "on_epoch", ")", "cls", ".", "check_logging_levels", "(", "fx_name", ",", "on_step", ",", "on_epoch", ")", "return", "on_step", ",", "on_epoch"], "docstring": "Check if the given hook name is allowed to log and return logging levels.", "docstring_tokens": ["check", "if", "the", "given", "hook", "name", "is", "allowed", "to", "log", "and", "return", "logging", "levels"], "docstring_summary": "Check if the given hook name is allowed to log and return logging levels.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\fx_validator.py", "partition": "test", "function_type": "class_method", "class_name": "_FxValidator", "start_line": 191, "end_line": 198, "hash": "8e98bd105b91b3c64849673f9c9292cb", "complexity": 1, "parameters": ["fx_name", "on_step", "on_epoch"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "func_name": "log_metrics", "original_string": "def log_metrics(self, metrics: _OUT_DICT, step: Optional[int] = None) -> None:\r\n        \"\"\"Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses\r\n        metrics[\"step\"] as a step.\r\n\r\n        Args:\r\n            metrics: Metric values\r\n            step: Step for which metrics should be logged. If a `step` metric is logged, this value will\r\n                be used else will default to `self.global_step` during training or the total log step count\r\n                during validation and testing.\r\n\r\n        \"\"\"\r\n        if not self.trainer.loggers or not metrics:\r\n            return\r\n\r\n        self._logged_metrics.update(metrics)\r\n\r\n        # turn all tensors to scalars\r\n        scalar_metrics = convert_tensors_to_scalars(metrics)\r\n\r\n        if step is None:\r\n            step_metric = scalar_metrics.pop(\"step\", None)\r\n            if step_metric is not None:\r\n                step = int(step_metric)\r\n            else:\r\n                # added metrics for convenience\r\n                scalar_metrics.setdefault(\"epoch\", self.trainer.current_epoch)\r\n                step = self.trainer.fit_loop.epoch_loop._batches_that_stepped\r\n\r\n        # log actual metrics\r\n        for logger in self.trainer.loggers:\r\n            logger.log_metrics(metrics=scalar_metrics, step=step)\r\n            logger.save()", "language": "python", "code": "def log_metrics(self, metrics: _OUT_DICT, step: Optional[int] = None) -> None:\r\n        \"\"\"Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses\r\n        metrics[\"step\"] as a step.\r\n\r\n        Args:\r\n            metrics: Metric values\r\n            step: Step for which metrics should be logged. If a `step` metric is logged, this value will\r\n                be used else will default to `self.global_step` during training or the total log step count\r\n                during validation and testing.\r\n\r\n        \"\"\"\r\n        if not self.trainer.loggers or not metrics:\r\n            return\r\n\r\n        self._logged_metrics.update(metrics)\r\n\r\n        # turn all tensors to scalars\r\n        scalar_metrics = convert_tensors_to_scalars(metrics)\r\n\r\n        if step is None:\r\n            step_metric = scalar_metrics.pop(\"step\", None)\r\n            if step_metric is not None:\r\n                step = int(step_metric)\r\n            else:\r\n                # added metrics for convenience\r\n                scalar_metrics.setdefault(\"epoch\", self.trainer.current_epoch)\r\n                step = self.trainer.fit_loop.epoch_loop._batches_that_stepped\r\n\r\n        # log actual metrics\r\n        for logger in self.trainer.loggers:\r\n            logger.log_metrics(metrics=scalar_metrics, step=step)\r\n            logger.save()", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "_OUT_DICT", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Logs", "the", "metric", "dict", "passed", "in", ".", "If", "`", "step", "`", "parameter", "is", "None", "and", "`", "step", "`", "key", "is", "presented", "is", "metrics", ",", "uses", "metrics", "[", "\"", "step", "\"", "]", "as", "a", "step", ".", "Args", ":", "metrics", ":", "Metric", "values", "step", ":", "Step", "for", "which", "metrics", "should", "be", "logged", ".", "If", "a", "`", "step", "`", "metric", "is", "logged", ",", "this", "value", "will", "be", "used", "else", "will", "default", "to", "`", "self", ".", "global_step", "`", "during", "training", "or", "the", "total", "log", "step", "count", "during", "validation", "and", "testing", ".", "\"", "\"", "\"", "if", "not", "self", ".", "trainer", ".", "loggers", "or", "not", "metrics", ":", "return", "self", ".", "_logged_metrics", ".", "update", "(", "metrics", ")", "scalar_metrics", "=", "convert_tensors_to_scalars", "(", "metrics", ")", "if", "step", "is", "None", ":", "step_metric", "=", "scalar_metrics", ".", "pop", "(", "\"", "step", "\"", ",", "None", ")", "if", "step_metric", "is", "not", "None", ":", "step", "=", "int", "(", "step_metric", ")", "else", ":", "scalar_metrics", ".", "setdefault", "(", "\"", "epoch", "\"", ",", "self", ".", "trainer", ".", "current_epoch", ")", "step", "=", "self", ".", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "_batches_that_stepped", "for", "logger", "in", "self", ".", "trainer", ".", "loggers", ":", "logger", ".", "log_metrics", "(", "metrics", "=", "scalar_metrics", ",", "step", "=", "step", ")", "logger", ".", "save", "(", ")"], "docstring": "Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses\r\n        metrics[\"step\"] as a step.\r\n\r\n        Args:\r\n            metrics: Metric values\r\n            step: Step for which metrics should be logged. If a `step` metric is logged, this value will\r\n                be used else will default to `self.global_step` during training or the total log step count\r\n                during validation and testing.", "docstring_tokens": ["logs", "the", "metric", "dict", "passed", "in", "if", "step", "parameter", "is", "none", "and", "step", "key", "is", "presented", "is", "metrics", "uses", "metrics", "step", "as", "a", "step", "args", "metrics", "metric", "values", "step", "step", "for", "which", "metrics", "should", "be", "logged", "if", "a", "step", "metric", "is", "logged", "this", "value", "will", "be", "used", "else", "will", "default", "to", "self", "global_step", "during", "training", "or", "the", "total", "log", "step", "count", "during", "validation", "and", "testing"], "docstring_summary": "Logs the metric dict passed in. If `step` parameter is None and `step` key is presented is metrics, uses", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_LoggerConnector", "start_line": 89, "end_line": 120, "hash": "5cc6ca43344005d46f6dcc4e7334685b", "complexity": 6, "parameters": ["metrics", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "func_name": "metrics", "original_string": "def metrics(self) -> _METRICS:\r\n        \"\"\"This function returns either batch or epoch metrics.\"\"\"\r\n        on_step = self._first_loop_iter is not None\r\n        assert self.trainer._results is not None\r\n        return self.trainer._results.metrics(on_step)", "language": "python", "code": "def metrics(self) -> _METRICS:\r\n        \"\"\"This function returns either batch or epoch metrics.\"\"\"\r\n        on_step = self._first_loop_iter is not None\r\n        assert self.trainer._results is not None\r\n        return self.trainer._results.metrics(on_step)", "code_tokens": ["def", "metrics", "(", "self", ")", "-", ">", "_METRICS", ":", "\"", "\"", "\"", "This", "function", "returns", "either", "batch", "or", "epoch", "metrics", ".", "\"", "\"", "\"", "on_step", "=", "self", ".", "_first_loop_iter", "is", "not", "None", "assert", "self", ".", "trainer", ".", "_results", "is", "not", "None", "return", "self", ".", "trainer", ".", "_results", ".", "metrics", "(", "on_step", ")"], "docstring": "This function returns either batch or epoch metrics.", "docstring_tokens": ["this", "function", "returns", "either", "batch", "or", "epoch", "metrics"], "docstring_summary": "This function returns either batch or epoch metrics.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py", "partition": "test", "function_type": "class_method", "class_name": "_LoggerConnector", "start_line": 232, "end_line": 236, "hash": "b9b31b060329126af6b69efb832cfc65", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "_generate_sync_fn", "original_string": "def _generate_sync_fn(self) -> None:\r\n        \"\"\"Used to compute the syncing function and cache it.\"\"\"\r\n        fn = self.no_op if self.fn is None or not self.should or self.rank_zero_only else self.fn\r\n        # save the function as `_fn` as the meta are being re-created and the object references need to match.\r\n        self._fn: Callable = partial(fn, reduce_op=self.op, group=self.group)", "language": "python", "code": "def _generate_sync_fn(self) -> None:\r\n        \"\"\"Used to compute the syncing function and cache it.\"\"\"\r\n        fn = self.no_op if self.fn is None or not self.should or self.rank_zero_only else self.fn\r\n        # save the function as `_fn` as the meta are being re-created and the object references need to match.\r\n        self._fn: Callable = partial(fn, reduce_op=self.op, group=self.group)", "code_tokens": ["def", "_generate_sync_fn", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Used", "to", "compute", "the", "syncing", "function", "and", "cache", "it", ".", "\"", "\"", "\"", "fn", "=", "self", ".", "no_op", "if", "self", ".", "fn", "is", "None", "or", "not", "self", ".", "should", "or", "self", ".", "rank_zero_only", "else", "self", ".", "fn", "self", ".", "_fn", ":", "Callable", "=", "partial", "(", "fn", ",", "reduce_op", "=", "self", ".", "op", ",", "group", "=", "self", ".", "group", ")"], "docstring": "Used to compute the syncing function and cache it.", "docstring_tokens": ["used", "to", "compute", "the", "syncing", "function", "and", "cache", "it"], "docstring_summary": "Used to compute the syncing function and cache it.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "partition": "test", "function_type": "class_method", "class_name": "_Sync", "start_line": 89, "end_line": 93, "hash": "5720e808b07560231c30c144d5b76a1d", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "log", "original_string": "def log(\r\n        self,\r\n        fx: str,\r\n        name: str,\r\n        value: _VALUE,\r\n        prog_bar: bool = False,\r\n        logger: bool = True,\r\n        on_step: bool = False,\r\n        on_epoch: bool = True,\r\n        # https://github.com/pytorch/pytorch/issues/96197\r\n        reduce_fx: Callable = torch.mean,\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_fn: Callable = _Sync.no_op,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"See :meth:`~lightning.pytorch.core.LightningModule.log`\"\"\"\r\n        # no metrics should be logged with graphs\r\n        if not enable_graph:\r\n            value = recursive_detach(value)\r\n\r\n        # storage key\r\n        key = f\"{fx}.{name}\"\r\n        # add dataloader_suffix to both key and fx\r\n        if add_dataloader_idx and self.dataloader_idx is not None:\r\n            key += f\".{self.dataloader_idx}\"\r\n            fx += f\".{self.dataloader_idx}\"\r\n\r\n        meta = _Metadata(\r\n            fx=fx,\r\n            name=name,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            dataloader_idx=self.dataloader_idx,\r\n            metric_attribute=metric_attribute,\r\n        )\r\n        meta.sync = _Sync(_should=sync_dist, fn=sync_dist_fn, _group=sync_dist_group, rank_zero_only=rank_zero_only)\r\n\r\n        # register logged value if it doesn't exist\r\n        if key not in self:\r\n            metric = _ResultMetric(meta, isinstance(value, Tensor))\r\n            self[key] = metric\r\n\r\n        # check the stored metadata and the current one match\r\n        elif meta != self[key].meta:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log({name}, ...)` twice in `{fx}` with different arguments. This is not allowed\"\r\n            )\r\n        self[key].to(value.device)\r\n\r\n        batch_size = self._extract_batch_size(self[key], batch_size, meta)\r\n        self.update_metrics(key, value, batch_size)", "language": "python", "code": "def log(\r\n        self,\r\n        fx: str,\r\n        name: str,\r\n        value: _VALUE,\r\n        prog_bar: bool = False,\r\n        logger: bool = True,\r\n        on_step: bool = False,\r\n        on_epoch: bool = True,\r\n        # https://github.com/pytorch/pytorch/issues/96197\r\n        reduce_fx: Callable = torch.mean,\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_fn: Callable = _Sync.no_op,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        metric_attribute: Optional[str] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"See :meth:`~lightning.pytorch.core.LightningModule.log`\"\"\"\r\n        # no metrics should be logged with graphs\r\n        if not enable_graph:\r\n            value = recursive_detach(value)\r\n\r\n        # storage key\r\n        key = f\"{fx}.{name}\"\r\n        # add dataloader_suffix to both key and fx\r\n        if add_dataloader_idx and self.dataloader_idx is not None:\r\n            key += f\".{self.dataloader_idx}\"\r\n            fx += f\".{self.dataloader_idx}\"\r\n\r\n        meta = _Metadata(\r\n            fx=fx,\r\n            name=name,\r\n            prog_bar=prog_bar,\r\n            logger=logger,\r\n            on_step=on_step,\r\n            on_epoch=on_epoch,\r\n            reduce_fx=reduce_fx,\r\n            enable_graph=enable_graph,\r\n            add_dataloader_idx=add_dataloader_idx,\r\n            dataloader_idx=self.dataloader_idx,\r\n            metric_attribute=metric_attribute,\r\n        )\r\n        meta.sync = _Sync(_should=sync_dist, fn=sync_dist_fn, _group=sync_dist_group, rank_zero_only=rank_zero_only)\r\n\r\n        # register logged value if it doesn't exist\r\n        if key not in self:\r\n            metric = _ResultMetric(meta, isinstance(value, Tensor))\r\n            self[key] = metric\r\n\r\n        # check the stored metadata and the current one match\r\n        elif meta != self[key].meta:\r\n            raise MisconfigurationException(\r\n                f\"You called `self.log({name}, ...)` twice in `{fx}` with different arguments. This is not allowed\"\r\n            )\r\n        self[key].to(value.device)\r\n\r\n        batch_size = self._extract_batch_size(self[key], batch_size, meta)\r\n        self.update_metrics(key, value, batch_size)", "code_tokens": ["def", "log", "(", "self", ",", "fx", ":", "str", ",", "name", ":", "str", ",", "value", ":", "_VALUE", ",", "prog_bar", ":", "bool", "=", "False", ",", "logger", ":", "bool", "=", "True", ",", "on_step", ":", "bool", "=", "False", ",", "on_epoch", ":", "bool", "=", "True", ",", "reduce_fx", ":", "Callable", "=", "torch", ".", "mean", ",", "enable_graph", ":", "bool", "=", "False", ",", "sync_dist", ":", "bool", "=", "False", ",", "sync_dist_fn", ":", "Callable", "=", "_Sync", ".", "no_op", ",", "sync_dist_group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "add_dataloader_idx", ":", "bool", "=", "True", ",", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "metric_attribute", ":", "Optional", "[", "str", "]", "=", "None", ",", "rank_zero_only", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "log", "`", "\"", "\"", "\"", "if", "not", "enable_graph", ":", "value", "=", "recursive_detach", "(", "value", ")", "key", "=", "f", "\"", "{", "fx", "}", ".", "{", "name", "}", "\"", "if", "add_dataloader_idx", "and", "self", ".", "dataloader_idx", "is", "not", "None", ":", "key", "+", "=", "f", "\"", ".", "{", "self", ".", "dataloader_idx", "}", "\"", "fx", "+", "=", "f", "\"", ".", "{", "self", ".", "dataloader_idx", "}", "\"", "meta", "=", "_Metadata", "(", "fx", "=", "fx", ",", "name", "=", "name", ",", "prog_bar", "=", "prog_bar", ",", "logger", "=", "logger", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ",", "reduce_fx", "=", "reduce_fx", ",", "enable_graph", "=", "enable_graph", ",", "add_dataloader_idx", "=", "add_dataloader_idx", ",", "dataloader_idx", "=", "self", ".", "dataloader_idx", ",", "metric_attribute", "=", "metric_attribute", ",", ")", "meta", ".", "sync", "=", "_Sync", "(", "_should", "=", "sync_dist", ",", "fn", "=", "sync_dist_fn", ",", "_group", "=", "sync_dist_group", ",", "rank_zero_only", "=", "rank_zero_only", ")", "if", "key", "not", "in", "self", ":", "metric", "=", "_ResultMetric", "(", "meta", ",", "isinstance", "(", "value", ",", "Tensor", ")", ")", "self", "[", "key", "]", "=", "metric", "elif", "meta", "!", "=", "self", "[", "key", "]", ".", "meta", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "called", "`", "self", ".", "log", "(", "{", "name", "}", ",", ".", ".", ".", ")", "`", "twice", "in", "`", "{", "fx", "}", "`", "with", "different", "arguments", ".", "This", "is", "not", "allowed", "\"", ")", "self", "[", "key", "]", ".", "to", "(", "value", ".", "device", ")", "batch_size", "=", "self", ".", "_extract_batch_size", "(", "self", "[", "key", "]", ",", "batch_size", ",", "meta", ")", "self", ".", "update_metrics", "(", "key", ",", "value", ",", "batch_size", ")"], "docstring": "See :meth:`~lightning.pytorch.core.LightningModule.log`", "docstring_tokens": ["see", "meth", "lightning", "pytorch", "core", "lightningmodule", "log"], "docstring_summary": "See :meth:`~lightning.pytorch.core.LightningModule.log`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "partition": "test", "function_type": "class_method", "class_name": "_ResultCollection", "start_line": 354, "end_line": 414, "hash": "eeae22e534d830edda5feabb4da5f983", "complexity": 6, "parameters": ["fx", "name", "value", "prog_bar", "logger", "on_step", "on_epoch", "# https", "enable_graph", "sync_dist", "sync_dist_fn", "sync_dist_group", "add_dataloader_idx", "batch_size", "metric_attribute", "rank_zero_only"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "reset", "original_string": "def reset(self, metrics: Optional[bool] = None, fx: Optional[str] = None) -> None:\r\n        \"\"\"Reset the result collection.\r\n\r\n        Args:\r\n            metrics: If True, only ``torchmetrics.Metric`` results are reset,\r\n                if False, only ``torch.Tensors`` are reset,\r\n                if ``None``, both are.\r\n            fx: Function to reset\r\n\r\n        \"\"\"\r\n        for item in self.values():\r\n            requested_type = metrics is None or metrics ^ item.is_tensor\r\n            same_fx = fx is None or fx == item.meta.fx\r\n            if requested_type and same_fx:\r\n                item.reset()", "language": "python", "code": "def reset(self, metrics: Optional[bool] = None, fx: Optional[str] = None) -> None:\r\n        \"\"\"Reset the result collection.\r\n\r\n        Args:\r\n            metrics: If True, only ``torchmetrics.Metric`` results are reset,\r\n                if False, only ``torch.Tensors`` are reset,\r\n                if ``None``, both are.\r\n            fx: Function to reset\r\n\r\n        \"\"\"\r\n        for item in self.values():\r\n            requested_type = metrics is None or metrics ^ item.is_tensor\r\n            same_fx = fx is None or fx == item.meta.fx\r\n            if requested_type and same_fx:\r\n                item.reset()", "code_tokens": ["def", "reset", "(", "self", ",", "metrics", ":", "Optional", "[", "bool", "]", "=", "None", ",", "fx", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Reset", "the", "result", "collection", ".", "Args", ":", "metrics", ":", "If", "True", ",", "only", "`", "`", "torchmetrics", ".", "Metric", "`", "`", "results", "are", "reset", ",", "if", "False", ",", "only", "`", "`", "torch", ".", "Tensors", "`", "`", "are", "reset", ",", "if", "`", "`", "None", "`", "`", ",", "both", "are", ".", "fx", ":", "Function", "to", "reset", "\"", "\"", "\"", "for", "item", "in", "self", ".", "values", "(", ")", ":", "requested_type", "=", "metrics", "is", "None", "or", "metrics", "^", "item", ".", "is_tensor", "same_fx", "=", "fx", "is", "None", "or", "fx", "=", "=", "item", ".", "meta", ".", "fx", "if", "requested_type", "and", "same_fx", ":", "item", ".", "reset", "(", ")"], "docstring": "Reset the result collection.\r\n\r\n        Args:\r\n            metrics: If True, only ``torchmetrics.Metric`` results are reset,\r\n                if False, only ``torch.Tensors`` are reset,\r\n                if ``None``, both are.\r\n            fx: Function to reset", "docstring_tokens": ["reset", "the", "result", "collection", "args", "metrics", "if", "true", "only", "torchmetrics", "metric", "results", "are", "reset", "if", "false", "only", "torch", "tensors", "are", "reset", "if", "none", "both", "are", "fx", "function", "to", "reset"], "docstring_summary": "Reset the result collection.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "partition": "test", "function_type": "class_method", "class_name": "_ResultCollection", "start_line": 495, "end_line": 509, "hash": "155f89f3205398d9e195ec8f8850a37b", "complexity": 6, "parameters": ["metrics", "fx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "to", "original_string": "def to(self, *args: Any, **kwargs: Any) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to the given device.\"\"\"\r\n        self.update(apply_to_collection(dict(self), (Tensor, Metric), move_data_to_device, *args, **kwargs))\r\n        return self", "language": "python", "code": "def to(self, *args: Any, **kwargs: Any) -> \"_ResultCollection\":\r\n        \"\"\"Move all data to the given device.\"\"\"\r\n        self.update(apply_to_collection(dict(self), (Tensor, Metric), move_data_to_device, *args, **kwargs))\r\n        return self", "code_tokens": ["def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "\"", "_ResultCollection", "\"", ":", "\"", "\"", "\"", "Move", "all", "data", "to", "the", "given", "device", ".", "\"", "\"", "\"", "self", ".", "update", "(", "apply_to_collection", "(", "dict", "(", "self", ")", ",", "(", "Tensor", ",", "Metric", ")", ",", "move_data_to_device", ",", "*", "args", ",", "*", "*", "kwargs", ")", ")", "return", "self"], "docstring": "Move all data to the given device.", "docstring_tokens": ["move", "all", "data", "to", "the", "given", "device"], "docstring_summary": "Move all data to the given device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "partition": "test", "function_type": "class_method", "class_name": "_ResultCollection", "start_line": 511, "end_line": 514, "hash": "955ede779f503e34c8e38c3eb54859a3", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "func_name": "_get_default_dtype", "original_string": "def _get_default_dtype() -> torch.dtype:\r\n    \"\"\"The default dtype for new tensors, but no lower than float32.\"\"\"\r\n    dtype = torch.get_default_dtype()\r\n    return dtype if dtype in (torch.float32, torch.float64) else torch.float32", "language": "python", "code": "def _get_default_dtype() -> torch.dtype:\r\n    \"\"\"The default dtype for new tensors, but no lower than float32.\"\"\"\r\n    dtype = torch.get_default_dtype()\r\n    return dtype if dtype in (torch.float32, torch.float64) else torch.float32", "code_tokens": ["def", "_get_default_dtype", "(", ")", "-", ">", "torch", ".", "dtype", ":", "\"", "\"", "\"", "The", "default", "dtype", "for", "new", "tensors", ",", "but", "no", "lower", "than", "float32", ".", "\"", "\"", "\"", "dtype", "=", "torch", ".", "get_default_dtype", "(", ")", "return", "dtype", "if", "dtype", "in", "(", "torch", ".", "float32", ",", "torch", ".", "float64", ")", "else", "torch", ".", "float32"], "docstring": "The default dtype for new tensors, but no lower than float32.", "docstring_tokens": ["the", "default", "dtype", "for", "new", "tensors", "but", "no", "lower", "than", "float32"], "docstring_summary": "The default dtype for new tensors, but no lower than float32.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py", "partition": "test", "function_type": "function", "start_line": 529, "end_line": 532, "hash": "837a43e39f1e9702f7523d5f8fb1b4e9", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "_scale_batch_size", "original_string": "def _scale_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    mode: str = \"power\",\r\n    steps_per_trial: int = 3,\r\n    init_val: int = 2,\r\n    max_trials: int = 25,\r\n    batch_arg_name: str = \"batch_size\",\r\n) -> Optional[int]:\r\n    \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n    error.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        mode: Search strategy to update the batch size:\r\n\r\n            - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n            - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n        steps_per_trial: number of steps to run with a given batch size.\r\n            Ideally 1 should be enough to test if an OOM error occurs,\r\n            however in practise a few are needed\r\n        init_val: initial batch size to start the search with\r\n        max_trials: max number of increases in batch size done before\r\n           algorithm is terminated\r\n        batch_arg_name: name of the attribute that stores the batch size.\r\n            It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n            with that name. We will look for this attribute name in the following places\r\n\r\n            - ``model``\r\n            - ``model.hparams``\r\n            - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping batch size scaler since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    # Save initial model, that is loaded after batch size is found\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".scale_batch_size_{uuid.uuid4()}.ckpt\")\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    # Arguments we adjust during the batch size finder, save for restoring\r\n    params = __scale_batch_dump_params(trainer)\r\n\r\n    # Set to values that are required by the algorithm\r\n    __scale_batch_reset_params(trainer, steps_per_trial)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    try:\r\n        new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=init_val)\r\n\r\n        if mode == \"power\":\r\n            new_size = _run_power_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n        elif mode == \"binsearch\":\r\n            new_size = _run_binary_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n\r\n        garbage_collection_cuda()\r\n\r\n        log.info(f\"Finished batch size finder, will continue with full run using batch size {new_size}\")\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        __scale_batch_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n\r\n    return new_size", "language": "python", "code": "def _scale_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    mode: str = \"power\",\r\n    steps_per_trial: int = 3,\r\n    init_val: int = 2,\r\n    max_trials: int = 25,\r\n    batch_arg_name: str = \"batch_size\",\r\n) -> Optional[int]:\r\n    \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n    error.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        mode: Search strategy to update the batch size:\r\n\r\n            - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n            - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n        steps_per_trial: number of steps to run with a given batch size.\r\n            Ideally 1 should be enough to test if an OOM error occurs,\r\n            however in practise a few are needed\r\n        init_val: initial batch size to start the search with\r\n        max_trials: max number of increases in batch size done before\r\n           algorithm is terminated\r\n        batch_arg_name: name of the attribute that stores the batch size.\r\n            It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n            with that name. We will look for this attribute name in the following places\r\n\r\n            - ``model``\r\n            - ``model.hparams``\r\n            - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping batch size scaler since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    # Save initial model, that is loaded after batch size is found\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".scale_batch_size_{uuid.uuid4()}.ckpt\")\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    # Arguments we adjust during the batch size finder, save for restoring\r\n    params = __scale_batch_dump_params(trainer)\r\n\r\n    # Set to values that are required by the algorithm\r\n    __scale_batch_reset_params(trainer, steps_per_trial)\r\n\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    try:\r\n        new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=init_val)\r\n\r\n        if mode == \"power\":\r\n            new_size = _run_power_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n        elif mode == \"binsearch\":\r\n            new_size = _run_binary_scaling(trainer, new_size, batch_arg_name, max_trials, params)\r\n\r\n        garbage_collection_cuda()\r\n\r\n        log.info(f\"Finished batch size finder, will continue with full run using batch size {new_size}\")\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        __scale_batch_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n\r\n    return new_size", "code_tokens": ["def", "_scale_batch_size", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "mode", ":", "str", "=", "\"", "power", "\"", ",", "steps_per_trial", ":", "int", "=", "3", ",", "init_val", ":", "int", "=", "2", ",", "max_trials", ":", "int", "=", "25", ",", "batch_arg_name", ":", "str", "=", "\"", "batch_size", "\"", ",", ")", "-", ">", "Optional", "[", "int", "]", ":", "\"", "\"", "\"", "Iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "(", "OOM", ")", "error", ".", "Args", ":", "trainer", ":", "A", "Trainer", "instance", ".", "mode", ":", "Search", "strategy", "to", "update", "the", "batch", "size", ":", "-", "`", "`", "'", "power", "'", "`", "`", ":", "Keep", "multiplying", "the", "batch", "size", "by", "2", ",", "until", "we", "get", "an", "OOM", "error", ".", "-", "`", "`", "'", "binsearch", "'", "`", "`", ":", "Initially", "keep", "multiplying", "by", "2", "and", "after", "encountering", "an", "OOM", "error", "do", "a", "binary", "search", "between", "the", "last", "successful", "batch", "size", "and", "the", "batch", "size", "that", "failed", ".", "steps_per_trial", ":", "number", "of", "steps", "to", "run", "with", "a", "given", "batch", "size", ".", "Ideally", "1", "should", "be", "enough", "to", "test", "if", "an", "OOM", "error", "occurs", ",", "however", "in", "practise", "a", "few", "are", "needed", "init_val", ":", "initial", "batch", "size", "to", "start", "the", "search", "with", "max_trials", ":", "max", "number", "of", "increases", "in", "batch", "size", "done", "before", "algorithm", "is", "terminated", "batch_arg_name", ":", "name", "of", "the", "attribute", "that", "stores", "the", "batch", "size", ".", "It", "is", "expected", "that", "the", "user", "has", "provided", "a", "model", "or", "datamodule", "that", "has", "a", "hyperparameter", "with", "that", "name", ".", "We", "will", "look", "for", "this", "attribute", "name", "in", "the", "following", "places", "-", "`", "`", "model", "`", "`", "-", "`", "`", "model", ".", "hparams", "`", "`", "-", "`", "`", "trainer", ".", "datamodule", "`", "`", "(", "the", "datamodule", "passed", "to", "the", "tune", "method", ")", "\"", "\"", "\"", "if", "trainer", ".", "fast_dev_run", ":", "rank_zero_warn", "(", "\"", "Skipping", "batch", "size", "scaler", "since", "`", "fast_dev_run", "`", "is", "enabled", ".", "\"", ")", "return", "None", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "f", "\"", ".", "scale_batch_size_", "{", "uuid", ".", "uuid4", "(", ")", "}", ".", "ckpt", "\"", ")", "trainer", ".", "save_checkpoint", "(", "ckpt_path", ")", "params", "=", "__scale_batch_dump_params", "(", "trainer", ")", "__scale_batch_reset_params", "(", "trainer", ",", "steps_per_trial", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "disable", "(", ")", "try", ":", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "init_val", ")", "if", "mode", "=", "=", "\"", "power", "\"", ":", "new_size", "=", "_run_power_scaling", "(", "trainer", ",", "new_size", ",", "batch_arg_name", ",", "max_trials", ",", "params", ")", "elif", "mode", "=", "=", "\"", "binsearch", "\"", ":", "new_size", "=", "_run_binary_scaling", "(", "trainer", ",", "new_size", ",", "batch_arg_name", ",", "max_trials", ",", "params", ")", "garbage_collection_cuda", "(", ")", "log", ".", "info", "(", "f", "\"", "Finished", "batch", "size", "finder", ",", "will", "continue", "with", "full", "run", "using", "batch", "size", "{", "new_size", "}", "\"", ")", "except", "Exception", "as", "ex", ":", "raise", "ex", "finally", ":", "__scale_batch_restore_params", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "enable", "(", ")", "trainer", ".", "_checkpoint_connector", ".", "restore", "(", "ckpt_path", ")", "trainer", ".", "strategy", ".", "remove_checkpoint", "(", "ckpt_path", ")", "return", "new_size"], "docstring": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n    error.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        mode: Search strategy to update the batch size:\r\n\r\n            - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n            - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n        steps_per_trial: number of steps to run with a given batch size.\r\n            Ideally 1 should be enough to test if an OOM error occurs,\r\n            however in practise a few are needed\r\n        init_val: initial batch size to start the search with\r\n        max_trials: max number of increases in batch size done before\r\n           algorithm is terminated\r\n        batch_arg_name: name of the attribute that stores the batch size.\r\n            It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n            with that name. We will look for this attribute name in the following places\r\n\r\n            - ``model``\r\n            - ``model.hparams``\r\n            - ``trainer.datamodule`` (the datamodule passed to the tune method)", "docstring_tokens": ["iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "oom", "error", "args", "trainer", "a", "trainer", "instance", "mode", "search", "strategy", "to", "update", "the", "batch", "size", "power", "keep", "multiplying", "the", "batch", "size", "by", "2", "until", "we", "get", "an", "oom", "error", "binsearch", "initially", "keep", "multiplying", "by", "2", "and", "after", "encountering", "an", "oom", "error", "do", "a", "binary", "search", "between", "the", "last", "successful", "batch", "size", "and", "the", "batch", "size", "that", "failed", "steps_per_trial", "number", "of", "steps", "to", "run", "with", "a", "given", "batch", "size", "ideally", "1", "should", "be", "enough", "to", "test", "if", "an", "oom", "error", "occurs", "however", "in", "practise", "a", "few", "are", "needed", "init_val", "initial", "batch", "size", "to", "start", "the", "search", "with", "max_trials", "max", "number", "of", "increases", "in", "batch", "size", "done", "before", "algorithm", "is", "terminated", "batch_arg_name", "name", "of", "the", "attribute", "that", "stores", "the", "batch", "size", "it", "is", "expected", "that", "the", "user", "has", "provided", "a", "model", "or", "datamodule", "that", "has", "a", "hyperparameter", "with", "that", "name", "we", "will", "look", "for", "this", "attribute", "name", "in", "the", "following", "places", "model", "model", "hparams", "trainer", "datamodule", "the", "datamodule", "passed", "to", "the", "tune", "method"], "docstring_summary": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "partition": "test", "function_type": "function", "start_line": 27, "end_line": 100, "hash": "aeddcaa7d1cff613630ed2cc4faadd04", "complexity": 7, "parameters": ["trainer", "mode", "steps_per_trial", "init_val", "max_trials", "batch_arg_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "_run_power_scaling", "original_string": "def _run_power_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.\"\"\"\r\n    # this flag is used to determine whether the previously scaled batch size, right before OOM, was a success or not\r\n    # if it was we exit, else we continue downscaling in case we haven't encountered a single optimal batch size\r\n    any_success = False\r\n    last_successful_size = new_size\r\n    for i in range(max_trials):\r\n        garbage_collection_cuda()\r\n\r\n        # reset after each try\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n\r\n            # Check if this is the last trial before trying to double\r\n            if i + 1 >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            # Force the train dataloader to reset as the batch size has changed\r\n            _reset_dataloaders(trainer)\r\n            any_success = True\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                # If we fail in power mode, half the size and return\r\n                garbage_collection_cuda()\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, factor=0.5, desc=\"failed\")\r\n                # Force the train dataloader to reset as the batch size has changed\r\n                _reset_dataloaders(trainer)\r\n                if any_success:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "language": "python", "code": "def _run_power_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.\"\"\"\r\n    # this flag is used to determine whether the previously scaled batch size, right before OOM, was a success or not\r\n    # if it was we exit, else we continue downscaling in case we haven't encountered a single optimal batch size\r\n    any_success = False\r\n    last_successful_size = new_size\r\n    for i in range(max_trials):\r\n        garbage_collection_cuda()\r\n\r\n        # reset after each try\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n\r\n            # Check if this is the last trial before trying to double\r\n            if i + 1 >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            # Force the train dataloader to reset as the batch size has changed\r\n            _reset_dataloaders(trainer)\r\n            any_success = True\r\n        except RuntimeError as exception:\r\n            if is_oom_error(exception):\r\n                # If we fail in power mode, half the size and return\r\n                garbage_collection_cuda()\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, factor=0.5, desc=\"failed\")\r\n                # Force the train dataloader to reset as the batch size has changed\r\n                _reset_dataloaders(trainer)\r\n                if any_success:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "code_tokens": ["def", "_run_power_scaling", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "new_size", ":", "int", ",", "batch_arg_name", ":", "str", ",", "max_trials", ":", "int", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ",", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Batch", "scaling", "mode", "where", "the", "size", "is", "doubled", "at", "each", "iteration", "until", "an", "OOM", "error", "is", "encountered", ".", "\"", "\"", "\"", "any_success", "=", "False", "last_successful_size", "=", "new_size", "for", "i", "in", "range", "(", "max_trials", ")", ":", "garbage_collection_cuda", "(", ")", "_reset_progress", "(", "trainer", ")", "try", ":", "_try_loop_run", "(", "trainer", ",", "params", ")", "last_successful_size", "=", "new_size", "if", "i", "+", "1", ">", "=", "max_trials", ":", "new_size", "=", "last_successful_size", "break", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "2", ".", "0", ",", "desc", "=", "\"", "succeeded", "\"", ")", "if", "not", "changed", ":", "break", "_reset_dataloaders", "(", "trainer", ")", "any_success", "=", "True", "except", "RuntimeError", "as", "exception", ":", "if", "is_oom_error", "(", "exception", ")", ":", "garbage_collection_cuda", "(", ")", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "0", ".", "5", ",", "desc", "=", "\"", "failed", "\"", ")", "_reset_dataloaders", "(", "trainer", ")", "if", "any_success", ":", "break", "else", ":", "raise", "return", "new_size"], "docstring": "Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.", "docstring_tokens": ["batch", "scaling", "mode", "where", "the", "size", "is", "doubled", "at", "each", "iteration", "until", "an", "oom", "error", "is", "encountered"], "docstring_summary": "Batch scaling mode where the size is doubled at each iteration until an OOM error is encountered.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "partition": "test", "function_type": "function", "start_line": 169, "end_line": 216, "hash": "a7078fbf7d0ff7bdbf49a9ed14f024f6", "complexity": 7, "parameters": ["trainer", "new_size", "batch_arg_name", "max_trials", "params", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "_run_binary_scaling", "original_string": "def _run_binary_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.\r\n\r\n    Hereafter, the batch size is further refined using a binary search\r\n\r\n    \"\"\"\r\n    low = 1\r\n    high = None\r\n    count = 0\r\n    last_successful_size = new_size\r\n    while True:\r\n        garbage_collection_cuda()\r\n\r\n        # reset after each try\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            # run loop\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n            count += 1\r\n\r\n            # Check if we've reached max_trials before trying to adjust batch size\r\n            if count >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            # Double in size\r\n            low = new_size\r\n            if high:\r\n                if high - low <= 1:\r\n                    break\r\n                midval = (high + low) // 2\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"succeeded\")\r\n            else:\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            # Force the train dataloader to reset as the batch size has changed\r\n            _reset_dataloaders(trainer)\r\n\r\n        except RuntimeError as exception:\r\n            # Only these errors should trigger an adjustment\r\n            if is_oom_error(exception):\r\n                # If we fail in power mode, half the size and return\r\n                garbage_collection_cuda()\r\n\r\n                high = new_size\r\n                midval = (high + low) // 2\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"failed\")\r\n\r\n                # Force the train dataloader to reset as the batch size has changed\r\n                _reset_dataloaders(trainer)\r\n\r\n                if high - low <= 1:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "language": "python", "code": "def _run_binary_scaling(\r\n    trainer: \"pl.Trainer\",\r\n    new_size: int,\r\n    batch_arg_name: str,\r\n    max_trials: int,\r\n    params: dict[str, Any],\r\n) -> int:\r\n    \"\"\"Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.\r\n\r\n    Hereafter, the batch size is further refined using a binary search\r\n\r\n    \"\"\"\r\n    low = 1\r\n    high = None\r\n    count = 0\r\n    last_successful_size = new_size\r\n    while True:\r\n        garbage_collection_cuda()\r\n\r\n        # reset after each try\r\n        _reset_progress(trainer)\r\n\r\n        try:\r\n            # run loop\r\n            _try_loop_run(trainer, params)\r\n            last_successful_size = new_size  # Store the current size before doubling\r\n            count += 1\r\n\r\n            # Check if we've reached max_trials before trying to adjust batch size\r\n            if count >= max_trials:\r\n                new_size = last_successful_size\r\n                break\r\n\r\n            # Double in size\r\n            low = new_size\r\n            if high:\r\n                if high - low <= 1:\r\n                    break\r\n                midval = (high + low) // 2\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"succeeded\")\r\n            else:\r\n                new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\"succeeded\")\r\n\r\n            if not changed:\r\n                break\r\n\r\n            # Force the train dataloader to reset as the batch size has changed\r\n            _reset_dataloaders(trainer)\r\n\r\n        except RuntimeError as exception:\r\n            # Only these errors should trigger an adjustment\r\n            if is_oom_error(exception):\r\n                # If we fail in power mode, half the size and return\r\n                garbage_collection_cuda()\r\n\r\n                high = new_size\r\n                midval = (high + low) // 2\r\n                new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\"failed\")\r\n\r\n                # Force the train dataloader to reset as the batch size has changed\r\n                _reset_dataloaders(trainer)\r\n\r\n                if high - low <= 1:\r\n                    break\r\n            else:\r\n                raise  # some other error not memory related\r\n\r\n    return new_size", "code_tokens": ["def", "_run_binary_scaling", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "new_size", ":", "int", ",", "batch_arg_name", ":", "str", ",", "max_trials", ":", "int", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ",", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Batch", "scaling", "mode", "where", "the", "size", "is", "initially", "is", "doubled", "at", "each", "iteration", "until", "an", "OOM", "error", "is", "encountered", ".", "Hereafter", ",", "the", "batch", "size", "is", "further", "refined", "using", "a", "binary", "search", "\"", "\"", "\"", "low", "=", "1", "high", "=", "None", "count", "=", "0", "last_successful_size", "=", "new_size", "while", "True", ":", "garbage_collection_cuda", "(", ")", "_reset_progress", "(", "trainer", ")", "try", ":", "_try_loop_run", "(", "trainer", ",", "params", ")", "last_successful_size", "=", "new_size", "count", "+", "=", "1", "if", "count", ">", "=", "max_trials", ":", "new_size", "=", "last_successful_size", "break", "low", "=", "new_size", "if", "high", ":", "if", "high", "-", "low", "<", "=", "1", ":", "break", "midval", "=", "(", "high", "+", "low", ")", "/", "/", "2", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "midval", ",", "desc", "=", "\"", "succeeded", "\"", ")", "else", ":", "new_size", ",", "changed", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "factor", "=", "2", ".", "0", ",", "desc", "=", "\"", "succeeded", "\"", ")", "if", "not", "changed", ":", "break", "_reset_dataloaders", "(", "trainer", ")", "except", "RuntimeError", "as", "exception", ":", "if", "is_oom_error", "(", "exception", ")", ":", "garbage_collection_cuda", "(", ")", "high", "=", "new_size", "midval", "=", "(", "high", "+", "low", ")", "/", "/", "2", "new_size", ",", "_", "=", "_adjust_batch_size", "(", "trainer", ",", "batch_arg_name", ",", "value", "=", "midval", ",", "desc", "=", "\"", "failed", "\"", ")", "_reset_dataloaders", "(", "trainer", ")", "if", "high", "-", "low", "<", "=", "1", ":", "break", "else", ":", "raise", "return", "new_size"], "docstring": "Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.\r\n\r\n    Hereafter, the batch size is further refined using a binary search", "docstring_tokens": ["batch", "scaling", "mode", "where", "the", "size", "is", "initially", "is", "doubled", "at", "each", "iteration", "until", "an", "oom", "error", "is", "encountered", "hereafter", "the", "batch", "size", "is", "further", "refined", "using", "a", "binary", "search"], "docstring_summary": "Batch scaling mode where the size is initially is doubled at each iteration until an OOM error is encountered.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "partition": "test", "function_type": "function", "start_line": 219, "end_line": 286, "hash": "5e9d986f9562cc60c6995ee4fdaf360a", "complexity": 9, "parameters": ["trainer", "new_size", "batch_arg_name", "max_trials", "params", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "func_name": "_adjust_batch_size", "original_string": "def _adjust_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    batch_arg_name: str = \"batch_size\",\r\n    factor: float = 1.0,\r\n    value: Optional[int] = None,\r\n    desc: Optional[str] = None,\r\n) -> tuple[int, bool]:\r\n    \"\"\"Helper function for adjusting the batch size.\r\n\r\n    Args:\r\n        trainer: instance of lightning.pytorch.Trainer\r\n        factor: value which the old batch size is multiplied by to get the\r\n            new batch size\r\n        value: if a value is given, will override the batch size with this value.\r\n            Note that the value of `factor` will not have an effect in this case\r\n        desc: either ``\"succeeded\"`` or ``\"failed\"``. Used purely for logging\r\n\r\n    Returns:\r\n        The new batch size for the next trial and a bool that signals whether the\r\n        new value is different than the previous batch size.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n    batch_size = lightning_getattr(model, batch_arg_name)\r\n    assert batch_size is not None\r\n\r\n    loop = trainer._active_loop\r\n    assert loop is not None\r\n    loop.setup_data()\r\n    combined_loader = loop._combined_loader\r\n    assert combined_loader is not None\r\n    try:\r\n        combined_dataset_length = combined_loader._dataset_length()\r\n        if batch_size >= combined_dataset_length:\r\n            rank_zero_info(f\"The batch size {batch_size} is greater or equal than the length of your dataset.\")\r\n            return batch_size, False\r\n    except NotImplementedError:\r\n        # all datasets are iterable style\r\n        pass\r\n\r\n    new_size = value if value is not None else int(batch_size * factor)\r\n    if desc:\r\n        rank_zero_info(f\"Batch size {batch_size} {desc}, trying batch size {new_size}\")\r\n    changed = new_size != batch_size\r\n    lightning_setattr(model, batch_arg_name, new_size)\r\n    return new_size, changed", "language": "python", "code": "def _adjust_batch_size(\r\n    trainer: \"pl.Trainer\",\r\n    batch_arg_name: str = \"batch_size\",\r\n    factor: float = 1.0,\r\n    value: Optional[int] = None,\r\n    desc: Optional[str] = None,\r\n) -> tuple[int, bool]:\r\n    \"\"\"Helper function for adjusting the batch size.\r\n\r\n    Args:\r\n        trainer: instance of lightning.pytorch.Trainer\r\n        factor: value which the old batch size is multiplied by to get the\r\n            new batch size\r\n        value: if a value is given, will override the batch size with this value.\r\n            Note that the value of `factor` will not have an effect in this case\r\n        desc: either ``\"succeeded\"`` or ``\"failed\"``. Used purely for logging\r\n\r\n    Returns:\r\n        The new batch size for the next trial and a bool that signals whether the\r\n        new value is different than the previous batch size.\r\n\r\n    \"\"\"\r\n    model = trainer.lightning_module\r\n    batch_size = lightning_getattr(model, batch_arg_name)\r\n    assert batch_size is not None\r\n\r\n    loop = trainer._active_loop\r\n    assert loop is not None\r\n    loop.setup_data()\r\n    combined_loader = loop._combined_loader\r\n    assert combined_loader is not None\r\n    try:\r\n        combined_dataset_length = combined_loader._dataset_length()\r\n        if batch_size >= combined_dataset_length:\r\n            rank_zero_info(f\"The batch size {batch_size} is greater or equal than the length of your dataset.\")\r\n            return batch_size, False\r\n    except NotImplementedError:\r\n        # all datasets are iterable style\r\n        pass\r\n\r\n    new_size = value if value is not None else int(batch_size * factor)\r\n    if desc:\r\n        rank_zero_info(f\"Batch size {batch_size} {desc}, trying batch size {new_size}\")\r\n    changed = new_size != batch_size\r\n    lightning_setattr(model, batch_arg_name, new_size)\r\n    return new_size, changed", "code_tokens": ["def", "_adjust_batch_size", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "batch_arg_name", ":", "str", "=", "\"", "batch_size", "\"", ",", "factor", ":", "float", "=", "1", ".", "0", ",", "value", ":", "Optional", "[", "int", "]", "=", "None", ",", "desc", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "tuple", "[", "int", ",", "bool", "]", ":", "\"", "\"", "\"", "Helper", "function", "for", "adjusting", "the", "batch", "size", ".", "Args", ":", "trainer", ":", "instance", "of", "lightning", ".", "pytorch", ".", "Trainer", "factor", ":", "value", "which", "the", "old", "batch", "size", "is", "multiplied", "by", "to", "get", "the", "new", "batch", "size", "value", ":", "if", "a", "value", "is", "given", ",", "will", "override", "the", "batch", "size", "with", "this", "value", ".", "Note", "that", "the", "value", "of", "`", "factor", "`", "will", "not", "have", "an", "effect", "in", "this", "case", "desc", ":", "either", "`", "`", "\"", "succeeded", "\"", "`", "`", "or", "`", "`", "\"", "failed", "\"", "`", "`", ".", "Used", "purely", "for", "logging", "Returns", ":", "The", "new", "batch", "size", "for", "the", "next", "trial", "and", "a", "bool", "that", "signals", "whether", "the", "new", "value", "is", "different", "than", "the", "previous", "batch", "size", ".", "\"", "\"", "\"", "model", "=", "trainer", ".", "lightning_module", "batch_size", "=", "lightning_getattr", "(", "model", ",", "batch_arg_name", ")", "assert", "batch_size", "is", "not", "None", "loop", "=", "trainer", ".", "_active_loop", "assert", "loop", "is", "not", "None", "loop", ".", "setup_data", "(", ")", "combined_loader", "=", "loop", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "try", ":", "combined_dataset_length", "=", "combined_loader", ".", "_dataset_length", "(", ")", "if", "batch_size", ">", "=", "combined_dataset_length", ":", "rank_zero_info", "(", "f", "\"", "The", "batch", "size", "{", "batch_size", "}", "is", "greater", "or", "equal", "than", "the", "length", "of", "your", "dataset", ".", "\"", ")", "return", "batch_size", ",", "False", "except", "NotImplementedError", ":", "pass", "new_size", "=", "value", "if", "value", "is", "not", "None", "else", "int", "(", "batch_size", "*", "factor", ")", "if", "desc", ":", "rank_zero_info", "(", "f", "\"", "Batch", "size", "{", "batch_size", "}", "{", "desc", "}", ",", "trying", "batch", "size", "{", "new_size", "}", "\"", ")", "changed", "=", "new_size", "!", "=", "batch_size", "lightning_setattr", "(", "model", ",", "batch_arg_name", ",", "new_size", ")", "return", "new_size", ",", "changed"], "docstring": "Helper function for adjusting the batch size.\r\n\r\n    Args:\r\n        trainer: instance of lightning.pytorch.Trainer\r\n        factor: value which the old batch size is multiplied by to get the\r\n            new batch size\r\n        value: if a value is given, will override the batch size with this value.\r\n            Note that the value of `factor` will not have an effect in this case\r\n        desc: either ``\"succeeded\"`` or ``\"failed\"``. Used purely for logging\r\n\r\n    Returns:\r\n        The new batch size for the next trial and a bool that signals whether the\r\n        new value is different than the previous batch size.", "docstring_tokens": ["helper", "function", "for", "adjusting", "the", "batch", "size", "args", "trainer", "instance", "of", "lightning", "pytorch", "trainer", "factor", "value", "which", "the", "old", "batch", "size", "is", "multiplied", "by", "to", "get", "the", "new", "batch", "size", "value", "if", "a", "value", "is", "given", "will", "override", "the", "batch", "size", "with", "this", "value", "note", "that", "the", "value", "of", "factor", "will", "not", "have", "an", "effect", "in", "this", "case", "desc", "either", "succeeded", "or", "failed", "used", "purely", "for", "logging", "returns", "the", "new", "batch", "size", "for", "the", "next", "trial", "and", "a", "bool", "that", "signals", "whether", "the", "new", "value", "is", "different", "than", "the", "previous", "batch", "size"], "docstring_summary": "Helper function for adjusting the batch size.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\batch_size_scaling.py", "partition": "test", "function_type": "function", "start_line": 289, "end_line": 334, "hash": "9b8283765299e8cf62737b24e2515438", "complexity": 5, "parameters": ["trainer", "batch_arg_name", "factor", "value", "desc"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "_exchange_scheduler", "original_string": "def _exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\r\n        # TODO: update docs here\r\n        \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\r\n        optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\r\n        from lightning.pytorch.core.optimizer import _validate_optimizers_attached\r\n\r\n        optimizers = trainer.strategy.optimizers\r\n\r\n        if len(optimizers) != 1:\r\n            raise MisconfigurationException(\r\n                f\"`model.configure_optimizers()` returned {len(optimizers)}, but\"\r\n                \" learning rate finder only works with single optimizer\"\r\n            )\r\n\r\n        optimizer = optimizers[0]\r\n\r\n        new_lrs = [self.lr_min] * len(optimizer.param_groups)\r\n        for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\r\n            param_group[\"lr\"] = new_lr\r\n            param_group[\"initial_lr\"] = new_lr\r\n\r\n        args = (optimizer, self.lr_max, self.num_training)\r\n        scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\r\n\r\n        trainer.strategy.optimizers = [optimizer]\r\n        trainer.strategy.lr_scheduler_configs = [LRSchedulerConfig(scheduler, interval=\"step\")]\r\n        _validate_optimizers_attached(trainer.optimizers, trainer.lr_scheduler_configs)", "language": "python", "code": "def _exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\r\n        # TODO: update docs here\r\n        \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\r\n        optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\r\n        from lightning.pytorch.core.optimizer import _validate_optimizers_attached\r\n\r\n        optimizers = trainer.strategy.optimizers\r\n\r\n        if len(optimizers) != 1:\r\n            raise MisconfigurationException(\r\n                f\"`model.configure_optimizers()` returned {len(optimizers)}, but\"\r\n                \" learning rate finder only works with single optimizer\"\r\n            )\r\n\r\n        optimizer = optimizers[0]\r\n\r\n        new_lrs = [self.lr_min] * len(optimizer.param_groups)\r\n        for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\r\n            param_group[\"lr\"] = new_lr\r\n            param_group[\"initial_lr\"] = new_lr\r\n\r\n        args = (optimizer, self.lr_max, self.num_training)\r\n        scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\r\n\r\n        trainer.strategy.optimizers = [optimizer]\r\n        trainer.strategy.lr_scheduler_configs = [LRSchedulerConfig(scheduler, interval=\"step\")]\r\n        _validate_optimizers_attached(trainer.optimizers, trainer.lr_scheduler_configs)", "code_tokens": ["def", "_exchange_scheduler", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Decorate", "`", "trainer", ".", "strategy", ".", "setup_optimizers", "`", "method", "such", "that", "it", "sets", "the", "user", "'", "s", "originally", "specified", "optimizer", "together", "with", "a", "new", "scheduler", "that", "takes", "care", "of", "the", "learning", "rate", "search", ".", "\"", "\"", "\"", "from", "lightning", ".", "pytorch", ".", "core", ".", "optimizer", "import", "_validate_optimizers_attached", "optimizers", "=", "trainer", ".", "strategy", ".", "optimizers", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "MisconfigurationException", "(", "f", "\"", "`", "model", ".", "configure_optimizers", "(", ")", "`", "returned", "{", "len", "(", "optimizers", ")", "}", ",", "but", "\"", "\"", "learning", "rate", "finder", "only", "works", "with", "single", "optimizer", "\"", ")", "optimizer", "=", "optimizers", "[", "0", "]", "new_lrs", "=", "[", "self", ".", "lr_min", "]", "*", "len", "(", "optimizer", ".", "param_groups", ")", "for", "param_group", ",", "new_lr", "in", "zip", "(", "optimizer", ".", "param_groups", ",", "new_lrs", ")", ":", "param_group", "[", "\"", "lr", "\"", "]", "=", "new_lr", "param_group", "[", "\"", "initial_lr", "\"", "]", "=", "new_lr", "args", "=", "(", "optimizer", ",", "self", ".", "lr_max", ",", "self", ".", "num_training", ")", "scheduler", "=", "_LinearLR", "(", "*", "args", ")", "if", "self", ".", "mode", "=", "=", "\"", "linear", "\"", "else", "_ExponentialLR", "(", "*", "args", ")", "trainer", ".", "strategy", ".", "optimizers", "=", "[", "optimizer", "]", "trainer", ".", "strategy", ".", "lr_scheduler_configs", "=", "[", "LRSchedulerConfig", "(", "scheduler", ",", "interval", "=", "\"", "step", "\"", ")", "]", "_validate_optimizers_attached", "(", "trainer", ".", "optimizers", ",", "trainer", ".", "lr_scheduler_configs", ")"], "docstring": "Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\r\n        optimizer together with a new scheduler that takes care of the learning rate search.", "docstring_tokens": ["decorate", "trainer", "strategy", "setup_optimizers", "method", "such", "that", "it", "sets", "the", "user", "s", "originally", "specified", "optimizer", "together", "with", "a", "new", "scheduler", "that", "takes", "care", "of", "the", "learning", "rate", "search"], "docstring_summary": "Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "class_method", "class_name": "_LRFinder", "start_line": 90, "end_line": 116, "hash": "27a0495aeaf596b553cbc10b61f0898c", "complexity": 4, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "plot", "original_string": "def plot(\r\n        self, suggest: bool = False, show: bool = False, ax: Optional[\"Axes\"] = None\r\n    ) -> Optional[Union[\"plt.Figure\", \"plt.SubFigure\"]]:\r\n        \"\"\"Plot results from lr_find run\r\n        Args:\r\n            suggest: if True, will mark suggested lr to use with a red point\r\n            show: if True, will show figure\r\n            ax: Axes object to which the plot is to be drawn. If not provided, a new figure is created.\r\n\r\n        \"\"\"\r\n        if not _MATPLOTLIB_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `plot` method, you must have Matplotlib installed.\"\r\n                \" Install it by running `pip install -U matplotlib`.\"\r\n            )\r\n        import matplotlib.pyplot as plt\r\n\r\n        lrs = self.results[\"lr\"]\r\n        losses = self.results[\"loss\"]\r\n\r\n        fig: Optional[Union[plt.Figure, plt.SubFigure]]\r\n        if ax is None:\r\n            fig, ax = plt.subplots()\r\n        else:\r\n            fig = ax.figure\r\n\r\n        # Plot loss as a function of the learning rate\r\n        ax.plot(lrs, losses)\r\n        if self.mode == \"exponential\":\r\n            ax.set_xscale(\"log\")\r\n        ax.set_xlabel(\"Learning rate\")\r\n        ax.set_ylabel(\"Loss\")\r\n\r\n        if suggest:\r\n            _ = self.suggestion()\r\n            if self._optimal_idx:\r\n                ax.plot(lrs[self._optimal_idx], losses[self._optimal_idx], markersize=10, marker=\"o\", color=\"red\")\r\n\r\n        if show:\r\n            plt.show()\r\n\r\n        return fig", "language": "python", "code": "def plot(\r\n        self, suggest: bool = False, show: bool = False, ax: Optional[\"Axes\"] = None\r\n    ) -> Optional[Union[\"plt.Figure\", \"plt.SubFigure\"]]:\r\n        \"\"\"Plot results from lr_find run\r\n        Args:\r\n            suggest: if True, will mark suggested lr to use with a red point\r\n            show: if True, will show figure\r\n            ax: Axes object to which the plot is to be drawn. If not provided, a new figure is created.\r\n\r\n        \"\"\"\r\n        if not _MATPLOTLIB_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `plot` method, you must have Matplotlib installed.\"\r\n                \" Install it by running `pip install -U matplotlib`.\"\r\n            )\r\n        import matplotlib.pyplot as plt\r\n\r\n        lrs = self.results[\"lr\"]\r\n        losses = self.results[\"loss\"]\r\n\r\n        fig: Optional[Union[plt.Figure, plt.SubFigure]]\r\n        if ax is None:\r\n            fig, ax = plt.subplots()\r\n        else:\r\n            fig = ax.figure\r\n\r\n        # Plot loss as a function of the learning rate\r\n        ax.plot(lrs, losses)\r\n        if self.mode == \"exponential\":\r\n            ax.set_xscale(\"log\")\r\n        ax.set_xlabel(\"Learning rate\")\r\n        ax.set_ylabel(\"Loss\")\r\n\r\n        if suggest:\r\n            _ = self.suggestion()\r\n            if self._optimal_idx:\r\n                ax.plot(lrs[self._optimal_idx], losses[self._optimal_idx], markersize=10, marker=\"o\", color=\"red\")\r\n\r\n        if show:\r\n            plt.show()\r\n\r\n        return fig", "code_tokens": ["def", "plot", "(", "self", ",", "suggest", ":", "bool", "=", "False", ",", "show", ":", "bool", "=", "False", ",", "ax", ":", "Optional", "[", "\"", "Axes", "\"", "]", "=", "None", ")", "-", ">", "Optional", "[", "Union", "[", "\"", "plt", ".", "Figure", "\"", ",", "\"", "plt", ".", "SubFigure", "\"", "]", "]", ":", "\"", "\"", "\"", "Plot", "results", "from", "lr_find", "run", "Args", ":", "suggest", ":", "if", "True", ",", "will", "mark", "suggested", "lr", "to", "use", "with", "a", "red", "point", "show", ":", "if", "True", ",", "will", "show", "figure", "ax", ":", "Axes", "object", "to", "which", "the", "plot", "is", "to", "be", "drawn", ".", "If", "not", "provided", ",", "a", "new", "figure", "is", "created", ".", "\"", "\"", "\"", "if", "not", "_MATPLOTLIB_AVAILABLE", ":", "raise", "MisconfigurationException", "(", "\"", "To", "use", "the", "`", "plot", "`", "method", ",", "you", "must", "have", "Matplotlib", "installed", ".", "\"", "\"", "Install", "it", "by", "running", "`", "pip", "install", "-", "U", "matplotlib", "`", ".", "\"", ")", "import", "matplotlib", ".", "pyplot", "as", "plt", "lrs", "=", "self", ".", "results", "[", "\"", "lr", "\"", "]", "losses", "=", "self", ".", "results", "[", "\"", "loss", "\"", "]", "fig", ":", "Optional", "[", "Union", "[", "plt", ".", "Figure", ",", "plt", ".", "SubFigure", "]", "]", "if", "ax", "is", "None", ":", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "else", ":", "fig", "=", "ax", ".", "figure", "ax", ".", "plot", "(", "lrs", ",", "losses", ")", "if", "self", ".", "mode", "=", "=", "\"", "exponential", "\"", ":", "ax", ".", "set_xscale", "(", "\"", "log", "\"", ")", "ax", ".", "set_xlabel", "(", "\"", "Learning", "rate", "\"", ")", "ax", ".", "set_ylabel", "(", "\"", "Loss", "\"", ")", "if", "suggest", ":", "_", "=", "self", ".", "suggestion", "(", ")", "if", "self", ".", "_optimal_idx", ":", "ax", ".", "plot", "(", "lrs", "[", "self", ".", "_optimal_idx", "]", ",", "losses", "[", "self", ".", "_optimal_idx", "]", ",", "markersize", "=", "10", ",", "marker", "=", "\"", "o", "\"", ",", "color", "=", "\"", "red", "\"", ")", "if", "show", ":", "plt", ".", "show", "(", ")", "return", "fig"], "docstring": "Plot results from lr_find run\r\n        Args:\r\n            suggest: if True, will mark suggested lr to use with a red point\r\n            show: if True, will show figure\r\n            ax: Axes object to which the plot is to be drawn. If not provided, a new figure is created.", "docstring_tokens": ["plot", "results", "from", "lr_find", "run", "args", "suggest", "if", "true", "will", "mark", "suggested", "lr", "to", "use", "with", "a", "red", "point", "show", "if", "true", "will", "show", "figure", "ax", "axes", "object", "to", "which", "the", "plot", "is", "to", "be", "drawn", "if", "not", "provided", "a", "new", "figure", "is", "created"], "docstring_summary": "Plot results from lr_find run", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "class_method", "class_name": "_LRFinder", "start_line": 118, "end_line": 159, "hash": "ca0aecbe1dc23bd5e89f8f47c2b96c86", "complexity": 7, "parameters": ["suggest", "show", "ax"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "suggestion", "original_string": "def suggestion(self, skip_begin: int = 10, skip_end: int = 1) -> Optional[float]:\r\n        \"\"\"This will propose a suggestion for an initial learning rate based on the point with the steepest negative\r\n        gradient.\r\n\r\n        Args:\r\n            skip_begin: how many samples to skip in the beginning; helps to avoid too naive estimates\r\n            skip_end: how many samples to skip in the end; helps to avoid too optimistic estimates\r\n\r\n        Returns:\r\n            The suggested initial learning rate to use, or `None` if a suggestion is not possible due to too few\r\n            loss samples.\r\n\r\n        \"\"\"\r\n        losses = torch.tensor(self.results[\"loss\"][skip_begin:-skip_end])\r\n        lrs = torch.tensor(self.results[\"lr\"][skip_begin:-skip_end])\r\n        is_finite = torch.isfinite(losses)\r\n        losses = losses[is_finite]\r\n        lrs = lrs[is_finite]\r\n\r\n        if len(losses) < 2:\r\n            # computing torch.gradient requires at least 2 points\r\n            log.error(\r\n                \"Failed to compute suggestion for learning rate because there are not enough points. Increase the loop\"\r\n                \" iteration limits or the size of your dataset/dataloader.\"\r\n            )\r\n            self._optimal_idx = None\r\n            return None\r\n\r\n        gradients = torch.gradient(losses, spacing=[lrs])[0]  # Compute the gradient of losses w.r.t. learning rates\r\n        min_grad = torch.argmin(gradients).item()\r\n        all_losses_idx = torch.arange(len(self.results[\"loss\"]))\r\n        idx_non_skipped = all_losses_idx[skip_begin:-skip_end]\r\n        idx_finite = idx_non_skipped[is_finite]\r\n        self._optimal_idx = idx_finite[min_grad].item()  # type: ignore\r\n        return self.results[\"lr\"][self._optimal_idx]", "language": "python", "code": "def suggestion(self, skip_begin: int = 10, skip_end: int = 1) -> Optional[float]:\r\n        \"\"\"This will propose a suggestion for an initial learning rate based on the point with the steepest negative\r\n        gradient.\r\n\r\n        Args:\r\n            skip_begin: how many samples to skip in the beginning; helps to avoid too naive estimates\r\n            skip_end: how many samples to skip in the end; helps to avoid too optimistic estimates\r\n\r\n        Returns:\r\n            The suggested initial learning rate to use, or `None` if a suggestion is not possible due to too few\r\n            loss samples.\r\n\r\n        \"\"\"\r\n        losses = torch.tensor(self.results[\"loss\"][skip_begin:-skip_end])\r\n        lrs = torch.tensor(self.results[\"lr\"][skip_begin:-skip_end])\r\n        is_finite = torch.isfinite(losses)\r\n        losses = losses[is_finite]\r\n        lrs = lrs[is_finite]\r\n\r\n        if len(losses) < 2:\r\n            # computing torch.gradient requires at least 2 points\r\n            log.error(\r\n                \"Failed to compute suggestion for learning rate because there are not enough points. Increase the loop\"\r\n                \" iteration limits or the size of your dataset/dataloader.\"\r\n            )\r\n            self._optimal_idx = None\r\n            return None\r\n\r\n        gradients = torch.gradient(losses, spacing=[lrs])[0]  # Compute the gradient of losses w.r.t. learning rates\r\n        min_grad = torch.argmin(gradients).item()\r\n        all_losses_idx = torch.arange(len(self.results[\"loss\"]))\r\n        idx_non_skipped = all_losses_idx[skip_begin:-skip_end]\r\n        idx_finite = idx_non_skipped[is_finite]\r\n        self._optimal_idx = idx_finite[min_grad].item()  # type: ignore\r\n        return self.results[\"lr\"][self._optimal_idx]", "code_tokens": ["def", "suggestion", "(", "self", ",", "skip_begin", ":", "int", "=", "10", ",", "skip_end", ":", "int", "=", "1", ")", "-", ">", "Optional", "[", "float", "]", ":", "\"", "\"", "\"", "This", "will", "propose", "a", "suggestion", "for", "an", "initial", "learning", "rate", "based", "on", "the", "point", "with", "the", "steepest", "negative", "gradient", ".", "Args", ":", "skip_begin", ":", "how", "many", "samples", "to", "skip", "in", "the", "beginning", ";", "helps", "to", "avoid", "too", "naive", "estimates", "skip_end", ":", "how", "many", "samples", "to", "skip", "in", "the", "end", ";", "helps", "to", "avoid", "too", "optimistic", "estimates", "Returns", ":", "The", "suggested", "initial", "learning", "rate", "to", "use", ",", "or", "`", "None", "`", "if", "a", "suggestion", "is", "not", "possible", "due", "to", "too", "few", "loss", "samples", ".", "\"", "\"", "\"", "losses", "=", "torch", ".", "tensor", "(", "self", ".", "results", "[", "\"", "loss", "\"", "]", "[", "skip_begin", ":", "-", "skip_end", "]", ")", "lrs", "=", "torch", ".", "tensor", "(", "self", ".", "results", "[", "\"", "lr", "\"", "]", "[", "skip_begin", ":", "-", "skip_end", "]", ")", "is_finite", "=", "torch", ".", "isfinite", "(", "losses", ")", "losses", "=", "losses", "[", "is_finite", "]", "lrs", "=", "lrs", "[", "is_finite", "]", "if", "len", "(", "losses", ")", "<", "2", ":", "log", ".", "error", "(", "\"", "Failed", "to", "compute", "suggestion", "for", "learning", "rate", "because", "there", "are", "not", "enough", "points", ".", "Increase", "the", "loop", "\"", "\"", "iteration", "limits", "or", "the", "size", "of", "your", "dataset", "/", "dataloader", ".", "\"", ")", "self", ".", "_optimal_idx", "=", "None", "return", "None", "gradients", "=", "torch", ".", "gradient", "(", "losses", ",", "spacing", "=", "[", "lrs", "]", ")", "[", "0", "]", "min_grad", "=", "torch", ".", "argmin", "(", "gradients", ")", ".", "item", "(", ")", "all_losses_idx", "=", "torch", ".", "arange", "(", "len", "(", "self", ".", "results", "[", "\"", "loss", "\"", "]", ")", ")", "idx_non_skipped", "=", "all_losses_idx", "[", "skip_begin", ":", "-", "skip_end", "]", "idx_finite", "=", "idx_non_skipped", "[", "is_finite", "]", "self", ".", "_optimal_idx", "=", "idx_finite", "[", "min_grad", "]", ".", "item", "(", ")", "return", "self", ".", "results", "[", "\"", "lr", "\"", "]", "[", "self", ".", "_optimal_idx", "]"], "docstring": "This will propose a suggestion for an initial learning rate based on the point with the steepest negative\r\n        gradient.\r\n\r\n        Args:\r\n            skip_begin: how many samples to skip in the beginning; helps to avoid too naive estimates\r\n            skip_end: how many samples to skip in the end; helps to avoid too optimistic estimates\r\n\r\n        Returns:\r\n            The suggested initial learning rate to use, or `None` if a suggestion is not possible due to too few\r\n            loss samples.", "docstring_tokens": ["this", "will", "propose", "a", "suggestion", "for", "an", "initial", "learning", "rate", "based", "on", "the", "point", "with", "the", "steepest", "negative", "gradient", "args", "skip_begin", "how", "many", "samples", "to", "skip", "in", "the", "beginning", "helps", "to", "avoid", "too", "naive", "estimates", "skip_end", "how", "many", "samples", "to", "skip", "in", "the", "end", "helps", "to", "avoid", "too", "optimistic", "estimates", "returns", "the", "suggested", "initial", "learning", "rate", "to", "use", "or", "none", "if", "a", "suggestion", "is", "not", "possible", "due", "to", "too", "few", "loss", "samples"], "docstring_summary": "This will propose a suggestion for an initial learning rate based on the point with the steepest negative", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "class_method", "class_name": "_LRFinder", "start_line": 161, "end_line": 195, "hash": "94160ff8416720f8b761c8f179f67586", "complexity": 2, "parameters": ["skip_begin", "skip_end"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "_lr_find", "original_string": "def _lr_find(\r\n    trainer: \"pl.Trainer\",\r\n    model: \"pl.LightningModule\",\r\n    min_lr: float = 1e-8,\r\n    max_lr: float = 1,\r\n    num_training: int = 100,\r\n    mode: str = \"exponential\",\r\n    early_stop_threshold: Optional[float] = 4.0,\r\n    update_attr: bool = False,\r\n    attr_name: str = \"\",\r\n) -> Optional[_LRFinder]:\r\n    \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking\r\n    a good starting learning rate.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        model: Model to tune.\r\n        min_lr: minimum learning rate to investigate\r\n        max_lr: maximum learning rate to investigate\r\n        num_training: number of learning rates to test\r\n        mode: Search strategy to update learning rate after each batch:\r\n\r\n            - ``'exponential'``: Increases the learning rate exponentially.\r\n            - ``'linear'``: Increases the learning rate linearly.\r\n\r\n        early_stop_threshold: Threshold for stopping the search. If the\r\n            loss at any point is larger than early_stop_threshold*best_loss\r\n            then the search is stopped. To disable, set to None.\r\n        update_attr: Whether to update the learning rate attribute or not.\r\n        attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n            automatically detected. Otherwise, set the name here.\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping learning rate finder since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    # Determine lr attr\r\n    if update_attr:\r\n        attr_name = _determine_lr_attr_name(model, attr_name)\r\n\r\n    # Save initial model, that is loaded after learning rate is found\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".lr_find_{uuid.uuid4()}.ckpt\")\r\n    ckpt_path = trainer.strategy.broadcast(ckpt_path)\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    start_steps = trainer.global_step\r\n\r\n    # Arguments we adjust during the lr finder, save for restoring\r\n    params = __lr_finder_dump_params(trainer)\r\n\r\n    # Set to values that are required by the algorithm\r\n    __lr_finder_reset_params(trainer, num_training, early_stop_threshold)\r\n\r\n    # Disable standard progress bar for fit\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    # Initialize lr finder object (stores results)\r\n    lr_finder = _LRFinder(mode, min_lr, max_lr, num_training)\r\n\r\n    lr_finder_finished = False\r\n    try:\r\n        # Configure optimizer and scheduler\r\n        lr_finder._exchange_scheduler(trainer)\r\n\r\n        # Fit, lr & loss logged in callback\r\n        _try_loop_run(trainer, params)\r\n\r\n        # Prompt if we stopped early\r\n        if trainer.global_step != num_training + start_steps:\r\n            log.info(f\"LR finder stopped early after {trainer.global_step} steps due to diverging loss.\")\r\n\r\n        # Transfer results from callback to lr finder object\r\n        lr_finder.results.update({\"lr\": trainer.callbacks[0].lrs, \"loss\": trainer.callbacks[0].losses})\r\n        lr_finder._total_batch_idx = trainer.fit_loop.total_batch_idx  # for debug purpose\r\n\r\n        __lr_finder_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        # Update results across ranks\r\n        lr_finder.results = trainer.strategy.broadcast(lr_finder.results)\r\n        lr_finder_finished = True\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        # Restore initial state of model (this will also restore the original optimizer state)\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n        trainer.fit_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.val_loop._combined_loader = None\r\n        trainer.fit_loop._combined_loader = None  # reset data fetcher to avoid issues with the next fit\r\n        trainer.fit_loop.setup_data()\r\n\r\n    # Apply LR suggestion after restoring so it persists for the real training run\r\n    # When used as a callback, the suggestion would otherwise be lost due to checkpoint restore\r\n    if update_attr and lr_finder_finished:\r\n        lr = lr_finder.suggestion()\r\n        if lr is not None:\r\n            # update the attribute on the LightningModule (e.g., lr or learning_rate)\r\n            lightning_setattr(model, attr_name, lr)\r\n            # also update the currently active optimizer(s) so training continues with the suggested LR\r\n            for opt in trainer.optimizers or []:\r\n                for pg in opt.param_groups:\r\n                    pg[\"lr\"] = lr\r\n            log.info(f\"Learning rate set to {lr}\")\r\n    return lr_finder", "language": "python", "code": "def _lr_find(\r\n    trainer: \"pl.Trainer\",\r\n    model: \"pl.LightningModule\",\r\n    min_lr: float = 1e-8,\r\n    max_lr: float = 1,\r\n    num_training: int = 100,\r\n    mode: str = \"exponential\",\r\n    early_stop_threshold: Optional[float] = 4.0,\r\n    update_attr: bool = False,\r\n    attr_name: str = \"\",\r\n) -> Optional[_LRFinder]:\r\n    \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking\r\n    a good starting learning rate.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        model: Model to tune.\r\n        min_lr: minimum learning rate to investigate\r\n        max_lr: maximum learning rate to investigate\r\n        num_training: number of learning rates to test\r\n        mode: Search strategy to update learning rate after each batch:\r\n\r\n            - ``'exponential'``: Increases the learning rate exponentially.\r\n            - ``'linear'``: Increases the learning rate linearly.\r\n\r\n        early_stop_threshold: Threshold for stopping the search. If the\r\n            loss at any point is larger than early_stop_threshold*best_loss\r\n            then the search is stopped. To disable, set to None.\r\n        update_attr: Whether to update the learning rate attribute or not.\r\n        attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n            automatically detected. Otherwise, set the name here.\r\n\r\n    \"\"\"\r\n    if trainer.fast_dev_run:\r\n        rank_zero_warn(\"Skipping learning rate finder since `fast_dev_run` is enabled.\")\r\n        return None\r\n\r\n    # Determine lr attr\r\n    if update_attr:\r\n        attr_name = _determine_lr_attr_name(model, attr_name)\r\n\r\n    # Save initial model, that is loaded after learning rate is found\r\n    ckpt_path = os.path.join(trainer.default_root_dir, f\".lr_find_{uuid.uuid4()}.ckpt\")\r\n    ckpt_path = trainer.strategy.broadcast(ckpt_path)\r\n    trainer.save_checkpoint(ckpt_path)\r\n\r\n    start_steps = trainer.global_step\r\n\r\n    # Arguments we adjust during the lr finder, save for restoring\r\n    params = __lr_finder_dump_params(trainer)\r\n\r\n    # Set to values that are required by the algorithm\r\n    __lr_finder_reset_params(trainer, num_training, early_stop_threshold)\r\n\r\n    # Disable standard progress bar for fit\r\n    if trainer.progress_bar_callback:\r\n        trainer.progress_bar_callback.disable()\r\n\r\n    # Initialize lr finder object (stores results)\r\n    lr_finder = _LRFinder(mode, min_lr, max_lr, num_training)\r\n\r\n    lr_finder_finished = False\r\n    try:\r\n        # Configure optimizer and scheduler\r\n        lr_finder._exchange_scheduler(trainer)\r\n\r\n        # Fit, lr & loss logged in callback\r\n        _try_loop_run(trainer, params)\r\n\r\n        # Prompt if we stopped early\r\n        if trainer.global_step != num_training + start_steps:\r\n            log.info(f\"LR finder stopped early after {trainer.global_step} steps due to diverging loss.\")\r\n\r\n        # Transfer results from callback to lr finder object\r\n        lr_finder.results.update({\"lr\": trainer.callbacks[0].lrs, \"loss\": trainer.callbacks[0].losses})\r\n        lr_finder._total_batch_idx = trainer.fit_loop.total_batch_idx  # for debug purpose\r\n\r\n        __lr_finder_restore_params(trainer, params)\r\n\r\n        if trainer.progress_bar_callback:\r\n            trainer.progress_bar_callback.enable()\r\n\r\n        # Update results across ranks\r\n        lr_finder.results = trainer.strategy.broadcast(lr_finder.results)\r\n        lr_finder_finished = True\r\n    except Exception as ex:\r\n        raise ex\r\n    finally:\r\n        # Restore initial state of model (this will also restore the original optimizer state)\r\n        trainer._checkpoint_connector.restore(ckpt_path)\r\n        trainer.strategy.remove_checkpoint(ckpt_path)\r\n        trainer.fit_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.restarting = False  # reset restarting flag as checkpoint restoring sets it to True\r\n        trainer.fit_loop.epoch_loop.val_loop._combined_loader = None\r\n        trainer.fit_loop._combined_loader = None  # reset data fetcher to avoid issues with the next fit\r\n        trainer.fit_loop.setup_data()\r\n\r\n    # Apply LR suggestion after restoring so it persists for the real training run\r\n    # When used as a callback, the suggestion would otherwise be lost due to checkpoint restore\r\n    if update_attr and lr_finder_finished:\r\n        lr = lr_finder.suggestion()\r\n        if lr is not None:\r\n            # update the attribute on the LightningModule (e.g., lr or learning_rate)\r\n            lightning_setattr(model, attr_name, lr)\r\n            # also update the currently active optimizer(s) so training continues with the suggested LR\r\n            for opt in trainer.optimizers or []:\r\n                for pg in opt.param_groups:\r\n                    pg[\"lr\"] = lr\r\n            log.info(f\"Learning rate set to {lr}\")\r\n    return lr_finder", "code_tokens": ["def", "_lr_find", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "min_lr", ":", "float", "=", "1e", "-", "8", ",", "max_lr", ":", "float", "=", "1", ",", "num_training", ":", "int", "=", "100", ",", "mode", ":", "str", "=", "\"", "exponential", "\"", ",", "early_stop_threshold", ":", "Optional", "[", "float", "]", "=", "4", ".", "0", ",", "update_attr", ":", "bool", "=", "False", ",", "attr_name", ":", "str", "=", "\"", "\"", ",", ")", "-", ">", "Optional", "[", "_LRFinder", "]", ":", "\"", "\"", "\"", "Enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", ",", "to", "reduce", "the", "amount", "of", "guesswork", "in", "picking", "a", "good", "starting", "learning", "rate", ".", "Args", ":", "trainer", ":", "A", "Trainer", "instance", ".", "model", ":", "Model", "to", "tune", ".", "min_lr", ":", "minimum", "learning", "rate", "to", "investigate", "max_lr", ":", "maximum", "learning", "rate", "to", "investigate", "num_training", ":", "number", "of", "learning", "rates", "to", "test", "mode", ":", "Search", "strategy", "to", "update", "learning", "rate", "after", "each", "batch", ":", "-", "`", "`", "'", "exponential", "'", "`", "`", ":", "Increases", "the", "learning", "rate", "exponentially", ".", "-", "`", "`", "'", "linear", "'", "`", "`", ":", "Increases", "the", "learning", "rate", "linearly", ".", "early_stop_threshold", ":", "Threshold", "for", "stopping", "the", "search", ".", "If", "the", "loss", "at", "any", "point", "is", "larger", "than", "early_stop_threshold", "*", "best_loss", "then", "the", "search", "is", "stopped", ".", "To", "disable", ",", "set", "to", "None", ".", "update_attr", ":", "Whether", "to", "update", "the", "learning", "rate", "attribute", "or", "not", ".", "attr_name", ":", "Name", "of", "the", "attribute", "which", "stores", "the", "learning", "rate", ".", "The", "names", "'", "learning_rate", "'", "or", "'", "lr", "'", "get", "automatically", "detected", ".", "Otherwise", ",", "set", "the", "name", "here", ".", "\"", "\"", "\"", "if", "trainer", ".", "fast_dev_run", ":", "rank_zero_warn", "(", "\"", "Skipping", "learning", "rate", "finder", "since", "`", "fast_dev_run", "`", "is", "enabled", ".", "\"", ")", "return", "None", "if", "update_attr", ":", "attr_name", "=", "_determine_lr_attr_name", "(", "model", ",", "attr_name", ")", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "f", "\"", ".", "lr_find_", "{", "uuid", ".", "uuid4", "(", ")", "}", ".", "ckpt", "\"", ")", "ckpt_path", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "ckpt_path", ")", "trainer", ".", "save_checkpoint", "(", "ckpt_path", ")", "start_steps", "=", "trainer", ".", "global_step", "params", "=", "__lr_finder_dump_params", "(", "trainer", ")", "__lr_finder_reset_params", "(", "trainer", ",", "num_training", ",", "early_stop_threshold", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "disable", "(", ")", "lr_finder", "=", "_LRFinder", "(", "mode", ",", "min_lr", ",", "max_lr", ",", "num_training", ")", "lr_finder_finished", "=", "False", "try", ":", "lr_finder", ".", "_exchange_scheduler", "(", "trainer", ")", "_try_loop_run", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "global_step", "!", "=", "num_training", "+", "start_steps", ":", "log", ".", "info", "(", "f", "\"", "LR", "finder", "stopped", "early", "after", "{", "trainer", ".", "global_step", "}", "steps", "due", "to", "diverging", "loss", ".", "\"", ")", "lr_finder", ".", "results", ".", "update", "(", "{", "\"", "lr", "\"", ":", "trainer", ".", "callbacks", "[", "0", "]", ".", "lrs", ",", "\"", "loss", "\"", ":", "trainer", ".", "callbacks", "[", "0", "]", ".", "losses", "}", ")", "lr_finder", ".", "_total_batch_idx", "=", "trainer", ".", "fit_loop", ".", "total_batch_idx", "__lr_finder_restore_params", "(", "trainer", ",", "params", ")", "if", "trainer", ".", "progress_bar_callback", ":", "trainer", ".", "progress_bar_callback", ".", "enable", "(", ")", "lr_finder", ".", "results", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "lr_finder", ".", "results", ")", "lr_finder_finished", "=", "True", "except", "Exception", "as", "ex", ":", "raise", "ex", "finally", ":", "trainer", ".", "_checkpoint_connector", ".", "restore", "(", "ckpt_path", ")", "trainer", ".", "strategy", ".", "remove_checkpoint", "(", "ckpt_path", ")", "trainer", ".", "fit_loop", ".", "restarting", "=", "False", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "restarting", "=", "False", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "val_loop", ".", "_combined_loader", "=", "None", "trainer", ".", "fit_loop", ".", "_combined_loader", "=", "None", "trainer", ".", "fit_loop", ".", "setup_data", "(", ")", "if", "update_attr", "and", "lr_finder_finished", ":", "lr", "=", "lr_finder", ".", "suggestion", "(", ")", "if", "lr", "is", "not", "None", ":", "lightning_setattr", "(", "model", ",", "attr_name", ",", "lr", ")", "for", "opt", "in", "trainer", ".", "optimizers", "or", "[", "]", ":", "for", "pg", "in", "opt", ".", "param_groups", ":", "pg", "[", "\"", "lr", "\"", "]", "=", "lr", "log", ".", "info", "(", "f", "\"", "Learning", "rate", "set", "to", "{", "lr", "}", "\"", ")", "return", "lr_finder"], "docstring": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking\r\n    a good starting learning rate.\r\n\r\n    Args:\r\n        trainer: A Trainer instance.\r\n        model: Model to tune.\r\n        min_lr: minimum learning rate to investigate\r\n        max_lr: maximum learning rate to investigate\r\n        num_training: number of learning rates to test\r\n        mode: Search strategy to update learning rate after each batch:\r\n\r\n            - ``'exponential'``: Increases the learning rate exponentially.\r\n            - ``'linear'``: Increases the learning rate linearly.\r\n\r\n        early_stop_threshold: Threshold for stopping the search. If the\r\n            loss at any point is larger than early_stop_threshold*best_loss\r\n            then the search is stopped. To disable, set to None.\r\n        update_attr: Whether to update the learning rate attribute or not.\r\n        attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n            automatically detected. Otherwise, set the name here.", "docstring_tokens": ["enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", "to", "reduce", "the", "amount", "of", "guesswork", "in", "picking", "a", "good", "starting", "learning", "rate", "args", "trainer", "a", "trainer", "instance", "model", "model", "to", "tune", "min_lr", "minimum", "learning", "rate", "to", "investigate", "max_lr", "maximum", "learning", "rate", "to", "investigate", "num_training", "number", "of", "learning", "rates", "to", "test", "mode", "search", "strategy", "to", "update", "learning", "rate", "after", "each", "batch", "exponential", "increases", "the", "learning", "rate", "exponentially", "linear", "increases", "the", "learning", "rate", "linearly", "early_stop_threshold", "threshold", "for", "stopping", "the", "search", "if", "the", "loss", "at", "any", "point", "is", "larger", "than", "early_stop_threshold", "best_loss", "then", "the", "search", "is", "stopped", "to", "disable", "set", "to", "none", "update_attr", "whether", "to", "update", "the", "learning", "rate", "attribute", "or", "not", "attr_name", "name", "of", "the", "attribute", "which", "stores", "the", "learning", "rate", "the", "names", "learning_rate", "or", "lr", "get", "automatically", "detected", "otherwise", "set", "the", "name", "here"], "docstring_summary": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "function", "start_line": 198, "end_line": 307, "hash": "52ce46c1a2c65d505bcbf2e1efcd1261", "complexity": 13, "parameters": ["trainer", "model", "min_lr", "max_lr", "num_training", "mode", "early_stop_threshold", "update_attr", "attr_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "on_train_batch_start", "original_string": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called before each training batch, logs the lr that will be used.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if self.progress_bar_refresh_rate and self.progress_bar is None:\r\n            self.progress_bar = tqdm(desc=\"Finding best initial lr\", total=self.num_training)\r\n\r\n        self.lrs.append(trainer.lr_scheduler_configs[0].scheduler.lr[0])  # type: ignore[union-attr]\r", "language": "python", "code": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called before each training batch, logs the lr that will be used.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        if self.progress_bar_refresh_rate and self.progress_bar is None:\r\n            self.progress_bar = tqdm(desc=\"Finding best initial lr\", total=self.num_training)\r\n\r\n        self.lrs.append(trainer.lr_scheduler_configs[0].scheduler.lr[0])  # type: ignore[union-attr]\r", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "before", "each", "training", "batch", ",", "logs", "the", "lr", "that", "will", "be", "used", ".", "\"", "\"", "\"", "if", "(", "trainer", ".", "fit_loop", ".", "batch_idx", "+", "1", ")", "%", "trainer", ".", "accumulate_grad_batches", "!", "=", "0", ":", "return", "if", "self", ".", "progress_bar_refresh_rate", "and", "self", ".", "progress_bar", "is", "None", ":", "self", ".", "progress_bar", "=", "tqdm", "(", "desc", "=", "\"", "Finding", "best", "initial", "lr", "\"", ",", "total", "=", "self", ".", "num_training", ")", "self", ".", "lrs", ".", "append", "(", "trainer", ".", "lr_scheduler_configs", "[", "0", "]", ".", "scheduler", ".", "lr", "[", "0", "]", ")"], "docstring": "Called before each training batch, logs the lr that will be used.", "docstring_tokens": ["called", "before", "each", "training", "batch", "logs", "the", "lr", "that", "will", "be", "used"], "docstring_summary": "Called before each training batch, logs the lr that will be used.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "class_method", "class_name": "_LRCallback", "start_line": 384, "end_line": 394, "hash": "16f73f34dab3e69a59f3dd8d9225cce9", "complexity": 4, "parameters": ["trainer", "pl_module", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\lr_finder.py", "func_name": "on_train_batch_end", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the training batch ends, logs the calculated loss.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        # _AutomaticOptimization.run turns None STEP_OUTPUT into an empty dict\r\n        if not outputs:\r\n            # need to add an element, because we also added one element to lrs in on_train_batch_start\r\n            # so add nan, because they are not considered when computing the suggestion\r\n            self.losses.append(float(\"nan\"))\r\n            return\r\n\r\n        if self.progress_bar:\r\n            self.progress_bar.update()\r\n\r\n        loss_tensor = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n        assert loss_tensor is not None\r\n        current_loss = loss_tensor.item()\r\n        current_step = trainer.global_step\r\n\r\n        # Avg loss (loss with momentum) + smoothing\r\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * current_loss\r\n        smoothed_loss = self.avg_loss / (1 - self.beta ** (current_step + 1))\r\n\r\n        # Check if we diverging\r\n        if (\r\n            self.early_stop_threshold is not None\r\n            and current_step > 1\r\n            and smoothed_loss > self.early_stop_threshold * self.best_loss\r\n        ):\r\n            trainer.should_stop = True  # stop signal\r\n            if self.progress_bar:\r\n                self.progress_bar.close()\r\n\r\n        trainer.should_stop = trainer.strategy.broadcast(trainer.should_stop)\r\n\r\n        # Save best loss for diverging checking\r\n        if smoothed_loss < self.best_loss or current_step == 1:\r\n            self.best_loss = smoothed_loss\r\n\r\n        self.losses.append(smoothed_loss)", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the training batch ends, logs the calculated loss.\"\"\"\r\n        if (trainer.fit_loop.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\r\n            return\r\n\r\n        # _AutomaticOptimization.run turns None STEP_OUTPUT into an empty dict\r\n        if not outputs:\r\n            # need to add an element, because we also added one element to lrs in on_train_batch_start\r\n            # so add nan, because they are not considered when computing the suggestion\r\n            self.losses.append(float(\"nan\"))\r\n            return\r\n\r\n        if self.progress_bar:\r\n            self.progress_bar.update()\r\n\r\n        loss_tensor = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n        assert loss_tensor is not None\r\n        current_loss = loss_tensor.item()\r\n        current_step = trainer.global_step\r\n\r\n        # Avg loss (loss with momentum) + smoothing\r\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * current_loss\r\n        smoothed_loss = self.avg_loss / (1 - self.beta ** (current_step + 1))\r\n\r\n        # Check if we diverging\r\n        if (\r\n            self.early_stop_threshold is not None\r\n            and current_step > 1\r\n            and smoothed_loss > self.early_stop_threshold * self.best_loss\r\n        ):\r\n            trainer.should_stop = True  # stop signal\r\n            if self.progress_bar:\r\n                self.progress_bar.close()\r\n\r\n        trainer.should_stop = trainer.strategy.broadcast(trainer.should_stop)\r\n\r\n        # Save best loss for diverging checking\r\n        if smoothed_loss < self.best_loss or current_step == 1:\r\n            self.best_loss = smoothed_loss\r\n\r\n        self.losses.append(smoothed_loss)", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "training", "batch", "ends", ",", "logs", "the", "calculated", "loss", ".", "\"", "\"", "\"", "if", "(", "trainer", ".", "fit_loop", ".", "batch_idx", "+", "1", ")", "%", "trainer", ".", "accumulate_grad_batches", "!", "=", "0", ":", "return", "if", "not", "outputs", ":", "self", ".", "losses", ".", "append", "(", "float", "(", "\"", "nan", "\"", ")", ")", "return", "if", "self", ".", "progress_bar", ":", "self", ".", "progress_bar", ".", "update", "(", ")", "loss_tensor", "=", "outputs", "if", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", "else", "outputs", "[", "\"", "loss", "\"", "]", "assert", "loss_tensor", "is", "not", "None", "current_loss", "=", "loss_tensor", ".", "item", "(", ")", "current_step", "=", "trainer", ".", "global_step", "self", ".", "avg_loss", "=", "self", ".", "beta", "*", "self", ".", "avg_loss", "+", "(", "1", "-", "self", ".", "beta", ")", "*", "current_loss", "smoothed_loss", "=", "self", ".", "avg_loss", "/", "(", "1", "-", "self", ".", "beta", "*", "*", "(", "current_step", "+", "1", ")", ")", "if", "(", "self", ".", "early_stop_threshold", "is", "not", "None", "and", "current_step", ">", "1", "and", "smoothed_loss", ">", "self", ".", "early_stop_threshold", "*", "self", ".", "best_loss", ")", ":", "trainer", ".", "should_stop", "=", "True", "if", "self", ".", "progress_bar", ":", "self", ".", "progress_bar", ".", "close", "(", ")", "trainer", ".", "should_stop", "=", "trainer", ".", "strategy", ".", "broadcast", "(", "trainer", ".", "should_stop", ")", "if", "smoothed_loss", "<", "self", ".", "best_loss", "or", "current_step", "=", "=", "1", ":", "self", ".", "best_loss", "=", "smoothed_loss", "self", ".", "losses", ".", "append", "(", "smoothed_loss", ")"], "docstring": "Called when the training batch ends, logs the calculated loss.", "docstring_tokens": ["called", "when", "the", "training", "batch", "ends", "logs", "the", "calculated", "loss"], "docstring_summary": "Called when the training batch ends, logs the calculated loss.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\lr_finder.py", "partition": "test", "function_type": "class_method", "class_name": "_LRCallback", "start_line": 397, "end_line": 439, "hash": "f428732b0d488c91f32b6b9cea846d63", "complexity": 11, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\tuning.py", "func_name": "scale_batch_size", "original_string": "def scale_batch_size(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        mode: str = \"power\",\r\n        steps_per_trial: int = 3,\r\n        init_val: int = 2,\r\n        max_trials: int = 25,\r\n        batch_arg_name: str = \"batch_size\",\r\n    ) -> Optional[int]:\r\n        \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n        error.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            mode: Search strategy to update the batch size:\r\n\r\n                - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n                - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                    do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n            steps_per_trial: number of steps to run with a given batch size.\r\n                Ideally 1 should be enough to test if an OOM error occurs,\r\n                however in practise a few are needed\r\n            init_val: initial batch size to start the search with\r\n            max_trials: max number of increases in batch size done before\r\n               algorithm is terminated\r\n            batch_arg_name: name of the attribute that stores the batch size.\r\n                It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n                with that name. We will look for this attribute name in the following places\r\n\r\n                - ``model``\r\n                - ``model.hparams``\r\n                - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n        \"\"\"\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_scale_batch_size_configuration(self._trainer)\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\r\n\r\n        batch_size_finder: Callback = BatchSizeFinder(\r\n            mode=mode,\r\n            steps_per_trial=steps_per_trial,\r\n            init_val=init_val,\r\n            max_trials=max_trials,\r\n            batch_arg_name=batch_arg_name,\r\n        )\r\n        # do not continue with the loop in case Tuner is used\r\n        batch_size_finder._early_exit = True\r\n        self._trainer.callbacks = [batch_size_finder] + self._trainer.callbacks\r\n\r\n        if method == \"fit\":\r\n            self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n        elif method == \"validate\":\r\n            self._trainer.validate(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"test\":\r\n            self._trainer.test(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"predict\":\r\n            self._trainer.predict(model, dataloaders, datamodule=datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not batch_size_finder]\r\n        return batch_size_finder.optimal_batch_size", "language": "python", "code": "def scale_batch_size(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        mode: str = \"power\",\r\n        steps_per_trial: int = 3,\r\n        init_val: int = 2,\r\n        max_trials: int = 25,\r\n        batch_arg_name: str = \"batch_size\",\r\n    ) -> Optional[int]:\r\n        \"\"\"Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n        error.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            mode: Search strategy to update the batch size:\r\n\r\n                - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n                - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                    do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n            steps_per_trial: number of steps to run with a given batch size.\r\n                Ideally 1 should be enough to test if an OOM error occurs,\r\n                however in practise a few are needed\r\n            init_val: initial batch size to start the search with\r\n            max_trials: max number of increases in batch size done before\r\n               algorithm is terminated\r\n            batch_arg_name: name of the attribute that stores the batch size.\r\n                It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n                with that name. We will look for this attribute name in the following places\r\n\r\n                - ``model``\r\n                - ``model.hparams``\r\n                - ``trainer.datamodule`` (the datamodule passed to the tune method)\r\n\r\n        \"\"\"\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_scale_batch_size_configuration(self._trainer)\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\r\n\r\n        batch_size_finder: Callback = BatchSizeFinder(\r\n            mode=mode,\r\n            steps_per_trial=steps_per_trial,\r\n            init_val=init_val,\r\n            max_trials=max_trials,\r\n            batch_arg_name=batch_arg_name,\r\n        )\r\n        # do not continue with the loop in case Tuner is used\r\n        batch_size_finder._early_exit = True\r\n        self._trainer.callbacks = [batch_size_finder] + self._trainer.callbacks\r\n\r\n        if method == \"fit\":\r\n            self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n        elif method == \"validate\":\r\n            self._trainer.validate(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"test\":\r\n            self._trainer.test(model, dataloaders, datamodule=datamodule)\r\n        elif method == \"predict\":\r\n            self._trainer.predict(model, dataloaders, datamodule=datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not batch_size_finder]\r\n        return batch_size_finder.optimal_batch_size", "code_tokens": ["def", "scale_batch_size", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "\"", "pl", ".", "LightningDataModule", "\"", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "\"", "pl", ".", "LightningDataModule", "\"", "]", "=", "None", ",", "method", ":", "Literal", "[", "\"", "fit", "\"", ",", "\"", "validate", "\"", ",", "\"", "test", "\"", ",", "\"", "predict", "\"", "]", "=", "\"", "fit", "\"", ",", "mode", ":", "str", "=", "\"", "power", "\"", ",", "steps_per_trial", ":", "int", "=", "3", ",", "init_val", ":", "int", "=", "2", ",", "max_trials", ":", "int", "=", "25", ",", "batch_arg_name", ":", "str", "=", "\"", "batch_size", "\"", ",", ")", "-", ">", "Optional", "[", "int", "]", ":", "\"", "\"", "\"", "Iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "(", "OOM", ")", "error", ".", "Args", ":", "model", ":", "Model", "to", "tune", ".", "train_dataloaders", ":", "A", "collection", "of", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "specifying", "training", "samples", ".", "In", "the", "case", "of", "multiple", "dataloaders", ",", "please", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "val_dataloaders", ":", "A", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", "sequence", "of", "them", "specifying", "validation", "samples", ".", "dataloaders", ":", "A", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", "sequence", "of", "them", "specifying", "val", "/", "test", "/", "predict", "samples", "used", "for", "running", "tuner", "on", "validation", "/", "testing", "/", "prediction", ".", "datamodule", ":", "An", "instance", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", ".", "method", ":", "Method", "to", "run", "tuner", "on", ".", "It", "can", "be", "any", "of", "`", "`", "(", "\"", "fit", "\"", ",", "\"", "validate", "\"", ",", "\"", "test", "\"", ",", "\"", "predict", "\"", ")", "`", "`", ".", "mode", ":", "Search", "strategy", "to", "update", "the", "batch", "size", ":", "-", "`", "`", "'", "power", "'", "`", "`", ":", "Keep", "multiplying", "the", "batch", "size", "by", "2", ",", "until", "we", "get", "an", "OOM", "error", ".", "-", "`", "`", "'", "binsearch", "'", "`", "`", ":", "Initially", "keep", "multiplying", "by", "2", "and", "after", "encountering", "an", "OOM", "error", "do", "a", "binary", "search", "between", "the", "last", "successful", "batch", "size", "and", "the", "batch", "size", "that", "failed", ".", "steps_per_trial", ":", "number", "of", "steps", "to", "run", "with", "a", "given", "batch", "size", ".", "Ideally", "1", "should", "be", "enough", "to", "test", "if", "an", "OOM", "error", "occurs", ",", "however", "in", "practise", "a", "few", "are", "needed", "init_val", ":", "initial", "batch", "size", "to", "start", "the", "search", "with", "max_trials", ":", "max", "number", "of", "increases", "in", "batch", "size", "done", "before", "algorithm", "is", "terminated", "batch_arg_name", ":", "name", "of", "the", "attribute", "that", "stores", "the", "batch", "size", ".", "It", "is", "expected", "that", "the", "user", "has", "provided", "a", "model", "or", "datamodule", "that", "has", "a", "hyperparameter", "with", "that", "name", ".", "We", "will", "look", "for", "this", "attribute", "name", "in", "the", "following", "places", "-", "`", "`", "model", "`", "`", "-", "`", "`", "model", ".", "hparams", "`", "`", "-", "`", "`", "trainer", ".", "datamodule", "`", "`", "(", "the", "datamodule", "passed", "to", "the", "tune", "method", ")", "\"", "\"", "\"", "_check_tuner_configuration", "(", "train_dataloaders", ",", "val_dataloaders", ",", "dataloaders", ",", "method", ")", "_check_scale_batch_size_configuration", "(", "self", ".", "_trainer", ")", "from", "lightning", ".", "pytorch", ".", "callbacks", ".", "batch_size_finder", "import", "BatchSizeFinder", "batch_size_finder", ":", "Callback", "=", "BatchSizeFinder", "(", "mode", "=", "mode", ",", "steps_per_trial", "=", "steps_per_trial", ",", "init_val", "=", "init_val", ",", "max_trials", "=", "max_trials", ",", "batch_arg_name", "=", "batch_arg_name", ",", ")", "batch_size_finder", ".", "_early_exit", "=", "True", "self", ".", "_trainer", ".", "callbacks", "=", "[", "batch_size_finder", "]", "+", "self", ".", "_trainer", ".", "callbacks", "if", "method", "=", "=", "\"", "fit", "\"", ":", "self", ".", "_trainer", ".", "fit", "(", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ")", "elif", "method", "=", "=", "\"", "validate", "\"", ":", "self", ".", "_trainer", ".", "validate", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "elif", "method", "=", "=", "\"", "test", "\"", ":", "self", ".", "_trainer", ".", "test", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "elif", "method", "=", "=", "\"", "predict", "\"", ":", "self", ".", "_trainer", ".", "predict", "(", "model", ",", "dataloaders", ",", "datamodule", "=", "datamodule", ")", "self", ".", "_trainer", ".", "callbacks", "=", "[", "cb", "for", "cb", "in", "self", ".", "_trainer", ".", "callbacks", "if", "cb", "is", "not", "batch_size_finder", "]", "return", "batch_size_finder", ".", "optimal_batch_size"], "docstring": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)\r\n        error.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            mode: Search strategy to update the batch size:\r\n\r\n                - ``'power'``: Keep multiplying the batch size by 2, until we get an OOM error.\r\n                - ``'binsearch'``: Initially keep multiplying by 2 and after encountering an OOM error\r\n                    do a binary search between the last successful batch size and the batch size that failed.\r\n\r\n            steps_per_trial: number of steps to run with a given batch size.\r\n                Ideally 1 should be enough to test if an OOM error occurs,\r\n                however in practise a few are needed\r\n            init_val: initial batch size to start the search with\r\n            max_trials: max number of increases in batch size done before\r\n               algorithm is terminated\r\n            batch_arg_name: name of the attribute that stores the batch size.\r\n                It is expected that the user has provided a model or datamodule that has a hyperparameter\r\n                with that name. We will look for this attribute name in the following places\r\n\r\n                - ``model``\r\n                - ``model.hparams``\r\n                - ``trainer.datamodule`` (the datamodule passed to the tune method)", "docstring_tokens": ["iteratively", "try", "to", "find", "the", "largest", "batch", "size", "for", "a", "given", "model", "that", "does", "not", "give", "an", "out", "of", "memory", "oom", "error", "args", "model", "model", "to", "tune", "train_dataloaders", "a", "collection", "of", "class", "torch", "utils", "data", "dataloader", "or", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "specifying", "training", "samples", "in", "the", "case", "of", "multiple", "dataloaders", "please", "see", "this", "ref", "section", "multiple", "dataloaders", "val_dataloaders", "a", "class", "torch", "utils", "data", "dataloader", "or", "a", "sequence", "of", "them", "specifying", "validation", "samples", "dataloaders", "a", "class", "torch", "utils", "data", "dataloader", "or", "a", "sequence", "of", "them", "specifying", "val", "test", "predict", "samples", "used", "for", "running", "tuner", "on", "validation", "testing", "prediction", "datamodule", "an", "instance", "of", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "method", "method", "to", "run", "tuner", "on", "it", "can", "be", "any", "of", "fit", "validate", "test", "predict", "mode", "search", "strategy", "to", "update", "the", "batch", "size", "power", "keep", "multiplying", "the", "batch", "size", "by", "2", "until", "we", "get", "an", "oom", "error", "binsearch", "initially", "keep", "multiplying", "by", "2", "and", "after", "encountering", "an", "oom", "error", "do", "a", "binary", "search", "between", "the", "last", "successful", "batch", "size", "and", "the", "batch", "size", "that", "failed", "steps_per_trial", "number", "of", "steps", "to", "run", "with", "a", "given", "batch", "size", "ideally", "1", "should", "be", "enough", "to", "test", "if", "an", "oom", "error", "occurs", "however", "in", "practise", "a", "few", "are", "needed", "init_val", "initial", "batch", "size", "to", "start", "the", "search", "with", "max_trials", "max", "number", "of", "increases", "in", "batch", "size", "done", "before", "algorithm", "is", "terminated", "batch_arg_name", "name", "of", "the", "attribute", "that", "stores", "the", "batch", "size", "it", "is", "expected", "that", "the", "user", "has", "provided", "a", "model", "or", "datamodule", "that", "has", "a", "hyperparameter", "with", "that", "name", "we", "will", "look", "for", "this", "attribute", "name", "in", "the", "following", "places", "model", "model", "hparams", "trainer", "datamodule", "the", "datamodule", "passed", "to", "the", "tune", "method"], "docstring_summary": "Iteratively try to find the largest batch size for a given model that does not give an out of memory (OOM)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\tuning.py", "partition": "test", "function_type": "class_method", "class_name": "Tuner", "start_line": 30, "end_line": 105, "hash": "adaf609dac97492b11d61c451aa2a8f1", "complexity": 7, "parameters": ["model", "train_dataloaders", "\"pl.LightningDataModule\"]]", "val_dataloaders", "dataloaders", "datamodule", "method", "\"validate\"", "\"test\"", "\"predict\"]", "mode", "steps_per_trial", "init_val", "max_trials", "batch_arg_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\tuner\\tuning.py", "func_name": "lr_find", "original_string": "def lr_find(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        min_lr: float = 1e-8,\r\n        max_lr: float = 1,\r\n        num_training: int = 100,\r\n        mode: str = \"exponential\",\r\n        early_stop_threshold: Optional[float] = 4.0,\r\n        update_attr: bool = True,\r\n        attr_name: str = \"\",\r\n    ) -> Optional[\"_LRFinder\"]:\r\n        \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\r\n        picking a good starting learning rate.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            min_lr: minimum learning rate to investigate\r\n            max_lr: maximum learning rate to investigate\r\n            num_training: number of learning rates to test\r\n            mode: Search strategy to update learning rate after each batch:\r\n\r\n                - ``'exponential'``: Increases the learning rate exponentially.\r\n                - ``'linear'``: Increases the learning rate linearly.\r\n\r\n            early_stop_threshold: Threshold for stopping the search. If the\r\n                loss at any point is larger than early_stop_threshold*best_loss\r\n                then the search is stopped. To disable, set to None.\r\n            update_attr: Whether to update the learning rate attribute or not.\r\n            attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n                automatically detected. Otherwise, set the name here.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If learning rate/lr in ``model`` or ``model.hparams`` isn't overridden,\r\n                or if you are using more than one optimizer.\r\n\r\n        \"\"\"\r\n        if method != \"fit\":\r\n            raise MisconfigurationException(\"method='fit' is the only valid configuration to run lr finder.\")\r\n\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_lr_find_configuration(self._trainer)\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.callbacks.lr_finder import LearningRateFinder\r\n\r\n        lr_finder_callback: Callback = LearningRateFinder(\r\n            min_lr=min_lr,\r\n            max_lr=max_lr,\r\n            num_training_steps=num_training,\r\n            mode=mode,\r\n            early_stop_threshold=early_stop_threshold,\r\n            update_attr=update_attr,\r\n            attr_name=attr_name,\r\n        )\r\n\r\n        lr_finder_callback._early_exit = True\r\n        self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\r\n\r\n        self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\r\n\r\n        return lr_finder_callback.optimal_lr", "language": "python", "code": "def lr_find(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        train_dataloaders: Optional[Union[TRAIN_DATALOADERS, \"pl.LightningDataModule\"]] = None,\r\n        val_dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        dataloaders: Optional[EVAL_DATALOADERS] = None,\r\n        datamodule: Optional[\"pl.LightningDataModule\"] = None,\r\n        method: Literal[\"fit\", \"validate\", \"test\", \"predict\"] = \"fit\",\r\n        min_lr: float = 1e-8,\r\n        max_lr: float = 1,\r\n        num_training: int = 100,\r\n        mode: str = \"exponential\",\r\n        early_stop_threshold: Optional[float] = 4.0,\r\n        update_attr: bool = True,\r\n        attr_name: str = \"\",\r\n    ) -> Optional[\"_LRFinder\"]:\r\n        \"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\r\n        picking a good starting learning rate.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            min_lr: minimum learning rate to investigate\r\n            max_lr: maximum learning rate to investigate\r\n            num_training: number of learning rates to test\r\n            mode: Search strategy to update learning rate after each batch:\r\n\r\n                - ``'exponential'``: Increases the learning rate exponentially.\r\n                - ``'linear'``: Increases the learning rate linearly.\r\n\r\n            early_stop_threshold: Threshold for stopping the search. If the\r\n                loss at any point is larger than early_stop_threshold*best_loss\r\n                then the search is stopped. To disable, set to None.\r\n            update_attr: Whether to update the learning rate attribute or not.\r\n            attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n                automatically detected. Otherwise, set the name here.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If learning rate/lr in ``model`` or ``model.hparams`` isn't overridden,\r\n                or if you are using more than one optimizer.\r\n\r\n        \"\"\"\r\n        if method != \"fit\":\r\n            raise MisconfigurationException(\"method='fit' is the only valid configuration to run lr finder.\")\r\n\r\n        _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\r\n        _check_lr_find_configuration(self._trainer)\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.callbacks.lr_finder import LearningRateFinder\r\n\r\n        lr_finder_callback: Callback = LearningRateFinder(\r\n            min_lr=min_lr,\r\n            max_lr=max_lr,\r\n            num_training_steps=num_training,\r\n            mode=mode,\r\n            early_stop_threshold=early_stop_threshold,\r\n            update_attr=update_attr,\r\n            attr_name=attr_name,\r\n        )\r\n\r\n        lr_finder_callback._early_exit = True\r\n        self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\r\n\r\n        self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\r\n\r\n        self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\r\n\r\n        return lr_finder_callback.optimal_lr", "code_tokens": ["def", "lr_find", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "train_dataloaders", ":", "Optional", "[", "Union", "[", "TRAIN_DATALOADERS", ",", "\"", "pl", ".", "LightningDataModule", "\"", "]", "]", "=", "None", ",", "val_dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "dataloaders", ":", "Optional", "[", "EVAL_DATALOADERS", "]", "=", "None", ",", "datamodule", ":", "Optional", "[", "\"", "pl", ".", "LightningDataModule", "\"", "]", "=", "None", ",", "method", ":", "Literal", "[", "\"", "fit", "\"", ",", "\"", "validate", "\"", ",", "\"", "test", "\"", ",", "\"", "predict", "\"", "]", "=", "\"", "fit", "\"", ",", "min_lr", ":", "float", "=", "1e", "-", "8", ",", "max_lr", ":", "float", "=", "1", ",", "num_training", ":", "int", "=", "100", ",", "mode", ":", "str", "=", "\"", "exponential", "\"", ",", "early_stop_threshold", ":", "Optional", "[", "float", "]", "=", "4", ".", "0", ",", "update_attr", ":", "bool", "=", "True", ",", "attr_name", ":", "str", "=", "\"", "\"", ",", ")", "-", ">", "Optional", "[", "\"", "_LRFinder", "\"", "]", ":", "\"", "\"", "\"", "Enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", ",", "to", "reduce", "the", "amount", "of", "guesswork", "in", "picking", "a", "good", "starting", "learning", "rate", ".", "Args", ":", "model", ":", "Model", "to", "tune", ".", "train_dataloaders", ":", "A", "collection", "of", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "specifying", "training", "samples", ".", "In", "the", "case", "of", "multiple", "dataloaders", ",", "please", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "val_dataloaders", ":", "A", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", "sequence", "of", "them", "specifying", "validation", "samples", ".", "dataloaders", ":", "A", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", "sequence", "of", "them", "specifying", "val", "/", "test", "/", "predict", "samples", "used", "for", "running", "tuner", "on", "validation", "/", "testing", "/", "prediction", ".", "datamodule", ":", "An", "instance", "of", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", ".", "method", ":", "Method", "to", "run", "tuner", "on", ".", "It", "can", "be", "any", "of", "`", "`", "(", "\"", "fit", "\"", ",", "\"", "validate", "\"", ",", "\"", "test", "\"", ",", "\"", "predict", "\"", ")", "`", "`", ".", "min_lr", ":", "minimum", "learning", "rate", "to", "investigate", "max_lr", ":", "maximum", "learning", "rate", "to", "investigate", "num_training", ":", "number", "of", "learning", "rates", "to", "test", "mode", ":", "Search", "strategy", "to", "update", "learning", "rate", "after", "each", "batch", ":", "-", "`", "`", "'", "exponential", "'", "`", "`", ":", "Increases", "the", "learning", "rate", "exponentially", ".", "-", "`", "`", "'", "linear", "'", "`", "`", ":", "Increases", "the", "learning", "rate", "linearly", ".", "early_stop_threshold", ":", "Threshold", "for", "stopping", "the", "search", ".", "If", "the", "loss", "at", "any", "point", "is", "larger", "than", "early_stop_threshold", "*", "best_loss", "then", "the", "search", "is", "stopped", ".", "To", "disable", ",", "set", "to", "None", ".", "update_attr", ":", "Whether", "to", "update", "the", "learning", "rate", "attribute", "or", "not", ".", "attr_name", ":", "Name", "of", "the", "attribute", "which", "stores", "the", "learning", "rate", ".", "The", "names", "'", "learning_rate", "'", "or", "'", "lr", "'", "get", "automatically", "detected", ".", "Otherwise", ",", "set", "the", "name", "here", ".", "Raises", ":", "MisconfigurationException", ":", "If", "learning", "rate", "/", "lr", "in", "`", "`", "model", "`", "`", "or", "`", "`", "model", ".", "hparams", "`", "`", "isn", "'", "t", "overridden", ",", "or", "if", "you", "are", "using", "more", "than", "one", "optimizer", ".", "\"", "\"", "\"", "if", "method", "!", "=", "\"", "fit", "\"", ":", "raise", "MisconfigurationException", "(", "\"", "method", "=", "'", "fit", "'", "is", "the", "only", "valid", "configuration", "to", "run", "lr", "finder", ".", "\"", ")", "_check_tuner_configuration", "(", "train_dataloaders", ",", "val_dataloaders", ",", "dataloaders", ",", "method", ")", "_check_lr_find_configuration", "(", "self", ".", "_trainer", ")", "from", "lightning", ".", "pytorch", ".", "callbacks", ".", "lr_finder", "import", "LearningRateFinder", "lr_finder_callback", ":", "Callback", "=", "LearningRateFinder", "(", "min_lr", "=", "min_lr", ",", "max_lr", "=", "max_lr", ",", "num_training_steps", "=", "num_training", ",", "mode", "=", "mode", ",", "early_stop_threshold", "=", "early_stop_threshold", ",", "update_attr", "=", "update_attr", ",", "attr_name", "=", "attr_name", ",", ")", "lr_finder_callback", ".", "_early_exit", "=", "True", "self", ".", "_trainer", ".", "callbacks", "=", "[", "lr_finder_callback", "]", "+", "self", ".", "_trainer", ".", "callbacks", "self", ".", "_trainer", ".", "fit", "(", "model", ",", "train_dataloaders", ",", "val_dataloaders", ",", "datamodule", ")", "self", ".", "_trainer", ".", "callbacks", "=", "[", "cb", "for", "cb", "in", "self", ".", "_trainer", ".", "callbacks", "if", "cb", "is", "not", "lr_finder_callback", "]", "return", "lr_finder_callback", ".", "optimal_lr"], "docstring": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\r\n        picking a good starting learning rate.\r\n\r\n        Args:\r\n            model: Model to tune.\r\n            train_dataloaders: A collection of :class:`torch.utils.data.DataLoader` or a\r\n                :class:`~lightning.pytorch.core.datamodule.LightningDataModule` specifying training samples.\r\n                In the case of multiple dataloaders, please see this :ref:`section <multiple-dataloaders>`.\r\n            val_dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.\r\n            dataloaders: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying val/test/predict\r\n                samples used for running tuner on validation/testing/prediction.\r\n            datamodule: An instance of :class:`~lightning.pytorch.core.datamodule.LightningDataModule`.\r\n            method: Method to run tuner on. It can be any of ``(\"fit\", \"validate\", \"test\", \"predict\")``.\r\n            min_lr: minimum learning rate to investigate\r\n            max_lr: maximum learning rate to investigate\r\n            num_training: number of learning rates to test\r\n            mode: Search strategy to update learning rate after each batch:\r\n\r\n                - ``'exponential'``: Increases the learning rate exponentially.\r\n                - ``'linear'``: Increases the learning rate linearly.\r\n\r\n            early_stop_threshold: Threshold for stopping the search. If the\r\n                loss at any point is larger than early_stop_threshold*best_loss\r\n                then the search is stopped. To disable, set to None.\r\n            update_attr: Whether to update the learning rate attribute or not.\r\n            attr_name: Name of the attribute which stores the learning rate. The names 'learning_rate' or 'lr' get\r\n                automatically detected. Otherwise, set the name here.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If learning rate/lr in ``model`` or ``model.hparams`` isn't overridden,\r\n                or if you are using more than one optimizer.", "docstring_tokens": ["enables", "the", "user", "to", "do", "a", "range", "test", "of", "good", "initial", "learning", "rates", "to", "reduce", "the", "amount", "of", "guesswork", "in", "picking", "a", "good", "starting", "learning", "rate", "args", "model", "model", "to", "tune", "train_dataloaders", "a", "collection", "of", "class", "torch", "utils", "data", "dataloader", "or", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "specifying", "training", "samples", "in", "the", "case", "of", "multiple", "dataloaders", "please", "see", "this", "ref", "section", "multiple", "dataloaders", "val_dataloaders", "a", "class", "torch", "utils", "data", "dataloader", "or", "a", "sequence", "of", "them", "specifying", "validation", "samples", "dataloaders", "a", "class", "torch", "utils", "data", "dataloader", "or", "a", "sequence", "of", "them", "specifying", "val", "test", "predict", "samples", "used", "for", "running", "tuner", "on", "validation", "testing", "prediction", "datamodule", "an", "instance", "of", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "method", "method", "to", "run", "tuner", "on", "it", "can", "be", "any", "of", "fit", "validate", "test", "predict", "min_lr", "minimum", "learning", "rate", "to", "investigate", "max_lr", "maximum", "learning", "rate", "to", "investigate", "num_training", "number", "of", "learning", "rates", "to", "test", "mode", "search", "strategy", "to", "update", "learning", "rate", "after", "each", "batch", "exponential", "increases", "the", "learning", "rate", "exponentially", "linear", "increases", "the", "learning", "rate", "linearly", "early_stop_threshold", "threshold", "for", "stopping", "the", "search", "if", "the", "loss", "at", "any", "point", "is", "larger", "than", "early_stop_threshold", "best_loss", "then", "the", "search", "is", "stopped", "to", "disable", "set", "to", "none", "update_attr", "whether", "to", "update", "the", "learning", "rate", "attribute", "or", "not", "attr_name", "name", "of", "the", "attribute", "which", "stores", "the", "learning", "rate", "the", "names", "learning_rate", "or", "lr", "get", "automatically", "detected", "otherwise", "set", "the", "name", "here", "raises", "misconfigurationexception", "if", "learning", "rate", "lr", "in", "model", "or", "model", "hparams", "isn", "t", "overridden", "or", "if", "you", "are", "using", "more", "than", "one", "optimizer"], "docstring_summary": "Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\tuner\\tuning.py", "partition": "test", "function_type": "class_method", "class_name": "Tuner", "start_line": 107, "end_line": 183, "hash": "1d0da19fd8776fa00767246f1deb53dd", "complexity": 4, "parameters": ["model", "train_dataloaders", "\"pl.LightningDataModule\"]]", "val_dataloaders", "dataloaders", "datamodule", "method", "\"validate\"", "\"test\"", "\"predict\"]", "min_lr", "max_lr", "num_training", "mode", "early_stop_threshold", "update_attr", "attr_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\argparse.py", "func_name": "_parse_env_variables", "original_string": "def _parse_env_variables(cls: type, template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\r\n    \"\"\"Parse environment arguments if they are defined.\r\n\r\n    Examples:\r\n\r\n        >>> from lightning.pytorch import Trainer\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace()\r\n        >>> import os\r\n        >>> os.environ[\"PL_TRAINER_DEVICES\"] = '42'\r\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace(devices=42)\r\n        >>> del os.environ[\"PL_TRAINER_DEVICES\"]\r\n\r\n    \"\"\"\r\n    env_args = {}\r\n    for arg_name in inspect.signature(cls).parameters:\r\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\r\n        val = os.environ.get(env)\r\n        if not (val is None or val == \"\"):\r\n            # todo: specify the possible exception\r\n            with suppress(Exception):\r\n                # converting to native types like int/float/bool\r\n                val = literal_eval(val)\r\n            env_args[arg_name] = val\r\n    return Namespace(**env_args)", "language": "python", "code": "def _parse_env_variables(cls: type, template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\r\n    \"\"\"Parse environment arguments if they are defined.\r\n\r\n    Examples:\r\n\r\n        >>> from lightning.pytorch import Trainer\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace()\r\n        >>> import os\r\n        >>> os.environ[\"PL_TRAINER_DEVICES\"] = '42'\r\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace(devices=42)\r\n        >>> del os.environ[\"PL_TRAINER_DEVICES\"]\r\n\r\n    \"\"\"\r\n    env_args = {}\r\n    for arg_name in inspect.signature(cls).parameters:\r\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\r\n        val = os.environ.get(env)\r\n        if not (val is None or val == \"\"):\r\n            # todo: specify the possible exception\r\n            with suppress(Exception):\r\n                # converting to native types like int/float/bool\r\n                val = literal_eval(val)\r\n            env_args[arg_name] = val\r\n    return Namespace(**env_args)", "code_tokens": ["def", "_parse_env_variables", "(", "cls", ":", "type", ",", "template", ":", "str", "=", "\"", "PL_", "%", "(", "cls_name", ")", "s_", "%", "(", "cls_argument", ")", "s", "\"", ")", "-", ">", "Namespace", ":", "\"", "\"", "\"", "Parse", "environment", "arguments", "if", "they", "are", "defined", ".", "Examples", ":", ">", ">", ">", "from", "lightning", ".", "pytorch", "import", "Trainer", ">", ">", ">", "_parse_env_variables", "(", "Trainer", ")", "Namespace", "(", ")", ">", ">", ">", "import", "os", ">", ">", ">", "os", ".", "environ", "[", "\"", "PL_TRAINER_DEVICES", "\"", "]", "=", "'", "42", "'", ">", ">", ">", "os", ".", "environ", "[", "\"", "PL_TRAINER_BLABLABLA", "\"", "]", "=", "'", "1", ".", "23", "'", ">", ">", ">", "_parse_env_variables", "(", "Trainer", ")", "Namespace", "(", "devices", "=", "42", ")", ">", ">", ">", "del", "os", ".", "environ", "[", "\"", "PL_TRAINER_DEVICES", "\"", "]", "\"", "\"", "\"", "env_args", "=", "{", "}", "for", "arg_name", "in", "inspect", ".", "signature", "(", "cls", ")", ".", "parameters", ":", "env", "=", "template", "%", "{", "\"", "cls_name", "\"", ":", "cls", ".", "__name__", ".", "upper", "(", ")", ",", "\"", "cls_argument", "\"", ":", "arg_name", ".", "upper", "(", ")", "}", "val", "=", "os", ".", "environ", ".", "get", "(", "env", ")", "if", "not", "(", "val", "is", "None", "or", "val", "=", "=", "\"", "\"", ")", ":", "with", "suppress", "(", "Exception", ")", ":", "val", "=", "literal_eval", "(", "val", ")", "env_args", "[", "arg_name", "]", "=", "val", "return", "Namespace", "(", "*", "*", "env_args", ")"], "docstring": "Parse environment arguments if they are defined.\r\n\r\n    Examples:\r\n\r\n        >>> from lightning.pytorch import Trainer\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace()\r\n        >>> import os\r\n        >>> os.environ[\"PL_TRAINER_DEVICES\"] = '42'\r\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\r\n        >>> _parse_env_variables(Trainer)\r\n        Namespace(devices=42)\r\n        >>> del os.environ[\"PL_TRAINER_DEVICES\"]", "docstring_tokens": ["parse", "environment", "arguments", "if", "they", "are", "defined", "examples", "from", "lightning", "pytorch", "import", "trainer", "_parse_env_variables", "trainer", "namespace", "import", "os", "os", "environ", "pl_trainer_devices", "42", "os", "environ", "pl_trainer_blablabla", "1", "23", "_parse_env_variables", "trainer", "namespace", "devices", "42", "del", "os", "environ", "pl_trainer_devices"], "docstring_summary": "Parse environment arguments if they are defined.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\argparse.py", "partition": "test", "function_type": "function", "start_line": 26, "end_line": 52, "hash": "94c952561daa327f4c642b8251d847a1", "complexity": 5, "parameters": ["cls", "template"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "flattened", "original_string": "def flattened(self, flattened: list[Any]) -> None:\r\n        \"\"\"Setter to conveniently update the list of iterables.\"\"\"\r\n        if len(flattened) != len(self._flattened):\r\n            raise ValueError(\r\n                f\"Mismatch in flattened length ({len(flattened)}) and existing length ({len(self._flattened)})\"\r\n            )\r\n        # update the iterable collection\r\n        self._iterables = tree_unflatten(flattened, self._spec)\r\n        self._flattened = flattened", "language": "python", "code": "def flattened(self, flattened: list[Any]) -> None:\r\n        \"\"\"Setter to conveniently update the list of iterables.\"\"\"\r\n        if len(flattened) != len(self._flattened):\r\n            raise ValueError(\r\n                f\"Mismatch in flattened length ({len(flattened)}) and existing length ({len(self._flattened)})\"\r\n            )\r\n        # update the iterable collection\r\n        self._iterables = tree_unflatten(flattened, self._spec)\r\n        self._flattened = flattened", "code_tokens": ["def", "flattened", "(", "self", ",", "flattened", ":", "list", "[", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Setter", "to", "conveniently", "update", "the", "list", "of", "iterables", ".", "\"", "\"", "\"", "if", "len", "(", "flattened", ")", "!", "=", "len", "(", "self", ".", "_flattened", ")", ":", "raise", "ValueError", "(", "f", "\"", "Mismatch", "in", "flattened", "length", "(", "{", "len", "(", "flattened", ")", "}", ")", "and", "existing", "length", "(", "{", "len", "(", "self", ".", "_flattened", ")", "}", ")", "\"", ")", "self", ".", "_iterables", "=", "tree_unflatten", "(", "flattened", ",", "self", ".", "_spec", ")", "self", ".", "_flattened", "=", "flattened"], "docstring": "Setter to conveniently update the list of iterables.", "docstring_tokens": ["setter", "to", "conveniently", "update", "the", "list", "of", "iterables"], "docstring_summary": "Setter to conveniently update the list of iterables.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\combined_loader.py", "partition": "test", "function_type": "class_method", "class_name": "CombinedLoader", "start_line": 313, "end_line": 321, "hash": "7423c49c95aab14aed8fdacf7b14c087", "complexity": 2, "parameters": ["flattened"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "__len__", "original_string": "def __len__(self) -> int:\r\n        \"\"\"Compute the number of batches.\"\"\"\r\n        if self._iterator is None:\r\n            raise RuntimeError(\"Please call `iter(combined_loader)` first.\")\r\n        return len(self._iterator)", "language": "python", "code": "def __len__(self) -> int:\r\n        \"\"\"Compute the number of batches.\"\"\"\r\n        if self._iterator is None:\r\n            raise RuntimeError(\"Please call `iter(combined_loader)` first.\")\r\n        return len(self._iterator)", "code_tokens": ["def", "__len__", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Compute", "the", "number", "of", "batches", ".", "\"", "\"", "\"", "if", "self", ".", "_iterator", "is", "None", ":", "raise", "RuntimeError", "(", "\"", "Please", "call", "`", "iter", "(", "combined_loader", ")", "`", "first", ".", "\"", ")", "return", "len", "(", "self", ".", "_iterator", ")"], "docstring": "Compute the number of batches.", "docstring_tokens": ["compute", "the", "number", "of", "batches"], "docstring_summary": "Compute the number of batches.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\combined_loader.py", "partition": "test", "function_type": "class_method", "class_name": "CombinedLoader", "start_line": 354, "end_line": 358, "hash": "917a8350b25f5b258fc74a72865c4e99", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "reset", "original_string": "def reset(self) -> None:\r\n        \"\"\"Reset the state and shutdown any workers.\"\"\"\r\n        if self._iterator is not None:\r\n            self._iterator.reset()\r\n            self._iterator = None\r\n        for iterable in self.flattened:\r\n            _shutdown_workers_and_reset_iterator(iterable)", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Reset the state and shutdown any workers.\"\"\"\r\n        if self._iterator is not None:\r\n            self._iterator.reset()\r\n            self._iterator = None\r\n        for iterable in self.flattened:\r\n            _shutdown_workers_and_reset_iterator(iterable)", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Reset", "the", "state", "and", "shutdown", "any", "workers", ".", "\"", "\"", "\"", "if", "self", ".", "_iterator", "is", "not", "None", ":", "self", ".", "_iterator", ".", "reset", "(", ")", "self", ".", "_iterator", "=", "None", "for", "iterable", "in", "self", ".", "flattened", ":", "_shutdown_workers_and_reset_iterator", "(", "iterable", ")"], "docstring": "Reset the state and shutdown any workers.", "docstring_tokens": ["reset", "the", "state", "and", "shutdown", "any", "workers"], "docstring_summary": "Reset the state and shutdown any workers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\combined_loader.py", "partition": "test", "function_type": "class_method", "class_name": "CombinedLoader", "start_line": 360, "end_line": 366, "hash": "d275c574fef47da8915887bc2fada33e", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "_dataset_length", "original_string": "def _dataset_length(self) -> int:\r\n        \"\"\"Compute the total length of the datasets according to the current mode.\"\"\"\r\n        datasets = [getattr(dl, \"dataset\", None) for dl in self.flattened]\r\n        lengths = [length for ds in datasets if (length := sized_len(ds)) is not None]\r\n        if not lengths:\r\n            raise NotImplementedError(\"All datasets are iterable-style datasets.\")\r\n        fn = _SUPPORTED_MODES[self._mode][\"fn\"]\r\n        return fn(lengths)", "language": "python", "code": "def _dataset_length(self) -> int:\r\n        \"\"\"Compute the total length of the datasets according to the current mode.\"\"\"\r\n        datasets = [getattr(dl, \"dataset\", None) for dl in self.flattened]\r\n        lengths = [length for ds in datasets if (length := sized_len(ds)) is not None]\r\n        if not lengths:\r\n            raise NotImplementedError(\"All datasets are iterable-style datasets.\")\r\n        fn = _SUPPORTED_MODES[self._mode][\"fn\"]\r\n        return fn(lengths)", "code_tokens": ["def", "_dataset_length", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Compute", "the", "total", "length", "of", "the", "datasets", "according", "to", "the", "current", "mode", ".", "\"", "\"", "\"", "datasets", "=", "[", "getattr", "(", "dl", ",", "\"", "dataset", "\"", ",", "None", ")", "for", "dl", "in", "self", ".", "flattened", "]", "lengths", "=", "[", "length", "for", "ds", "in", "datasets", "if", "(", "length", ":", "=", "sized_len", "(", "ds", ")", ")", "is", "not", "None", "]", "if", "not", "lengths", ":", "raise", "NotImplementedError", "(", "\"", "All", "datasets", "are", "iterable", "-", "style", "datasets", ".", "\"", ")", "fn", "=", "_SUPPORTED_MODES", "[", "self", ".", "_mode", "]", "[", "\"", "fn", "\"", "]", "return", "fn", "(", "lengths", ")"], "docstring": "Compute the total length of the datasets according to the current mode.", "docstring_tokens": ["compute", "the", "total", "length", "of", "the", "datasets", "according", "to", "the", "current", "mode"], "docstring_summary": "Compute the total length of the datasets according to the current mode.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\combined_loader.py", "partition": "test", "function_type": "class_method", "class_name": "CombinedLoader", "start_line": 368, "end_line": 375, "hash": "74a234acb0f2a40c4f61edcc3ca69b5b", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\combined_loader.py", "func_name": "_load_state_dicts", "original_string": "def _load_state_dicts(self, states: list[dict[str, Any]]) -> None:\r\n        \"\"\"Loads the state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        if not states:\r\n            return\r\n        stateful_loaders = [loader for loader in self.flattened if isinstance(loader, _Stateful)]\r\n        if len(stateful_loaders) != len(states):\r\n            raise RuntimeError(\r\n                f\"The CombinedLoader has {len(stateful_loaders)} stateful loaders, but found {len(states)} states\"\r\n                \" in the checkpoint. Please make sure you define the same dataloaders that were used when saving\"\r\n                \" the checkpoint.\"\r\n            )\r\n        for loader, state_dict in zip(stateful_loaders, states):\r\n            loader.load_state_dict(state_dict)", "language": "python", "code": "def _load_state_dicts(self, states: list[dict[str, Any]]) -> None:\r\n        \"\"\"Loads the state dicts for iterables in `self.flattened` that are stateful.\"\"\"\r\n        if not states:\r\n            return\r\n        stateful_loaders = [loader for loader in self.flattened if isinstance(loader, _Stateful)]\r\n        if len(stateful_loaders) != len(states):\r\n            raise RuntimeError(\r\n                f\"The CombinedLoader has {len(stateful_loaders)} stateful loaders, but found {len(states)} states\"\r\n                \" in the checkpoint. Please make sure you define the same dataloaders that were used when saving\"\r\n                \" the checkpoint.\"\r\n            )\r\n        for loader, state_dict in zip(stateful_loaders, states):\r\n            loader.load_state_dict(state_dict)", "code_tokens": ["def", "_load_state_dicts", "(", "self", ",", "states", ":", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "the", "state", "dicts", "for", "iterables", "in", "`", "self", ".", "flattened", "`", "that", "are", "stateful", ".", "\"", "\"", "\"", "if", "not", "states", ":", "return", "stateful_loaders", "=", "[", "loader", "for", "loader", "in", "self", ".", "flattened", "if", "isinstance", "(", "loader", ",", "_Stateful", ")", "]", "if", "len", "(", "stateful_loaders", ")", "!", "=", "len", "(", "states", ")", ":", "raise", "RuntimeError", "(", "f", "\"", "The", "CombinedLoader", "has", "{", "len", "(", "stateful_loaders", ")", "}", "stateful", "loaders", ",", "but", "found", "{", "len", "(", "states", ")", "}", "states", "\"", "\"", "in", "the", "checkpoint", ".", "Please", "make", "sure", "you", "define", "the", "same", "dataloaders", "that", "were", "used", "when", "saving", "\"", "\"", "the", "checkpoint", ".", "\"", ")", "for", "loader", ",", "state_dict", "in", "zip", "(", "stateful_loaders", ",", "states", ")", ":", "loader", ".", "load_state_dict", "(", "state_dict", ")"], "docstring": "Loads the state dicts for iterables in `self.flattened` that are stateful.", "docstring_tokens": ["loads", "the", "state", "dicts", "for", "iterables", "in", "self", "flattened", "that", "are", "stateful"], "docstring_summary": "Loads the state dicts for iterables in `self.flattened` that are stateful.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\combined_loader.py", "partition": "test", "function_type": "class_method", "class_name": "CombinedLoader", "start_line": 381, "end_line": 393, "hash": "54ac453f47448cb0a97e38f70087301a", "complexity": 6, "parameters": ["states", "Any]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\compile.py", "func_name": "from_compiled", "original_string": "def from_compiled(model: OptimizedModule) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance LightningModule from the output of ``torch.compile``.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    The ``torch.compile`` function returns a ``torch._dynamo.OptimizedModule``, which wraps the LightningModule\r\n    passed in as an argument, but doesn't inherit from it. This means that the output of ``torch.compile`` behaves\r\n    like a LightningModule, but it doesn't inherit from it (i.e. `isinstance` will fail).\r\n\r\n    Use this method to obtain a LightningModule that still runs with all the optimizations from ``torch.compile``.\r\n\r\n    \"\"\"\r\n    if not isinstance(model, OptimizedModule):\r\n        raise ValueError(f\"`model` is required to be a `OptimizedModule`. Found a `{type(model).__name__}` instead.\")\r\n\r\n    orig_module = model._orig_mod\r\n\r\n    if not isinstance(orig_module, pl.LightningModule):\r\n        _check_mixed_imports(model)\r\n        raise ValueError(\r\n            f\"`model` is expected to be a compiled LightningModule. Found a `{type(orig_module).__name__}` instead\"\r\n        )\r\n\r\n    orig_module._compiler_ctx = {\r\n        \"compiler\": \"dynamo\",\r\n        \"dynamo_ctx\": model.dynamo_ctx,\r\n        \"original_forward\": orig_module.forward,\r\n        \"original_training_step\": orig_module.training_step,\r\n        \"original_validation_step\": orig_module.validation_step,\r\n        \"original_test_step\": orig_module.test_step,\r\n        \"original_predict_step\": orig_module.predict_step,\r\n    }\r\n\r\n    orig_module.forward = model.dynamo_ctx(orig_module.forward)  # type: ignore[method-assign]\r\n    orig_module.training_step = model.dynamo_ctx(orig_module.training_step)  # type: ignore[method-assign]\r\n    orig_module.validation_step = model.dynamo_ctx(orig_module.validation_step)  # type: ignore[method-assign]\r\n    orig_module.test_step = model.dynamo_ctx(orig_module.test_step)  # type: ignore[method-assign]\r\n    orig_module.predict_step = model.dynamo_ctx(orig_module.predict_step)  # type: ignore[method-assign]\r\n    return orig_module", "language": "python", "code": "def from_compiled(model: OptimizedModule) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance LightningModule from the output of ``torch.compile``.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    The ``torch.compile`` function returns a ``torch._dynamo.OptimizedModule``, which wraps the LightningModule\r\n    passed in as an argument, but doesn't inherit from it. This means that the output of ``torch.compile`` behaves\r\n    like a LightningModule, but it doesn't inherit from it (i.e. `isinstance` will fail).\r\n\r\n    Use this method to obtain a LightningModule that still runs with all the optimizations from ``torch.compile``.\r\n\r\n    \"\"\"\r\n    if not isinstance(model, OptimizedModule):\r\n        raise ValueError(f\"`model` is required to be a `OptimizedModule`. Found a `{type(model).__name__}` instead.\")\r\n\r\n    orig_module = model._orig_mod\r\n\r\n    if not isinstance(orig_module, pl.LightningModule):\r\n        _check_mixed_imports(model)\r\n        raise ValueError(\r\n            f\"`model` is expected to be a compiled LightningModule. Found a `{type(orig_module).__name__}` instead\"\r\n        )\r\n\r\n    orig_module._compiler_ctx = {\r\n        \"compiler\": \"dynamo\",\r\n        \"dynamo_ctx\": model.dynamo_ctx,\r\n        \"original_forward\": orig_module.forward,\r\n        \"original_training_step\": orig_module.training_step,\r\n        \"original_validation_step\": orig_module.validation_step,\r\n        \"original_test_step\": orig_module.test_step,\r\n        \"original_predict_step\": orig_module.predict_step,\r\n    }\r\n\r\n    orig_module.forward = model.dynamo_ctx(orig_module.forward)  # type: ignore[method-assign]\r\n    orig_module.training_step = model.dynamo_ctx(orig_module.training_step)  # type: ignore[method-assign]\r\n    orig_module.validation_step = model.dynamo_ctx(orig_module.validation_step)  # type: ignore[method-assign]\r\n    orig_module.test_step = model.dynamo_ctx(orig_module.test_step)  # type: ignore[method-assign]\r\n    orig_module.predict_step = model.dynamo_ctx(orig_module.predict_step)  # type: ignore[method-assign]\r\n    return orig_module", "code_tokens": ["def", "from_compiled", "(", "model", ":", "OptimizedModule", ")", "-", ">", "\"", "pl", ".", "LightningModule", "\"", ":", "\"", "\"", "\"", "Returns", "an", "instance", "LightningModule", "from", "the", "output", "of", "`", "`", "torch", ".", "compile", "`", "`", ".", ".", ".", "warning", ":", ":", "This", "is", "an", ":", "ref", ":", "`", "experimental", "<", "versioning", ":", "Experimental", "API", ">", "`", "feature", ".", "The", "`", "`", "torch", ".", "compile", "`", "`", "function", "returns", "a", "`", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "`", ",", "which", "wraps", "the", "LightningModule", "passed", "in", "as", "an", "argument", ",", "but", "doesn", "'", "t", "inherit", "from", "it", ".", "This", "means", "that", "the", "output", "of", "`", "`", "torch", ".", "compile", "`", "`", "behaves", "like", "a", "LightningModule", ",", "but", "it", "doesn", "'", "t", "inherit", "from", "it", "(", "i", ".", "e", ".", "`", "isinstance", "`", "will", "fail", ")", ".", "Use", "this", "method", "to", "obtain", "a", "LightningModule", "that", "still", "runs", "with", "all", "the", "optimizations", "from", "`", "`", "torch", ".", "compile", "`", "`", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "model", ",", "OptimizedModule", ")", ":", "raise", "ValueError", "(", "f", "\"", "`", "model", "`", "is", "required", "to", "be", "a", "`", "OptimizedModule", "`", ".", "Found", "a", "`", "{", "type", "(", "model", ")", ".", "__name__", "}", "`", "instead", ".", "\"", ")", "orig_module", "=", "model", ".", "_orig_mod", "if", "not", "isinstance", "(", "orig_module", ",", "pl", ".", "LightningModule", ")", ":", "_check_mixed_imports", "(", "model", ")", "raise", "ValueError", "(", "f", "\"", "`", "model", "`", "is", "expected", "to", "be", "a", "compiled", "LightningModule", ".", "Found", "a", "`", "{", "type", "(", "orig_module", ")", ".", "__name__", "}", "`", "instead", "\"", ")", "orig_module", ".", "_compiler_ctx", "=", "{", "\"", "compiler", "\"", ":", "\"", "dynamo", "\"", ",", "\"", "dynamo_ctx", "\"", ":", "model", ".", "dynamo_ctx", ",", "\"", "original_forward", "\"", ":", "orig_module", ".", "forward", ",", "\"", "original_training_step", "\"", ":", "orig_module", ".", "training_step", ",", "\"", "original_validation_step", "\"", ":", "orig_module", ".", "validation_step", ",", "\"", "original_test_step", "\"", ":", "orig_module", ".", "test_step", ",", "\"", "original_predict_step", "\"", ":", "orig_module", ".", "predict_step", ",", "}", "orig_module", ".", "forward", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "forward", ")", "orig_module", ".", "training_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "training_step", ")", "orig_module", ".", "validation_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "validation_step", ")", "orig_module", ".", "test_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "test_step", ")", "orig_module", ".", "predict_step", "=", "model", ".", "dynamo_ctx", "(", "orig_module", ".", "predict_step", ")", "return", "orig_module"], "docstring": "Returns an instance LightningModule from the output of ``torch.compile``.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    The ``torch.compile`` function returns a ``torch._dynamo.OptimizedModule``, which wraps the LightningModule\r\n    passed in as an argument, but doesn't inherit from it. This means that the output of ``torch.compile`` behaves\r\n    like a LightningModule, but it doesn't inherit from it (i.e. `isinstance` will fail).\r\n\r\n    Use this method to obtain a LightningModule that still runs with all the optimizations from ``torch.compile``.", "docstring_tokens": ["returns", "an", "instance", "lightningmodule", "from", "the", "output", "of", "torch", "compile", "warning", "this", "is", "an", "ref", "experimental", "versioning", "experimental", "api", "feature", "the", "torch", "compile", "function", "returns", "a", "torch", "_dynamo", "optimizedmodule", "which", "wraps", "the", "lightningmodule", "passed", "in", "as", "an", "argument", "but", "doesn", "t", "inherit", "from", "it", "this", "means", "that", "the", "output", "of", "torch", "compile", "behaves", "like", "a", "lightningmodule", "but", "it", "doesn", "t", "inherit", "from", "it", "i", "e", "isinstance", "will", "fail", "use", "this", "method", "to", "obtain", "a", "lightningmodule", "that", "still", "runs", "with", "all", "the", "optimizations", "from", "torch", "compile"], "docstring_summary": "Returns an instance LightningModule from the output of ``torch.compile``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\compile.py", "partition": "test", "function_type": "function", "start_line": 23, "end_line": 61, "hash": "a697408c3fd192a81da55b43c4313e38", "complexity": 3, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\compile.py", "func_name": "to_uncompiled", "original_string": "def to_uncompiled(model: Union[\"pl.LightningModule\", \"torch._dynamo.OptimizedModule\"]) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance of LightningModule without any compilation optimizations from a compiled model.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    This takes either a ``torch._dynamo.OptimizedModule`` returned by ``torch.compile()`` or a ``LightningModule``\r\n    returned by ``from_compiled``.\r\n\r\n    Note: this method will in-place modify the ``LightningModule`` that is passed in.\r\n\r\n    \"\"\"\r\n    if isinstance(model, OptimizedModule):\r\n        original = model._orig_mod\r\n        if not isinstance(original, pl.LightningModule):\r\n            raise TypeError(\r\n                f\"Unexpected error, the wrapped model should be a LightningModule, found {type(model).__name__}\"\r\n            )\r\n\r\n    elif isinstance(model, pl.LightningModule):\r\n        if model._compiler_ctx is None:\r\n            raise ValueError(\r\n                \"`model` is required to be a compiled LightningModule. Found a non-compiled LightningModule instead.\"\r\n            )\r\n        original = model\r\n\r\n    else:\r\n        raise ValueError(\"`model` must either be an instance of OptimizedModule or LightningModule\")\r\n\r\n    ctx = original._compiler_ctx\r\n    if ctx is not None:\r\n        original.forward = ctx[\"original_forward\"]  # type: ignore[method-assign]\r\n        original.training_step = ctx[\"original_training_step\"]  # type: ignore[method-assign]\r\n        original.validation_step = ctx[\"original_validation_step\"]  # type: ignore[method-assign]\r\n        original.test_step = ctx[\"original_test_step\"]  # type: ignore[method-assign]\r\n        original.predict_step = ctx[\"original_predict_step\"]  # type: ignore[method-assign]\r\n        original._compiler_ctx = None\r\n\r\n    return original", "language": "python", "code": "def to_uncompiled(model: Union[\"pl.LightningModule\", \"torch._dynamo.OptimizedModule\"]) -> \"pl.LightningModule\":\r\n    \"\"\"Returns an instance of LightningModule without any compilation optimizations from a compiled model.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    This takes either a ``torch._dynamo.OptimizedModule`` returned by ``torch.compile()`` or a ``LightningModule``\r\n    returned by ``from_compiled``.\r\n\r\n    Note: this method will in-place modify the ``LightningModule`` that is passed in.\r\n\r\n    \"\"\"\r\n    if isinstance(model, OptimizedModule):\r\n        original = model._orig_mod\r\n        if not isinstance(original, pl.LightningModule):\r\n            raise TypeError(\r\n                f\"Unexpected error, the wrapped model should be a LightningModule, found {type(model).__name__}\"\r\n            )\r\n\r\n    elif isinstance(model, pl.LightningModule):\r\n        if model._compiler_ctx is None:\r\n            raise ValueError(\r\n                \"`model` is required to be a compiled LightningModule. Found a non-compiled LightningModule instead.\"\r\n            )\r\n        original = model\r\n\r\n    else:\r\n        raise ValueError(\"`model` must either be an instance of OptimizedModule or LightningModule\")\r\n\r\n    ctx = original._compiler_ctx\r\n    if ctx is not None:\r\n        original.forward = ctx[\"original_forward\"]  # type: ignore[method-assign]\r\n        original.training_step = ctx[\"original_training_step\"]  # type: ignore[method-assign]\r\n        original.validation_step = ctx[\"original_validation_step\"]  # type: ignore[method-assign]\r\n        original.test_step = ctx[\"original_test_step\"]  # type: ignore[method-assign]\r\n        original.predict_step = ctx[\"original_predict_step\"]  # type: ignore[method-assign]\r\n        original._compiler_ctx = None\r\n\r\n    return original", "code_tokens": ["def", "to_uncompiled", "(", "model", ":", "Union", "[", "\"", "pl", ".", "LightningModule", "\"", ",", "\"", "torch", ".", "_dynamo", ".", "OptimizedModule", "\"", "]", ")", "-", ">", "\"", "pl", ".", "LightningModule", "\"", ":", "\"", "\"", "\"", "Returns", "an", "instance", "of", "LightningModule", "without", "any", "compilation", "optimizations", "from", "a", "compiled", "model", ".", ".", ".", "warning", ":", ":", "This", "is", "an", ":", "ref", ":", "`", "experimental", "<", "versioning", ":", "Experimental", "API", ">", "`", "feature", ".", "This", "takes", "either", "a", "`", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "`", "returned", "by", "`", "`", "torch", ".", "compile", "(", ")", "`", "`", "or", "a", "`", "`", "LightningModule", "`", "`", "returned", "by", "`", "`", "from_compiled", "`", "`", ".", "Note", ":", "this", "method", "will", "in", "-", "place", "modify", "the", "`", "`", "LightningModule", "`", "`", "that", "is", "passed", "in", ".", "\"", "\"", "\"", "if", "isinstance", "(", "model", ",", "OptimizedModule", ")", ":", "original", "=", "model", ".", "_orig_mod", "if", "not", "isinstance", "(", "original", ",", "pl", ".", "LightningModule", ")", ":", "raise", "TypeError", "(", "f", "\"", "Unexpected", "error", ",", "the", "wrapped", "model", "should", "be", "a", "LightningModule", ",", "found", "{", "type", "(", "model", ")", ".", "__name__", "}", "\"", ")", "elif", "isinstance", "(", "model", ",", "pl", ".", "LightningModule", ")", ":", "if", "model", ".", "_compiler_ctx", "is", "None", ":", "raise", "ValueError", "(", "\"", "`", "model", "`", "is", "required", "to", "be", "a", "compiled", "LightningModule", ".", "Found", "a", "non", "-", "compiled", "LightningModule", "instead", ".", "\"", ")", "original", "=", "model", "else", ":", "raise", "ValueError", "(", "\"", "`", "model", "`", "must", "either", "be", "an", "instance", "of", "OptimizedModule", "or", "LightningModule", "\"", ")", "ctx", "=", "original", ".", "_compiler_ctx", "if", "ctx", "is", "not", "None", ":", "original", ".", "forward", "=", "ctx", "[", "\"", "original_forward", "\"", "]", "original", ".", "training_step", "=", "ctx", "[", "\"", "original_training_step", "\"", "]", "original", ".", "validation_step", "=", "ctx", "[", "\"", "original_validation_step", "\"", "]", "original", ".", "test_step", "=", "ctx", "[", "\"", "original_test_step", "\"", "]", "original", ".", "predict_step", "=", "ctx", "[", "\"", "original_predict_step", "\"", "]", "original", ".", "_compiler_ctx", "=", "None", "return", "original"], "docstring": "Returns an instance of LightningModule without any compilation optimizations from a compiled model.\r\n\r\n    .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n    This takes either a ``torch._dynamo.OptimizedModule`` returned by ``torch.compile()`` or a ``LightningModule``\r\n    returned by ``from_compiled``.\r\n\r\n    Note: this method will in-place modify the ``LightningModule`` that is passed in.", "docstring_tokens": ["returns", "an", "instance", "of", "lightningmodule", "without", "any", "compilation", "optimizations", "from", "a", "compiled", "model", "warning", "this", "is", "an", "ref", "experimental", "versioning", "experimental", "api", "feature", "this", "takes", "either", "a", "torch", "_dynamo", "optimizedmodule", "returned", "by", "torch", "compile", "or", "a", "lightningmodule", "returned", "by", "from_compiled", "note", "this", "method", "will", "in", "place", "modify", "the", "lightningmodule", "that", "is", "passed", "in"], "docstring_summary": "Returns an instance of LightningModule without any compilation optimizations from a compiled model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\compile.py", "partition": "test", "function_type": "function", "start_line": 64, "end_line": 101, "hash": "1d7ccd0f19ad068e5fc466375719e928", "complexity": 6, "parameters": ["model", "\"torch._dynamo.OptimizedModule\"]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\consolidate_checkpoint.py", "func_name": "_format_checkpoint", "original_string": "def _format_checkpoint(checkpoint: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.\"\"\"\r\n    # Rename the model key\r\n    checkpoint[\"state_dict\"] = checkpoint.pop(\"model\")\r\n\r\n    optimizer_keys = [key for key in checkpoint if re.match(\"optimizer_[0-9]+\", key)]\r\n    if not optimizer_keys:\r\n        return checkpoint\r\n\r\n    # Optimizers are saved in special keys named `optimizer_0`, `optimizer_1`, etc.\r\n    # These need to be merged back into a Python list\r\n    checkpoint[\"optimizer_states\"] = [checkpoint.pop(f\"optimizer_{opt_idx}\") for opt_idx in range(len(optimizer_keys))]\r\n    return checkpoint", "language": "python", "code": "def _format_checkpoint(checkpoint: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.\"\"\"\r\n    # Rename the model key\r\n    checkpoint[\"state_dict\"] = checkpoint.pop(\"model\")\r\n\r\n    optimizer_keys = [key for key in checkpoint if re.match(\"optimizer_[0-9]+\", key)]\r\n    if not optimizer_keys:\r\n        return checkpoint\r\n\r\n    # Optimizers are saved in special keys named `optimizer_0`, `optimizer_1`, etc.\r\n    # These need to be merged back into a Python list\r\n    checkpoint[\"optimizer_states\"] = [checkpoint.pop(f\"optimizer_{opt_idx}\") for opt_idx in range(len(optimizer_keys))]\r\n    return checkpoint", "code_tokens": ["def", "_format_checkpoint", "(", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Converts", "the", "special", "FSDP", "checkpoint", "format", "to", "the", "standard", "format", "the", "Lightning", "Trainer", "can", "load", ".", "\"", "\"", "\"", "checkpoint", "[", "\"", "state_dict", "\"", "]", "=", "checkpoint", ".", "pop", "(", "\"", "model", "\"", ")", "optimizer_keys", "=", "[", "key", "for", "key", "in", "checkpoint", "if", "re", ".", "match", "(", "\"", "optimizer_", "[", "0", "-", "9", "]", "+", "\"", ",", "key", ")", "]", "if", "not", "optimizer_keys", ":", "return", "checkpoint", "checkpoint", "[", "\"", "optimizer_states", "\"", "]", "=", "[", "checkpoint", ".", "pop", "(", "f", "\"", "optimizer_", "{", "opt_idx", "}", "\"", ")", "for", "opt_idx", "in", "range", "(", "len", "(", "optimizer_keys", ")", ")", "]", "return", "checkpoint"], "docstring": "Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.", "docstring_tokens": ["converts", "the", "special", "fsdp", "checkpoint", "format", "to", "the", "standard", "format", "the", "lightning", "trainer", "can", "load"], "docstring_summary": "Converts the special FSDP checkpoint format to the standard format the Lightning Trainer can load.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\consolidate_checkpoint.py", "partition": "test", "function_type": "function", "start_line": 9, "end_line": 21, "hash": "25386e9c5e1d8f8c1df19bd421af62bc", "complexity": 5, "parameters": ["checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "extract_batch_size", "original_string": "def extract_batch_size(batch: BType) -> int:\r\n    \"\"\"Unpack a batch to find a ``torch.Tensor``.\r\n\r\n    Returns:\r\n        ``len(tensor)`` when found, or ``1`` when it hits an empty or non iterable.\r\n\r\n    \"\"\"\r\n    error_msg = (\r\n        \"We could not infer the batch_size from the batch. Either simplify its structure\"\r\n        \" or provide the batch_size as `self.log(..., batch_size=batch_size)`.\"\r\n    )\r\n    batch_size = None\r\n    try:\r\n        for bs in _extract_batch_size(batch):\r\n            if batch_size is None:\r\n                batch_size = bs\r\n            elif batch_size != bs:\r\n                warning_cache.warn(\r\n                    \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\r\n                    f\" found is {batch_size}. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\"\r\n                )\r\n                break\r\n    except RecursionError:\r\n        raise RecursionError(error_msg)\r\n\r\n    if batch_size is None:\r\n        raise MisconfigurationException(error_msg)\r\n\r\n    return batch_size", "language": "python", "code": "def extract_batch_size(batch: BType) -> int:\r\n    \"\"\"Unpack a batch to find a ``torch.Tensor``.\r\n\r\n    Returns:\r\n        ``len(tensor)`` when found, or ``1`` when it hits an empty or non iterable.\r\n\r\n    \"\"\"\r\n    error_msg = (\r\n        \"We could not infer the batch_size from the batch. Either simplify its structure\"\r\n        \" or provide the batch_size as `self.log(..., batch_size=batch_size)`.\"\r\n    )\r\n    batch_size = None\r\n    try:\r\n        for bs in _extract_batch_size(batch):\r\n            if batch_size is None:\r\n                batch_size = bs\r\n            elif batch_size != bs:\r\n                warning_cache.warn(\r\n                    \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\r\n                    f\" found is {batch_size}. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\"\r\n                )\r\n                break\r\n    except RecursionError:\r\n        raise RecursionError(error_msg)\r\n\r\n    if batch_size is None:\r\n        raise MisconfigurationException(error_msg)\r\n\r\n    return batch_size", "code_tokens": ["def", "extract_batch_size", "(", "batch", ":", "BType", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Unpack", "a", "batch", "to", "find", "a", "`", "`", "torch", ".", "Tensor", "`", "`", ".", "Returns", ":", "`", "`", "len", "(", "tensor", ")", "`", "`", "when", "found", ",", "or", "`", "`", "1", "`", "`", "when", "it", "hits", "an", "empty", "or", "non", "iterable", ".", "\"", "\"", "\"", "error_msg", "=", "(", "\"", "We", "could", "not", "infer", "the", "batch_size", "from", "the", "batch", ".", "Either", "simplify", "its", "structure", "\"", "\"", "or", "provide", "the", "batch_size", "as", "`", "self", ".", "log", "(", ".", ".", ".", ",", "batch_size", "=", "batch_size", ")", "`", ".", "\"", ")", "batch_size", "=", "None", "try", ":", "for", "bs", "in", "_extract_batch_size", "(", "batch", ")", ":", "if", "batch_size", "is", "None", ":", "batch_size", "=", "bs", "elif", "batch_size", "!", "=", "bs", ":", "warning_cache", ".", "warn", "(", "\"", "Trying", "to", "infer", "the", "`", "batch_size", "`", "from", "an", "ambiguous", "collection", ".", "The", "batch", "size", "we", "\"", "f", "\"", "found", "is", "{", "batch_size", "}", ".", "To", "avoid", "any", "miscalculations", ",", "use", "`", "self", ".", "log", "(", ".", ".", ".", ",", "batch_size", "=", "batch_size", ")", "`", ".", "\"", ")", "break", "except", "RecursionError", ":", "raise", "RecursionError", "(", "error_msg", ")", "if", "batch_size", "is", "None", ":", "raise", "MisconfigurationException", "(", "error_msg", ")", "return", "batch_size"], "docstring": "Unpack a batch to find a ``torch.Tensor``.\r\n\r\n    Returns:\r\n        ``len(tensor)`` when found, or ``1`` when it hits an empty or non iterable.", "docstring_tokens": ["unpack", "a", "batch", "to", "find", "a", "torch", "tensor", "returns", "len", "tensor", "when", "found", "or", "1", "when", "it", "hits", "an", "empty", "or", "non", "iterable"], "docstring_summary": "Unpack a batch to find a ``torch.Tensor``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\data.py", "partition": "test", "function_type": "function", "start_line": 61, "end_line": 89, "hash": "97334f127dff15712a861645cf98a08c", "complexity": 6, "parameters": ["batch"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "has_len_all_ranks", "original_string": "def has_len_all_ranks(\r\n    dataloader: object,\r\n    strategy: \"pl.strategies.Strategy\",\r\n    allow_zero_length_dataloader_with_multiple_devices: bool = False,\r\n) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented on all ranks.\"\"\"\r\n    local_length = sized_len(dataloader)\r\n    if local_length is None:\r\n        # __len__ is not defined, skip these checks\r\n        return False\r\n\r\n    total_length = strategy.reduce(torch.tensor(local_length, device=strategy.root_device), reduce_op=\"sum\")\r\n    if total_length == 0:\r\n        rank_zero_warn(\r\n            f\"Total length of `{type(dataloader).__name__}` across ranks is zero.\"\r\n            \" Please make sure this was your intention.\"\r\n        )\r\n    if total_length > 0 and local_length == 0:\r\n        dataloader_cls_name = type(dataloader).__name__\r\n        if not allow_zero_length_dataloader_with_multiple_devices:\r\n            raise RuntimeError(\r\n                f\"`{dataloader_cls_name}` within local rank has zero length.\"\r\n                \" Please make sure that it returns at least 1 batch.\"\r\n            )\r\n        rank_zero_warn(\r\n            f\"Total length of `{dataloader_cls_name}` across ranks is zero, but local rank has zero\"\r\n            \" length. Please be cautious of uneven batch length.\"\r\n        )\r\n\r\n    if has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return True", "language": "python", "code": "def has_len_all_ranks(\r\n    dataloader: object,\r\n    strategy: \"pl.strategies.Strategy\",\r\n    allow_zero_length_dataloader_with_multiple_devices: bool = False,\r\n) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented on all ranks.\"\"\"\r\n    local_length = sized_len(dataloader)\r\n    if local_length is None:\r\n        # __len__ is not defined, skip these checks\r\n        return False\r\n\r\n    total_length = strategy.reduce(torch.tensor(local_length, device=strategy.root_device), reduce_op=\"sum\")\r\n    if total_length == 0:\r\n        rank_zero_warn(\r\n            f\"Total length of `{type(dataloader).__name__}` across ranks is zero.\"\r\n            \" Please make sure this was your intention.\"\r\n        )\r\n    if total_length > 0 and local_length == 0:\r\n        dataloader_cls_name = type(dataloader).__name__\r\n        if not allow_zero_length_dataloader_with_multiple_devices:\r\n            raise RuntimeError(\r\n                f\"`{dataloader_cls_name}` within local rank has zero length.\"\r\n                \" Please make sure that it returns at least 1 batch.\"\r\n            )\r\n        rank_zero_warn(\r\n            f\"Total length of `{dataloader_cls_name}` across ranks is zero, but local rank has zero\"\r\n            \" length. Please be cautious of uneven batch length.\"\r\n        )\r\n\r\n    if has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return True", "code_tokens": ["def", "has_len_all_ranks", "(", "dataloader", ":", "object", ",", "strategy", ":", "\"", "pl", ".", "strategies", ".", "Strategy", "\"", ",", "allow_zero_length_dataloader_with_multiple_devices", ":", "bool", "=", "False", ",", ")", "-", ">", "TypeGuard", "[", "Sized", "]", ":", "\"", "\"", "\"", "Checks", "if", "a", "given", "object", "has", "`", "`", "__len__", "`", "`", "method", "implemented", "on", "all", "ranks", ".", "\"", "\"", "\"", "local_length", "=", "sized_len", "(", "dataloader", ")", "if", "local_length", "is", "None", ":", "return", "False", "total_length", "=", "strategy", ".", "reduce", "(", "torch", ".", "tensor", "(", "local_length", ",", "device", "=", "strategy", ".", "root_device", ")", ",", "reduce_op", "=", "\"", "sum", "\"", ")", "if", "total_length", "=", "=", "0", ":", "rank_zero_warn", "(", "f", "\"", "Total", "length", "of", "`", "{", "type", "(", "dataloader", ")", ".", "__name__", "}", "`", "across", "ranks", "is", "zero", ".", "\"", "\"", "Please", "make", "sure", "this", "was", "your", "intention", ".", "\"", ")", "if", "total_length", ">", "0", "and", "local_length", "=", "=", "0", ":", "dataloader_cls_name", "=", "type", "(", "dataloader", ")", ".", "__name__", "if", "not", "allow_zero_length_dataloader_with_multiple_devices", ":", "raise", "RuntimeError", "(", "f", "\"", "`", "{", "dataloader_cls_name", "}", "`", "within", "local", "rank", "has", "zero", "length", ".", "\"", "\"", "Please", "make", "sure", "that", "it", "returns", "at", "least", "1", "batch", ".", "\"", ")", "rank_zero_warn", "(", "f", "\"", "Total", "length", "of", "`", "{", "dataloader_cls_name", "}", "`", "across", "ranks", "is", "zero", ",", "but", "local", "rank", "has", "zero", "\"", "\"", "length", ".", "Please", "be", "cautious", "of", "uneven", "batch", "length", ".", "\"", ")", "if", "has_iterable_dataset", "(", "dataloader", ")", ":", "rank_zero_warn", "(", "\"", "Your", "`", "IterableDataset", "`", "has", "`", "__len__", "`", "defined", ".", "\"", "\"", "In", "combination", "with", "multi", "-", "process", "data", "loading", "(", "when", "num_workers", ">", "1", ")", ",", "\"", "\"", "`", "__len__", "`", "could", "be", "inaccurate", "if", "each", "worker", "is", "not", "configured", "independently", "\"", "\"", "to", "avoid", "having", "duplicate", "data", ".", "\"", ")", "return", "True"], "docstring": "Checks if a given object has ``__len__`` method implemented on all ranks.", "docstring_tokens": ["checks", "if", "a", "given", "object", "has", "__len__", "method", "implemented", "on", "all", "ranks"], "docstring_summary": "Checks if a given object has ``__len__`` method implemented on all ranks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\data.py", "partition": "test", "function_type": "function", "start_line": 92, "end_line": 128, "hash": "8a0faa375f2ff36e42c4d5e6f5fcc0df", "complexity": 7, "parameters": ["dataloader", "strategy", "allow_zero_length_dataloader_with_multiple_devices"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\data.py", "func_name": "_dataloader_init_kwargs_resolve_sampler", "original_string": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n    mode: Optional[RunningStage] = None,\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\r\n\r\n    If the dataloader is being used for prediction, the sampler will be wrapped into an `_IndexBatchSamplerWrapper`, so\r\n    Lightning can keep track of its indices.\r\n\r\n    \"\"\"\r\n    is_predicting = mode == RunningStage.PREDICTING\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n    batch_sampler_cls = type(batch_sampler)\r\n\r\n    if batch_sampler is not None and (batch_sampler_cls is not BatchSampler or is_predicting):\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            # This is a PyTorch `BatchSampler` subclass for which we captured the init args\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            if is_predicting:\r\n                success, args, kwargs = _replace_value_in_saved_args(\r\n                    \"drop_last\", False, args, kwargs, default_kwargs, arg_names\r\n                )\r\n                if not success:\r\n                    rank_zero_warn(\r\n                        f\"Trying to inject `drop_last=False` into batch sampler since you are predicting, however \"\r\n                        f\"it seems the class `{batch_sampler_cls.__qualname__}` does not support it. \"\r\n                        \"Your predictions might be incomplete. To mitigate this, expose `drop_last` in \"\r\n                        \"the `__init__` method of your custom class.\"\r\n                    )\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            # This is a sampler for which we could not capture the init args, but it kinda looks like a batch sampler\r\n            # even if it does not inherit from PyTorch's interface.\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=(False if is_predicting else batch_sampler.drop_last),\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    # an unexpected `TypeError`, continue failure\r\n                    raise\r\n\r\n                # There could either be too few or too many arguments. Customizing the message based on this doesn't\r\n                # make much sense since our MisconfigurationException is going to be raised from the original one.\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler` and\"\r\n                    \" instantiate your custom batch sampler inside the `*_dataloader` hook of your module,\"\r\n                    \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                    \" responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        elif is_predicting:\r\n            rank_zero_warn(\r\n                f\"You are using a custom batch sampler `{batch_sampler_cls.__qualname__}` for prediction.\"\r\n                \" Lightning would normally set `drop_last=False` to ensure all samples are returned, but for\"\r\n                \" custom samplers it can't guarantee this. Make sure your sampler is configured correctly to return\"\r\n                \" all indices.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        else:\r\n            # The sampler is not a PyTorch `BatchSampler`, we don't know how to inject a custom sampler or\r\n            # how to adjust the `drop_last` value\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                \" responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        if is_predicting:\r\n            batch_sampler = _IndexBatchSamplerWrapper(batch_sampler)\r\n\r\n        # batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "language": "python", "code": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n    mode: Optional[RunningStage] = None,\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\r\n\r\n    If the dataloader is being used for prediction, the sampler will be wrapped into an `_IndexBatchSamplerWrapper`, so\r\n    Lightning can keep track of its indices.\r\n\r\n    \"\"\"\r\n    is_predicting = mode == RunningStage.PREDICTING\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n    batch_sampler_cls = type(batch_sampler)\r\n\r\n    if batch_sampler is not None and (batch_sampler_cls is not BatchSampler or is_predicting):\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            # This is a PyTorch `BatchSampler` subclass for which we captured the init args\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            if is_predicting:\r\n                success, args, kwargs = _replace_value_in_saved_args(\r\n                    \"drop_last\", False, args, kwargs, default_kwargs, arg_names\r\n                )\r\n                if not success:\r\n                    rank_zero_warn(\r\n                        f\"Trying to inject `drop_last=False` into batch sampler since you are predicting, however \"\r\n                        f\"it seems the class `{batch_sampler_cls.__qualname__}` does not support it. \"\r\n                        \"Your predictions might be incomplete. To mitigate this, expose `drop_last` in \"\r\n                        \"the `__init__` method of your custom class.\"\r\n                    )\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            # This is a sampler for which we could not capture the init args, but it kinda looks like a batch sampler\r\n            # even if it does not inherit from PyTorch's interface.\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=(False if is_predicting else batch_sampler.drop_last),\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    # an unexpected `TypeError`, continue failure\r\n                    raise\r\n\r\n                # There could either be too few or too many arguments. Customizing the message based on this doesn't\r\n                # make much sense since our MisconfigurationException is going to be raised from the original one.\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler` and\"\r\n                    \" instantiate your custom batch sampler inside the `*_dataloader` hook of your module,\"\r\n                    \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                    \" responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        elif is_predicting:\r\n            rank_zero_warn(\r\n                f\"You are using a custom batch sampler `{batch_sampler_cls.__qualname__}` for prediction.\"\r\n                \" Lightning would normally set `drop_last=False` to ensure all samples are returned, but for\"\r\n                \" custom samplers it can't guarantee this. Make sure your sampler is configured correctly to return\"\r\n                \" all indices.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n        else:\r\n            # The sampler is not a PyTorch `BatchSampler`, we don't know how to inject a custom sampler or\r\n            # how to adjust the `drop_last` value\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set `Trainer(use_distributed_sampler=False)`. If you choose the latter, you will be\"\r\n                \" responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        if is_predicting:\r\n            batch_sampler = _IndexBatchSamplerWrapper(batch_sampler)\r\n\r\n        # batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "code_tokens": ["def", "_dataloader_init_kwargs_resolve_sampler", "(", "dataloader", ":", "DataLoader", ",", "sampler", ":", "Union", "[", "Sampler", ",", "Iterable", "]", ",", "mode", ":", "Optional", "[", "RunningStage", "]", "=", "None", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "This", "function", "is", "used", "to", "handle", "the", "sampler", ",", "batch_sampler", "arguments", "associated", "within", "a", "DataLoader", "for", "its", "re", "-", "instantiation", ".", "If", "the", "dataloader", "is", "being", "used", "for", "prediction", ",", "the", "sampler", "will", "be", "wrapped", "into", "an", "`", "_IndexBatchSamplerWrapper", "`", ",", "so", "Lightning", "can", "keep", "track", "of", "its", "indices", ".", "\"", "\"", "\"", "is_predicting", "=", "mode", "=", "=", "RunningStage", ".", "PREDICTING", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "\"", "batch_sampler", "\"", ")", "batch_sampler_cls", "=", "type", "(", "batch_sampler", ")", "if", "batch_sampler", "is", "not", "None", "and", "(", "batch_sampler_cls", "is", "not", "BatchSampler", "or", "is_predicting", ")", ":", "if", "hasattr", "(", "batch_sampler", ",", "\"", "__pl_saved_args", "\"", ")", ":", "args", "=", "batch_sampler", ".", "__pl_saved_args", "kwargs", "=", "batch_sampler", ".", "__pl_saved_kwargs", "default_kwargs", "=", "batch_sampler", ".", "__pl_saved_default_kwargs", "arg_names", "=", "batch_sampler", ".", "__pl_saved_arg_names", "if", "is_predicting", ":", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "\"", "drop_last", "\"", ",", "False", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "rank_zero_warn", "(", "f", "\"", "Trying", "to", "inject", "`", "drop_last", "=", "False", "`", "into", "batch", "sampler", "since", "you", "are", "predicting", ",", "however", "\"", "f", "\"", "it", "seems", "the", "class", "`", "{", "batch_sampler_cls", ".", "__qualname__", "}", "`", "does", "not", "support", "it", ".", "\"", "\"", "Your", "predictions", "might", "be", "incomplete", ".", "To", "mitigate", "this", ",", "expose", "`", "drop_last", "`", "in", "\"", "\"", "the", "`", "__init__", "`", "method", "of", "your", "custom", "class", ".", "\"", ")", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "\"", "sampler", "\"", ",", "sampler", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "raise", "TypeError", "(", "\"", "Trying", "to", "inject", "a", "modified", "sampler", "into", "the", "batch", "sampler", ";", "however", ",", "it", "seems", "the", "class", "\"", "f", "\"", "`", "{", "batch_sampler_cls", ".", "__qualname__", "}", "`", "does", "not", "have", "an", "argument", "called", "`", "sampler", ".", "`", "To", "mitigate", "\"", "\"", "this", ",", "expose", "an", "argument", "`", "sampler", "`", "in", "the", "`", "__init__", "`", "method", "of", "your", "custom", "class", ".", "\"", ")", "batch_sampler", "=", "_reinstantiate_wrapped_cls", "(", "batch_sampler", ",", "*", "args", ",", "*", "*", "kwargs", ")", "elif", "hasattr", "(", "batch_sampler", ",", "\"", "batch_size", "\"", ")", "and", "hasattr", "(", "batch_sampler", ",", "\"", "drop_last", "\"", ")", ":", "try", ":", "batch_sampler", "=", "batch_sampler_cls", "(", "sampler", ",", "batch_size", "=", "batch_sampler", ".", "batch_size", ",", "drop_last", "=", "(", "False", "if", "is_predicting", "else", "batch_sampler", ".", "drop_last", ")", ",", ")", "except", "TypeError", "as", "ex", ":", "import", "re", "match", "=", "re", ".", "match", "(", "r", "\"", ".", "*", "__init__", "\\", "(", "\\", ")", "(", "got", "multiple", "values", ")", "|", "(", "missing", "\\", "d", "required", ")", "\"", ",", "str", "(", "ex", ")", ")", "if", "not", "match", ":", "raise", "raise", "TypeError", "(", "\"", "Lightning", "can", "'", "t", "inject", "a", "(", "distributed", ")", "sampler", "into", "your", "batch", "sampler", ",", "because", "it", "doesn", "'", "t", "\"", "\"", "subclass", "PyTorch", "'", "s", "`", "BatchSampler", "`", ".", "To", "mitigate", "this", ",", "either", "follow", "the", "API", "of", "`", "BatchSampler", "`", "and", "\"", "\"", "instantiate", "your", "custom", "batch", "sampler", "inside", "the", "`", "*", "_dataloader", "`", "hook", "of", "your", "module", ",", "\"", "\"", "or", "set", "`", "Trainer", "(", "use_distributed_sampler", "=", "False", ")", "`", ".", "If", "you", "choose", "the", "latter", ",", "you", "will", "be", "\"", "\"", "responsible", "for", "handling", "the", "distributed", "sampling", "within", "your", "batch", "sampler", ".", "\"", ")", "from", "ex", "elif", "is_predicting", ":", "rank_zero_warn", "(", "f", "\"", "You", "are", "using", "a", "custom", "batch", "sampler", "`", "{", "batch_sampler_cls", ".", "__qualname__", "}", "`", "for", "prediction", ".", "\"", "\"", "Lightning", "would", "normally", "set", "`", "drop_last", "=", "False", "`", "to", "ensure", "all", "samples", "are", "returned", ",", "but", "for", "\"", "\"", "custom", "samplers", "it", "can", "'", "t", "guarantee", "this", ".", "Make", "sure", "your", "sampler", "is", "configured", "correctly", "to", "return", "\"", "\"", "all", "indices", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "else", ":", "raise", "TypeError", "(", "\"", "Lightning", "can", "'", "t", "inject", "a", "(", "distributed", ")", "sampler", "into", "your", "batch", "sampler", ",", "because", "it", "doesn", "'", "t", "\"", "\"", "subclass", "PyTorch", "'", "s", "`", "BatchSampler", "`", ".", "To", "mitigate", "this", ",", "either", "follow", "the", "API", "of", "`", "BatchSampler", "`", "\"", "\"", "or", "set", "`", "Trainer", "(", "use_distributed_sampler", "=", "False", ")", "`", ".", "If", "you", "choose", "the", "latter", ",", "you", "will", "be", "\"", "\"", "responsible", "for", "handling", "the", "distributed", "sampling", "within", "your", "batch", "sampler", ".", "\"", ")", "if", "is_predicting", ":", "batch_sampler", "=", "_IndexBatchSamplerWrapper", "(", "batch_sampler", ")", "return", "{", "\"", "sampler", "\"", ":", "None", ",", "\"", "shuffle", "\"", ":", "False", ",", "\"", "batch_sampler", "\"", ":", "batch_sampler", ",", "\"", "batch_size", "\"", ":", "1", ",", "\"", "drop_last", "\"", ":", "False", ",", "}", "return", "{", "\"", "sampler", "\"", ":", "sampler", ",", "\"", "shuffle", "\"", ":", "False", ",", "\"", "batch_sampler", "\"", ":", "None", "}"], "docstring": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\r\n\r\n    If the dataloader is being used for prediction, the sampler will be wrapped into an `_IndexBatchSamplerWrapper`, so\r\n    Lightning can keep track of its indices.", "docstring_tokens": ["this", "function", "is", "used", "to", "handle", "the", "sampler", "batch_sampler", "arguments", "associated", "within", "a", "dataloader", "for", "its", "re", "instantiation", "if", "the", "dataloader", "is", "being", "used", "for", "prediction", "the", "sampler", "will", "be", "wrapped", "into", "an", "_indexbatchsamplerwrapper", "so", "lightning", "can", "keep", "track", "of", "its", "indices"], "docstring_summary": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\data.py", "partition": "test", "function_type": "function", "start_line": 232, "end_line": 335, "hash": "a1b049b86f58054c46ccfbe657694af1", "complexity": 15, "parameters": ["dataloader", "sampler", "Iterable]", "mode"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\deepspeed.py", "func_name": "convert_zero_checkpoint_to_fp32_state_dict", "original_string": "def convert_zero_checkpoint_to_fp32_state_dict(\r\n    checkpoint_dir: _PATH, output_file: _PATH, tag: str | None = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with\r\n    ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed. It gets copied into the top\r\n    level checkpoint dir, so the user can easily do the conversion at any point in the future. Once extracted, the\r\n    weights don't require DeepSpeed and can be used in any application. Additionally the script has been modified to\r\n    ensure we keep the lightning state inside the state dict for being able to run\r\n    ``LightningModule.load_from_checkpoint('...')```.\r\n\r\n    Args:\r\n        checkpoint_dir: path to the desired checkpoint folder.\r\n            (one that contains the tag-folder, like ``global_step14``)\r\n        output_file: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\r\n        tag: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt\r\n            to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\r\n\r\n    Examples::\r\n\r\n        # Lightning deepspeed has saved a directory instead of a file\r\n        convert_zero_checkpoint_to_fp32_state_dict(\r\n            \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\",\r\n            \"lightning_model.pt\"\r\n        )\r\n\r\n    \"\"\"\r\n    if not _DEEPSPEED_AVAILABLE:\r\n        raise ModuleNotFoundError(str(_DEEPSPEED_AVAILABLE))\r\n\r\n    from deepspeed.utils.zero_to_fp32 import (\r\n        get_fp32_state_dict_from_zero_checkpoint,\r\n        get_model_state_file,\r\n        get_optim_files,\r\n    )\r\n\r\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\r\n\r\n    # additional logic to ensure we keep the lightning state dict as well from rank 0.\r\n    deepspeed_states = [\r\n        \"module\",\r\n        \"optimizer\",\r\n        \"lr_scheduler\",\r\n        \"csr_tensor_module_names\",\r\n        \"skipped_steps\",\r\n        \"global_steps\",\r\n        \"dp_world_size\",\r\n        \"mp_world_size\",\r\n    ]\r\n    checkpoint_dir = ds_checkpoint_dir(checkpoint_dir)\r\n    optim_files = get_optim_files(checkpoint_dir)\r\n    optim_state = torch.load(optim_files[0], map_location=CPU_DEVICE, weights_only=False)\r\n    zero_stage = optim_state[\"optimizer_state_dict\"][\"zero_stage\"]\r\n    model_file = get_model_state_file(checkpoint_dir, zero_stage)\r\n    client_state = torch.load(model_file, map_location=CPU_DEVICE, weights_only=False)\r\n    client_state = {key: value for key, value in client_state.items() if key not in deepspeed_states}\r\n    # State dict keys will include reference to wrapper _LightningModuleWrapperBase in old checkpoints created in\r\n    # Lightning version < 2.1. Delete the `_forward_module` prefix before saving.\r\n    state_dict = {_remove_prefix(k, \"_forward_module.\"): state_dict[k] for k in state_dict}\r\n    client_state[\"state_dict\"] = state_dict\r\n\r\n    print(f\"Saving fp32 state dict to {output_file}\")\r\n    torch.save(client_state, output_file)\r\n\r\n    return client_state", "language": "python", "code": "def convert_zero_checkpoint_to_fp32_state_dict(\r\n    checkpoint_dir: _PATH, output_file: _PATH, tag: str | None = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with\r\n    ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed. It gets copied into the top\r\n    level checkpoint dir, so the user can easily do the conversion at any point in the future. Once extracted, the\r\n    weights don't require DeepSpeed and can be used in any application. Additionally the script has been modified to\r\n    ensure we keep the lightning state inside the state dict for being able to run\r\n    ``LightningModule.load_from_checkpoint('...')```.\r\n\r\n    Args:\r\n        checkpoint_dir: path to the desired checkpoint folder.\r\n            (one that contains the tag-folder, like ``global_step14``)\r\n        output_file: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\r\n        tag: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt\r\n            to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\r\n\r\n    Examples::\r\n\r\n        # Lightning deepspeed has saved a directory instead of a file\r\n        convert_zero_checkpoint_to_fp32_state_dict(\r\n            \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\",\r\n            \"lightning_model.pt\"\r\n        )\r\n\r\n    \"\"\"\r\n    if not _DEEPSPEED_AVAILABLE:\r\n        raise ModuleNotFoundError(str(_DEEPSPEED_AVAILABLE))\r\n\r\n    from deepspeed.utils.zero_to_fp32 import (\r\n        get_fp32_state_dict_from_zero_checkpoint,\r\n        get_model_state_file,\r\n        get_optim_files,\r\n    )\r\n\r\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\r\n\r\n    # additional logic to ensure we keep the lightning state dict as well from rank 0.\r\n    deepspeed_states = [\r\n        \"module\",\r\n        \"optimizer\",\r\n        \"lr_scheduler\",\r\n        \"csr_tensor_module_names\",\r\n        \"skipped_steps\",\r\n        \"global_steps\",\r\n        \"dp_world_size\",\r\n        \"mp_world_size\",\r\n    ]\r\n    checkpoint_dir = ds_checkpoint_dir(checkpoint_dir)\r\n    optim_files = get_optim_files(checkpoint_dir)\r\n    optim_state = torch.load(optim_files[0], map_location=CPU_DEVICE, weights_only=False)\r\n    zero_stage = optim_state[\"optimizer_state_dict\"][\"zero_stage\"]\r\n    model_file = get_model_state_file(checkpoint_dir, zero_stage)\r\n    client_state = torch.load(model_file, map_location=CPU_DEVICE, weights_only=False)\r\n    client_state = {key: value for key, value in client_state.items() if key not in deepspeed_states}\r\n    # State dict keys will include reference to wrapper _LightningModuleWrapperBase in old checkpoints created in\r\n    # Lightning version < 2.1. Delete the `_forward_module` prefix before saving.\r\n    state_dict = {_remove_prefix(k, \"_forward_module.\"): state_dict[k] for k in state_dict}\r\n    client_state[\"state_dict\"] = state_dict\r\n\r\n    print(f\"Saving fp32 state dict to {output_file}\")\r\n    torch.save(client_state, output_file)\r\n\r\n    return client_state", "code_tokens": ["def", "convert_zero_checkpoint_to_fp32_state_dict", "(", "checkpoint_dir", ":", "_PATH", ",", "output_file", ":", "_PATH", ",", "tag", ":", "str", "|", "None", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Convert", "ZeRO", "2", "or", "3", "checkpoint", "into", "a", "single", "fp32", "consolidated", "`", "`", "state_dict", "`", "`", "file", "that", "can", "be", "loaded", "with", "`", "`", "torch", ".", "load", "(", "file", ")", "`", "`", "+", "`", "`", "load_state_dict", "(", ")", "`", "`", "and", "used", "for", "training", "without", "DeepSpeed", ".", "It", "gets", "copied", "into", "the", "top", "level", "checkpoint", "dir", ",", "so", "the", "user", "can", "easily", "do", "the", "conversion", "at", "any", "point", "in", "the", "future", ".", "Once", "extracted", ",", "the", "weights", "don", "'", "t", "require", "DeepSpeed", "and", "can", "be", "used", "in", "any", "application", ".", "Additionally", "the", "script", "has", "been", "modified", "to", "ensure", "we", "keep", "the", "lightning", "state", "inside", "the", "state", "dict", "for", "being", "able", "to", "run", "`", "`", "LightningModule", ".", "load_from_checkpoint", "(", "'", ".", ".", ".", "'", ")", "`", "`", "`", ".", "Args", ":", "checkpoint_dir", ":", "path", "to", "the", "desired", "checkpoint", "folder", ".", "(", "one", "that", "contains", "the", "tag", "-", "folder", ",", "like", "`", "`", "global_step14", "`", "`", ")", "output_file", ":", "path", "to", "the", "pytorch", "fp32", "state_dict", "output", "file", "(", "e", ".", "g", ".", "path", "/", "pytorch_model", ".", "bin", ")", "tag", ":", "checkpoint", "tag", "used", "as", "a", "unique", "identifier", "for", "checkpoint", ".", "If", "not", "provided", "will", "attempt", "to", "load", "tag", "in", "the", "file", "named", "`", "`", "latest", "`", "`", "in", "the", "checkpoint", "folder", ",", "e", ".", "g", ".", ",", "`", "`", "global_step14", "`", "`", "Examples", ":", ":", "convert_zero_checkpoint_to_fp32_state_dict", "(", "\"", "lightning_logs", "/", "version_0", "/", "checkpoints", "/", "epoch", "=", "0", "-", "step", "=", "0", ".", "ckpt", "/", "\"", ",", "\"", "lightning_model", ".", "pt", "\"", ")", "\"", "\"", "\"", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "str", "(", "_DEEPSPEED_AVAILABLE", ")", ")", "from", "deepspeed", ".", "utils", ".", "zero_to_fp32", "import", "(", "get_fp32_state_dict_from_zero_checkpoint", ",", "get_model_state_file", ",", "get_optim_files", ",", ")", "state_dict", "=", "get_fp32_state_dict_from_zero_checkpoint", "(", "checkpoint_dir", ",", "tag", ")", "deepspeed_states", "=", "[", "\"", "module", "\"", ",", "\"", "optimizer", "\"", ",", "\"", "lr_scheduler", "\"", ",", "\"", "csr_tensor_module_names", "\"", ",", "\"", "skipped_steps", "\"", ",", "\"", "global_steps", "\"", ",", "\"", "dp_world_size", "\"", ",", "\"", "mp_world_size", "\"", ",", "]", "checkpoint_dir", "=", "ds_checkpoint_dir", "(", "checkpoint_dir", ")", "optim_files", "=", "get_optim_files", "(", "checkpoint_dir", ")", "optim_state", "=", "torch", ".", "load", "(", "optim_files", "[", "0", "]", ",", "map_location", "=", "CPU_DEVICE", ",", "weights_only", "=", "False", ")", "zero_stage", "=", "optim_state", "[", "\"", "optimizer_state_dict", "\"", "]", "[", "\"", "zero_stage", "\"", "]", "model_file", "=", "get_model_state_file", "(", "checkpoint_dir", ",", "zero_stage", ")", "client_state", "=", "torch", ".", "load", "(", "model_file", ",", "map_location", "=", "CPU_DEVICE", ",", "weights_only", "=", "False", ")", "client_state", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "client_state", ".", "items", "(", ")", "if", "key", "not", "in", "deepspeed_states", "}", "state_dict", "=", "{", "_remove_prefix", "(", "k", ",", "\"", "_forward_module", ".", "\"", ")", ":", "state_dict", "[", "k", "]", "for", "k", "in", "state_dict", "}", "client_state", "[", "\"", "state_dict", "\"", "]", "=", "state_dict", "print", "(", "f", "\"", "Saving", "fp32", "state", "dict", "to", "{", "output_file", "}", "\"", ")", "torch", ".", "save", "(", "client_state", ",", "output_file", ")", "return", "client_state"], "docstring": "Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with\r\n    ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed. It gets copied into the top\r\n    level checkpoint dir, so the user can easily do the conversion at any point in the future. Once extracted, the\r\n    weights don't require DeepSpeed and can be used in any application. Additionally the script has been modified to\r\n    ensure we keep the lightning state inside the state dict for being able to run\r\n    ``LightningModule.load_from_checkpoint('...')```.\r\n\r\n    Args:\r\n        checkpoint_dir: path to the desired checkpoint folder.\r\n            (one that contains the tag-folder, like ``global_step14``)\r\n        output_file: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\r\n        tag: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt\r\n            to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\r\n\r\n    Examples::\r\n\r\n        # Lightning deepspeed has saved a directory instead of a file\r\n        convert_zero_checkpoint_to_fp32_state_dict(\r\n            \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\",\r\n            \"lightning_model.pt\"\r\n        )", "docstring_tokens": ["convert", "zero", "2", "or", "3", "checkpoint", "into", "a", "single", "fp32", "consolidated", "state_dict", "file", "that", "can", "be", "loaded", "with", "torch", "load", "file", "load_state_dict", "and", "used", "for", "training", "without", "deepspeed", "it", "gets", "copied", "into", "the", "top", "level", "checkpoint", "dir", "so", "the", "user", "can", "easily", "do", "the", "conversion", "at", "any", "point", "in", "the", "future", "once", "extracted", "the", "weights", "don", "t", "require", "deepspeed", "and", "can", "be", "used", "in", "any", "application", "additionally", "the", "script", "has", "been", "modified", "to", "ensure", "we", "keep", "the", "lightning", "state", "inside", "the", "state", "dict", "for", "being", "able", "to", "run", "lightningmodule", "load_from_checkpoint", "args", "checkpoint_dir", "path", "to", "the", "desired", "checkpoint", "folder", "one", "that", "contains", "the", "tag", "folder", "like", "global_step14", "output_file", "path", "to", "the", "pytorch", "fp32", "state_dict", "output", "file", "e", "g", "path", "pytorch_model", "bin", "tag", "checkpoint", "tag", "used", "as", "a", "unique", "identifier", "for", "checkpoint", "if", "not", "provided", "will", "attempt", "to", "load", "tag", "in", "the", "file", "named", "latest", "in", "the", "checkpoint", "folder", "e", "g", "global_step14", "examples", "lightning", "deepspeed", "has", "saved", "a", "directory", "instead", "of", "a", "file", "convert_zero_checkpoint_to_fp32_state_dict", "lightning_logs", "version_0", "checkpoints", "epoch", "0", "step", "0", "ckpt", "lightning_model", "pt"], "docstring_summary": "Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be loaded with", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\deepspeed.py", "partition": "test", "function_type": "function", "start_line": 45, "end_line": 108, "hash": "4cfb7a643e48ee9451f5f6f4293dfef0", "complexity": 5, "parameters": ["checkpoint_dir", "output_file", "tag"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\grads.py", "func_name": "grad_norm", "original_string": "def grad_norm(module: Module, norm_type: Union[float, int, str], group_separator: str = \"/\") -> dict[str, float]:\r\n    \"\"\"Compute each parameter's gradient's norm and their overall norm.\r\n\r\n    The overall norm is computed over all gradients together, as if they\r\n    were concatenated into a single vector.\r\n\r\n    Args:\r\n        module: :class:`torch.nn.Module` to inspect.\r\n        norm_type: The type of the used p-norm, cast to float if necessary.\r\n            Can be ``'inf'`` for infinity norm.\r\n        group_separator: The separator string used by the logger to group\r\n            the gradients norms in their own subfolder instead of the logs one.\r\n\r\n    Return:\r\n        norms: The dictionary of p-norms of each parameter's gradient and\r\n            a special entry for the total p-norm of the gradients viewed\r\n            as a single vector.\r\n\r\n    \"\"\"\r\n    norm_type = float(norm_type)\r\n    if norm_type <= 0:\r\n        raise ValueError(f\"`norm_type` must be a positive number or 'inf' (infinity norm). Got {norm_type}\")\r\n\r\n    norms = {\r\n        f\"grad_{norm_type}_norm{group_separator}{name}\": p.grad.data.norm(norm_type)\r\n        for name, p in module.named_parameters()\r\n        if p.grad is not None\r\n    }\r\n    if norms:\r\n        total_norm = torch.tensor(list(norms.values())).norm(norm_type)\r\n        norms[f\"grad_{norm_type}_norm_total\"] = total_norm\r\n    return norms", "language": "python", "code": "def grad_norm(module: Module, norm_type: Union[float, int, str], group_separator: str = \"/\") -> dict[str, float]:\r\n    \"\"\"Compute each parameter's gradient's norm and their overall norm.\r\n\r\n    The overall norm is computed over all gradients together, as if they\r\n    were concatenated into a single vector.\r\n\r\n    Args:\r\n        module: :class:`torch.nn.Module` to inspect.\r\n        norm_type: The type of the used p-norm, cast to float if necessary.\r\n            Can be ``'inf'`` for infinity norm.\r\n        group_separator: The separator string used by the logger to group\r\n            the gradients norms in their own subfolder instead of the logs one.\r\n\r\n    Return:\r\n        norms: The dictionary of p-norms of each parameter's gradient and\r\n            a special entry for the total p-norm of the gradients viewed\r\n            as a single vector.\r\n\r\n    \"\"\"\r\n    norm_type = float(norm_type)\r\n    if norm_type <= 0:\r\n        raise ValueError(f\"`norm_type` must be a positive number or 'inf' (infinity norm). Got {norm_type}\")\r\n\r\n    norms = {\r\n        f\"grad_{norm_type}_norm{group_separator}{name}\": p.grad.data.norm(norm_type)\r\n        for name, p in module.named_parameters()\r\n        if p.grad is not None\r\n    }\r\n    if norms:\r\n        total_norm = torch.tensor(list(norms.values())).norm(norm_type)\r\n        norms[f\"grad_{norm_type}_norm_total\"] = total_norm\r\n    return norms", "code_tokens": ["def", "grad_norm", "(", "module", ":", "Module", ",", "norm_type", ":", "Union", "[", "float", ",", "int", ",", "str", "]", ",", "group_separator", ":", "str", "=", "\"", "/", "\"", ")", "-", ">", "dict", "[", "str", ",", "float", "]", ":", "\"", "\"", "\"", "Compute", "each", "parameter", "'", "s", "gradient", "'", "s", "norm", "and", "their", "overall", "norm", ".", "The", "overall", "norm", "is", "computed", "over", "all", "gradients", "together", ",", "as", "if", "they", "were", "concatenated", "into", "a", "single", "vector", ".", "Args", ":", "module", ":", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "to", "inspect", ".", "norm_type", ":", "The", "type", "of", "the", "used", "p", "-", "norm", ",", "cast", "to", "float", "if", "necessary", ".", "Can", "be", "`", "`", "'", "inf", "'", "`", "`", "for", "infinity", "norm", ".", "group_separator", ":", "The", "separator", "string", "used", "by", "the", "logger", "to", "group", "the", "gradients", "norms", "in", "their", "own", "subfolder", "instead", "of", "the", "logs", "one", ".", "Return", ":", "norms", ":", "The", "dictionary", "of", "p", "-", "norms", "of", "each", "parameter", "'", "s", "gradient", "and", "a", "special", "entry", "for", "the", "total", "p", "-", "norm", "of", "the", "gradients", "viewed", "as", "a", "single", "vector", ".", "\"", "\"", "\"", "norm_type", "=", "float", "(", "norm_type", ")", "if", "norm_type", "<", "=", "0", ":", "raise", "ValueError", "(", "f", "\"", "`", "norm_type", "`", "must", "be", "a", "positive", "number", "or", "'", "inf", "'", "(", "infinity", "norm", ")", ".", "Got", "{", "norm_type", "}", "\"", ")", "norms", "=", "{", "f", "\"", "grad_", "{", "norm_type", "}", "_norm", "{", "group_separator", "}", "{", "name", "}", "\"", ":", "p", ".", "grad", ".", "data", ".", "norm", "(", "norm_type", ")", "for", "name", ",", "p", "in", "module", ".", "named_parameters", "(", ")", "if", "p", ".", "grad", "is", "not", "None", "}", "if", "norms", ":", "total_norm", "=", "torch", ".", "tensor", "(", "list", "(", "norms", ".", "values", "(", ")", ")", ")", ".", "norm", "(", "norm_type", ")", "norms", "[", "f", "\"", "grad_", "{", "norm_type", "}", "_norm_total", "\"", "]", "=", "total_norm", "return", "norms"], "docstring": "Compute each parameter's gradient's norm and their overall norm.\r\n\r\n    The overall norm is computed over all gradients together, as if they\r\n    were concatenated into a single vector.\r\n\r\n    Args:\r\n        module: :class:`torch.nn.Module` to inspect.\r\n        norm_type: The type of the used p-norm, cast to float if necessary.\r\n            Can be ``'inf'`` for infinity norm.\r\n        group_separator: The separator string used by the logger to group\r\n            the gradients norms in their own subfolder instead of the logs one.\r\n\r\n    Return:\r\n        norms: The dictionary of p-norms of each parameter's gradient and\r\n            a special entry for the total p-norm of the gradients viewed\r\n            as a single vector.", "docstring_tokens": ["compute", "each", "parameter", "s", "gradient", "s", "norm", "and", "their", "overall", "norm", "the", "overall", "norm", "is", "computed", "over", "all", "gradients", "together", "as", "if", "they", "were", "concatenated", "into", "a", "single", "vector", "args", "module", "class", "torch", "nn", "module", "to", "inspect", "norm_type", "the", "type", "of", "the", "used", "p", "norm", "cast", "to", "float", "if", "necessary", "can", "be", "inf", "for", "infinity", "norm", "group_separator", "the", "separator", "string", "used", "by", "the", "logger", "to", "group", "the", "gradients", "norms", "in", "their", "own", "subfolder", "instead", "of", "the", "logs", "one", "return", "norms", "the", "dictionary", "of", "p", "norms", "of", "each", "parameter", "s", "gradient", "and", "a", "special", "entry", "for", "the", "total", "p", "norm", "of", "the", "gradients", "viewed", "as", "a", "single", "vector"], "docstring_summary": "Compute each parameter's gradient's norm and their overall norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\grads.py", "partition": "test", "function_type": "function", "start_line": 21, "end_line": 52, "hash": "6b3ad0e98dfce9814d70276615dd8d1b", "complexity": 5, "parameters": ["module", "norm_type", "int", "str]", "group_separator"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\memory.py", "func_name": "recursive_detach", "original_string": "def recursive_detach(in_dict: Any, to_cpu: bool = False) -> Any:\r\n    \"\"\"Detach all tensors in `in_dict`.\r\n\r\n    May operate recursively if some of the values in `in_dict` are dictionaries\r\n    which contain instances of `Tensor`. Other types in `in_dict` are\r\n    not affected by this utility function.\r\n\r\n    Args:\r\n        in_dict: Dictionary with tensors to detach\r\n        to_cpu: Whether to move tensor to cpu\r\n\r\n    Return:\r\n        out_dict: Dictionary with detached tensors\r\n\r\n    \"\"\"\r\n\r\n    def detach_and_move(t: Tensor, to_cpu: bool) -> Tensor:\r\n        t = t.detach()\r\n        if to_cpu:\r\n            t = t.cpu()\r\n        return t\r\n\r\n    return apply_to_collection(in_dict, Tensor, detach_and_move, to_cpu=to_cpu)", "language": "python", "code": "def recursive_detach(in_dict: Any, to_cpu: bool = False) -> Any:\r\n    \"\"\"Detach all tensors in `in_dict`.\r\n\r\n    May operate recursively if some of the values in `in_dict` are dictionaries\r\n    which contain instances of `Tensor`. Other types in `in_dict` are\r\n    not affected by this utility function.\r\n\r\n    Args:\r\n        in_dict: Dictionary with tensors to detach\r\n        to_cpu: Whether to move tensor to cpu\r\n\r\n    Return:\r\n        out_dict: Dictionary with detached tensors\r\n\r\n    \"\"\"\r\n\r\n    def detach_and_move(t: Tensor, to_cpu: bool) -> Tensor:\r\n        t = t.detach()\r\n        if to_cpu:\r\n            t = t.cpu()\r\n        return t\r\n\r\n    return apply_to_collection(in_dict, Tensor, detach_and_move, to_cpu=to_cpu)", "code_tokens": ["def", "recursive_detach", "(", "in_dict", ":", "Any", ",", "to_cpu", ":", "bool", "=", "False", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Detach", "all", "tensors", "in", "`", "in_dict", "`", ".", "May", "operate", "recursively", "if", "some", "of", "the", "values", "in", "`", "in_dict", "`", "are", "dictionaries", "which", "contain", "instances", "of", "`", "Tensor", "`", ".", "Other", "types", "in", "`", "in_dict", "`", "are", "not", "affected", "by", "this", "utility", "function", ".", "Args", ":", "in_dict", ":", "Dictionary", "with", "tensors", "to", "detach", "to_cpu", ":", "Whether", "to", "move", "tensor", "to", "cpu", "Return", ":", "out_dict", ":", "Dictionary", "with", "detached", "tensors", "\"", "\"", "\"", "def", "detach_and_move", "(", "t", ":", "Tensor", ",", "to_cpu", ":", "bool", ")", "-", ">", "Tensor", ":", "t", "=", "t", ".", "detach", "(", ")", "if", "to_cpu", ":", "t", "=", "t", ".", "cpu", "(", ")", "return", "t", "return", "apply_to_collection", "(", "in_dict", ",", "Tensor", ",", "detach_and_move", ",", "to_cpu", "=", "to_cpu", ")"], "docstring": "Detach all tensors in `in_dict`.\r\n\r\n    May operate recursively if some of the values in `in_dict` are dictionaries\r\n    which contain instances of `Tensor`. Other types in `in_dict` are\r\n    not affected by this utility function.\r\n\r\n    Args:\r\n        in_dict: Dictionary with tensors to detach\r\n        to_cpu: Whether to move tensor to cpu\r\n\r\n    Return:\r\n        out_dict: Dictionary with detached tensors", "docstring_tokens": ["detach", "all", "tensors", "in", "in_dict", "may", "operate", "recursively", "if", "some", "of", "the", "values", "in", "in_dict", "are", "dictionaries", "which", "contain", "instances", "of", "tensor", "other", "types", "in", "in_dict", "are", "not", "affected", "by", "this", "utility", "function", "args", "in_dict", "dictionary", "with", "tensors", "to", "detach", "to_cpu", "whether", "to", "move", "tensor", "to", "cpu", "return", "out_dict", "dictionary", "with", "detached", "tensors"], "docstring_summary": "Detach all tensors in `in_dict`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\memory.py", "partition": "test", "function_type": "function", "start_line": 23, "end_line": 45, "hash": "a7657e3d960da31bbd2bbb2a41a8c53f", "complexity": 2, "parameters": ["in_dict", "to_cpu"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\memory.py", "func_name": "garbage_collection_cuda", "original_string": "def garbage_collection_cuda() -> None:\r\n    \"\"\"Garbage collection Torch (CUDA) memory.\"\"\"\r\n    gc.collect()\r\n    try:\r\n        # This is the last thing that should cause an OOM error, but seemingly it can.\r\n        torch.cuda.empty_cache()\r\n    except RuntimeError as exception:\r\n        if not is_oom_error(exception):\r\n            # Only handle OOM errors\r\n            raise", "language": "python", "code": "def garbage_collection_cuda() -> None:\r\n    \"\"\"Garbage collection Torch (CUDA) memory.\"\"\"\r\n    gc.collect()\r\n    try:\r\n        # This is the last thing that should cause an OOM error, but seemingly it can.\r\n        torch.cuda.empty_cache()\r\n    except RuntimeError as exception:\r\n        if not is_oom_error(exception):\r\n            # Only handle OOM errors\r\n            raise", "code_tokens": ["def", "garbage_collection_cuda", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Garbage", "collection", "Torch", "(", "CUDA", ")", "memory", ".", "\"", "\"", "\"", "gc", ".", "collect", "(", ")", "try", ":", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "except", "RuntimeError", "as", "exception", ":", "if", "not", "is_oom_error", "(", "exception", ")", ":", "raise"], "docstring": "Garbage collection Torch (CUDA) memory.", "docstring_tokens": ["garbage", "collection", "torch", "cuda", "memory"], "docstring_summary": "Garbage collection Torch (CUDA) memory.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\memory.py", "partition": "test", "function_type": "function", "start_line": 82, "end_line": 91, "hash": "480e837c5eb359f05ea00cfbcaaf96a2", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "_is_registry", "original_string": "def _is_registry(text: Optional[_PATH]) -> bool:\r\n    \"\"\"Check if a string equals 'registry' or starts with 'registry:'.\r\n\r\n    Args:\r\n        text: The string to check\r\n\r\n    >>> _is_registry(\"registry\")\r\n    True\r\n    >>> _is_registry(\"REGISTRY:model-name\")\r\n    True\r\n    >>> _is_registry(\"something_registry\")\r\n    False\r\n    >>> _is_registry(\"\")\r\n    False\r\n\r\n    \"\"\"\r\n    if not isinstance(text, str):\r\n        return False\r\n\r\n    # Pattern matches exactly 'registry' or 'registry:' followed by any characters\r\n    pattern = r\"^registry(:.*|$)\"\r\n    return bool(re.match(pattern, text.lower()))", "language": "python", "code": "def _is_registry(text: Optional[_PATH]) -> bool:\r\n    \"\"\"Check if a string equals 'registry' or starts with 'registry:'.\r\n\r\n    Args:\r\n        text: The string to check\r\n\r\n    >>> _is_registry(\"registry\")\r\n    True\r\n    >>> _is_registry(\"REGISTRY:model-name\")\r\n    True\r\n    >>> _is_registry(\"something_registry\")\r\n    False\r\n    >>> _is_registry(\"\")\r\n    False\r\n\r\n    \"\"\"\r\n    if not isinstance(text, str):\r\n        return False\r\n\r\n    # Pattern matches exactly 'registry' or 'registry:' followed by any characters\r\n    pattern = r\"^registry(:.*|$)\"\r\n    return bool(re.match(pattern, text.lower()))", "code_tokens": ["def", "_is_registry", "(", "text", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "a", "string", "equals", "'", "registry", "'", "or", "starts", "with", "'", "registry", ":", "'", ".", "Args", ":", "text", ":", "The", "string", "to", "check", ">", ">", ">", "_is_registry", "(", "\"", "registry", "\"", ")", "True", ">", ">", ">", "_is_registry", "(", "\"", "REGISTRY", ":", "model", "-", "name", "\"", ")", "True", ">", ">", ">", "_is_registry", "(", "\"", "something_registry", "\"", ")", "False", ">", ">", ">", "_is_registry", "(", "\"", "\"", ")", "False", "\"", "\"", "\"", "if", "not", "isinstance", "(", "text", ",", "str", ")", ":", "return", "False", "pattern", "=", "r", "\"", "^", "registry", "(", ":", ".", "*", "|", "$", ")", "\"", "return", "bool", "(", "re", ".", "match", "(", "pattern", ",", "text", ".", "lower", "(", ")", ")", ")"], "docstring": "Check if a string equals 'registry' or starts with 'registry:'.\r\n\r\n    Args:\r\n        text: The string to check\r\n\r\n    >>> _is_registry(\"registry\")\r\n    True\r\n    >>> _is_registry(\"REGISTRY:model-name\")\r\n    True\r\n    >>> _is_registry(\"something_registry\")\r\n    False\r\n    >>> _is_registry(\"\")\r\n    False", "docstring_tokens": ["check", "if", "a", "string", "equals", "registry", "or", "starts", "with", "registry", "args", "text", "the", "string", "to", "check", "_is_registry", "registry", "true", "_is_registry", "registry", "model", "name", "true", "_is_registry", "something_registry", "false", "_is_registry", "false"], "docstring_summary": "Check if a string equals 'registry' or starts with 'registry:'.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 28, "end_line": 49, "hash": "d99854b72543727b4ff0adaed152f77b", "complexity": 2, "parameters": ["text"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "_parse_registry_model_version", "original_string": "def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]:\r\n    \"\"\"Parse the model version from a registry path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n\r\n    Returns:\r\n        string name and version of the model\r\n\r\n    >>> _parse_registry_model_version(\"registry:model-name:version:1.0\")\r\n    ('model-name', '1.0')\r\n    >>> _parse_registry_model_version(\"registry:model-name\")\r\n    ('model-name', '')\r\n    >>> _parse_registry_model_version(\"registry:VERSION:v2\")\r\n    ('', 'v2')\r\n\r\n    \"\"\"\r\n    if not ckpt_path or not _is_registry(ckpt_path):\r\n        raise ValueError(f\"Invalid registry path: {ckpt_path}\")\r\n\r\n    # Split the path by ':'\r\n    parts = str(ckpt_path).split(\":\")\r\n    # Default values\r\n    model_name, version = \"\", \"\"\r\n\r\n    # Extract the model name and version based on the parts\r\n    if len(parts) >= 2 and parts[1].lower() != \"version\":\r\n        model_name = parts[1]\r\n    if len(parts) == 3 and parts[1].lower() == \"version\":\r\n        version = parts[2]\r\n    elif len(parts) == 4 and parts[2].lower() == \"version\":\r\n        version = parts[3]\r\n\r\n    return model_name, version", "language": "python", "code": "def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]:\r\n    \"\"\"Parse the model version from a registry path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n\r\n    Returns:\r\n        string name and version of the model\r\n\r\n    >>> _parse_registry_model_version(\"registry:model-name:version:1.0\")\r\n    ('model-name', '1.0')\r\n    >>> _parse_registry_model_version(\"registry:model-name\")\r\n    ('model-name', '')\r\n    >>> _parse_registry_model_version(\"registry:VERSION:v2\")\r\n    ('', 'v2')\r\n\r\n    \"\"\"\r\n    if not ckpt_path or not _is_registry(ckpt_path):\r\n        raise ValueError(f\"Invalid registry path: {ckpt_path}\")\r\n\r\n    # Split the path by ':'\r\n    parts = str(ckpt_path).split(\":\")\r\n    # Default values\r\n    model_name, version = \"\", \"\"\r\n\r\n    # Extract the model name and version based on the parts\r\n    if len(parts) >= 2 and parts[1].lower() != \"version\":\r\n        model_name = parts[1]\r\n    if len(parts) == 3 and parts[1].lower() == \"version\":\r\n        version = parts[2]\r\n    elif len(parts) == 4 and parts[2].lower() == \"version\":\r\n        version = parts[3]\r\n\r\n    return model_name, version", "code_tokens": ["def", "_parse_registry_model_version", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ")", "-", ">", "tuple", "[", "str", ",", "str", "]", ":", "\"", "\"", "\"", "Parse", "the", "model", "version", "from", "a", "registry", "path", ".", "Args", ":", "ckpt_path", ":", "The", "checkpoint", "path", "Returns", ":", "string", "name", "and", "version", "of", "the", "model", ">", ">", ">", "_parse_registry_model_version", "(", "\"", "registry", ":", "model", "-", "name", ":", "version", ":", "1", ".", "0", "\"", ")", "(", "'", "model", "-", "name", "'", ",", "'", "1", ".", "0", "'", ")", ">", ">", ">", "_parse_registry_model_version", "(", "\"", "registry", ":", "model", "-", "name", "\"", ")", "(", "'", "model", "-", "name", "'", ",", "'", "'", ")", ">", ">", ">", "_parse_registry_model_version", "(", "\"", "registry", ":", "VERSION", ":", "v2", "\"", ")", "(", "'", "'", ",", "'", "v2", "'", ")", "\"", "\"", "\"", "if", "not", "ckpt_path", "or", "not", "_is_registry", "(", "ckpt_path", ")", ":", "raise", "ValueError", "(", "f", "\"", "Invalid", "registry", "path", ":", "{", "ckpt_path", "}", "\"", ")", "parts", "=", "str", "(", "ckpt_path", ")", ".", "split", "(", "\"", ":", "\"", ")", "model_name", ",", "version", "=", "\"", "\"", ",", "\"", "\"", "if", "len", "(", "parts", ")", ">", "=", "2", "and", "parts", "[", "1", "]", ".", "lower", "(", ")", "!", "=", "\"", "version", "\"", ":", "model_name", "=", "parts", "[", "1", "]", "if", "len", "(", "parts", ")", "=", "=", "3", "and", "parts", "[", "1", "]", ".", "lower", "(", ")", "=", "=", "\"", "version", "\"", ":", "version", "=", "parts", "[", "2", "]", "elif", "len", "(", "parts", ")", "=", "=", "4", "and", "parts", "[", "2", "]", ".", "lower", "(", ")", "=", "=", "\"", "version", "\"", ":", "version", "=", "parts", "[", "3", "]", "return", "model_name", ",", "version"], "docstring": "Parse the model version from a registry path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n\r\n    Returns:\r\n        string name and version of the model\r\n\r\n    >>> _parse_registry_model_version(\"registry:model-name:version:1.0\")\r\n    ('model-name', '1.0')\r\n    >>> _parse_registry_model_version(\"registry:model-name\")\r\n    ('model-name', '')\r\n    >>> _parse_registry_model_version(\"registry:VERSION:v2\")\r\n    ('', 'v2')", "docstring_tokens": ["parse", "the", "model", "version", "from", "a", "registry", "path", "args", "ckpt_path", "the", "checkpoint", "path", "returns", "string", "name", "and", "version", "of", "the", "model", "_parse_registry_model_version", "registry", "model", "name", "version", "1", "0", "model", "name", "1", "0", "_parse_registry_model_version", "registry", "model", "name", "model", "name", "_parse_registry_model_version", "registry", "version", "v2", "v2"], "docstring_summary": "Parse the model version from a registry path.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 52, "end_line": 85, "hash": "8a3bfcdcc4efd6b96656c3ef941ca898", "complexity": 9, "parameters": ["ckpt_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "_determine_model_name", "original_string": "def _determine_model_name(ckpt_path: Optional[_PATH], default_model_registry: Optional[str]) -> str:\r\n    \"\"\"Determine the model name from the checkpoint path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n        default_model_registry: The default model registry\r\n\r\n    Returns:\r\n        string name of the model with optional version\r\n\r\n    >>> _determine_model_name(\"registry:model-name:version:1.0\", \"default-model\")\r\n    'model-name:1.0'\r\n    >>> _determine_model_name(\"registry:model-name\", \"default-model\")\r\n    'model-name'\r\n    >>> _determine_model_name(\"registry:version:v2\", \"default-model\")\r\n    'default-model:v2'\r\n\r\n    \"\"\"\r\n    # try to find model and version\r\n    model_name, model_version = _parse_registry_model_version(ckpt_path)\r\n    # omitted model name try to use the model registry from Trainer\r\n    if not model_name and default_model_registry:\r\n        model_name = default_model_registry\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{ckpt_path}'\")\r\n    model_registry = model_name\r\n    model_registry += f\":{model_version}\" if model_version else \"\"\r\n    return model_registry", "language": "python", "code": "def _determine_model_name(ckpt_path: Optional[_PATH], default_model_registry: Optional[str]) -> str:\r\n    \"\"\"Determine the model name from the checkpoint path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n        default_model_registry: The default model registry\r\n\r\n    Returns:\r\n        string name of the model with optional version\r\n\r\n    >>> _determine_model_name(\"registry:model-name:version:1.0\", \"default-model\")\r\n    'model-name:1.0'\r\n    >>> _determine_model_name(\"registry:model-name\", \"default-model\")\r\n    'model-name'\r\n    >>> _determine_model_name(\"registry:version:v2\", \"default-model\")\r\n    'default-model:v2'\r\n\r\n    \"\"\"\r\n    # try to find model and version\r\n    model_name, model_version = _parse_registry_model_version(ckpt_path)\r\n    # omitted model name try to use the model registry from Trainer\r\n    if not model_name and default_model_registry:\r\n        model_name = default_model_registry\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{ckpt_path}'\")\r\n    model_registry = model_name\r\n    model_registry += f\":{model_version}\" if model_version else \"\"\r\n    return model_registry", "code_tokens": ["def", "_determine_model_name", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "default_model_registry", ":", "Optional", "[", "str", "]", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Determine", "the", "model", "name", "from", "the", "checkpoint", "path", ".", "Args", ":", "ckpt_path", ":", "The", "checkpoint", "path", "default_model_registry", ":", "The", "default", "model", "registry", "Returns", ":", "string", "name", "of", "the", "model", "with", "optional", "version", ">", ">", ">", "_determine_model_name", "(", "\"", "registry", ":", "model", "-", "name", ":", "version", ":", "1", ".", "0", "\"", ",", "\"", "default", "-", "model", "\"", ")", "'", "model", "-", "name", ":", "1", ".", "0", "'", ">", ">", ">", "_determine_model_name", "(", "\"", "registry", ":", "model", "-", "name", "\"", ",", "\"", "default", "-", "model", "\"", ")", "'", "model", "-", "name", "'", ">", ">", ">", "_determine_model_name", "(", "\"", "registry", ":", "version", ":", "v2", "\"", ",", "\"", "default", "-", "model", "\"", ")", "'", "default", "-", "model", ":", "v2", "'", "\"", "\"", "\"", "model_name", ",", "model_version", "=", "_parse_registry_model_version", "(", "ckpt_path", ")", "if", "not", "model_name", "and", "default_model_registry", ":", "model_name", "=", "default_model_registry", "if", "not", "model_name", ":", "raise", "ValueError", "(", "f", "\"", "Invalid", "model", "registry", ":", "'", "{", "ckpt_path", "}", "'", "\"", ")", "model_registry", "=", "model_name", "model_registry", "+", "=", "f", "\"", ":", "{", "model_version", "}", "\"", "if", "model_version", "else", "\"", "\"", "return", "model_registry"], "docstring": "Determine the model name from the checkpoint path.\r\n\r\n    Args:\r\n        ckpt_path: The checkpoint path\r\n        default_model_registry: The default model registry\r\n\r\n    Returns:\r\n        string name of the model with optional version\r\n\r\n    >>> _determine_model_name(\"registry:model-name:version:1.0\", \"default-model\")\r\n    'model-name:1.0'\r\n    >>> _determine_model_name(\"registry:model-name\", \"default-model\")\r\n    'model-name'\r\n    >>> _determine_model_name(\"registry:version:v2\", \"default-model\")\r\n    'default-model:v2'", "docstring_tokens": ["determine", "the", "model", "name", "from", "the", "checkpoint", "path", "args", "ckpt_path", "the", "checkpoint", "path", "default_model_registry", "the", "default", "model", "registry", "returns", "string", "name", "of", "the", "model", "with", "optional", "version", "_determine_model_name", "registry", "model", "name", "version", "1", "0", "default", "model", "model", "name", "1", "0", "_determine_model_name", "registry", "model", "name", "default", "model", "model", "name", "_determine_model_name", "registry", "version", "v2", "default", "model", "default", "model", "v2"], "docstring_summary": "Determine the model name from the checkpoint path.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 88, "end_line": 115, "hash": "21f6de9a1939de5c809272567cf09577", "complexity": 5, "parameters": ["ckpt_path", "default_model_registry"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "_determine_model_folder", "original_string": "def _determine_model_folder(model_name: str, default_root_dir: str) -> str:\r\n    \"\"\"Determine the local model folder based on the model registry.\r\n\r\n    Args:\r\n        model_name: The model name\r\n        default_root_dir: The default root directory\r\n\r\n    Returns:\r\n        string path to the local model folder\r\n\r\n    >>> _determine_model_folder(\"model-name\", \"/path/to/root\")\r\n    '/path/to/root/model-name'\r\n    >>> _determine_model_folder(\"model-name:1.0\", \"/path/to/root\")\r\n    '/path/to/root/model-name_1.0'\r\n\r\n    \"\"\"\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{model_name}'\")\r\n    # download the latest checkpoint from the model registry\r\n    model_name = model_name.replace(\"/\", \"_\")\r\n    model_name = model_name.replace(\":\", \"_\")\r\n    local_model_dir = os.path.join(default_root_dir, model_name)\r\n    return local_model_dir", "language": "python", "code": "def _determine_model_folder(model_name: str, default_root_dir: str) -> str:\r\n    \"\"\"Determine the local model folder based on the model registry.\r\n\r\n    Args:\r\n        model_name: The model name\r\n        default_root_dir: The default root directory\r\n\r\n    Returns:\r\n        string path to the local model folder\r\n\r\n    >>> _determine_model_folder(\"model-name\", \"/path/to/root\")\r\n    '/path/to/root/model-name'\r\n    >>> _determine_model_folder(\"model-name:1.0\", \"/path/to/root\")\r\n    '/path/to/root/model-name_1.0'\r\n\r\n    \"\"\"\r\n    if not model_name:\r\n        raise ValueError(f\"Invalid model registry: '{model_name}'\")\r\n    # download the latest checkpoint from the model registry\r\n    model_name = model_name.replace(\"/\", \"_\")\r\n    model_name = model_name.replace(\":\", \"_\")\r\n    local_model_dir = os.path.join(default_root_dir, model_name)\r\n    return local_model_dir", "code_tokens": ["def", "_determine_model_folder", "(", "model_name", ":", "str", ",", "default_root_dir", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Determine", "the", "local", "model", "folder", "based", "on", "the", "model", "registry", ".", "Args", ":", "model_name", ":", "The", "model", "name", "default_root_dir", ":", "The", "default", "root", "directory", "Returns", ":", "string", "path", "to", "the", "local", "model", "folder", ">", ">", ">", "_determine_model_folder", "(", "\"", "model", "-", "name", "\"", ",", "\"", "/", "path", "/", "to", "/", "root", "\"", ")", "'", "/", "path", "/", "to", "/", "root", "/", "model", "-", "name", "'", ">", ">", ">", "_determine_model_folder", "(", "\"", "model", "-", "name", ":", "1", ".", "0", "\"", ",", "\"", "/", "path", "/", "to", "/", "root", "\"", ")", "'", "/", "path", "/", "to", "/", "root", "/", "model", "-", "name_1", ".", "0", "'", "\"", "\"", "\"", "if", "not", "model_name", ":", "raise", "ValueError", "(", "f", "\"", "Invalid", "model", "registry", ":", "'", "{", "model_name", "}", "'", "\"", ")", "model_name", "=", "model_name", ".", "replace", "(", "\"", "/", "\"", ",", "\"", "_", "\"", ")", "model_name", "=", "model_name", ".", "replace", "(", "\"", ":", "\"", ",", "\"", "_", "\"", ")", "local_model_dir", "=", "os", ".", "path", ".", "join", "(", "default_root_dir", ",", "model_name", ")", "return", "local_model_dir"], "docstring": "Determine the local model folder based on the model registry.\r\n\r\n    Args:\r\n        model_name: The model name\r\n        default_root_dir: The default root directory\r\n\r\n    Returns:\r\n        string path to the local model folder\r\n\r\n    >>> _determine_model_folder(\"model-name\", \"/path/to/root\")\r\n    '/path/to/root/model-name'\r\n    >>> _determine_model_folder(\"model-name:1.0\", \"/path/to/root\")\r\n    '/path/to/root/model-name_1.0'", "docstring_tokens": ["determine", "the", "local", "model", "folder", "based", "on", "the", "model", "registry", "args", "model_name", "the", "model", "name", "default_root_dir", "the", "default", "root", "directory", "returns", "string", "path", "to", "the", "local", "model", "folder", "_determine_model_folder", "model", "name", "path", "to", "root", "path", "to", "root", "model", "name", "_determine_model_folder", "model", "name", "1", "0", "path", "to", "root", "path", "to", "root", "model", "name_1", "0"], "docstring_summary": "Determine the local model folder based on the model registry.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 118, "end_line": 140, "hash": "4bd79adf181a92724fb42bb56b1f233d", "complexity": 2, "parameters": ["model_name", "default_root_dir"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "find_model_local_ckpt_path", "original_string": "def find_model_local_ckpt_path(\r\n    ckpt_path: Optional[_PATH], default_model_registry: Optional[str], default_root_dir: str\r\n) -> str:\r\n    \"\"\"Find the local checkpoint path for a model.\"\"\"\r\n    model_registry = _determine_model_name(ckpt_path, default_model_registry)\r\n    local_model_dir = _determine_model_folder(model_registry, default_root_dir)\r\n\r\n    # todo: resolve if there are multiple checkpoints\r\n    folder_files = [fn for fn in os.listdir(local_model_dir) if fn.endswith(\".ckpt\")]\r\n    if not folder_files:\r\n        raise RuntimeError(f\"Parsing files from downloaded model: {model_registry}\")\r\n    # print(f\"local RANK {self.trainer.local_rank}: using model files: {folder_files}\")\r\n    return os.path.join(local_model_dir, folder_files[0])", "language": "python", "code": "def find_model_local_ckpt_path(\r\n    ckpt_path: Optional[_PATH], default_model_registry: Optional[str], default_root_dir: str\r\n) -> str:\r\n    \"\"\"Find the local checkpoint path for a model.\"\"\"\r\n    model_registry = _determine_model_name(ckpt_path, default_model_registry)\r\n    local_model_dir = _determine_model_folder(model_registry, default_root_dir)\r\n\r\n    # todo: resolve if there are multiple checkpoints\r\n    folder_files = [fn for fn in os.listdir(local_model_dir) if fn.endswith(\".ckpt\")]\r\n    if not folder_files:\r\n        raise RuntimeError(f\"Parsing files from downloaded model: {model_registry}\")\r\n    # print(f\"local RANK {self.trainer.local_rank}: using model files: {folder_files}\")\r\n    return os.path.join(local_model_dir, folder_files[0])", "code_tokens": ["def", "find_model_local_ckpt_path", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "default_model_registry", ":", "Optional", "[", "str", "]", ",", "default_root_dir", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Find", "the", "local", "checkpoint", "path", "for", "a", "model", ".", "\"", "\"", "\"", "model_registry", "=", "_determine_model_name", "(", "ckpt_path", ",", "default_model_registry", ")", "local_model_dir", "=", "_determine_model_folder", "(", "model_registry", ",", "default_root_dir", ")", "folder_files", "=", "[", "fn", "for", "fn", "in", "os", ".", "listdir", "(", "local_model_dir", ")", "if", "fn", ".", "endswith", "(", "\"", ".", "ckpt", "\"", ")", "]", "if", "not", "folder_files", ":", "raise", "RuntimeError", "(", "f", "\"", "Parsing", "files", "from", "downloaded", "model", ":", "{", "model_registry", "}", "\"", ")", "return", "os", ".", "path", ".", "join", "(", "local_model_dir", ",", "folder_files", "[", "0", "]", ")"], "docstring": "Find the local checkpoint path for a model.", "docstring_tokens": ["find", "the", "local", "checkpoint", "path", "for", "a", "model"], "docstring_summary": "Find the local checkpoint path for a model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 143, "end_line": 155, "hash": "27ee83a546913a61611a8f52511d5614", "complexity": 4, "parameters": ["ckpt_path", "default_model_registry", "default_root_dir"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_registry.py", "func_name": "download_model_from_registry", "original_string": "def download_model_from_registry(ckpt_path: Optional[_PATH], trainer: \"pl.Trainer\") -> None:\r\n    \"\"\"Download a model from the Lightning Model Registry.\"\"\"\r\n    if trainer.local_rank == 0:\r\n        if not module_available(\"litmodels\"):\r\n            raise ImportError(\r\n                \"The `litmodels` package is not installed. Please install it with `pip install litmodels`.\"\r\n            )\r\n\r\n        from litmodels import download_model\r\n\r\n        model_registry = _determine_model_name(ckpt_path, trainer._model_registry)\r\n        local_model_dir = _determine_model_folder(model_registry, trainer.default_root_dir)\r\n\r\n        # print(f\"Rank {self.trainer.local_rank} downloads model checkpoint '{model_registry}'\")\r\n        model_files = download_model(model_registry, download_dir=local_model_dir)\r\n        # print(f\"Model checkpoint '{model_registry}' was downloaded to '{local_model_dir}'\")\r\n        if not model_files:\r\n            raise RuntimeError(f\"Download model failed - {model_registry}\")\r\n\r\n    trainer.strategy.barrier(\"download_model_from_registry\")", "language": "python", "code": "def download_model_from_registry(ckpt_path: Optional[_PATH], trainer: \"pl.Trainer\") -> None:\r\n    \"\"\"Download a model from the Lightning Model Registry.\"\"\"\r\n    if trainer.local_rank == 0:\r\n        if not module_available(\"litmodels\"):\r\n            raise ImportError(\r\n                \"The `litmodels` package is not installed. Please install it with `pip install litmodels`.\"\r\n            )\r\n\r\n        from litmodels import download_model\r\n\r\n        model_registry = _determine_model_name(ckpt_path, trainer._model_registry)\r\n        local_model_dir = _determine_model_folder(model_registry, trainer.default_root_dir)\r\n\r\n        # print(f\"Rank {self.trainer.local_rank} downloads model checkpoint '{model_registry}'\")\r\n        model_files = download_model(model_registry, download_dir=local_model_dir)\r\n        # print(f\"Model checkpoint '{model_registry}' was downloaded to '{local_model_dir}'\")\r\n        if not model_files:\r\n            raise RuntimeError(f\"Download model failed - {model_registry}\")\r\n\r\n    trainer.strategy.barrier(\"download_model_from_registry\")", "code_tokens": ["def", "download_model_from_registry", "(", "ckpt_path", ":", "Optional", "[", "_PATH", "]", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Download", "a", "model", "from", "the", "Lightning", "Model", "Registry", ".", "\"", "\"", "\"", "if", "trainer", ".", "local_rank", "=", "=", "0", ":", "if", "not", "module_available", "(", "\"", "litmodels", "\"", ")", ":", "raise", "ImportError", "(", "\"", "The", "`", "litmodels", "`", "package", "is", "not", "installed", ".", "Please", "install", "it", "with", "`", "pip", "install", "litmodels", "`", ".", "\"", ")", "from", "litmodels", "import", "download_model", "model_registry", "=", "_determine_model_name", "(", "ckpt_path", ",", "trainer", ".", "_model_registry", ")", "local_model_dir", "=", "_determine_model_folder", "(", "model_registry", ",", "trainer", ".", "default_root_dir", ")", "model_files", "=", "download_model", "(", "model_registry", ",", "download_dir", "=", "local_model_dir", ")", "if", "not", "model_files", ":", "raise", "RuntimeError", "(", "f", "\"", "Download", "model", "failed", "-", "{", "model_registry", "}", "\"", ")", "trainer", ".", "strategy", ".", "barrier", "(", "\"", "download_model_from_registry", "\"", ")"], "docstring": "Download a model from the Lightning Model Registry.", "docstring_tokens": ["download", "a", "model", "from", "the", "lightning", "model", "registry"], "docstring_summary": "Download a model from the Lightning Model Registry.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_registry.py", "partition": "test", "function_type": "function", "start_line": 158, "end_line": 177, "hash": "61935932a60f6845c54683ed17dd52ef", "complexity": 4, "parameters": ["ckpt_path", "trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "is_picklable", "original_string": "def is_picklable(obj: object) -> bool:\r\n    \"\"\"Tests if an object can be pickled.\"\"\"\r\n    try:\r\n        pickle.dumps(obj)\r\n        return True\r\n    except (pickle.PickleError, AttributeError, RuntimeError, TypeError):\r\n        return False", "language": "python", "code": "def is_picklable(obj: object) -> bool:\r\n    \"\"\"Tests if an object can be pickled.\"\"\"\r\n    try:\r\n        pickle.dumps(obj)\r\n        return True\r\n    except (pickle.PickleError, AttributeError, RuntimeError, TypeError):\r\n        return False", "code_tokens": ["def", "is_picklable", "(", "obj", ":", "object", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Tests", "if", "an", "object", "can", "be", "pickled", ".", "\"", "\"", "\"", "try", ":", "pickle", ".", "dumps", "(", "obj", ")", "return", "True", "except", "(", "pickle", ".", "PickleError", ",", "AttributeError", ",", "RuntimeError", ",", "TypeError", ")", ":", "return", "False"], "docstring": "Tests if an object can be pickled.", "docstring_tokens": ["tests", "if", "an", "object", "can", "be", "pickled"], "docstring_summary": "Tests if an object can be pickled.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 30, "end_line": 36, "hash": "2d5c1313d5387ce6d9773af54d81f7e6", "complexity": 2, "parameters": ["obj"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "clean_namespace", "original_string": "def clean_namespace(hparams: MutableMapping) -> None:\r\n    \"\"\"Removes all unpicklable entries from hparams.\"\"\"\r\n    del_attrs = [k for k, v in hparams.items() if not is_picklable(v)]\r\n\r\n    for k in del_attrs:\r\n        rank_zero_warn(\r\n            f\"Attribute '{k}' removed from hparams because it cannot be pickled. You can suppress this warning by\"\r\n            f\" setting `self.save_hyperparameters(ignore=['{k}'])`.\",\r\n        )\r\n        del hparams[k]", "language": "python", "code": "def clean_namespace(hparams: MutableMapping) -> None:\r\n    \"\"\"Removes all unpicklable entries from hparams.\"\"\"\r\n    del_attrs = [k for k, v in hparams.items() if not is_picklable(v)]\r\n\r\n    for k in del_attrs:\r\n        rank_zero_warn(\r\n            f\"Attribute '{k}' removed from hparams because it cannot be pickled. You can suppress this warning by\"\r\n            f\" setting `self.save_hyperparameters(ignore=['{k}'])`.\",\r\n        )\r\n        del hparams[k]", "code_tokens": ["def", "clean_namespace", "(", "hparams", ":", "MutableMapping", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Removes", "all", "unpicklable", "entries", "from", "hparams", ".", "\"", "\"", "\"", "del_attrs", "=", "[", "k", "for", "k", ",", "v", "in", "hparams", ".", "items", "(", ")", "if", "not", "is_picklable", "(", "v", ")", "]", "for", "k", "in", "del_attrs", ":", "rank_zero_warn", "(", "f", "\"", "Attribute", "'", "{", "k", "}", "'", "removed", "from", "hparams", "because", "it", "cannot", "be", "pickled", ".", "You", "can", "suppress", "this", "warning", "by", "\"", "f", "\"", "setting", "`", "self", ".", "save_hyperparameters", "(", "ignore", "=", "[", "'", "{", "k", "}", "'", "]", ")", "`", ".", "\"", ",", ")", "del", "hparams", "[", "k", "]"], "docstring": "Removes all unpicklable entries from hparams.", "docstring_tokens": ["removes", "all", "unpicklable", "entries", "from", "hparams"], "docstring_summary": "Removes all unpicklable entries from hparams.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 39, "end_line": 48, "hash": "11bfbec0ea81b7033467aab0b8dec738", "complexity": 4, "parameters": ["hparams"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "parse_class_init_keys", "original_string": "def parse_class_init_keys(cls: type) -> tuple[str, Optional[str], Optional[str]]:\r\n    \"\"\"Parse key words for standard ``self``, ``*args`` and ``**kwargs``.\r\n\r\n    Examples:\r\n\r\n        >>> class Model:\r\n        ...     def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):\r\n        ...         pass\r\n        >>> parse_class_init_keys(Model)\r\n        ('self', 'my_args', 'my_kwargs')\r\n\r\n    \"\"\"\r\n    init_parameters = inspect.signature(cls.__init__).parameters  # type: ignore[misc]\r\n    # docs claims the params are always ordered\r\n    # https://docs.python.org/3/library/inspect.html#inspect.Signature.parameters\r\n    init_params = list(init_parameters.values())\r\n    # self is always first\r\n    n_self = init_params[0].name\r\n\r\n    def _get_first_if_any(\r\n        params: list[inspect.Parameter],\r\n        param_type: Literal[inspect._ParameterKind.VAR_POSITIONAL, inspect._ParameterKind.VAR_KEYWORD],\r\n    ) -> Optional[str]:\r\n        for p in params:\r\n            if p.kind == param_type:\r\n                return p.name\r\n        return None\r\n\r\n    n_args = _get_first_if_any(init_params, inspect.Parameter.VAR_POSITIONAL)\r\n    n_kwargs = _get_first_if_any(init_params, inspect.Parameter.VAR_KEYWORD)\r\n\r\n    return n_self, n_args, n_kwargs", "language": "python", "code": "def parse_class_init_keys(cls: type) -> tuple[str, Optional[str], Optional[str]]:\r\n    \"\"\"Parse key words for standard ``self``, ``*args`` and ``**kwargs``.\r\n\r\n    Examples:\r\n\r\n        >>> class Model:\r\n        ...     def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):\r\n        ...         pass\r\n        >>> parse_class_init_keys(Model)\r\n        ('self', 'my_args', 'my_kwargs')\r\n\r\n    \"\"\"\r\n    init_parameters = inspect.signature(cls.__init__).parameters  # type: ignore[misc]\r\n    # docs claims the params are always ordered\r\n    # https://docs.python.org/3/library/inspect.html#inspect.Signature.parameters\r\n    init_params = list(init_parameters.values())\r\n    # self is always first\r\n    n_self = init_params[0].name\r\n\r\n    def _get_first_if_any(\r\n        params: list[inspect.Parameter],\r\n        param_type: Literal[inspect._ParameterKind.VAR_POSITIONAL, inspect._ParameterKind.VAR_KEYWORD],\r\n    ) -> Optional[str]:\r\n        for p in params:\r\n            if p.kind == param_type:\r\n                return p.name\r\n        return None\r\n\r\n    n_args = _get_first_if_any(init_params, inspect.Parameter.VAR_POSITIONAL)\r\n    n_kwargs = _get_first_if_any(init_params, inspect.Parameter.VAR_KEYWORD)\r\n\r\n    return n_self, n_args, n_kwargs", "code_tokens": ["def", "parse_class_init_keys", "(", "cls", ":", "type", ")", "-", ">", "tuple", "[", "str", ",", "Optional", "[", "str", "]", ",", "Optional", "[", "str", "]", "]", ":", "\"", "\"", "\"", "Parse", "key", "words", "for", "standard", "`", "`", "self", "`", "`", ",", "`", "`", "*", "args", "`", "`", "and", "`", "`", "*", "*", "kwargs", "`", "`", ".", "Examples", ":", ">", ">", ">", "class", "Model", ":", ".", ".", ".", "def", "__init__", "(", "self", ",", "hparams", ",", "*", "my_args", ",", "anykw", "=", "42", ",", "*", "*", "my_kwargs", ")", ":", ".", ".", ".", "pass", ">", ">", ">", "parse_class_init_keys", "(", "Model", ")", "(", "'", "self", "'", ",", "'", "my_args", "'", ",", "'", "my_kwargs", "'", ")", "\"", "\"", "\"", "init_parameters", "=", "inspect", ".", "signature", "(", "cls", ".", "__init__", ")", ".", "parameters", "init_params", "=", "list", "(", "init_parameters", ".", "values", "(", ")", ")", "n_self", "=", "init_params", "[", "0", "]", ".", "name", "def", "_get_first_if_any", "(", "params", ":", "list", "[", "inspect", ".", "Parameter", "]", ",", "param_type", ":", "Literal", "[", "inspect", ".", "_ParameterKind", ".", "VAR_POSITIONAL", ",", "inspect", ".", "_ParameterKind", ".", "VAR_KEYWORD", "]", ",", ")", "-", ">", "Optional", "[", "str", "]", ":", "for", "p", "in", "params", ":", "if", "p", ".", "kind", "=", "=", "param_type", ":", "return", "p", ".", "name", "return", "None", "n_args", "=", "_get_first_if_any", "(", "init_params", ",", "inspect", ".", "Parameter", ".", "VAR_POSITIONAL", ")", "n_kwargs", "=", "_get_first_if_any", "(", "init_params", ",", "inspect", ".", "Parameter", ".", "VAR_KEYWORD", ")", "return", "n_self", ",", "n_args", ",", "n_kwargs"], "docstring": "Parse key words for standard ``self``, ``*args`` and ``**kwargs``.\r\n\r\n    Examples:\r\n\r\n        >>> class Model:\r\n        ...     def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):\r\n        ...         pass\r\n        >>> parse_class_init_keys(Model)\r\n        ('self', 'my_args', 'my_kwargs')", "docstring_tokens": ["parse", "key", "words", "for", "standard", "self", "args", "and", "kwargs", "examples", "class", "model", "def", "__init__", "self", "hparams", "my_args", "anykw", "42", "my_kwargs", "pass", "parse_class_init_keys", "model", "self", "my_args", "my_kwargs"], "docstring_summary": "Parse key words for standard ``self``, ``*args`` and ``**kwargs``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 51, "end_line": 82, "hash": "1fe894a7a62187fc04f550ac393957ca", "complexity": 3, "parameters": ["cls"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "get_init_args", "original_string": "def get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args", "language": "python", "code": "def get_init_args(frame: types.FrameType) -> dict[str, Any]:  # pragma: no-cover\r\n    \"\"\"For backwards compatibility: #16369.\"\"\"\r\n    _, local_args = _get_init_args(frame)\r\n    return local_args", "code_tokens": ["def", "get_init_args", "(", "frame", ":", "types", ".", "FrameType", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "For", "backwards", "compatibility", ":", "_", ",", "local_args", "=", "_get_init_args", "(", "frame", ")", "return", "local_args"], "docstring": "For backwards compatibility: #16369.", "docstring_tokens": ["for", "backwards", "compatibility", "16369"], "docstring_summary": "For backwards compatibility: #16369.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 85, "end_line": 88, "hash": "de6c3a5253c8c24bd1f8db5b5d82fc8e", "complexity": 1, "parameters": ["frame"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "collect_init_args", "original_string": "def collect_init_args(\r\n    frame: types.FrameType,\r\n    path_args: list[dict[str, Any]],\r\n    inside: bool = False,\r\n    classes: tuple[type, ...] = (),\r\n) -> list[dict[str, Any]]:\r\n    \"\"\"Recursively collects the arguments passed to the child constructors in the inheritance tree.\r\n\r\n    Args:\r\n        frame: the current stack frame\r\n        path_args: a list of dictionaries containing the constructor args in all parent classes\r\n        inside: track if we are inside inheritance path, avoid terminating too soon\r\n        classes: the classes in which to inspect the frames\r\n\r\n    Return:\r\n          A list of dictionaries where each dictionary contains the arguments passed to the\r\n          constructor at that level. The last entry corresponds to the constructor call of the\r\n          most specific class in the hierarchy.\r\n\r\n    \"\"\"\r\n    _, _, _, local_vars = inspect.getargvalues(frame)\r\n    # frame.f_back must be of a type types.FrameType for get_init_args/collect_init_args due to mypy\r\n    if not isinstance(frame.f_back, types.FrameType):\r\n        return path_args\r\n\r\n    local_self, local_args = _get_init_args(frame)\r\n    if \"__class__\" in local_vars and (not classes or isinstance(local_self, classes)):\r\n        # recursive update\r\n        path_args.append(local_args)\r\n        return collect_init_args(frame.f_back, path_args, inside=True, classes=classes)\r\n    if not inside:\r\n        return collect_init_args(frame.f_back, path_args, inside=False, classes=classes)\r\n    return path_args", "language": "python", "code": "def collect_init_args(\r\n    frame: types.FrameType,\r\n    path_args: list[dict[str, Any]],\r\n    inside: bool = False,\r\n    classes: tuple[type, ...] = (),\r\n) -> list[dict[str, Any]]:\r\n    \"\"\"Recursively collects the arguments passed to the child constructors in the inheritance tree.\r\n\r\n    Args:\r\n        frame: the current stack frame\r\n        path_args: a list of dictionaries containing the constructor args in all parent classes\r\n        inside: track if we are inside inheritance path, avoid terminating too soon\r\n        classes: the classes in which to inspect the frames\r\n\r\n    Return:\r\n          A list of dictionaries where each dictionary contains the arguments passed to the\r\n          constructor at that level. The last entry corresponds to the constructor call of the\r\n          most specific class in the hierarchy.\r\n\r\n    \"\"\"\r\n    _, _, _, local_vars = inspect.getargvalues(frame)\r\n    # frame.f_back must be of a type types.FrameType for get_init_args/collect_init_args due to mypy\r\n    if not isinstance(frame.f_back, types.FrameType):\r\n        return path_args\r\n\r\n    local_self, local_args = _get_init_args(frame)\r\n    if \"__class__\" in local_vars and (not classes or isinstance(local_self, classes)):\r\n        # recursive update\r\n        path_args.append(local_args)\r\n        return collect_init_args(frame.f_back, path_args, inside=True, classes=classes)\r\n    if not inside:\r\n        return collect_init_args(frame.f_back, path_args, inside=False, classes=classes)\r\n    return path_args", "code_tokens": ["def", "collect_init_args", "(", "frame", ":", "types", ".", "FrameType", ",", "path_args", ":", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ",", "inside", ":", "bool", "=", "False", ",", "classes", ":", "tuple", "[", "type", ",", ".", ".", ".", "]", "=", "(", ")", ",", ")", "-", ">", "list", "[", "dict", "[", "str", ",", "Any", "]", "]", ":", "\"", "\"", "\"", "Recursively", "collects", "the", "arguments", "passed", "to", "the", "child", "constructors", "in", "the", "inheritance", "tree", ".", "Args", ":", "frame", ":", "the", "current", "stack", "frame", "path_args", ":", "a", "list", "of", "dictionaries", "containing", "the", "constructor", "args", "in", "all", "parent", "classes", "inside", ":", "track", "if", "we", "are", "inside", "inheritance", "path", ",", "avoid", "terminating", "too", "soon", "classes", ":", "the", "classes", "in", "which", "to", "inspect", "the", "frames", "Return", ":", "A", "list", "of", "dictionaries", "where", "each", "dictionary", "contains", "the", "arguments", "passed", "to", "the", "constructor", "at", "that", "level", ".", "The", "last", "entry", "corresponds", "to", "the", "constructor", "call", "of", "the", "most", "specific", "class", "in", "the", "hierarchy", ".", "\"", "\"", "\"", "_", ",", "_", ",", "_", ",", "local_vars", "=", "inspect", ".", "getargvalues", "(", "frame", ")", "if", "not", "isinstance", "(", "frame", ".", "f_back", ",", "types", ".", "FrameType", ")", ":", "return", "path_args", "local_self", ",", "local_args", "=", "_get_init_args", "(", "frame", ")", "if", "\"", "__class__", "\"", "in", "local_vars", "and", "(", "not", "classes", "or", "isinstance", "(", "local_self", ",", "classes", ")", ")", ":", "path_args", ".", "append", "(", "local_args", ")", "return", "collect_init_args", "(", "frame", ".", "f_back", ",", "path_args", ",", "inside", "=", "True", ",", "classes", "=", "classes", ")", "if", "not", "inside", ":", "return", "collect_init_args", "(", "frame", ".", "f_back", ",", "path_args", ",", "inside", "=", "False", ",", "classes", "=", "classes", ")", "return", "path_args"], "docstring": "Recursively collects the arguments passed to the child constructors in the inheritance tree.\r\n\r\n    Args:\r\n        frame: the current stack frame\r\n        path_args: a list of dictionaries containing the constructor args in all parent classes\r\n        inside: track if we are inside inheritance path, avoid terminating too soon\r\n        classes: the classes in which to inspect the frames\r\n\r\n    Return:\r\n          A list of dictionaries where each dictionary contains the arguments passed to the\r\n          constructor at that level. The last entry corresponds to the constructor call of the\r\n          most specific class in the hierarchy.", "docstring_tokens": ["recursively", "collects", "the", "arguments", "passed", "to", "the", "child", "constructors", "in", "the", "inheritance", "tree", "args", "frame", "the", "current", "stack", "frame", "path_args", "a", "list", "of", "dictionaries", "containing", "the", "constructor", "args", "in", "all", "parent", "classes", "inside", "track", "if", "we", "are", "inside", "inheritance", "path", "avoid", "terminating", "too", "soon", "classes", "the", "classes", "in", "which", "to", "inspect", "the", "frames", "return", "a", "list", "of", "dictionaries", "where", "each", "dictionary", "contains", "the", "arguments", "passed", "to", "the", "constructor", "at", "that", "level", "the", "last", "entry", "corresponds", "to", "the", "constructor", "call", "of", "the", "most", "specific", "class", "in", "the", "hierarchy"], "docstring_summary": "Recursively collects the arguments passed to the child constructors in the inheritance tree.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 110, "end_line": 142, "hash": "3aa00db0ec0b72971bb7744584cd8c7c", "complexity": 6, "parameters": ["frame", "path_args", "Any]]", "inside", "classes", "...]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "_lightning_get_all_attr_holders", "original_string": "def _lightning_get_all_attr_holders(model: \"pl.LightningModule\", attribute: str) -> list[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets all of the objects or dicts that holds attribute. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders: list[Any] = []\r\n\r\n    # Check if attribute in model\r\n    if hasattr(model, attribute):\r\n        holders.append(model)\r\n\r\n    # Check if attribute in model.hparams, either namespace or dict\r\n    if hasattr(model, \"hparams\") and attribute in model.hparams:\r\n        holders.append(model.hparams)\r\n\r\n    trainer = model._trainer\r\n    # Check if the attribute in datamodule (datamodule gets registered in Trainer)\r\n    if trainer is not None and trainer.datamodule is not None:\r\n        if hasattr(trainer.datamodule, attribute):\r\n            holders.append(trainer.datamodule)\r\n\r\n        if hasattr(trainer.datamodule, \"hparams\") and attribute in trainer.datamodule.hparams:\r\n            holders.append(trainer.datamodule.hparams)\r\n\r\n    return holders", "language": "python", "code": "def _lightning_get_all_attr_holders(model: \"pl.LightningModule\", attribute: str) -> list[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets all of the objects or dicts that holds attribute. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders: list[Any] = []\r\n\r\n    # Check if attribute in model\r\n    if hasattr(model, attribute):\r\n        holders.append(model)\r\n\r\n    # Check if attribute in model.hparams, either namespace or dict\r\n    if hasattr(model, \"hparams\") and attribute in model.hparams:\r\n        holders.append(model.hparams)\r\n\r\n    trainer = model._trainer\r\n    # Check if the attribute in datamodule (datamodule gets registered in Trainer)\r\n    if trainer is not None and trainer.datamodule is not None:\r\n        if hasattr(trainer.datamodule, attribute):\r\n            holders.append(trainer.datamodule)\r\n\r\n        if hasattr(trainer.datamodule, \"hparams\") and attribute in trainer.datamodule.hparams:\r\n            holders.append(trainer.datamodule.hparams)\r\n\r\n    return holders", "code_tokens": ["def", "_lightning_get_all_attr_holders", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "attribute", ":", "str", ")", "-", ">", "list", "[", "Any", "]", ":", "\"", "\"", "\"", "Special", "attribute", "finding", "for", "Lightning", ".", "Gets", "all", "of", "the", "objects", "or", "dicts", "that", "holds", "attribute", ".", "Checks", "for", "attribute", "in", "model", "namespace", ",", "the", "old", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ".", "\"", "\"", "\"", "holders", ":", "list", "[", "Any", "]", "=", "[", "]", "if", "hasattr", "(", "model", ",", "attribute", ")", ":", "holders", ".", "append", "(", "model", ")", "if", "hasattr", "(", "model", ",", "\"", "hparams", "\"", ")", "and", "attribute", "in", "model", ".", "hparams", ":", "holders", ".", "append", "(", "model", ".", "hparams", ")", "trainer", "=", "model", ".", "_trainer", "if", "trainer", "is", "not", "None", "and", "trainer", ".", "datamodule", "is", "not", "None", ":", "if", "hasattr", "(", "trainer", ".", "datamodule", ",", "attribute", ")", ":", "holders", ".", "append", "(", "trainer", ".", "datamodule", ")", "if", "hasattr", "(", "trainer", ".", "datamodule", ",", "\"", "hparams", "\"", ")", "and", "attribute", "in", "trainer", ".", "datamodule", ".", "hparams", ":", "holders", ".", "append", "(", "trainer", ".", "datamodule", ".", "hparams", ")", "return", "holders"], "docstring": "Special attribute finding for Lightning.\r\n\r\n    Gets all of the objects or dicts that holds attribute. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule.", "docstring_tokens": ["special", "attribute", "finding", "for", "lightning", "gets", "all", "of", "the", "objects", "or", "dicts", "that", "holds", "attribute", "checks", "for", "attribute", "in", "model", "namespace", "the", "old", "hparams", "namespace", "dict", "and", "the", "datamodule"], "docstring_summary": "Special attribute finding for Lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 236, "end_line": 262, "hash": "009a31ff10e72c6b9bfea5585b8d307c", "complexity": 9, "parameters": ["model", "attribute"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "_lightning_get_first_attr_holder", "original_string": "def _lightning_get_first_attr_holder(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets the object or dict that holds attribute, or None. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule, returns the last one that has it.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        return None\r\n    # using the last holder to preserve backwards compatibility\r\n    return holders[-1]", "language": "python", "code": "def _lightning_get_first_attr_holder(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special attribute finding for Lightning.\r\n\r\n    Gets the object or dict that holds attribute, or None. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule, returns the last one that has it.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        return None\r\n    # using the last holder to preserve backwards compatibility\r\n    return holders[-1]", "code_tokens": ["def", "_lightning_get_first_attr_holder", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "attribute", ":", "str", ")", "-", ">", "Optional", "[", "Any", "]", ":", "\"", "\"", "\"", "Special", "attribute", "finding", "for", "Lightning", ".", "Gets", "the", "object", "or", "dict", "that", "holds", "attribute", ",", "or", "None", ".", "Checks", "for", "attribute", "in", "model", "namespace", ",", "the", "old", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ",", "returns", "the", "last", "one", "that", "has", "it", ".", "\"", "\"", "\"", "holders", "=", "_lightning_get_all_attr_holders", "(", "model", ",", "attribute", ")", "if", "len", "(", "holders", ")", "=", "=", "0", ":", "return", "None", "return", "holders", "[", "-", "1", "]"], "docstring": "Special attribute finding for Lightning.\r\n\r\n    Gets the object or dict that holds attribute, or None. Checks for attribute in model namespace, the old hparams\r\n    namespace/dict, and the datamodule, returns the last one that has it.", "docstring_tokens": ["special", "attribute", "finding", "for", "lightning", "gets", "the", "object", "or", "dict", "that", "holds", "attribute", "or", "none", "checks", "for", "attribute", "in", "model", "namespace", "the", "old", "hparams", "namespace", "dict", "and", "the", "datamodule", "returns", "the", "last", "one", "that", "has", "it"], "docstring_summary": "Special attribute finding for Lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 265, "end_line": 276, "hash": "b5b2b67d7aa8e933941080814d4286cc", "complexity": 2, "parameters": ["model", "attribute"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "lightning_hasattr", "original_string": "def lightning_hasattr(model: \"pl.LightningModule\", attribute: str) -> bool:\r\n    \"\"\"Special hasattr for Lightning.\r\n\r\n    Checks for attribute in model namespace, the old hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    return _lightning_get_first_attr_holder(model, attribute) is not None", "language": "python", "code": "def lightning_hasattr(model: \"pl.LightningModule\", attribute: str) -> bool:\r\n    \"\"\"Special hasattr for Lightning.\r\n\r\n    Checks for attribute in model namespace, the old hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    return _lightning_get_first_attr_holder(model, attribute) is not None", "code_tokens": ["def", "lightning_hasattr", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "attribute", ":", "str", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Special", "hasattr", "for", "Lightning", ".", "Checks", "for", "attribute", "in", "model", "namespace", ",", "the", "old", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ".", "\"", "\"", "\"", "return", "_lightning_get_first_attr_holder", "(", "model", ",", "attribute", ")", "is", "not", "None"], "docstring": "Special hasattr for Lightning.\r\n\r\n    Checks for attribute in model namespace, the old hparams namespace/dict, and the datamodule.", "docstring_tokens": ["special", "hasattr", "for", "lightning", "checks", "for", "attribute", "in", "model", "namespace", "the", "old", "hparams", "namespace", "dict", "and", "the", "datamodule"], "docstring_summary": "Special hasattr for Lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 279, "end_line": 285, "hash": "e004c74c6af2d4ac37b36d826f502659", "complexity": 1, "parameters": ["model", "attribute"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "lightning_getattr", "original_string": "def lightning_getattr(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the\r\n    datamodule.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holder = _lightning_get_first_attr_holder(model, attribute)\r\n    if holder is None:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    if isinstance(holder, dict):\r\n        return holder[attribute]\r\n    return getattr(holder, attribute)", "language": "python", "code": "def lightning_getattr(model: \"pl.LightningModule\", attribute: str) -> Optional[Any]:\r\n    \"\"\"Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the\r\n    datamodule.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holder = _lightning_get_first_attr_holder(model, attribute)\r\n    if holder is None:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    if isinstance(holder, dict):\r\n        return holder[attribute]\r\n    return getattr(holder, attribute)", "code_tokens": ["def", "lightning_getattr", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "attribute", ":", "str", ")", "-", ">", "Optional", "[", "Any", "]", ":", "\"", "\"", "\"", "Special", "getattr", "for", "Lightning", ".", "Checks", "for", "attribute", "in", "model", "namespace", ",", "the", "old", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ".", "Raises", ":", "AttributeError", ":", "If", "`", "`", "model", "`", "`", "doesn", "'", "t", "have", "`", "`", "attribute", "`", "`", "in", "any", "of", "model", "namespace", ",", "the", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ".", "\"", "\"", "\"", "holder", "=", "_lightning_get_first_attr_holder", "(", "model", ",", "attribute", ")", "if", "holder", "is", "None", ":", "raise", "AttributeError", "(", "f", "\"", "{", "attribute", "}", "is", "neither", "stored", "in", "the", "model", "namespace", "\"", "\"", "nor", "the", "`", "hparams", "`", "namespace", "/", "dict", ",", "nor", "the", "datamodule", ".", "\"", ")", "if", "isinstance", "(", "holder", ",", "dict", ")", ":", "return", "holder", "[", "attribute", "]", "return", "getattr", "(", "holder", ",", "attribute", ")"], "docstring": "Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the\r\n    datamodule.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.", "docstring_tokens": ["special", "getattr", "for", "lightning", "checks", "for", "attribute", "in", "model", "namespace", "the", "old", "hparams", "namespace", "dict", "and", "the", "datamodule", "raises", "attributeerror", "if", "model", "doesn", "t", "have", "attribute", "in", "any", "of", "model", "namespace", "the", "hparams", "namespace", "dict", "and", "the", "datamodule"], "docstring_summary": "Special getattr for Lightning. Checks for attribute in model namespace, the old hparams namespace/dict, and the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 288, "end_line": 307, "hash": "075956d13ad10c07cff5ebc80355de70", "complexity": 3, "parameters": ["model", "attribute"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\parsing.py", "func_name": "lightning_setattr", "original_string": "def lightning_setattr(model: \"pl.LightningModule\", attribute: str, value: Any) -> None:\r\n    \"\"\"Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will\r\n    also set the attribute on datamodule, if it exists.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    for holder in holders:\r\n        if isinstance(holder, dict):\r\n            holder[attribute] = value\r\n        else:\r\n            setattr(holder, attribute, value)", "language": "python", "code": "def lightning_setattr(model: \"pl.LightningModule\", attribute: str, value: Any) -> None:\r\n    \"\"\"Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will\r\n    also set the attribute on datamodule, if it exists.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.\r\n\r\n    \"\"\"\r\n    holders = _lightning_get_all_attr_holders(model, attribute)\r\n    if len(holders) == 0:\r\n        raise AttributeError(\r\n            f\"{attribute} is neither stored in the model namespace\"\r\n            \" nor the `hparams` namespace/dict, nor the datamodule.\"\r\n        )\r\n\r\n    for holder in holders:\r\n        if isinstance(holder, dict):\r\n            holder[attribute] = value\r\n        else:\r\n            setattr(holder, attribute, value)", "code_tokens": ["def", "lightning_setattr", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "attribute", ":", "str", ",", "value", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Special", "setattr", "for", "Lightning", ".", "Checks", "for", "attribute", "in", "model", "namespace", "and", "the", "old", "hparams", "namespace", "/", "dict", ".", "Will", "also", "set", "the", "attribute", "on", "datamodule", ",", "if", "it", "exists", ".", "Raises", ":", "AttributeError", ":", "If", "`", "`", "model", "`", "`", "doesn", "'", "t", "have", "`", "`", "attribute", "`", "`", "in", "any", "of", "model", "namespace", ",", "the", "hparams", "namespace", "/", "dict", ",", "and", "the", "datamodule", ".", "\"", "\"", "\"", "holders", "=", "_lightning_get_all_attr_holders", "(", "model", ",", "attribute", ")", "if", "len", "(", "holders", ")", "=", "=", "0", ":", "raise", "AttributeError", "(", "f", "\"", "{", "attribute", "}", "is", "neither", "stored", "in", "the", "model", "namespace", "\"", "\"", "nor", "the", "`", "hparams", "`", "namespace", "/", "dict", ",", "nor", "the", "datamodule", ".", "\"", ")", "for", "holder", "in", "holders", ":", "if", "isinstance", "(", "holder", ",", "dict", ")", ":", "holder", "[", "attribute", "]", "=", "value", "else", ":", "setattr", "(", "holder", ",", "attribute", ",", "value", ")"], "docstring": "Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will\r\n    also set the attribute on datamodule, if it exists.\r\n\r\n    Raises:\r\n        AttributeError:\r\n            If ``model`` doesn't have ``attribute`` in any of\r\n            model namespace, the hparams namespace/dict, and the datamodule.", "docstring_tokens": ["special", "setattr", "for", "lightning", "checks", "for", "attribute", "in", "model", "namespace", "and", "the", "old", "hparams", "namespace", "dict", "will", "also", "set", "the", "attribute", "on", "datamodule", "if", "it", "exists", "raises", "attributeerror", "if", "model", "doesn", "t", "have", "attribute", "in", "any", "of", "model", "namespace", "the", "hparams", "namespace", "dict", "and", "the", "datamodule"], "docstring_summary": "Special setattr for Lightning. Checks for attribute in model namespace and the old hparams namespace/dict. Will", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\parsing.py", "partition": "test", "function_type": "function", "start_line": 310, "end_line": 331, "hash": "48a5177668da2d2fb1327c40fb3bcc6b", "complexity": 4, "parameters": ["model", "attribute", "value"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\seed.py", "func_name": "isolate_rng", "original_string": "def isolate_rng(include_cuda: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"A context manager that resets the global random state on exit to what it was before entering.\r\n\r\n    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\r\n\r\n    Args:\r\n        include_cuda: Whether to allow this function to also control the `torch.cuda` random number generator.\r\n            Set this to ``False`` when using the function in a forked process where CUDA re-initialization is\r\n            prohibited.\r\n\r\n    Example:\r\n        >>> import torch\r\n        >>> torch.manual_seed(1)  # doctest: +ELLIPSIS\r\n        <torch._C.Generator object at ...>\r\n        >>> with isolate_rng():\r\n        ...     [torch.rand(1) for _ in range(3)]\r\n        [tensor([0.7576]), tensor([0.2793]), tensor([0.4031])]\r\n        >>> torch.rand(1)\r\n        tensor([0.7576])\r\n\r\n    \"\"\"\r\n    states = _collect_rng_states(include_cuda)\r\n    yield\r\n    _set_rng_states(states)", "language": "python", "code": "def isolate_rng(include_cuda: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"A context manager that resets the global random state on exit to what it was before entering.\r\n\r\n    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\r\n\r\n    Args:\r\n        include_cuda: Whether to allow this function to also control the `torch.cuda` random number generator.\r\n            Set this to ``False`` when using the function in a forked process where CUDA re-initialization is\r\n            prohibited.\r\n\r\n    Example:\r\n        >>> import torch\r\n        >>> torch.manual_seed(1)  # doctest: +ELLIPSIS\r\n        <torch._C.Generator object at ...>\r\n        >>> with isolate_rng():\r\n        ...     [torch.rand(1) for _ in range(3)]\r\n        [tensor([0.7576]), tensor([0.2793]), tensor([0.4031])]\r\n        >>> torch.rand(1)\r\n        tensor([0.7576])\r\n\r\n    \"\"\"\r\n    states = _collect_rng_states(include_cuda)\r\n    yield\r\n    _set_rng_states(states)", "code_tokens": ["def", "isolate_rng", "(", "include_cuda", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "context", "manager", "that", "resets", "the", "global", "random", "state", "on", "exit", "to", "what", "it", "was", "before", "entering", ".", "It", "supports", "isolating", "the", "states", "for", "PyTorch", ",", "Numpy", ",", "and", "Python", "built", "-", "in", "random", "number", "generators", ".", "Args", ":", "include_cuda", ":", "Whether", "to", "allow", "this", "function", "to", "also", "control", "the", "`", "torch", ".", "cuda", "`", "random", "number", "generator", ".", "Set", "this", "to", "`", "`", "False", "`", "`", "when", "using", "the", "function", "in", "a", "forked", "process", "where", "CUDA", "re", "-", "initialization", "is", "prohibited", ".", "Example", ":", ">", ">", ">", "import", "torch", ">", ">", ">", "torch", ".", "manual_seed", "(", "1", ")", "<", "torch", ".", "_C", ".", "Generator", "object", "at", ".", ".", ".", ">", ">", ">", ">", "with", "isolate_rng", "(", ")", ":", ".", ".", ".", "[", "torch", ".", "rand", "(", "1", ")", "for", "_", "in", "range", "(", "3", ")", "]", "[", "tensor", "(", "[", "0", ".", "7576", "]", ")", ",", "tensor", "(", "[", "0", ".", "2793", "]", ")", ",", "tensor", "(", "[", "0", ".", "4031", "]", ")", "]", ">", ">", ">", "torch", ".", "rand", "(", "1", ")", "tensor", "(", "[", "0", ".", "7576", "]", ")", "\"", "\"", "\"", "states", "=", "_collect_rng_states", "(", "include_cuda", ")", "yield", "_set_rng_states", "(", "states", ")"], "docstring": "A context manager that resets the global random state on exit to what it was before entering.\r\n\r\n    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\r\n\r\n    Args:\r\n        include_cuda: Whether to allow this function to also control the `torch.cuda` random number generator.\r\n            Set this to ``False`` when using the function in a forked process where CUDA re-initialization is\r\n            prohibited.\r\n\r\n    Example:\r\n        >>> import torch\r\n        >>> torch.manual_seed(1)  # doctest: +ELLIPSIS\r\n        <torch._C.Generator object at ...>\r\n        >>> with isolate_rng():\r\n        ...     [torch.rand(1) for _ in range(3)]\r\n        [tensor([0.7576]), tensor([0.2793]), tensor([0.4031])]\r\n        >>> torch.rand(1)\r\n        tensor([0.7576])", "docstring_tokens": ["a", "context", "manager", "that", "resets", "the", "global", "random", "state", "on", "exit", "to", "what", "it", "was", "before", "entering", "it", "supports", "isolating", "the", "states", "for", "pytorch", "numpy", "and", "python", "built", "in", "random", "number", "generators", "args", "include_cuda", "whether", "to", "allow", "this", "function", "to", "also", "control", "the", "torch", "cuda", "random", "number", "generator", "set", "this", "to", "false", "when", "using", "the", "function", "in", "a", "forked", "process", "where", "cuda", "re", "initialization", "is", "prohibited", "example", "import", "torch", "torch", "manual_seed", "1", "doctest", "ellipsis", "torch", "_c", "generator", "object", "at", "with", "isolate_rng", "torch", "rand", "1", "for", "_", "in", "range", "3", "tensor", "0", "7576", "tensor", "0", "2793", "tensor", "0", "4031", "torch", "rand", "1", "tensor", "0", "7576"], "docstring_summary": "A context manager that resets the global random state on exit to what it was before entering.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\seed.py", "partition": "test", "function_type": "function", "start_line": 22, "end_line": 45, "hash": "ee7075d294202e6748356a4060c34a8e", "complexity": 1, "parameters": ["include_cuda"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\signature_utils.py", "func_name": "is_param_in_hook_signature", "original_string": "def is_param_in_hook_signature(\r\n    hook_fx: Callable, param: str, explicit: bool = False, min_args: Optional[int] = None\r\n) -> bool:\r\n    \"\"\"\r\n    Args:\r\n        hook_fx: the hook callable\r\n        param: the name of the parameter to check\r\n        explicit: whether the parameter has to be explicitly declared\r\n        min_args: whether the `signature` has at least `min_args` parameters\r\n    \"\"\"\r\n    if hasattr(hook_fx, \"__wrapped__\"):\r\n        # in case the hook has a decorator\r\n        hook_fx = hook_fx.__wrapped__\r\n    parameters = inspect.getfullargspec(hook_fx)\r\n    args = parameters.args[1:]  # ignore `self`\r\n    return (\r\n        param in args\r\n        or (not explicit and (parameters.varargs is not None))\r\n        or (isinstance(min_args, int) and len(args) >= min_args)\r\n    )", "language": "python", "code": "def is_param_in_hook_signature(\r\n    hook_fx: Callable, param: str, explicit: bool = False, min_args: Optional[int] = None\r\n) -> bool:\r\n    \"\"\"\r\n    Args:\r\n        hook_fx: the hook callable\r\n        param: the name of the parameter to check\r\n        explicit: whether the parameter has to be explicitly declared\r\n        min_args: whether the `signature` has at least `min_args` parameters\r\n    \"\"\"\r\n    if hasattr(hook_fx, \"__wrapped__\"):\r\n        # in case the hook has a decorator\r\n        hook_fx = hook_fx.__wrapped__\r\n    parameters = inspect.getfullargspec(hook_fx)\r\n    args = parameters.args[1:]  # ignore `self`\r\n    return (\r\n        param in args\r\n        or (not explicit and (parameters.varargs is not None))\r\n        or (isinstance(min_args, int) and len(args) >= min_args)\r\n    )", "code_tokens": ["def", "is_param_in_hook_signature", "(", "hook_fx", ":", "Callable", ",", "param", ":", "str", ",", "explicit", ":", "bool", "=", "False", ",", "min_args", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Args", ":", "hook_fx", ":", "the", "hook", "callable", "param", ":", "the", "name", "of", "the", "parameter", "to", "check", "explicit", ":", "whether", "the", "parameter", "has", "to", "be", "explicitly", "declared", "min_args", ":", "whether", "the", "`", "signature", "`", "has", "at", "least", "`", "min_args", "`", "parameters", "\"", "\"", "\"", "if", "hasattr", "(", "hook_fx", ",", "\"", "__wrapped__", "\"", ")", ":", "hook_fx", "=", "hook_fx", ".", "__wrapped__", "parameters", "=", "inspect", ".", "getfullargspec", "(", "hook_fx", ")", "args", "=", "parameters", ".", "args", "[", "1", ":", "]", "return", "(", "param", "in", "args", "or", "(", "not", "explicit", "and", "(", "parameters", ".", "varargs", "is", "not", "None", ")", ")", "or", "(", "isinstance", "(", "min_args", ",", "int", ")", "and", "len", "(", "args", ")", ">", "=", "min_args", ")", ")"], "docstring": "Args:\r\n        hook_fx: the hook callable\r\n        param: the name of the parameter to check\r\n        explicit: whether the parameter has to be explicitly declared\r\n        min_args: whether the `signature` has at least `min_args` parameters", "docstring_tokens": ["args", "hook_fx", "the", "hook", "callable", "param", "the", "name", "of", "the", "parameter", "to", "check", "explicit", "whether", "the", "parameter", "has", "to", "be", "explicitly", "declared", "min_args", "whether", "the", "signature", "has", "at", "least", "min_args", "parameters"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\signature_utils.py", "partition": "test", "function_type": "function", "start_line": 17, "end_line": 36, "hash": "6350e8dee732ed63a14647a32b297f62", "complexity": 6, "parameters": ["hook_fx", "param", "explicit", "min_args"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "func_name": "_is_leaf_or_primitive_container", "original_string": "def _is_leaf_or_primitive_container(pytree: PyTree) -> bool:\r\n    \"\"\"Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.\"\"\"\r\n    is_leaf = _get_node_type(pytree) not in SUPPORTED_NODES\r\n    if is_leaf:\r\n        return True\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, _ = flatten_fn(pytree)\r\n    return all(isinstance(child, (int, float, str)) for child in child_pytrees)", "language": "python", "code": "def _is_leaf_or_primitive_container(pytree: PyTree) -> bool:\r\n    \"\"\"Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.\"\"\"\r\n    is_leaf = _get_node_type(pytree) not in SUPPORTED_NODES\r\n    if is_leaf:\r\n        return True\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, _ = flatten_fn(pytree)\r\n    return all(isinstance(child, (int, float, str)) for child in child_pytrees)", "code_tokens": ["def", "_is_leaf_or_primitive_container", "(", "pytree", ":", "PyTree", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Customized", ":", "func", ":", "`", "torch", ".", "utils", ".", "_pytree", ".", "_is_leaf", "`", "to", "avoid", "flattening", "containers", "of", "primitives", ".", "\"", "\"", "\"", "is_leaf", "=", "_get_node_type", "(", "pytree", ")", "not", "in", "SUPPORTED_NODES", "if", "is_leaf", ":", "return", "True", "node_type", "=", "_get_node_type", "(", "pytree", ")", "flatten_fn", "=", "SUPPORTED_NODES", "[", "node_type", "]", ".", "flatten_fn", "child_pytrees", ",", "_", "=", "flatten_fn", "(", "pytree", ")", "return", "all", "(", "isinstance", "(", "child", ",", "(", "int", ",", "float", ",", "str", ")", ")", "for", "child", "in", "child_pytrees", ")"], "docstring": "Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.", "docstring_tokens": ["customized", "func", "torch", "utils", "_pytree", "_is_leaf", "to", "avoid", "flattening", "containers", "of", "primitives"], "docstring_summary": "Customized :func:`torch.utils._pytree._is_leaf` to avoid flattening containers of primitives.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\_pytree.py", "partition": "test", "function_type": "function", "start_line": 5, "end_line": 14, "hash": "680e4ca13f01b89b2b6193d898ddae19", "complexity": 3, "parameters": ["pytree"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\_pytree.py", "func_name": "_tree_flatten", "original_string": "def _tree_flatten(pytree: PyTree) -> tuple[list[Any], TreeSpec]:\r\n    \"\"\"Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.\"\"\"\r\n    if _is_leaf_or_primitive_container(pytree):\r\n        return [pytree], LeafSpec()\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, context = flatten_fn(pytree)\r\n\r\n    result: list[Any] = []\r\n    children_specs: list[TreeSpec] = []\r\n    for child in child_pytrees:\r\n        flat, child_spec = _tree_flatten(child)\r\n        result += flat\r\n        children_specs.append(child_spec)\r\n\r\n    return result, TreeSpec(node_type, context, children_specs)", "language": "python", "code": "def _tree_flatten(pytree: PyTree) -> tuple[list[Any], TreeSpec]:\r\n    \"\"\"Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.\"\"\"\r\n    if _is_leaf_or_primitive_container(pytree):\r\n        return [pytree], LeafSpec()\r\n\r\n    node_type = _get_node_type(pytree)\r\n    flatten_fn = SUPPORTED_NODES[node_type].flatten_fn\r\n    child_pytrees, context = flatten_fn(pytree)\r\n\r\n    result: list[Any] = []\r\n    children_specs: list[TreeSpec] = []\r\n    for child in child_pytrees:\r\n        flat, child_spec = _tree_flatten(child)\r\n        result += flat\r\n        children_specs.append(child_spec)\r\n\r\n    return result, TreeSpec(node_type, context, children_specs)", "code_tokens": ["def", "_tree_flatten", "(", "pytree", ":", "PyTree", ")", "-", ">", "tuple", "[", "list", "[", "Any", "]", ",", "TreeSpec", "]", ":", "\"", "\"", "\"", "Copy", "of", ":", "func", ":", "`", "torch", ".", "utils", ".", "_pytree", ".", "tree_flatten", "`", "using", "our", "custom", "leaf", "function", ".", "\"", "\"", "\"", "if", "_is_leaf_or_primitive_container", "(", "pytree", ")", ":", "return", "[", "pytree", "]", ",", "LeafSpec", "(", ")", "node_type", "=", "_get_node_type", "(", "pytree", ")", "flatten_fn", "=", "SUPPORTED_NODES", "[", "node_type", "]", ".", "flatten_fn", "child_pytrees", ",", "context", "=", "flatten_fn", "(", "pytree", ")", "result", ":", "list", "[", "Any", "]", "=", "[", "]", "children_specs", ":", "list", "[", "TreeSpec", "]", "=", "[", "]", "for", "child", "in", "child_pytrees", ":", "flat", ",", "child_spec", "=", "_tree_flatten", "(", "child", ")", "result", "+", "=", "flat", "children_specs", ".", "append", "(", "child_spec", ")", "return", "result", ",", "TreeSpec", "(", "node_type", ",", "context", ",", "children_specs", ")"], "docstring": "Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.", "docstring_tokens": ["copy", "of", "func", "torch", "utils", "_pytree", "tree_flatten", "using", "our", "custom", "leaf", "function"], "docstring_summary": "Copy of :func:`torch.utils._pytree.tree_flatten` using our custom leaf function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\_pytree.py", "partition": "test", "function_type": "function", "start_line": 17, "end_line": 33, "hash": "d1452b6564e6365a7e2c6dbb43380647", "complexity": 3, "parameters": ["pytree"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migration_index", "original_string": "def _migration_index() -> dict[str, list[Callable[[_CHECKPOINT], _CHECKPOINT]]]:\r\n    \"\"\"Migration functions returned here will get executed in the order they are listed.\"\"\"\r\n    return {\r\n        \"0.10.0\": [_migrate_model_checkpoint_early_stopping],\r\n        \"1.6.0\": [_migrate_loop_global_step_to_progress_tracking, _migrate_loop_current_epoch_to_progress_tracking],\r\n        \"1.6.5\": [_migrate_loop_batches_that_stepped],\r\n        \"1.9.0\": [_migrate_model_checkpoint_save_on_train_epoch_end_default],\r\n        \"2.0.0\": [\r\n            _drop_apex_amp_state,\r\n            _migrate_loop_structure_after_tbptt_removal,\r\n            _migrate_loop_structure_after_optimizer_loop_removal,\r\n            _migrate_loop_structure_after_dataloader_loop_removal,\r\n        ],\r\n    }", "language": "python", "code": "def _migration_index() -> dict[str, list[Callable[[_CHECKPOINT], _CHECKPOINT]]]:\r\n    \"\"\"Migration functions returned here will get executed in the order they are listed.\"\"\"\r\n    return {\r\n        \"0.10.0\": [_migrate_model_checkpoint_early_stopping],\r\n        \"1.6.0\": [_migrate_loop_global_step_to_progress_tracking, _migrate_loop_current_epoch_to_progress_tracking],\r\n        \"1.6.5\": [_migrate_loop_batches_that_stepped],\r\n        \"1.9.0\": [_migrate_model_checkpoint_save_on_train_epoch_end_default],\r\n        \"2.0.0\": [\r\n            _drop_apex_amp_state,\r\n            _migrate_loop_structure_after_tbptt_removal,\r\n            _migrate_loop_structure_after_optimizer_loop_removal,\r\n            _migrate_loop_structure_after_dataloader_loop_removal,\r\n        ],\r\n    }", "code_tokens": ["def", "_migration_index", "(", ")", "-", ">", "dict", "[", "str", ",", "list", "[", "Callable", "[", "[", "_CHECKPOINT", "]", ",", "_CHECKPOINT", "]", "]", "]", ":", "\"", "\"", "\"", "Migration", "functions", "returned", "here", "will", "get", "executed", "in", "the", "order", "they", "are", "listed", ".", "\"", "\"", "\"", "return", "{", "\"", "0", ".", "10", ".", "0", "\"", ":", "[", "_migrate_model_checkpoint_early_stopping", "]", ",", "\"", "1", ".", "6", ".", "0", "\"", ":", "[", "_migrate_loop_global_step_to_progress_tracking", ",", "_migrate_loop_current_epoch_to_progress_tracking", "]", ",", "\"", "1", ".", "6", ".", "5", "\"", ":", "[", "_migrate_loop_batches_that_stepped", "]", ",", "\"", "1", ".", "9", ".", "0", "\"", ":", "[", "_migrate_model_checkpoint_save_on_train_epoch_end_default", "]", ",", "\"", "2", ".", "0", ".", "0", "\"", ":", "[", "_drop_apex_amp_state", ",", "_migrate_loop_structure_after_tbptt_removal", ",", "_migrate_loop_structure_after_optimizer_loop_removal", ",", "_migrate_loop_structure_after_dataloader_loop_removal", ",", "]", ",", "}"], "docstring": "Migration functions returned here will get executed in the order they are listed.", "docstring_tokens": ["migration", "functions", "returned", "here", "will", "get", "executed", "in", "the", "order", "they", "are", "listed"], "docstring_summary": "Migration functions returned here will get executed in the order they are listed.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 43, "end_line": 56, "hash": "44b300c42f3c14489811259142943c5d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_model_checkpoint_early_stopping", "original_string": "def _migrate_model_checkpoint_early_stopping(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The checkpoint and early stopping keys were renamed.\r\n\r\n    Version: 0.10.0\r\n    Commit: a5d1176\r\n\r\n    \"\"\"\r\n    keys_mapping = {\r\n        \"checkpoint_callback_best_model_score\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"checkpoint_callback_best_model_path\": (ModelCheckpoint, \"best_model_path\"),\r\n        \"checkpoint_callback_best\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"early_stop_callback_wait\": (EarlyStopping, \"wait_count\"),\r\n        \"early_stop_callback_patience\": (EarlyStopping, \"patience\"),\r\n    }\r\n    checkpoint[\"callbacks\"] = checkpoint.get(\"callbacks\") or {}\r\n\r\n    for key, new_path in keys_mapping.items():\r\n        if key in checkpoint:\r\n            value = checkpoint[key]\r\n            callback_type, callback_key = new_path\r\n            checkpoint[\"callbacks\"][callback_type] = checkpoint[\"callbacks\"].get(callback_type) or {}\r\n            checkpoint[\"callbacks\"][callback_type][callback_key] = value\r\n            del checkpoint[key]\r\n    return checkpoint", "language": "python", "code": "def _migrate_model_checkpoint_early_stopping(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The checkpoint and early stopping keys were renamed.\r\n\r\n    Version: 0.10.0\r\n    Commit: a5d1176\r\n\r\n    \"\"\"\r\n    keys_mapping = {\r\n        \"checkpoint_callback_best_model_score\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"checkpoint_callback_best_model_path\": (ModelCheckpoint, \"best_model_path\"),\r\n        \"checkpoint_callback_best\": (ModelCheckpoint, \"best_model_score\"),\r\n        \"early_stop_callback_wait\": (EarlyStopping, \"wait_count\"),\r\n        \"early_stop_callback_patience\": (EarlyStopping, \"patience\"),\r\n    }\r\n    checkpoint[\"callbacks\"] = checkpoint.get(\"callbacks\") or {}\r\n\r\n    for key, new_path in keys_mapping.items():\r\n        if key in checkpoint:\r\n            value = checkpoint[key]\r\n            callback_type, callback_key = new_path\r\n            checkpoint[\"callbacks\"][callback_type] = checkpoint[\"callbacks\"].get(callback_type) or {}\r\n            checkpoint[\"callbacks\"][callback_type][callback_key] = value\r\n            del checkpoint[key]\r\n    return checkpoint", "code_tokens": ["def", "_migrate_model_checkpoint_early_stopping", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "The", "checkpoint", "and", "early", "stopping", "keys", "were", "renamed", ".", "Version", ":", "0", ".", "10", ".", "0", "Commit", ":", "a5d1176", "\"", "\"", "\"", "keys_mapping", "=", "{", "\"", "checkpoint_callback_best_model_score", "\"", ":", "(", "ModelCheckpoint", ",", "\"", "best_model_score", "\"", ")", ",", "\"", "checkpoint_callback_best_model_path", "\"", ":", "(", "ModelCheckpoint", ",", "\"", "best_model_path", "\"", ")", ",", "\"", "checkpoint_callback_best", "\"", ":", "(", "ModelCheckpoint", ",", "\"", "best_model_score", "\"", ")", ",", "\"", "early_stop_callback_wait", "\"", ":", "(", "EarlyStopping", ",", "\"", "wait_count", "\"", ")", ",", "\"", "early_stop_callback_patience", "\"", ":", "(", "EarlyStopping", ",", "\"", "patience", "\"", ")", ",", "}", "checkpoint", "[", "\"", "callbacks", "\"", "]", "=", "checkpoint", ".", "get", "(", "\"", "callbacks", "\"", ")", "or", "{", "}", "for", "key", ",", "new_path", "in", "keys_mapping", ".", "items", "(", ")", ":", "if", "key", "in", "checkpoint", ":", "value", "=", "checkpoint", "[", "key", "]", "callback_type", ",", "callback_key", "=", "new_path", "checkpoint", "[", "\"", "callbacks", "\"", "]", "[", "callback_type", "]", "=", "checkpoint", "[", "\"", "callbacks", "\"", "]", ".", "get", "(", "callback_type", ")", "or", "{", "}", "checkpoint", "[", "\"", "callbacks", "\"", "]", "[", "callback_type", "]", "[", "callback_key", "]", "=", "value", "del", "checkpoint", "[", "key", "]", "return", "checkpoint"], "docstring": "The checkpoint and early stopping keys were renamed.\r\n\r\n    Version: 0.10.0\r\n    Commit: a5d1176", "docstring_tokens": ["the", "checkpoint", "and", "early", "stopping", "keys", "were", "renamed", "version", "0", "10", "0", "commit", "a5d1176"], "docstring_summary": "The checkpoint and early stopping keys were renamed.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 59, "end_line": 82, "hash": "f435e35527827d2690b6d0e60fba24cd", "complexity": 5, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_global_step_to_progress_tracking", "original_string": "def _migrate_loop_global_step_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: c67b075\r\n    PR: #13645, #11805\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    # for automatic optimization\r\n    optim_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.optimizer_loop.optim_progress\"]\r\n    optim_progress[\"optimizer\"][\"step\"][\"total\"][\"completed\"] = global_step\r\n    # for manual optimization\r\n    optim_step_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.manual_loop.optim_step_progress\"]\r\n    optim_step_progress[\"total\"][\"completed\"] = global_step\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_global_step_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: c67b075\r\n    PR: #13645, #11805\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    # for automatic optimization\r\n    optim_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.optimizer_loop.optim_progress\"]\r\n    optim_progress[\"optimizer\"][\"step\"][\"total\"][\"completed\"] = global_step\r\n    # for manual optimization\r\n    optim_step_progress = checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.batch_loop.manual_loop.optim_step_progress\"]\r\n    optim_step_progress[\"total\"][\"completed\"] = global_step\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_global_step_to_progress_tracking", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Sets", "the", "`", "global_step", "`", "value", "for", "checkpoints", "before", "v1", ".", "6", "without", "the", "progress", "tracking", "state", ".", "It", "will", "be", "overwritten", "by", "the", "loop", "'", "s", "state", "if", "it", "was", "also", "saved", ".", "Version", ":", "1", ".", "6", ".", "0", "Commit", ":", "c67b075", "PR", ":", "\"", "\"", "\"", "global_step", "=", "checkpoint", "[", "\"", "global_step", "\"", "]", "checkpoint", ".", "setdefault", "(", "\"", "loops", "\"", ",", "{", "\"", "fit_loop", "\"", ":", "_get_fit_loop_initial_state_1_6_0", "(", ")", "}", ")", "checkpoint", "[", "\"", "loops", "\"", "]", ".", "setdefault", "(", "\"", "fit_loop", "\"", ",", "_get_fit_loop_initial_state_1_6_0", "(", ")", ")", "optim_progress", "=", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "[", "\"", "epoch_loop", ".", "batch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", "]", "optim_progress", "[", "\"", "optimizer", "\"", "]", "[", "\"", "step", "\"", "]", "[", "\"", "total", "\"", "]", "[", "\"", "completed", "\"", "]", "=", "global_step", "optim_step_progress", "=", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "[", "\"", "epoch_loop", ".", "batch_loop", ".", "manual_loop", ".", "optim_step_progress", "\"", "]", "optim_step_progress", "[", "\"", "total", "\"", "]", "[", "\"", "completed", "\"", "]", "=", "global_step", "return", "checkpoint"], "docstring": "Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: c67b075\r\n    PR: #13645, #11805", "docstring_tokens": ["sets", "the", "global_step", "value", "for", "checkpoints", "before", "v1", "6", "without", "the", "progress", "tracking", "state", "it", "will", "be", "overwritten", "by", "the", "loop", "s", "state", "if", "it", "was", "also", "saved", "version", "1", "6", "0", "commit", "c67b075", "pr", "13645", "11805"], "docstring_summary": "Sets the `global_step` value for checkpoints before v1.6 without the progress tracking state. It will be", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 85, "end_line": 103, "hash": "ae1277fe973196798c6a1e52db3e4b02", "complexity": 1, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_current_epoch_to_progress_tracking", "original_string": "def _migrate_loop_current_epoch_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: aea96e4\r\n    PR: #11805\r\n\r\n    \"\"\"\r\n    epoch = checkpoint[\"epoch\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_progress\"][\"current\"][\"completed\"] = epoch\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_current_epoch_to_progress_tracking(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: aea96e4\r\n    PR: #11805\r\n\r\n    \"\"\"\r\n    epoch = checkpoint[\"epoch\"]\r\n    checkpoint.setdefault(\"loops\", {\"fit_loop\": _get_fit_loop_initial_state_1_6_0()})\r\n    checkpoint[\"loops\"].setdefault(\"fit_loop\", _get_fit_loop_initial_state_1_6_0())\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_progress\"][\"current\"][\"completed\"] = epoch\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_current_epoch_to_progress_tracking", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Sets", "the", "`", "current_epoch", "`", "value", "for", "checkpoints", "before", "v1", ".", "6", "without", "the", "progress", "tracking", "state", ".", "It", "will", "be", "overwritten", "by", "the", "loop", "'", "s", "state", "if", "it", "was", "also", "saved", ".", "Version", ":", "1", ".", "6", ".", "0", "Commit", ":", "aea96e4", "PR", ":", "\"", "\"", "\"", "epoch", "=", "checkpoint", "[", "\"", "epoch", "\"", "]", "checkpoint", ".", "setdefault", "(", "\"", "loops", "\"", ",", "{", "\"", "fit_loop", "\"", ":", "_get_fit_loop_initial_state_1_6_0", "(", ")", "}", ")", "checkpoint", "[", "\"", "loops", "\"", "]", ".", "setdefault", "(", "\"", "fit_loop", "\"", ",", "_get_fit_loop_initial_state_1_6_0", "(", ")", ")", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "[", "\"", "epoch_progress", "\"", "]", "[", "\"", "current", "\"", "]", "[", "\"", "completed", "\"", "]", "=", "epoch", "return", "checkpoint"], "docstring": "Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be\r\n    overwritten by the loop's state if it was also saved.\r\n\r\n    Version: 1.6.0\r\n    Commit: aea96e4\r\n    PR: #11805", "docstring_tokens": ["sets", "the", "current_epoch", "value", "for", "checkpoints", "before", "v1", "6", "without", "the", "progress", "tracking", "state", "it", "will", "be", "overwritten", "by", "the", "loop", "s", "state", "if", "it", "was", "also", "saved", "version", "1", "6", "0", "commit", "aea96e4", "pr", "11805"], "docstring_summary": "Sets the `current_epoch` value for checkpoints before v1.6 without the progress tracking state. It will be", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 106, "end_line": 119, "hash": "8235d2f0b4b561cc11431ffbbf4073d9", "complexity": 1, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_batches_that_stepped", "original_string": "def _migrate_loop_batches_that_stepped(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.\r\n\r\n    Version: 1.6.5\r\n    Commit: c67b075\r\n    PR: #13645\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.state_dict\"].setdefault(\"_batches_that_stepped\", global_step)\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_batches_that_stepped(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.\r\n\r\n    Version: 1.6.5\r\n    Commit: c67b075\r\n    PR: #13645\r\n\r\n    \"\"\"\r\n    global_step = checkpoint[\"global_step\"]\r\n    checkpoint[\"loops\"][\"fit_loop\"][\"epoch_loop.state_dict\"].setdefault(\"_batches_that_stepped\", global_step)\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_batches_that_stepped", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Sets", "the", "`", "_batches_that_stepped", "`", "default", "value", "for", "checkpoints", "before", "v1", ".", "6", ".", "5", "which", "don", "'", "t", "have", "this", "key", ".", "Version", ":", "1", ".", "6", ".", "5", "Commit", ":", "c67b075", "PR", ":", "\"", "\"", "\"", "global_step", "=", "checkpoint", "[", "\"", "global_step", "\"", "]", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "[", "\"", "epoch_loop", ".", "state_dict", "\"", "]", ".", "setdefault", "(", "\"", "_batches_that_stepped", "\"", ",", "global_step", ")", "return", "checkpoint"], "docstring": "Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.\r\n\r\n    Version: 1.6.5\r\n    Commit: c67b075\r\n    PR: #13645", "docstring_tokens": ["sets", "the", "_batches_that_stepped", "default", "value", "for", "checkpoints", "before", "v1", "6", "5", "which", "don", "t", "have", "this", "key", "version", "1", "6", "5", "commit", "c67b075", "pr", "13645"], "docstring_summary": "Sets the `_batches_that_stepped` default value for checkpoints before v1.6.5 which don't have this key.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 122, "end_line": 132, "hash": "fc3b6a66e50175474a705eb54ea1b85b", "complexity": 1, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_model_checkpoint_save_on_train_epoch_end_default", "original_string": "def _migrate_model_checkpoint_save_on_train_epoch_end_default(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this\r\n    migration drops it from the state-keys saved in the checkpoint dict so that the keys match when the Trainer loads\r\n    the callback state.\r\n\r\n    Version: 1.9.0\r\n    Commit: f4ca56\r\n    PR: #15300, #15606\r\n\r\n    \"\"\"\r\n    if \"callbacks\" not in checkpoint:\r\n        return checkpoint\r\n\r\n    def new_key(old_key: str) -> str:\r\n        if not old_key.startswith(\"ModelCheckpoint\"):\r\n            return old_key\r\n        return re.sub(\", 'save_on_train_epoch_end': (None|True|False)\", \"\", old_key)\r\n\r\n    num_keys = len(checkpoint[\"callbacks\"])\r\n    # Note: only iterate over keys that are strings. The legacy state key was the type of the callback.\r\n    new_callback_states = {\r\n        new_key(old_key): state for old_key, state in checkpoint[\"callbacks\"].items() if isinstance(old_key, str)\r\n    }\r\n    if len(new_callback_states) < num_keys:\r\n        rank_zero_warn(\r\n            \"You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys\"\r\n            \" that would end up colliding with each other after an upgrade, which means we can't differentiate\"\r\n            \" which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint`\"\r\n            \" callbacks will not be able to reload the state.\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint\r\n\r\n    checkpoint[\"callbacks\"] = new_callback_states\r\n    return checkpoint", "language": "python", "code": "def _migrate_model_checkpoint_save_on_train_epoch_end_default(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this\r\n    migration drops it from the state-keys saved in the checkpoint dict so that the keys match when the Trainer loads\r\n    the callback state.\r\n\r\n    Version: 1.9.0\r\n    Commit: f4ca56\r\n    PR: #15300, #15606\r\n\r\n    \"\"\"\r\n    if \"callbacks\" not in checkpoint:\r\n        return checkpoint\r\n\r\n    def new_key(old_key: str) -> str:\r\n        if not old_key.startswith(\"ModelCheckpoint\"):\r\n            return old_key\r\n        return re.sub(\", 'save_on_train_epoch_end': (None|True|False)\", \"\", old_key)\r\n\r\n    num_keys = len(checkpoint[\"callbacks\"])\r\n    # Note: only iterate over keys that are strings. The legacy state key was the type of the callback.\r\n    new_callback_states = {\r\n        new_key(old_key): state for old_key, state in checkpoint[\"callbacks\"].items() if isinstance(old_key, str)\r\n    }\r\n    if len(new_callback_states) < num_keys:\r\n        rank_zero_warn(\r\n            \"You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys\"\r\n            \" that would end up colliding with each other after an upgrade, which means we can't differentiate\"\r\n            \" which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint`\"\r\n            \" callbacks will not be able to reload the state.\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint\r\n\r\n    checkpoint[\"callbacks\"] = new_callback_states\r\n    return checkpoint", "code_tokens": ["def", "_migrate_model_checkpoint_save_on_train_epoch_end_default", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "The", "`", "`", "save_on_train_epoch_end", "`", "`", "was", "removed", "from", "the", "state", "-", "key", "of", "`", "`", "ModelCheckpoint", "`", "`", "in", "1", ".", "9", ".", "0", ",", "and", "this", "migration", "drops", "it", "from", "the", "state", "-", "keys", "saved", "in", "the", "checkpoint", "dict", "so", "that", "the", "keys", "match", "when", "the", "Trainer", "loads", "the", "callback", "state", ".", "Version", ":", "1", ".", "9", ".", "0", "Commit", ":", "f4ca56", "PR", ":", "\"", "\"", "\"", "if", "\"", "callbacks", "\"", "not", "in", "checkpoint", ":", "return", "checkpoint", "def", "new_key", "(", "old_key", ":", "str", ")", "-", ">", "str", ":", "if", "not", "old_key", ".", "startswith", "(", "\"", "ModelCheckpoint", "\"", ")", ":", "return", "old_key", "return", "re", ".", "sub", "(", "\"", ",", "'", "save_on_train_epoch_end", "'", ":", "(", "None", "|", "True", "|", "False", ")", "\"", ",", "\"", "\"", ",", "old_key", ")", "num_keys", "=", "len", "(", "checkpoint", "[", "\"", "callbacks", "\"", "]", ")", "new_callback_states", "=", "{", "new_key", "(", "old_key", ")", ":", "state", "for", "old_key", ",", "state", "in", "checkpoint", "[", "\"", "callbacks", "\"", "]", ".", "items", "(", ")", "if", "isinstance", "(", "old_key", ",", "str", ")", "}", "if", "len", "(", "new_callback_states", ")", "<", "num_keys", ":", "rank_zero_warn", "(", "\"", "You", "have", "multiple", "`", "ModelCheckpoint", "`", "callback", "states", "in", "this", "checkpoint", ",", "but", "we", "found", "state", "keys", "\"", "\"", "that", "would", "end", "up", "colliding", "with", "each", "other", "after", "an", "upgrade", ",", "which", "means", "we", "can", "'", "t", "differentiate", "\"", "\"", "which", "of", "your", "checkpoint", "callbacks", "needs", "which", "states", ".", "At", "least", "one", "of", "your", "`", "ModelCheckpoint", "`", "\"", "\"", "callbacks", "will", "not", "be", "able", "to", "reload", "the", "state", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "checkpoint", "checkpoint", "[", "\"", "callbacks", "\"", "]", "=", "new_callback_states", "return", "checkpoint"], "docstring": "The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this\r\n    migration drops it from the state-keys saved in the checkpoint dict so that the keys match when the Trainer loads\r\n    the callback state.\r\n\r\n    Version: 1.9.0\r\n    Commit: f4ca56\r\n    PR: #15300, #15606", "docstring_tokens": ["the", "save_on_train_epoch_end", "was", "removed", "from", "the", "state", "key", "of", "modelcheckpoint", "in", "1", "9", "0", "and", "this", "migration", "drops", "it", "from", "the", "state", "keys", "saved", "in", "the", "checkpoint", "dict", "so", "that", "the", "keys", "match", "when", "the", "trainer", "loads", "the", "callback", "state", "version", "1", "9", "0", "commit", "f4ca56", "pr", "15300", "15606"], "docstring_summary": "The ``save_on_train_epoch_end`` was removed from the state-key of ``ModelCheckpoint`` in 1.9.0, and this", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 183, "end_line": 217, "hash": "9ed7ee8fe6edc104b77ce1c86c41eb95", "complexity": 6, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_drop_apex_amp_state", "original_string": "def _drop_apex_amp_state(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint\r\n    dict.\r\n\r\n    Version: 2.0.0\r\n    Commit: e544676ff434ed96c6dd3b4e73a708bcb27ebcf1\r\n    PR: #16149\r\n\r\n    \"\"\"\r\n    key = \"amp_scaling_state\"\r\n    if key in checkpoint:\r\n        rank_zero_warn(\"This checkpoint contains apex AMP data, but apex support has been removed in v2.0.0.\")\r\n        del checkpoint[key]\r\n    return checkpoint", "language": "python", "code": "def _drop_apex_amp_state(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint\r\n    dict.\r\n\r\n    Version: 2.0.0\r\n    Commit: e544676ff434ed96c6dd3b4e73a708bcb27ebcf1\r\n    PR: #16149\r\n\r\n    \"\"\"\r\n    key = \"amp_scaling_state\"\r\n    if key in checkpoint:\r\n        rank_zero_warn(\"This checkpoint contains apex AMP data, but apex support has been removed in v2.0.0.\")\r\n        del checkpoint[key]\r\n    return checkpoint", "code_tokens": ["def", "_drop_apex_amp_state", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Apex", "support", "was", "removed", "in", "v2", ".", "0", ".", "0", ",", "and", "this", "migration", "drops", "it", "from", "the", "state", "-", "keys", "saved", "in", "the", "checkpoint", "dict", ".", "Version", ":", "2", ".", "0", ".", "0", "Commit", ":", "e544676ff434ed96c6dd3b4e73a708bcb27ebcf1", "PR", ":", "\"", "\"", "\"", "key", "=", "\"", "amp_scaling_state", "\"", "if", "key", "in", "checkpoint", ":", "rank_zero_warn", "(", "\"", "This", "checkpoint", "contains", "apex", "AMP", "data", ",", "but", "apex", "support", "has", "been", "removed", "in", "v2", ".", "0", ".", "0", ".", "\"", ")", "del", "checkpoint", "[", "key", "]", "return", "checkpoint"], "docstring": "Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint\r\n    dict.\r\n\r\n    Version: 2.0.0\r\n    Commit: e544676ff434ed96c6dd3b4e73a708bcb27ebcf1\r\n    PR: #16149", "docstring_tokens": ["apex", "support", "was", "removed", "in", "v2", "0", "0", "and", "this", "migration", "drops", "it", "from", "the", "state", "keys", "saved", "in", "the", "checkpoint", "dict", "version", "2", "0", "0", "commit", "e544676ff434ed96c6dd3b4e73a708bcb27ebcf1", "pr", "16149"], "docstring_summary": "Apex support was removed in v2.0.0, and this migration drops it from the state-keys saved in the checkpoint", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 220, "end_line": 233, "hash": "c8042618644d189323433df78103725d", "complexity": 2, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_structure_after_tbptt_removal", "original_string": "def _migrate_loop_structure_after_tbptt_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The\r\n    optimizer loop and the manual loop were previously children of the training batch loop. After its removal, they\r\n    became the children of the training epoch loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 7807454\r\n    PR: #16337, #16172\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    # remap `x.batch_loop.y` to `x.y`\r\n    old_key_new_key_mapping = {\r\n        \"epoch_loop.batch_loop.manual_loop.optim_step_progress\": \"epoch_loop.manual_loop.optim_step_progress\",\r\n        \"epoch_loop.batch_loop.manual_loop.state_dict\": \"epoch_loop.manual_loop.state_dict\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.optim_progress\": \"epoch_loop.optimizer_loop.optim_progress\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.state_dict\": \"epoch_loop.optimizer_loop.state_dict\",\r\n    }\r\n    for old, new in list(old_key_new_key_mapping.items()):\r\n        if old in fit_loop:\r\n            fit_loop[new] = fit_loop[old]\r\n            del fit_loop[old]\r\n\r\n    # We can safely drop this key: our default implementation of `batch_loop` did not have state.\r\n    # If there was state from a custom batch loop, we wouldn't be able to load it meaningfully.\r\n    # But just in case, we save a copy of it in `epoch_loop.state_dict` in case the user wants to process it after\r\n    # loading the checkpoint.\r\n    if \"epoch_loop.batch_loop.state_dict\" in fit_loop and fit_loop[\"epoch_loop.batch_loop.state_dict\"]:\r\n        fit_loop[\"epoch_loop.state_dict\"][\"old_batch_loop_state_dict\"] = fit_loop[\"epoch_loop.batch_loop.state_dict\"]\r\n    fit_loop.pop(\"epoch_loop.batch_loop.state_dict\", None)\r\n\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_tbptt_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The\r\n    optimizer loop and the manual loop were previously children of the training batch loop. After its removal, they\r\n    became the children of the training epoch loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 7807454\r\n    PR: #16337, #16172\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    # remap `x.batch_loop.y` to `x.y`\r\n    old_key_new_key_mapping = {\r\n        \"epoch_loop.batch_loop.manual_loop.optim_step_progress\": \"epoch_loop.manual_loop.optim_step_progress\",\r\n        \"epoch_loop.batch_loop.manual_loop.state_dict\": \"epoch_loop.manual_loop.state_dict\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.optim_progress\": \"epoch_loop.optimizer_loop.optim_progress\",\r\n        \"epoch_loop.batch_loop.optimizer_loop.state_dict\": \"epoch_loop.optimizer_loop.state_dict\",\r\n    }\r\n    for old, new in list(old_key_new_key_mapping.items()):\r\n        if old in fit_loop:\r\n            fit_loop[new] = fit_loop[old]\r\n            del fit_loop[old]\r\n\r\n    # We can safely drop this key: our default implementation of `batch_loop` did not have state.\r\n    # If there was state from a custom batch loop, we wouldn't be able to load it meaningfully.\r\n    # But just in case, we save a copy of it in `epoch_loop.state_dict` in case the user wants to process it after\r\n    # loading the checkpoint.\r\n    if \"epoch_loop.batch_loop.state_dict\" in fit_loop and fit_loop[\"epoch_loop.batch_loop.state_dict\"]:\r\n        fit_loop[\"epoch_loop.state_dict\"][\"old_batch_loop_state_dict\"] = fit_loop[\"epoch_loop.batch_loop.state_dict\"]\r\n    fit_loop.pop(\"epoch_loop.batch_loop.state_dict\", None)\r\n\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_tbptt_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "truncated", "backpropagation", "was", "removed", ".", "The", "optimizer", "loop", "and", "the", "manual", "loop", "were", "previously", "children", "of", "the", "training", "batch", "loop", ".", "After", "its", "removal", ",", "they", "became", "the", "children", "of", "the", "training", "epoch", "loop", ".", "Version", ":", "2", ".", "0", ".", "0", "Commit", ":", "7807454", "PR", ":", "\"", "\"", "\"", "if", "\"", "loops", "\"", "not", "in", "checkpoint", ":", "return", "checkpoint", "if", "\"", "fit_loop", "\"", "not", "in", "checkpoint", "[", "\"", "loops", "\"", "]", ":", "return", "checkpoint", "fit_loop", "=", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "old_key_new_key_mapping", "=", "{", "\"", "epoch_loop", ".", "batch_loop", ".", "manual_loop", ".", "optim_step_progress", "\"", ":", "\"", "epoch_loop", ".", "manual_loop", ".", "optim_step_progress", "\"", ",", "\"", "epoch_loop", ".", "batch_loop", ".", "manual_loop", ".", "state_dict", "\"", ":", "\"", "epoch_loop", ".", "manual_loop", ".", "state_dict", "\"", ",", "\"", "epoch_loop", ".", "batch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", ":", "\"", "epoch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", ",", "\"", "epoch_loop", ".", "batch_loop", ".", "optimizer_loop", ".", "state_dict", "\"", ":", "\"", "epoch_loop", ".", "optimizer_loop", ".", "state_dict", "\"", ",", "}", "for", "old", ",", "new", "in", "list", "(", "old_key_new_key_mapping", ".", "items", "(", ")", ")", ":", "if", "old", "in", "fit_loop", ":", "fit_loop", "[", "new", "]", "=", "fit_loop", "[", "old", "]", "del", "fit_loop", "[", "old", "]", "if", "\"", "epoch_loop", ".", "batch_loop", ".", "state_dict", "\"", "in", "fit_loop", "and", "fit_loop", "[", "\"", "epoch_loop", ".", "batch_loop", ".", "state_dict", "\"", "]", ":", "fit_loop", "[", "\"", "epoch_loop", ".", "state_dict", "\"", "]", "[", "\"", "old_batch_loop_state_dict", "\"", "]", "=", "fit_loop", "[", "\"", "epoch_loop", ".", "batch_loop", ".", "state_dict", "\"", "]", "fit_loop", ".", "pop", "(", "\"", "epoch_loop", ".", "batch_loop", ".", "state_dict", "\"", ",", "None", ")", "return", "checkpoint"], "docstring": "Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The\r\n    optimizer loop and the manual loop were previously children of the training batch loop. After its removal, they\r\n    became the children of the training epoch loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 7807454\r\n    PR: #16337, #16172", "docstring_tokens": ["adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "truncated", "backpropagation", "was", "removed", "the", "optimizer", "loop", "and", "the", "manual", "loop", "were", "previously", "children", "of", "the", "training", "batch", "loop", "after", "its", "removal", "they", "became", "the", "children", "of", "the", "training", "epoch", "loop", "version", "2", "0", "0", "commit", "7807454", "pr", "16337", "16172"], "docstring_summary": "Adjusts the loop structure since it changed when the support for truncated backpropagation was removed. The", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 236, "end_line": 272, "hash": "043e58ee625fc8ea560683bdba8307a2", "complexity": 7, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_structure_after_optimizer_loop_removal", "original_string": "def _migrate_loop_structure_after_optimizer_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization\r\n    mode was removed. There is no longer a loop over optimizer, and hence no position to store for resuming the loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 6a56586\r\n    PR: #16539, #16598\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    # optimizer_position is no longer used\r\n    if \"epoch_loop.optimizer_loop.optim_progress\" in fit_loop:\r\n        fit_loop[\"epoch_loop.optimizer_loop.optim_progress\"].pop(\"optimizer_position\", None)\r\n\r\n    # the subloop attribute names have changed\r\n    if \"epoch_loop.optimizer_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.automatic_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.optimizer_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.automatic_optimization.optim_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.optimizer_loop.optim_progress\"\r\n        )\r\n    if \"epoch_loop.manual_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.manual_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.manual_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.manual_optimization.optim_step_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.manual_loop.optim_step_progress\"\r\n        )\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_optimizer_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization\r\n    mode was removed. There is no longer a loop over optimizer, and hence no position to store for resuming the loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 6a56586\r\n    PR: #16539, #16598\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    if \"fit_loop\" not in checkpoint[\"loops\"]:\r\n        return checkpoint\r\n    fit_loop = checkpoint[\"loops\"][\"fit_loop\"]\r\n\r\n    # optimizer_position is no longer used\r\n    if \"epoch_loop.optimizer_loop.optim_progress\" in fit_loop:\r\n        fit_loop[\"epoch_loop.optimizer_loop.optim_progress\"].pop(\"optimizer_position\", None)\r\n\r\n    # the subloop attribute names have changed\r\n    if \"epoch_loop.optimizer_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.automatic_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.optimizer_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.automatic_optimization.optim_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.optimizer_loop.optim_progress\"\r\n        )\r\n    if \"epoch_loop.manual_loop.state_dict\" in fit_loop:\r\n        fit_loop[\"epoch_loop.manual_optimization.state_dict\"] = fit_loop.pop(\"epoch_loop.manual_loop.state_dict\")\r\n        fit_loop[\"epoch_loop.manual_optimization.optim_step_progress\"] = fit_loop.pop(\r\n            \"epoch_loop.manual_loop.optim_step_progress\"\r\n        )\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_optimizer_loop_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "multiple", "optimizers", "in", "automatic", "optimization", "mode", "was", "removed", ".", "There", "is", "no", "longer", "a", "loop", "over", "optimizer", ",", "and", "hence", "no", "position", "to", "store", "for", "resuming", "the", "loop", ".", "Version", ":", "2", ".", "0", ".", "0", "Commit", ":", "6a56586", "PR", ":", "\"", "\"", "\"", "if", "\"", "loops", "\"", "not", "in", "checkpoint", ":", "return", "checkpoint", "if", "\"", "fit_loop", "\"", "not", "in", "checkpoint", "[", "\"", "loops", "\"", "]", ":", "return", "checkpoint", "fit_loop", "=", "checkpoint", "[", "\"", "loops", "\"", "]", "[", "\"", "fit_loop", "\"", "]", "if", "\"", "epoch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", "in", "fit_loop", ":", "fit_loop", "[", "\"", "epoch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", "]", ".", "pop", "(", "\"", "optimizer_position", "\"", ",", "None", ")", "if", "\"", "epoch_loop", ".", "optimizer_loop", ".", "state_dict", "\"", "in", "fit_loop", ":", "fit_loop", "[", "\"", "epoch_loop", ".", "automatic_optimization", ".", "state_dict", "\"", "]", "=", "fit_loop", ".", "pop", "(", "\"", "epoch_loop", ".", "optimizer_loop", ".", "state_dict", "\"", ")", "fit_loop", "[", "\"", "epoch_loop", ".", "automatic_optimization", ".", "optim_progress", "\"", "]", "=", "fit_loop", ".", "pop", "(", "\"", "epoch_loop", ".", "optimizer_loop", ".", "optim_progress", "\"", ")", "if", "\"", "epoch_loop", ".", "manual_loop", ".", "state_dict", "\"", "in", "fit_loop", ":", "fit_loop", "[", "\"", "epoch_loop", ".", "manual_optimization", ".", "state_dict", "\"", "]", "=", "fit_loop", ".", "pop", "(", "\"", "epoch_loop", ".", "manual_loop", ".", "state_dict", "\"", ")", "fit_loop", "[", "\"", "epoch_loop", ".", "manual_optimization", ".", "optim_step_progress", "\"", "]", "=", "fit_loop", ".", "pop", "(", "\"", "epoch_loop", ".", "manual_loop", ".", "optim_step_progress", "\"", ")", "return", "checkpoint"], "docstring": "Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization\r\n    mode was removed. There is no longer a loop over optimizer, and hence no position to store for resuming the loop.\r\n\r\n    Version: 2.0.0\r\n    Commit: 6a56586\r\n    PR: #16539, #16598", "docstring_tokens": ["adjusts", "the", "loop", "structure", "since", "it", "changed", "when", "the", "support", "for", "multiple", "optimizers", "in", "automatic", "optimization", "mode", "was", "removed", "there", "is", "no", "longer", "a", "loop", "over", "optimizer", "and", "hence", "no", "position", "to", "store", "for", "resuming", "the", "loop", "version", "2", "0", "0", "commit", "6a56586", "pr", "16539", "16598"], "docstring_summary": "Adjusts the loop structure since it changed when the support for multiple optimizers in automatic optimization", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 275, "end_line": 305, "hash": "2f58e88a583597acd910547bbda115c6", "complexity": 6, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\migration.py", "func_name": "_migrate_loop_structure_after_dataloader_loop_removal", "original_string": "def _migrate_loop_structure_after_dataloader_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the\r\n    ``_EvaluationEpochLoop`` (now ``_EvaluationLoop``) and ``_PredictionEpochLoop`` (now ``_PredictionLoop``).\r\n\r\n    Version: 2.0.0\r\n    Commit: ec4f592ecfe238edd83185f6c6905fb1e2406d61\r\n    PR: #16726\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    loops = checkpoint[\"loops\"]\r\n    for loop_key in (\"predict_loop\", \"validate_loop\", \"test_loop\"):\r\n        if loop_key not in loops:\r\n            continue\r\n        loop = loops[loop_key]\r\n        loop.pop(\"dataloader_progress\", None)  # no longer used\r\n        epoch_loop_key = \"epoch_loop.\"\r\n        epoch_loop_dict = {k[len(epoch_loop_key) :]: loop.pop(k) for k in list(loop) if k.startswith(epoch_loop_key)}\r\n        loop.update(epoch_loop_dict)\r\n    return checkpoint", "language": "python", "code": "def _migrate_loop_structure_after_dataloader_loop_removal(checkpoint: _CHECKPOINT) -> _CHECKPOINT:\r\n    \"\"\"The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the\r\n    ``_EvaluationEpochLoop`` (now ``_EvaluationLoop``) and ``_PredictionEpochLoop`` (now ``_PredictionLoop``).\r\n\r\n    Version: 2.0.0\r\n    Commit: ec4f592ecfe238edd83185f6c6905fb1e2406d61\r\n    PR: #16726\r\n\r\n    \"\"\"\r\n    if \"loops\" not in checkpoint:\r\n        return checkpoint\r\n    loops = checkpoint[\"loops\"]\r\n    for loop_key in (\"predict_loop\", \"validate_loop\", \"test_loop\"):\r\n        if loop_key not in loops:\r\n            continue\r\n        loop = loops[loop_key]\r\n        loop.pop(\"dataloader_progress\", None)  # no longer used\r\n        epoch_loop_key = \"epoch_loop.\"\r\n        epoch_loop_dict = {k[len(epoch_loop_key) :]: loop.pop(k) for k in list(loop) if k.startswith(epoch_loop_key)}\r\n        loop.update(epoch_loop_dict)\r\n    return checkpoint", "code_tokens": ["def", "_migrate_loop_structure_after_dataloader_loop_removal", "(", "checkpoint", ":", "_CHECKPOINT", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "The", "dataloader", "loops", "(", "`", "`", "_DataLoaderLoop", "`", "`", ",", "`", "`", "_PredictionLoop", "`", ",", "and", "`", "`", "_EvaluationLoop", "`", "`", ")", "were", "flattened", "into", "the", "`", "`", "_EvaluationEpochLoop", "`", "`", "(", "now", "`", "`", "_EvaluationLoop", "`", "`", ")", "and", "`", "`", "_PredictionEpochLoop", "`", "`", "(", "now", "`", "`", "_PredictionLoop", "`", "`", ")", ".", "Version", ":", "2", ".", "0", ".", "0", "Commit", ":", "ec4f592ecfe238edd83185f6c6905fb1e2406d61", "PR", ":", "\"", "\"", "\"", "if", "\"", "loops", "\"", "not", "in", "checkpoint", ":", "return", "checkpoint", "loops", "=", "checkpoint", "[", "\"", "loops", "\"", "]", "for", "loop_key", "in", "(", "\"", "predict_loop", "\"", ",", "\"", "validate_loop", "\"", ",", "\"", "test_loop", "\"", ")", ":", "if", "loop_key", "not", "in", "loops", ":", "continue", "loop", "=", "loops", "[", "loop_key", "]", "loop", ".", "pop", "(", "\"", "dataloader_progress", "\"", ",", "None", ")", "epoch_loop_key", "=", "\"", "epoch_loop", ".", "\"", "epoch_loop_dict", "=", "{", "k", "[", "len", "(", "epoch_loop_key", ")", ":", "]", ":", "loop", ".", "pop", "(", "k", ")", "for", "k", "in", "list", "(", "loop", ")", "if", "k", ".", "startswith", "(", "epoch_loop_key", ")", "}", "loop", ".", "update", "(", "epoch_loop_dict", ")", "return", "checkpoint"], "docstring": "The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the\r\n    ``_EvaluationEpochLoop`` (now ``_EvaluationLoop``) and ``_PredictionEpochLoop`` (now ``_PredictionLoop``).\r\n\r\n    Version: 2.0.0\r\n    Commit: ec4f592ecfe238edd83185f6c6905fb1e2406d61\r\n    PR: #16726", "docstring_tokens": ["the", "dataloader", "loops", "_dataloaderloop", "_predictionloop", "and", "_evaluationloop", "were", "flattened", "into", "the", "_evaluationepochloop", "now", "_evaluationloop", "and", "_predictionepochloop", "now", "_predictionloop", "version", "2", "0", "0", "commit", "ec4f592ecfe238edd83185f6c6905fb1e2406d61", "pr", "16726"], "docstring_summary": "The dataloader loops (``_DataLoaderLoop``, ``_PredictionLoop`, and ``_EvaluationLoop``) were flattened into the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\migration.py", "partition": "test", "function_type": "function", "start_line": 308, "end_line": 328, "hash": "850fc25adb17ed253e3aa465950036b5", "complexity": 6, "parameters": ["checkpoint"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "migrate_checkpoint", "original_string": "def migrate_checkpoint(\r\n    checkpoint: _CHECKPOINT, target_version: Optional[str] = None\r\n) -> tuple[_CHECKPOINT, dict[str, list[str]]]:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary.\r\n\r\n    Args:\r\n        checkpoint: A dictionary with the loaded state from the checkpoint file.\r\n        target_version: Run migrations only up to this version (inclusive), even if migration index contains\r\n            migration functions for newer versions than this target. Mainly useful for testing.\r\n\r\n    Note:\r\n        The migration happens in-place. We specifically avoid copying the dict to avoid memory spikes for large\r\n        checkpoints and objects that do not support being deep-copied.\r\n\r\n    \"\"\"\r\n    ckpt_version = _get_version(checkpoint)\r\n    if Version(ckpt_version) > Version(pl.__version__):\r\n        rank_zero_warn(\r\n            f\"The loaded checkpoint was produced with Lightning v{ckpt_version}, which is newer than your current\"\r\n            f\" Lightning version: v{pl.__version__}\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint, {}\r\n\r\n    index = _migration_index()\r\n    applied_migrations = {}\r\n    for migration_version, migration_functions in index.items():\r\n        if not _should_upgrade(checkpoint, migration_version, target_version):\r\n            continue\r\n        for migration_function in migration_functions:\r\n            checkpoint = migration_function(checkpoint)\r\n\r\n        applied_migrations[migration_version] = [fn.__name__ for fn in migration_functions]\r\n\r\n    if ckpt_version != pl.__version__:\r\n        _set_legacy_version(checkpoint, ckpt_version)\r\n    _set_version(checkpoint, pl.__version__)\r\n    return checkpoint, applied_migrations", "language": "python", "code": "def migrate_checkpoint(\r\n    checkpoint: _CHECKPOINT, target_version: Optional[str] = None\r\n) -> tuple[_CHECKPOINT, dict[str, list[str]]]:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary.\r\n\r\n    Args:\r\n        checkpoint: A dictionary with the loaded state from the checkpoint file.\r\n        target_version: Run migrations only up to this version (inclusive), even if migration index contains\r\n            migration functions for newer versions than this target. Mainly useful for testing.\r\n\r\n    Note:\r\n        The migration happens in-place. We specifically avoid copying the dict to avoid memory spikes for large\r\n        checkpoints and objects that do not support being deep-copied.\r\n\r\n    \"\"\"\r\n    ckpt_version = _get_version(checkpoint)\r\n    if Version(ckpt_version) > Version(pl.__version__):\r\n        rank_zero_warn(\r\n            f\"The loaded checkpoint was produced with Lightning v{ckpt_version}, which is newer than your current\"\r\n            f\" Lightning version: v{pl.__version__}\",\r\n            category=PossibleUserWarning,\r\n        )\r\n        return checkpoint, {}\r\n\r\n    index = _migration_index()\r\n    applied_migrations = {}\r\n    for migration_version, migration_functions in index.items():\r\n        if not _should_upgrade(checkpoint, migration_version, target_version):\r\n            continue\r\n        for migration_function in migration_functions:\r\n            checkpoint = migration_function(checkpoint)\r\n\r\n        applied_migrations[migration_version] = [fn.__name__ for fn in migration_functions]\r\n\r\n    if ckpt_version != pl.__version__:\r\n        _set_legacy_version(checkpoint, ckpt_version)\r\n    _set_version(checkpoint, pl.__version__)\r\n    return checkpoint, applied_migrations", "code_tokens": ["def", "migrate_checkpoint", "(", "checkpoint", ":", "_CHECKPOINT", ",", "target_version", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "tuple", "[", "_CHECKPOINT", ",", "dict", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "\"", "\"", "\"", "Applies", "Lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary", ".", "Args", ":", "checkpoint", ":", "A", "dictionary", "with", "the", "loaded", "state", "from", "the", "checkpoint", "file", ".", "target_version", ":", "Run", "migrations", "only", "up", "to", "this", "version", "(", "inclusive", ")", ",", "even", "if", "migration", "index", "contains", "migration", "functions", "for", "newer", "versions", "than", "this", "target", ".", "Mainly", "useful", "for", "testing", ".", "Note", ":", "The", "migration", "happens", "in", "-", "place", ".", "We", "specifically", "avoid", "copying", "the", "dict", "to", "avoid", "memory", "spikes", "for", "large", "checkpoints", "and", "objects", "that", "do", "not", "support", "being", "deep", "-", "copied", ".", "\"", "\"", "\"", "ckpt_version", "=", "_get_version", "(", "checkpoint", ")", "if", "Version", "(", "ckpt_version", ")", ">", "Version", "(", "pl", ".", "__version__", ")", ":", "rank_zero_warn", "(", "f", "\"", "The", "loaded", "checkpoint", "was", "produced", "with", "Lightning", "v", "{", "ckpt_version", "}", ",", "which", "is", "newer", "than", "your", "current", "\"", "f", "\"", "Lightning", "version", ":", "v", "{", "pl", ".", "__version__", "}", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "checkpoint", ",", "{", "}", "index", "=", "_migration_index", "(", ")", "applied_migrations", "=", "{", "}", "for", "migration_version", ",", "migration_functions", "in", "index", ".", "items", "(", ")", ":", "if", "not", "_should_upgrade", "(", "checkpoint", ",", "migration_version", ",", "target_version", ")", ":", "continue", "for", "migration_function", "in", "migration_functions", ":", "checkpoint", "=", "migration_function", "(", "checkpoint", ")", "applied_migrations", "[", "migration_version", "]", "=", "[", "fn", ".", "__name__", "for", "fn", "in", "migration_functions", "]", "if", "ckpt_version", "!", "=", "pl", ".", "__version__", ":", "_set_legacy_version", "(", "checkpoint", ",", "ckpt_version", ")", "_set_version", "(", "checkpoint", ",", "pl", ".", "__version__", ")", "return", "checkpoint", ",", "applied_migrations"], "docstring": "Applies Lightning version migrations to a checkpoint dictionary.\r\n\r\n    Args:\r\n        checkpoint: A dictionary with the loaded state from the checkpoint file.\r\n        target_version: Run migrations only up to this version (inclusive), even if migration index contains\r\n            migration functions for newer versions than this target. Mainly useful for testing.\r\n\r\n    Note:\r\n        The migration happens in-place. We specifically avoid copying the dict to avoid memory spikes for large\r\n        checkpoints and objects that do not support being deep-copied.", "docstring_tokens": ["applies", "lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary", "args", "checkpoint", "a", "dictionary", "with", "the", "loaded", "state", "from", "the", "checkpoint", "file", "target_version", "run", "migrations", "only", "up", "to", "this", "version", "inclusive", "even", "if", "migration", "index", "contains", "migration", "functions", "for", "newer", "versions", "than", "this", "target", "mainly", "useful", "for", "testing", "note", "the", "migration", "happens", "in", "place", "we", "specifically", "avoid", "copying", "the", "dict", "to", "avoid", "memory", "spikes", "for", "large", "checkpoints", "and", "objects", "that", "do", "not", "support", "being", "deep", "copied"], "docstring_summary": "Applies Lightning version migrations to a checkpoint dictionary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\utils.py", "partition": "test", "function_type": "function", "start_line": 38, "end_line": 75, "hash": "7dfe48deba4156aed3f98f1bdc17ee43", "complexity": 7, "parameters": ["checkpoint", "target_version"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "_pl_migrate_checkpoint", "original_string": "def _pl_migrate_checkpoint(checkpoint: _CHECKPOINT, checkpoint_path: Optional[_PATH] = None) -> _CHECKPOINT:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.\r\n\r\n    This function is used by the Lightning Trainer when resuming from a checkpoint.\r\n\r\n    \"\"\"\r\n    old_version = _get_version(checkpoint)\r\n    checkpoint, migrations = migrate_checkpoint(checkpoint)\r\n    new_version = _get_version(checkpoint)\r\n    if not migrations or checkpoint_path is None:\r\n        # the checkpoint was already a new one, no migrations were needed\r\n        return checkpoint\r\n\r\n    # include the full upgrade command, including the path to the loaded file in the error message,\r\n    # so user can copy-paste and run if they want\r\n    # side-step bug: ValueError: path is on mount 'C:', start on mount 'D:'\r\n    path_hint = os.path.relpath(checkpoint_path, os.getcwd()) if not _IS_WINDOWS else os.path.abspath(checkpoint_path)\r\n    _log.info(\r\n        f\"Lightning automatically upgraded your loaded checkpoint from v{old_version} to v{new_version}.\"\r\n        \" To apply the upgrade to your files permanently, run\"\r\n        f\" `python -m lightning.pytorch.utilities.upgrade_checkpoint {str(path_hint)}`\"\r\n    )\r\n    return checkpoint", "language": "python", "code": "def _pl_migrate_checkpoint(checkpoint: _CHECKPOINT, checkpoint_path: Optional[_PATH] = None) -> _CHECKPOINT:\r\n    \"\"\"Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.\r\n\r\n    This function is used by the Lightning Trainer when resuming from a checkpoint.\r\n\r\n    \"\"\"\r\n    old_version = _get_version(checkpoint)\r\n    checkpoint, migrations = migrate_checkpoint(checkpoint)\r\n    new_version = _get_version(checkpoint)\r\n    if not migrations or checkpoint_path is None:\r\n        # the checkpoint was already a new one, no migrations were needed\r\n        return checkpoint\r\n\r\n    # include the full upgrade command, including the path to the loaded file in the error message,\r\n    # so user can copy-paste and run if they want\r\n    # side-step bug: ValueError: path is on mount 'C:', start on mount 'D:'\r\n    path_hint = os.path.relpath(checkpoint_path, os.getcwd()) if not _IS_WINDOWS else os.path.abspath(checkpoint_path)\r\n    _log.info(\r\n        f\"Lightning automatically upgraded your loaded checkpoint from v{old_version} to v{new_version}.\"\r\n        \" To apply the upgrade to your files permanently, run\"\r\n        f\" `python -m lightning.pytorch.utilities.upgrade_checkpoint {str(path_hint)}`\"\r\n    )\r\n    return checkpoint", "code_tokens": ["def", "_pl_migrate_checkpoint", "(", "checkpoint", ":", "_CHECKPOINT", ",", "checkpoint_path", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "_CHECKPOINT", ":", "\"", "\"", "\"", "Applies", "Lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary", "and", "prints", "infos", "for", "the", "user", ".", "This", "function", "is", "used", "by", "the", "Lightning", "Trainer", "when", "resuming", "from", "a", "checkpoint", ".", "\"", "\"", "\"", "old_version", "=", "_get_version", "(", "checkpoint", ")", "checkpoint", ",", "migrations", "=", "migrate_checkpoint", "(", "checkpoint", ")", "new_version", "=", "_get_version", "(", "checkpoint", ")", "if", "not", "migrations", "or", "checkpoint_path", "is", "None", ":", "return", "checkpoint", "path_hint", "=", "os", ".", "path", ".", "relpath", "(", "checkpoint_path", ",", "os", ".", "getcwd", "(", ")", ")", "if", "not", "_IS_WINDOWS", "else", "os", ".", "path", ".", "abspath", "(", "checkpoint_path", ")", "_log", ".", "info", "(", "f", "\"", "Lightning", "automatically", "upgraded", "your", "loaded", "checkpoint", "from", "v", "{", "old_version", "}", "to", "v", "{", "new_version", "}", ".", "\"", "\"", "To", "apply", "the", "upgrade", "to", "your", "files", "permanently", ",", "run", "\"", "f", "\"", "`", "python", "-", "m", "lightning", ".", "pytorch", ".", "utilities", ".", "upgrade_checkpoint", "{", "str", "(", "path_hint", ")", "}", "`", "\"", ")", "return", "checkpoint"], "docstring": "Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.\r\n\r\n    This function is used by the Lightning Trainer when resuming from a checkpoint.", "docstring_tokens": ["applies", "lightning", "version", "migrations", "to", "a", "checkpoint", "dictionary", "and", "prints", "infos", "for", "the", "user", "this", "function", "is", "used", "by", "the", "lightning", "trainer", "when", "resuming", "from", "a", "checkpoint"], "docstring_summary": "Applies Lightning version migrations to a checkpoint dictionary and prints infos for the user.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\utils.py", "partition": "test", "function_type": "function", "start_line": 136, "end_line": 158, "hash": "44f11bad307fa8cfbdc7f82d7ef7e440", "complexity": 4, "parameters": ["checkpoint", "checkpoint_path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\migration\\utils.py", "func_name": "_should_upgrade", "original_string": "def _should_upgrade(checkpoint: _CHECKPOINT, target: str, max_version: Optional[str] = None) -> bool:\r\n    \"\"\"Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.\"\"\"\r\n    target_version = Version(target)\r\n    is_lte_max_version = max_version is None or target_version <= Version(max_version)\r\n    return is_lte_max_version and Version(_get_version(checkpoint)) < target_version", "language": "python", "code": "def _should_upgrade(checkpoint: _CHECKPOINT, target: str, max_version: Optional[str] = None) -> bool:\r\n    \"\"\"Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.\"\"\"\r\n    target_version = Version(target)\r\n    is_lte_max_version = max_version is None or target_version <= Version(max_version)\r\n    return is_lte_max_version and Version(_get_version(checkpoint)) < target_version", "code_tokens": ["def", "_should_upgrade", "(", "checkpoint", ":", "_CHECKPOINT", ",", "target", ":", "str", ",", "max_version", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "whether", "a", "checkpoint", "qualifies", "for", "an", "upgrade", "when", "the", "version", "is", "lower", "than", "the", "given", "target", ".", "\"", "\"", "\"", "target_version", "=", "Version", "(", "target", ")", "is_lte_max_version", "=", "max_version", "is", "None", "or", "target_version", "<", "=", "Version", "(", "max_version", ")", "return", "is_lte_max_version", "and", "Version", "(", "_get_version", "(", "checkpoint", ")", ")", "<", "target_version"], "docstring": "Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.", "docstring_tokens": ["returns", "whether", "a", "checkpoint", "qualifies", "for", "an", "upgrade", "when", "the", "version", "is", "lower", "than", "the", "given", "target"], "docstring_summary": "Returns whether a checkpoint qualifies for an upgrade when the version is lower than the given target.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\migration\\utils.py", "partition": "test", "function_type": "function", "start_line": 176, "end_line": 180, "hash": "6511f688dffa94619a3a1ae4030e8ff3", "complexity": 3, "parameters": ["checkpoint", "target", "max_version"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "_register_hook", "original_string": "def _register_hook(self) -> Optional[RemovableHandle]:\r\n        \"\"\"Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the\r\n        hook is called, it will remove itself from the from the module, meaning that recursive models will only record\r\n        their input- and output shapes once. Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\r\n\r\n        Return:\r\n            A handle for the installed hook, or ``None`` if registering the hook is not possible.\r\n\r\n        \"\"\"\r\n\r\n        def hook(_: nn.Module, inp: Any, out: Any) -> None:\r\n            if len(inp) == 1:\r\n                inp = inp[0]\r\n\r\n            self._in_size = parse_batch_shape(inp)\r\n            self._out_size = parse_batch_shape(out)\r\n            assert self._hook_handle is not None\r\n            self._hook_handle.remove()\r\n\r\n        def hook_with_kwargs(_: nn.Module, args: Any, kwargs: Any, out: Any) -> None:\r\n            # We can't write them in the same function, since the forward hook\r\n            # uses positional arguments.\r\n\r\n            inp = (*args, *kwargs.values()) if kwargs is not None else args\r\n            hook(_, inp, out)\r\n\r\n        handle = None\r\n        if not isinstance(self._module, torch.jit.ScriptModule):\r\n            handle = self._module.register_forward_hook(hook_with_kwargs, with_kwargs=True)\r\n\r\n        return handle", "language": "python", "code": "def _register_hook(self) -> Optional[RemovableHandle]:\r\n        \"\"\"Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the\r\n        hook is called, it will remove itself from the from the module, meaning that recursive models will only record\r\n        their input- and output shapes once. Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\r\n\r\n        Return:\r\n            A handle for the installed hook, or ``None`` if registering the hook is not possible.\r\n\r\n        \"\"\"\r\n\r\n        def hook(_: nn.Module, inp: Any, out: Any) -> None:\r\n            if len(inp) == 1:\r\n                inp = inp[0]\r\n\r\n            self._in_size = parse_batch_shape(inp)\r\n            self._out_size = parse_batch_shape(out)\r\n            assert self._hook_handle is not None\r\n            self._hook_handle.remove()\r\n\r\n        def hook_with_kwargs(_: nn.Module, args: Any, kwargs: Any, out: Any) -> None:\r\n            # We can't write them in the same function, since the forward hook\r\n            # uses positional arguments.\r\n\r\n            inp = (*args, *kwargs.values()) if kwargs is not None else args\r\n            hook(_, inp, out)\r\n\r\n        handle = None\r\n        if not isinstance(self._module, torch.jit.ScriptModule):\r\n            handle = self._module.register_forward_hook(hook_with_kwargs, with_kwargs=True)\r\n\r\n        return handle", "code_tokens": ["def", "_register_hook", "(", "self", ")", "-", ">", "Optional", "[", "RemovableHandle", "]", ":", "\"", "\"", "\"", "Registers", "a", "hook", "on", "the", "module", "that", "computes", "the", "input", "-", "and", "output", "size", "(", "s", ")", "on", "the", "first", "forward", "pass", ".", "If", "the", "hook", "is", "called", ",", "it", "will", "remove", "itself", "from", "the", "from", "the", "module", ",", "meaning", "that", "recursive", "models", "will", "only", "record", "their", "input", "-", "and", "output", "shapes", "once", ".", "Registering", "hooks", "on", ":", "class", ":", "`", "~", "torch", ".", "jit", ".", "ScriptModule", "`", "is", "not", "supported", ".", "Return", ":", "A", "handle", "for", "the", "installed", "hook", ",", "or", "`", "`", "None", "`", "`", "if", "registering", "the", "hook", "is", "not", "possible", ".", "\"", "\"", "\"", "def", "hook", "(", "_", ":", "nn", ".", "Module", ",", "inp", ":", "Any", ",", "out", ":", "Any", ")", "-", ">", "None", ":", "if", "len", "(", "inp", ")", "=", "=", "1", ":", "inp", "=", "inp", "[", "0", "]", "self", ".", "_in_size", "=", "parse_batch_shape", "(", "inp", ")", "self", ".", "_out_size", "=", "parse_batch_shape", "(", "out", ")", "assert", "self", ".", "_hook_handle", "is", "not", "None", "self", ".", "_hook_handle", ".", "remove", "(", ")", "def", "hook_with_kwargs", "(", "_", ":", "nn", ".", "Module", ",", "args", ":", "Any", ",", "kwargs", ":", "Any", ",", "out", ":", "Any", ")", "-", ">", "None", ":", "inp", "=", "(", "*", "args", ",", "*", "kwargs", ".", "values", "(", ")", ")", "if", "kwargs", "is", "not", "None", "else", "args", "hook", "(", "_", ",", "inp", ",", "out", ")", "handle", "=", "None", "if", "not", "isinstance", "(", "self", ".", "_module", ",", "torch", ".", "jit", ".", "ScriptModule", ")", ":", "handle", "=", "self", ".", "_module", ".", "register_forward_hook", "(", "hook_with_kwargs", ",", "with_kwargs", "=", "True", ")", "return", "handle"], "docstring": "Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the\r\n        hook is called, it will remove itself from the from the module, meaning that recursive models will only record\r\n        their input- and output shapes once. Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\r\n\r\n        Return:\r\n            A handle for the installed hook, or ``None`` if registering the hook is not possible.", "docstring_tokens": ["registers", "a", "hook", "on", "the", "module", "that", "computes", "the", "input", "and", "output", "size", "s", "on", "the", "first", "forward", "pass", "if", "the", "hook", "is", "called", "it", "will", "remove", "itself", "from", "the", "from", "the", "module", "meaning", "that", "recursive", "models", "will", "only", "record", "their", "input", "and", "output", "shapes", "once", "registering", "hooks", "on", "class", "torch", "jit", "scriptmodule", "is", "not", "supported", "return", "a", "handle", "for", "the", "installed", "hook", "or", "none", "if", "registering", "the", "hook", "is", "not", "possible"], "docstring_summary": "Registers a hook on the module that computes the input- and output size(s) on the first forward pass. If the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "class_method", "class_name": "LayerSummary", "start_line": 84, "end_line": 114, "hash": "7e70c2a724be1a79246173969f443177", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "detach_hook", "original_string": "def detach_hook(self) -> None:\r\n        \"\"\"Removes the forward hook if it was not already removed in the forward pass.\r\n\r\n        Will be called after the summary is created.\r\n\r\n        \"\"\"\r\n        if self._hook_handle is not None:\r\n            self._hook_handle.remove()", "language": "python", "code": "def detach_hook(self) -> None:\r\n        \"\"\"Removes the forward hook if it was not already removed in the forward pass.\r\n\r\n        Will be called after the summary is created.\r\n\r\n        \"\"\"\r\n        if self._hook_handle is not None:\r\n            self._hook_handle.remove()", "code_tokens": ["def", "detach_hook", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Removes", "the", "forward", "hook", "if", "it", "was", "not", "already", "removed", "in", "the", "forward", "pass", ".", "Will", "be", "called", "after", "the", "summary", "is", "created", ".", "\"", "\"", "\"", "if", "self", ".", "_hook_handle", "is", "not", "None", ":", "self", ".", "_hook_handle", ".", "remove", "(", ")"], "docstring": "Removes the forward hook if it was not already removed in the forward pass.\r\n\r\n        Will be called after the summary is created.", "docstring_tokens": ["removes", "the", "forward", "hook", "if", "it", "was", "not", "already", "removed", "in", "the", "forward", "pass", "will", "be", "called", "after", "the", "summary", "is", "created"], "docstring_summary": "Removes the forward hook if it was not already removed in the forward pass.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "class_method", "class_name": "LayerSummary", "start_line": 116, "end_line": 123, "hash": "3d76c1fb9d136e2bb743c0bf3bcf3036", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "_forward_example_input", "original_string": "def _forward_example_input(self) -> None:\r\n        \"\"\"Run the example input through each layer to get input- and output sizes.\"\"\"\r\n        model = self._model\r\n        # the summary is supported without a trainer instance so we need to use the underscore property\r\n        trainer = self._model._trainer\r\n\r\n        input_ = model.example_input_array\r\n        input_ = model._on_before_batch_transfer(input_)\r\n        input_ = model._apply_batch_transfer_handler(input_)\r\n\r\n        mode = _ModuleMode()\r\n        mode.capture(model)\r\n        model.eval()\r\n\r\n        # FlopCounterMode does not support ScriptModules before torch 2.4.0, so we use a null context\r\n        flop_context = (\r\n            contextlib.nullcontext()\r\n            if (\r\n                not _TORCH_GREATER_EQUAL_2_4\r\n                and any(isinstance(m, torch.jit.ScriptModule) for m in self._model.modules())\r\n            )\r\n            else self._flop_counter\r\n        )\r\n\r\n        forward_context = contextlib.nullcontext() if trainer is None else trainer.precision_plugin.forward_context()\r\n        with torch.no_grad(), forward_context, flop_context:\r\n            # let the model hooks collect the input- and output shapes\r\n            if isinstance(input_, (list, tuple)):\r\n                model(*input_)\r\n            elif isinstance(input_, dict):\r\n                model(**input_)\r\n            else:\r\n                model(input_)\r\n        mode.restore(model)", "language": "python", "code": "def _forward_example_input(self) -> None:\r\n        \"\"\"Run the example input through each layer to get input- and output sizes.\"\"\"\r\n        model = self._model\r\n        # the summary is supported without a trainer instance so we need to use the underscore property\r\n        trainer = self._model._trainer\r\n\r\n        input_ = model.example_input_array\r\n        input_ = model._on_before_batch_transfer(input_)\r\n        input_ = model._apply_batch_transfer_handler(input_)\r\n\r\n        mode = _ModuleMode()\r\n        mode.capture(model)\r\n        model.eval()\r\n\r\n        # FlopCounterMode does not support ScriptModules before torch 2.4.0, so we use a null context\r\n        flop_context = (\r\n            contextlib.nullcontext()\r\n            if (\r\n                not _TORCH_GREATER_EQUAL_2_4\r\n                and any(isinstance(m, torch.jit.ScriptModule) for m in self._model.modules())\r\n            )\r\n            else self._flop_counter\r\n        )\r\n\r\n        forward_context = contextlib.nullcontext() if trainer is None else trainer.precision_plugin.forward_context()\r\n        with torch.no_grad(), forward_context, flop_context:\r\n            # let the model hooks collect the input- and output shapes\r\n            if isinstance(input_, (list, tuple)):\r\n                model(*input_)\r\n            elif isinstance(input_, dict):\r\n                model(**input_)\r\n            else:\r\n                model(input_)\r\n        mode.restore(model)", "code_tokens": ["def", "_forward_example_input", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Run", "the", "example", "input", "through", "each", "layer", "to", "get", "input", "-", "and", "output", "sizes", ".", "\"", "\"", "\"", "model", "=", "self", ".", "_model", "trainer", "=", "self", ".", "_model", ".", "_trainer", "input_", "=", "model", ".", "example_input_array", "input_", "=", "model", ".", "_on_before_batch_transfer", "(", "input_", ")", "input_", "=", "model", ".", "_apply_batch_transfer_handler", "(", "input_", ")", "mode", "=", "_ModuleMode", "(", ")", "mode", ".", "capture", "(", "model", ")", "model", ".", "eval", "(", ")", "flop_context", "=", "(", "contextlib", ".", "nullcontext", "(", ")", "if", "(", "not", "_TORCH_GREATER_EQUAL_2_4", "and", "any", "(", "isinstance", "(", "m", ",", "torch", ".", "jit", ".", "ScriptModule", ")", "for", "m", "in", "self", ".", "_model", ".", "modules", "(", ")", ")", ")", "else", "self", ".", "_flop_counter", ")", "forward_context", "=", "contextlib", ".", "nullcontext", "(", ")", "if", "trainer", "is", "None", "else", "trainer", ".", "precision_plugin", ".", "forward_context", "(", ")", "with", "torch", ".", "no_grad", "(", ")", ",", "forward_context", ",", "flop_context", ":", "if", "isinstance", "(", "input_", ",", "(", "list", ",", "tuple", ")", ")", ":", "model", "(", "*", "input_", ")", "elif", "isinstance", "(", "input_", ",", "dict", ")", ":", "model", "(", "*", "*", "input_", ")", "else", ":", "model", "(", "input_", ")", "mode", ".", "restore", "(", "model", ")"], "docstring": "Run the example input through each layer to get input- and output sizes.", "docstring_tokens": ["run", "the", "example", "input", "through", "each", "layer", "to", "get", "input", "and", "output", "sizes"], "docstring_summary": "Run the example input through each layer to get input- and output sizes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "class_method", "class_name": "ModelSummary", "start_line": 338, "end_line": 371, "hash": "ec0ee196fffdb4562e61ef7882faa884", "complexity": 8, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "_get_summary_data", "original_string": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "language": "python", "code": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "code_tokens": ["def", "_get_summary_data", "(", "self", ")", "-", ">", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "\"", "\"", "\"", "Makes", "a", "summary", "listing", "with", ":", "Layer", "Name", ",", "Layer", "Type", ",", "Number", "of", "Parameters", ",", "Input", "Sizes", ",", "Output", "Sizes", ",", "Model", "Size", "\"", "\"", "\"", "arrays", "=", "[", "(", "\"", "\"", ",", "list", "(", "map", "(", "str", ",", "range", "(", "len", "(", "self", ".", "_layer_summary", ")", ")", ")", ")", ")", ",", "(", "\"", "Name", "\"", ",", "self", ".", "layer_names", ")", ",", "(", "\"", "Type", "\"", ",", "self", ".", "layer_types", ")", ",", "(", "\"", "Params", "\"", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "param_nums", ")", ")", ")", ",", "(", "\"", "Mode", "\"", ",", "[", "\"", "train", "\"", "if", "mode", "else", "\"", "eval", "\"", "for", "mode", "in", "self", ".", "training_modes", "]", ")", ",", "(", "\"", "FLOPs", "\"", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "(", "sum", "(", "x", ".", "values", "(", ")", ")", "for", "x", "in", "self", ".", "flop_counts", ".", "values", "(", ")", ")", ")", ")", ")", ",", "]", "if", "self", ".", "_model", ".", "example_input_array", "is", "not", "None", ":", "arrays", ".", "append", "(", "(", "\"", "In", "sizes", "\"", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "in_sizes", "]", ")", ")", "arrays", ".", "append", "(", "(", "\"", "Out", "sizes", "\"", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "out_sizes", "]", ")", ")", "total_leftover_params", "=", "self", ".", "total_parameters", "-", "self", ".", "total_layer_params", "if", "total_leftover_params", ">", "0", ":", "self", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "return", "arrays"], "docstring": "Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size", "docstring_tokens": ["makes", "a", "summary", "listing", "with", "layer", "name", "layer", "type", "number", "of", "parameters", "input", "sizes", "output", "sizes", "model", "size"], "docstring_summary": "Makes a summary listing with:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "class_method", "class_name": "ModelSummary", "start_line": 373, "end_line": 395, "hash": "7cb8e138284d7de76ca4383598fcb5ca", "complexity": 8, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "_add_leftover_params_to_summary", "original_string": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\" \"].append(\" \")\r\n        layer_summaries[\"Name\"].append(LEFTOVER_PARAMS_NAME)\r\n        layer_summaries[\"Type\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"Params\"].append(get_human_readable_count(total_leftover_params))\r\n        layer_summaries[\"Mode\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"FLOPs\"].append(NOT_APPLICABLE)\r\n        if \"In sizes\" in layer_summaries:\r\n            layer_summaries[\"In sizes\"].append(NOT_APPLICABLE)\r\n        if \"Out sizes\" in layer_summaries:\r\n            layer_summaries[\"Out sizes\"].append(NOT_APPLICABLE)", "language": "python", "code": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\" \"].append(\" \")\r\n        layer_summaries[\"Name\"].append(LEFTOVER_PARAMS_NAME)\r\n        layer_summaries[\"Type\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"Params\"].append(get_human_readable_count(total_leftover_params))\r\n        layer_summaries[\"Mode\"].append(NOT_APPLICABLE)\r\n        layer_summaries[\"FLOPs\"].append(NOT_APPLICABLE)\r\n        if \"In sizes\" in layer_summaries:\r\n            layer_summaries[\"In sizes\"].append(NOT_APPLICABLE)\r\n        if \"Out sizes\" in layer_summaries:\r\n            layer_summaries[\"Out sizes\"].append(NOT_APPLICABLE)", "code_tokens": ["def", "_add_leftover_params_to_summary", "(", "self", ",", "arrays", ":", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ",", "total_leftover_params", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary", ".", "\"", "\"", "\"", "layer_summaries", "=", "dict", "(", "arrays", ")", "layer_summaries", "[", "\"", "\"", "]", ".", "append", "(", "\"", "\"", ")", "layer_summaries", "[", "\"", "Name", "\"", "]", ".", "append", "(", "LEFTOVER_PARAMS_NAME", ")", "layer_summaries", "[", "\"", "Type", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "layer_summaries", "[", "\"", "Params", "\"", "]", ".", "append", "(", "get_human_readable_count", "(", "total_leftover_params", ")", ")", "layer_summaries", "[", "\"", "Mode", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "layer_summaries", "[", "\"", "FLOPs", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "if", "\"", "In", "sizes", "\"", "in", "layer_summaries", ":", "layer_summaries", "[", "\"", "In", "sizes", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")", "if", "\"", "Out", "sizes", "\"", "in", "layer_summaries", ":", "layer_summaries", "[", "\"", "Out", "sizes", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")"], "docstring": "Add summary of params not associated with module or layer to model summary.", "docstring_tokens": ["add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary"], "docstring_summary": "Add summary of params not associated with module or layer to model summary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "class_method", "class_name": "ModelSummary", "start_line": 397, "end_line": 409, "hash": "3a133e2de2d4817579a13810826fc5f3", "complexity": 3, "parameters": ["arrays", "list[str]]]", "total_leftover_params"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "_format_summary_table", "original_string": "def _format_summary_table(\r\n    total_parameters: int,\r\n    trainable_parameters: int,\r\n    model_size: float,\r\n    total_training_modes: dict[str, int],\r\n    total_flops: int,\r\n    *cols: tuple[str, list[str]],\r\n) -> str:\r\n    \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\r\n    string defining the summary table that are nicely formatted.\"\"\"\r\n    n_rows = len(cols[0][1])\r\n    n_cols = 1 + len(cols)\r\n\r\n    # Get formatting width of each column\r\n    col_widths = []\r\n    for c in cols:\r\n        col_width = max(len(str(a)) for a in c[1]) if n_rows else 0\r\n        col_width = max(col_width, len(c[0]))  # minimum length is header length\r\n        col_widths.append(col_width)\r\n\r\n    # Formatting\r\n    s = \"{:<{}}\"\r\n    total_width = sum(col_widths) + 3 * n_cols\r\n    header = [s.format(c[0], w) for c, w in zip(cols, col_widths)]\r\n\r\n    # Summary = header + divider + Rest of table\r\n    summary = \" | \".join(header) + \"\\n\" + \"-\" * total_width\r\n    for i in range(n_rows):\r\n        line = []\r\n        for c, w in zip(cols, col_widths):\r\n            line.append(s.format(str(c[1][i]), w))\r\n        summary += \"\\n\" + \" | \".join(line)\r\n    summary += \"\\n\" + \"-\" * total_width\r\n\r\n    summary += \"\\n\" + s.format(get_human_readable_count(trainable_parameters), 10)\r\n    summary += \"Trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters - trainable_parameters), 10)\r\n    summary += \"Non-trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters), 10)\r\n    summary += \"Total params\"\r\n    summary += \"\\n\" + s.format(get_formatted_model_size(model_size), 10)\r\n    summary += \"Total estimated model params size (MB)\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"train\"], 10)\r\n    summary += \"Modules in train mode\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"eval\"], 10)\r\n    summary += \"Modules in eval mode\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_flops), 10)\r\n    summary += \"Total Flops\"\r\n\r\n    return summary", "language": "python", "code": "def _format_summary_table(\r\n    total_parameters: int,\r\n    trainable_parameters: int,\r\n    model_size: float,\r\n    total_training_modes: dict[str, int],\r\n    total_flops: int,\r\n    *cols: tuple[str, list[str]],\r\n) -> str:\r\n    \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\r\n    string defining the summary table that are nicely formatted.\"\"\"\r\n    n_rows = len(cols[0][1])\r\n    n_cols = 1 + len(cols)\r\n\r\n    # Get formatting width of each column\r\n    col_widths = []\r\n    for c in cols:\r\n        col_width = max(len(str(a)) for a in c[1]) if n_rows else 0\r\n        col_width = max(col_width, len(c[0]))  # minimum length is header length\r\n        col_widths.append(col_width)\r\n\r\n    # Formatting\r\n    s = \"{:<{}}\"\r\n    total_width = sum(col_widths) + 3 * n_cols\r\n    header = [s.format(c[0], w) for c, w in zip(cols, col_widths)]\r\n\r\n    # Summary = header + divider + Rest of table\r\n    summary = \" | \".join(header) + \"\\n\" + \"-\" * total_width\r\n    for i in range(n_rows):\r\n        line = []\r\n        for c, w in zip(cols, col_widths):\r\n            line.append(s.format(str(c[1][i]), w))\r\n        summary += \"\\n\" + \" | \".join(line)\r\n    summary += \"\\n\" + \"-\" * total_width\r\n\r\n    summary += \"\\n\" + s.format(get_human_readable_count(trainable_parameters), 10)\r\n    summary += \"Trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters - trainable_parameters), 10)\r\n    summary += \"Non-trainable params\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters), 10)\r\n    summary += \"Total params\"\r\n    summary += \"\\n\" + s.format(get_formatted_model_size(model_size), 10)\r\n    summary += \"Total estimated model params size (MB)\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"train\"], 10)\r\n    summary += \"Modules in train mode\"\r\n    summary += \"\\n\" + s.format(total_training_modes[\"eval\"], 10)\r\n    summary += \"Modules in eval mode\"\r\n    summary += \"\\n\" + s.format(get_human_readable_count(total_flops), 10)\r\n    summary += \"Total Flops\"\r\n\r\n    return summary", "code_tokens": ["def", "_format_summary_table", "(", "total_parameters", ":", "int", ",", "trainable_parameters", ":", "int", ",", "model_size", ":", "float", ",", "total_training_modes", ":", "dict", "[", "str", ",", "int", "]", ",", "total_flops", ":", "int", ",", "*", "cols", ":", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", ",", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Takes", "in", "a", "number", "of", "arrays", ",", "each", "specifying", "a", "column", "in", "the", "summary", "table", ",", "and", "combines", "them", "all", "into", "one", "big", "string", "defining", "the", "summary", "table", "that", "are", "nicely", "formatted", ".", "\"", "\"", "\"", "n_rows", "=", "len", "(", "cols", "[", "0", "]", "[", "1", "]", ")", "n_cols", "=", "1", "+", "len", "(", "cols", ")", "col_widths", "=", "[", "]", "for", "c", "in", "cols", ":", "col_width", "=", "max", "(", "len", "(", "str", "(", "a", ")", ")", "for", "a", "in", "c", "[", "1", "]", ")", "if", "n_rows", "else", "0", "col_width", "=", "max", "(", "col_width", ",", "len", "(", "c", "[", "0", "]", ")", ")", "col_widths", ".", "append", "(", "col_width", ")", "s", "=", "\"", "{", ":", "<", "{", "}", "}", "\"", "total_width", "=", "sum", "(", "col_widths", ")", "+", "3", "*", "n_cols", "header", "=", "[", "s", ".", "format", "(", "c", "[", "0", "]", ",", "w", ")", "for", "c", ",", "w", "in", "zip", "(", "cols", ",", "col_widths", ")", "]", "summary", "=", "\"", "|", "\"", ".", "join", "(", "header", ")", "+", "\"", "\\", "n", "\"", "+", "\"", "-", "\"", "*", "total_width", "for", "i", "in", "range", "(", "n_rows", ")", ":", "line", "=", "[", "]", "for", "c", ",", "w", "in", "zip", "(", "cols", ",", "col_widths", ")", ":", "line", ".", "append", "(", "s", ".", "format", "(", "str", "(", "c", "[", "1", "]", "[", "i", "]", ")", ",", "w", ")", ")", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "\"", "|", "\"", ".", "join", "(", "line", ")", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "\"", "-", "\"", "*", "total_width", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "trainable_parameters", ")", ",", "10", ")", "summary", "+", "=", "\"", "Trainable", "params", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_parameters", "-", "trainable_parameters", ")", ",", "10", ")", "summary", "+", "=", "\"", "Non", "-", "trainable", "params", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_parameters", ")", ",", "10", ")", "summary", "+", "=", "\"", "Total", "params", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "get_formatted_model_size", "(", "model_size", ")", ",", "10", ")", "summary", "+", "=", "\"", "Total", "estimated", "model", "params", "size", "(", "MB", ")", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "total_training_modes", "[", "\"", "train", "\"", "]", ",", "10", ")", "summary", "+", "=", "\"", "Modules", "in", "train", "mode", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "total_training_modes", "[", "\"", "eval", "\"", "]", ",", "10", ")", "summary", "+", "=", "\"", "Modules", "in", "eval", "mode", "\"", "summary", "+", "=", "\"", "\\", "n", "\"", "+", "s", ".", "format", "(", "get_human_readable_count", "(", "total_flops", ")", ",", "10", ")", "summary", "+", "=", "\"", "Total", "Flops", "\"", "return", "summary"], "docstring": "Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\r\n    string defining the summary table that are nicely formatted.", "docstring_tokens": ["takes", "in", "a", "number", "of", "arrays", "each", "specifying", "a", "column", "in", "the", "summary", "table", "and", "combines", "them", "all", "into", "one", "big", "string", "defining", "the", "summary", "table", "that", "are", "nicely", "formatted"], "docstring_summary": "Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "function", "start_line": 443, "end_line": 492, "hash": "4bc1ccb8753338690fda1333a0779d52", "complexity": 7, "parameters": ["total_parameters", "trainable_parameters", "model_size", "total_training_modes", "int]", "total_flops", "*cols", "list[str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "get_human_readable_count", "original_string": "def get_human_readable_count(number: int) -> str:\r\n    \"\"\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.\r\n\r\n    Examples:\r\n        >>> get_human_readable_count(123)\r\n        '123  '\r\n        >>> get_human_readable_count(1234)  # (one thousand)\r\n        '1.2 K'\r\n        >>> get_human_readable_count(2e6)   # (two million)\r\n        '2.0 M'\r\n        >>> get_human_readable_count(3e9)   # (three billion)\r\n        '3.0 B'\r\n        >>> get_human_readable_count(4e14)  # (four hundred trillion)\r\n        '400 T'\r\n        >>> get_human_readable_count(5e15)  # (more than trillion)\r\n        '5,000 T'\r\n\r\n    Args:\r\n        number: a positive integer number\r\n\r\n    Return:\r\n        A string formatted according to the pattern described above.\r\n\r\n    \"\"\"\r\n    assert number >= 0\r\n    labels = PARAMETER_NUM_UNITS\r\n    num_digits = int(math.floor(math.log10(number)) + 1 if number > 0 else 1)\r\n    num_groups = int(math.ceil(num_digits / 3))\r\n    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions\r\n    shift = -3 * (num_groups - 1)\r\n    number = number * (10**shift)\r\n    index = num_groups - 1\r\n    if index < 1 or number >= 100:\r\n        return f\"{int(number):,d} {labels[index]}\"\r\n\r\n    return f\"{number:,.1f} {labels[index]}\"", "language": "python", "code": "def get_human_readable_count(number: int) -> str:\r\n    \"\"\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.\r\n\r\n    Examples:\r\n        >>> get_human_readable_count(123)\r\n        '123  '\r\n        >>> get_human_readable_count(1234)  # (one thousand)\r\n        '1.2 K'\r\n        >>> get_human_readable_count(2e6)   # (two million)\r\n        '2.0 M'\r\n        >>> get_human_readable_count(3e9)   # (three billion)\r\n        '3.0 B'\r\n        >>> get_human_readable_count(4e14)  # (four hundred trillion)\r\n        '400 T'\r\n        >>> get_human_readable_count(5e15)  # (more than trillion)\r\n        '5,000 T'\r\n\r\n    Args:\r\n        number: a positive integer number\r\n\r\n    Return:\r\n        A string formatted according to the pattern described above.\r\n\r\n    \"\"\"\r\n    assert number >= 0\r\n    labels = PARAMETER_NUM_UNITS\r\n    num_digits = int(math.floor(math.log10(number)) + 1 if number > 0 else 1)\r\n    num_groups = int(math.ceil(num_digits / 3))\r\n    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions\r\n    shift = -3 * (num_groups - 1)\r\n    number = number * (10**shift)\r\n    index = num_groups - 1\r\n    if index < 1 or number >= 100:\r\n        return f\"{int(number):,d} {labels[index]}\"\r\n\r\n    return f\"{number:,.1f} {labels[index]}\"", "code_tokens": ["def", "get_human_readable_count", "(", "number", ":", "int", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Abbreviates", "an", "integer", "number", "with", "K", ",", "M", ",", "B", ",", "T", "for", "thousands", ",", "millions", ",", "billions", "and", "trillions", ",", "respectively", ".", "Examples", ":", ">", ">", ">", "get_human_readable_count", "(", "123", ")", "'", "123", "'", ">", ">", ">", "get_human_readable_count", "(", "1234", ")", "'", "1", ".", "2", "K", "'", ">", ">", ">", "get_human_readable_count", "(", "2e6", ")", "'", "2", ".", "0", "M", "'", ">", ">", ">", "get_human_readable_count", "(", "3e9", ")", "'", "3", ".", "0", "B", "'", ">", ">", ">", "get_human_readable_count", "(", "4e14", ")", "'", "400", "T", "'", ">", ">", ">", "get_human_readable_count", "(", "5e15", ")", "'", "5", ",", "000", "T", "'", "Args", ":", "number", ":", "a", "positive", "integer", "number", "Return", ":", "A", "string", "formatted", "according", "to", "the", "pattern", "described", "above", ".", "\"", "\"", "\"", "assert", "number", ">", "=", "0", "labels", "=", "PARAMETER_NUM_UNITS", "num_digits", "=", "int", "(", "math", ".", "floor", "(", "math", ".", "log10", "(", "number", ")", ")", "+", "1", "if", "number", ">", "0", "else", "1", ")", "num_groups", "=", "int", "(", "math", ".", "ceil", "(", "num_digits", "/", "3", ")", ")", "num_groups", "=", "min", "(", "num_groups", ",", "len", "(", "labels", ")", ")", "shift", "=", "-", "3", "*", "(", "num_groups", "-", "1", ")", "number", "=", "number", "*", "(", "10", "*", "*", "shift", ")", "index", "=", "num_groups", "-", "1", "if", "index", "<", "1", "or", "number", ">", "=", "100", ":", "return", "f", "\"", "{", "int", "(", "number", ")", ":", ",", "d", "}", "{", "labels", "[", "index", "]", "}", "\"", "return", "f", "\"", "{", "number", ":", ",", ".", "1f", "}", "{", "labels", "[", "index", "]", "}", "\""], "docstring": "Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.\r\n\r\n    Examples:\r\n        >>> get_human_readable_count(123)\r\n        '123  '\r\n        >>> get_human_readable_count(1234)  # (one thousand)\r\n        '1.2 K'\r\n        >>> get_human_readable_count(2e6)   # (two million)\r\n        '2.0 M'\r\n        >>> get_human_readable_count(3e9)   # (three billion)\r\n        '3.0 B'\r\n        >>> get_human_readable_count(4e14)  # (four hundred trillion)\r\n        '400 T'\r\n        >>> get_human_readable_count(5e15)  # (more than trillion)\r\n        '5,000 T'\r\n\r\n    Args:\r\n        number: a positive integer number\r\n\r\n    Return:\r\n        A string formatted according to the pattern described above.", "docstring_tokens": ["abbreviates", "an", "integer", "number", "with", "k", "m", "b", "t", "for", "thousands", "millions", "billions", "and", "trillions", "respectively", "examples", "get_human_readable_count", "123", "123", "get_human_readable_count", "1234", "one", "thousand", "1", "2", "k", "get_human_readable_count", "2e6", "two", "million", "2", "0", "m", "get_human_readable_count", "3e9", "three", "billion", "3", "0", "b", "get_human_readable_count", "4e14", "four", "hundred", "trillion", "400", "t", "get_human_readable_count", "5e15", "more", "than", "trillion", "5", "000", "t", "args", "number", "a", "positive", "integer", "number", "return", "a", "string", "formatted", "according", "to", "the", "pattern", "described", "above"], "docstring_summary": "Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "function", "start_line": 499, "end_line": 534, "hash": "a768e2683d948d510e92322596f2e3e0", "complexity": 4, "parameters": ["number"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "func_name": "summarize", "original_string": "def summarize(lightning_module: \"pl.LightningModule\", max_depth: int = 1) -> ModelSummary:\r\n    \"\"\"Summarize the LightningModule specified by `lightning_module`.\r\n\r\n    Args:\r\n        lightning_module: `LightningModule` to summarize.\r\n\r\n        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the\r\n            layer summary off. Default: 1.\r\n\r\n    Return:\r\n        The model summary object\r\n\r\n    \"\"\"\r\n    return ModelSummary(lightning_module, max_depth=max_depth)", "language": "python", "code": "def summarize(lightning_module: \"pl.LightningModule\", max_depth: int = 1) -> ModelSummary:\r\n    \"\"\"Summarize the LightningModule specified by `lightning_module`.\r\n\r\n    Args:\r\n        lightning_module: `LightningModule` to summarize.\r\n\r\n        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the\r\n            layer summary off. Default: 1.\r\n\r\n    Return:\r\n        The model summary object\r\n\r\n    \"\"\"\r\n    return ModelSummary(lightning_module, max_depth=max_depth)", "code_tokens": ["def", "summarize", "(", "lightning_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "max_depth", ":", "int", "=", "1", ")", "-", ">", "ModelSummary", ":", "\"", "\"", "\"", "Summarize", "the", "LightningModule", "specified", "by", "`", "lightning_module", "`", ".", "Args", ":", "lightning_module", ":", "`", "LightningModule", "`", "to", "summarize", ".", "max_depth", ":", "The", "maximum", "depth", "of", "layer", "nesting", "that", "the", "summary", "will", "include", ".", "A", "value", "of", "0", "turns", "the", "layer", "summary", "off", ".", "Default", ":", "1", ".", "Return", ":", "The", "model", "summary", "object", "\"", "\"", "\"", "return", "ModelSummary", "(", "lightning_module", ",", "max_depth", "=", "max_depth", ")"], "docstring": "Summarize the LightningModule specified by `lightning_module`.\r\n\r\n    Args:\r\n        lightning_module: `LightningModule` to summarize.\r\n\r\n        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the\r\n            layer summary off. Default: 1.\r\n\r\n    Return:\r\n        The model summary object", "docstring_tokens": ["summarize", "the", "lightningmodule", "specified", "by", "lightning_module", "args", "lightning_module", "lightningmodule", "to", "summarize", "max_depth", "the", "maximum", "depth", "of", "layer", "nesting", "that", "the", "summary", "will", "include", "a", "value", "of", "0", "turns", "the", "layer", "summary", "off", "default", "1", "return", "the", "model", "summary", "object"], "docstring_summary": "Summarize the LightningModule specified by `lightning_module`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py", "partition": "test", "function_type": "function", "start_line": 551, "end_line": 564, "hash": "a8989d3deff3faedb73ddb732004cb49", "complexity": 1, "parameters": ["lightning_module", "max_depth"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "average_shard_parameters", "original_string": "def average_shard_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n\r\n        def partitioned_size(p: Parameter) -> int:\r\n            return p.partitioned_size() if RequirementCache(\"deepspeed<0.6.6\") else p.partition_numel()\r\n\r\n        return sum(partitioned_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "language": "python", "code": "def average_shard_parameters(self) -> int:\r\n        \"\"\"Returns the number of parameters in this module.\"\"\"\r\n\r\n        def partitioned_size(p: Parameter) -> int:\r\n            return p.partitioned_size() if RequirementCache(\"deepspeed<0.6.6\") else p.partition_numel()\r\n\r\n        return sum(partitioned_size(p) if not _tensor_has_shape(p) else 0 for p in self._module.parameters())", "code_tokens": ["def", "average_shard_parameters", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Returns", "the", "number", "of", "parameters", "in", "this", "module", ".", "\"", "\"", "\"", "def", "partitioned_size", "(", "p", ":", "Parameter", ")", "-", ">", "int", ":", "return", "p", ".", "partitioned_size", "(", ")", "if", "RequirementCache", "(", "\"", "deepspeed", "<", "0", ".", "6", ".", "6", "\"", ")", "else", "p", ".", "partition_numel", "(", ")", "return", "sum", "(", "partitioned_size", "(", "p", ")", "if", "not", "_tensor_has_shape", "(", "p", ")", "else", "0", "for", "p", "in", "self", ".", "_module", ".", "parameters", "(", ")", ")"], "docstring": "Returns the number of parameters in this module.", "docstring_tokens": ["returns", "the", "number", "of", "parameters", "in", "this", "module"], "docstring_summary": "Returns the number of parameters in this module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "partition": "test", "function_type": "class_method", "class_name": "DeepSpeedLayerSummary", "start_line": 44, "end_line": 50, "hash": "6df5581f7596ceb84baa1e1270d4d62c", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "_get_summary_data", "original_string": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Params per Device\", list(map(get_human_readable_count, self.parameters_per_layer))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "language": "python", "code": "def _get_summary_data(self) -> list[tuple[str, list[str]]]:\r\n        \"\"\"Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\r\n\r\n        \"\"\"\r\n        arrays = [\r\n            (\" \", list(map(str, range(len(self._layer_summary))))),\r\n            (\"Name\", self.layer_names),\r\n            (\"Type\", self.layer_types),\r\n            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\r\n            (\"Params per Device\", list(map(get_human_readable_count, self.parameters_per_layer))),\r\n            (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\r\n            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\r\n        ]\r\n        if self._model.example_input_array is not None:\r\n            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\r\n            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\r\n\r\n        total_leftover_params = self.total_parameters - self.total_layer_params\r\n        if total_leftover_params > 0:\r\n            self._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n\r\n        return arrays", "code_tokens": ["def", "_get_summary_data", "(", "self", ")", "-", ">", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ":", "\"", "\"", "\"", "Makes", "a", "summary", "listing", "with", ":", "Layer", "Name", ",", "Layer", "Type", ",", "Number", "of", "Parameters", ",", "Input", "Sizes", ",", "Output", "Sizes", ",", "Model", "Size", "\"", "\"", "\"", "arrays", "=", "[", "(", "\"", "\"", ",", "list", "(", "map", "(", "str", ",", "range", "(", "len", "(", "self", ".", "_layer_summary", ")", ")", ")", ")", ")", ",", "(", "\"", "Name", "\"", ",", "self", ".", "layer_names", ")", ",", "(", "\"", "Type", "\"", ",", "self", ".", "layer_types", ")", ",", "(", "\"", "Params", "\"", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "param_nums", ")", ")", ")", ",", "(", "\"", "Params", "per", "Device", "\"", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "self", ".", "parameters_per_layer", ")", ")", ")", ",", "(", "\"", "Mode", "\"", ",", "[", "\"", "train", "\"", "if", "mode", "else", "\"", "eval", "\"", "for", "mode", "in", "self", ".", "training_modes", "]", ")", ",", "(", "\"", "FLOPs", "\"", ",", "list", "(", "map", "(", "get_human_readable_count", ",", "(", "sum", "(", "x", ".", "values", "(", ")", ")", "for", "x", "in", "self", ".", "flop_counts", ".", "values", "(", ")", ")", ")", ")", ")", ",", "]", "if", "self", ".", "_model", ".", "example_input_array", "is", "not", "None", ":", "arrays", ".", "append", "(", "(", "\"", "In", "sizes", "\"", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "in_sizes", "]", ")", ")", "arrays", ".", "append", "(", "(", "\"", "Out", "sizes", "\"", ",", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "out_sizes", "]", ")", ")", "total_leftover_params", "=", "self", ".", "total_parameters", "-", "self", ".", "total_layer_params", "if", "total_leftover_params", ">", "0", ":", "self", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "return", "arrays"], "docstring": "Makes a summary listing with:\r\n\r\n        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size", "docstring_tokens": ["makes", "a", "summary", "listing", "with", "layer", "name", "layer", "type", "number", "of", "parameters", "input", "sizes", "output", "sizes", "model", "size"], "docstring_summary": "Makes a summary listing with:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "partition": "test", "function_type": "class_method", "class_name": "DeepSpeedSummary", "start_line": 88, "end_line": 111, "hash": "7a584a78a1c26ec179f5c2d13f850bb8", "complexity": 8, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "func_name": "_add_leftover_params_to_summary", "original_string": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        super()._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\"Params per Device\"].append(NOT_APPLICABLE)", "language": "python", "code": "def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], total_leftover_params: int) -> None:\r\n        \"\"\"Add summary of params not associated with module or layer to model summary.\"\"\"\r\n        super()._add_leftover_params_to_summary(arrays, total_leftover_params)\r\n        layer_summaries = dict(arrays)\r\n        layer_summaries[\"Params per Device\"].append(NOT_APPLICABLE)", "code_tokens": ["def", "_add_leftover_params_to_summary", "(", "self", ",", "arrays", ":", "list", "[", "tuple", "[", "str", ",", "list", "[", "str", "]", "]", "]", ",", "total_leftover_params", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary", ".", "\"", "\"", "\"", "super", "(", ")", ".", "_add_leftover_params_to_summary", "(", "arrays", ",", "total_leftover_params", ")", "layer_summaries", "=", "dict", "(", "arrays", ")", "layer_summaries", "[", "\"", "Params", "per", "Device", "\"", "]", ".", "append", "(", "NOT_APPLICABLE", ")"], "docstring": "Add summary of params not associated with module or layer to model summary.", "docstring_tokens": ["add", "summary", "of", "params", "not", "associated", "with", "module", "or", "layer", "to", "model", "summary"], "docstring_summary": "Add summary of params not associated with module or layer to model summary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\utilities\\model_summary\\model_summary_deepspeed.py", "partition": "test", "function_type": "class_method", "class_name": "DeepSpeedSummary", "start_line": 114, "end_line": 118, "hash": "32616634736caf5aa864ae818a455d0d", "complexity": 1, "parameters": ["arrays", "list[str]]]", "total_leftover_params"]}
